id,title,abstract,label8911,Zzyzx: Scalable fault tolerance through Byzantine locking,"Zzyzx is a Byzantine fault-tolerant replicated state machine protocol that outperforms prior approaches and provides near-linear throughput scaling. Using a new technique called Byzantine Locking, Zzyzx allows a client to extract state from an underlying replicated state machine and access it via a second protocol specialized for use by a single client. This second protocol requires just one round-trip and 2 f + 1 responsive servers-compared to Zyzzyva, this results in 39-43% lower response times and a factor of 2.2-2.9 higher throughput. Furthermore, the extracted state can be transferred to other servers, allowing non-overlapping sets of servers to manage different state. Thus, Zzyzx allows throughput to be scaled by adding servers when concurrent data sharing is not common. When data sharing is common, performance can match that of the underlying replicated state machine protocol.",no8910,A redundant nested invocation suppression mechanism for active replication fault-tolerant Web service,"Zwass suggested that middleware and message service is one of the five fundamental technologies used to realize electronic commerce (EC) [Zwass, V. (1996)]. The simple object access protocol (SOAP) is recognized as a more promising middleware for EC applications among other leading candidates such as CORBA. We notice that the fault-tolerance issue is somewhat neglected in the current standard, i.e., SOAP 1.1. We therefore proposed a fault tolerant Web service called fault-tolerant SOAP or FT-SOAP through which Web services can be built with higher resilience to failure. Active replication is a common approach to building highly available and reliable distributed software applications. The redundant nested invocation (RNI) problem arises when servers in a replicated group issues nested invocations to other server groups in response to a client invocation. In this work, we propose a mechanism to perform auto-suppression of redundant nested invocation in an active replication FT-SOAP system. Our approach ensures the portability requirement of a middleware, especially for FT-SOAP.",no8909,Fault tolerant Web service,"Zwass (1996) suggested that middleware and message service is one of the five fundamental technologies used to realize electronic commerce (EC). The simple object access protocol (SOAP) is recognized as a more promising middleware for EC applications among other leading candidates such as CORBA. Many recent polls reveal however that security and reliability issues are major concerns that discourage people from engaging in EC transactions. We notice that the fault-tolerance issue is somewhat neglected in the current standard, i.e., SOAP 1.1. We therefore propose a fault tolerant Web service called fault-tolerant SOAP or FT-SOAP through which Web services can be built with higher resilience to failure. FT-SOAP is based on our previous experience with an object fault tolerant service (OFS) [Liang, D. et al., (1999)] and OMG's fault tolerant CORBA (FT-CORBA). There are many architectural differences between SOAP and CORBA. One of the major contributions of this work is to discuss the impact on FT-SOAP design due to these architectural differences. Our experience shows that Web services built on a SOAP framework enjoy higher flexibility as opposed to those built on CORBA. We also point out the limitations of the current feature sets of SOAP 1.1. We believe our experience is valuable not only to the fault-tolerance community, but also to other communities as well, in particular, to those who are familiar with the CORBA platform.",no8908,"Problems with Precision: A Response to ""Comments on 'Data Mining Static Code Attributes to Learn Defect Predictors'""","Zhang and Zhang argue that predictors are useless unless they have high precison&recall. We have a different view, for two reasons. First, for SE data sets with large neg/pos ratios, it is often required to lower precision to achieve higher recall. Second, there are many domains where low precision detectors are useful.",no8907,Polarization Rotation Correction in Radiometry: An Error Analysis,"Yueh proposed a method of using the third Stokes parameter T<sub>U</sub> to correct brightness temperatures such as T<sub>v</sub> and T<sub>h</sub> for polarization rotation. This paper presents an extended error analysis of the estimation of T<sub>v</sub>, T<sub>h</sub>, and T<sub>Q</sub> equiv T<sub>v</sub> - T<sub>h</sub> by Yueh's method. In order to carry out the analysis, we first develop a forward model of polarization rotation that accounts for the random nature of thermal radiation, receiver noise, and (to first order) calibration. Analytic formulas are then derived for the bias, standard deviation (STD), and root-mean-square error (RMSE) of estimated T<sub>Q</sub>, T<sub>v</sub>, and T<sub>h</sub>, as functions of scene and radiometer parameters. These formulas are validated through independent calculation via Monte Carlo simulation. Examination of the formulas reveals that: 1) natural T<sub>U</sub> from planetary surface radiation, of the magnitude expected on Earth at L-band, has a negligible effect on correction for polarization rotation; 2) RMSE is a function of rotation angle Omega, but the value of Omega that minimizes RMSE is not known prior to instrument fabrication; and 3) if residual calibration errors can be sufficiently reduced via postlaunch calibration, then Yueh's method reduces the error incurred by polarization rotation to negligibility.",no8906,A cache-defect-aware code placement algorithm for improving the performance of processors,"Yield improvement through exploiting fault-free sections of defective chips is a well-known technique (Koren and Singh (1990) and Stapper et al. (1980)). The idea is to partition the circuitry of a chip in a way that fault-free sections can function independently. Many fault tolerant techniques for improving the yield of processors with a cache memory have been proposed. In this paper, we propose a defect-aware code placement technique which offsets the performance degradation of a processor with a defective cache memory. To the best of our knowledge, this is the first compiler-based technique which offsets the performance degradation due to cache defects. Experiments demonstrate that the technique can compensate the performance degradation even when 5% of cache lines are faulty. In some cases the technique was able to offset the impact even in presence of 25% faulty cache-lines.",no8905,Offline analysis techniques for the improvement of defect inspection recipes,"Yield enhancement techniques for the latest generation of devices need sensitive inspection recipes in order to detect the ever-smaller defects that can result in yield loss. Offline analysis techniques (using MATLAB, for example) for the improvement of bright-field defect-inspection tool recipes are presented. Simple techniques are given for the rapid incorporation or modification of care-areas/don't-care areas into pre-existing recipes. Postprocessing analyses of defect data are presented to show their efficacy in improving the signal-to-noise ratio for defects that might otherwise be hidden in the noise created by 'nuisance' defects. Examples are presented to show how design-databases and reticle inspection data can be harnessed in understanding defect mechanisms.",no8904,A neural-network approach to recognize defect spatial pattern in semiconductor fabrication,"Yield enhancement in semiconductor fabrication is important. Even though IC yield loss may be attributed to many problems, the existence of defects on the wafer is one of the main causes. When the defects on the wafer form spatial patterns, it is usually a clue for the identification of equipment problems or process variations. This research intends to develop an intelligent system, which will recognize defect spatial patterns to aid in the diagnosis of failure causes. The neural-network architecture named adaptive resonance theory network 1 (ART1) was adopted for this purpose. Actual data obtained from a semiconductor manufacturing company in Taiwan were used in experiments with the proposed system. Comparison between ART1 and another unsupervised neural network, self-organizing map (SOM), was also conducted. The results show that ART1 architecture can recognize the similar defect spatial patterns more easily and correctly",no8903,Improving Automatic Detection of Defects in Castings by Applying Wavelet Technique,"X-ray-based inspection systems are a well-accepted technique for identification and evaluation of internal defects in castings, such as cracks, porosities, and foreign inclusions. In this paper, some images showing typical internal defects in the castings derived from an X-ray inspection system are processed by some traditional methods and wavelet technique in order to facilitate automatic detection of these internal defects. An X-ray inspection system used to detect the internal defects of castings and the typical internal casting defects is first addressed. Second, the second-order derivative and morphology operations, the row-by-row adaptive thresholding, and the two-dimensional (2-D) wavelet transform methods are described as potentially useful processing techniques. The first method can effectively detect air-holes and foreign-inclusion defects, and the second one can be suitable for detecting shrinkage cavities. Wavelet techniques, however, can effectively detect the three typical defects with a selected wavelet base and multiresolution levels. Results indicate that 2-D wavelet transform is a powerful method to analyze images derived from X-ray inspection for automatically detecting typical internal defects in the casting",no8902,Comparison of different ANN techniques for automatic defect detection in X-Ray images,"X-ray imaging is extensively used in the NDT. In the conventional method, interpretation of the large number of radiographs for defect detection and evaluation is carried out manually by operator or expert, which makes the system subjective. Also interpretation of large number of images is tedious and may lead to misinterpretation. Automation of Non-Destructive evaluation techniques is gaining greater relevance but automatic analysis of X-Ray images is still a complex problem, as the images are noisy, low contrast with a number of artifacts. ANN's are systems which can be trained to analyze input data based on conditions provided to derive required output. This makes the system automatic reducing the subjective interference in analysis of data. Artificial neural network based systems are thus a feasible solution to this problem of X-Ray NDT. Due to complex nature of input images and noise present, Noise removal becomes a problem in X-Ray images. Preprocessing techniques based on statistical analysis have shown improvement in image noise reduction. Pixels/group of pixels, which deviate from the general structural pattern and grey scale distribution are located. The statistically processed pixel values are used to obtain the features vector from defective as well as from non-defective areas. Software for pre-processing and analyzing NDT images has been developed. Software allows user to train neural networks for defect detection. Once trained satisfactorily, the software scans the new input image and uses the trained ANN for defect detection. The final image with defect regions marked will be displayed. This system can be used to obtain the probable defective areas in a given input image. This paper presents performance of MLP and RBF for detection of defect. The effect of different types of input viz. template and moments on performance of ANN is discussed.",no8901,Empirical cupping correction for CT scanners with tube voltage modulation (ECCU),"X-ray CT measures the attenuation of polychromatic x-rays through an object of interest. The CT data aquired are the negative logarithm of the relative x- ray intensity behind the patient. These data must undergo water precorrection to linearize the measured data and convert them into line integrals through the patient that can be reconstructed to yield the final CT image. The function to linearize the measured projection data depends on the tube voltage U. In most circumstances, CT scans are carried out with a constant tube voltage. For those cases there are dozens of different techniques to carry out water precor-rection. In our case the tube voltage is rather modulated as a function of the object. We propose an empirical cupping correction (ECCU) algorithm to correct for CT cupping artifacts that are induced by non-linearities in the projection data. The method is rawdata-based, empirical and does neither require knowledge of the x-ray spectrum nor of the attenuation coefficients. It aims at linearizing the attenuation data using a precorrection function of polynomial form in the polychromatic attenuation data q and in the tube voltage U. The coefficients of the polynomial are determined once using a calibration scan of a homogeneous phantom. Computing the coefficients is done in image domain by fitting a series of basis images to a template image. The template image is obtained directly from the uncorrected phantom image and no assumptions on the phantom size or of its positioning are made. Rawdata are precorrected by passing them through the once-determined polynomial. Numerical examples are shown to demonstrate the quality of the precorrection. ECCU achieves to remove the cupping artifacts and to obtain well-calibrated CT-values. A combination of ECCU with analytical techniques yielding a hybrid cupping correction method is possible and allows for channel-dependent correction functions.",no8900,Detection and Classification of Wood Defects by ANN,"X-ray as a method of measurement was adopted to detect wood defects nondestructively. Due to the intensity of x-ray that crosses the object changes, defects in wood were detected by the difference of X-ray absorption parameter, and therefore it used computer to process and analyze the image. On the basis of image processing of nondestructive testing and characteristic construction, defects mathematic model were established by using characteristic parameters. According to signal characters of nondestructive testing, artificial neural networks were set up. Meanwhile, adopt BP networks model to recognize all characteristic parameters, which reflected characters of wood defects. BP networks used coefficient matrix of each unit, including input layer, intermediate layer (concealed layer) and output layer, to get the model of input vector and finish networks recognition through the networks learning. The test results show that the method is very successful for detection and classification of wood defects",no8899,Applying transformation-based error-driven learning to structured natural language queries,"XML information retrieval (XML-IR) systems aim to provide users with highly exhaustive and highly specific results. To interact with XML-IR systems, users must express both their content and structural requirement, in the form of a structured query. Traditionally, these structured queries have been formatted using formal languages such as XPath or NEXI. Unfortunately, formal query languages are very complex and too difficult to be used by experienced, let alone casual users. Therefore, recent research has investigated the idea of specifying users' content and structural needs via natural language queries (NLQs). In previous research we developed NLPX, a natural language interface to an XML-IR system. Here we present additions we have made to NLPX. The additions involve the application of transformation-based error-driven learning (TBL) to structured NLQs, to derive special connotations and group words into an atomic unit of information. TBL has successfully been applied to other areas of natural language processing; however, this paper presents the first time it has been applied to structured NLQs. Here, we investigate the applicability of TBL to NLQs and compare the TBL-based system, with our previous system and a system with a formal language interference. Our results show that TBL is effective for structured NLQs, and that structured NLQs a viable interface tor XML-IR systems",no8898,The implementation of a COTS based fault tolerant avionics bus architecture,"X2000 is a technology program at the Jet Propulsion Laboratory to develop enabling technologies for future flight missions at affordable cost. The cost constraints mandate the use of commercial-off-the-shelf (COTS) components and standards in the X2000 multi-mission avionics system architecture. The X2000 has selected two commercial bus standards, the IEEE 1394 and I<sup>2</sup>C, as the avionics system buses. These two buses work together to provide the performance, scalability, low power consumption, and fault tolerance required by long life deep space missions. We report our approach to implementing a fault-tolerant bus architecture for the X2000 avionics system using these two COTS buses. The system approach is described first. Then, the focus of the rest of the discussion is on the implementation of two ASICs, digital I/O and mixed signal I/O ASICs, which are the key components of this COTS based fault-tolerant bus architecture",no8897,Software fault tolerance of distributed programs using computation slicing,"Writing correct distributed programs is hard. In spite of extensive testing and debugging, software faults persist even in commercial grade software. Many distributed systems, especially those employed in safety-critical environments, should be able to operate properly even in the presence of software faults. Monitoring the execution of a distributed system, and, on detecting a fault, initiating the appropriate corrective action is an important way to tolerate such faults. This gives rise to the predicate detection problem which involves finding a consistent cut of a distributed computation, if it exists, that satisfies the given global predicate. Detecting a predicate in a computation is, however, an NP-complete problem. To ameliorate the associated combinatorial explosion problem, we introduce the notion of computation slice in our earlier papers [5, 10]. Intuitively, slice is a concise representation of those consistent cuts that satisfy a certain condition. To detect a predicate, rather than searching the state-space of the computation, it is much more efficient to search the state-space of the slice. In this paper we provide efficient algorithms to compute the slice for several classes of predicates. Our experimental results demonstrate that slicing can lead to an exponential improvement over existing techniques in terms of lime and space.",no8896,Joint write policy and fault-tolerance mechanism selection for caches in DSM technologies: Energy-reliability trade-off,"Write-through caches potentially have higher reliability than write-back caches. However, write-back caches are more energy efficient. This paper provides a comparison between the write-back and write-through policies based on the combination of reliability and energy consumption criteria. In the experiments, SIMPLESCALAR tool and CACTI model are used to evaluate the characteristics of the caches. The results show that a write-through cache with one parity bit per word is as reliable as a write-back cache with SEC-DED code per word. Furthermore, the results show that the energy saving of the write-through cache over the write-back cache increases if any of the following changes happens: i) a decrease in the feature size, ii) a decrease in the L2 cache size, and iii) an increase in the L1 cache size. The results also show that when feature size is bigger than 32 nm, the write-back cache is usually more energy efficient. However, for 32 nm and smaller feature sizes the write-through cache can be more energy efficient.",no8895,Error compensation of workpiece localization,"Workpiece localization has direct relations with many manufacturing automation applications. In order to gain accurate workpiece measurement by coordinate measuring machines (CMM) or on-machine measurement system, the touch trigger probe is widely adopted. In spite of the high repeatability of the touch trigger probe, there are still error sources associated with the probe. In this paper, we will focus on probe radius compensation. Several compensation methods in related papers are reviewed. In addition, a new radius compensation method is proposed in this paper. Simulation and experimental results of probe radius compensation by different methods are given. It is shown that our proposed method has the best performance both in terms of compensation accuracy and computational time. The method is also implemented in a computer aided setup (CAS) system.",no8894,Cesar-FD: An Effective Stateful Fault Detection Mechanism in Drug Discovery Grid,"Workflow management system is widely accepted and used in the wide area network environment, especially in the e-science application scenarios, to coordinate the operation of different functional components and to provide more powerful functions. The error-prone nature of the wide area network environment makes the fault-tolerance requirements of workflow management become more and more urgent. In this paper, we propose Cesar-FD, a stateful fault detection mechanism, which builds up states related to the runtime and external environments of workflow management system by aggregating multiple messages and provides more accurate notifications asynchronously. We demonstrate the use of this mechanism in the drug discovery grid environment by two use cases. We also show that it can be used to detect faulty situations more accurately.",no8893,Polish N-Grams and Their Correction Process,"Word n-gram statistics collected from over 1 300 000 000 words are presented. Eventhough they were collected from various good sources, they contain several types of errors. The paper focuses on the process of partly supervised correction of the n- grams. Types of errors are described as well as our software allowing efficient and fast corrections.",no8892,Efficient computation of confidence intervals forword error rates,"Word error rate is a standard measure of quality for different tasks such as speech recognition, OCR or machine translation. As such, it is important to compute it together with confidence intervals. Previous works in the literature employ Monte Carlo methods in order to compute those intervals. We show how to compute them without simulations. We also adapt a method that compares two systems over the same test data so that it can be used without simulations.",no8891,Defects detection based on principal component analyses and support vector machines,"Woods are used in many fields. The appearance of woods is important for the quality of wood products. In this paper, we present an image series fusion method based principal component analyses and recognize the defects by support vector machines. We select the histogram of the feature image as feature vector, and send it to support vector machines for recognition and classification. The results show that this method can fuse the image series and detect the defects.",no8890,Applying multifractal spectrum combined with fractal discrete brownian motion model to wood defects recognition for sawing,"Wood nondestructive testing technology is a new and comprehensive subject. In recent years it has achieved fast development. X-ray computed tomography (CT) scanning technology has been applied to the detection of internal defects in the logs for the purpose of obtaining prior information, which can be used to arrive at better wood sawing decision. Fractal geometry and its multifractal extension are new tools which can be used for describing, modeling, analyzing and processing different complex shapes and images. A method in CT image edge detection by using multifractal theory combined with fractal Brownian motion is applied in the paper. First its multifractal spectrum is estimated. Then different types of pixels are classified by the spectrum, smoothing edge point and singular edge point.",no8889,Recognizing the Patterns of Wood Inner Defects Based on Wavelet Neural Networks,"Wood nondestructive detection technology is a new interdisciplinary technology, which has been successfully applied in wood production, wood processing, wood structure detection and many other fields. In the paper, ultrasonic nondestructive testing for wood defects is studied based on the energy spectrum variety of the ultrasonic signals by means of wavelet transform, coefficient of wavelet node and the artificial neural networks. The original signals of different elm specimen are dispelled by wavelet packet, and the signal energy variety of crunodes in the 5<sup>th</sup> layer wavelet bundle of both defect specimen and normal specimen without any defect is obtained. The experiment results show that the energy change of defect wood specimen mostly depends on the degree of wood defects. And the defect degree is proportional to the energy change. By comparing the energy variety of every signal crunode in the 5<sup>th</sup> layer wavelet bundle, it is explicit that the variety of the crunode (5,0) among 32 crunodes is the biggest. And the crunode contains defect character information mostly. The energy varieties of 32 crunodes in the 5<sup>th</sup> layer and wavelet radix of (5,0) crunode are respectively regarded as the character inputs of the artificial neural networks (ANN). Two ANN networks are analyzed according to the ability of identifying wood defect patterns through training network. The identifying results show that taking wavelet radix of (5,0) crunode as the character input is more effictive in recognizing the defect patterns of wood inner defects.",no8888,Two dimensional image reconstruction of log cross-section defect based on stress wave technique,"Wood is a material that is produced biologically in the growing tree, making it vulnerable to the attack of fungi. This will reduce the quality of wood, especially for log. The 2D image reconstruction contributed greatly for log cross-section defect testing in order to promote the utilization rate of wood resources. At first, this paper studied the stress wave computerized tomography technique, and introduced the straight-line tracing technique and the Algebraic Reconstruction Technique (ART) algorithm. Then, the medium model was constructed for numerical simulation analysis. The reconstruction of the medium model was conducted using straight line tracing - ART algorithm, and the impact of the number of iterations for image reconstruction accuracy was analyzed. At last, this paper validated the feasibility for two-dimensional image reconstruction of log internal defects using this method by physical model testing. Empirical and medium model results showed that the convergence of straight line tracing - ART algorithm was fast and reconstruction image was good. The two-dimensional image reconstruction of log internal defects could basically be realized using the straight line tracing-algebraic reconstruction algorithm method. And the feasibility and practicality of theory and technique that proposed in this paper were validated by practical testing.",no8887,On The Generalization of Error-Correcting WOM Codes,"WOM (write once memory) codes are codes for efficiently storing and updating data in a memory whose state transition is irreversible. Storage media that can be classified as WOM includes flash memories, optical disks and punch cards. Error-correcting WOM codes can correct errors besides its regular data updating capability. They are increasingly important for electronic memories using MLCs (multi-level cells), where the stored data are prone to errors. In this paper, we study error-correcting WOM codes that generalize the classic models. In particular, we study codes for jointly storing and updating multiple variables - instead of one variable - in WOMs with multi-level cells. The error-correcting codes we study here are also a natural extension of the recently proposed floating codes. We analyze the performance of the generalized error- correcting WOM codes and present several bounds. The number of valid states for a code is an important measure of its complexity. We present three optimal codes for storing two binary variables in n q-ary cells, where n = 1,2,3, respectively. We prove that among all the codes with the minimum number of valid states, the three codes maximize the total number of times the variables can be updated.",no8886,A New Fault Ride-through Strategy for Doubly Fed Wind-Power Induction Generator,"Withstanding grid faults becomes an obligation for the bulk wind generation units connected to the transmission network and it is highly desired for distribution wind generators. In this paper, a proposed scheme is implemented for DFIG to keep it operating during transient grid faults. Challenges imposed on the generator configuration and the control during the fault and recovering periods are presented. A comprehensive time domain model for the DFIG with the decoupled dq controller is implemented using Matlab/Simulink software. Intensive simulation results are discussed to ensure the validity and feasibility of the proposed fault ride through technique. The scheme protects the DFIG components, fulfills the grid code requirements, and optimizes the hardware added to the generator.",no8885,Distributed event processing for fault management of Web Services,"Within service orientation (SO) web services (WS) are the defacto standard for implementing service-oriented systems. While consumers of WS want to get uninterrupted and reliable service from the service providers WS providers can not always provide services in the expected level due to faults and failures in the system. As a result the fault management of these systems is becoming crucial. This work presents a distributed event-driven architecture for fault management of Web Services. According to the architecture managed WS report different events to the event databases. From event databases these events are sent to event processors. The event processors are distributed over the network. They process the events, detect fault scenarios in the event stream and manage faults in the WS.",no8884,Efficient techniques for reducing error latency in on-line periodic BIST,"With transient and intermittent operational faults becoming a dominant failure mode in modern digital systems, the deployment of on-line test technology is becoming a major design objective. On-line periodic BIST is a testing method for the detection of operational faults in digital systems. The method applies a near-minimal deterministic test sequence periodically to the circuit under test and checks the circuit responses to detect the existence of operational faults. On-line periodic BIST is characterized by full error coverage, bounded error latency, moderate space and time redundancy. In this paper, we present various techniques to minimize the error latency without sacrificing the full error coverage. These techniques are primarily based on the reordering the test vectors or the selective repetition of test vectors. Our analytical and preliminary experimental results demonstrate that our techniques lead to a significant reduction in the error latency.",no8883,Efficient fault simulation techniques and test configuration generation for embedded FPGAs,"With today's system-on-a-chip (SOC) technology, BIST-based techniques are the best solution for the testing of embedded FPGAs with low controllability and observability. In the past, test configurations are usually derived manually and there still lacks of an efficient fault simulator to evaluate the resulted fault coverage. Based on the BIST-based structure, an efficient fault simulator (FPGAsim) for FPGAs is proposed to alleviate it. The fault models can be updated by using a script file as well as the FPGA dimensions. Therefore, the flexibility of FPGAsim is very high. According to the regular structure during the BIST sessions, the simulation complexity can be reduced from O(N<sup>2 </sup>) to O(1). That is, the simulation complexity is independent of the size of an FPGA. The fault simulator proposed above is also helpful for solving the following problems. 1) Given a set of target fault models, generate the required test configurations with 100% fault coverage. 2) Given a set of target fault models and a test length constraint, generate the test configurations with the highest fault coverage. 3) Set the priority of test configurations such that test length/test time can be reduced. In other words, FPGAsim can be used for generation of optimal test configurations",no8882,A Sun Tracking Error Monitor for Photovoltaic Concentrators,"With today's PV markets bogged down by the shortage of solar grade silicon a handful of start-ups but also well established manufacturers, try to take advantage of the situation and steadily stride towards the commercialisation of photovoltaic concentration technologies. To aid the completion of their ongoing development cycles, and implement production automation and quality control processes, specific instrumentation and machinery is to be developed. Assessment of sun tracking accuracy should not be overlooked, and even more by those players raising very high concentration concepts over the 100X frontier. Some analyses point out that the acceptance angle of present designs in concentration optics may be overestimated even from a theoretical point of view, which added to the still uncertain acceptance angle losses inflicted on the overall system by mass assembly processes, may finally shrink the allowable tolerance and divert the entire burden to the tracking accuracy. Instrumentation for the monitoring of sun tracking operative performance, providing enough sensitivity to gauge the sub-degree accuracy ranges required by high concentration systems is therefore needed, and technical feasibility of this proposal is proven here based in state-of-the-art solid state image sensors",no8881,Research on low voltage ride through technology for symmetrical grid fault,"With the wind power installation capacity sharply increasing, the effects caused by wind farms on the regional power system stability becomes very remarkable. Accordingly, more and more researches are focused on the low voltage ride through (LVRT) technology used for DFIG under short-time grid fault, and new power grid regulation requires power system to achieve LVRT capability. But we have no interrelated criterion in our country. First, the requirement and applying of LVRT is introduced, and the electromagnetism relation of DFIG stability factor under symmetrical grid fault is analyzed. Then based on the professional power system simulate software platform PSCAD/EMTDC, a LVRT simulation experiment which applied crowbar protection method when grid voltage falls down is done. Recur to this experiment, DFIG performance characteristic is analyzed and some helpful conclusions are achieved. The emulation experiment validate the availability of proposed electromagnetism relation, remarkable effect of improved crowbar, exactness of simulant model.",no8880,Design of Fault Diagnosis System Based on B/S Structure,"With the wide application of modern electrical technique to weapon systems, weapons become more and more complicated, integrated, high-speed and intellectualized. To insure weapons in their good conditions, the function of fault diagnosis gets more important than before in the process of repairing. Now, it cannot give fault diagnosis quickly and correctly only by conventional means. So developing proper system is in need. This paper designed and developed a network fault diagnosis system based on B/S (Browser/Server) frame through the analysis of the need of fault diagnosis. Taking good use of testing information and diagnosis rules, the system realized open and distributed diagnosing process, and provided a flat of sharing information, which is the technique basis of combining testing and diagnosing. During design, we use Macromedia Dreamweaver MX 2004 as developing software and Java Scripts and VB Scripts in ASP language as scripts to program. It is proved in LAN that the system can utilize present diagnosis rules in database to diagnose fault distributed. Besides, the system is stable with expansibility and security.",no8879,Adaptive Motion Vector Retrieval Schemes for H.264 Error Concealment,"With the ubiquitous application of Internet and wireless networks, H.264 video communication becomes more and more popular. However, due to the high-efficiently predictive coding and the variable length entropy coding, it is more sensitive to transmission errors. Error concealment (EC) is just an approach to utilize the spatial and temporal correlations to conceal the corrupted region. In this paper, first we propose variable block size error concealment (VBSEC) scheme inspired by variable block size motion estimation (VBSME) in H.264. This scheme provides four EC modes and four sub-block partitions. The whole corrupted macro-block (MB) will be divided into variable block size adoptively according to the actual motion. More precise motion vectors (MV) will be predicted for each sub-block Then MV refinement (MVR) scheme is proposed to refine the MV of the heterogeneous sub-block by utilizing three step search (TSS) algorithm adaptively. Both VBSEC and MVR are based on our improved spatio-temporal boundary matching algorithm (STBMA). By utilizing these schemes, we can reconstruct the corrupted MB in the inter frame more accurately. The experimental results show that our proposed scheme can obtain maximum PSNR gain up to 1.82dB and 1.52dB, respectively compared with the boundary matching algorithm (BMA) adopted in the JM11.0 reference software and STBMA.",no8878,"Yield Improvement, Fault-Tolerance to the Rescue?","With the technology entering the nano dimension, manufacturing processes are less and less reliable, thus drastically impacting the yield. A possible solution to alleviate this problem in the future could consist in using fault tolerant architectures to tolerate manufacturing defects. In this paper, we analyze the conditions that make the use of a classical triple modular redundancy (TMR) architecture interesting for a yield improvement purpose.",no8877,"Prediction, Detection, and Correction of Faraday Rotation in Full-Polarimetric L-Band SAR Data","With the synthetic aperture radar (SAR) sensor PALSAR onboard the Advanced Land Observing Satellite, a new full-polarimetric spaceborne L-band SAR instrument has been launched into orbit. At L-band, Faraday rotation (FR) can reach significant values, degrading the quality of the received SAR data. One-way rotations exceeding 25 <sup>deg</sup> are likely to happen during the lifetime of PALSAR, which will significantly reduce the accuracy of geophysical parameter recovery if uncorrected. Therefore, the estimation and correction of FR effects is a prerequisite for data quality and continuity. In this paper, methods for estimating FR are presented and analyzed. The first unambiguous detection of FR in SAR data is presented. A set of real data examples indicates the quality and sensitivity of FR estimation from PALSAR data, allowing the measurement of FR with high precision in areas where such measurements were previously inaccessible. In examples, we present the detection of kilometer-scale ionospheric disturbances, a spatial scale that is not detectable by ground-based GPS measurements. An FR prediction method is presented and validated. Approaches to correct for the estimated FR effects are applied, and their effectiveness is tested on real data.",no8876,Code construction and FPGA implementation of a low-error-floor multi-rate low-density Parity-check code decoder,"With the superior error correction capability, low-density parity-check (LDPC) codes have initiated wide scale interests in satellite communication, wireless communication, and storage fields. In the past, various structures of single code-rate LDPC decoders have been reported. However, to cover a wide range of service requirements and diverse interference conditions in wireless applications, LDPC decoders that can operate at both high and low code rates are desirable. In this paper, a 9-k code length multi-rate LDPC decoder architecture is presented and implemented on a Xilinx field-programmable gate array device. Using pin selection, three operating modes, namely, the irregular 1/2 code mode, the regular 5/8 code mode, and the regular 7/8 code mode, are supported. Furthermore, to suppress the error floor level, a characterization on the conditions for short cycles in a LDPC code matrix expanded from a small base matrix is presented, and a cycle elimination algorithm is developed to detect and break such short cycles. The effectiveness of the cycle elimination algorithm has been verified by both simulation and hardware measurements, which show that the error floor is suppressed to a much lower level without incurring any performance penalty. The implemented decoder is tested in an experimental LDPC orthogonal frequency division multiplexing system and achieves the superior measured performance of block error rate below 10<sup>-7</sup> at signal-to-noise ratio of 1.8 dB.",no8875,A method of inverter circuit fault diagnosis based on BP neural network and D-S evidence theory,"With the study and analysis on intelligent fault diagnosis for inverting circuit, an improved diagnosis method combined BP neuron network and D-S evidence theory was proposed. Each measuring point was extracted by BP neural network to obtain the local diagnosis, which is adopted to design the belief function of D-S evidence theory. Multiple monitoring points' information is fused to receive the comprehensive global diagnosis result. The experimental results show that this method has the better feasibility and effectiveness on fault diagnosis in inverter's key components-inverting circuit.",no8874,Solving Reliable Coverage In Fault Tolerant Energy Efficient Wireless Sensor Network,"With the strong push from applications, fault tolerant and energy efficient property of wireless sensor network has gradually become a hot point. Based on the heuristic method used to generate minimum covers, a novel scheme namely reliable coverage scheme is proposed to solve the reliable coverage problem within the cluster in a hierarchical structured network. Simulations show that this scheme is able to efficiently utilize energy to prolong network lifetime while at the same time keeping missed monitoring at a low level",no8873,Clinic: A Service Oriented Approach for Fault Tolerance in Wireless Sensor Networks,"With the size and complexity of modern Wireless Sensor Networks (WSNs) systems, a system's ability to recover from faults is becoming more important. A self-healing system is one that has the capability to recover from faults without human intervention during execution. Since WSNs are inherently fault-prone and since their on-site maintenance is infeasible, scalable self-healing is crucial for enabling the deployment of large-scale sensor network applications. Previous work has typically dealt with single faults in isolation, has imposed constraints on systems, or required new protocol elements. In this paper, we attempt to solve some of these problems through the use of service-oriented architecture. We propose a service-oriented self-healing approach, called Clinic, that works with existing network components, e.g. routing protocols, and resources without adding extra overhead on the network. In Clinic, different network capabilities are viewed as services of the network instead of being isolated capabilities of individual nodes. This view of the network promotes collaboration among nodes and information reuse by sharing information collected by one service with other network services. Preliminary evaluation showed that Clinic achieved fault tolerance while keeping low communication overhead by reusing only the information collected by other network services to heal from faults.",no8872,Built in Defect Prognosis for Embedded Memories,"With the shrinking technology and increasing statistical defects, multiple design respins are required based on yield learning. Hence, a solution is required to efficiently diagnose the failure types of memory during production in the shortest time frame possible. This paper introduces a novel method of fault classification through image based prognosis of predefined fail signature dictionary. In contrary to the existing bitmap diagnosis methodologies, this method predicts the compressed failure map without generating and transferring complete bitmap to the tester. The proposed methodology supports testing through a very low cost ATE. This architecture is partitioned to achieve sharing among various memories and at-speed testing.",no8871,Low Area Adaptive Fail-Data Compression Methodology for Defect Classification and Production Phase Prognosis,"With the shrinking technology and increasing statistical defects, multiple design respins are required based on yield learning. Hence, a solution is required to efficiently diagnose the failure types of memory during production in the shortest time frame possible. This paper introduces a novel method of fault classification through image based prognosis of predefined fail signature dictionary. In contrary to the existing Bitmap Diagnosis methodologies, this method predicts the compressed failure map without generating and transferring complete Bitmap to the tester. The proposed methodology supports testing through a very low cost ATE. This architecture is partitioned to achieve sharing among various memories and at-speed testing.",no8870,Use of DGPS corrections with low power GPS receivers in a post SA environment,"With the removal of the dithering effects of Selective Availability (SA), use of Differential GPS (DGPS) corrections can now be applied for extended periods of time allowing enhanced performance for low power configurations of a Si RF based GPS receiver. The software selectable low power settings, implemented by Si RF, employ three states; track, navigate and trickle. During track and trickle states there is no UART communication making reception of DGPS correction unavailable. During the NAV state (when the navigation calculation is performed), corrections may be received. Previously, SA induced error shortened the viable extrapolation time to less than 30 seconds; else significant navigation error would build up between measurements. Additionally, the need to return to a full power state every 30 seconds significantly increased the overall average power dissipation over standard TricklePower<sup>TM</sup> operation. Now that SA (the dominate error source of the DGPS correction) has been removed, the time limit that a DGPS correction can be applied has been extended from 30 seconds to several minutes without significant degradation in navigation performance. This opens up opportunity for low power GPS receiver operation to make use of the DGPS correction to improve navigation without severely impacting the average power requirements. Si RF's implementations of low power operation, leverages off its unique architecture that allows 100 ms signal reacquisition allowing a pseudorange measurement to as little 200 ms. The chipset is then shut down for 800 ms, significantly reducing the power consumption, while still maintaining 1 Hz navigation updates",no8869,Method to minimize dose for CT attenuation correction in SPECT,"With the recent introduction of hybrid SPECT/CT systems, with diagnostic multi-slice CT (Computed Tomography) capabilities the CT dose delivered to the patient may become an issue, as attenuation compensation in SPECT using co-registered CT data becomes more common in clinical practice. In general, any CT data can be converted to a volume of linear attenuation coefficients (LAC), also referred to as the ""attenuation map"", or mu-map. We investigated the range of CT settings that minimize the dose to the patient and allow for CT image quality that is appropriate for attenuation correction. SPECT/CT registration issues are outside the scope of this work, and we assume that the objects are perfectly registered. Various phantoms are used to analyze both CT and attenuation-corrected SPECT image quality, where the CT data itself is used to generate the mu-maps. Conclusion: The effective dose to the patient from a CT scan for the purpose of SPECT AC can be lowered to at least 30% to 50% without affecting the image quality of the SPECT AC, using CT derived mu-maps, if an appropriate scan protocol with a very smooth (MTF at 50%: 1.0 Ip/cm) CT reconstruction kernel is used. A further clinical investigation is needed to confirm these findings in clinical practice",no8868,Similarity-Based Bayesian Learning from Semi-structured Log Files for Fault Diagnosis of Web Services,"With the rapid development of XML language which has good flexibility and interoperability, more and more log files of software running information are represented in XML format, especially for Web services. Fault diagnosis by analyzing semi-structured and XML like log files is becoming an important issue in this area. For most related learning methods, there is a basic assumption that training data should be in identical structure, which does not hold in many situations in practice. In order to learn from training data in different structures, we propose a similarity-based Bayesian learning approach for fault diagnosis in this paper. Our method is to first estimate similarity degrees of structural elements from different log files. Then the basic structure of combined Bayesian network (CBN) is constructed, and the similarity-based learning algorithm is used to compute probabilities in CBN. Finally, test log data can be classified into possible fault categories based on the generated CBN. Experimental results show our approach outperforms other learning approaches on those training datasets which have different structures.",no8867,An error robust macro-block mode decision for H.26L stream,"With the rapid development of the Internet, more and more attention is focused on IP video streaming. We introduce an RD optimal macro-block mode decision scheme for the new H.26L video stream. Based on the statistical error propagation model of the Internet and the unequal NAL packet of H.26L, our new scheme can be more error robust than those currently adopted in the H.26L test mode.",no8866,A new method of fault tolerance TCP,"With the rapid development of Internet, the need of high availability of data services on Internet becomes more urgent. But as one of the most useful protocol on Internet, TCP protocol software can not solve the high availability due to the failure of hardware or software on server/client. In this paper, we propose a new method that can implement fault tolerance TCP to improve the high availability of data transmission. First, we analyze some existing methods of fault tolerant TCP; then based on the characteristic of present server architecture, we put forward our new method of fault tolerant TCP; and lastly, we describe in detail how to implement and test our method. Experimental results show that our fault-tolerant TCP can offer high available and high effective communication support for reliable data service on Internet.",no8865,Research of the Middleware Based Fault Tolerance for the Complex Distributed Simulation Applications,"With the rapid development of computer simulation technology, the Radar simulation applications scale up increasingly. More and more Radar simulation applications adopt distributed structure to improve system performance and availability. Hence, how to enhance the robustness and efficiency of these complex distributed simulation systems is a hot point. At the same time, fault tolerance middleware makes the applications more robust, available and reliable. Therefore, we strengthen the functionalities of existing fault tolerant middleware and integrate our middleware with the complex distributed simulation systems to provide efficient fault tolerance with balanced workload allocation among different replicas for the distributed simulation applications.",no8864,Study on Integration Diagnosis System for Automobile Faults and Its Key Technologies,"With the rapid development of automobile electronics level, the mechatronics of automobile product becomes more and more obviously. When we try our best to improve function of automobile by electronic controlled system, the difficulty of failure diagnosis is increasing too. In order to improve the maintenance quality and efficiency, automobile manufacturer and maintenance server have established the integration diagnosis system on the basis of on-board diagnosis and off-board diagnosis. There are two types of automobile fault diagnosis integration system, primary integration system which composed of detection technology and expert system; higher integration which composed of detection technology, expert system and network communication. These two systems may be composed of the parameters measuring module, data fusion module, and fault diagnosis module, information obtaining module and network communication module. Based on the analysis of module functions, the reason and basis to some key technologies such as measuring apparatus or equipment communication standards, the available of fault diagnosis expert system, the multifunction inferring methods and fault diagnosis knowledge obtaining by network are presented in this paper.",no8863,Fuzzy Logic Thermal Error Compensation for Computer Numerical Control Noncircular Turnning System,"With the new emerging technologies of high performance machining and the increasing demand for improved machining accuracy in recent years, the problem of thermal deformation of machine tool structures is becoming more critical than ever. The computer numerical control (CNC) turning system for noncircular section pistons is designed with giant magnetostrictive actuator (GMA) as the turning module. When the temperature of the cooler system of the GMA varies for about 6degC, the dimension error of the surface contour varies for about 20 micron, and cannot meet the precision requirement of the piston's contour dimension. In this paper, a method using fuzzy logic control for the compensation of the thermally induced error is developed. The fuzzy rule is used to compensate directly for the nonlinearity and uncertainty of the cooler system. The rule development strategy for the compensation system is to change the feed quantity of GMA to control the tool the pistons dimension. The fuzzy logic control developed here is a two-input single-output controller. The two inputs are the temperature deviation from setpoint error, and error rate. The output is the compensated value of the feed system. The triangular membership functions are used to define the input linguistic variables and the output linguistic variables. The fuzzy logic approach incorporates many advantages of using fuzzy logic such as the incorporation of heuristic knowledge, ease of implementation and the lack of a need for an accurate mathematical model. The experimental results are presented and the effectiveness of the fuzzy thermal error compensation control technique is discussed. Accuracy is greatly improved using the developed error compensation system",no8862,Applying object-orientation and IEC 61850 standard to architecture design of power system fault information processing,"With the maturing of the IEC 61850, utilities are beginning to implement substation automation systems (SAS) that are based on this new international standard. This paper describes such an implementing research for power fault information processing system on East China electric power group in China. In particular, it presents the idea of applying object-oriented methodology to architecture design and providing an open interface of IEC 61850. Based on the idea and technique, some benefits are brought.",no8861,Implementing research for IEC 61850-based fault analysis system,"With the maturing of the IEC 61850, utilities are beginning to implement substation automation systems (SAS) that are based on this new international standard. This paper describes such an implementing research for power fault analysis system on east china electric power group in China. In particular, it presents the idea of applying object-oriented methodology to architecture design and providing an open interface of IEC 61850 in the substation layer. Based on the idea and technique, some benefits are brought.",no8860,Design and Implementation of Failover Federates Supporting Fault Tolerance for HLA Based Simulations,"With the increasing scale and complexity of HLA based simulations, fault tolerance is gradually becoming a pressing problem. This paper addresses the challenges in realizing a failover federate to support fault tolerance for HLA based simulations. Based on the analysis of the fault tolerance problem, the failover federate is described firstly. It comprises a primary federate and a standby federate. Next, the failover federate is designed through exploiting the fault detection problem and fault tolerance dispatching method. Additionally, the implementation detail is explained. Through some testing, it proves that the reliability of the whole simulation systems can be promoted through the introduction of the standby federates.",no8859,A Weighted Finite-State Framework for Correcting Errors in Natural Scene OCR,"With the increasing market of cheap cameras, natural scene text has to be handled in an efficient way. Some works deal with text detection in the image while more recent ones point out the challenge of text extraction and recognition. We propose here an OCR correction system to handle traditional issues of recognizer errors but also the ones due to natural scene images, i.e. cut characters, artistic display, incomplete sentences (present in advertisements) and out- of-vocabulary (OOV) words such as acronyms and so on. The main algorithm bases on finite-state machines (FSMs) to deal with learned OCR confusions, capital/accented letters and lexicon look-up. Moreover, as OCR is not considered as a black box, several outputs are taken into account to intermingle recognition and correction steps. Based on a public database of natural scene words, detailed results are also presented along with future works.",no8858,Incremental fault-tolerant design in an object-oriented setting,"With the increasing emphasis on dependability in complex, distributed systems, it is essential that system development can be done gradually and at different levels of detail. We propose an incremental treatment of faults as a refinement process on object-oriented system specifications. An intolerant system specification is a natural abstraction from which a fault-tolerant system can evolve. With each refinement step a fault and its treatment are introduced, so the fault-tolerance of the system increases during the design process. Different kinds of faults are identified and captured by separate refinement relations according to how the tolerant system relates to abstract properties of the intolerant one in terms of safety, and liveness. The specification language utilized is object-oriented and based upon first-order predicates on communication traces. Fault-tolerance refinement relations are formalized within this framework",no8857,An advanced methodology for the reporting of original adders for process tool defect monitoring,"With the increasing cost of bare silicon wafers, many IC manufacturers are choosing to run fewer and fewer process tool qualifications. This decision places an increasingly larger number of production wafers at risk from process tool excursions as well as an increasingly high demand on the dependability and consistency of the qualification data. In this paper, we describe our methodology of determining only original adders in process tool qualification. The importance of reporting only added defects to the process tool control chart was justified by showing a correlation between inline yield-enhancement charts and un-patterned tool qualifications. In the past, it has been very hard to correlate un-patterned charts to inline yield-limiting charts and also to final yield",no8856,Effective Post-BIST Fault Diagnosis for Multiple Faults,"With the increasing complexity of LSI, built-in self test (BIST) is one of the promising techniques in the production test. From our observation during the manufacturing test, multiple stuck-at faults often exist in the failed chips during the yield ramp-up. Therefore the authors propose a method for diagnosing multiple stuck-at faults based on the compressed responses from BIST. The fault diagnosis based on the compressed responses from BIST was called the post-BIST fault diagnosis (Takahashi et al., 2005, Takamatsu, 2005). The efficiency on the success ratio and the feasibility of diagnosing large circuits are discussed. From the experimental results for ISCAS and STARC03 (Sato et al., 2005) benchmark circuits, it is clear that high success ratios that are about 98% are obtained by the proposed diagnosis method. From the experimental result for the large circuits with 100K gates, the feasibility of diagnosing the large circuits within the practical CPU times can be confirmed. The feasibility of diagnosing multiple stuck-at faults on the post-BIST fault diagnosis was proven",no8855,Testing of hard faults in simultaneous multi-threaded processors,"With the increasing circuit complexity and aggressive technology scaling, faults such as dielectric, conductor, and metallization failures are becoming more common. Traditional stuck-at testing does not detect these types of faults because these faults may be dormant and need to be stressed to manifest as ""fails"" (during burn-in). As voltage scaling and power consumption reduce the effectiveness of burn-in, these faults will cause failures during the useful life of the part. We propose the use of a test thread on simultaneous multi-threaded processors, to provide a means of detection of lifetime failures. In this initial study, the test thread is allowed to compete with executing programs for processor resources. When the system has only one workload thread, the test thread is able to execute with no significant impact on the workload thread. When the system has many active workload threads, the test thread has somewhat larger impact on the execution time of the workload threads.",no8854,Nova: A Robustness-oriented Byzantine Fault Tolerance Protocol,"With the increased complexity, malicious faults have become an important reasons that affect the reliability of the distributed system, especially the web-scale infrastructures, i.e. Amazon S3, Google AppEngine etc. Most such systems assume benign fault model which can't depict the malicious actions. The goal of Byzantine Fault Tolerance protocol (BFT for short) is to mask the malicious behaviors and it has been proved that some new proposed BFTs are suitable to support practical applications. But these BFTs still lack in robustness, a simple fault injection may cause significantly decrease in throughput or run in low throughput without violating the BFT safety property. We propose a new robustness-oriented BFT named Nova. Experiments show Nova has comparable throughput as PBFT in normal case and behave stably under the malicious attack. Compared with other BFTs, Nova can support practical applications more effectively.",no8853,Defect Tracing System Based on Orthogonal Defect Classification,"With the increase of the software complexity, the defect measurement becomes a task of high priority. We give a new software defect analytical methodology based on orthogonal classification. This method has two folds. Then a set of orthogonal defect classification (ODC) reference model is given which includes activity, trigger, severity, origin, content and type of defect. In the end, it gives a support tool and concrete workflow of defect tracing. In contrast with the traditional method, this method not only has the advantages of popularity, haleness and low cost, but also improves the accuracy of identifying defects notably. Thus it offers a strong support for the prevention of defects in software products.",no8852,Motion correction of head movements in PET: realisation for routine usage,With the increase of scanner resolution head motion in PET brain studies becomes an increasingly serious limitation. Methods to correct for motion have been proposed. In this work the realisation of a motion tracking system in a PET environment and the motion correction of list mode data with the MAF method is presented. In a phantom study the method is validated and the loss in image quality is documented in a phantom with simulated movements. The relevance of motion correction for patient data above the level of system resolution is studied. In a real patient study we show the effect of motion and the applicability of the presented system.,no8851,FPGA based design of a novel enhanced error detection and correction technique,"With the increase of data transmission and hence sources of noise and interference, engineers have been struggling with the demand for more efficient and reliable techniques for detecting and correcting errors in received data. Although several techniques and approaches have been proposed and applied in the last decade, data reliability in transmission is still a problem. In this paper we propose a high efficient combined error detection and correction technique based on the Orthogonal Codes Convolution, Closest Match, and vertical parity. This method has been experimentally implemented and simulated using Field Programmable Gate Array (FPGA). Simulation results show that the proposed technique detects 99.99% of the errors and corrects as predicted up to (n/2-1) bits of errors in the received impaired n-bit code.",no8850,Hardware Controlled and Software Independent Fault Tolerant FPGA Architecture,"With the increase in complexity of fabrication techniques, yield of the chip production decreases at deep sub-micron technologies. In future fault tolerant techniques will be important to increase the yield of the VLSI chips in advanced fabrication technologies. In regular structure like FPGA, redundancy is commonly used for fault tolerance. Most of the techniques found so far in literature talk about software based changes in the configuration data. In this work we present a solution in which configuration bit stream of FPGA is modified by a hardware controller that is present on the chip itself. The technique uses redundant columns for replacing faulty cells. Experiments on different circuits using VPR tool shows that there is an average 2.6% increase in the critical path delay while no increase in the area per cell due to our approach.",no8849,Fault Diagnosis Method Study on Automobile Electrical Controlled System Based on Fusing of ANN and D-S Evidence Theory,"With the improvement of automobile electric degree, more and more people begin to pay attention to the fault diagnosis method and theories of electric controlled system. The precision and accuracy of on-board diagnosis methods, which with OBDII standard and has been widely used at present need to be further improvement. So, in this paper, take the engine idling instability as the example, put forward a multi-sensor diagnosis method which fusing neural network and D-S evidence theory, this method mainly use for on-board diagnosis system datapsilas fusing process and analysis. The experimental result shows that, this method can make use of various faultspsila redundant and complementation information sufficiently, and then promote the recognition ability obviously. With electric controlled technology widely used in automobile, the performance of automobile products has been promoted largely, but these also make fault diagnosis become more difficult, traditional methods such as experience or simple instrument could not meet the flexible diagnosis demand. At present, the On-Board diagnosis with OBDII standard has been applied for electric controlled systempsilas fault diagnosis, but it could only for 70%-80%psilas fault, and the diagnosis results are mainly presented by fault code or data flow, and still need otherpsilas help, and the accuracy degree still needs further improvement. Therefore, looking for the more precious and intelligent method for electric controlled system become the key direction in automobile fault diagnosis field.",no8848,Numerical analysis and defects of forging technology for synchronizing steel ring of automobile,"With the high development of the motor industry, there is a great demand for precise and complex auto parts. New forging processes, material and energy saving processes are very important to the production of new type of auto parts. But forging mechanism is very complicated,and numerical simulation is the most effective method for analyzing the plastic deformation process. In this paper, several key techniques of synchronizer ring is simulated by Deform-3D, the effect of workpiece diameter, die structure and friction factor on forging quality and the flowing and strain stress distributing disciplinarian of metal material are particular analyzed. The defects occurred in the forging process is forecasted, eliminated forming disciplinarian and optimized process parameters and die structure through numerical simulation. The theory of precision forging, technology of FEM simulation and die CAD are integrated through this research, which helps to shorten developing period, prolong life-span of die, and cut down the cost, etc. The progeny of the research can provide accordance for other synchronizing ring production and die design and it also can be used for the leading production directly.",no8847,Improved Evaluation Algorithm for Performance Prediction with Error Analysis,"With the help of a proper performance model and evaluation algorithm, the performance metrics of information systems can be determined at the early stages of the development process. In our work, the response time and the throughput performance metrics of multi-tier Web applications have been predicted based on a queueing model. The goal of our work is to extend the mean-value analysis (MVA) algorithm according to the investigation of the thread pool. Web applications have been tested with concurrent user sessions in order to validate the proposed algorithm in different versions of ASP.NET environment. Moreover, error analysis has been performed to demonstrate the accuracy of the proposed algorithm.",no8846,A self-optimization of the fault management strategy for device software,"With the growth of network technologies, abundance of network resources, and increase of various services, mobile devices have gained much functionality and intelligence. At the same time, mobile devices are becoming complicated and many software related problems appear. The traditional remote repair method needs the software providers to supply fault information with corresponding repair strategy. It is inconvenient for users when the sold mobile devices have software faults. However, it is impossible for the manufacturers to supply all the fault information and repair-strategy before selling them. So far, no method has been given to collect repair-strategy from the sold mobile device and optimize the self-repair strategy. In this paper, we propose a self-optimization method to learn the software repair strategy from the sold mobile devices and to optimize self-repair strategy based on the Open Mobile Alliance (OMA) Device Management (DM) standard. The managed objects (MOs) are defined for collecting the strategy data and the self-optimization algorithm is proposed and implemented at the central server.",no8845,Error spreading: a perception-driven approach to handling error in continuous media streaming,"With the growing popularity of the Internet, there is increasing interest in using it for audio and video transmission. Perceptual studies of audio and video viewing have shown that viewers find bursty losses, mostly caused by congestion, to be the most annoying disturbance, and hence these are critical issues to be addressed for continuous media streaming applications. Classical error handling techniques have mostly been geared toward ensuring that the transmission is correct, with no attention to timeliness. For isochronous traffic like audio and video, timeliness is a key criterion, and given the high degree of content redundancy, some loss of content is quite acceptable. We introduce the concept of error spreading, which is a transformation technique that permutes the input sequence of packets (from a continuous stream of data) before transmission. The packets are unscrambled at the receiving end. The transformation is designed to ensure that bursty losses in the transformed domain get spread all over the sequence in the original domain, thus improving the perceptual quality of the stream. Our error spreading idea deals with both cases where the stream has or does not have inter-frame dependencies. We next describe a continuous media transmission protocol and experimentally validate its performance based on this idea. We also show that our protocol can be used complementary to other error handling protocols",no8844,Guided Reasoning of Complex E-Business Process with Business Bug Patterns,"With the growing complexity of e-business applications and the urgent need for ensuring its reliability, much effort has been made to advocate the application of model checking in probing hidden flaws in these applications. This work devotes itself to the performance enhancement in reasoning e-business processes with model checking. Our major contribution lies in: (1) a set of business bug patterns are extracted from workflow patterns to exploit existing business knowledge in probing undesired violations in e-business processes; (2) the semantics of business bug patterns are formally captured with the IEEE standard of PSL; (3) guided verification algorithms are development based on the above findings to accelerate the reasoning of complex e-business applications. Their efficiencies are testified with three concrete business cases in banking and manufacturing domains with our business process verification toolkit of OPAL",no8843,Verification and Validation of (Real Time) COTS Products using Fault Injection Techniques,"With the goal of reducing time to market and project costs, the current trend of real time business and mission critical systems is evolving from the development of custom made applications to the use of commercial off the shelf (COTS) products. Obviously, the same confidence and quality of the custom made software components is expected from the commercial applications. In most cases, such products (COTS) are not designed with stringent timing and/or safety requirements as priorities. Thus, to decrease the gap between the use of custom made components and COTS components, this paper presents a methodology for evaluating COTS products in the scope of dependable, real time systems, through the application of fault injection techniques at key points of the software engineering process. By combining the use of robustness testing (fault injection at interface level) with software fault injection (using educated fault injection operators), a COTS component can be assessed in the context of the system it will belong to, with special emphasis given to timing and safety constraints that are usually imposed by the target real time dependable environment. In the course of this work, three case studies have been performed to assess the methodology using realistic scenarios that used common COTS products. Results for one case study are presented",no8842,Assessing inter-modular error propagation in distributed software,"With the functionality of most embedded systems based on software (SW), interactions amongst SW modules arise, resulting in error propagation across them. During SW development, it would be helpful to have a framework that clearly demonstrates the error propagation and containment capabilities of the different SW components. In this paper, we assess the impact of inter-modular error propagation. Adopting a white-box SW approach, we make the following contributions: (a) we study and characterize the error propagation process and derive a set of metrics that quantitatively represents the inter-modular SW interactions, (b) we use a real embedded target system used in an aircraft arrestment system to perform fault-injection experiments to obtain experimental values for the metrics proposed, (c) we show how the set of metrics can be used to obtain the required analytical framework for error propagation analysis. We find that the derived analytical framework establishes a very close correlation between the analytical and experimental values obtained. The intent is to use this framework to be able to systematically develop SW such that inter-modular error propagation is reduced by design",no8841,A Novel Fault Diagnosis System for MANET Based on Hybrid GA-BP Algorithm,"With the fast development of mobile ad hoc networks (MANETs), fault diagnosis has become a critical need to guarantee robust service for various applications. Many techniques have been suggested to solve this problem, but they still cannot satisfy the special need of MANETs. In this paper, we propose a new fault diagnosis system using hybrid GA-BP neural network. The results of simulation demonstrate that the performance of this system is excellent.",no8840,Research on calibration system error of 6-axis force/torque sensor integrated in humanoid robot foot,"With the fast development of humanoid robot with high intelligence and accuracy, the improvement of comprehensive performance of 6-axis force/toque sensor(F/T sensor) has been constantly emphasized and further put forward to a higher demand. Except that a good proper mechanical design to guarantee the precision of the F/T sensor, the calibration quality is one of the most important factors of influencing the precision of F/T sensor too. The influencing factor of the precision of F/T sensor and system error source have been analysed from the view point of the calibration in this correspondence for the improvement of optimization design of sensor structure and calibration system in order to reduce or eliminate the error effects, which offers the theoretical foundation for improving the comprehensive performance and measurement accuracy of the F/T sensor.",no8839,A data mining based method: Detecting software defects in source code,"With the expansion of software size and complexity, how to detect defects becomes a challenging problem. This paper proposes a defect detection method which applies data mining techniques in source code to detect two types of defects in one process. The two types of defects are rule-violating defects and copy-paste related defects which may include semantic defects. During the process, this method can also extract implicit programming rules without prior knowledge of the software and detect copy-paste segments with different granularities. The method is evaluated with the Linux kernel that contains more than 4 million lines of C code. The result shows that the resulting system can quickly detect many programming rules and violations to the rules. After using the novel pruning techniques, it will greatly reduce the effort of manually checking violations so as a large number of false positives are effectively eliminated. As an illustrative example of its effectiveness, a case study shows that among the top 50 violations reported by the proposed model, 11 defects can be confirmed after examining the source code.",no8838,The research on fault equivalent analysis method in testability experiment validation,"With the existing fault injection techniques, many faults that can fully expose testability design defects can not be injected. To solve this problem, a method of fault equivalent analysis is proposed. By this means, some characteristics are extracted from the faults those unable to be injected, and ldquoyield analysisrdquo or ldquoyielded analysisrdquo is performed. Then the minimal cut sets of atom faults is obtained and selected, which are finally equivalent to the atom faults sequence. Applications show that the method not only solves the problem that many faults are not able to be injected, but also ensures the effect of testability experiment validation.",no8837,1<sup>st</sup> workshop on fault-tolerance for HPC at extreme scale FTXS 2010,"With the emergence of many-core processors, accelerators, and alternative/heterogeneous architectures, the HPC community faces a new challenge: a scaling in number of processing elements that supersedes the historical trend of scaling in processor frequencies. The attendant increase in system complexity has first-order implications for fault tolerance. Mounting evidence invalidates traditional assumptions of HPC fault tolerance: faults are increasingly multiple-point instead of single-point and interdependent instead of independent; silent failures and silent data corruption are no longer rare enough to discount; stabilization time consumes a larger fraction of useful system lifetime, with failure rates projected to exceed one per hour on the largest systems; and application interrupt rates are apparently diverging from system failure rates.",no8836,1<sup>st</sup> workshop on fault-tolerance for HPC at extreme scale FTXS 2010,"With the emergence of many-core processors, accelerators, and alternative/heterogeneous architectures, the HPC community faces a new challenge: a scaling in number of processing elements that supersedes the historical trend of scaling in processor frequencies. The attendant increase in system complexity has first-order implications for fault tolerance. Mounting evidence invalidates traditional assumptions of HPC fault tolerance: faults are increasingly multiple-point instead of single-point and interdependent instead of independent; silent failures and silent data corruption are no longer rare enough to discount; stabilization time consumes a larger fraction of useful system lifetime, with failure rates projected to exceed one per hour on the largest systems; and application interrupt rates are apparently diverging from system failure rates.",no8835,Analysis and Comparison of Fault Simulation,"With the development of VLSI, circuit Design for Testability has become the focus of attention. Fault diagnosis and detection VLSI has become an important part of the development of essential. This paper is based on DFT theory as background, introduced the concept of fault simulation. Then introduce several fault simulation algorithm. And they conducted a comparative analysis. This paper expounds fault simulation algorithms on the improvement and development direction.",no8834,Double Redundant Fault-Tolerance Service Routing Model in ESB,"With the development of the Service Oriented Architecture (SOA), the Enterprise Service Bus (ESB) is becoming more and more important in the management of mass services. The main function of it is service routing which focuses on delivery of message among different services. At present, some routing patterns have been implemented to finish the messaging, but they are all static configuration service routing. Once one service fails in its operation, the whole service system will not be able to detect such fault, so the whole business function will also fail finally. In order to solve this problem, we present a double redundant fault tolerant service routing model. This model has its own double redundant fault tolerant mechanism and algorithm to guarantee that if the original service fails, another replica service that has the same function will return the response message instead automatically. The service requester will receive the response message transparently without taking care where it comes from. Besides, the state of failed service will be recorded for service management. At the end of this article, we evaluated the performance of double redundant fault tolerant service routing model. Our analysis shows that, by importing double redundant fault tolerance, we can improve the fault-tolerant capability of the services routing apparently. It will solve the limitation of existent static service routing and ensure the reliability of messaging in SOA.",no8833,"A Lightweight, Fault-Tolerant, Load Balancing Service Discovery and Invocation Algorithm for Pervasive Computing Environment","With the development of the related technologies and the increasing applications of ad hoc networks, pervasive computing is becoming more and more powerful. In order to achieve the goal of wide use of computing capability at anytime and everywhere, service discovery service must be used in pervasive computing applications. As a very important problem in pervasive computing, there have been lots of researches on service discovery, but the load balance problem has been ignored. While it is essential that services could be found in pervasive computing environment, it is also important that each device that can provide services is load balancing. In this paper, we present a service discovery and invocation algorithm that is lightweight, fault-tolerant and load balancing. From the simulation results, we can see that the algorithm is reliable and robust enough to adapt to the devices limitation and frequent changes of devices in pervasive computing environment.",no8832,"Merging PMU, Operational, and Non-Operational Data for Interpreting Alarms, Locating Faults and Preventing Cascades","With the development of synchronized sampling technique and other advanced measurement approaches, the merging of various substation data to be used in new applications in the EMS solutions has not yet been explored adequately. This paper deals with the integration of time correlated information from Phasor Measurement Units, SCADA and non-operational data captured by other intelligent electronic devices such as protective relays and digital fault recorders, as well as their applications in alarm processing, fault location and cascading event analysis. A set of new control center visualization tools shows that the merging of PMU, operational and non-operational data could improve the effectiveness of alarm processing, accuracy of fault location and ability to detect cascades.",no8831,Airborne laser depth sounding: improvements in position- and depth estimates by local corrections for sea surface slope,"With the development of laser and data acquisition technology, the sounding density in laser depth sensing can be increased. When the distance between each laser shot is less than half the length of the sea surface wave, it is possible to estimate the local angle of the sea surface waves. The estimated slope angle is used to calculate the angular deflection of the light beam that penetrates the water surface. This allows a better estimation of the beam center position on the sea bottom than can be done by averaging of a flat sea surface. The possibility to improve the position and depth accuracy is examined with experimental data from a laser depth sounding system by comparing the repeatability of the depth estimations from different flights over the same area. The experimental data is collected from two different sites with bottom depths between 3 m and 9 m. The surface maximum wavelength and significant waveheight varies up to 8 m and 0.6 m respectively. The different measurements on a site are done using the same flight direction and a shot distance of approximately 1 m. It is seen that the combined position and depth corrections for local surface slope angles can reduce the maximum depth difference between different measurements over the same area by up to 10 cm. The use of different spatial filtering of the water surface data along with different algorithms to calibrate the position and depth corrections is demonstrated",no8830,Comprehension and evaluation on significant error risk of CPA audit in the information-processing environment,"With the development of IT, kinds of information systems and software have been applied in the audited enterprises. That means the audited enterprises have set up their computer-based information-processing environment. In such environment, the uncertainty factors of the certified public accountants (CPA) audit are increasing and the audit risk is enhanced. It is more suitable for the audit risk factors to be described in significant error risk (SER) and detection risk in the information-processing environment. The key of controlling the whole audit risk is to comprehend and evaluate the significant error risk objectively. This essay explains what is the SER and why the SER is the key in controlling CPA audit risk and introduces a quantitative method to help auditors to assess it objectively. Because risk has obviously grey and ambiguous characteristics, it will be helpful for auditors to use the method that involved grey principle components and fuzzy comprehensive evaluation to assess the SER. Only by understanding the risk factors in the information-processing environment of the audited enterprises deeply and evaluating the SER objectively can help auditors reach the audit aims and control the audit risk under an acceptable level.",no8829,Infrared technology in the fault diagnosis of substation equipment,"With the development of infrared technology and the further application in electric power system, it plays a more and more important role in electrical equipment fault diagnosis. Improving the accuracy of infrared diagnosis technology and its application effect is of important practicality value to the research of infrared diagnosis application technology. From the point of electric power system daily patrol, the paper expatiates how to diagnose the most popular radiation fault and trouble using the infrared imaging equipment, the operation process of obtaining infrared images of electrical equipment and the analysis of infrared images. In addition, the paper presents a series of management methods associating with infrared diagnosis daily work.",no8828,Infrared technology in the fault diagnosis of substation equipment,"With the development of infrared technology and the further application in electric power system, it plays a more and more important role in electrical equipment fault diagnosis. Improving the accuracy of infrared diagnosis technology and its application effect is of important practicality value to the research of infrared diagnosis application technology. From the point of electric power system daily patrol, the paper expatiates how to diagnose the most popular radiation fault and trouble using the infrared imaging equipment, the operation process of obtaining infrared images of electrical equipment and the analysis of infrared images. In addition, the paper presents a series of management methods associating with infrared diagnosis daily work.",no8827,The Application of Safety Simulation Technology in the Fault Diagnosis of the Chemical Process,"With the development of information and computational technology, the safety simulation technique is becoming more and more useful in the chemical process hazard assessment, hazard identification, and safety control system design and operating personnel training etc.The fault diagnosis of the gravity water tank is studied by using dynamic simulation of HYSYS (Hyprotech System for Engineers). The simulation results presents the method need not design problem-specific observer to estimate unmeasured state variables, and can identification and diagnosis faults simultaneously as well. The parameters of the chemical process are updated via on-line correction.",no8826,A novel insulation on-line monitoring and fault diagnosis system used for traction substation,"With the development of high speed railways, there are higher requirements to insure the reliability of the traction power supply system. In this paper, a novel insulation on-line monitoring and fault diagnosis system based on the embedded Linux operation system is introduced. The system can continuously monitor the insulation state of the power instruments, such as traction transformers, arresters, circuit breakers, and insulators. Different monitoring techniques were applied in the system. The embedded Linux operation system with TCP/IP protocol makes the field monitoring devices possess the powerful data processing and networking functions. The monitoring system builds a good basis for traction substation insulation on-line monitoring and diagnosis.",no8825,Benchmarking a Semantic Web Service Architecture for Fault-tolerant B2B Integration,"With the development and maturity of Service- Oriented Architectures (SOA) to support business-tobusiness transactions, organizations are implementing Web services to expose their public functionalities associated with internal systems and business processes. In many business processes, Web services need to provide is a high level of availability, since the globalization of the Internet enables business partners to easily switch to other competitors when services are not available. Along with the development of SOA, considerable technological advances are being made to use the semantic Web to achieve the automated processing and integration of data and applications. This paper describes the implementation and benchmarking of an architecture that semantically integrates Web services with a peer-to-peer infrastructure to increase service availability through fault-tolerance.",no8824,Fault-tolerant control system research based on dual CPU redundancy for high-voltage inverter,"With the deepening of China's reform and the rapid development of economic construction, the needs of energy-saving and environmental protection are growing significantly. As the optimal method of induction motor control, high-voltage frequency conversion technology has been widely used. In some industries the reliability of high-voltage high frequency device is concerned. The purpose of this paper is to provide the method of multi-CPU redundancy, fault-tolerant design based on high-voltage inverter, to enhance the reliability of the high voltage inverter. With the application, high-voltage inverter control system can automatically switch main and standby CPU without stopping motor or affecting the operation. The situation that frequency can not be adjusted or motor must be stopped for errors can be avoided. This improves the continuous operation and reliability of control system and the whole system.",no8823,Mitigating Soft Errors in System-on-Chip Design,"With the continuous downscaling of CMOS technologies, the reliability has become a major bottleneck in the evolution of the next generation scaling. Technology trends such as transistor downsizing, use of new materials and high performance computer architecture continue to increase the sensitivity of systems to soft errors. Today the technologies are moving into the period of nanotechnologies and system-on-chip (SoC) designs are widely used in most of the applications, the issues of soft errors and reliability in complex SoC designs are set to become and increasingly challenging. This paper gives a review to the soft error in SoC designs and then presents the fault tolerant solution.",no8822,High Continuous Availability Digital Information System Based on Stratus Fault-Tolerant Server,"With the construction of harmonious society, health improvement and the rapid development of information technology, People put forward higher requirements for the hospital. Hospital information system as an online services system requires continuous operation. Server system is the key to support hospital operations. System paralyzed accident caused by Server system failure is also not uncommon. Aiming at the problem of insufficient reliability of the traditional Cluster cluster server system, The article made a in-depth technical analysis on the performance of the Stratus fault-tolerant server. Combing with the characteristics of hospital information system, it proposed the digital hospital information system structure based on Stratus fault-tolerant server and explored and analyzed the economic and technical advantages of the program. The application effect demonstrates that the program is of the economic good and can realize continuous availability.",no8821,A Web-Based Fault Diagnosis of Analogue Circuits System,"With the coming of the network era and the continuous improvement of information requirement, the fault diagnosis of devices is advancing from traditional mono device and field mode to distributed and remote mode, which can largely improve the ability of device maintenance. Several NI technologies are used in this paper to realize virtual instrument remote application and computer support collaborative work environment for device remote fault diagnosis. A simulated experiment on Fault Diagnosis of Analogue Circuits system is performed and the feasibility of the scheme is proved.",no8820,Finite element analysis of internal winding faults in distribution transformers,"With the appearance of deregulation, distribution transformer predictive maintenance is becoming more important for utilities to prevent forced outages with the consequential costs. To detect and diagnose a transformer internal fault requires a transformer model to simulate these faults. This paper presents finite element analysis of internal winding faults in a distribution transformer. The transformer with a turn-to-earth fault or a turn-to-turn fault is modeled using coupled electromagnetic and structural finite elements. The terminal behaviors of the transformer are studied by an indirect coupling of the finite element method and circuit simulation. The procedure was realized using a commercially available software. The normal case and various faulty cases were simulated and the terminal behaviors of the transformer were studied and compared with field experimental results. The comparison results validate the finite element model to simulate internal faults in a distribution transformer.",no8819,Finite Element Analysis of Internal Winding Faults in Distribution Transformers,"With the appearance of deregulation, distribution transformer predictive maintenance is becoming more important for utilities to prevent forced outages with the consequential costs. To detect and diagnose a transformer internal fault requires a transformer model to simulate these faults. This paper presents finite element analysis of internal winding faults in a distribution transformer. The transformer with a turn-to-earth fault or a turn-to-turn fault is modeled using coupled electromagnetic and structural finite elements. The terminal behaviors of the transformer are studied by an indirect coupling of the finite element method and circuit simulation. The procedure was realized using a commercially available software. The normal case and various faulty cases were simulated and the terminal behaviors of the transformer were studied and compared with field experimental results. The comparison results validate the finite element model to simulate internal faults in a distribution transformer.",no8818,Use of substation IED data for improved alarm processing and fault location,"With the advent of technology, substations of modern days are being equipped with different types of IEDs (Intelligent Electronic Devices) such as Digital Protective Relay (DPR), Digital Fault Recorders (DFR), Phasor Measurement Units (PMU), etc. These devices are capable of recording huge amount of data and thus integration and appropriate use of those data can be beneficial to the power industry. There are several issues to be solved in this regard: (1) Which data to be used and when (for what application), (2) Accuracy of such data (in the measurement process from the place of data capture to where it is used), (3) Extraction of useful information from captured data and (4) Use of the information in applications. This paper focuses on these issues and also some new applications which can use those substation IED data.",no8817,An RT-level fault model with high gate level correlation,"With the advent of new RT-level design and test flows, new tools are needed to migrate at the RT-level the activities of fault simulation testability analysis, and test pattern generation. This paper focuses on fault simulation at the RT-level, and aims at exploiting the capabilities of VHDL simulators to compute faulty responses. The simulator was implemented as a phototypical tool, and experimental results show that simulation of a faulty circuit is no more costly than simulation of the original circuit. The reliability of the fault coverage figures computed at the RT-level is increased thanks to an analysis of inherent VHDL redundancies, and by foreseeing classical synthesis optimizations. A set of rules is used to compute a fault list that exhibits good correlation with stuck-at faults",no8816,A Multi-core Approach to Providing Fault Tolerance for Non-deterministic Services,"With the advent of multi- and many-core architectures, new opportunities in fault-tolerant computing have become available. In this paper we propose a novel process replication method that provides transparent failover of non-deterministic TCP services by utilizing spare CPU cores. Our method does not require any changes to the TCP protocol, does not require any changes to the client software, and unlike existing solutions, it does not require any changes to the server applications either. We measure performance overhead on two real-world applications, a multimedia streaming service and an Internet Relay Chat daemon and show that the imposed overhead is minimal as the price of seamless failover. Our prototype implementation consists of a kernel module for Linux 2.6 without any changes to the existing kernel code.",no8815,Fault-tolerant Video on Demand in RSerPool Architecture,"With the advent of Internet, video over IP is gaining popularity. In such an environment, scalability and fault tolerance will be the key issues. Existing video on demand (VoD) service systems are usually neither scalable nor tolerant to server faults and hence fail to comply to multi-user, failure-prone networks such as the Internet. Current research areas concerning VoD often focus on increasing the throughput and reliability of single server, but rarely addresses the smooth provision of service during server as well as network failures. Reliable Server Pooling (RSerPool), being capable of providing high availability by using multiple redundant servers as single source point, can be a solution to overcome the above failures. During a possible server failure, the continuity of service is retained by another server. In order to achieve transparent failover, efficient state sharing is an important requirement. In this paper, we present an elegant, simple, efficient and scalable approach which has been developed to facilitate the transfer of state by the client itself, using extended cookie mechanism, which ensures that there is no noticeable change in disruption or the video quality.",no8814,Designing quantum adder circuits and evaluating their error performance,"With the advent of efficient quantum algorithms and technological advances, design of quantum circuits has gained importance. Minimization of the gate count and the number of gate levels are the two major objectives in quantum circuit design. The peculiar nature of quantum decoherence that leads to quantum errors mandates completion of all the quantum gate operations within a time bound, hence reduction in the gate count and the number of circuit levels leads to lowering the errors and the overall cost in quantum circuits. In this paper, we propose the design of adder circuits using CNOT and C<sup>k</sup>NOT gates, with significant reduction in gate count and number of gate levels over their existing counterparts in the literature. We then present a software model for evaluating errors in quantum computing circuits and employ it for evaluating the error performance of our proposed quantum adder circuits.",no8813,Network intrusion and fault detection: a statistical anomaly approach,"With the advent and explosive growth of the global Internet and electronic commerce environments, adaptive/automatic network/service intrusion and anomaly detection in wide area data networks and e-commerce infrastructures is fast gaining critical research and practical importance. We present and demonstrate the use of a general-purpose hierarchical multitier multiwindow statistical anomaly detection technology and system that operates automatically, adaptively, and proactively, and can be applied to various networking technologies, including both wired and wireless ad hoc networks. Our method uses statistical models and multivariate classifiers to detect anomalous network conditions. Some numerical results are also presented that demonstrate that our proposed methodology can reliably detect attacks with traffic anomaly intensity as low as 3-5 percent of the typical background traffic intensity, thus promising to generate an effective early warning.",no8812,Autonomous Fault Recovery Technology for Achieving Fault-Tolerance in Video on Demand System,"With the advances of compression technology, storage devices and networks, video on demand (VoD) service is becoming popular. The system needs to provide continuous service and heterogeneous service levels for users. However, these requirements cannot be satisfied in conventional VoD system which is constructed on redundant content servers and centralized management. In this paper, autonomous VoD system is proposed to meet the requirements. The system is constructed on faded information field architecture. Under the proposed architecture, autonomous fault detection and fault recovery technologies are proposed to achieve fault-tolerance for continuous service. The effectiveness of the proposed technologies are proved through simulation. The results show that an average of 30% improvement in recovery time and users' video service can be recovered without stopping compared with conventional VoD system",no8811,IVF: Characterizing the vulnerability of microprocessor structures to intermittent faults,"With the advancement of CMOS manufacturing process to nano-scale, future shipped microprocessors will be increasingly vulnerable to intermittent faults. Quantitatively characterizing the vulnerability of microprocessor structures to intermittent faults at early design stage is significantly helpful to balance system performance and reliability. Prior researches have proposed several metrics to characterize the vulnerability of microprocessor structures to soft errors and permanent faults, however, the vulnerability of these structures to intermittent faults are still rarely considered. In this work, we propose a metric intermittent vulnerability factor (IVF) to characterize the vulnerability of microprocessor structures to intermittent faults. A structure's IVF is the probability an intermittent fault in that structure causes an external visible error. We instrument a cycle-accurate execution-driven simulator Sim-Alpha to compute IVFs for reorder buffer and register file. Experimental results show that the IVF of reorder buffer is much higher than that of register file. Besides, IVF varies significantly across different structures and workloads, which implies partial protection to the most vulnerable structures to improve system reliability with less overhead.",no8810,ORBIT: Effective Issue Queue Soft-Error Vulnerability Mitigation on Simultaneous Multithreaded Architectures Using Operand Readiness-Based Instruction Dispatch,"With the advance of semiconductor processing technology, soft errors have become an increasing cause of failures of microprocessors fabricated using smaller and more densely integrated transistors with lower threshold voltages and tighter noise margins. With diminishing performance returns on wider issue superscalar processors, the microprocessor design industry has opted for using simultaneous multithreaded (SMT) architectures in commercial processors to exploit thread-level parallelism (TLP). SMT techniques enhance overall system performance but also introduce greater susceptibility to soft errors - concurrently executing multiple threads exposes many program runtime states to soft-error strikes at any given time. The issue queue (IQ) is a key micro architecture structure to exploit instruction-level and thread-level parallelism. On SMT processors, the IQ buffers a large number of instructions from multiple threads and is more susceptible to soft-error strikes. In this paper, we explore the use of operand-readiness-based instruction dispatch (ORBIT) as an effective mechanism to mitigate IQ soft-error vulnerability on SMT processors. We observe that IQ soft-error vulnerability is largely affected by instructions waiting for their source operands. The overall IQ soft-error vulnerability can be effectively reduced by minimizing the number of waiting instructions and their residency cycles in the IQ. We develop six techniques that aim to improve IQ reliability with negligible performance degradation on SMT processors. Moreover, we extend our techniques with prediction methods that can anticipate the readiness of source operands ahead of time. The ORBIT schemes integrated with reliability-awareness and readiness prediction achieve more attractive reliability/performance trade-offs. The best of the proposed schemes (e.g. Predict_DelayACE) reduces IQ vulnerability by 79% with only 1% throughput IPC and 3% harmonic IPC reduction across all studied workloads.",no8809,Optimal scheduling of imprecise computation tasks in the presence of multiple faults,"With the advance of applications such as multimedia, image/speech processing and real-time AI, real-time computing models allowing to express the timeliness versus precision trade-off are becoming increasingly popular. In the imprecise computation model, a task is divided into a mandatory part and an optional part. The mandatory part should be completed by the deadline even under worst-case scenario; however, the optional part refines the output of a mandatory part within the limits of the available computing capacity. A non-decreasing reward function is associated with the execution of each optional part. Since the mandatory parts have hard deadlines, provisions should be taken against faults which may occur during execution. An FT-Optimal framework allows the computation of a schedule that simultaneously maximizes the total reward and tolerates transient faults of mandatory parts. We extend the framework to a set of tasks with multiple deadlines, multiple recovery blocks and precedence constraints among them. To this aim, we first obtain the exact characterization of imprecise computation schedules which can tolerate up to k faults, without missing any deadlines of mandatory parts. Then, we show how to generate FT-Optimal schedules in an efficient way. Our solution works for both linear and general concave reward functions",no8808,Timing-Error-Tolerant Network-on-Chip Design Methodology,"With technology scaling, the wire delay as a fraction of the total delay is increasing, and the communication architecture is becoming a major bottleneck for system performance in systems on chip (SoCs). A communication-centric design paradigm, networks on chip (NoCs), has been proposed recently to address the communication issues of SoCs. As the geometries of devices approach the physical limits of operation, NoCs will be susceptible to various noise sources such as crosstalk, coupling noise, process variations, etc. Designing systems under such uncertain conditions become a challenge, as it is harder to predict the timing behavior of the system. The use of conservative design methodologies that consider all possible delay variations due to the noise sources, targeting safe system operation under all conditions will result in poor system performance. An aggressive design approach that provides resilience against such timing errors is required for maximizing system performance. In this paper, we present T-error, which is a timing-error-tolerant aggressive design method to design the individual components of the NoC (such as switches, links, and network interfaces), so that the communication subsystem can be clocked at a much higher frequency than a traditional conservative design (up to 1.5x increase in frequency). The NoC is designed to tolerate timing errors that arise from overclocking without substantially affecting the latency for communication. We also present a way to dynamically configure the NoC between the overclocked mode and the normal mode, where the frequency of operation is lower than or equal to the traditional design's frequency, so that the error recovery penalty is completely hidden under normal operation. Experiments on several benchmark applications show large performance improvement (up to 33% reduction in average packet latency) for the proposed system when compared to traditional systems.",no8807,Temporal and Spatial Requirements for Optimized Fault Location,"With technological advancements data availability in power systems is drastically increased. Intelligent electronic devices are capable of communicating recorded data. Data can be stored, easily interfaced from different access points and, intelligent techniques can be used for automated fault analysis. After summarizing obstacles in the current framework of fault location analysis, this paper will explore temporal and spatial aspects of available data. This leads to introducing implementation framework of automated optimized fault location that is capable of taking advantage of both the time and space aspects of data.",no8806,Double sampling data checking technique: an online testing solution for multisource noise-induced errors on on-chip interconnects and buses,"With processors and system-on-chips using nano-meter technologies, several design and test efforts have been recently developed to eliminate and test for many emerging DSM noise effects. In this paper, we show the emergence of multisource noise effects, where multiple DSM noise sources combine to produce functional and timing errors even when each separate noise source itself does not. We show the dynamic nature of multisource noise, and the need for online testing to detect such noise errors. We propose an online approach based on low-cost double-sampling data checking circuit to test for such noise effects in on-chip buses. Based on the proposed circuit, an effective and efficient testing methodology has been developed to facilitate online testing for generic on-chip buses. The applicability of this methodology is demonstrated through embedding the online detection circuit in a bus design. The validated design shows the effectiveness of the proposed testing methodology for multisource noise-induced errors in global interconnects and buses.",no8805,"System architecture for error-resilient, embedded JPEG2000 wireless delivery","With new, third generation mobile terminals several multimedia-based applications will be soon available. The reliable transmission of audiovisual content will probably become one of the most asked services. On the other hand, when wireless delivery is addressed, it would be desirable to reach very high compression ratio keeping a good image perceptual quality. With these requirements the choice of JPEG2000 as the source encoding stage assures excellent results. However a non-ideal wireless network could seriously affect the received image decoding, despite JPEG2000's error resilience capabilities. In this paper a flexible DSP/FPGA system with high error-resilience features is proposed. Experimental results show very promising visual quality, with a limited complexity overhead, even in the presence of a mean loss rate of 10%.",no8804,Fault-tolerant task scheduling in multiprocessor systems based on primary-backup scheme,"With multiprocessor systems, redundant scheduling is a technique that trades processing power for increased reliability through redundancy. One novel approach, called primary-backup task scheduling, is a large number of used in hard real-time multiprocessor systems to guarantee the deadlines of tasks faults. In this paper, we analysis and compare the scheduling algorithm based on primary-backup replication in current distributed system. Finally, the important features that fault-tolerant Task Scheduling in Multiprocessor Systems based on primary-backup are summarized, and the future research strategies and tends are given.",no8803,Atmospheric correction of spectral imagery: evaluation of the FLAASH algorithm with AVIRIS data,"With its combination of good spatial and spectral resolution, visible to near infrared spectral imaging from aircraft or spacecraft is a highly valuable technology for remote sensing of the Earth's surface. Typically it is desirable to eliminate atmospheric effects on the imagery, a process known as atmospheric correction. We review the basic methodology of first-principles atmospheric correction and present results from the latest version of the FLAASH (fast line-of-sight atmospheric analysis of spectral hypercubes) algorithm. We show some comparisons of ground truth spectra with FLAASH-processed AVIRIS (airborne visible/infrared imaging spectrometer) data, including results obtained using different processing options, and with results from the ACORN (atmospheric correction now) algorithm that derive from an older MODTRAN4 spectral database.",no8802,Fault handling in embedded industrial measurement and control systems: issues and a case study,"With increasingly complex control systems used in a variety of commercial, aerospace, and military applications, system faults may occur during system operations. These faults inevitably result in abnormal operations and production shutdown or even disasters. Therefore, improving system reliability has become a major concern in safety-critical systems. This paper primarily addresses the issues in embedded fault-tolerant control system designs and presents a case study on vibration suppression in the aerospace industry. The design issues on embedded control systems such as component failures, sampling jitters and control delay, network-induced delay and packet loss in network transmission are discussed, all of which may have damaging effects on the closed-loop system performance. A case study on vibration control for a launch vehicle payload fairing using multiple embedded PZT actuators are also presented, where an adaptive actuator failure compensation scheme is successfully implemented. Fault-tolerant control turns out to be effective in creating more robust industrial measurement and control systems.",no8801,Fault diagnosis of electronic system using artificial intelligence,"With increasing system complexity, shorter product life cycles, lower production costs, and changing technologies, the need for intelligent tools for all stages of a product's lifecycle is becoming increasingly important. The purpose of this article is to give a brief review how AI has been used in the field of electronic fault diagnosis. Topics discussed include: rule-based diagnostic systems; model-based diagnostic systems; case-based reasoning (CBR); fuzzy reasoning and artificial neural networks (ANN); hybrid approaches; IEEE diagnostic standards and automated diagnostic tool future developments.",no8800,On the significance of fault tree analysis in practice,"With increasing system complexity and extensive use of computerized control of industrial processes and plants, it is essential to have a systematic approach for identifying failures that can expose people and environment for unacceptable risks. With focus on a drive system used to control a linear motor, the fault tree analysis method is utilized to reveal design weaknesses and to find mitigations that can improve the system safety characteristics. Starting with a set of top level hazards, elements with high risk impact are identified, and appropriate mitigations are suggested.",no8799,Performance Evaluation of Probe-Send Fault-tolerant Network-on-chip Router,"With increasing reliability concerns for current and next generation VLSI technologies, fault-tolerance is fast becoming an integral part of system-on-chip and multi-core architectures. Another trend for such architectures is network-on-chip (NoC) becoming a standard for on-chip global communication. In an earlier work, a generic fault-tolerant routing algorithm in the context of NoCs has been presented. The proposed routing algorithm works in two phases, namely path exploration (PE) and normal communication. This paper presents fundamental insights into various novel PE approaches, their feasibility and performance trade-offs for k-ary 2-cube NoCs. The dependence of the normal communication phase on the probability of finding paths and their quality in the first phase emphasizes the PE's significance. One major contribution of this work is the investigation of application of constrained randomness to PE for optimizing the quality of paths. Another contribution is the proposed use of merging of traffic to reduce the reconfiguration time by a large amount (73.8% on an average).",no8798,Characterization of a Fault-tolerant NoC Router,"With increasing reliability concerns for current and next generation VLSI technologies, fault-tolerance is fast becoming an integral part of system-on-chip (SoC) and multi-core architectures. Another concern for these architectures is increasing global wire lengths with associated issues leading to network-on-chips (NoC) becoming standard for on-chip global communication. We recognize these issues and present an on-chip generic fault-tolerant routing algorithm. The microarchitecture of a NoC router implementing the proposed routing algorithm for a k-ary 2-cube topology is provided. The proposed router works in two phases. In the first phase, the network is explored for an existing path between source-destination pairs after reset or during system reconfiguration after fault detection. Existing paths are cached and used in the second phase of data communication during normal system operation. The presented router architecture also proposes a concept of dynamic multiplexing of virtual channels on physical channels to efficiently utilize physical channel bandwidth. The above approaches complement each other and when combined together, result in an efficiently realizable high-performance NoC fault-tolerant router. An implementation characterization of this k-ary 2-cube torus router in terms of area, power and critical path delay in IBM Cu-08 technology is presented, along with bandwidth and latency characterization for relevant cases.",no8797,Design and analysis of a reduced phase error digital carrier recovery architecture for high-order quadrature amplitude modulation signals,"With increasing order of quadrature amplitude modulation (QAM), the bandwidth efficiency is improved in digital communication. However, in practice, the modulation order is limited, since conventional digital carrier recovery (CR) algorithms give rise to unacceptable phase error. The authors present an efficient software-aided technique for phase error reduction in CR for high-order QAM, based on the simple and well-known fourth power CR loop. Analytical and simulation results indicate that the new technique has several attractive features such as approximate of invariance of phase error improvement over modulation order and low hardware complexity for modulation orders as high as 256-QAM. Experimental results for 64 and 256-QAM illustrate phase error variance of less than -110-dBc/Hz at the frequency offset of 10-kHz, that is, 30-dB reduction of phase error variance or 3-dB increase in system processing gain compared to the conventional fourth power CR loop. This allows a significant improvement of bandwidth efficiency by increasing the modulation order, at the cost of slight complexity overhead.",no8796,Feature-Based Fault Detection Approaches,"With increasing complexity of systems it is becoming more and more time consuming and difficult to achieve reliable fault detection strategies. Using model-based methods requires detailed knowledge about the systems behavior and seems in some cases successful in theory but un-applicable in real-time due to high computation requirements. In this contribution, an idea and algorithm for feature-based fault detection approach is proposed. The main idea of this approach is to detect and identify faults in a complex system without any kind of modeling. By extracting features from relevant sensor signals, yielded from hardware-in-the-loop simulations, and combining them in a matrix, it is possible for a human operator to denote subsets of the matrix as fault-free and faulty areas. An advantage is the ability to set individual thresholds for the subsets, giving a more robustness towards false alarms and a possibility to denote individual subsets to relevant faults. From this, it will be shown that identification of faults is possible. In order to achieve a fault detection and identification ability, it is necessary to implement the faults of interest in a test rig and conduct hardware-in-the-loop simulations. The raw data from fault-free and faulty simulations are used in the training of the matrix and the algorithm detects and identifies the faults in a robust way. The results are compared to a classical fault detection method that uses fixed thresholds. It will be shown how a sensor bias fault and a pressure relief valve fault are detected and identified",no8795,A Probabilistic Characterization of Fault Rings in Adaptively-Routed Mesh Interconnection Networks,"With increase in concern for reliability in the current and next generation of multiprocessors system-on-chip (MP-SoCs), multi-computers, cluster computers, and peer-to-peer communication networks, fault-tolerance has become an integral part of these systems. One of the fundamental issues regarding fault-tolerance is how to efficiently route a faulty network where each component is associated with some probability of failure. Adaptive fault-tolerant routing algorithms have been frequently suggested in the literature as means of improving communication performance and fault-tolerant demands in computer systems. Also, several results have been reported on usage of fault rings in providing detours to messages blocked by faults and in routing messages adaptively around the rectangular faulty regions. In order to analyze the performance of such routing schemes, one must investigate the characteristics of fault rings. In this paper, we derive mathematical expressions to compute the probability of message facing the fault rings in the well-known mesh interconnection network. We also conduct extensive simulation experiments using a variety of faults, the results of which are used to confirm the accuracy of the proposed models.",no8794,Performance comparison of MLP and RBF neural networks for fault location in distribution networks with DGs,"With high penetration of distributed generations (DGs), power distribution system is regarded as a multisource system in which fault location scheme must be direction sensitive. This paper presents an automated fault location method using radial basis function neural network (RBFNN) for a distribution system with DG units. In the proposed method, the fault type is first determined by normalizing the fault currents of the main source and then fault location is predicted by using RBFNN. Several case studies have been considered to verify the accuracy of the RBFNN. A comparison is also made between the RBFNN and the conventional multilayer perceptron neural network for locating faults in a power distribution system with DGs. The test results showed that the RBFNN can accurately determine the location of faults in a distribution system with several DG units.",no8793,Adaptive online testing for efficient hard fault detection,"With growing semiconductor integration, the reliability of individual transistors is expected to rapidly decline in future technology generations. In such a scenario, processors would need to be equipped with fault tolerance mechanisms to tolerate in-field silicon defects. Periodic online testing is a popular technique to detect such failures; however, it tends to impose a heavy testing penalty. In this paper, we propose an adaptive online testing framework to significantly reduce the testing overhead. The proposed approach is unique in its ability to assess the hardware health and apply suitably detailed tests. Thus, a significant chunk of the testing time can be saved for the healthy components. We further extend the framework to work with the StageNet CMP fabric, which provides the flexibility to group together pipeline stages with similar health conditions, thereby reducing the overall testing burden. For a modest 2.6% sensor area overhead, the proposed scheme was able to achieve an 80% reduction in software test instructions over the lifetime of a 16-core CMP.",no8792,Software Fault Protection with ARINC 653,"With flight software becoming ever more complex, assuming that it behaves perfectly is no longer realistic. At the same time Verification and Validation (V&V) is consuming up to 50% of flight software development costs. The adaptation of fault protection concepts to flight software is attractive, particularly in the context of the fault containment and health management capabilities of ARINC 653. We propose a proactive, unified, model-based approach in which the behavior of the software is monitored against a model of its expected behavior. We describe how that may be incorporated into the ARINC 653 health management architecture. We describe software capabilities that facilitate software fault protection. These capabilities include enhancements to the ARINC 653 application executive, tools for software instrumentation, and a temporal logic runtime monitoring framework for high-level specification and monitoring. We analyze the aspects of the software that should be modeled and the types of failure responses. We show how these concepts may be applied to the Mission Data System (MDS) flight software framework.",no8791,Techniques and experience in on-line transformer condition monitoring and fault diagnosis in ElectraNet SA,"With evolving maintenance strategies in the electricity industry internationally, there has been increasing pressure to develop improved techniques for condition monitoring. Specifically there has been a trade off between the speed and accuracy of testing. Traditionally, transformer condition monitoring involved high accuracy tests, which due to their duration, could only be performed on a discrete periodic basis. ElectraNet SA has experienced many limitations associated with this form of condition monitoring, and there has been a trend towards high speed on-line monitoring techniques for power transformers. Though these new techniques do not provide the level of accuracy found in traditional forms of testing, they overcome many of their limitations. This paper, describes ElectraNet SA's techniques and experience with power transformer monitoring",no8790,ECC-Cache: A Novel Low Power Scheme to Protect Large-Capacity L2 Caches from Transiant Faults,"With dramatic scaling in feature size of VLSI technology, the capacity of on-chip L2 cache increases rapidly, how to guarantee the reliability of large capacity L2 cache has become an important issue. However, increasing the reliability of L2 cache tends to reduce its performance and brings more power consumption. This paper presents ECC-Cache, a novel low power fault-tolerant architecture which divided the traditional method of detecting and correcting errors using some uniform coding scheme into two steps, and uses a hybrid method which protects clean data and dirty data in different way to enhance the reliability of L2 cache. This paper also compares the performance and power consumption of ECC-Cache with that of some other proposed schemes, experimental results show that ECC-Cache is effective to guarantee the reliability of large-capacity L2 cache, while bringing little impact to system performance and power consumption. We find that ECC-Cache performs better than the uniform-ECC scheme adopted in some widespread used commercial processors and some proposed schemes in other papers. Compared with the cache which has no protection, ECC-Cache only consumes 3% to 6% additional power and degrades performance no more than 2%.",no8789,Application of video error resilience techniques for mobile broadcast multicast services (MBMS),"With data throughput for mobile devices constantly increasing, services such as video broadcast and multicast are becoming feasible. The 3GPP (3rd Generation Partnership Project) committee is currently working on a standard for mobile broadcast and multicast services (MBMS). MBMS is expected to enable easier deployment of video and multimedia services on 3G networks. We present an overview of the standard including the proposed architecture and requirements focusing on radio aspects. We discuss the issue of video error resilience in such services that is critical to maintain consistent quality for terminals. The error resilience techniques currently used in video streaming services are not suitable for MBMS services. We analyze the error resilience techniques that are applicable within the context of MBMS standard and present our early research in this area.",no8788,A scatter correction using thickness iteration in dual-energy radiography,"With area detectors, scattered radiation causes the dominant error in separation of different materials properly. Several methods for the scatter correction in dual energy imaging had been suggested and improved results. Such methods, however, need additional lead blocks or detectors, and additional exposures to estimate the scatter fraction for every correction. We suggest the scatter correction by using a database of the fraction and distribution of scattered radiations. To verify this method we did MCNP simulation. The generation of the scatter information for each combination of thickness of an aluminum-water phantom had been done. Based on the uncorrected signals, thickness of each material can be calculated by a conventional dual-energy algorithm. And then the scatter information of corresponding thickness from the look-up table is used to correct the original signals. The iterative scatter correction reduced relative-thickness error in results. This scatter correction method can be applied to two-material dual-energy radiography like mammography, contrast imaging, or industrial inspection.",no8787,Study on eliminating the quality defects of vulcanized products with large size rubber-belt vulcanizing machine,"With ANSYS software hot plate thermal fields are analyzed and based on the MATLAB software, the functional relationship between hot plate temperature and the length is plotted, and then the fitting curve is given; the vulcanization intensity of belt is calculated, and the three-dimensional model figure of vulcanization intensity over time is drawn; the vulcanization effect of belt is calculated, the hot plate curing effect and the length of the curve is drawn to address the tape when the quality of intermittent curing deficiencies, the hot plate the equivalent scope was summarized.",no8786,Fault Tolerance in FPGA Architecture Using Hardware Controller - A Design Approach,"With advancement in process technology, the feature size is decreasing which leads to higher defect densities. More sophisticated techniques at increased costs are required to avoid defects. If nano-technology fabrication are applied the yield may go down to zero as avoiding defect during fabrication will not be a feasible option Hence, feature architecture have to be defect tolerant. In regular structure like FPGA, redundancy is commonly used for fault tolerance. In this work we present a solution in which configuration bit-stream of FPGA is modified by a hardware controller that is present on the chip itself. The technique uses redundant device for replacing faulty device and increases the yield. The design is implemented using FPGA Altera Quartus II EC121Q240C6.",no8785,LOFT: A Latency-Oriented Fault Tolerant Transport Protocol for Wireless Sensor-Actuator Networks,"Wireless sensor-actuator networks, or WSANs, refer to a group of sensors and actuators which collect data from the environment and perform application-specific actions in response. To act responsively and accurately, an efficient and reliable data transport protocol is crucial for the sensors to inform the actuators about the environmental events. Unfortunately, the low-power multi-hop communications in WSANs are inherently unreliable; the frequent sensor and link failures as well as the excessive delays due to congestion further aggravate the problem. In this paper, we propose a latency-oriented fault tolerant data transport protocol in WSANs. We argue that reliable data transport in such a real-time system should resist to the transmission failures, and should also consider the importance and freshness of the reported data. We articulate this argument and provide a cross-layer two-step data transport protocol for on- time and fault tolerant data delivery from sensors to actuators. Our protocol adopts smart priority scheduling that differentiates the event data of non-uniform importance. It balances the workload of sensors by checking their queue utilization and copes with node and link failures by an adaptive replication algorithm. We evaluate our protocol through extensive simulations, and the results demonstrate that it achieves the desirable reliability for WSANs.",no8784,Self-Managed Fault Management in Wireless Sensor Networks,"Wireless sensor networks usually deploy in the harsh operational environment where the physical presence of human administrators is impractical. Applications and systems of these networks are thus expected to operate with the minimum aid or supervision. Biologically-inspired behaviors, such as self-healing, self-adaptation, have already been recognized as the desirable features for these systems to self-adapt to various unpredicted changes occurred in the environment. In this paper, we address such biological feature in terms of fault management. We propose a hierarchical structure to properly distribute fault management tasks among sensor nodes by introducing more dasiaself-managingpsila functions. In addition, we also consider an alternate solution to self-reconfigure fault management function of sensor nodes adapting to various system requirements, such as replacement of a faulty node.",no8783,Application of error control codes (ECC) in ultra-low power RF transceivers,"Wireless sensor networks provide the ability to gather and communicate critical environmental, industrial or security information to enable rapid responses to potential problems. The limited embedded battery life time requires ultra low power sensing, processing and communication systems. To achieve this goal, new approaches at the device, circuit, system and network level need to be pursued (Roundy et al., 2003). Adoption of error control codes (ECC) reduces the required transmit power for reliable communication, while increasing the processing energy of the encoding and decoding operations. This paper discusses the above trade off for systems with and without standard ECC, such as convolutional and Reed Solomon codes. The comparison of the required energy per bit, based on several implemented decoders, shows that the adoption of an ECC with simple decoding structures (such as Reed Solomon) is quite energy efficient. This has specially been observed for long distances.",no8782,Anshan: Wireless Sensor Networks for Equipment Fault Diagnosis in the Process Industry,"Wireless sensor networks provide an opportunity to enhance the current equipment diagnosis systems in the process industry, which have been based so far on wired networks. In this paper, we use our experience in the Anshan Iron and Steel Factory, China, as an example to present the issues from the real field of process industry, and our solutions. The challenges are three fold: First, very high reliability is required; second, energy consumption is constrained; and third, the environment is very challenging and constrained. To address these issues, it is necessary to put systematic efforts on network topology and node placement, network protocols, embedded software, and hardware. In this paper, we propose two technologies i.e. design for reliability and energy efficiency (DRE), and design for reconfiguration (DRC). Using these techniques we developed Anshan, a wireless sensor network for monitoring the temperature of rollers in a continuously annealing line and detecting equipment failures. Project Anshan includes 406 sensor nodes and has been running for four months continuously.",no8781,5B: emerging technologies - reliable and fault-tolerant wireless sensor networks,"Wireless sensor networks create invisible interconnections with the physical world for the measurement, monitoring, and management of data from multiple sensors and probes with little constraint on location. These networks provide distributed processing, data storage, wireless communication, and dedicated application software with high reliability, inherent redundancy, failure-tolerant security and easily encrypted privacy. They have enormous potential to transform our society and are subjects of intense current research and application development. Three enabling hardware technologies which constitute a network node are microprocessors, MEMS sensors, and low-power radios. Sensor networks represent the paradigm shift in computing where they anticipate our needs and sometimes act on our behalf. The objective of this presentation is to discuss the reliable and fault-tolerant wireless sensor networks, focusing on environmental, behavioral, and biomedical areas. Special focus will be on wearable monitors and body wireless sensor network. An example of physiological monitoring by body area network will be discussed.",no8780,A Survey on Fault Tolerant Routing Techniques in Wireless Sensor Networks,"Wireless sensor networks are without a doubt one of the central issues in current research topics due to the harsh environmental conditions in which such networks can be deployed and their unique sensor network characteristics,specifically limited power supply, sensing, processing and communication capabilities. Presented with many challenges and design issues that affect the data routing, a need for a fault tolerant routing protocol becomes essential. In this paper, we summarize and highlight the key ideas of existing fault tolerant techniques of routing protocols, survey existing routing protocols proposed to support fault tolerance. Finally, we provide some future research directions in the area of fault tolerance in wireless sensor networks routing.",no8779,Decentralized Fault Detection and Management for Wireless Sensor Networks,"Wireless Sensor Networks are increasingly being deployed in long-lived, challenging application scenarios which demand a high level of availability and reliability. To achieve these characteristics in inherently unreliable and resource constrained sensor network environments, fault tolerance is required. This paper presents a generic and efficient fault tolerance algorithm for Wireless Sensor Networks. In contrast to existing approaches, the algorithm presented in this paper is entirely decentralized and can thus be used to support fully autonomic fault tolerance in sensor network environments.",no8778,Neural fault isolator for Wireless Sensor Networks,"Wireless sensor networks are emerging as an innovative technology that can help to improve business processes. In such environments malfunctions and break-down states must be efficiently diagnosed to reduce to a minimum the economic losses. In this paper we present a fault isolation approach based on neural networks, which utilizes only a minimum set of information such as the sensor value, node ID and timestamp as inputs. We believe that this information set could be provided by any WSN regardless of its specific implementation. This abstraction makes the fault isolator generically applicable in enterprise business systems. The neural fault isolator was evaluated in a trial with 36 nodes and has proved to be highly efficient in the isolation of failed components.",no8777,Online drift correction in wireless sensor networks using spatio-temporal modeling,"Wireless sensor networks are deployed for the purpose of sensing and monitoring an area of interest. Sensors in the sensor network can suffer from both random and systematic bias problems. Even when the sensors are properly calibrated at the time of their deployment, they develop drift in their readings leading to erroneous inferences being made by the network. The drift in this context is defined as a slow, unidirectional, long-term change in the sensor measurements. In this paper we present a novel algorithm for detecting and correcting sensors drifts by utilising the spatio-temporal correlation between neigbouring sensors. Based on the assumption that neighbouring sensors have correlated measurements and that the instantiation of drift in a sensor is uncorrelated with other sensors, each sensor runs a support vector regression algorithm on its neigbourspsila corrected readings to obtain a predicted value for its measurements. It then uses this predicted data to self-assess its measurement and detect and correct its drift using a Kalman filter. The algorithm is run recursively and is totally decentralized. We demonstrate using real data obtained from the Intel Berkeley Laboratory that our algorithm successfully suppresses drifts developed in sensors and thereby prolongs the effective lifetime of the network.",no8776,A High Energy Efficiency Link Layer Adaptive Error Control Mechanism for Wireless Sensor Networks,"Wireless sensor networks (WSNs) require simple and facile error control schemes because of the low complexity and high energy efficiency request of sensor nodes. In this paper, we discuss ARQ, FEC, and Chase combing hybrid ARQ (HARQ) schemes using energy efficiency analysis on different communication distances and link layer frame lengths. We proposed a high energy efficiency adaptive error control mechanism (AEC-RSSI). Our mathematical analysis shows that the AEC-RSSI mechanism achieves better performance of data transmission comparing with FEC, ARQ and HARQ, in terms of the overall energy efficiency of the communications in a WSN.",no8775,Fault management in wireless sensor networks,"Wireless sensor networks (WSNs) have gradually emerged as one of the key growth areas for pervasive computing in the twenty-first century. Recent advances in WSN technologies have made possible the development of new wireless monitoring and environmental control applications. However, the nature of these applications and harsh environments also created significant challenges for sensor networks to maintain a high quality of service in potentially harsh environments. Therefore, efficient fault management and robust management architectures have become essential for WSNs. In this article, we address these challenges by surveying existing fault management approaches for WSNs. We divide the fault management process into three phases: fault detection, diagnosis, and recovery and classify existing approaches according to these phases. Finally, we outline future challenges for fault management in WSNs.",no8774,Cluster-Based Error Messages Detecting and Processing for Wireless Sensor Networks,"Wireless sensor networks (WSNs) have emerged as a new technology about acquiring and processing messages for a variety of applications. Faults occurring to sensor nodes are common due to lack of power or environmental interference. In order to guarantee the network reliability of service, it is necessary for the WSN to be able to detect and processes the faults and take appropriate actions. In this paper, we propose a novel approach to distinguish and filter the error messages for cluter-based WSNs. The simulation results show that the proposed method not only can avoid frequent re-clustering but also can save the energy of sensor nodes, thus prolong the lifetime of sensor network.",no8773,Fault Aware Wireless Sensor Networks,"Wireless Sensor Networks (WSNs) collect information about the physical environment aiding to a wide variety of applications ranging from target detection to monitoring of harmful chemical gases. The drive to scale down the system size and cost has resulted in constraints in the quality of components. Reliable and accurate performance of sensors is necessary in critical applications. In this paper, we present statistical data analysis and signal processing techniques at the sensor node level to detect sensor faults and to eliminate noise. We also present the simulation of the proposed algorithm using real sensor data and demonstrate that the algorithm can distinguish between sensor faults and environmental events. Furthermore, we describe the real-time implementation of the developed algorithm. The information regarding the faulty sensor is broadcast to all nodes and the central processing base station node thereby achieving autonomous node level operation and a complete fault aware system.",no8772,A Mobile Agent-Based Architecture for Fault Tolerance in Wireless Sensor Networks,"Wireless Sensor Networks (WSNs) are prone to failures as they are usually deployed in remote and unattended environments. To mitigate the effect of these failures, fault tolerance becomes imperative. Nonetheless, it remains to be a second tier activityit should not undermine the execution of the mission oriented tasks of WSNs through overly taxing their resources. We define architecture for fault tolerance in WSNs that is based on a federation of mobile agents that is used both for diagnostic intelligence and repair regimen, focusing on being lightweight in energy, communication and resources. Mobile agents are classified here as local, metropolitan, and global, providing fault tolerance at node, network and functional levels. Interactions between mobile agents are inspired by honey bee dance language that builds on semantics of errors classification and their demographic distribution. Our quantitative modeling substantiates that the proposed fault tolerance framework mandates minimalist communication through contextualized bee-inspired interactions, achieving adaptive sensitivity, and hysteresis-based stability",no8771,Distributed Intermittent Fault Diagnosis in Wireless Sensor Networks Using Clustering,"Wireless sensor networks (WSNs) are an important tool for monitoring distributed remote environments. Faults occurring to sensor nodes are common due to the sensor device itself and the harsh environment where the sensor nodes are deployed. It is well known that the distributed fault detection (DFD)scheme checks out the failed nodes by exchanging data and mutually testing among neighbor nodes in this network. But the fault detection accuracy of aDFD scheme would decrease rapidly when the number of neighbor nodes to be diagnosed is small and the node's failure ratio is high. In this paper, aDFD scheme using clustering is proposed which satisfies three important diagnosis properties such as consistency, completeness and accuracy. These properties have been also proved. Simulation results demonstrate that the proposed DFD scheme increases the fault detection accuracy in comparison with a DFD scheme without clustering.",no8770,Improving Fault Management Using Voting Mechanism in Wireless Sensor Networks,Wireless Sensor Networks (WSN) have the potential of significantly enhancing our ability to monitor and interact with our physical environment. Realizing a fault management operation is critical to the success of WSN. The main challenge is providing fault-tolerance (FT) while conserving the limited resources of the network. In this work we propose a new method for fault management that do the fault detection and fault recovery in decentralized. Simulation result shows that proposed method have better performance.,no8769,Fuzzy data fusion for fault detection in Wireless Sensor Networks,"Wireless Sensor Networks (WSN) can produce decisions that are unreliable due to the large inherent uncertainties in the areas which they are deployed. It is vital for the applications where WSN's are deployed that accurate decisions can be made from the data produced. Fault detection is a vital pursuit, however it is a challenging task. In this paper we present a fuzzy logic data fusion approach to fault detection within a Wireless Sensor Network using a Statistical Process Control and a clustered covariance method. Through the use of a fuzzy logic data fusion approach we have introduced a novel technique into this area to reduce uncertainty and false-positives within the fault detection process.",no8768,Low Cost Differential GPS Receivers (LCD-GPS): The Differential Correction Function,"Wireless Sensor Networks (WSN) are used in many applications such as environmental data collection, smart home, smart care and intelligent transportation system. Sensor nodes composing the WSN cooperate together in order to monitor physical entities such as temperature, humidity, sound, atmospheric pressure, motion or pollutants at different locations. To have location information, it is possible to configure nodes with their locations, in small deployments, but in large-scale deployments or when the nodes are mobile, the use of GPS is very interesting. However, the current accuracy of standard civil GPS is not sufficient for all WSN applications. Indeed, GPS measurements suffer from many errors especially in city. To improve GPS accuracy the differential mode (DGPS) has been introduced. In this paper, we present a WSN used to provide a DGPS solution. It's consisting of a set of low cost standard civil GPS communicating receivers. We present the design, implementation and some experimental results of this solution.",no8767,The Application of Wireless Sensor Networks in Machinery Fault Diagnosis,"Wireless sensor network is a thriving information collecting and processing technology, which is widely used in military field, industry and environmental monitoring, etc. In a wireless sensor network which is made up of tens of thousands battery-powered sensor nodes, data fusion technology can be used to reduce communication traffic in order to save energy. With respect to large mechanical equipment, traditional wired sensors are commonly used for fault detection and diagnosis. There will be no wiring problem if wireless sensor networks are used, which is favorable to detect potential problems in mechanical equipment without affecting normal production of enterprises. In this paper, the application of wireless sensor networks in machinery fault diagnosis is studied, a data fusion model for machinery fault diagnosis in wireless sensor networks and PCA neural data fusion algorithm are proposed, and the effectiveness of the method is demonstrated in an experiment.",no8766,A Causal Model Method for Fault Diagnosis in Wireless Sensor Networks,"Wireless sensor network are composed of many wireless sensing devices namely sensor nodes, they are small in size, limited in resources and randomly deployed in harsh environment. Therefore, it is not uncommon for sensor networks to have malfunction behaviour, power shortage or network failure. To address this issue, we proposed a new scheme based on Causal Model Method (CMM) which applies fault sources analyzer for component-level fault diagnosis in wireless sensor networks. Our new method consists of three phases to define the node failure sources as collect, classify, and correct. Once the fault source has been classified, CMM mechanism will enable reconfigure process to compensate for the erroneous sensor nodes impact. We have conducted a simulation study to our work using the Georgia Tech Network Simulator (GTNetS). Our simulation results show that CMM can improve wireless senor networks performance and reliability.",no8765,Performance analysis of distributed intermittent fault diagnosis in wireless sensor networks using clustering,"Wireless sensor network (WSN) has become monitoring solution of variety of applications. As one of the key technologies involved in WSNs, node fault detection is indispensable in most WSN applications. Faults occurring to sensor nodes are common due to the sensor device itself and the harsh environment where the sensor nodes are deployed. The goal of this paper is to locate the faulty sensors in the wireless sensor network. In this paper, a distributed fault diagnosis (DFD) scheme using clustering is proposed which satisfies three important diagnosis properties such as consistency, completeness and accuracy. To evaluate the performance of the proposed algorithm a comparative analysis and implementation is presented in this paper using an existing diagnosis algorithm i.e. distributed fault diagnosis algorithm (DFD).",no8764,Sentomist: Unveiling Transient Sensor Network Bugs via Symptom Mining,"Wireless Sensor Network (WSN) applications are typically event-driven. While the source codes of these applications may look simple, they are executed with a complicated concurrency model, which frequently introduces software bugs, in particular, transient bugs. Such buggy logics may only be triggered by some occasionally interleaved events that bear implicit dependency, but can lead to fatal system failures. Unfortunately, these deeply-hidden bugs or even their symptoms can hardly be identified by state-of-the-art debugging tools, and manual identification from massive running traces can be prohibitively expensive. In this paper, we present Sentomist (Sensor application anatomist), a novel tool for identifying potential transient bugs in WSN applications. The Sentomist design is based on a key observation that transient bugs make the behaviors of a WSN system deviate from the normal, and thus outliers (i.e., abnormal behaviors) are good indicators of potential bugs. Sentomist introduces the notion of event-handling interval to systematically anatomize the long-term execution history of an event-driven WSN system into groups of intervals. It then applies a customized outlier detection algorithm to quickly identify and rank abnormal intervals. This dramatically reduces the human efforts of inspection (otherwise, we have to manually check tremendous data samples, typically with brute force inspection) and thus greatly speeds up debugging. We have implemented Sentomist based on the concurrency model of TinyOS. We apply Sentomist to test a series of representative real-life WSN applications that contain transient bugs. These bugs, though caused by complicated interactions that can hardly be predicted during the programming stage, are successfully confined by Sentomist.",no8763,A Comparative Study of Voice Over Wireless Networks Using NS-2 Simulation with an Integrated Error Model,"Wireless communication is the fastest growing field and with the emergence of IEEE 802.11 based devices, wireless access is becoming more popular. Many multimedia applications for IP networks have been developed and thus the demand for quality of service (QoS) has increased. In this paper our primary objective is to evaluate 802.11e EDCF framework for video, voice and background traffic all at the same time. Our assessment is based on an error model called E-model, MOS for VoIP and PSNR for video. We also studied the effects of random uniform error model on various types of traffic. As expected, wireless networks are more prone to errors than wired networks",no8762,Robust and extreme unequal error protection scheme for the transmission of scalable data over OFDM systems,Wireless applications are subject to the end-to-end quality of service (QoS) requirements. This paper presents a new resources allocation algorithm that allows to transmit scalable multimedia data over a frequency selective channel with partial channel knowledge. The available resources are subject to payload and QoS constraints and the algorithm aims at maximizing the transmission robustness to channel estimation errors. The impact of this technique is evaluated for a MPEG-4 audio application.,no8761,Propagation model for estimating vor bearing error in the presence of windturbines  Hybridation of parabolic equation with physical optics,"Windturbines near VOR ground station can yield significant bearing errors in the azimuth estimation. We propose a model that combines the parabolic equation and the physical optics approximation to predict these errors. It accounts for a possible hilly terrain, and a generic model of windturbines that includes dielectric blades. All the hypotheses made in the model are carefully justified by means of numerical simulations. In a realistic test case, this model is employed to compute the error caused by a complete windfarm located on a hilly terrain within acceptable computation time.",no8760,Study of fault ride-through for DFIG based wind turbines,"Wind power generator and total capacity has increased dramatically in the last 10 years. Most generators now being installed use doubly-fed induction machines (DFIGs). These allow active and reactive power control through the rotor side converter. Therefore today's wind turbines have a significant impact on the power system. To ensure power quality, several utilities have introduced special grid connection codes for wind farm developers. The requirements range from reactive power control, frequency response and, last but not least, fault ride-through. All these requirements, especially fault ride-through, are a challenge for wind turbine producers. New control strategies and hardware are needed which utilize the flexibility provided by the DFIG converters. This paper outlines the proposed grid codes. It then describes a detailed DFIG model and control strategy. The performance of the new control through severe fault conditions is demonstrated.",no8759,Simultaneous optimization for wind derivatives based on prediction errors,"Wind power energy has been paid much attention recently for various reasons, and the production of electricity with wind energy has been increasing rapidly for a few decades. In this work, we will propose a new type of weather derivatives based on the prediction errors for wind speeds, and estimate their hedge effect on wind power energy businesses. At first, we will investigate the correlation of prediction errors between the power output and the wind speed in a Japanese wind farm. Then we will develop a methodology that will optimally construct a wind derivative based on the prediction errors using nonparametric regressions. A simultaneous optimization technique of the loss and payoff functions for wind derivatives is demonstrated based on the empirical data.",no8758,Based on CAN bus wind generating set online monitoring and fault diagnosis system,"Wind generating set online monitoring and fault diagnosis system, using CAN bus, combines by sensor system, scene gathering and processing, monitoring system, analysis and diagnosis system, network system, etc.. This system combines signal processing, artificial intelligence, communications, DSP, databases, computer networks and so on, decomposes a complete online monitor and diagnosis duty to each computer on different level, coordinates mutually, completes the monitor task together; and the online way integrated experts wisdom of diagnostics, has realized the long-distance monitor and the diagnosis, therefore enterprises may register the correlation the long-distance diagnostic center Website to carry on the analysis diagnosis directly. Practice has proved that this system integrated data collection, performance analysis, fault diagnosis, artificial intelligence technologies in one integrated information system, realized running status monitor and the breakdown diagnosis to the wind generating set. The system has a certain practicality and effectiveness.",no8757,The solution of the ground fault using GOOSE message in the wind farm system,"Wind farm systems presented some unique challenges for protection. Ground faults on feeders will result in unfaulted phase voltages rising to line levels and transient over voltages can be produced, which can degrade insulation, resulting in eventual equipment failure. This paper focused on the particular problem of feeder ground faults. A novel, yet simple solution is presented that makes use of a peer-to-peer fast message - GOOSE message. It designed and implemented information and communication model of the Wind-Turbine-Generator (WTG) IED. And it introduced the timing sequence for a feeder fault using the transfer trip solution. Finally GOOSE message is designed to transfer trip information. By Simulation test of GOOSE message and analyzing sequence diagram of IED's interaction for transfer trip, we know that wind turbine equipment can be avoided damage using GOOSE message, which is a good solution of the ground fault.",no8756,Fisheye lens distortion correction on multicore and hardware accelerator platforms,"Wide-angle (fisheye) lenses are often used in virtual reality and computer vision applications to widen the field of view of conventional cameras. Those lenses, however, distort images. For most real-world applications the video stream needs to be transformed, at real-time (20 frames/sec or better), back to the natural-looking, central perspective space. This paper presents the implementation, optimization and characterization of a fisheye lens distortion correction application on three platforms: a conventional, homogeneous multicore processor by Intel, a heterogeneous multicore (Cell BE), and an FPGA implementing an automatically generated streaming accelerator. We evaluate the interaction of the application with those architectures using both high- and low-level performance metrics. In macroscopic terms, we find that todays mainstream conventional multicores are not effective in supporting real-time distortion correction, at least not with the currently commercially available core counts. Architectures, such as the Cell BE and FPGAs, offer the necessary computational power and scalability, at the expense of significantly higher development effort. Among these three platforms, only the FPGA and a fully optimized version of the code running on the Cell processor can provide realtime processing speed. In general, FPGAs meet the expectations of performance, flexibility, and low overhead. General purpose multicores are, on the other hand, much easier to program.",no8755,Why automation needs error avoidance guidelines and evaluation?,"Why do we need automation? Many technologies cite three major reasons: to eliminate the dull, the dangerous, and the dirty routines. It is difficult to argue with this answer, but many things are automated for other reasons - to simplify a complex task, to reduce the work force, to entertain - or simply because it can be done. However, none of the above matters relate to the findings of this paper, whereby automation leads to humans making mistakes.",no8754,Architecting Fault Tolerant Systems,"While typical solutions focus on fault tolerance (and specifically, exception handling) during the design and implementation phases of the software life-cycle (e.g., Java and Windows NT exception handling), more recently the need for explicit exception handling solutions during the entire life cycle has been advocated by some researchers. Several solutions have been proposed for fault tolerance via exception handling at the software architecture and component levels. This paper describes how the two concepts of fault tolerance and software architectures have been integrated so far. It is structured in two parts (overview on fault tolerance and exception handling, and integrating fault tolerance into software architecture) and is based on a survey study on architecting fault tolerant systems where more than fifteen approaches have been analyzed and classified. This paper concludes by identifying those issues that remain still open and require deeper investigation.",no8753,Consensus-based fault-tolerant total order multicast,"While total order broadcast (or atomic broadcast) primitives have received a lot of attention, this paper concentrates on total order multicast to multiple groups in the context of asynchronous distributed systems in which processes may suffer crash failures. Multicast to Multiple Groups means that each message is sent to a subset of the process groups composing the system, distinct messages possibly having distinct destination groups. Total Order means that all message deliveries must be totally ordered. This paper investigates a consensus-based approach to solve this problem and proposes a corresponding protocol to implement this multicast primitive. This protocol is based on two underlying building blocks, namely, uniform reliable multicast and uniform consensus. Its design characteristics lie in the two following properties. The first one is a minimality property, more precisely, only the sender of a message and processes of its destination groups have to participate in the total order multicast of the message. The second property is a locality property: No execution of a consensus has to involve processes belonging to distinct groups (i.e., consensus is executed on a per group basis). This locality property is particularly useful when one is interested in using the total order multicast primitive in large-scale distributed systems. In addition to a correctness proof, an improvement that reduces the cost of the protocol is also suggested",no8752,On the Use of Bloom Filters for Defect Maps in Nanocomputing,"While the exact manufacturing process for nanoscale computing devices is uncertain, it is abundantly clear that future technology nodes will see an increase in defect rates. Therefore, it is of paramount importance to construct new architectures and design methodologies that can tolerate large numbers of defects. Defect maps are a necessity in the future design flows, and research on their practical construction is essential. In this work, we study the use of Bloom filters as a data structure for defect maps. We show that Bloom filters provide the right tradeoff between accuracy and space-efficiency. In particular, they can help simplify the nanosystem design flow by embedding defect information within the nanosystem delivered by the manufacturers. We develop a novel nanoscale memory design that uses this concept. It does not rely on a voting strategy, and utilizes the device redundancy more effectively than existing approaches",no8751,Speeding up Fault Injection for Asynchronous Logic by FPGA-Based Emulation,"While stability and robustness of synchronous circuits becomes increasingly problematic due to shrinking feature sizes, delay-insensitive asynchronous circuits are supposed to provide inherent protection against various fault types. However, results on experimental evaluation and analysis of these fault tolerance properties are scarce, mainly due to the lack of suitable prototyping platforms. Using a soft-core processor as an example, this paper shows how an off-the-shelf FPGA can be used for asynchronous four state logic designs, on which future fault injection experiments will be conducted.",no8750,DMTracker: finding bugs in large-scale parallel programs by detecting anomaly in data movements,"While software reliability in large-scale systems becomes increasingly important, debugging in large-scale parallel systems remains a daunting task. This paper proposes an innovative technique to find hard-to-detect software bugs that can cause severe problems such as data corruptions and deadlocks in parallel programs automatically via detecting their abnormal behaviors in data movements. Based on the observation that data movements in parallel programs typically follow certain patterns, our idea is to extract data movement (DM)-based invariants at program runtime and check the violations of these invariants. These violations indicate potential bugs such as data races and memory corruption bugs that manifest themselves in data movements. We have built a tool, called DMTracker, based on the above idea: automatically extract DM-based invariants and detect the violations of them. Our experiments with two real-world bug cases in MVAPICH/MVAPICH2, a popular MPI library, have shown that DMTracker can effectively detect them and report abnormal data movements to help programmers quickly diagnose the root causes of bugs. In addition, DMTracker incurs very low runtime overhead, from 0.9% to 6.0%, in our experiments with High Performance Linpack (HPL) and NAS Parallel Benchmarks (NPB), which indicates that DMTracker can be deployed in production runs.",no8749,Exploring the Maintenance Process through the Defect Management in the Open Source Projects - Four Case Studies,"While Open Source Software are becoming evermore widespread and used these days, their maintenance is coming important issue. Earlier studies have shown that defect and version management systems are rich and valuable sources for evaluation of maintenance but they have not studied the use of separate management system for support and feature request. Therefore, in this research we study defect reports, support and feature requests of Open Source Software projects through four case studies from SourceForge. Results showed that most of the case studies used actively those systems but discussion forums were even more active. Although reports and requests were submitted, most of them did not cause any changes or further actions because they were closed shortly as duplicates, invalid or without any resolution.",no8748,A novel stuck-at based method for transistor stuck-open fault diagnosis,"While most of the fault diagnosis tools are based on gate level fault models, for instance the stuck-at model, many faults are actually at the transistor level. The stuck-open fault is one example. In this paper we introduce a method which extends the use of available gate level stuck-at fault diagnosis tools to stuck-open fault diagnosis. The method transforms the transistor level circuit description to a gate level description where stuck-open faults are represented by stuck-at faults, so that the stuck-open faults can be diagnosed directly by any of the stuck-at fault diagnosis tools. The transformation is only performed on selected gates and thus has little extra computational cost. This method also applies to the diagnosis of multiple stuck-open faults within a gate. Successful diagnosis results are presented using wafer test data and an internal diagnosis tool from Philips",no8747,Zapmem: A Framework for Testing the Effect of Memory Corruption Errors on Operating System Kernel Reliability,"While monolithic operating system kernels are composed of many subsystems, during runtime they all share a common address space, making fault propagation a serious issue. The code quality of each subsystem is different, as OS development is a complex task commonly divided by different groups with different degrees of expertise. Since the memory space into which this code runs is shared, the occurrence of bugs or errors in one of the subsystems may propagate to others and affect general OS reliability. It is necessary, then, to test how errors propagate between the different kernel subsystems and how they affect reliability. This work presents a simple new technique to inject memory corruption faults and Zapmem, a fault injection tool which uses such technique to test the effect on reliability from memory corruption of statically allocated kernel data. Zapmem associates the runtime memory addresses to the corresponding high level (source code) memory structure definitions, which indicate which kernel subsystem allocated that memory region, and the tool has minimal intrusiveness, as our technique does not require kernel instrumentation. The efficacy of our approach and preliminary results are also presented.",no8746,A simulation technique for the evaluation of random error effects in time-domain measurement systems,"While many papers deal with time-domain network analyzer calibration procedures for the correction of systematic errors, little work has been published about the treatment of random errors. This paper is focused on the evaluation of random error effects in time-domain measurement systems. As a first step, an experimental identification of the measurement system random errors is achieved. Random errors addressed are jitter, vertical noise, and fast time drifts. Based on this identification, mathematical models are developed to simulate random errors. At a second step, time-domain measurements are simulated with these random errors. These simulations are used to predict measurement system repeatability and dynamic range. Then, as an application example, simulations of the measurement of the complex propagation coefficient and S parameters of a lossy mismatched microstrip line are achieved. By comparison with real measurements, it is shown that random error effects can be accurately predicted by Monte Carlo simulations",no8745,A learning-based approach for fault tolerance on grid resources scheduling,"While Grid environment has developed increasingly, unfortunately the importance of fault tolerance has not been remarkable in Grid resource management. On the other hand, the cost of computing by grid is important because grid is an economy-based system. Most organizations intend to spend little on their own computations by grid. Therefore, using a better approach to resource scheduling to avoid fault is necessary. This paper presents a new approach on fault tolerance mechanisms for the resource scheduling on grid by using Case-Based Reasoning technique in a local fashion. This approach applies a specific structure in order to prepare fault tolerance between executer nodes to retain system in a safe state with minimum data transferring. Certainly, this algorithm increases fault tolerant confidence therefore, performance of grid will be high.",no8744,Geometric robust watermarking based on a new mesh model correction approach,"While geometric attacks are one of the most challenging problems in watermarking, random bending is probably the most difficult to handle among all geometric attacks. We present a watermarking scheme based on a new deformable mesh model to combat such attacks. The distortion is corrected using the distortion field (DF) estimated by minimizing the matching error between the meshes of the original and the attacked image. A CDMA watermarking method is used for testing the proposed method, which embeds a multi-bit signature in the DCT domain and uses mesh model correction to achieve robustness. Experiments show that the proposed scheme can survive a wide range of random bending attacks.",no8743,Fault-tolerant static scheduling for grids,"While fault-tolerance is desirable for grid applications because of the distributed and dynamic nature of grid resources, it has seldom been considered in static scheduling. We present a fault-tolerant static scheduler for grid applications that uses task duplication and combines the advantages of static scheduling, namely no overhead for the fault-free case, and of dynamic scheduling, namely low overhead in case of a fault. We also give preliminary experimental results on our scheme.",no8742,All digital ADC with linearity correction and temperature compensation,"While digital circuits benefit from high-density digital CMOS technology, the design of analog and mixed signal blocks in the same technology is a higher challenge at each new technology node. AD converters are an example of such blocks. A possible solution is the implementation of ADCs in digital technology using logic gates as a voltage controlled oscillator. However, the limited linearity and temperature sensitivity are known issues. In this paper, a linearity correction technique that is also able to compensate for temperature effects is used. Results indicate the feasibility of the approach.",no8741,Perceptually optimized error resilient transcoding using attention-based intra refresh,"While deployment of wireless channels has become widespread and fast-growing for mobile applications, transmitting data over these existing error-prone networks can be very unreliable and challenging due to time-varying interference and channel errors. Many error-resilient algorithms have been proposed to provide adequate resilient features in order to protect video data from channel errors. However, these algorithms often aim to achieve the optimal decoded video quality in terms of mean square error without any consideration for the visual quality. In this paper, we present a perceptually error-resilient method for video transcoding based on the attention-based intra refresh technique and the characteristics of the human visual system to enhance the perceptual performance of the transcoded video. Specifically, the foveated just noticeable distortion and visual attention models are employed to estimate the perceptual loss impact due to error propagation for allocating intra-refreshed macroblocks in the transcoded video. Experimental results show that the proposed method can achieve a much better performance than the existing methods in terms of both the visual quality and perceptual quality measure.",no8740,System RAS implications of DRAM soft errors,"While attention in the realm of computer design has shifted away from the classic DRAM soft-error rate (SER) and focused instead on SRAM and microprocessor latch sensitivities as sources of potential errors, DRAM SER nonetheless remains a challenging problem. This is true even though both cosmic ray-induced and alpha-particle-induced DRAM soft errors have been well modeled and, to a certain degree, well understood. However, the often-overlooked alignment of a DRAM hard error and a random soft error can have major reliability, availability, and serviceability (RAS) implications for systems that require an extremely long mean time between failures. The net of this effect is that what appears to be a well-behaved, single-bit soft error ends up overwhelming a seemingly state-of-the-art mitigation technique. This paper describes some of the history of DRAM soft-error discovery and the subsequent development of mitigation strategies. It then examines some architectural considerations that can exacerbate the effect of DRAM soft errors and may have system-level implications for today's standard fault-tolerance schemes.",no8739,Predicting defects in SAP Java code: An experience report,"Which components of a large software system are the most defect-prone? In a study on a large SAP Java system, we evaluated and compared a number of defect predictors, based on code features such as complexity metrics, static error detectors, change frequency, or component imports, thus replicating a number of earlier case studies in an industrial context. We found the overall predictive power to be lower than expected; still, the resulting regression models successfully predicted 50-60% of the 20% most defect-prone components.",yes8738,Error-controlled computation for termination of programs,"Whether a program can terminate or not has direct impact on software safety. As false results can occur due to calculation errors on floating point numbers, the terminability can be false given a loop program and any initial value on R<sup>n</sup>. In this paper, a recursive algorithm is suggested for calculating the values of arithmetic expressions to arbitrary precision. Using the error-controlled computation method (ECC), we can determine the initial value is a terminating point or not.",no8737,Self-Calibration Algorithm for the Amplitude and Phase Error of the Multiple Beam Antenna,"When we use the multiple beam antenna to estimate the direction of arrival (DOA) by using the multiple signal classification (MUSIC) algorithm, the uncertainty element's amplitude and phase will effect the DOA estimation performance of the beam antenna. So a universal technique for self-calibration based on the minimize cost function is introduced. By minimize the cost function it can get the array covariance matrix and compensate the array uncertainty without estimation. Therefore, the proposed method is advantage over the source calibration algorithm that resorts to computing the sample minimize cost function. The performance of the proposed method is demonstrated by using the actual parameter of multiple beam antenna, and the computer simulations show that the proposed method provide comparable performance that can not only self-calibrate its sensitivity to the array uncertainty with reduce complexity compute with resource, but also further increase its precision.",no8736,Red-eye detection and correction using inpainting in digital photographs,"When we take pictures with flash, red-eye effect often appears in photographs. Flash light passing through pupil is reflected on the blood vessels, and arrives at a camera lens. This phenomenon makes red-eyes in photographs. Several algorithms have been proposed for removal of red-eyes in digital photographs. This paper proposes a red-eye removal algorithm using inpainting and eye-metric information, which is largely composed of two parts: red-eye detection and red-eye correction. For red-eye detection, face regions are detected first. Next, red-eye regions are segmented in the face regions using multi-cues such as redness, shape, and color information. By region growing, we select regions, which are to be completed with iris texture by an exemplar-based inpainting method. Then, for red-eye correction, pupils are painted with the appropriate radii calculated from the iris size and size ratio. Experimental results with a large number of test photographs with red-eye effect show that the proposed algorithm is effective and the corrected eyes look more natural than those processed by the conventional algorithms.",no8735,Efficient diagnosis for multiple intermittent scan chain hold-time faults,"When VLSI design and process enter the stage of ultra deep submicron (UDSM), process variations, signal integrity (SI) and design integrity (DI) issues can no longer be ignored. These factors introduce some new problems in VLSI design, test and diagnosis, which increase lime-to-market, time-to-volume and cost for silicon debug. Intermittent scan chain hold-time fault is one of such problems we encountered in practice. The fault sites have to be located to speedup silicon debug and improve yield. Recent study of the problem proposed a statistical algorithm to diagnose the faulty scan chains if only one fault per chain. Based on the previous work, in this paper, an efficient diagnosis algorithm is proposed to diagnose faulty scan chains with multiple faults per chain. The presented experimental results on industrial designs show that the proposed algorithm achieves good diagnosis resolution in reasonable time.",no8734,The GPS Contribution to the Error Budget of Surface Elevations Derived From Airborne LIDAR,"When using airborne LIDAR to produce digital elevation models, the global positioning system (GPS) positioning of the LIDAR instrument is often the limiting factor, with accuracies typically quoted as being 10-30 cm. However, a comprehensive analysis of the accuracy and precision of GPS positioning of aircraft over large temporal and spatial scales is lacking from the literature. Here, an assessment is made of the likely GPS contribution to the airborne LIDAR measurement error budget by analyzing more than 500 days of continuous GPS data over a range of baseline lengths (3-960 km) and elevation differences (400-2000 m). Height errors corresponding to the 95th percentile are <0.15 m when using algorithms commonly applied in commercial software over 3-km baselines. These errors increase to 0.25 m at 45 km and <0.5 m at 250 km. At aircraft altitudes, relative heights are shown to be potentially biased by additional errors approaching 0.2 m, partly due to unmodeled tropospheric zenith total delay (ZTD). The application of advanced algorithms, including parameterization of the residual ZTD, gives error budgets that are largely constant despite baseline length and elevation differences. In this case, height errors corresponding to the 95th percentile are <0.22 m out to 960 km, and similar levels are shown for one randomly chosen day over a 2300-km baseline.",no8733,Fault detection and isolation in cooperative manipulators via artificial neural networks,"When two or more robotic manipulators are working cooperatively, faults can put at risk the task, the robots, or the manipulated load. In this work, two artificial neural networks are employed in a fault detection and isolation system for cooperative robotic manipulators. A multilayer perceptron is utilized to reproduce the dynamics of the cooperative system The difference between its outputs and the actual velocity measurements generates the residual vector. This vector is classified by a radial basis function network that produces the fault information. Simulations with two robotic manipulators performing a cooperative task are presented, indicating that free-swinging joint faults are correctly detected and isolated. The main contribution of this work is the first application of fault detection and isolation to cooperative manipulators with faults at the robots' joints",no8732,Radiometric correction of RADARSAT-1 images for mapping the snow water equivalent (SWE) in a mountainous environment,"When trying to monitor the snow characteristics from RADARSAT-1 SAR data in a mountainous environment like the Coast Mountains (B.C. Canada), radiometric corrections must first be applied to correct for the distortions induced by the slant projection of SAR systems and by the highly variable terrain. This paper presents and discusses the results obtained from the implementation of two radiometric slope correction methods on Fine beam RADARSAT-1 images. For slope less than 30, both algorithms have almost the same effect. But, for very steep slopes, both algorithms are deficient and may not compensate enough.",no8731,Content-Adaptive Interpolation for Spatial Error Concealment,"When transmitting encoded images over a communication channel, the reconstructed image quality can be substantially degraded by channel errors. This paper presents a spatial error concealment algorithm that utilizes variance of surrounding pixels, and then classifies each error block (EB) into two categories: uniform block and edge block. For uniform block, nearest border prior spatial interpolation is adopted to restore missing pixels. We use the Wiener interpolation algorithm and special interpolation sequence for edge block. Experimental results indicate the proposed algorithm can attain well restored quality of intra-frames both subjectively and objectively. Meanwhile, the computational cost of the proposed algorithm can be significantly reduced, compared to Lipsilas method. And the restored quality is almost the same, sometimes even much better.",no8730,Fault-tolerant clock synchronization for embedded distributed multi-cluster systems,"When time-triggered (TT) systems are to be deployed for large embedded real-time (RT) control systems in cars and airplanes, one way to overcome bandwidth limitations and achieve complexity reduction is the organization in clusters of strongly interacting computing nodes with well-defined interfaces. In this case, clock synchronization of different cluster times supports meaningful exchange of time-related data between clusters and allows coordinated control. This paper addresses fault-tolerant clock synchronization of clusters for TT systems that are already internally synchronized. By addressing systematic and stochastic errors of cluster times differently, the influence of systematic errors is eliminated and the quality of synchronization only depends on stochastic errors. Since systematic errors of cluster times are at least an order of magnitude larger than stochastic errors for typical RT embedded control systems, the presented algorithm achieves a significant improvement to known synchronization algorithms. An implementation of the proposed clock synchronization algorithm on top of the Time-Triggered Architecture and experiments show that synchronization is achieved with accuracy values of less than one microsecond.",no8729,Application of expert system based on mixing reasoning in traction substation fault diagnosis,When there is a fault in traction substation the fault components and their reasons are required to be identified by operators quickly and accurately. A fault diagnosis expert system based on mixing reasoning is built in this paper. It can reason according to logical relations of relay protection and breakers and fault waveform of voltage and current provided by Fault Record System (FRS). Mistrip and failure to trip of relay protection and breakers can both be distinguished The reasoning result has a good credibility. Traction substation fault diagnosis expert system may be used as a module of integrated automation.,no8728,Multi-classifier fusion approach based on data clustering for analog circuits fault diagnosis,"When there are large amount of fault classes in analog circuits, normally single multi-class classifier cannot achieve satisfactory diagnosis accuracy because of its difficult training process. A method of multi-classifier fusion diagnosis approach based on data clustering is presented in this paper to improve fault diagnosis veracity. After extracting fault feature vectors by wavelet transform, fuzzy C-mean clustering algorithm is used to pre-partition the feature space into multiple sub-class groups as binary tree. According to the structure of the fault tree, multi-classifiers are created to form hierarchical diagnosis system. Simulation experiments demonstrate that the proposed approach for analog circuit fault diagnosis is superior to conventional ones. The fault diagnosis accuracy is greater than 98%. It has good performance in tackling large number of fault classes in analog circuits.",no8727,Petri nets and mobile agent composed fault unit lock scheme,"When the power system happened with the protection information lost or the incorrect operation, at present it still depends on the backup protection to isolate the fault with prolonged time and the extended tripped area. In this paper use the Petri nets as the synthetically analysis on the protection information tool and the mobile agent the distributional intelligence tool, finds a better solution to the above problem. The substation level central system carries on the integration process to both the distributional protection information and dynamic information along with the protection acting process, and then it dispatch the mobile agents to carry on the task download and logic calculation to the correlation distributed nodes. In case of the need for further information initiation, it activate the sequence trip to carry on the fault area dynamic lock, finally complete the fault isolation based on the smallest unit.",no8726,On the monitoring of the defects of squirrel cage induction motors,"When the electric motor is an important element in industrial process in terms of safety and efficiency, we must make an early detection. The earlier the incipient fault is detected the easier remediable faults will be cheaper. Monitoring is made through the spectral analysis of the stator current. Usually the fast Fourier transform is used. However its real achievement is waning in regards of on-line methods like the discrete Fourier transform or sliding Hartley transform. This technique is advisable in order to have an updated spectrum at each sampling time. This is the best method to show up the sidebands in the stator current when an incipient fault occurs meanwhile the motor is operating. Another way is to make a time-frequency analysis. In this case, we can discern the instant where the fault appears always through the spectral analysis of the stator current of the induction motor. Experimental results show the efficiency of the presented method.",no8725,Systematic error compensation for RPC model using semi-parametric estimation,"When the conventional method of parameter estimation is used to construct the RPC model, the uncertain factors can result in inconsistency between the RPC model and the reality. The inconsistency shows up as evident systematic error in the constructed RPC model. To tackle the problem, a non-parametric component is introduced on the base of the parametric model to account for the unknown factors and their effects. The new method, namely the semi-parametric estimation, could effectively compensate the effect of systematic errors. This paper studied the construction of the RPC model of remote sensing images using the semi-parametric estimation method. The experiment with SPOT-5 imagery demonstrated that the semi-parametric estimation method could improve the precision of fitting the rigorous imaging model by RPC model.",no8724,Design and implementation of the real-time color image correction system,"When the color linear array CCD camera works, the output color image has the dislocation phenomenon which has seriously affected the human visual effect. At the same time the dislocation phenomenon has also brought great trouble to the image processing. In order to resolve this problem, a real-time collection system with the PCI interface has been developed. The system not only has the image collection function but also has the communication function. The system is so convenient that it can be easy to achieve control and data transfer. In the system, hardware circuit module is designed with the block-style thinking. The FPGA is the core of the whole hardware circuit module, responsible for data transmission, timing logic control and serial communication. Double buffer structure ensures the data transmission reliably. The PCI driver development and the PC software application are described seriously in the software module. The DMA mode and the event notification method are adopted in the driver. The RGB color correction algorithm is introduced seriously in the software application. The experiment results show that the system has met the requirement targets. It can achieve the maximum transfer rate of 40MBps when running in the DMA mode. The system is stable and real-time when it works outdoors.",no8723,The effect of 3D building reconstruction errors on propagation prediction using geospatial data in cyberspace,"When the 3D building structures visualized in Google Earth are reconstructed using photogrametric method, errors or inaccuracies will occur to the building vertices and the building heights. In this paper the statistics of these errors are discussed and the effect of these errors on the propagation prediction results is examined in detail. It is found that our reconstruction method introduces distance errors to the building vertices and height errors to the building heights. These errors are less than 0.5 meters in 95% of the cases. The vertex error will cause an average mean error of -0.2 dB and an average standard deviation of 5.1 dB to the predicted path gains compared to the reference case. And the height error, in the cases investigated in this paper, is very small and can be ignored. These results match the observations in the literature for different propagation environments. The 3D reconstruction method is then shown to be of satisfactory accuracy in terms of propagation prediction.",no8722,On maximizing the fault coverage for a given test length limit in a synchronous sequential circuit,"When storage requirements or limits on test application time do not allow a complete (compact) test set to be used for a circuit, a partial test set that detects as many faults as possible is required. Motivated by this application, we address the following problem. Given a test sequence T of length L for a synchronous sequential circuit and a length M<L, find a test sequence T<sub>S</sub> of length at most M such that the fault coverage of T<sub>S</sub> is maximal. A similar problem was considered before for combinational and scan circuits, and solved by test ordering. Test ordering is not possible with the single test sequence considered here. We solve this problem by using a vector omission process that allows the length of the sequence T to be reduced while allowing controlled reductions in the number of detected faults. In this way, it is possible to obtain a sequence T<sub>S</sub> that has the desired length and a maximal fault coverage.",no8721,Cross-Cultural Differences of Entrepreneurs' Error Orientation: Comparing Chinese Entrepreneurs and German Entrepreneurs,"When starting up firms, entrepreneurs will easily be in the situation of being away from predetermined goals or criteria. How entrepreneurs cope with errors is critical to the development of their firms. The research used the free software, Mx, and explored the error orientation of entrepreneurs in four industries, including IT and software, catering and hotel, machinery and parts, and construction, to find the differences of entrepreneurs' error orientation by comparing Chinese entrepreneurs and German entrepreneurs. The results suggest that Chinese entrepreneurs pay more attention to the ability of solving problems caused by errors and the ability of learning from errors. German entreprenurs pay more attention to communicating with other when an error occurs. Entrepreneurs of either culture background value the ability of coping with errors. The implication of the results for IT industry is also included.",no8720,Research on Suppression of Secondary Arc Current under Different Fault Locations for 500kV Transmission Line,"When single-phase grounding fault occurs in high-voltage transmission line, secondary arc current and recovery voltage must be suppressed in order to ensure that single-phase autoreclosing operates reliably and successfully. This paper adopts the suppression measure of secondary arc current - shunt reactor with neutral small reactor-which is applied widely in many countries, and then uses PSCAD software to simulate suppression effect about different fault point locations toward an example of 500 kv double-ended sources high-voltage transmission line. According to the simulation results, suppression effects about different locations are distinct. Moreover, this measure can suppress secondary arc current effectively, ensure success of single phase autoreclosing operation and finally achieve security and stability of power system.",no8719,"Simulation-based validation and defect localization for evolving, semi-formal requirements models","When requirements models are developed in an iterative and evolutionary way, requirements validation becomes a major problem. In order to detect and fix problems early, the specification should be validated as early as possible, and should also be revalidated after each evolutionary step. In this paper, we show how the ideas of continuous integration and automatic regression testing in the field of coding can be adapted for simulation-based, automatic revalidation of requirements models after each incremental step. While the basic idea is fairly obvious, we are confronted with a major obstacle: requirements models under development are incomplete and semi-formal most of the time, while classic simulation approaches require complete, formal models. We present how we can simulate incomplete, semi-formal models by interactively recording missing behavior or functionality. However, regression simulations must run automatically and do not permit interactivity. We therefore have developed a technique where the simulation engine automatically resorts to the interactively recorded behavior in those cases where it does not get enough information from the model during a regression simulation run. Finally, we demonstrate how the information gained from model evolution and regression simulation can be exploited for locating defects in the model.",no8718,Robust TCP connections for fault tolerant computing,"When processes on two different machines communicate, they most often do so using the TCP protocol. While TCP is appropriate for a wide range of applications, it has shortcomings in other application areas. One of these areas is fault tolerant distributed computing. For some of those applications, TCP does not address link failures adequately: TCP breaks the connection if connectivity is lost for some duration (typically minutes). This is sometimes undesirable. The paper proposes robust TCP connections, a solution to the problem of broken TCP connections. The paper presents a session layer protocol on top of TCP that ensures reconnection, and provides exactly-once delivery for all transmitted data. A prototype has been implemented as a Java library. The prototype has less than 10% overhead on TCP sockets with respect to the most important performance figures.",no8717,Shading extraction and correction for scanned book images,"When one scans document pages from a bound book, shading artifacts are commonly occurred in the book spine area. In this letter, we propose a general-purpose method for image shading correction based on an assumption that the reflectance function of the page surface is piecewise constant and the illumination function is smooth. The proposed method is able to completely correct more general types of shading artifacts which are nonuniformly distributed along the book spine. Comparison experiments on a synthetic and a variety of real scanned book images demonstrate the feasibility and effectiveness of the proposed method.",no8716,Parameter calculation based on perturbation theory for fault conditions of induction motors,"When motor faults (rotor broken-bar, stator turn- to-turn short, etc.) occur in a motor, its internal current distribution and electromagnetic field will change, which will further cause some variations to stator and rotor parameters. In this paper, the method conjoint of Finite Element Method and Energy Perturbation Method is used to calculate the motor's inductance parameter, and finite-element method is used to calculate the gross energy of internal magnetic field, and perturbation theory is used to calculate the motor's inductance parameter. The variation of inductance parameter under fault conditions is analyzed to generalize principles about variation of inductance parameter before and after the fault. From the results we can conclude that the magnetic energy will more and more numerous along with the increasing number of rotor broken-bar, and will become smaller along with the increasing degree of turn- to-turn short. Along with the increasing number of rotor broken- bar, stator winding self-inductance will be influenced little, but the self-inductance would be smaller and smaller along with the increasing degree of stator turn-to-turn short. The calculation parameter in this paper can be used to analysis of motor faults.",no8715,Decision trees for error concealment in video decoding,"When macro-blocks are lost in a video decoder such as MPEG-2, the decoder can try to conceal the error by estimating or interpolating the missing area. Many different methods for this type of post-processing concealment have been proposed, operating in the spatial, frequency, or temporal domains, or some hybrid combination of them. In this paper, we show how the use of a decision tree that can adaptively choose among several different error concealment methods can outperform each single method. We also propose two promising new methods for temporal error concealment.",no8714,Fault Diagnosis for Analogy Circuits Based on Support Vector Machines,"When it is hard to obtain training samples, the fault classifier based on support vector machine (SVM) can diagnose faults with high accuracy. It can easily be generalized and put to practical use. In this paper, a fault classifier based on support vector machine (SVM) is proposed for analog circuits. It can classify the faults in the target circuit effectively and accurately. In order to test the algorithm, an analog circuit fault diagnosis system based on SVM is designed for the measurement circuit that approximates the square curve with a broken line. After being trained with practical measurement data, the system is shown to be capable of diagnosing faults hidden in real measurement data accurately. Therefore, the effectiveness of the algorithm is verified.",no8713,Quantization errors in digital motor control systems,"When implementing a motor control drive scheme digitally, the quantization errors always exist in the system. Two major sources of quantization errors are analog-to-digital (A-D) process and numerical calculation in the fixed-point computing device. Typically, the effects of quantization errors due to A-D conversion contribute less than one produced by numerical calculation. This paper studies the quantization errors in a sensorless direct vector control system of induction motor system using a 32-bit fixed-point digital signal processor (DSP) from Texas Instruments (TMS320x28xx series). The investigation of quantization errors produced by numerical computation in such DSP is focused. Both simulation and experiment are carried out within DSP itself in three kinds of data formats; 16-bit fixed-point, 32-bit fixed-point, and floating-point. By comparing the results between floating-point and fixed-point implementation on one machine, numerical issues related to quantization errors can be verified and resolved. As a result, the system performance and behavior can be affected by quantization errors in 16-bit word length while it is not significant in the 32-bit word length.",no8712,Fast vignetting correction and color matching for panoramic image stitching,"When images are stitched together to form a panorama there is often color mismatch between the source images due to vignetting and differences in exposure and white balance between images. In this paper a low complexity method is proposed to correct vignetting and differences in color between images, producing panoramas that look consistent across all source images. Unlike most previous methods which require complex non-linear optimization to solve for correction parameters, our method requires only linear regressions with a low number of parameters, resulting in a fast, computationally efficient method. Experimental results show the proposed method effectively removes vignetting effects and produces images that are highly visually consistent in color and brightness.",no8711,Accurate Rank Ordering of Error Candidates for Efficient HDL Design Debugging,"When hardware description languages (HDLs) are used in describing the behavior of a digital circuit, design errors (or bugs) almost inevitably appear in the HDL code of the circuit. Existing approaches attempt to reduce efforts involved in this debugging process by extracting a reduced set of error candidates. However, the derived set can still contain many error candidates, and finding true design errors among the candidates in the set may still consume much valuable time. A <i>debugging</i> <i>priority</i> method was proposed to speed up the error-searching process in the derived error candidate set. The idea is to display error candidates in an order that corresponds to an individual's degree of suspicion. With this method, error candidates are placed in a rank order based on their probability of being an error. The more likely an error candidate is a design error (or a bug), the higher the rank order that it has. With the displayed rank order, circuit designers should find design errors quicker than with blind searching when searching for design errors among all the derived candidates. However, the currently used confidence score (CS) for deriving the <i>debugging</i> <i>priority</i> has some flaws in estimating the likelihood of correctness of error candidates due to the <i>masking</i> <i>error</i> situation. This reduces the degree of accuracy in establishing a <i>debugging</i> <i>priority</i> . Therefore, the objective of this work is to develop a new probabilistic confidence score (PCS) that takes the <i>masking</i> <i>error</i> situation into consideration in order to provide a more reliable and accurate <i>debugging</i> <i>priority</i>. The experimental results show that our proposed PCS achieves better results in estimating the likelihood of correctness and can indeed suggest a <i>debugging</i> <i>priority</i> with better accuracy, as compared to the CS.",no8710,fMRI Activation during Observation of Others' Reach Errors,"When exposed to novel dynamical conditions (e.g., externally imposed forces), neurologically intact subjects easily adjust motor commands on the basis of their own reaching errors. Subjects can also benefit from visual observation of others' kinematic errors. Here, using fMRI, we scanned subjects watching movies depicting another person learning to reach in a novel dynamic environment created by a robotic device. Passive observation of reaching movements (whether or not they were perturbed by the robot) was associated with increased activation in fronto-parietal regions that are normally recruited in active reaching. We found significant clusters in parieto-occipital cortex, intraparietal sulcus, as well as in dorsal premotor cortex. Moreover, it appeared that part of the network that has been shown to be engaged in processing self-generated reach error is also involved in observing reach errors committed by others. Specifically, activity in left intraparietal sulcus and left dorsal premotor cortex, as well as in right cerebellar cortex, was modulated by the amplitude of observed kinematic errors.",no8709,Research on non-communication protection of distribution lines based on fault components,"When distribution power lines run normally, there is asymmetrical current and voltage component including negative sequence and zero sequence components. So non-communication protection based on sequence components might mal-operate or mis-operate in the cases. The paper proposed a new noncommunication protection scheme based on fault component for distribution line. The protection can be applied to different neutral grounding mode including direct and indirect grounding neutral system and isn't affected by normal running asymmetrical component. According to the fault component (or the incremental quantities of three sequence component), it can assure that one end breaker of fault line trips as quickly as possible and other end breaker is accelerated to trip. Simulation shows that the protection scheme is both correct and effective.",no8708,A model of soft error effects in generic IP processors,"When designing reliability-aware digital circuits, either hardware or software techniques may be adopted to provide a certain degree of failure detection/tolerance, caused by either hardware faults or soft-errors. These techniques are quite well established when working at a low abstraction level, whereas are currently under investigation when moving to higher abstraction levels, in order to cope with the increasing complexity of the systems being designed. This paper presents a model of soft error effects to be adopted when defining software-only techniques to achieve fault detection capabilities. The work identifies on a generic IP processor the misbehaviors caused by soft errors, classifies and analyzes them with respect to the possibility of detecting them by means of previously published approaches. An experimental validation of the proposed model is carried out on the Leon2 processor.",no8707,Surviving Attacks and Intrusions: What can we Learn from Fault Models,"When designing or analyzing applications or infrastructures with high reliability, safety, security, or survivability demands, the fundamental questions are: what is required of the application and can the infrastructure support these requirements. In the design and analysis of fault-tolerant systems, fault models have served us well to describe the theoretical limits. But with the inclusion of malicious acts, the direct application of fault models has exposed limited applicability. However, we can take advantage of the powerful fault models if we defer their direct application from the events that lead to faults, that is, the fault causes, and instead focus on the effects. This way one can avoid questions referring to the meaning of fault models in the context of previously unsuitable faults like Trojan horses or Denial of Service (DoS) attacks. Instead, we can use fault models at the level of abstraction where the application maps on the infrastructure. In this paper fault models are discussed in the context of system survivability and malicious act. It is shown that these models can be used to balance the demands put on the application and the capabilities of the underlying infrastructure. Active and imposed fault descriptions are defined that allow to match the mechanisms that provide survivability to the application with the infrastructure-imposed limitations. By defining a system as a collection of functionalities, individual functionalities and their associated fault descriptions can be analyzed in isolation.",no8706,An industrial environment for high-level fault-tolerant structures insertion and validation,"When designing a VLSI circuits, most of the efforts are now performed at levels of abstractions higher than gate. Correspondingly to this clear trend, there is a growing request to tackle safety-critical issues directly at the RT-level. This paper presents a complete environment for considering safety issues at the RT level. The environment was implemented and tested by an industry for devising a sample safety-critical device. Designers were permitted to assess the effects of transient faults, automatically add fault-tolerant structures, and validate the results working on the same circuit descriptions and acting in a coherent framework. The evaluation showed the effectiveness of the proposed environment.",no8705,On the relation between design contracts and errors: a software development strategy,"When designing a software module or system, a systems engineer must consider and differentiate between how the system responds to external and internal errors. External errors cannot be eliminated and must be tolerated by the system, while the number of internal errors should be minimized and the resulting faults should be detected and removed. This paper presents a development strategy based on design contracts and a case study of an industrial project in which the strategy was successfully applied. The goal of the strategy is to minimize the number of internal errors during the development of a software system while accommodating external errors. A distinction is made between weak and strong contracts. These two types of contracts are applicable to external and internal errors, respectively. According to the strategy, strong contracts should be applied initially to promote the correctness of the system. Before releasing, the contracts governing external interfaces should be weakened and error management of external errors enabled. This transformation of a strong contract to a weak one is harmless to client modules",no8704,Servo Performance Enhancement of Motion System via a Quantization Error Estimation MethodIntroduction to Nanoscale Servo Control,"When compared to the accuracy of nanoscale control, the resolution of current positioning sensors is relatively low. Because of this, the output from low-precision sensors normally includes quantization errors that could degrade control performance. As a result, in this paper, a method of quantization error estimation based on the least square method is examined. In the proposed method, estimation accuracy is improved by taking into account the effect of input disturbances. Furthermore, a bias adjustment method is proposed that is expected to satisfy the constraints on quantization error. The effectiveness of the proposed method is demonstrated by simulations and experiments.",no8703,Color correction of multiview video with average color as reference,"When capturing multiview video, there can be significant variations in the color of views captured with different cameras. This negatively affects compression efficiency when multiview video is coded using inter-view prediction. In this paper we propose a method for correcting the color of multiview video sets as a preprocessing step to compression. Unlike previous work where one of the captured views is used as the color reference, we correct all views to match the average color of the set of views. Block based disparity estimation is used to find matching points between all views in the video set, and the average color is calculated for these matching points. Least squares regressions are used to find functions that will make each view match the average color. Experimental results show that the proposed method results in video sets that closely match in subjective color. Furthermore, when multiview video is compressed with JMVM, the proposed method increases compression efficiency by up to 1.0 dB compared to compressing the original uncorrected video.",no8702,Error Reporting Logic,"When a system fails to meet its specification, it can be difficult to find the source of the error and determine how to fix it. In this paper, we introduce error reporting logic (ERL), an algorithm and tool that produces succinct explanations for why a target system violates a specification expressed in first order predicate logic. ERL analyzes the specification to determine which parts contributed to the failure, and it displays an error message specific to those parts. Additionally, ERL uses a heuristic to determine which object in the target system is responsible for the error. Results from a small user study suggest that the combination of a more focused error message and a responsible object for the error helps users to find the failure in the system more effectively. The study also yielded insights into how the users find and fix errors that may guide future research.",no8701,Study on faulty feeder selection methods of single-phase earthed fault in non-solidly grounded systems,"When a single-phase earthed fault happened in non-solidly grounded systems, it is very important to select the faulty feeder rapidly in order to improve the power supply reliability. All currently used detection principles of single-phase earthed fault are analyzed in the paper. The advantages and disadvantages of each selection principle are also summarized. Finally, it presents my view on single-phase earthed fault detection: detection methods using transient signals have better sensitivity than that using steady state signals; it is not only applicable for Peterson coiled systems, but also applicable for intermittent arc earthed fault. With the development of modern microelectronic technique, the transient signal generated by earthed fault can be easily recorded and processed by complicated digital algorithm; hence, transient detection method must be more applicable in the future faulty feeder selection devices.",no8700,Automatic generation of diagnostic expert systems from fault trees,"When a fault tolerant computer-based system fails, diagnosis and repair must be performed to bring the system back to an operational state. The use of fault tolerance design implies that several components or subsystems may have failed, and that perhaps many of these faults have been tolerated before the system actually succumbed to failure. Diagnosis procedures are then needed to determine the most likely source of failure and to guide repair actions. Expert systems are often used to guide diagnostics, but the derivation of an expert system requires knowledge (i.e., a conceptual model) of failure symptoms. In this paper, we consider the problem of diagnosing a system for which there may be little experience, given that it might be a one-of-a-kind system or because access to the system may be limited. We conjecture that the same fault tree model used to help aid in the design and analysis of the system can provide the conceptual model of system component interactions needed in order to define a diagnostic process. We explore the use of a fault tree model (along with the probabilities of failure for the basic events) along with partial knowledge of the state of the system (i.e., the system has failed, and perhaps some components are known to be operational or failed) to produce a diagnostic aid.",no8699,A fault location scheme based on spectrum characteristic of fault-generated high-frequency transient signals,"When a fault occurs on a transmission line, fault-generated high-frequency signals contains a lot of information about fault. This paper proposes a fault location scheme for transmission lines based on spectrum characteristic of high-frequency components, the difference of adjacent two natural frequencies in one ends of transmission line is used to compute fault distance. This method need not identify arriving time of first traveling wave and traveling wave from fault point, it is insensitive to swings, fault type, fault resistance and system source parameters. The scheme performance was proven using power system computer aided design (PSCAD) simulations in 500-kV power system considering critical fault cases.",no8698,An Analytic Model for Fault Diagnosis in Power Systems Considering Malfunctions of Protective Relays and Circuit Breakers,"When a fault occurs on a section or a component in a given power system, if one or more protective relays (PRs) and/or circuit breakers (CBs) associated do not work properly, or in other words, a malfunction or malfunctions happen with these PRs and/or CBs, the outage area could be extended. As a result, the complexity of the fault diagnosis could be greatly increased. The existing analytic models for power system fault diagnosis do not systematically address the possible malfunctions of PRs and/or CBs, and hence may lead to incorrect diagnosis results if such malfunctions do occur. Given this background, based on the existing analytic models, an effort is made to develop a new analytic model to well take into account of the possible malfunctions of PRs and/or CBs, and further to improve the accuracy of fault diagnosis results. The developed model does not only estimate the faulted section(s), but also identify the malfunctioned PRs and/or CBs as well as the missing and/or false alarms. A software system is developed for practical applications, and realistic fault scenarios from an actual power system are served for demonstrating the correctness of the presented model and the efficiency of the developed software system.",no8697,On Using Simplification and Correction Tables for Integrity Maintenance in Integrated Databases,"When a database is defined as views over autonomous sources, inconsistencies with respect to global integrity constraints are to be expected. This paper investigates the possibility of using simplification techniques for integrity constraints in order to maintain, in an incremental way, a correction table of virtual updates which, if executed, would restore consistency; access can be made through auxiliary views that take the table into account. The approach employs assumptions about local source consistency as well as cross-source constraints whenever possible",no8696,Corrective maintenance maturity model (CM<sup>3</sup>): maintainer's education and training,"What is the point of improving maintenance processes if the most important asset, people, is not properly utilised? Knowledge of the product(s) maintained, maintenance processes and communications skills is very important for achieving quality software and for improving maintenance and development processes. We present CM<sup>3</sup>: Maintainer's Education and Training-a maturity model for educating and training maintenance engineers. This model is the result of a comparative study of two industrial processes utilised at ABB, and of process models such as IEEE 1219, ISO/IEC 12207, CMM, People CMM, and TickIT",no8695,Comparing Web Services Performance and Recovery in the Presence of Faults,"Web-services are supported by a complex software infrastructure that must ensure high performance and availability to the client applications. Web services industry holds a well established platform for performance benchmarking (e.g., TPC-App and SPEC jAppServer2004 benchmarks). In addition, several studies have been published recently by main vendors focusing web services performance. However, as peak performance evaluation has been the main focus, the characterization of the impact of faults in such systems has been largely disregarded. This paper proposes an approach for the evaluation and comparison of performance and recovery time in web services infrastructures. This approach is based on fault injection and is illustrated through a concrete example of benchmarking three alternative software solutions for web services deployment.",no8694,Web services system supporting quality fault-tolerance,"Web services provide reusable software components so that a single Web service can be used by multiple users or a single user can use multiple Web services. In this aspect, the reliability of Web services is becoming more and more important. However, the current fault-tolerance method requires the user application to change to a certain format and does not extend and consider the quality factors.",no8693,A Fault Tolerant Web Service Architecture,"Web services have been pointed as a suitable technology for the development and execution of distributed applications. However, the Web service architecture still lacks facilities to support fault tolerance. The goal of this paper is to propose a fault tolerant Web service architecture. The architecture provides service mediation and monitoring. The main contribution of this paper is the use of Web service standards to include fault tolerance in the Web service architecture.",no8692,A fault tolerant web services architecture based reflection,"Web services have been enjoying great popularities in recent years. The high usability of the Web service is becoming a new focus for research. How to provide generic fault tolerant mechanisms in Web services is worth researching. A fault tolerant web services architecture named Fault Tolerant Web Services is proposed in the article. In the architecture, the fault tolerant mechanisms are transparent and easy to use. The users can tailor their own fault tolerance mechanisms and the application programmers almost neednpsilat to care the fault tolerant mechanisms. The Architecture is set forth in detail in the article. The workflow of the system is narrated by three states of a fault tolerant Web service.",no8691,Web Services for Automated Fault Analysis in Electrical Power System,"Web Services for Automated Fault Analysis (WS-AFA) in Electrical Power System is described in this paper. WS-AFA is a new solution to investigate and analyze fault and disturbance records from Digital Fault Recorders (DFRs) or other Intelligent Electronic Devices (IEDs) in substations. The paper describes the overall system architecture as well as the implementation of the services. C# and Dot NET technology has been successfully used for efficient implementation of the Web services. WS-AFA is composed of signal segmentation, signal analysis, fault type classification, fault recorded viewer and fault location services. Such services are designed to enhance manual investigation performed by engineers in power utility.",no8690,The Fast and the Fair: A Fault-Injection-Driven Comparison of Restart Oracles for Reliable Web Services,"Web services are typically deployed in Internet or Intranet environments, making message transfers susceptible to a wide variety of network, protocol and system failures. To mitigate these problems, reliable messaging solutions for Web services have been proposed that retry messages suspected to be lost. It is of interest to evaluate the performance of such reliable messaging solutions, and in this paper we therefore utilise a newly developed fault-injection environment for the analysis of time-out strategies for the Web services reliable messaging standard. We compare oracles that determine retransmission times with respect to the tradeoff between two metrics: the effective transfer time and the overhead in terms of additional message transmissions. Our fault-injection environment allows faults to be invoked in the IP layer, representing a variety of failure situations in the underlying system. The study presented in this paper includes two adaptive oracles, which set the length of the retransmission interval based on round trip time measurements, and two static oracles. The study considers both HTTP and Mail as SOAP transports. We conclude that adaptive oracles may significantly outperform static oracles when the underlying system exhibits more complex behaviour",no8689,Fault-Based Web Services Testing,"Web services are considered a new paradigm for building software applications that has many advantages over the previous paradigms; however, Web services are still not widely used because Service Requesters do not trust Web services that were built by others. Testing can participate in solving this problem because it can be used to assess the quality attributes of Web services and hence increase the requesters' trustworthiness. This paper proposes an approach that can be used to test the robustness and other related attribute of Web services, and that can be easily enhanced to assess other quality attributes. The framework is based on rules for test case generation that are designed by, firstly, analyzing WSDL document to know what faults could affect the robustness quality attribute of Web services, and secondly, using the fault-based testing techniques to detect such faults. A proof of concept tool that depends on these rules has been implemented in order to assess the usefulness of the rules in detecting robustness faults in different Web services platforms.",no8688,Fault Tolerance Connectors for Unreliable Web Services,"Web Services are commonly used to implement service oriented architectures/applications. Service-oriented applications are large-scale distributed applications, typically highly dynamic, by definition loosely coupled and often unstable due to the unreliability of Web Services, which can be moved, deleted, and are subject to various sources of failures. In this paper, we propose customizable fault-tolerance connectors to add fault-tolerance to unreliable Web Services, thus filling the gap between clients and Web Service providers. Connectors are designed by clients, providers or dependability experts using the original WSDL description of the service. These connectors insert detection actions (e.g. runtime assertions) and recovery mechanisms (based on various replications strategies). The connectors can use identical or equivalent available service replicas. The benefits of this approach are demonstrated experimentally.",no8687,Finding Bugs in Web Applications Using Dynamic Test Generation and Explicit-State Model Checking,"Web script crashes and malformed dynamically generated webpages are common errors, and they seriously impact the usability of Web applications. Current tools for webpage validation cannot handle the dynamically generated pages that are ubiquitous on today's Internet. We present a dynamic test generation technique for the domain of dynamic Web applications. The technique utilizes both combined concrete and symbolic execution and explicit-state model checking. The technique generates tests automatically, runs the tests capturing logical constraints on inputs, and minimizes the conditions on the inputs to failing tests so that the resulting bug reports are small and useful in finding and fixing the underlying faults. Our tool Apollo implements the technique for the PHP programming language. Apollo generates test inputs for a Web application, monitors the application for crashes, and validates that the output conforms to the HTML specification. This paper presents Apollo's algorithms and implementation, and an experimental evaluation that revealed 673 faults in six PHP Web applications.",no8686,Error Correcting Output Coding-Based Conditional Random Fields for Web Page Prediction,"Web page prefetching has been used efficiently to reduce the access latency problem of the Internet, its success mainly relies on the accuracy of Web page prediction. As powerful sequential learning models, conditional random fields (CRFs) have been used successfully to improve the Web page prediction accuracy when the total number of unique Web pages is small. However, because the training complexity of CRFs is quadratic to the number of labels, when applied to a Web site with a large number of unique pages, the training of CRFs may become very slow and even intractable. In this paper, we decrease the training time and computational resource requirements of CRFs training by integrating error correcting output coding (ECOC) method. Moreover, since the performance of ECOC-based methods crucially depends on the ECOC code matrix in use, we employ a coding method, search coding, to design the code matrix of good quality.",no8685,Mapping software faults with web security vulnerabilities,"Web applications are typically developed with hard time constraints and are often deployed with critical software bugs, making them vulnerable to attacks. The classification and knowledge of the typical software bugs that lead to security vulnerabilities is of utmost importance. This paper presents a field study analyzing 655 security patches of six widely used web applications. Results are compared against other field studies on general software faults (i.e., faults not specifically related to security), showing that only a small subset of software fault types is related to security. Furthermore, the detailed analysis of the code of the patches has shown that web application vulnerabilities result from software bugs affecting a restricted collection of statements. A detailed analysis of the conditions/locations where each fault was observed in our field study is presented allowing future definition of realistic fault models that cause security vulnerabilities in web applications, which is the key element to design a realistic attack injector.",no8684,Comparing Error Detection Techniques for Web Applications: An Experimental Study,"Web applications are highly sensitive to the occurrence of user-visible failures. Despite the usage of system-level monitoring tools there are still some application-level errors that escape to those tools and end up to be seen in the Web pages of the final users. Complementary error detection mechanisms should then be used to overcome this problem.In this paper, we present an experimental study where we measured the effectiveness of four different error-detection mechanisms under different fault-load. For the effect we used two benchmarks (JPetstore and TPC-W) and a software fault-injector. The results show that although system-level monitoring tools are very effective in most of the cases, there are other detection mechanisms that present a better latency and coverage when dealing with errors at the application-level. Particularly, the usage of external monitoring schemes seems to be of utmost importance.",no8683,Soft-Errors Phenomenon Impacts on Design for Reliability Technologies,"We will mainly address here the ""alter ego"" of quality, which is reliability, and is becoming a growing concern for designers using the latest technologies. After the DFM nodes in 90nm and 65nm, we are entering the DFR area, or Design For Reliability straddling from 65nm to 45nm and beyond. Because of the randomness character of reliability - failures can happen anytime anywhere - executives should mitigate reliability problems in terms of risk, which costs include cost of recalls, warranty costs, and loss of goodwill.",no8682,Using machine learning for estimating the defect content after an inspection,"We view the problem of estimating the defect content of a document after an inspection as a machine learning problem: The goal is to learn from empirical data the relationship between certain observable features of an inspection (such as the total number of different defects detected) and the number of defects actually contained in the document. We show that some features can carry significant nonlinear information about the defect content. Therefore, we use a nonlinear regression technique, neural networks, to solve the learning problem. To select the best among all neural networks trained on a given data set, one usually reserves part of the data set for later cross-validation; in contrast, we use a technique which leaves the full data set for training. This is an advantage when the data set is small. We validate our approach on a known empirical inspection data set. For that benchmark, our novel approach clearly outperforms both linear regression and the current standard methods in software engineering for estimating the defect content, such as capture-recapture. The validation also shows that our machine learning approach can be successful even when the empirical inspection data set is small.",yes8681,Single-trial fMRI Shows Contralesional Activity Linked to Overt Naming Errors in Chronic Aphasic Patients,"We used fMRI to investigate the roles played by perilesional and contralesional cortical regions during language production in stroke patients with chronic aphasia. We applied comprehensive psycholinguistic analyses based on well-established models of lexical access to overt picture-naming responses which were evaluated using a single trial design that permitted distinction between correct and incorrect responses on a trial-by-trial basis. Although both correct and incorrect naming responses were associated with left-sided perilesional activation, incorrect responses were selectively associated with robust right-sided contralesional activity. Most notably, incorrect responses elicited overactivation in the right inferior frontal gyrus that was not observed in the contrasts for patients' correct responses or for responses of age-matched control subjects. Errors were produced at slightly later onsets than accurate responses and comprised predominantly semantic paraphasias and omissions. Both types of errors were induced by pictures with greater numbers of alternative names, and omissions were also induced by pictures with late acquired names. These two factors, number of alternative names per picture and age of acquisition, were positively correlated with activation in left and right inferior frontal gyri in patients as well as control subjects. These results support the hypothesis that some right frontal activation may normally be associated with increasing naming difficulty, but in patients with aphasia, right frontal overactivation may reflect ineffective effort when left hemisphere perilesional resources are insufficient. They also suggest that contralesional areas continue to play a roledysfunctional rather than compensatoryin chronic aphasic patients who have experienced a significant degree of recovery.",no8680,Systematic Error of the Nose-to-Nose Sampling-Oscilloscope Calibration,"We use traceable swept-sine and electrooptic-sampling-system-based sampling-oscilloscope calibrations to measure the systematic error of the nose-to-nose calibration, and compare the results to simulations. Our results show that the errors in the nose-to-nose calibration are small at low frequencies, but significant at high frequencies.",no8679,Location-known-exactly human-observer ROC studies of attenuation and other corrections for SPECT lung imaging,"We use receiver operating characteristic (ROC) analysis of a location-known-exactly (LKE) lesion detection task to compare the image quality of SPECT reconstruction with and without various combinations of attenuation correction (AC), scatter correction (SC) and resolution compensation (RC). Hybrid images were generated from Tc-99m labelled NeoTect clinical backgrounds into which Monte Carlo simulated solitary pulmonary nodule (SPN) lung lesions were added, then reconstructed using several strategies. Results from a human-observer study show that attenuation correction degrades SPN detection, while resolution correction improves SPN detection, even when the lesion location is known. This agrees with the results of a previous localization-response operating characteristic (LROC) study using the same images, indicating that location uncertainty is not the sole source of the changes in detection accuracy.",no8678,Impact of Spacecraft Shielding on Direct Ionization Soft Error Rates for Sub-130 nm Technologies,"We use ray tracing software to model various levels of spacecraft shielding complexity and energy deposition pulse height analysis to study how it affects the direct ionization soft error rate of microelectronic components in space. The analysis incorporates the galactic cosmic ray background, trapped proton, and solar heavy ion environments as well as the October 1989 and July 2000 solar particle events.",no8677,A Chu Spaces Semantics of BPEL-Like Fault Handling,We use Chu spaces and an algebra of them to give a denotational semantics of a subset of BPEL. The emphasis is on the scope-based fault handling mechanism. We propose BPEL-F as an abstraction of the subset of BPEL including typical control flow and fault handling. Chu spaces form the main semantic domain. We study the influence of fault handling to the algebraic operators of Chu spaces. and present modified versions of the sequence and concurrence operators. The trigger operator is designed to model the scope-based fault handling. We present valuation functions mapping BPEL-F constructs to Chu spaces.,no8676,Whither generic recovery from application faults? A fault study using open-source software,"We test the hypothesis that generic recovery techniques, such as process pairs, can survive most application faults without using application-specific information. We examine in detail the faults that occur in three, large, open-source applications: the Apache Web server, the GNOME desktop environment and the MySQL database. Using information contained in the bug reports and source code, we classify faults based on how they depend on the operating environment. We find that 72-87% of the faults are independent of the operating environment and are hence deterministic (non-transient). Recovering from the failures caused by these faults requires the use of application-specific knowledge. Half of the remaining faults depend on a condition in the operating environment that is likely to persist on retry, and the failures caused by these faults are also likely to require application-specific recovery. Unfortunately, only 5-14% of the faults were triggered by transient conditions, such as timing and synchronization, that naturally fix themselves during recovery. Our results indicate that classical application-generic recovery techniques, such as process pairs, will not be sufficient to enable applications to survive most failures caused by application faults",no8675,Razor II: In Situ Error Detection and Correction for PVT and SER Tolerance,"We take advantage of these findings and propose a Razor II approach that introduces two components. First, instead of performing both error detection and correction in the FF, Razor II performs only detection in the FF, while correction is performed through architectural replay.",no8674,Introducing SW-based fault handling mechanisms to cope with EMI in embedded electronics: are they a good remedy?,"We summarize a study on the effectiveness of two software-based fault handling mechanisms in terms of detecting conducted electromagnetic interference (EMI) in microprocessors. One of these techniques deals with processor control flow checking. The second one is used to detect errors in code variables. In order to check the effectiveness of such techniques in RF ambient, an EIC 61.000-4-29 normative-compliant conducted RF-generator was implemented to inject spurious electromagnetic noise into the supply lines of a commercial off-the-shelf (COTS) microcontroller-based system. Experimental results suggest that the considered techniques present a good effectiveness to detect this type of faults, despite the multiple-fault injection nature of EMI in the processor control and data flows, which in most cases result in a complete system functional loss (the system must be reset).",no8673,Empirical method for topographic correction in aerial photographs,"We suggest an empirical method to correct topographic effects on vegetation classification of panchromatic aerial photographs. The method is based on the use of spatial interpolation technique that constructs a luminance surface from targets of high brightness values. The luminance surface is then used to correct the topographic effects differentially, by increasing brightness values in shaded areas and decreasing brightness values of lightened areas. For this purpose, the use of a trapezoidal function was found successful in the reduction of standard deviation of brightness values of trees, shrubs, and herbaceous plants after empirical correction. This method outperformed a frequently used digital elevation model-based topographic correction in terms of overall classification accuracy of the resulting images.",no8672,Up-sampling with Shift Method for Windmill Correction,"We suggest a simple and efficient approach to reduce, and, in some cases, eliminate the windmill artifact. Our method does not require hardware changes. The idea is to improve the z-sampling by applying up-sampling in the detector row direction. One recent result show that the linear interpolation can be improved by shifting the samples by some small amount; we utilize this approach for our purposes. Evaluation shows that using some specified amount of the shift in interpolation, the proposed method provides a significant improvement of the windmill artifact with the clinical data, without a noticeable loss of z-resolution.",no8671,Security games with decision and observation errors,"We study two-player security games which can be viewed as sequences of nonzero-sum matrix games played by an Attacker and a Defender. The evolution of the game is based on a stochastic fictitious play process. Players do not have access to each other's payoff matrix. Each has to observe the other's actions up to present and plays the action generated based on the best response to these observations. However, when the game is played over a communication network, there are several practical issues that need to be taken into account: First, the players may make random decision errors from time to time. Second, the players' observations of each other's previous actions may be incorrect. The players will try to compensate for these errors based on the information they have. We examine convergence property of the game in such scenarios, and establish convergence to the equilibrium point under some mild assumptions when both players are restricted to two actions.",no8670,On undetectable faults in partial scan circuits using transparent-scan,"We study the undetectable faults in partial scan circuits under a test application scheme referred to as transparent-scan. The transparent-scan approach allows very aggressive test compaction compared to other approaches. We demonstrate that, unlike other approaches that provide high levels of test compaction for partial scan circuits, this approach does not increase the number of undetectable faults. We also discuss the monotonicity of the number of undetectable faults with increased levels of scan.",no8669,Dense error correction via l1-minimization,"We study the problem of recovering a non-negative sparse signal x isin Ropf<sup>n</sup> from highly corrupted linear measurements y = Ax+e isin Ropf<sup>m</sup>, where e is an unknown (and unbounded) error. Motivated by an observation from computer vision, we prove that for highly correlated dictionaries A, any non-negative, sufficiently sparse signal x can be recovered by solving an lscr<sup>1</sup>-minimization problem: min ||x||<sub>1</sub> + ||e||<sub>1</sub> subject to y = Ax + e. If the fraction rho of errors is bounded away from one and the support of x grows sublinearly in the dimension m of the observation, for large m, the above lscr<sup>1</sup>-minimization recovers all sparse signals x from almost all sign-and-support patterns of e. This suggests that accurate and efficient recovery of sparse signals is possible even with nearly 100% of the observations corrupted.",no8668,Consistent detection of global predicates under a weak fault assumption,"We study the problem of detecting general global predicates in distributed systems where all application processes and at most t<m monitor processes may be subject to crash faults, where m is the total number of monitor processes in the system. We introduce two new observation modalities called negotiably and discernibly (which correspond to possibly and definitely in fault-free systems) and present detection algorithms for them which work under increasingly weak fault assumptions",no8667,Memory-based context-sensitive spelling correction at web scale,"We study the problem of correcting spelling mistakes in text using memory-based learning techniques and a very large database of token n-gram occurrences in web text as training data. Our approach uses the context in which an error appears to select the most likely candidate from words which might have been intended in its place. Using a novel correction algorithm and a massive database of training data, we demonstrate higher accuracy on correcting real- word errors than previous work, and very high accuracy at a new task of ranking corrections to non-word errors given by a standard spelling correction package.",no8666,Average error rate of linear diversity reception schemes over generalized gamma fading channels,"We study the performance of M-ary modulation schemes in the presence of additive white Gaussian noise (AWGN) and slow fading. Selection combining (SC), equal gain combining (EGC), and maximal ratio combining (MRC) diversity schemes are considered. The fading channel is modeled by the generalized gamma distribution, which includes the Rayleigh, Nakagami, Weibull, and log-normal distributions as special or limiting cases. The Suzuki distribution can be adequately approximated by the generalized gamma distribution. The exact average symbol error rates (ASER) for coherent multilevel modulation schemes with SC and MRC are presented by using the moment generating function (MGF) based approach while that of EGC is obtained by employing a characteristic function (CHF) based approach. The analysis results for the three combiners are compared and discussed. Simulation results are also provided.",no8665,Optical Wireless Communications With Heterodyne Detection Over Turbulence Channels With Pointing Errors,"We study the error performance of an heterodyne differential phase-shift keying (DPSK) optical wireless (OW) communication system operating under various intensity fluctuations conditions. Specifically, it is assumed that the propagating signal suffers from the combined effects of atmospheric turbulence-induced fading, misalignment fading (i.e., pointing errors) and path-loss. Novel closed-form expressions for the statistics of the random attenuation of the propagation channel are derived and the bit-error rate (BER) performance is investigated for all the above fading effects. Numerical results are provided to evaluate the error performance of OW systems with the presence of atmospheric turbulence and/or misalignment. Moreover, nonlinear optimization is also considered to find the optimum beamwidth that achieves the minimum BER for a given signal-to-noise ratio value.",no8664,Hybrid Solution: A FEC Algorithm for Fault Tolerant Routing in Sensor Networks,"We study the characteristics of wireless sensor networks (WSN) and present a lightweight FEC coding algorithm combined with a smart fault tolerant routing scheme in this paper. The proposed coding-decoding algorithm is based on XOR operation and requires very little computation and storage space, which are critical for WSN. There are few existing channel coding algorithms (FEC) put forward for use in sensor networks, and they are not very suitable, due to their high computing, storage and delay cost. Further more, normal FEC coding algorithms are not flexible enough to suit the variable states in WSN. We adopt a cross-layer design wherein higher network layers use information about packet loss to adjust the coding level according to the dynamics of the network. And our routing scheme has the ability to discover and select robust paths to reliably relay data packets. Simulation result shows that our coding algorithm and self-adaptive routing scheme perform better than existing schemes.",no8663,Sensor Minimization Problems with Static or Dynamic Observers for Fault Diagnosis,"We study sensor minimization problems in the context of fault diagnosis. Fault diagnosis consists of synthesizing a diagnoser that observes a given plant and identifies faults in the plant as soon as possible after their occurrence. Existing literature on this problem has considered the case of static observers, where the set of observable events does not change during execution of the system. In this paper, we consider static as well as dynamic observers, where the observer can switch sensors on or off, thus dynamically changing the set of events it wishes to observe.",no8662,Extended Hamming product codes analytical performance evaluation for low error rate applications,"We study product codes based on extended Hamming codes. We focus on their performance at low error rates, which are important for wireless multimedia applications. We present the basis and a complete set of techniques which allows one to analytically evaluate this performance without resorting to extremely long simulations. We present new theoretical results concerning the popular approximation where the bit error rate is nearly equal to the frame error rate times the ratio of the minimum distance to the codeword length. We prove that: 1) binary codes with a transitive automorphism group satisfy this approximation with equality; and 2) extended Hamming product codes belong to this class. Closed-form expressions for their dominant multiplicity values are derived. Analytical curves are plotted, discussed, and validated by comparison with iterative decoding. This analytical approach is then extended to both shortened and punctured codes, which are important for practical design. The first case is solved by applying the extended MacWilliams identity to the dual codes. For punctured codes, we present a new analytical approach for estimating their average performance using a ""random"" puncturer.",no8661,Fault-Tolerant Sensor Coverage for Achieving Wanted Coverage Lifetime with Minimum Cost,"We study how to select and arrange multiple types of wireless sensors to build a star network that meets the coverage, the lifetime, the fault-tolerance, and the minimum-cost requirements, where the network lifetime, the acceptable failure probability of the network, and the failure rate of each type of sensors are given as parameters. This problem is NP-hard. We model this problem as an integer linear programming minimization problem. We then present an efficient approximation algorithm to find a feasible solution to the problem, which provides a sensor arrangement and a scheduling. We show that, through numerical experiments, our approximation provides solutions with approximation ratios less than 1.4.",no8660,Sensitivity Analysis on Bio-op Errors in DNA Computing,"We simulate the biological operations of a DNA computational algorithm for a combinatorial problem. We then perform sensitivity analysis in which we vary bio-op error rates to see which bio-ops affect the end result. Finally, we review three approaches to tune the algorithm in order to minimize significant error.",no8659,New Limits on Fault-Tolerant Quantum Computation,"We show that quantum circuits cannot be made fault-tolerant against a depolarizing noise level of thetas = (6 - 2radic2)/7 ap 45%, thereby improving on a previous bound of 50% (due to Razborov, 2004). More precisely, the circuit model for which we prove this bound contains perfect gates from the Clifford group (CNOT, Hadamard, S, X, Y, Z) and arbitrary additional one-qubit gates that are subject to depolarizing noise thetas. We prove that this set of gates cannot be universal for arbitrary (even classical) computation, from which the upper bound on the noise threshold for fault-tolerant quantum computation follows",no8658,A simple and efficient burst error correcting code based on an array code,"We show that a widely used array code, known as the even-odd code, which is targeted at phased burst errors, may also be useful for non-phased burst errors. A new decoder is proposed for this code which effectively converts it into a more general burst error correcting code. The proposed scheme is shown to be capable of correcting almost all bursts up to a certain length, such that its performance is attractive for many communication applications. Since the failure rate is sufficiently low, the code can be practically classified as a burst error correcting code. The redundancy in this code is equal to twice the maximal burst length, which is the same redundancy as the lower bound of conventional burst error correcting codes (the Reiger bound). Both the encoder and the decoder have very low complexity, both in terms of number of operations and in terms: of computer code size. We analyze the probability of failure, provide tight upper and lower bounds, and show that asymptotically this probability approaches zero for large blocks.",no8657,Application of CL/EBIC-SEM techniques for characterization of irradiation induced defects in triple junction solar cells,"We report the results of the characterization of irradiated InGaP<sub>2</sub>/GaAs/Ge multijunction (MJ) solar cells using the cathodoluminescence (CL) imaging/spectroscopy and electron beam induced current (EBIC) modes of scanning electron microscopy (SEM). These techniques were applied to verify the influence of irradiation damage on the optoelectronic properties of each subcell triple junction structure and correlate illuminated (AM0, 1 sun, 25C) current-voltage (IV) and quantum efficiency (QE) characteristics.",no8656,Advanced 3b4b channel coding for low error-rate optical links at 2.488 Gbit/s,We report on the performance of an optimized parallel channel coder for high-speed optical transmission systems. The coding properties are discussed by an evaluation of the signal statistics of the coded pulse train in the time and frequency domain. The discussion is mainly based on the results for the power spectral density (PSD) and the autocorrelation function (AKF). The theoretical investigations have been verified measurements with a developed 2.488 Gbit/s optical transmission system. Reliability studies have shown a system's bit error rate below 10<sup>-13</sup>,no8655,Atomic-scale surface structures and structural defects of hydrothermal batio3 nanoparticles revealed by HRTEM,"We report on the effects of reactive conditions on the formation of BT nanoparticles via hydrothermal process with a focus on their atomic-scale surface microstructures and structural defects characterized by HRTEM. The results showed that large Ba/Ti molar ratios in the precursors could lead to large particles with a cubic morphology. Smaller particles with a weaker agglomeration behavior were observed in the products synthesized via solvothermal process using ethylene glycol (EG) as reaction medium, in comparison to using either pure water or water-EG mixed solution as reactive medium. A terrace-ledge-kink surface structure and structural defects such as anti-phase boundaries (APBs) and edge dislocations with Burger's vector of or were observed at the edges of the 1/2<sub>d100</sub> or 1/2<sub>d111</sub> were observed at the edges of the nanoparticles. The {110} surfaces were found to be reconstructed and composed of the corners bound by the {100} mini-faces. APBs near the edges of BT nanoparticles were formed by the intersection of two crystalline parts with displacement deviation from each other by 1/2<sub>d110</sub>.",no8654,Advances in scatter correction for 3D PET/CT,"We report on several significant improvements to the implementation of image-based scatter correction for 3D PET and PET/CT. Among these advances are: a new algorithm to scale the estimated scatter sinogram to the measured data, thereby largely compensating for external scatter; the ability to handle CT image truncation during this scaling; the option to iterate the scatter calculation for improved accuracy; the use of ordered subset estimation maximization (OSEM) reconstruction for the estimated emission images from which the scatter contributions are simulated; reporting of data quality parameters such as scatter and randoms fractions, and noise equivalent count rate (NECR), for each patient bed position; and extensive quality control output. Scatter correction (2 iterations, OSEM) typically requires 15-45 sec per bed. Very good agreement between the estimated scatter and measured emission data for several typical clinical scans is reported for CPS Pico-3D and HiRez LSO PET/CT systems. The data characteristics extracted during scatter correction are useful for patient specific count rate modeling and scan optimization",no8653,Honeypots: practical means to validate malicious fault assumptions,"We report on an experiment run with several honeypots for 4 months. The motivation of this work resides in our wish to use data collected by honeypots to validate fault assumptions required when designing intrusion-tolerant systems. This work in progress establishes the foundations for a feasibility study into that direction. After a review of the state of the art with respect to honeypots, we present our test bed, discuss results obtained and lessons learned. Avenues for future work are also proposed.",no8652,Data-driven fault diagnosis of oil rig motor pumps applying automatic definition and selection of features,"We report about fault diagnosis experiments to improve the maintenance quality of motor pumps installed on oil rigs. We rely on the data-driven approach to the learning of the fault classes, i.e. supervised learning in pattern recognition. Features are extracted from the vibration signals to detect and diagnose misalignment and mechanical looseness problems. We show the results of automatic pattern recognition methods to define and select features that describe the faults of the provided examples. The support vector machine is chosen as the classification architecture.",no8651,A general design technique for fault diagnostic systems,"We put forward a design method for fault diagnostic systems (FDSs) by proposing a fault model and using the incremental hybrid learning algorithm which tightly combines symbolic learning and neural networks. It is capable of overcoming several shortcomings in existing diagnostic systems, such as the lack of universality, the unbalance in the use of fault prior knowledge and the dynamic data and the dilemma of stability and plasticity. Experiment showed the FDS implemented by this kind of method had a good diagnostic ability",no8650,Error analysis of 3D motion estimation algorithms in the differential case,"We put forth in this paper a geometrically motivated 3D (three-dimensional) motion error analysis, which is capable of supporting investigation of global effect such as inherent ambiguities. The error expression that we derive allows us to predict the exact conditions likely to cause ambiguities and how these ambiguities vary with motion types such as lateral or forward motion. Our formulation, though geometrically motivated, is employed to model the effect of noise.",no8649,"Fault diameter and fault tolerance of HCN(n,n)","We provide the way to make an n+1 node disjoint parallel path between any two nodes of HCN(n,n) which has a better network cost than the hypercube, and prove that the fault diameter of HCN(n,n) is dia(HCN(n,n))+4 by result. These parallel paths can reduce the time of transmitting messages between nodes, and they mean that if some nodes of HCN(n,n) would fail, there is still no communication delay time. Also, by analyzing the fault tolerance of the interconnection network HCN(n,n), we prove that there is maximal fault tolerance",no8648,"Impact of Channel Errors on Decentralized Detection Performance of Wireless Sensor Networks: A Study of Binary Modulations, Rayleigh-Fading and Nonfading Channels, and Fusion-Combiners","We provide new results on the performance of wireless sensor networks in which a number of identical sensor nodes transmit their binary decisions, regarding a binary hypothesis, to a fusion center (FC) by means of a modulation scheme. Each link between a sensor and the fusion center is modeled independent and identically distibuted (i.i.d.) either as slow Rayleigh-fading or as nonfading. The FC employs a counting rule (CR) or another combining scheme to make a final decision. Main results obtained are the following: 1) in slow fading, a) the correctness of using an average bit error rate of a link, averaged with respect to the fading distribution, for assessing the performance of a CR and b) with proper choice of threshold, on/off keying (OOK), in addition to energy saving, exhibits asymptotic (large number of sensors) performance comparable to that of FSK; and 2) for a large number of sensors, a) for slow fading and a counting rule, given a minimum sensor-to-fusion link SNR, we determine a minimum sensor decision quality, in order to achieve zero asymptotic errors and b) for Rayleigh-fading and nonfading channels and PSK (FSK) modulation, using a large deviation theory, we derive asymptotic error exponents of counting rule, maximal ratio (square law), and equal gain combiners.",no8647,On undetectable faults in partial scan circuits,We provide a definition of undetectable faults in partial scan circuits under a test application scheme where a test consists of primary input vectors applied at-speed between scan operations. We also provide sufficient conditions for a fault to be undetectable under this test application scheme. We present experimental results on finite-state machine benchmarks to demonstrate the effectiveness of these conditions in identifying undetectable faults.,no8646,High-speed error correcting code LSI with throughput of 5 to 48 Gbps,"We proved that the hardware implementation of the proposed code and the new packet synchronization system was effectively realized by using a unique circuit configuration. A three-dimensional size-five coder and decoding-synchronization system was implemented on FPGA. The developed FPGA was applied to a high-speed MPEG communication device, which can transmit a movie signal of 20 Mbps.",no8645,The Unbounded-Error Communication Complexity of Symmetric Functions,"We prove an essentially tight lower bound on the unbounded-error communication complexity of every symmetric function, i.e.,f(x,y)=D(|x Lambda y|), where D:{0,1,...,n}-rarr{0,1} is a given predicate and x,y range over {0,1}<sup>n</sup>. Specifically, we show that the communication complexity of f is between Theta(k/log<sup>5</sup> n) and Theta(k log n), where k is the number of value changes of D in {0,1,...,n}. The unbounded-error model is the most powerful of the basic models of communication (both classical and quantum), and proving lower bounds in it is a considerable challenge. The only previous nontrivial lower bounds for explicit functions in this model appear in the ground breaking work of Forster (2001) and its extensions. Our proof is built around two novel ideas. First, we show that a given predicate D gives rise to a rapidly mixing random walk on Z<sub>2</sub> <sup>n</sup>, which allows us to reduce the problem to communication lower bounds for typical predicates. Second, we use Paturi's approximation lower bounds (1992), suitably generalized here to clusters of real nodes in [0,n] and interpreted in their dualform, to prove that a typical predicate behaves analogous to PARITY with respect to a smooth distribution on the inputs.",no8644,Constellation space invariance of orthogonal space-time block codes with application to evaluation of the exact probability of error,"We prove a new interesting property of space-time block codes (STBCs) that are based on generalized orthogonal designs. For flat block-fading channels, it is shown that the internal structure of the vector space of the input constellation remains invariant to the combined effect of the STBC and the channel, except for a scaling factor. The established constellation space invariance property is entirely due to the specific structure of the STBCs based on the generalized orthogonal designs. Using this property, we obtain simple exact expressions for the error probability of the maximum likelihood (ML) decoder in the general case when the channel, STBC, and input signal constellations are arbitrary. Such expressions are obtained in both the cases when the channel realization is deterministic (fixed) and random. In the latter case, simple expressions are derived for the average error probability.",no8643,MI-based correction of intensity inhomogeneity using singularity function analysis,"We proposed a new approach for correcting intensity nonuniformity (intensity inhomogeneity, bias field, or shading), that hampers the use of automatic image processing techniques. The intensity nonuniformity is perceived as a smooth intensity variation across the image. The proposed approach is on the basis of singularity function analysis, and mutual information is criterion of stopping iterative process. The rationale is that the low frequency component containing bias field is removed, followed by reconstruction from its high frequency component not containing bias field. Then the estimated bias field and the corrected image can be obtained",no8642,An effective schedulability analysis for fault-tolerant hard real-time systems,"We propose worst-case response time schedulability analysis for fault-tolerant hard real-time systems which takes into account the effects of temporary faults. The major contribution of our approach is to consider the recovery of tasks running with higher priorities. This characteristic is very useful since faulty tasks certainly have a shorter period of time to meet their deadlines. Due to its flexibility and simplicity, the proposed approach provides an effective schedulability analysis, where system predictability can be fully guaranteed",no8641,Tunable Infrared Semiconductor Lasers Based on Electromagnetically Induced Optical Defects,"We propose tunable midinfrared laser systems based on dynamic formation of electromagnetically induced optical defect sites. Such defects occur in a waveguide structure having a uniformly corrugated quantum well structure in the absence of any structural defect or phase slip. In the absence of a control laser field, such a corrugated structure causes a uniform perturbation of refractive index along the waveguide. However, when a relatively small region of such a waveguide structure is illuminated from the side by the control laser field, electromagnetically induced transparency occurs in this region while its refractive index corrugation is removed. We also show that such a coherently induced defect can be dynamically moved along the waveguide structure by just steering the control laser beam to illuminate different locations. We utilize the fact that the phase associated with this defect site can be adjusted via changing the length of the illuminated region to present a tunable distributed feedback laser where its lasing wavelength can be continuously varied within the stop-band. We study the case where two coherently induced defect sites happen along the waveguide structure and discuss the impacts of illumination of the whole waveguide structure with the control laser field. We show that the latter can either make the waveguide coherently transparent by destroying its refractive index perturbation and stop-band, or generate a gain-without-inversion grating. Formation of such a grating allows the waveguide to act as a tunable partly gain-coupled distributed feedback laser.",no8640,Mutation-based diagnostic test generation for hardware design error diagnosis,We propose the use of mutation-based error injection to guide the generation of high-quality diagnostic test patterns. A software-based fault localization technique is employed to derive a ranked candidate list of suspect statements. Experimental results for a set of Verilog designs demonstrate that a finer diagnostic resolution can be achieved by patterns generated by the proposed method.,no8639,The robust middleware approach for transparent and systematic fault tolerance in parallel and distributed systems,"We propose the robust middleware approach to transparent fault tolerance in parallel and distributed systems. The proposed approach inserts a robust middleware between algorithms/programs and system architecture/hardware. With the robust middleware, hardware faults are transparent to algorithms/programs so that ordinary algorithms/programs developed for fault-free networks can run on faulty parallel/distributed systems without modifications. Moreover, the robust middleware automatically adds fault tolerance capability to ordinary algorithms/programs so that no hardware redundancy or reconfiguration capability is required and no assumption is made about the availability of a complete subnetwork (at a lower dimension or smaller size). We also propose nomadic agent multithreaded programming as a novel fault-aware programming paradigm that is independent of network topologies and fault patterns. Nomadic agent multithreaded programming is adaptive to fault/traffic/workload patterns, and can take advantages of various components of the robust middleware, including the fault tolerance features and multiple embeddings, without relying on specialized robust algorithms",no8638,RACE: a software-based fault tolerance scheme for systematically transforming ordinary algorithms to robust algorithms,"We propose the robust algorithm-configured emulation (RACE) scheme for efficient parallel computation and communication in the presence of faults. A wide variety of algorithms originally designed for fault-free meshes, tori, and k-ary n-cubes can be transformed to corresponding robust algorithm through RACE. In particular optimal robust algorithms can be derived for total exchange (TE) and ascend/descend operations with a factor of 1+o (1) slowdown. Also, RACE can tolerate a large number of faulty elements, without relying on hardware redundancy or any assumption about the availability of a complete subarray",no8637,BOAs: backoff adaptive scheme for task allocation with fault tolerance and uncertainty management,"We propose the backoff adaptive scheme (BOAs) as a new technique for the automatic allocation of tasks amongst a team of heterogeneous mobile robots. It is an optimal, decentralized decision making scheme that utilizes explicit communication between the agents. A structured and unified framework is also proposed for task specification. This scheme is fault tolerant (to robot malfunctions) and allows for uncertainty in the nature of task specification in terms of the actual number of robots required. Team demography may change without the need for the respecification of tasks. The adaptive feature in BOAs further improves the flexibility of the team. Realistic simulations are carried out to verify the effectiveness of the scheme.",no8636,Improved error concealment algorithms based on H.264/AVC non-normative decoder,"We propose several improved error concealment (EC) algorithms based on the H.264/AVC non-normative decoder. The major differences are that motion compensated EC is introduced for intra frames, whereas spatial EC is introduced for inter frames. As for the EC of intra frames, scene change detection, motion activity detection and MV retrieval are hierarchically performed to decide whether spatial or temporal information is to be used. As for the EC of inter frames, scene change is also detected to avoid merging the scenes from different video shots. Therefore, the main idea of the proposed algorithms is that both spatial and temporal correlations are utilized for the EC of intra and inter frames. Both subjective and objective simulations under Internet conditions show that the proposed algorithm greatly outperforms that in the H.264/AVC non-normative decoder.",no8635,Design and implementation of error control algorithms for Bluetooth system: open-loop and closed-loop algorithms,We propose open-loop and closed-loop link quality control (LQC) algorithms using the correlation output of the access code for a short-range radio network. The new schemes can decrease the number of retransmissions and data overhead which result in improving the throughput of the Bluetooth system without extra hardware burden.,no8634,Optimal fault-tolerant routing scheme for generalized hypercube,"We propose node path vector (NPV) to capture complete shortest routing information for a generalized hypercube system. We also introduce the concept of relay node technique to reduce the computation complexity in obtaining NPV. Optimal fault-tolerant routing scheme (OFTRS) is further proposed to derive an optimal or sub-optimal routing path for any communication pair in a generalized hypercube system. Compared to previous work, OFTRS does not omit any routing information for optimal and sub-optimal path even in a generalized hypercube system with large number of faulty nodes and links while the previous schemes can potentially omit 60% routing paths. Thus it considerably improves the quality of fault-tolerant routing. In addition, our proposed scheme is distributed and relying only on non-faulty neighboring nodes, thus it has high applicability. Finally, the algorithm guarantees to route through the optimal or sub-optimal path as long as a path between the source-destination pair exists.",no8633,Error resilient transcoding of Scalable Video bitstreams,"We propose in this paper a novel error resilient transcoding scheme that can be placed at the boundary between wired and wireless networks via heterogeneous network links. This error resilient transcoder shall seamlessly complement the standard scalable video coding (SVC) bitstream to offer additional error resilient adaptation capability for receiving devices. The novel error resilient transcoding scheme consists of three different modules; each is designed to meet various levels of complexity need. The three modules are all based on the loss-aware rate-distortion optimization (LA-RDO) mode decision algorithm we have previously developed for SVC. However, each individual module can be tailored to different complexity requirements depending on whether and how the LA-RDO mode decision is implemented. Another innovation of this approach is the design of a fast rate control algorithm in order to maintain consistent bitrates between input and output of the transcoder. This rate control algorithm only needs picture-level bit information for training target quantization parameters. Simulation results demonstrate that, comparing with standard SVC, the proposed approach is able to achieve up to 4 dB gain for the enhancement layer video and up to 1 dB gain for the base layer video.",no8632,FlexiMAC: A flexible TDMA-based MAC protocol for fault-tolerant and energy-efficient wireless sensor networks,"We propose FlexiMAC, a novel TDMA-based protocol for efficient data gathering in wireless sensor networks that provides end-to-end guarantees on data delivery: throughput, fair access, and robust self-healing, whilst also respecting the severe energy and memory constraints of wireless sensor networks. Flex-iMAC achieves this balance through a synchronized and flexible slot structure in which nodes in the network can build, modify, or extend their scheduled number of slots during execution, based on their local information. This scheme allows FlexiMAC to be strongly fault tolerant and highly energy efficient. FlexiMAC further minimizes energy by selecting optimum node transmission power for a given topology. FlexiMAC is scalable for large number of nodes because it allows communication slots to be reused by nodes outside each others' interference range, and its depth-first-search schedule minimizes buffering. Simulations show that FlexiMAC ensures energy efficiency and is robust to network dynamics (faults such as dropped packets, nodes joining or leaving the network) under various network configurations.",no8631,Lowering Error Floors Using Dithered Belief Propagation,"We propose dithered belief propagation decoding algorithms to reduce the number of decoding failures of a belief propagation decoder and lower the error floor. The random nature of the algorithms enables a low hardware complexity compared to previously reported techniques. We introduce two dithering methods that target check node operations and channel input values, respectively. We present simulation results that confirm the error rate gains in the floor region, and that relate those gains with the maximum number of decoding iterations. The results show that the first algorithm can achieve good error rate gains with a low iteration limit. For the second algorithm, results show that with a large iteration limit, high FER gains are possible. Furthermore the average time complexity remains the same as that of a standard belief propagation algorithm.",no8630,Using an RBF Neural Network to Locate Program Bugs,"We propose an RBF (radial basis function) neural network-based fault localization method to help programmers locate bugs in a more effective way. An RBF neural network with a three-layer feed-forward structure is employed to learn the relationship between the statement coverage of a test case and its corresponding execution result. The trained network is then given as input a set of virtual test cases, each covering only a single statement. The output of the network for each test case is considered to be the suspiciousness of the corresponding statement; a statement with a higher suspiciousness has a higher likelihood of containing a bug. The set of statements ranked in descending order by their suspiciousness are then examined by programmers one by one until a bug is located. Three case studies on different programs (space, grep and make) were conducted with each faulty version having exactly one bug. An additional program gcc was also used to demonstrate the concept of extending the proposed method to programs with multiple bugs. Our experimental data suggest that an RBF neural network-based fault localization method is more effective in locating a program bug (by examining less code before the first faulty statement containing the bug is identified) than another popular method, Tarantula, which also uses the coverage and execution results to compute the suspiciousness of each statement.",no8629,An Intelligent Error Detection Model for Reliable QoS Constraints Running on Pervasive Computing,"We propose an intelligence predictive model for reliable QoS constraints running on pervasive computing. FTA is a system that is suitable for detecting and recovering software error based on pervasive computing environment as RCSM(Reconfigurable Context-Sensitive Middleware) by using software techniques. One of the methods to detect error for session's recovery inspects process database periodically. But this method has a weak point of inspecting all processes without regard to session. Therefore, we propose FTA. This method detects error by inspecting by hooking method. If an error is found, FTA informs GSM of the error. GSM informs Daemon or SA-SMA of the error. Daemon creates SA-SMA and so on. SA-SMA creates Video Service Provide Instance and so on.",no8628,Symbol recognition by error-tolerant subgraph matching between region adjacency graphs,We propose an error-tolerant subgraph isomorphism algorithm formulated in terms of region adjacency graphs (RAG). A set of edit operations to transform one RAG into another one are defined as regions are represented by polylines and string matching techniques are used to measure their similarity. The algorithm follows a branch and bound approach driven by the RAG edit operations. This formulation allows matching computing under distorted inputs and also reaching a solution in a near polynomial time. The algorithm has been used for recognizing symbols in hand drawn diagrams,no8627,Combinational circuit fault diagnosis using logic emulation,"We propose an emulation-based diagnosis technique for combinational circuits in this paper. To verify our approach, a hardware emulator is implemented by using Altera MAX+Plus II CPLD Development System. Our approach reduces the CPU time required by a software-based diagnosis technique significantly, and greatly eliminates the hardware requirements with circuit partitioning techniques and novel fault injection elements (FIEs). Moreover, our diagnosis algorithm also decreases the times of simulation when performing diagnosis. Experimental results for ISCAS-85 benchmark circuits show that our emulation system is 45 times faster than Kokan's (1999) on the average.",no8626,An extreme value injection approach with reduced learning time to make MLNs multiple-weight-fault tolerant,"We propose an efficient method for making multilayered neural networks(MLN) fault-tolerant to all multiple weight faults in an interval by injecting intentionally two extreme values in the interval in a learning phase. The degree of fault-tolerance to a multiple weight fault is measured by the number of essential multiple links. First, we analytically discuss how to choose effectively the multiple links to be injected, and present a learning algorithm for making MLNs fault tolerant to all multiple (i.e., simultaneous) faults in the interval defined by two multi-dimensional extreme points. Then it is shown that after the learning algorithm successfully finishes, MLNs become fault tolerant to all multiple faults in the interval. The time in a weight modification cycle is almost linear for the fault multiplicity. The simulation results show that the computing time drastically reduces as the multiplicity increases.",no8625,Efficient Utilization of Error Protection Techniques for Transmission of Data-Partitioned H.264 Video in a Capacity Constrained Network,"We propose an efficient error protection technique for data-partitioned H.264 video in a capacity constrained network. Our scheme maximizes video quality by choosing the optimal point in the application layer and medium access control (MAC) layer redundancy. We have shown that, in a capacity constrained network and highly lossy environment, neither forward error correction (FEC) nor retransmissions alone can result in optimum performance. Instead, it is the combination of these two techniques that effectively reduces the overall loss.",no8624,"A Distributed (Constant of R, 2)-Approximation Algorithm for Fault-Tolerant Facility Location","We propose an approximation algorithm for the problem of Fault-Tolerant Facility Location which is implemented in a distributed and asynchronous manner within O(n) rounds of communication. Here n is the number of vertices in the network. As far as we know, the performance guarantee of similar algorithms (centralized) remains unknown except a special case where all cities have a uniform connectivity requirement. In this paper, we assume the shortest-path routing scheme deployed, as well as a constant (given) size of R, which represents the distinct levels of fault-tolerant capability provided by the system (i. e distinct connectivity requirements), and prove that the cost of our solution is no more than |R| A F* + 2 A C* in the general case, where F* and C* are respectively the facility cost and connection cost in an optimal solution. Further more, extensive numerical experiments showed that the quality of our solutions is comparable to the optimal solutions when |R| is no more than 10.",no8623,Construction of an Agent-based Fault-Tolerant Object Group Model,"We propose an agent-based fault-tolerant object group (AFTOG) model for achieving effective object management and reliable fault recovery. We define five kinds of agents as internal processing agent, registration agent, state handling agent, user interface agent, and service agent. The roles of the agents in the proposed model are not only to reduce the remote interactions between distributed objects but also to guarantee more effective service execution. Through the simulations, it is validated that the proposed model decreases the interactions of the object components and supports effective fault recovery, while providing more stable and reliable services.",no8622,Adaptive unequal error control for video over the Internet,"We propose an adaptive unequal error protection scheme to help protect a video stream over the Internet. The data partition approach and the resynchronization marker are applied to generate two types of packets with different importance. Our scheme is very adaptive to network traffic conditions and can be easily implemented via software. With the proposed scheme, more protection is provided for the more important packets. The final quality can therefore be improved at a higher packet loss ratio.",no8621,Keynote: Hierarchical Fault Detection in Embedded Control Software,"We propose a two-tiered hierarchical approach for detecting faults in embedded control software during their runtime operation: The observed behavior is monitored against the appropriate specifications at two different levels, namely, the software level and the controlled-system level. (The additional controlled- system level monitoring safeguards against any possible incompleteness at the software level monitoring.) A software fault is immediately detected when an observed behavior is rejected by a software level monitor. In contrast, when a system level monitor rejects an observed behavior it indicates a system level failure, and an additional isolation step is required to conclude whether a software fault occurred. This is done by tracking the executed behavior in the system model comprising of the models for the software and those for the nonfaulty hardware components: An acceptance by such a model indicates the presence of a software fault. The design of both the software-level and system-level monitors is modular and hence scalable (there exists one monitor for each property), and further the monitors are constructed directly from the property specifications and do not require any software or system model. Such models are required only for the fault isolation step when the detection occurs at the system level. We use input-output extended finite automata (I/O- EFA) for software as well as system level modeling, and also for modeling the property monitors. Note since the control changes only at the discrete times when the system/environment states are sampled, the controlled- system has a discrete-time hybrid dynamics which can be modeled as an I/O-EFA.",no8620,A serial unequal error protection code system using trellis coded modulation and an adaptive equalizer for fading channels,"We propose a serial unequal error protection (UEP) scheme using trellis coded modulation and an adaptive equalizer for use in mobile fading channel communication environments. We propose two types of signal constellations, TRAP and RING, to realize unequal error protection and show their performance in fading channels using computer simulations.",no8619,Transient-fault recovery using simultaneous multithreading,"We propose a scheme for transient-fault recovery called Simultaneously, and Redundantly Threaded processors with Recovery (SRTR) that enhances a previously proposed scheme for transient-fault detection, called Simultaneously and Redundantly Threaded (SRT) processors. SRT replicates an application into two communicating threads, one executing ahead of the other. The trailing thread repeats the computation performed by the leading thread, and the values produced by the two threads are compared. In SRT a leading instruction may commit before the check for faults occurs, relying on the trailing thread to trigger detection. In contrast, SRTR must not allow any leading instruction to commit before checking occurs, since a faulty instruction cannot be undone once the instruction commits. To avoid stalling leading instructions at commit while waiting for their trailing counterparts, SRTR exploits the time between the completion and commit of leading instructions. SRTR compares the leading and trailing values as soon as the trailing instruction completes, typically before the leading instruction reaches the commit point. To avoid increasing the bandwidth demand on the register file for checking register values, SRTR uses the register value queue (RVQ) to hold register values for checking. To reduce the bandwidth pressure on the RVQ itself SRTR employs dependence-based checking elision (DBCE). By reasoning that faults propagate through dependent instructions, DBCE exploits register (true) dependence chains so that only the last instruction in a chain uses the RVQ, and has the leading and trailing values checked. SRTR performs within 1% and 7% of SRT for SPEC95 integer and floating-point programs, respectively. While SRTR without DBCE incurs about 18% performance loss when the number of RVQ ports is reduced from four (which is performance-equivalent to an unlimited number) to two ports, with DBCE, a two-ported RVQ performs within 2% of a four-ported RVQ",no8618,Design of fault detection filters for periodic systems,"We propose a numerically reliable computational approach to design fault detection filters for periodic systems. This approach is based on a new numerically stable algorithm to compute least order annihilators without explicitly building time-invariant lifted system representations. The main computation in this algorithm is the orthogonal reduction of a periodic matrix pair to a periodic Kronecker-like form, from which the periodic realization of the detector is directly obtained.",no8617,Measuring HMM similarity with the Bayes probability of error and its application to online handwriting recognition,"We propose a novel similarity measure for hidden Markov models (HMMs). This measure calculates the Bayes probability of error for HMM state correspondences and propagates it along the Viterbi path in a similar way to the HMM Viterbi scoring. It can be applied as a tool to interpret misclassifications, as a stop criterion in iterative HMM training or as a distance measure for HMM clustering. The similarity measure is evaluated in the context of online handwriting recognition on lower case character models which have been trained from the UNIPEN database. We compare the similarities with experimental classifications. The results show that similar and misclassified class pairs are highly correlated. The measure is not limited to handwriting recognition, but can be used in other applications that use HMM based methods",no8616,A mobile multicast protocol with error control for IP networks,"We propose a new protocol to achieve fault recovery of multicast applications in an IP internetwork with mobile participants. Our protocol uses the basic unicast routing capability of IETF Mobile IP as the foundation to leverage existing static host reliable IP multicast models to provide reliable multicast services for mobile hosts as well. We believe that the resulting scheme is simple, scalable, transparent, and independent of the underlying multicast routing facility. A key feature of our protocol is the use of a multicast forwarding agent (MFA) to address the scalability and reliability issues in the reliable mobile multicast applications. Our simulation results show the distinct performance advantages of our protocol using MFAs over two other approaches proposed for the mobile multicast service, namely mobile multicast protocol (MoM) and bi-directional tunneling, particularly as the number of mobile group members and home agents increases",no8615,Acceleration of Byzantine Fault Tolerance by Parallelizing Consensuses,We propose a new method that accelerates existing Byzantine Fault Tolerance (BFT) protocols for asynchronous distributed systems by parallelizing the involved consensuses. BFT realizes a reliable system against Byzantine failures and is usually solved by repeatedly executing consensus for a set of requests. Our method consistently parallelizes the consensus by introducing a new extra consensus on the order of processing agreed requests. We show the correctness of our method and analyze its performance in comparison with an existing non-parallelizing method and a naively parallelizing method. The results indicate that our parallelizing method is approximately 20% faster than those methods in such configurations where many replicas are running in order to increase reliability.,no8614,Model-based fault localization in large-scale computing systems,"We propose a new fault localization technique for software bugs in large-scale computing systems. Our technique always collects per-process function call traces of a target system, and derives a concise execution model that reflects its normal function calling behaviors using the traces. To find the cause of a failure, we compare the derived model with the traces collected when the system failed, and compute a suspect score that quantifies how likely a particular part of call traces explains the failure. The execution model consists of a call probability of each function in the system that we estimate using the normal traces. Functions with low probabilities in the model give high anomaly scores when called upon a failure. Frequently-called functions in the model also give high scores when not called. Finally, we report the function call sequences ranked with the suspect scores to the human analyst, narrowing further manual localization down to a small part of the overall system. We have applied our proposed method to fault localization of a known non-deterministic bug in a distributed parallel job manager. Experimental results on a three-site, 78-node distributed environment demonstrate that our method quickly locates an anomalous event that is highly correlated with the bug, indicating the effectiveness of our approach.",no8613,A rectilinear-monotone polygonal fault block model for fault-tolerant minimal routing in mesh,"We propose a new fault block model, minimal-connected-component (MCC), for fault-tolerant adaptive routing in mesh-connected multiprocessor systems. This model refines the widely used rectangular model by including fewer nonfaulty nodes in fault blocks. The positions of source/destination nodes relative to faulty nodes are taken into consideration when constructing fault blocks. The main idea behind it is that a node will be included in a fault block only if using it in a routing will definitely make the route nonminimal. The resulting fault blocks are of the rectilinear-monotone polygonal shapes. A sufficient and necessary condition is proposed for the existence of the minimal ""Manhattan"" routes in the presence of such fault blocks. Based on the condition, an algorithm is proposed to determine the existence of Manhattan routes. Since MCC is designed to facilitate minimal route finding, if there exists no minimal route under MCC fault model, then there will be absolutely no minimal route whatsoever. We also present two adaptive routing algorithms that construct a Manhattan route avoiding all fault blocks, should such routes exist.",no8612,Error concealment for spatially Scalable Video Coding using hallucination,"We propose a new error concealment method based on hallucination for Scalable Video Coding with spatial scalability. In this method, parts of the frames which lose the enhancement layer are up-sampled from base layer and ldquohallucinatedrdquo as concealment frames. The database for hallucination is generated from the high-resolution and low-resolution frame-pairs near the lost frames in the video sequence. The effectiveness of hallucination here lies in the similarity between the nearby frames in the video sequence. Experiments show that the proposed method has superior results over the state-of-the-art error concealment method for spatially Scalable Video Coding.",no8611,Automating detection of faults in TCP implementations,"We propose a new environment for testing the behavior of TCP. We analyze existing test methodologies and show that there is still a need to reduce the loads imposed on both the worker and the expert. The Auto Detector is extensible and so well supports both the automation of predefined tasks and the communication tools needed to ensure gradual refinement by the participants. To show the effectiveness of our proposal, actual software flaws found in HP/UX and the Windows operating system are taken as examples of how to realize the automation of reproduction and detection tasks; it is flexible enough to resolve real-world issues.",no8610,Static ownership inference for reasoning against concurrency errors,We propose a new approach for reasoning about concurrency in object-oriented programs. Central to our approach is static ownership inference analysis - we conjecture that this analysis has important application in reasoning against concurrency errors.,no8609,A Fast QC Method for Testing Contact Hole Roughness by Defect Review SEM Image Analysis,"We propose a new and fast method for monitoring contact hole roughness (CHR), which can be a major yield-loss factor for advanced SRAMs. The method, defect-review scanning electron microscopy (SEM) image processing, can monitor CHR 100 times faster than the conventional method by critical dimension (CD)-SEM. The speed can facilitate faster identification of process countermeasures by, for example, making detailed monitoring of CHR variation within a wafer practicable. Results for CHR obtained by both new and conventional methods show similar trends for differences in process conditions. Also, we experimentally confirmed the new method's measuring variation of the rate of deformed contact holes.",no8608,Iterative refinement of range images with anisotropic error distribution,"We propose a method which refines the range measurement of range finders by computing correspondences of vertices of multiple range images acquired from various viewpoints. Our method assumes that a range image acquired by a laser rangefinder has anisotropic error distribution which is parallel to the ray direction. Thus, we find the corresponding points of range images along with the ray direction. We iteratively converge range images to minimize the distance of corresponding points. We demonstrate the effectiveness of our method by presenting the experimental results of artificial and real range data. Also, we show that our method refines a 3D shape more accurately as opposed to that achieved by using the Gaussian filter.",no8607,Fault isolation in discrete event systems by observational abstraction,"We propose a method for fault isolation in discrete event systems such as object oriented control systems, where the observations are the logged error messages. The method is based on automatic abstraction that preserves only the behavior relevant to fault isolation. In this way we avoid the state space explosion, and a model checker can be used to reason about the temporal properties of the system. The result is a fault isolation table that maps possible error logs to isolated faults, and fault isolation thus reduces to table lookup. The fault isolation table can also be used as an analysis tool at the design level to find both faults that cannot be isolated as well as redundant error messages.",no8606,BIST based fault diagnosis using ambiguous test set,"We propose a method for diagnosing single stuck-at faults under a built-in self-test (BIST) environment. Under the BIST environment, it is difficult to determine which BIST vectors produced errors due to the high degree of test response compaction. Therefore the detecting test set that is determined in BIST session includes non-detecting tests. We call the detecting test set determined after BIST session an ""ambiguous diagnostic test set"". Firstly, we propose a method for identifying candidate faults based on the ambiguous diagnostic test set. Moreover we propose a method for identifying candidate non-detecting tests that belong to the ambiguous diagnostic test set. Diagnosis by using more accurate diagnostic test sets is able to improve the diagnostic ambiguity.",no8605,A machine-learning-based fault diagnosis approach for intelligent condition monitoring,"We propose a machine-learning-based fault diagnosis approach for condition monitoring on the constant-speed rotating machines via vibration signals. There are mainly five phases in our approach, i.e., vibration signal measurement, discrete-wavelet-transformation-based preprocessing, feature extraction, base-line encoding, and fuzzy neural network. The advantage of this approach can identify the condition and faults of machine without sufficient diagnosis knowledge. Experimental results have demonstrated this approach is a useful tool for condition monitoring application.",no8604,Low Cost Correction of OCR Errors Using Learning in a Multi-Engine Environment,We propose a low cost method for the correction of the output of OCR engines through the use of human labor. The method employs an error estimator neural network that learns to assess the error probability of every word from ground truth data. The error estimator uses features computed from the outputs of multiple OCR engines. The output probability error estimate is used to decide which words are inspected by humans. The error estimator is trained to optimize the area under the word error ROC leading to an improved efficiency of the human correction process. A significant reduction in cost is achieved by clustering similar words together during the correction process. We also show how active learning techniques are used to further improve the efficiency of the error estimator.,no8603,HAFT: A hybrid FPGA with amorphous and fault-tolerant architecture,"We propose a hybrid FPGA architecture with a dense and defective nano-crossbar serving as its configuration memory. An amorphous routing architecture is adopted to optimally allocate logic and routing resource on per-mapping basis and to achieve high logic density. This hybrid FPGA is designed to be efficient in using nano-crosspoints, highly tolerant to memory defects, and versatile to provide features such as variable-granularity logic blocks and variable-length bypassing interconnects. A new placement algorithm and a modified delay-based routing procedure are designed to match with many unconventional architectural features of the proposed FPGA. Assuming zero defect-rate in the nano-crossbar, an FPGA with the proposed architecture can achieve a 30% improvement in logic density, 12% improvement in average net delay, and 8% improvement in the critical-path delay for the largest 20 MCNC benchmark circuits over an island-style baseline with the same nano-scale memory. As the rate of defects in the memory increases from 0% to 50%, this hybrid FPGA remains fully functional and its improvement in logic density and delay performance only drops by approximately 23%.",no8602,"An integrated decision, control and fault detection scheme for cooperating unmanned vehicle formations","We propose a hierarchical and decentralized scheme for integrated decision, control and fault detection in cooperating unmanned aerial systems flying in formations and operating in adversarial environments. To handle, in a cooperative fashion, events that may adversely affect the outcome of a multi-vehicle mission, events such as actuator faults, body damage, network interruption/delays, and vehicle loss, we present a decision-control system whose architecture comprises three main components: formation control and trajectory generation, abrupt and nonabrupt fault detection, and decision-making relying on optimization under uncertainty. The scheme seeks to provide the most effective team adaptation to contingencies despite partially known environments and limited available information. The integrated decision, control and fault detection scheme is demonstrated numerically by means of high-fidelity, nonlinear 6-DOF simulations of multiple formation flying airships. For a rendezvous mission, the paper shows that concurrent nonabrupt and abrupt type faults can be detected and effectively compensated for both at the formation control and at the decision-making levels, despite network mishaps, which represents a novelty in itself.",no8601,Proxy-Based Reference Picture Selection for Error Resilient Conversational Video in Mobile Networks,We propose a frame dependency management strategy for error robust transmission of conversational video in mobile networks. We consider an end-to-end video transmission scenario that involves both a wireless uplink as well as a wireless downlink plus some intermediate wireline network transmission. We also investigate the special cases of an end-to-end scenario where only a wireless uplink or a wireless downlink is present. We cope with packet loss on the downlink by retransmitting lost packets from the base station to the receiver for error recovery. Retransmissions are enabled by using fixed-distance reference picture selection during encoding with a prediction distance that corresponds to the round-trip time of the downlink combined with accelerated decoding. We deal with transmission errors on the uplink by sending acknowledgments and predicting the next frame to encode from those slices that have been correctly received by the base station. We show that these two separate approaches for uplink and downlink efficiently complement one another and the resulting end-to-end scheme is characterized by very low computational complexity. We compare our scheme to several state-of-the-art error resiliency approaches and report significant improvements.,no8600,A fault tolerant approach to microprocessor design,"We propose a fault-tolerant approach to reliable microprocessor design. Our approach, based on the use of an online checker component in the processor pipeline, provides significant resistance to core processor design errors and operational faults such as supply voltage noise and energetic particle strikes. We show through cycle-accurate simulation and timing analysis of a physical checker design that our approach preserves system performance while keeping area overheads and power demands low. Furthermore, analyses suggest that the checker is a fairly simple state machine that can be formally verified, scaled in performance, and reused. Further simulation analyses show virtually no performance impacts when our simple checker design is coupled with a high-performance microprocessor model. Timing analyses indicate that a fully synthesized unpipelined 4-wide checker component in 0.25 m technology is capable of checking Alpha instructions at 288 MHz. Physical analyses also confirm that costs are quite modest; our prototype checker requires less than 6% the area and 1.5% the power of an Alpha 21264 processor in the same technology. Additional improvements to the checker component are described which allow for improved detection of design, fabrication and operational faults.",no8599,Compression-free Checksum-based Fault-Detection Schemes for Pipelined Processors,"We propose a fault-detection scheme for pipelined, multithreaded processors. The scheme is based on checksums and improves on previous schemes in terms of fault coverage and detection latency by not using compression but storing complete checksums from several pipeline stages. We validate the scheme experimentally and derive checksum polynomials that lead to perfect fault coverage.",no8598,"A fault tolerant, peer-to-peer replication network","We propose a fault tolerant, peer-to-peer replication network for synchronizing files across multiple hosts. The proposed topology is constructed by applying existing technologies and tools to ensure that files are kept synchronized even after subsequent modifications. One of its main advantages lies in the fact that there is no central authority to coordinate the process, hosts are connected in a peer-to-peer fashion, thus avoiding a single point of failure. Our proposal is intended for use in networks of personal computers where a small number of hosts have to be synchronized.",no8597,Fault Management in Functionally Distributed Transport Networking for Large Scale Networks,"We propose a fault management method in functionally distributed transport networking that separates the control-plane processing part (control element, CE) from the forwarding-plane processing part (forwarding element, FE) of the router. In this architecture, one path-control process in the CE consolidates and processes the path computations and the path settings for multiple routers. This leads to reduction in the path-control complexity and efficient operation of large scale networks. On the other hand, if faults occur in a CE and the CE become unable to serve a routing function, all of the FEs controlled by the CE will be affected. Therefore, it is absolutely critical to ensure the high reliability of the CE in this architecture. The proposed method takes the redundant configuration of N+m CEs and switches from a fault CE to a standby CE. Additionally, we describe the operation of each component in the proposed method and evaluate its feasibility by using software implementation.",no8596,Observer-based fault diagnosis of power electronics systems,"We propose a fault diagnosis method for power electronics systems that extends classical observer-based fault-sensitive detection filters for linear time-invariant systems to switched-linear systems commonly encountered in power electronics. The result is a piecewise-linear detection filter, which in the absence of faults, works the same way as an observer-it predicts the system states exactly. If a fault occurs, the state predicted by the filter differs from the true state of the system, and by appropriately choosing the filter gain, the filter residual has certain geometrical characteristics that makes the fault identifiable. An experimental platform to verify the feasibility of the proposed method is presented along with simulation and experimental results illustrating the feasibility and effectiveness of the method.",no8595,Fault Detection System Activated by Failure Information,"We propose a fault detection system activated by an application when the application recognizes the occurrence of a failure, in order to realize self managing systems that automatically find the source of a failure. In existing detection systems, there are three issues for constructing self managing applications: i) the detection results are not sent to the applications, ii) they can not identify the source failure from all of the detected failures, and iii) configuring the detection system for networked system is hard work. For overcoming these issues, the proposed system takes three approaches: i) the system receives failure information from an application and returns a result set to the application, ii) the system identifies the source failure using relationships among errors, and Hi) the system obtains information of the monitored system from a database. The relationship is expressed by a tree. This tree is called error relationship tree. The database provides information which are system entities such as hardware devices, software object, and network topology. When the proposed system starts looking for the source of a failure, causal relations from an error relation tree are referred to, and the correspondence of error definitions and actual objects is derived using the database. We show the design of the detection operation activated by the failure information and the architecture of the proposed system.",no8594,A scalable fault tolerant approach to core election in an inter-domain multicast routing environment,"We propose a consensus protocol to rearrange the multicast core based tree, after a core failure. However, the applications of this protocol are not limited to core failure. It can be used in any situation where a common value should be fixed by consensus in an inter-domain routing environment. This protocol is a hierarchical (two-level) extension of the original proposal by Chandra and Toueg (1996). However, in order to use the algorithm in an inter-domain environment such as the Internet, we introduce randomization in our consensus protocol. Indeed, we minimise by this way the total number of rounds. A comparison with other consensus protocols shows that our solution gives a better complexity of the algorithm. This solution also opens new perspectives in terms of core election in an inter-domain multicast routing environment, since the classical approaches do not scale, or are based on manual configurations",no8593,Dynamic Test Compaction for Transition Faults in Broadside Scan Testing Based on an Influence Cone Measure,"We propose a compact test generation method for transition faults, which is driven by a conflict-avoidance scheme employed during test generation. Based on an influence-cone function for transition faults in broadside scan testing, two dynamic test compaction schemes, named selfish test compaction and unselfish test compaction respectively, are proposed. The selfish test compaction tries to compact as many faults as possible into the current test, while the unselfish scheme attempts to compact the tests of the hard-to-compact faults into the current test. Potential conflicts produced by the signal requirements at the pseudo-primary outputs in the first frame are avoided through the use of an input dependency graph. Experimental results and comparison with existing approaches demonstrate the efficiency and effectiveness of the proposed method.",no8592,A Classification-Based Fault Detection and Isolation Scheme for the Ion Implanter,"We propose a classification-based fault detection and isolation scheme for the ion implanter. The proposed scheme consists of two parts: 1) the classification part and 2) the fault detection and isolation part. In the classification part, we propose a hybrid classification tree (HCT) with learning capability to classify the recipe of a working wafer in the ion implanter, and a k-fold cross-validation error is treated as the accuracy of the classification result. In the fault detection and isolation part, we propose a warning signal generation criteria based on the classification accuracy to detect and fault isolation scheme based on the HCT to isolate the actual fault of an ion implanter. We have compared the proposed classifier with the existing classification software and tested the validity of the proposed fault detection and isolation scheme for real cases to obtain successful results",no8591,Effective and efficient localization of multiple faults using value replacement,"We previously presented a fault localization technique called value replacement that repeatedly alters the state of an executing program to locate a faulty statement [9]. The technique searches for program statements involving values that can be altered during runtime to cause the incorrect output of a failing run to become correct. We showed that highly effective fault localization results could be achieved by the technique on programs containing single faults. In the current work, we generalize value replacement so that it can also perform effectively in the presence of multiple faults. We improve scalability by describing two techniques that significantly improve the efficiency of value replacement. In our experimental study, our generalized technique effectively isolates multiple simultaneous faults in time on the order of minutes in each case, whereas in , the technique had sometimes required time on the order of hours to isolate only single faults.",no8590,Introduction to fault attacks on smartcard,We present what can be achieved by attacks through faults induction on smart cards. We first describe the different means to perform fault attacks on chips and explain how fault attacks on cryptographic algorithms are used to recover secret keys. We next study the impact of fault attacks when focused on the disruption of the functional software layer. We conclude with the overall impact of this type of attacks on the smartcard environment and the need for software countermeasures and their limits.,no8589,The Wackamole approach to fault tolerant networks,"We present Wackamole, a high availability tool for clusters of servers. Wackamole ensures that a server handles the requests that arrive on any of the service's public IP addresses. Wackamole is a completely distributed software solution based on a provably correct algorithm that negotiates the assignment of IP addresses among the available servers upon detection of faults and recoveries, and provides N-way fail-over, so that any one of a number of servers can cover for any other. Using a simple algorithm that utilizes strong group communication semantics, Wackamole demonstrates the application of group communication to address a critical availability problem at the core of the system, even in the presence of cascading network or server faults and recoveries. The same architecture is extended to provide a similar service for highly available routers.",no8588,Combinational Logic Soft Error Correction,"We present two techniques for correcting radiation-induced soft errors in combinational logic - error correction using duplication, and error correction using time-shifted outputs. Simulation results show that both techniques reduce combinational logic soft error rate by more than an order of magnitude. Soft errors affecting sequential elements (latches and flip-flops) at combinational logic outputs are automatically corrected using these techniques",no8587,Post-Error Correcting Code Modeling of Burst Channels Using Hidden Markov Models With Applications to Magnetic Recording,"We present two approaches for modeling burst channels using hidden Markov models (HMMs). The first method is based on the maximum-likelihood approach and improves on the computational efficiency of earlier methods. We present new algorithms for scaling and for determining the model parameters by using smart search techniques. We then generalize a gap length analysis and apply it to modeling HMMs. The algorithms are low-complexity and memory-efficient. Finally, we present simulation results for modeling errors in magnetic storage channels and show how this can be used for evaluating decoder failure rates by using Wolf's method, from real observed data",no8586,Using Web services for atmospheric correction of remote sensing data,"We present the technical implementation details for a prototype central atmospheric correction parameter server called NOMAD (Networked On-line Mapping of Atmospheric Data). Using a Web service, aerosol optical depth (AOD) values are transmitted via the network to a Web service aware atmospheric correction application. Information about the date and location of the image are extracted automatically from the image, allowing a best estimate of AOD at 500 nm to be retrieved from the central server database. A Web service approach was adopted to allow easy cross platform development in multiple software languages. Using the Web Services Definition Language (WSDL) description of the atmospheric correction parameter server Web service, application developers can easily make their atmospheric correction applications ""Web aware"". On the server side, researchers maintaining the central database are free to concentrate on providing the best quality data available.",no8585,Computer Modeling of YBCO Fault Current Limiter Strips Lines in Over-Critical Regime With Temperature Dependent Parameters,"We present the results of an advanced numerical model for fault current limiter (FCL) based on HTS thin films in which both thermal and electromagnetic aspects are taken into account. This model allows simulating the behavior of FCL in the over-critical current regime and we used it for studying strip lines of a YBCO/Au FCL on sapphire substrate. The electromagnetic and thermal equations have been implemented in finite-element method (FEM) software in order to obtain a model for investigating the comportment of the superconductor when the current exceeds I<sub>c</sub>. In particular, materials equations have been implemented in order to simulate the electrical behavior of superconducting devices with strong over-critical currents. We report results of simulations in voltage source mode where currents largely exceed I<sub>c</sub>. The global behavior of the FCL is compared with measurements, showing a good agreement. The use of FEM simulations offers the advantage to give access to local variables such as current density or temperature. Studies with this model can replace expensive experiments where very high current density might damage or destroy the FCL device.",no8584,RedCAN<sup>TM</sup>: simulations of two fault recovery algorithms for CAN,"We present the RedCAN concept to achieve fault tolerance against node and link failures in a CAN-bus system by means of configurable switches. The basic idea in RedCAN is to isolate faulty nodes or bus segments by configuring switches that will evade a faulty node or segment and exclude it from bus access. We propose changes to the original centralized protocol, vulnerable to single point failures, and show that with a new distributed algorithm considerable more efficiency can be achieved also when network size is growing. The distributed algorithm introduces redundancy and hereby increases robustness of the system. Furthermore, the new algorithm has logarithmic complexity, as opposed to the centralized algorithms linear complexity, as the number of nodes increase. The results were gathered through a new simulator, the ""RedCAN Simulation Manager"", also presented. Simulations allow assessing the break-even point between centralized and distributed algorithms reconfiguration latencies as well as give ideas for further research.",no8583,Principles of multi-level reflection for fault tolerant architectures,"We present the principles of multi-level reflection as an enabling technology for the design and implementation of adaptive fault tolerant systems. By exhibiting the structural and behavioral aspects of a software component, the reflection paradigm enables the design and implementation of appropriate non-functional mechanisms at a meta-level. The separation of concerns provided by reflective architectures makes reflection a perfect match for fault tolerance mechanisms. However, in order to provide the necessary and sufficient information for error detection and recovery, reflection must be applied to all system layers in an orthogonal manner. This is the main motivation behind the notion of multi-level reflection that is introduced. We describe the basic concepts of this new architectural paradigm, and illustrate them with concrete examples. We also discuss some practical work that has recently been carried out to start implementing the proposed framework.",no8582,Communication protocols for a fault-tolerant automated highway system,"We present the design and verification of inter-vehicle communication protocols for the operation of an automated highway system in the presence of faults. The protocols form part of a fault-tolerant control hierarchy proposed in earlier work. Our goal here is to implement discrete-event supervisory controllers to stop the faulty vehicle or take it out of the highway in a safe manner. Because these actions require cooperation among vehicles in the neighborhood of the faulty vehicle, the supervisory controllers are implemented by means of inter-vehicle communication protocols. The logical correctness of the proposed protocols is verified using automatic verification tools. We discuss the safety of the proposed design in terms of the possibility of collisions and highlight the problems associated with carrying out a complete safety analysis",no8581,Design and implementation of color correction system for images captured by digital camera,"We present the design and implementation of a color correction system for images captured by digital camera. In general, photographing is affected by various factors such as objective camera settings and many environmental contents as well as individual user's skill. So it is not unusual for common users to take unnatural photographs which have inaccurate colors. Although there have been considerable research efforts for the color correction/reproduction, accurate handling of the color characteristics is still not a trivial task for the common users because it often requires specialized device, data format, and professional knowledge on the color science. Our goal in this paper is to develop an easy-to-use color correction system which is friendly to the average digital camera users. The experimental results show that the color correction problem can be greatly simplified by using the proposed system. The accuracy and robustness of the proposed system are also verified on the experimental results of several indoor and outdoor images.",no8580,Testing ThumbPod: Softcore bugs are hard to find,"We present the debug and test strategies used in the ThumbPod system for Embedded Fingerprint Authentication. ThumbPod uses multiple levels of programming (Java, C and hardware) with a hierarchy of programmable architectures (KVM on top of a SPARC core on top of an FPGA). The ThumbPod project teamed up seven graduate students in the concurrent development and verification of all these programming layers. We pay special attention to the strengths and weaknesses of our bottom-up testing approach.",no8579,A pragmatic approach to concurrent error detection in sequential circuits implemented using FPGAs with embedded memory,"We present several low-cost concurrent error detection schemes for a sequential circuit implemented using FPGAs with embedded memory blocks. The experimental results show that for many of the examined circuits, a reasonable level of error detection can be obtained at the circuitry overhead of less than 10% - a level recommended by proponents of a ""pragmatic"" approach to on-line testing.",no8578,Monitoring a tunneling in an urbanized area with Terrasar-X interferometry  Surface deformation measurements and atmospheric error treatment,"We present results from a deformation monitoring to demonstrate potential and limitations of TerraSAR-X interferometry to measure vertical displacements due to the tunneling of main sewerage pipes along the river Emscher in Germany. In spite of higher sensitivity for deformation gradients the potential for deformation monitoring benefits from high spatial and temporal resolution of the TerraSAR-X data. We analyzed a large stack of TerraSAR-X stripmap scenes to derive regional pattern of vertical displacements with differential SAR Interferometry and small-scale displacements and deformation of objects (infrastructure and houses) in time series of SAR-scenes with Persistent Scatterer Interferometry (PSI). First results from PSI are promising with a great number of detected PS. We show deformation measurements with Artificial Corner Reflectors. Short-time interferograms (11 or 22 days) show high coherence for large areas and therefore are likely less infected by unwrapping errors. Atmospheric errors are important for X-Band SAR. Expected deformation in our application is in the range of mm to cm, similar to tropospheric delay features in their spatial and temporal extent. The atmospheric phase screen in PSI and stacking procedures are smoothing the nonuniform deformation history of progressing tunneling.",no8577,The role of defects on CdTe detector performance,"We present results from a characterisation of bulk defects in CdTe wafers, and their role in the degradation of charge transport performance of CdTe radiation detectors. Sub-bandgap IR microscopy and X-ray Lang topography have been used to characterise material quality prior to device processing. IR microscopy clearly identifies extended defects such as tellurium precipitates in the material bulk, whilst Lang topography characterises stacking faults, crystallite boundaries and other crystallographic features in the near-surface region. After fabrication of contacts onto the material, ion beam induced charge imaging is used to investigate the correlations between material defects and charge transport. Digital ion beam induced charge imaging is used to produce high resolution maps of charge signal amplitude, carrier drift time, and carrier drift mobility.",no8576,Optical Proximity Correction for 0.13 micrometer SiGe:C BiCMOS,"We present results for a rule based optical proximity (RB-OPC) and a model based optical proximity correction (MB-OPC) for 0.13 micrometer SiGe:C BiCMOS technology. The technology provides integrated high performance heterojunction bipolar transistors (HBTs) with cut-off frequencies up to 300 GHz. This requires an optical proximity correction of critical layers with an excellent mask quality. This paper provides results of the MB-OPC and RB-OPC using the Mentor Calibre software in comparison to uncorrected structures (NO-OPC). We show RB- and MB-OPC methods for the shallow trench and gate layer, and the RB-OPC for the emitter window-, contact- and metal layers. We will discuss the impact of the RB- and MB-OPC rules on the process margin and yield in the 0.13 micrometer SiGe:C BiCMOS technology, based on CD-SEM data obtained from the evaluation of the RB- and MB-OPC corrected SRAM cells.",no8575,Corrective Maintenance Maturity Model: Problem Management,"We present our PhD thesis, in which we suggest a process model for handling software problems within corrective maintenance. Our model is called CM<sup>3</sup>: Problem Management.",no8574,Experiences with EtheReal: a fault-tolerant real-time Ethernet switch,"We present our experiences with the implementation of a real-time Ethernet switch called EtheReal. EtheReal provides three innovations for real-time traffic over switched Ethernet networks. First, EtheReal delivers connection oriented hard bandwidth guarantees without requiring any changes to the end host operating system and network hardware/software. For ease of deployment by commercial vendors, EtheReal is implemented in software over Ethernet switches, with no special hardware requirements. QoS support is contained within two modules, switches and end-host user level libraries that expose a socket like API to real time applications. Secondly, EtheReal provides automatic fault detection and recovery mechanisms that operate within the constraints of a real-time network. Finally EtheReal supports server-side push applications with a guaranteed bandwidth link-layer multicast scheme. Performance results from the implementation show that EtheReal switches deliver bandwidth guarantees to real time-applications within 0.6% of the contracted value, even in the presence of interfering best-effort traffic between the same pair of communicating hosts.",no8573,Improved exponential bounds and approximation for the Q-function with application to average error probability computation,"We present new exponential bounds for the Gaussian Q-function or, equivalently, of the complementary error function er f c(.). More precisely, the new bound is in the form of the sum of exponential functions that, in the limit, approaches the exact value. Then, a quite accurate and simple approximated expression given by the sum of two exponential functions is reported. Moreover, some new simple bounds for the inverse er f c(.) are derived. The results are applied to the general problem of evaluating the average error probability in fading channels. An example of application to the computation of the pairwise error probability of space-time codes is also presented.",no8572,New exponential bounds and approximations for the computation of error probability in fading channels,"We present new exponential bounds for the Gaussian Q function (one- and two-dimensional) and its inverse, and for M-ary phase-shift-keying (MPSK), M-ary differential phase-shift-keying (MDPSK) error probabilities over additive white Gaussian noise channels. More precisely, the new bounds are in the form of the sum of exponential functions that, in the limit, approach the exact value. Then, a quite accurate and simple approximate expression given by the sum of two exponential functions is reported. The results are applied to the general problem of evaluating the average error probability in fading channels. Some examples of applications are also presented for the computation of the pairwise error probability of space-time codes and the average error probability of MPSK and MDPSK in fading channels.",no8571,Evaluation of replication and fault detection in P2P-MPI,"We present in this paper an evaluation of fault management in the grid middleware P2P-MPI. One of P2P-MPI's objective is to support environments using commodity hardware. Hence, running programs is failure prone and a particular attention must be paid to fault management. The fault management covers two issues: fault-tolerance and fault detection. P2P-MPI provides a transparent fault tolerance facility based on replication of computations. Fault detection concerns the monitoring of the program execution by the system. The monitoring is done through a distributed set of modules called failure detectors. In this paper, we report results from several experiments which show the overhead of replication, and the cost of fault detection.",no8570,"An empirical study of modifying the Fagan inspection process and the resulting main effects and interaction effects among defects found, effort required, rate of preparation and inspection, number of team members and product 1st pass quality","We present findings from a six sigma black belt project. Every black belt project has a charter that defines the customer focus and the goals of the project. This project is designed to identify the key factors that impact effectiveness for software inspections and to compare Fagan inspections and modified Fagan inspections used at Motorola. Empirical data is collected and simulation models of the generic processes are created. The models that are created abstract away unnecessary details of the process and provide a test-bed to evaluate the methodologies relative to their effectiveness, cost in effort, time required (duration), and complexity of the activity.",no8569,Automated Support for Propagating Bug Fixes,"We present empirical results indicating that when programmers fix bugs, they often fail to propagate the fixes to all of the locations in a code base where they are applicable, thereby leaving instances of the bugs in the code. We propose a practical approach to help programmers to propagate many bug fixes completely. This entails first extracting a programming rule from a bug fix, in the form of a graph minor of an enhanced procedure dependence graph. Our approach assists the programmer in specifying rules by automatically matching simple rule templates; the programmer may also edit rules or compose them from scratch. A graph matching algorithm for detecting rule violations is then used to locate the places in the code base where the bug fix is applicable. Our approach does not require that rules occur repeatedly in the code base. We present empirical results indicating that the approach nevertheless exhibits good precision.",no8568,ConfErr: A tool for assessing resilience to human configuration errors,"We present ConfErr, a tool for testing and quantifying the resilience of software systems to human-induced configuration errors. ConfErr uses human error models rooted in psychology and linguistics to generate realistic configuration mistakes; it then injects these mistakes and measures their effects, producing a resilience profile of the system under test. The resilience profile, capturing succinctly how sensitive the target software is to different classes of configuration errors, can be used for improving the software or to compare systems to each other. ConfErr is highly portable, because all mutations are performed on abstract representations of the configuration files. Using ConfErr, we found several serious flaws in the MySQL and Postgres databases, Apache web server, and BIND and djbdns name servers; we were also able to directly compare the resilience of functionally-equivalent systems, such as MySQL and Postgres.",no8567,A component-based design of a fault-tolerant multimedia communication protocol,"We present component-based design of a fault-tolerant (FT) multimedia communication protocol. Specifically, we show how a fault-intolerant multimedia protocol can be transformed to be able to handle certain fault cases by composing it with FT components, thereby providing dependability attributes in the resulting FT version of the protocol. We present the design of the FT components, namely correctors and detectors, and then show their correctness as well as that of the final FT protocol. Utilizing the concepts of category theory, we elucidate the overall composition of FT components and the fault-intolerant program to result in a FT program. The proposed methodology is illustrated by transforming the label distribution protocol into its FT version considering specific failure scenarios.",no8566,"Cluster delegation: high-performance, fault-tolerant data sharing in NFS","We present cluster delegation, an enhancement to the NFSv4 file system, that improves both performance and recoverability in computing clusters. Cluster delegation allows data sharing among clients by extending the NFSv4 delegation model so that multiple clients manage a single file without interacting with the server. Based on cluster delegation, we implement a fast commit primitive, cooperative caching, and the ability to recover the uncommitted updates of a failed computer. Cluster delegation supports both read and write operations in the cooperative cache, while preserving the consistency guarantees of NFSv4. We have implemented cluster delegation by modifying the Linux NFSv4 client and show that it improves client performance and reduces server load by more than half.",no8565,Closed-form error analysis of the non-identical Nakagami-m relay fading channel,"We present closed-form expressions for the average bit error probability (ABEP) of BPSK, QPSK and M-QAM of an amplify-and-forward average power scaling dual-hop relay transmission, over non-identical Nakagami-m fading channels, with integer values of m. Additionally, we evaluate in closed-form the ABEP under sufficiently large signal-to-noise ratio for the source-relay link, valid for arbitrary rn. Numerical and simulation results show the validity of the proposed mathematical analysis and point out the effect of the two hops unbalanced fading conditions on the error performance.",no8564,Concurrent bug patterns and how to test them,We present and categorize a taxonomy of concurrent bug patterns. We then use the taxonomy to create new timing heuristics for ConTest. Initial industrial experience indicates that these heuristics improve the bug finding ability of ConTest. We also show how concurrent bug patterns can be derived from concurrent design patterns. Further research is required to complete the concurrent bug taxonomy and formal experiments are needed to show that heuristics derived from the taxonomy improve the bug finding ability of ConTest.,no8563,Analytical bounds on the error performance of the DVB-T system in time-invariant channels,"We present an upper bound on the BER performance of the DVB-T system for time-invariant channels. A unified approach is taken comprising all feasible combinations of convolutional encoder rates and constellations, for both non-hierarchical and hierarchical transmission modes. The validity of the estimated BER is compared thoroughly with the simulated BER obtained with BerbeX, a DVB-T compliant software, for the three channels specified in the DVB-T standard",no8562,Behavioral fault simulation : implementation and experimental results,"We present an original approach for performing Behavioral Fault Simulation (BFS). This approach involves three main steps : (i) the definition of an internal modeling of behavioral descriptions, and the determination of a Fault Model; (ii) the definition of a fault simulation technique; (iii) the implementation of this technique. We give in this paper a description of the BFS software implementation. We point out how object oriented programming has been used for defining an evolutive and efficient tool. Finally, this paper deals with experiments conducted on ITC'99 benchmarks in order to validate a VHDL behavioral fault simulator (BFS). The effectiveness of the BFS software is clearly demonstrated through the obtained results",no8561,Requirements specification and analysis of fault-tolerant digital systems,"We present an integrated computer-aided design environment, the PrT (predicate/transition) net system, in order to systematically introduce fault-tolerant properties into the design of complicated digital systems. This is accomplished by exploiting a formal specification of the system requirements in which the amount of necessary redundancy can be determined. The system is based on an integration of PrT nets with regular expressions. PrT nets are used to describe and analyze a high level system and regular expressions are used to describe and analyze the more detailed system structures. Both models provide us with well-defined levels of fault diagnosis needed in the digital system design. An S-invariant technique can be used to check the constancy of PrT nets; and a finite state automaton can be used to check the acceptability of regular expressions. Furthermore, the regular expression can also enable a system designer to determine redundancy in order to perform error correction. In consequence, our approach is superior to the current techniques for requirements analysis. Finally, main results are presented in the form of four propositions and supported by some experiments",no8560,Evaluation of the Low Frame Error Rate Performance of LDPC Codes Using Importance Sampling,"We present an importance sampling method for the evaluation of the low frame error rate (FER) performance of LDPC codes under iterative decoding. It relies on a combinatorial characterization of absorbing sets, which are the dominant cause of decoder failure in the low FER region. The biased density in the importance sampling scheme is a mean-shifted version of the original Gaussian density, which is suitably centered between a codeword and a dominant absorbing set. This choice of biased density yields an unbiased estimator for the FER with a variance lower by several orders of magnitude than the standard Monte Carlo estimator. Using this importance sampling scheme in software, we obtain good agreement with the experimental results obtained from a fast hardware emulator of the decoder.",no8559,Measurement and analysis of physical defects for dynamic supply current testing,We present an iDDT fault analysis study based on physical measurements of circuits with built-in defects. A variety of defects were inserted into basic circuit components. The measured results were utilized to better model the effect of defects on iDDT and improve simulated fault models.,no8558,Supporting component and architectural re-usage by detection and tolerance of integration faults,"We present an extended interface description language supporting the avoidance and the automatic-detection and tolerance of inconsistency classes likely to occur when integrating pre-developed components. In particular, the approach developed allows the automatic generation of component wrapping mechanisms aimed at handling the occurrence of local and global inconsistencies during runtime. On the whole, the application of the procedure suggested supports re-usage of components and of architectural patterns by allowing their easy adaptation to the specific needs of the application considered.",no8557,Using memory errors to attack a virtual machine,"We present an experimental study showing that soft memory errors can lead to serious security vulnerabilities in Java and .NET virtual machines, or in any system that relies on type-checking of untrusted programs as a protection mechanism. Our attack works by sending to the JVM a Java program that is designed so that almost any memory error in its address space will allow it to take control of the JVM. All conventional Java and .NET virtual machines are vulnerable to this attack. The technique of the attack is broadly applicable against other language-based security schemes such as proof-carrying code. We measured the attack on two commercial Java virtual machines: Sun's and IBM's. We show that a single-bit error in the Java program's data space can be exploited to execute arbitrary code with a probability of about 70%, and multiple-bit errors with a lower probability. Our attack is particularly relevant against smart cards or tamper-resistant computers, where the user has physical access (to the outside of the computer) and can use various means to induce faults; we have successfully used heat. Fortunately, there are some straightforward defenses against this attack.",no8556,A H.263 compatible error resilient video coder,"We present an error resilient video coder compatible with the ITU-T H.263 standard. Resynchronization flag insertion, error detection, localization and concealment in the decoder, and dynamic programming mode selection based on error tracking are the three main adopted error-resilient strategies. An information feedback method, which utilizes the H.263 video bit stream but does not modify its syntax, is described. Simulation results for the binary symmetric channel (BSC) with random bit errors are given to show the robustness of the proposed video coder",no8555,A new method of non-stationary signal analysis for control motor bearing fault diagnosis,"We present an equal phase sampling method (EPSM) based technique to diagnose nonstationary servomotor bearing faults. Conventional equal time sampling method (ETSM) is based on the time periodical feature of a constant rotating speed system. As the servomotor changes its rotating speed and direction frequently, it will loose its time periodical feature. However, it still keeps its space periodical feature. The new method of nonstationary signal analysis based on EPSM can eliminate the influence of rotating speed and direction changes. The mathematical simulation and experiment results prove that this method is very suitable and effective for servomotor bearing fault diagnosis.",no8554,Fault-tolerant Ethernet for IP-based process control: A demonstration,"We present an efficient middleware-based fault-tolerant Ethernet (FTE) prototype developed for process control networks. This unique approach requires no change of commercial-off-the-shelf (COTS) hardware (switch, hub, Ethernet physical link and network interface card (NIC)) and software (Ethernet driver and protocol), yet it is transparent to application software. The FTE performs failure detection and recovery for handling multiple points of network failures and supports communications with non FTE-native devices. In this demonstration, we focus on presenting the failure detection and recovery behavior under various failure modes and scenarios. Further, multiple failure handling, node departure and non FTE-native node and FTE node communication scenarios will be presented. The FTE protocol status will be displayed using an FTE user interface on a COTS-based network system",no8553,Fault-tolerant Ethernet middleware for IP-based process control networks,"We present an efficient middleware-based fault-tolerant Ethernet (FTE) developed for process control networks. Our approach is unique and practical in the sense that it requires no change to commercial off-the-shelf hardware (switch, hub, Ethernet physical link, and network interface card) and software (commercial Ethernet NIC card driver and standard protocol such as TCP/IP) yet it is transparent to IP-based applications. The FTE performs failure detection and recovery for handling multiple points of network faults and supports communications with non-FTE-capable devices. Our experimentation shows that FTE performs efficiently, achieving less than 1-ms end-to-end swap time and less than 2-sec failover time, regardless of the concurrent application and system loads. In this paper, we describe the FTE architecture, the challenging technical issues addressed, our performance evaluation results, and the lessons learned in design and development of such an open-network-based fault-tolerant network",no8552,Automatic Fault Localization for Property Checking,"We present an efficient fully automatic approach to fault localization for safety properties stated in linear temporal logic. We view the failure as a contradiction between the specification and the actual behavior and look for components that explain this discrepancy. We find these components by solving the satisfiability of a propositional Boolean formula. We show how to construct this formula and how to extend it so that we find exactly those components that can be used to repair the circuit for a given set of counterexamples. Furthermore, we discuss how to efficiently solve the formula by using the proper decision heuristics and simulation-based preprocessing. We demonstrate the quality and efficiency of our approach by experimental results.",no8551,An efficient fault-tolerant location management protocol for personal communication networks,"We present an efficient fault-tolerant distributed location management protocol for personal communications service (PCS) networks. It achieves low connection-establishment delay, and under certain conditions, low overall cost compared to the current PCS location management protocol (i.e., IS-41). It also effectively avoids the shortcomings of IS-41, namely, centralized location management and triangular routing. Another feature of the protocol is its ability to recover from loss or corruption of the location information carried by the mobile host. For larger networks, this paper proposes two approaches to reduce the overhead of the distributed location management scheme. Further, we formally prove that our location management scheme maintains the correct location information of every subscriber in the system",no8550,Robust Speech Recognition Using a Cepstral Minimum-Mean-Square-Error-Motivated Noise Suppressor,"We present an efficient and effective nonlinear feature-domain noise suppression algorithm, motivated by the minimum-mean-square-error (MMSE) optimization criterion, for noise-robust speech recognition. Distinguishing from the log-MMSE spectral amplitude noise suppressor proposed by Ephraim and Malah (E&M), our new algorithm is aimed to minimize the error expressed explicitly for the Mel-frequency cepstra instead of discrete Fourier transform (DFT) spectra, and it operates on the Mel-frequency filter bank's output. As a consequence, the statistics used to estimate the suppression factor become vastly different from those used in the E&M log-MMSE suppressor. Our algorithm is significantly more efficient than the E&M's log-MMSE suppressor since the number of the channels in the Mel-frequency filter bank is much smaller (23 in our case) than the number of bins (256) in DFT. We have conducted extensive speech recognition experiments on the standard Aurora-3 task. The experimental results demonstrate a reduction of the recognition word error rate by 48% over the standard ICSLP02 baseline, 26% over the cepstral mean normalization baseline, and 13% over the popular E&M's log-MMSE noise suppressor. The experiments also show that our new algorithm performs slightly better than the ETSI advanced front end (AFE) on the well-matched and mid-mismatched settings, and has 8% and 10% fewer errors than our earlier SPLICE (stereo-based piecewise linear compensation for environments) system on these settings, respectively.",no8549,Effective congestion and error control for scalable video coding extension of the H.264/AVC,"We present an effective congestion and error control mechanism for scalable video coding (SVC) extension of the H.264/AVC video dissemination over Internet. The congestion control is used to determine the appropriate number of SVC video layers based on bandwidth inference congestion (BIC) control protocol for layered multicast scenarios and the error control is achieved by unequal forward error correction (FEC) layered protection using block erasure coding. Through the real Internet streaming experiments, we demonstrate the effectiveness of the proposed layered SVC delivery, in terms of subscription layer, average packet loss rate and PSNRs, under several layered-definition scalabilities.",no8548,Test pattern generation for timing-induced functional errors in hardware-software systems,"We present an ATPG algorithm for the covalidation of hardware-software systems. Specifically, we target the detection of timing-induced functional errors in the design by using a design fault model which we propose. The computational time required by the test generation process is sufficiently low that the ATPG tool can be used by a designer to achieve a significant reduction in validation cost",no8547,Synthesis of fault-tolerant embedded systems with checkpointing and replication,"We present an approach to the synthesis of fault-tolerant hard real-time systems for safety-critical applications. We use checkpointing with rollback recovery and active replication for tolerating transient faults. Processes are statically scheduled and communications are performed using the time-triggered protocol. Our synthesis approach decides the assignment of fault-tolerance policies to processes, the optimal placement of checkpoints and the mapping of processes to processors such that transient faults are tolerated and the timing constraints of the application are satisfied. We present several synthesis algorithms which are able to find fault-tolerant implementations given a limited amount of resources. The developed algorithms are evaluated using extensive experiments, including a real-life example",no8546,Detecting Duplicate Bug Report Using Character N-Gram-Based Features,"We present an approach to identify duplicate bug reports expressed in free-form text. Duplicate reports needs to be identified to avoid a situation where duplicate reports get assigned to multiple developers. Also, duplicate reports can contain complementary information which can be useful for bug fixing. Automatic identification of duplicate reports (from thousands of existing reports in a bug repository) can increase the productivity of a Triager by reducing the amount of time a Triager spends in searching for duplicate bug reports of any incoming report. The proposed method uses character N-gram-based model for the task of duplicate bug report detection. Previous approaches are word-based whereas this study investigates the usefulness of low-level features based on characters which have certain inherent advantages (such as natural-language independence, robustness towards noisy data and effective handling of domain specific term variations) over word-based features for the problem of duplicate bug report detection. The proposed solution is evaluated on a publicly-available dataset consisting of more than 200 thousand bug reports from the open-source Eclipse project. The dataset consists of ground-truth (pre-annotated dataset having bug reports tagged as duplicate by the Triager). Empirical results and evaluation metrics quantifying retrieval performance indicate that the approach is effective.",no8545,Fault Management based on peer-to-peer paradigms; A case study report from the CELTIC project Madeira,"We present an approach to fault management based on an architecture for distributed and collaborative network management as developed in the CELTIC project Madeira. It uses peer-to-peer communication facilities and a logical overlay network facilitating decentralized and iterative alarm processing and correlation. We argue that such an approach might help to overcome key challenges that are posed by NGN scenarios to traditional centralized network management systems. Its feasibility is demonstrated by means of a case study from the area of wireless mesh networks, where an application prototype has been developed.",no8544,Value-based scheduling of distributed fault-tolerant real-time systems with soft and hard timing constraints,We present an approach for scheduling of fault-tolerant embedded applications composed of soft and hard real-time processes running on distributed embedded systems. The hard processes are critical and must always complete on time. A soft process can complete after its deadline and its completion time is associated with a value function that characterizes its contribution to the quality-of-service of the application. We propose a quasi-static scheduling algorithm to generate a tree of fault-tolerant distributed schedules that maximize the application's quality value and guarantee hard deadlines.,no8543,EPIC: profiling the propagation and effect of data errors in software,"We present an approach for analyzing the propagation and effect of data errors in modular software enabling the profiling of the vulnerabilities of software to find 1) the modules and signals most likely exposed to propagating errors and 2) the modules and signals which, when subjected to error, tend to cause more damage than others from a systems operation point-of-view. We discuss how to use the obtained profiles to identify where dependability structures and mechanisms will likely be the most effective, i.e., how to perform a cost-benefit analysis for dependability. A fault-injection-based method for estimation of the various measures is described and the software of a real embedded control system is profiled to show the type of results obtainable by the analysis framework.",no8542,An approach for analysing and improving fault tolerance in radio architectures,"We present an approach for analysing and improving fault-tolerance aspects in radio architectures. This is a necessary step to be taken in order to implement reliable radio systems in future nanoscale technologies. We present problem formulation, optimisation approach and implementation methodology. We are adding fault tolerance at architecture level by taking advantage of existing parallel structures and using a spare module approach in order to minimise hardware overhead needed. These issues have been analysed and demonstrated using two radio case studies: a UMTS MIMO and a GSM diversity receivers",no8541,Combinatorial designs in multiple faults localization for battlefield networks,We present an application of combinatorial designs and variance analysis to correlating events in the midst of multiple network faults. The network fault model is based on the probabilistic dependency graph that accounts for the uncertainty about the state of network elements. Orthogonal arrays help reduce the exponential number of failure configurations to a small subset on which further analysis is performed. The preliminary results show that statistical analysis can pinpoint the probable causes of the observed symptoms with high accuracy and significant level of confidence. An example demonstrates how multiple soft link failures are localized in MIL-STD 188-220's datalink layer to explain the end-to-end connectivity problems in the network layer This technique can be utilized for the networks operating in an unreliable environment such as wireless and/or military networks.,no8540,Distributed construction of a fault-tolerant network from a tree,"We present an algorithm by which nodes arranged in a tree, with each node initially knowing only its parent and children, can construct a fault-tolerant communication structure (an expander graph) among themselves in a distributed and scalable way. The tree overlayed with this logical expander is a useful structure for distributed applications that require the intrinsic ""treeness"" from the topology but cannot afford any obstruction in communication due to failures. At the core of our construction is a novel distributed mechanism that samples nodes uniformly at random from the tree. In the event of node joins, node departures or node failures, the expander maintains its own fault tolerance and permits the reformation of the tree. We present simulation results to quantify the convergence of our algorithm to a fault tolerant network having both good vertex connectivity and expansion properties.",no8539,BugFix: A learning-based tool to assist developers in fixing bugs,"We present a tool called BugFix that can assist developers in fixing program bugs. Our tool automatically analyzes the debugging situation at a statement and reports a prioritized list of relevant bug-fix suggestions that are likely to guide the developer to an appropriate fix at that statement. BugFix incorporates ideas from machine learning to automatically learn from new debugging situations and bug fixes over time. This enables more effective prediction of the most relevant bug-fix suggestions for newly-encountered debugging situations. The tool takes into account the static structure of a statement, the dynamic values used at that statement by both passing and failing runs, and the interesting value mapping pairs associated with that statement. We present a case study illustrating the efficacy of BugFix in helping developers to fix bugs.",no8538,Design diversity for concurrent error detection in sequential logic circuits,"We present a technique using diverse duplication to implement concurrent error detection (CED) in sequential logic circuits. We examine three different approaches for this purpose: (1) identical state encoding of the two sequential logic implementations, duplication of flip-flops, diverse implementation of the combinational logic part (output logic and next-state logic) and comparators on flip-flop outputs and primary outputs; (2) diverse state encoding of the two implementations, duplication of flip-flops, diverse combinational logic implementation and comparators on primary outputs only; and (3) identical state encoding, parity prediction for the flip-flops, diverse combinational logic implementation, comparators on primary outputs and parity checkers on flip-flop outputs. Our results for the simulated sequential benchmark circuits demonstrate that the third approach is most efficient in protecting sequential logic circuits against multiple and common-mode failures. The computational complexity of the data integrity analysis of the third approach is of the same order as that of the first approach and is at least an order of magnitude less than that of the second approach",no8537,Automatic Generation of Instructions to Robustly Test Delay Defects in Processors,"We present a technique for generating instruction sequences to test a processor functionally. We target delay defects with this technique using an ATPG engine to generate delay tests locally, a verification engine to map the tests globally, and a feedback mechanism that makes the entire procedure faster. We demonstrate nearly 96% coverage of delay faults with the instruction sequences generated. These instruction sequences can be loaded into the cache to test the processor functionally.",no8536,Beat: Boolean expression fault-based test case generator,"We present a system which generates test cases from Boolean expressions. The system is based on the integration of several fault-based test case selection strategies developed by us. Our system generates test cases that are guaranteed to detect all single operator fault and all single operand faults when the Boolean expression is in irredundant disjunctive normal form. Apart from being an automated test case generation tool developed for software testing practitioners, this system can also be used as a training or self-learning tool for students as well as software testing practitioners.",no8535,Fault tree analysis for software design,"We present a study on software fault tree analysis (SFTA) conducted at the Software Assurance Technology Center at NASA Goddard Space Flight Center. While researchers have made various attempts at SFTA, software assurance practitioners have been slow to adopt it. One reason is the intense manual effort needed to identify and draw the fault trees for the code of large software projects. Another is the lack of commercial tools to assist in the technique for software. Most SFTA research efforts have been directed at requirements or code. Performing SFTA on the design may enable application of SFTA to critical code only, thus reducing the amount of effort. We attempt to develop a relationship between UMLTM design diagrams and fault tree symbology to enable adaptation of a commercial FTA tool to at least one software design language. Such a result would reduce the amount of fault tree effort both for size (design instead of code) and for manual effort.",no8534,A statistical model to locate faults at input levels,"We present a statistical model to locate faults at the input level based on the failure patterns and the success patterns. The model neither needs to be fed with software module, code or trace information, nor does it require re-executing the program. To evaluate the model, precision and recall are adopted as the criteria. Five programs are examined and 17 testing experiments are conducted in which the model gains 0.803 in precision and 0.697 in recall on average.",no8533,Software Defects Prediction using Operating Characteristic Curves,We present a software defect prediction model using operating characteristic curves. The main idea behind our proposed technique is to use geometric insight in helping construct an efficient and fast prediction method to accurately predict the. cumulative number of failures at any given stage during the software development process. Our predictive approach uses the number of detected faults instead of the software failure-occurrence time in the testing phase. Experimental results illustrate the effectiveness and the much improved performance of the proposed method in comparison with the Bayesian prediction approaches.,no8532,Simulator for fault tolerance in large scale distributed systems,"We present a simulation model designed for the evaluation of fault tolerance solutions working in large scale distributed systems. This model extends the MONARC simulation model with new capabilities for fault tolerance simulation. The model includes failure behavior and capabilities to detect and react to faults. We also present an implementation of this model in MONARC, together with specific evaluation results. The model's implementation considers permanent and transient failures occurring within processing units, network components, as well as databases. The model is easily extendable, allowing the additions of new failure models, as required by different experiments. The model can be used in conjunction with key performance metrics, being able to easily pinpoint areas of failures within the simulated environments.",no8531,A desktop environment for assessment of fault diagnosis based fault tolerant flight control laws,"We present a simulation based software environment conceived to allow an easy assessment of fault diagnosis based fault tolerant control techniques. The new tool is primary intended for the development of advanced flight control applications with fault accommodation abilities, where the requirements for increased autonomy and safety play a premier role.",no8530,Propagating Bug Fixes with Fast Subgraph Matching,"We present a powerful and efficient approach to the problem of propagating a bug fix to all the locations in a code base to which it applies. Our approach represents bug and fix patterns as subgraphs of a system dependence graph, and it employs a fast, index-based subgraph matching algorithm to discover unfixed bug-pattern instances remaining in a code base. We have also developed a graphical tool to help programmers specify bug patterns and fix patterns easily. We evaluated our approach by applying it to bug fixes in four large open-source projects. The results indicate that the approach exhibits good recall and precision and excellent efficiency.",no8529,A persistent diagnostic technique for unstable defects,"We present a persistent diagnostic technique for unstable defects, such as open defects or delay defects. A new ""segment model"" diagnosis for the completely open defects is discussed. Here, we not only focus on the behavior of the principal offender, but also the behavior of the accomplices which cause the unstable behavior of the defect. In this paper, a technique using the layout information for an open fault diagnosis, and a testing method for the delay fault are discussed. Some experimental results of actual chips are shown.",no8528,"Performance improvement in high capacity, ultra-long distance, WDM systems using forward error correction codes","We present a performance study of a forward error correction (FEC) code using theoretical models, Monte-Carlo computer simulations, and a long-haul WDM transmission experiment. With a 14% redundancy code, the Q-factor was increased by 6.2 dB for both linear and non-linear impairments.",no8527,Optimizing a highly fault tolerant software RAID for many core systems,We present a parallel software driver for a RAID architecture to detect and correct corrupted disk blocks in addition to tolerate disk failures. The necessary computations demand parallel execution to avoid the processor being the bottleneck for a RAID with high bandwidth. The driver employs the processing power of multicore and manycore systems. We report on the performance of a prototype implementation on a quadcore processor that indicates linear speedup and promises good scalability on larger machines. We use reordering of I/O orders to ensure balance between CPU and disk load.,no8526,Virtual Multiresolution Screen Space Errors: Hierarchical Level-of-Detail (HLOD) Refinement Through Hardware Occlusion Queries,"We present a novelty metric to perform the refinement of a HLOD-based system that takes into account visibility information. The information is gathered from the result of a hardware occlusion query (HOQ) performed on the bounding volume of a given node in the hierarchy. Although the advantages of doing this are clear, previous approaches treat refinement criteria and HOQ as independent subjects. For this reason, HOQs have been used restrictively as if their result were Boolean. In contrast to that, we fully exploit the results of the queries to be able to take into account visibility information within refinement conditions. We do this by interpreting the result of a given HOQ as the virtual resolution of a screen space where the refinement decision takes place. Our new error metric is general enough to be employed in any HLOD-based system as the quantity that guides its refinement. Despite its simplicity, in our experiments we obtained a meaningful performance boost (compared to previous approaches) in the frame-rate with almost no loss in image quality",no8525,Tate Pairing with Strong Fault Resiliency,"We present a novel non-linear error coding framework which incorporates strong adversarial fault detection capabilities into identity based encryption schemes built using Tate pairing computations. The presented algorithms provide quantifiable resilience in a well defined strong attacker model. Given the emergence of fault attacks as a serious threat to pairing based cryptography, the proposed technique solves a key problem when incorporated into software and hardware implementations.",no8524,Empirical interval estimates for the defect content after an inspection,"We present a novel method for estimating the number of defects contained in a document using the results of an inspection of the document. The method is empirical, being based on observations made during past inspections of comparable documents. The method yields an interval estimate, that is, a whole range of values which is likely to contain the true value of the number of defects in the document. We also derive point estimates from the interval estimate. The method is validated using a known empirical inspection dataset and clearly outperforms existing approaches for estimating the defect content after inspections.",no8523,A novel framework for robust video streaming based on H.264/AVC MGS coding and unequal error protection,"We present a novel framework to provide robust video streaming service over time-varying error-prone network. The scheme is based on the medium granularity scalability (MGS) video coding of the H.264/AVC standard, which adopts a hierarchical prediction structure for the group-of-pictures (GOP). We determine the optimal allocation of protection strength for different network abstraction layer (NAL) units according to their individual importance to the end-to-end video quality. To analyse the importance of the NAL units, we emulate the error concealment if one frame is considered as lost and take into account the propagation distortion within the GOP. An efficient algorithm is proposed to account for the non-convex rate-distortion characteristics associated with the NAL units in the hierarchical GOP. With this framework, we can provide robust video streaming for the range of packet loss rates from 0% to 40% with about 30% additional channel bit-rate for the channel coding. The simulation results demonstrate high flexibility and efficiency of the proposed framework, which can effectively prevent frequent loss of frames.",no8522,Modeling cross-sensory and sensorimotor correlations to detect and localize faults in mobile robots,"We present a novel framework for learning cross- sensory and sensorimotor correlations in order to detect and localize faults in mobile robots. Unlike traditional fault detection and identification schemes, we do not use a priori models of fault states or system dynamics. Instead, we utilize additional information and possible source of redundancy that mobile robots have available to them, namely a hierarchical graph representing stages of sensory processing at multiple levels of abstractions and their outputs. We learn statistical models of correlations between elements in the hierarchy, in addition to the control signals, and use this to detect and identify changes in the capabilities of the robot. The framework is instantiated using Self-Organizing Maps, a simple unsupervised learning algorithm. Results indicate that the system can detect sensory and motor faults in a mobile robot and identify their cause, without using a priori models of the robot or its fault states.",no8521,A novel technique for coupling three dimensional mesh adaptation with an a posteriori error estimator,"We present a novel error estimation driven 3D unstructured mesh adaptation technique based on a posteriori error estimation techniques with upper and lower error bounds. In contrast to other work (Oden, 2002; Prudhomme et al., 2003) we present this approach in three dimensions using unstructured meshing techniques to potentiate an automatically adaptation of 3D unstructured meshes without any user interaction. The motivation for this approach, the applicability and usability is presented with real-world examples.",no8520,Defect-Tolerant CMOL Cell Assignment via Satisfiability,"We present a novel CAD approach to cell assignment of CMOL, a hybrid CMOS/molecular circuit architecture. Our method transforms any logically synthesized circuit based on AND/OR/NOT gates to a NOR gate circuit and maps the NOR gates to CMOL. We encode the CMOL cell assignment problem as Boolean conditions. The Boolean constraints are satisfiable if and only if there exists a solution to map all the NOR gates to the CMOL cells. We further investigate various types of static defects for the CMOL architecture and propose a reconfiguration technique that can deal with these defects. We introduce a new CMOL static defect model and provide an automated solution for CMOL cell assignment. Experiments show that our approach can result in smaller area (CMOL cell usage) and better timing delay than prior approach.",no8519,Fault recovery port-based fast spanning tree algorithm (FRP-FAST) for the fault-tolerant Ethernet on the arbitrary switched network topology,"We present a novel approach, named Fault Recovery Port-Based Fast Spanning Tree Algorithm (FRP-FAST), of the Fault-Tolerant Ethernet (FTE) extension method to the arbitrary switched network topology with providing a significant improvement of failure detection and the spanning tree rebuilding time on the switched Ethernet. We provide a mechanism that expedites failure detection time using peer-based hello message algorithm and eliminates the chance of any transient loop creation during the spanning tree reconstruction using a pre-configured recovery port. As a result, unlike IEEE 802.1D, the scheme does not block data transmission on unaffected data path during the spanning tree discovery phase. The FRP-FAST algorithm has been implemented in the kernel mode of Windows NT-based PC using 3 NICs (3 port switch). The measured failure detection and recovery time meets control industry's 2 seconds requirement.",no8518,Geometric and shading correction for images of printed materials: a unified approach using boundary,"We present a novel approach that uses boundary interpolation to correct (1) geometric distortion and (2) shading artifacts present in images of printed materials. Unlike existing approaches, our algorithm can simultaneously correct a variety of geometric distortions, including skew, fold distortion, binder curl, and combinations of these. In addition, the same interpolation framework can estimate the intrinsic illumination component of the distorted image to correct shading artifacts.",no8517,An approach for analysing the propagation of data errors in software,"We present a novel approach for analysing the propagation of data errors in software. The concept of error permeability is introduced as a basic measure upon which we define a set of related measures. These measures guide us in the process of analysing the vulnerability of software to find the modules that are most likely exposed to propagating errors. Based on the analysis performed with error permeability and its related measures, we describe how to select suitable locations for error detection mechanisms (EDMs) and error recovery mechanisms (ERMs). A method for experimental estimation of error permeability, based on fault injection, is described and the software of a real embedded control system analysed to show the type of results obtainable by the analysis framework. The results show that the developed framework is very useful for analysing error propagation and software vulnerability and for deciding where to place EDMs and ERMs.",no8516,"TRAILS, a Toolkit for Efficient, Realistic and Evolving Models of Mobility, Faults and Obstacles in Wireless Networks","We present a new simulation toolkit called TRAILS (Toolkit for Realism and Adaptivity In Large-scale Simulations), which extends the ns-2 simulator by adding important functionality and optimizing certain critical simulator operations. The added features provide the tools to study wireless networks of high dynamics. TRAILS facilitates the implementation of advanced mobility patterns, obstacle presence and disaster scenarios, and failures injection that can dynamically change throughout the execution of the simulation. Moreover, we define a set of utilities that enhance the use of ns-2. This functionality is implemented in a simple and flexible architecture, that follows design patterns, object oriented and generic programming principles, maintaining a proper balance between reusability, extendability and ease of use. We evaluate the performance of TRAILS and show that it offers significant speed-ups regarding the execution time of ns-2 in certain important, common wireless settings. Our results also show that this is achieved with minimum overhead in terms of memory usage.",no8515,A partition-based approach for identifying failing scan cells in scan-BIST with applications to system-on-chip fault diagnosis,"We present a new partition-based fault diagnosis technique for identifying failing scan cells in a scan-BIST environment. This approach relies on a two-step scan chain partitioning scheme. In the first step, an interval-based partitioning scheme is used to generate a small number of partitions, where each element of a partition consists of a set of scan cells. In the second step, additional partitions are created using an earlier-proposed random-selection partitioning method. Two-step partitioning leads to higher diagnostic resolution than a scheme that relies only on random-selection partitioning, with only a small amount of additional hardware. The proposed scheme is especially suitable for a system-on-chip (SOC) composed of multiple embedded cores, where test access is provided by means of a TestRail that is threaded through the internal scan chains of the embedded cores. We present experimental results for the six largest ISCAS-89 benchmark circuits and for two SOCs crafted from some of the ISCAS-89 circuits.",no8514,A New Approach of Fault Localization Using Value Replacement,"We present a new method that bases on value replacement which considers both the control dependence and the data dependence. The key idea of value replacement is to see which program statements exercised during a failing run use values that can be altered so that the execution instead produces correct output. This approach is effective in locating statements that either faulty statements or directly effecting the faulty statements. Our approach also analyze the possibility of the statements that are faulty, this can be applied to more areas.",no8513,Safety optimization: a combination of fault tree analysis and optimization techniques,"We present a new form of quantitative safety analysis -safety optimization. This method is a combination of fault tree analysis (FTA) and mathematical optimization techniques. With the use of the results of FTA, statistics, and a quantification of the costs of hazards, it allows to find the optimal configuration of a given system with respect to opposed safety requirements. Furthermore, the system may not only be examined for safety, but usability as well. We illustrate this method on a real-world case study: the height control system of the Elbtunnel in Hamburg. Safety optimization showed some significant problems in trustworthiness of the system, yielded optimal values for configuration of free parameters and showed possible modifications to improve the system.",no8512,GOOFI: generic object-oriented fault injection tool,We present a new fault injection tool called GOOFI (Generic Object-Oriented Fault Injection). GOOFI is designed to be adaptable to various target systems and different fault injection techniques. The tool is highly portable between different host platforms since it relies on the Java programming language and an SQL compatible database. The current version of the tool supports pre-runtime software implemented fault injection and scan-chain implemented fault injection.,no8511,Frame loss error concealment for spatial scalability using hallucination,"We present a new error concealment algorithm for spatially scalable video coding with frame loss in the enhancement layer, based on the technique of hallucination. For a lost enhancement layer frame, the error concealment is done as hallucinating its base layer frame, using the database trained from previously decoded frames nearby to the lost one. Simulation results show that the proposed method could out-perform the state-of-the-art error concealment algorithms of SVC significantly.",no8510,A New Class of Highly Fault Tolerant Erasure Code for the Disk Array,"We present a new class of erasure codes of size ntimesn (n is a prime number) called T-code, a new family of simple, highly fault tolerant XOR-based erasure codes for storage systems (with fault tolerance up to 15). T-code is not maximum distance separable (MDS), but has many other advantages, such as high fault tolerance, simple computability, and high efficiency of coding and decoding. Because of its superior quantity over many other erasure codes for the storage system, this new coding technology is more suited in RAID or dRAID systems.",no8509,Estimating circuit fault-tolerance by means of transient-fault injection in VHDL,"We present a new approach to estimate the reliability of complex circuits used in harmful radiation environments. This goal can be attained in an early stage of the design process. Usually, this step is performed in laboratory, by means of radiation facilities (particle accelerators). In our case, we estimate the expected tolerance of the complex circuit with respect to SEU during the VHDL specification step. By doing so, the early-estimated reliability level is used to balance the design process into a trade-off between maximum area overhead due to the insertion of redundancy and the minimum reliability required for a given application. This approach is being automated through the development of a CAD tool.",no8508,Recent improvements on the specification of transient-fault tolerant VHDL descriptions: a case-study for area overhead analysis,"We present a new approach to design reliable complex circuits with respect to transient faults in memory elements. These circuits are intended to be used in harmful environments like radiation. During the design flow, this methodology is also used to perform an early-estimation of the obtained reliability level. Usually, this reliability estimation step is performed in the laboratory, by means of radiation facilities (particle accelerators). By doing so, the early-estimated reliability level is used to balance the design process into a trade-off between maximum area overhead due to the insertion of redundancy and the minimum reliability required for a given application. This approach is being automated through the development of a CAD tool (FT-PRO). Finally, we present also a case-study of a simple microprocessor used to analyze the FT-PRO performance in terms of the area overhead required to implement the fault-tolerant circuit.",no8507,Transient-fault tolerant VHDL descriptions: a case-study for area overhead analysis,"We present a new approach to design reliable complex circuits with respect to transient faults in memory elements. These circuits are intended to be used in harmful environments like radiation. During the design flow this methodology is also used to perform an early-estimation of the obtained reliability level. Usually, this reliability estimation step is performed in the laboratory, by means of radiation facilities (particle accelerators). By doing so, the early-estimated reliability level is used to balance the design process into a trade-off between maximum area overhead due to the insertion of redundancy and the minimum reliability required for a given application. This approach is being automated through the development of a CAD tool (FT-PRO). Finally, we present also a case-study of a simple microprocessor used to analyze the FT-PRO performance in terms of the area overhead required to implement the fault-tolerant circuit.",no8506,A new learning approach to design fault tolerant ANNs: finally a zero HW-SW overhead,"We present a new approach to design fault tolerant artificial neural networks (ANNs). Additionally, this approach allows estimating the final network reliability. This approach is based on the mutation analysis technique and is used during the training process of the ANN. The basic idea is to train the ANN in the presence of faults (single-fault model is assumed). To do so, a set of faults is injected into the code describing the ANN. This procedure yields mutation versions of the original ANN code, which in turn are used to train the network in an iterative process with the designer until the moment when the ANN is no longer sensible to the single faults injected. In other words, the network became tolerant to the considered set of faults. A practical example where an ANN is used to recognize an electrocardiogram (ECG) and to measure ECG parameters illustrates the proposed methodology.",no8505,A Safe Fault Tolerant Multi-view Approach for Vision-Based Protective Devices,"We present a new approach that realizes an image-based fault tolerant distance computation for a multi-view camera system which conservatively approximates the shortest distance between unknown objects and 3D volumes. Our method addresses the industrial application of vision-based protective devices which are used to detect intrusions of humans into areas of dangerous machinery, in order to prevent injuries. This requires hardware redundancy for compensation of hardware failures without loss of functionality and safety. By taking sensor failures during the fusion process of distances from different cameras into account, this is realized implicitly, with the benefit of no additional hardware cost. In particular we employ multiple camera perspectives for safe and non-conservative occlusion handling of obstacles and formulate general system assumptions which are also appropriate for other applications like multi-view reconstruction methods.",no8504,A Model of Bug Dynamics for Open Source Software,We present a model to describe open source software (OSS) bug dynamics. We validated the model using real world data and performed simulation experiments. The results show that the model has the ability to predict bug occurrences and failure rates. The results also reveal that there exists an optimal release cycle for effectively managing OSS quality.,no8503,"Intelligent, Fault Tolerant Control for Autonomous Systems","We present a methodology for intelligent control of an autonomous and resource constrained embedded system. Geared towards mastering permanent and transient faults by dynamic reconfiguration, our approach uses rules for describing device functionality, valid environmental interactions, and goals the system has to reach. Besides rules, we use functions that characterize a goal's target activity profile. The target activity profile controls the frequency our system uses to reach the corresponding goal. In the paper we discuss a first implementation of the given methodology, and introduce useful extensions. In order to underline the feasibility and effectiveness of the presented control system, we present a case study that has been carried out on a prototype system.",no8502,Empirical Analysis of Software Fault Content and Fault Proneness Using Bayesian Methods,"We present a methodology for Bayesian analysis of software quality. We cast our research in the broader context of constructing a causal framework that can include process, product, and other diverse sources of information regarding fault introduction during the software development process. In this paper, we discuss the aspect of relating internal product metrics to external quality metrics. Specifically, we build a Bayesian network (BN) model to relate object-oriented software metrics to software fault content and fault proneness. Assuming that the relationship can be described as a generalized linear model, we derive parametric functional forms for the target node conditional distributions in the BN. These functional forms are shown to be able to represent linear, Poisson, and binomial logistic regression. The models are empirically evaluated using a public domain data set from a software subsystem. The results show that our approach produces statistically significant estimations and that our overall modeling method performs no worse than existing techniques.",yes8501,A model-based objective evaluation of eye movement correction in EEG recordings,"We present a method to quantitatively and objectively compare algorithms for correction of eye movement artifacts in a simulated ongoing electroencephalographic signal (EEG). A realistic model of the human head is used, together with eye tracker data, to generate a data set in which potentials of ocular and cerebral origin are simulated. This approach bypasses the common problem of brain-potential contaminated electro-oculographic signals (EOGs), when monitoring or simulating eye movements. The data are simulated for five different EEG electrode configurations combined with four different EOG electrode configurations. In order to objectively compare correction performance for six algorithms, listed in Table III, we determine the signal to noise ratio of the EEG before and after artifact correction. A score indicating correction performance is derived, and for each EEG configuration the optimal correction algorithm and the optimal number of EOG electrodes are determined. In general, the second-order blind identification correction algorithm in combination with 6 EOG electrodes performs best for all EEG configurations evaluated on the simulated data.",no8500,Design of Timing Error Detectors for Orthogonal Space-Time Block Codes,"We present a method for the design of low complexity timing error detectors in orthogonal space-time block coding (OSTBC) receivers. A general expression for the S-curve of timing error detectors is derived. Based on this result, we obtain sufficient conditions for a difference of threshold crossings timing estimate that is robust to channel fading. A number of timing error detectors for 3- and 4-transmit antenna codes are presented. The performance is evaluated by examining their tracking capabilities within a timing loop of an OSTBC receiver. Symbol-error-rate results are presented showing negligible loss due to timing synchronization. In addition, we study the performance as a function of the timing drift and show that the receiver is able to track up to the normalized timing drift bandwidth of 0.001",no8499,Fault localization with nearest neighbor queries,"We present a method for performing fault localization using similar program spectra. Our method assumes the existence of a faulty run and a larger number of correct runs. It then selects according to a distance criterion the correct run that most resembles the faulty run, compares the spectra corresponding to these two runs, and produces a report of ""suspicious"" parts of the program. Our method is widely applicable because it does not require any knowledge of the program input and no more information from the user than a classification of the runs as either ""correct"" or ""faulty"". To experimentally validate the viability of the method, we implemented it in a tool, Whither, using basic block profiling spectra. We experimented with two different similarity measures and the Siemens suite of 132 programs with injected bugs. To measure the success of the tool, we developed a generic method for establishing the quality of a report. The method is based on the way an ""ideal user"" would navigate the program using the report to save effort during debugging. The best results obtained were, on average, above 50%, meaning that our ideal user would avoid looking half of the program.",no8498,Optimizing testing efficiency with error-prone path identification and genetic algorithms,"We present a method for optimizing software testing efficiency by identifying the most error prone path clusters in a program. We do this by developing variable length genetic algorithms that optimize and select the software path clusters which are weighted with sources of error indexes. Although various methods have been applied to detecting and reducing errors in a whole system, there is little research into partitioning a system into smaller error prone domains for testing. Exhaustive software testing is rarely possible because it becomes intractable for even medium sized software. Typically only parts of a program can be tested, but these parts are not necessarily the most error prone. Therefore, we are developing a more selective approach to testing by focusing on those parts that are most likely to contain faults, so that the most error prone paths can be tested first. By identifying the most error prone paths, the testing efficiency can be increased.",no8497,Hardware evolution of analog circuits for in-situ robotic fault-recovery,"We present a method for evolving and implementing artificial neural networks (ANNs) on field programmable analog arrays (FPAAs). These FPAAs offer the small size and low power usage desirable for space applications. We use two cascaded FPAAs to create a two layer ANN. Then, starting from a population of random settings for the network, we are able to evolve an effective controller for several different robot morphologies. We demonstrate the effectiveness of our method by evolving two types of ANN controllers: one for biped locomotion and one for restoration of mobility to a damaged quadruped. Both robots exhibit nonlinear properties, making them difficult to control. All candidate controllers are evaluated in hardware; no simulation is used.",no8496,Real-word spelling correction using Google Web 1T n-gram with backoff,We present a method for correcting real-word spelling errors using the Google Web 1T n-gram data set and a normalized and modified version of the longest common subsequence (LCS) string matching algorithm. Our method is focused mainly on how to improve the correction recall (the fraction of errors corrected) while keeping the correction precision (the fraction of suggestions that are correct) as high as possible. Evaluation results on a standard data set show that our method performs very well.,no8495,Measurement-based frame error model for simulating outdoor Wi-Fi networks,"We present a measurement-based model of the frame error process on a Wi-Fi channel in rural environments. Measures are obtained in controlled conditions, and careful statistical analysis is performed on the data, providing information which the network simulation literature is lacking. Results indicate that most network simulators use a frame loss model that can miss important transmission impairments even at a short distance, particularly when considering antenna radiation pattern anisotropy and multi-rate switching.",no8494,Collecting broken frames: Error statistics in IEEE 802.11b/g links,"We present a measurement method that allows to capture the complete set of all PSDU (PLCP Service Data Unit) transmissions and receptions in live IEEE 802.11b/g links with very high timing resolution. This tool provides an in-depth view of the statistics of frame-losses as it makes it possible to distinguish between different loss types such as complete miss, partial corruption and physical-layer capture. Getting access to this low-level statistics on nodes that actively participate in transmissions themselves is a challenging task since the software-interface provided to the network layer needs to remain untouched and cannot be used for tracing. In this contribution we describe in detail how to non-intrusively circumvent these restrictions and also present initial results.",no8493,Solving dynamic fault trees using a new Hybrid Bayesian Network inference algorithm,"We present a hybrid Bayesian network (HBN) framework to analyse dynamic fault trees. By incorporating a new approximate inference algorithm for HBNs involving dynamically discretising the domain of all continuous variables, accurate approximations for the failure distribution of both static and dynamic fault tree constructs are obtained. Unlike in other approaches no numerical integration techniques or simulation methods are required. Moreover, no exact expression for the posterior marginal is needed and no conditional probability tables need to be completed. Sensitivity analysis, uncertainty, diagnosis, common cause failure analysis, can all be easily performed within this framework. Posterior estimates of parameterised marginal failure distributions can also be obtained using available raw failure data together with prior information from expert judgement.",no8492,Fault-tolerant static scheduling for real-time distributed embedded systems,"We present a heuristic for producing automatically a distributed fault-tolerant schedule of a given data-flow algorithm onto a given distributed architecture. The faults considered are processor failures, with a fail-silent behavior. Fault-tolerance is achieved with the software redundancy of computations and the time redundancy of data-dependencies",no8491,A fault-tolerant technique for scheduling periodic tasks in real-time systems,"We present a heuristic for producing a fault-tolerant schedule of given periodic tasks in distributed real-time systems. Tasks are divided into two classes according to their task utilization. In order to recover from faults, a hybrid scheme based on space redundancy and time redundancy is used. We use a very simple and fast heuristic to provide fault tolerance and reduce time overhead in case of transient faults in distributed real-time systems. We show that our approach can improve processor utilization.",no8490,System-level hardware-based protection of memories against soft-errors,"We present a hardware-based approach to improve the resilience of a computer system against the errors occurred in the main memory with the help of error detecting and correcting (EDAC) codes. Checksums are placed in the same type of memory locations and addressed in the same way as normal data. Consequently, the checksums are accessible from the exterior of the main memory just as normal data and this enables implicit fault-tolerance for interconnection and solid-state secondary storage sub-systems. A small hardware module is used to manage the sequential retrieval of checksums each time the integrity of the data accessed by the processor sub-system needs to be verified. The proposed approach has the following properties: (a) it is cost efficient since it can be used with simple storage and interconnection sub-systems that do not possess any inherent EDAC mechanism, (b) it allows on-line modifications of the memory protection levels, and (c) no modification of the application software is required.",no8489,A hardware Gaussian noise generator using the Box-Muller method and its error analysis,"We present a hardware Gaussian noise generator based on the Box-Muller method that provides highly accurate noise samples. The noise generator can be used as a key component in a hardware-based simulation system, such as for exploring channel code behavior at very low bit error rates, as low as 10<sup>-12</sup> to 10<sup>-13</sup>. The main novelties of this work are accurate analytical error analysis and bit-width optimization for the elementary functions involved in the Box-Muller method. Two 16-bit noise samples are generated every clock cycle and, due to the accurate error analysis, every sample is analytically guaranteed to be accurate to one unit in the last place. An implementation on a Xilinx Virtex-4 XC4VLX100-12 FPGA occupies 1,452 slices, three block RAMs, and 12 DSP slices, and is capable of generating 750 million samples per second at a clock speed of 375 MHz. The performance can be improved by exploiting concurrent execution: 37 parallel instances of the noise generator at 95 MHz on a Xilinx Virtex-II Pro XC2VP100-7 FPGA generate seven billion samples per second and can run over 200 times faster than the output produced by software running on an Intel Pentium-4 3 GHz PC. The noise generator is currently being used at the Jet Propulsion Laboratory, NASA to evaluate the performance of low-density parity-check codes for deep-space communications",no8488,Identifying the root causes of memory bugs using corrupted memory location suppression,"We present a general approach for automatically isolating the root causes of memory-related bugs in software. Our approach is based on the observation that most memory bugs involve uses of corrupted memory locations. By iteratively suppressing (nullifying) the effects of these corrupted memory locations during program execution, our approach gradually isolates the root cause of a memory bug. Our approach can work for common memory bugs such as buffer overflows, uninitialized reads, and double frees. However, our approach is particularly effective in finding root causes for memory bugs in which memory corruption propagates during execution until an observable failure such as a program crash occurs.",no8487,Automatic Test Pattern Generation for Interconnect Open Defects,"We present a fully automated flow to generate test patterns for interconnect open defects. Both inter-layer opens (open- via defects) and arbitrary intra-layer opens can be targeted. An aggressor-victim model used in industry is employed to describe the electrical behavior of the open defect. The flow is implemented using standard commercial tools for parameter extraction (PEX) and test generation (ATPG). A highly optimized branch-and bound algorithm to determine the values to be assigned to the aggressor lines is used to reduce both the ATPG efforts and the number of aborts. The resulting test sets are smaller and achieve a higher defect coverage than stuck-at n-detection test sets, and are robust against process variations.",no8486,A Markov framework for error control techniques based on selective retransmission in video transmission over wireless channels,"We present a framework, based on Markov models, for the analysis of error control techniques in video transmission over wireless channels. We focus on retransmission-based techniques, which require a feedback channel but also enable to perform adaptive error control. Traditional studies of these methodologies usually consider a uniform stream of data packets. Instead, video transmission poses the non-trivial challenge that the packets have different sizes, and, even more importantly, are incrementally encoded; thus, a carefully tailored model is required. We therefore proceed on two different sides. First, we consider a low-level description of the system, where two main inputs are combined, namely, a video packet generation process and a wireless channel model, both described by Markov Chains with a tunable number of states. Secondly, from a highlevel perspective, we represent the whole system evolution with another Markov Chain describing the error control process, which can feed the packet generation process back with retransmissions. The framework is able to evaluate hybrid automatic repeat request with selective retransmission, but can also be adapted to study pure automatic repeat request or forward error correction schemes. In this way, we are able to comparatively evaluate different solutions for video transmission, as well as to quantitatively assess their performance trends in a variety of scenarios. Thus, our framework can be used as an effective tool to understand the behavior of error control techniques applied to video transmission over wireless, and eventually identify design guidelines for such systems.",no8485,Designing Run-Time Fault-Tolerance Using Dynamic Updates,We present a framework for designing run-time fault- tolerance using dynamic program updates triggered by faults. This is an important problem in the design of autonomous systems as it is often the case that a running program needs to be upgraded to its fault-tolerant version once faults occur. We formally state fault-triggered program updates as a design problem. We then present a sound and complete algorithm that automates the design of fault- triggered updates for replacing a program that does not tolerate faults with a fault-tolerant version thereof at run-time. We also define three classes of fault-triggered dynamic updates that tolerate faults during the update. We demonstrate our approach in the context of a fault-triggered update for the gate controller of a parking lot.,no8484,Re-engineering fault tolerance requirements: a case study in specifying fault tolerant flight control systems,We present a formal specification of fault tolerance requirements for an analytical redundancy based fault tolerant flight control system. The development of the specification is driven by the performance and fault tolerance requirements contained in the US Air Force military specification MIL-F-9490D. The design constraints imposed to the system from adopting the analytical redundancy approach are captured within the specification. We draw some preliminary conclusions from our study,no8483,Hardware-based Error Rate Testing of Digital Baseband Communication Systems,"We present a flexible architecture for evaluating the bit-error-rate (BER) performance of prototype digital baseband communication systems. Using an efficient elastic buffer interface, an arbitrary baseband module can be added to the cascaded architecture of a digital baseband communication system, independent of the module's operating rate, its position in the cascade structure, and its latency. The proposed BER tester uses an accurate fading channel model and a Gaussian noise generator to provide a realistic and repeatable test environment in the laboratory. This evaluation environment should reduce the need for time-consuming field tests, hence reducing the time-to-market and increasing productivity.",no8482,Exact fault simulation for systems on silicon that protects each core's intellectual property (IP),"We present a fault simulation approach for multicore systems on silicon (SOC) (a) that provides exact fault coverage for the entire SOC, (b) does so without revealing any intellectual property (IP) of core vendors, and (c) whose run time is comparable to that required by the existing approaches that require all IP to be revealed. This fault simulator assumes a full scan SOC design and is first in a suite of simulation, test generation, and DFT tools that are currently under development. The proposed approach allows flexibility in selection of a test methodology for SOC, reduces test application cost and area and performance overheads, and allows more comprehensive testing",no8481,Defect Tolerance Based on Coding and Series Replication in Transistor-Logic Demultiplexer Circuits,"We present a family of defect tolerant transistor-logic demultiplexer circuits that can defend against both stuck-ON (short defect) and stuck-OFF (open defect) transistors. Short defects are handled by having two or more transistors in series in the circuit, controlled by the same signal. Open defects are handled by having two or more parallel branches in the circuit, controlled by the same signals, or more efficiently, by using a transistor-replication method based on coding theory. These circuits are evaluated, in comparison with an unprotected demultiplexer circuit, by: 1) modeling each circuit's ability to tolerate defects and 2) calculating the cost of the defect tolerance as each circuit's redundancy factor R, which is the relative number of transistors required by the circuit. The defect-tolerance model takes the form of a function giving the failure probability of the entire demultiplexer circuit as a function of the defect probabilities of its component transistors, for both defect types. With the advent of defect tolerance as a new design goal for the circuit designer, this new form of performance analysis has become necessary.",no8480,How a cyber-physical system can efficiently obtain a snapshot of physical information even in the presence of sensor faults,We present a distributed algorithm for cyber-physical systems to obtain a snapshot of sensor data. The snapshot is an approximate representation of sensor data; it is an interpolation as a function of space coordinates. The new algorithm exploits a prioritized medium access control (MAC) protocol to efficiently transmit information of the sensor data. It scales to a very large number of sensors and it is able to operate in the presence of sensor faults.,no8479,Distributed fault diagnostics for tactical networks,"We present a design and an evaluation of a distributed fault diagnostic system (FDS) that copes with changing wireless network topology, complexity and size of fault propagation patterns, constrained bandwidth, and limited computing power of the mobile devices. The presented FDS consists of several components: run-time synthesis algorithm to generate network-wide fault dependency model (FPM), scalable Bayesian inference algorithms, and novel techniques for optimally distributing inference to ensure the scalability of our approach. We describe three algorithms for distributing inference, each of them using different technique for maximizing the fault-symptom locality: Fault-based Adaptive algorithm, Topology-based Adaptive algorithm, and Topology-based Probabilistic algorithm. We have evaluated the performance of the proposed approach in a simulated environment using abstract models of a real-life tactical network, and compared it to a centralized approach. We found that our techniques allows for a significant gain in the processing time (30 times improvement for the best performing technique), and exhibit only a minimal reduction (3% percentage points) in the accuracy of the fault diagnostics.",no8478,A constraint logic programming framework for the synthesis of fault-tolerant schedules for distributed embedded systems,"We present a constraint logic programming (CLP) approach for synthesis of fault-tolerant hard real-time applications on distributed heterogeneous architectures. We address time-triggered systems, where processes and messages are statically scheduled based on schedule tables. We use process re-execution for recovering from multiple transient faults. We propose three scheduling approaches, which each present a trade-off between schedule simplicity and performance, (i) full transparency, (ii) slack sharing and (iii) conditional, and provide various degrees of transparency. We have developed a CLP framework that produces the fault-tolerant schedules, guaranteeing schedulability in the presence of transient faults. We show how the framework can be used to tackle design optimization problems.The proposed approach has been evaluated using extensive experiments.",no8477,"Reliable 3D surface acquisition, registration and validation using statistical error models","We present a complete data acquisition and processing chain for the reliable inspection of industrial parts considering anisotropic noise. Data acquisition is performed with a stripe projection system that was modeled and calibrated using photogrammetric techniques. Covariance matrices are attached individually to points during 3D coordinate computation. Different datasets are registered using a new multi-view registration technique. In the validation step, the registered datasets are compared with the CAD model to verify that the measured part meets its specification. While previous methods have only considered the geometrical discrepancies between the sensed part and its CAD model, we also consider statistical information to decide whether the differences are significant",no8476,Precision and error analysis of MATLAB applications during automated hardware synthesis for FPGAs,"We present a compiler that takes high level signal and image processing algorithms described in MATLAB and generates an optimized hardware for an FPGA with external memory. We propose a precision analysis algorithm to determine the minimum number of bits required by an integer variable and a combined precision and error analysis algorithm to infer the minimum number of bits required by a floating point variable. Our results show that on average, our algorithms generate hardware requiring a factor of 5 less FPGA resources in terms of the configurable logic blocks (CLBs) consumed as compared to the hardware generated without these optimizations. We show that our analysis results in the reduction in the size of lookup tables for functions like sin, cos, sqrt, exp etc. Our precision analysis also enables us to pack various array elements into a single memory location to reduce the number external memory accesses. We show that such a technique improves the performance of the generated hardware by an average of 35%",no8475,Memory fault tolerance software mechanisms: design and configuration support through SWN models,"We present a case study of a software fault tolerance mechanisms, the distributed memory, designed and implemented within the European projects TIRAN and DEPAUDE, and currently under study within the Italian project ISIDE. The studied mechanisms are part of a complete framework of general purpose software fault tolerance mechanisms. We show a method for the compositional construction of models of the DM and of the environment in which it operates, expressed in the stochastic well formed nets (SWN) formalism. Different versions of submodels, at different detail level are presented and compared using some behaviour inheritance notions taken from the literature.",no8474,A Candidate Fault Model for AspectJ Pointcuts,"We present a candidate fault model for pointcuts in AspectJ programs. The fault model identifies faults that we believe are likely to occur when writing pointcuts in the AspectJ language. Categories of fault types are identified, and each individual fault type is described as categorized. We argue that a fault model that focuses on the unique constructs of the AspectJ language is needed for the systematic and effective testing of AspectJ programs. Our pointcut fault model is a first step towards such a model",no8473,Characterizing and predicting which bugs get fixed: an empirical study of Microsoft Windows,"We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter. Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68% and recall of 64% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.",no8472,Fault Analysis and Parameter Identification of Permanent-Magnet Motors by the Finite-Element Method,"We performed a time-stepping finite-element-method (FEM) analysis to study a rotor surface-mounted permanent-magnet synchronous machine with insulation failure inter-turn fault. We used FEM for magnetic field study and determining the machine parameters under various fault conditions. We studied the effect of machine pole number and number of faulted turns on machine parameters. Finally, we used the FEM machine model for studying permanent-magnet machine behavior under different fault conditions.",no8471,Model-based synthesis of fault trees from Matlab-Simulink models,"We outline a new approach to safety analysis in which concepts of computer HAZOP are fused with the idea of software fault tree analysis to enable a continuous assessment of an evolving programmable design developed in Matlab-Simulink. We also discuss the architecture of a tool that we have developed to support the new method and enable its application in complex environments. We show that the method and the tool enable the integrated hardware and software analysis of a programmable system and that in the course of that analysis they automate and simplify the development of fault trees for the system. Finally, we propose a demonstration of the method and the tool and we outline the experimental platform and aims of that demonstration.",no8470,Combining FT-MPI with H2O: Fault-Tolerant MPI Across Administrative Boundaries,"We observe increasing interest in aggregating geographically distributed, heterogeneous resources to perform large scale computations. MPI remains the most popular programming paradigm for such applications; however, as the size of computing environments increases, fault tolerance aspects become critically important. We argue that the fault tolerance model proposed by FT-MPI fits well in geographically distributed environments, even though its current implementation is confined to a single administrative domain. We propose to overcome these limitations by combining FTMPI with the H2O resource sharing framework. Our approach allows users to run fault tolerant MPI programs on heterogeneous, geographically distributed shared machines, without sacrificing performance and with minimal involvement of resource providers.",no8469,Impact of Transversal Defects on Confinement Loss of an All-Solid 2-D Photonic-Bandgap Fiber,"We numerically investigate the impact of transversal defects on the minimum confinement loss value in the case of a solid-core photonic-bandgap fiber with parabolic germanium-doped inclusions. We show that a standard deviation of only 5% of either the diameter, the refractive-index contrast, or the position of the high-index inclusions could double the minimal value-as a function of frequency-of losses. Moreover, we demonstrate that, in our case, accurately controlling the position of the doped inclusions along the first ring around the core is more important than having an accurate control on their diameter and index contrast. Oddly enough, we also point out that the defects could lead to a decrease of the minimum loss value. Furthermore, we demonstrate that a structure made of inclusions of two different refractive indices could have lower loss than both structures made of only one of these two types of inclusion.",no8468,An Evolving Model of Software Bug Reports,"We model software bug reports as a topological network called reporter network. By statistical analysis we And that the reporter network displays a number of features (scale-free, small-world, and etc.) shared by other complex networks. In order to understand the origins of these features, an evolving complex network model is proposed for the first time. The experimental results show that the model is able to reproduce many of statistical properties of the reporter network. Moreover, the scaling exponents of power-law distribution are calculated analytically. The calculation results agree well with simulation results.",no8467,Spectral RTL Test Generation for Gate-Level Stuck-at Faults,"We model RTL faults as stuck-at faults on primary inputs, primary outputs, and flip-flops. Tests for these faults are analyzed using Hadamard matrices for Walsh functions and random noise level at each primary input. This information then helps generate vector sequences. At the gate-level, a fault simulator and an integer linear program (ILP) compact the test sequences. We give results for four ITC'99 and four ISC AS'89 benchmark circuits, and an experimental processor. The RTL spectral vectors performed equally well on multiple gate-level implementations. Compared to a gate-level ATPG, RTL vectors produced similar or higher coverage in shorter CPU times",no8466,FDTD Study of Defect Modes in Two-Dimensional Silver Metallo-Dielectric Photonic Crystal,We model defective silver metallo-dielectric photonic crystal by the finite-difference time-domain (FDTD) method. The Drude model with parameters fit to empirical data was used.,no8465,Measuring experimental error in microprocessor simulation,"We measure the experimental error that arises from the use of non-validated simulators in computer architecture research, with the goal of increasing the rigor of simulation-based studies. We describe the methodology that we used to validate a microprocessor simulator against a Compaq DS-10L workstation, which contains an Alpha 21264 processor. Our evaluation suite consists of a set of 21 microbenchmarks that stress different aspects of the 21264 microarchitecture. Using the microbenchmark suite as the set of workloads, we describe how we reduced our simulator error to an arithmetic mean of 2%, and include details about the specific aspects of the pipeline that required extra care to reduce the error. We show how these low-level optimizations reduce average error from 40% to less than 20% on macrobenchmarks drawn from the SPEC2000 suite. Finally, we examine the degree to which performance optimizations are stable across different simulators, showing that researchers would draw different conclusions, in some cases, if using validated simulators",no8464,Practical fault localization for dynamic web applications,"We leverage combined concrete and symbolic execution and several fault-localization techniques to create a uniquely powerful tool for localizing faults in PHP applications. The tool automatically generates tests that expose failures, and then automatically localizes the faults responsible for those failures, thus overcoming the limitation of previous fault-localization techniques that a test suite be available upfront. The fault-localization techniques we employ combine variations on the Tarantula algorithm with a technique based on maintaining a mapping between statements and the fragments of output they produce. We implemented these techniques in a tool called Apollo, and evaluated them by localizing 75 randomly selected faults that were exposed by automatically generated tests in four PHP applications. Our findings indicate that, using our best technique, 87.7% of the faults under consideration are localized to within 1% of all executed statements, which constitutes an almost five-fold improvement over the Tarantula algorithm.",no8463,A different view of fault prediction,"We investigated a different mode of using the prediction model to identify the files associated with a fixed percentage of the faults. The tester could ask the tool to identify which files are likely to contain the bulks of faults, with the tester selecting any desired percentage of faults. Again the tool would return a list ordered in decreasing order of the predicted numbers of faults in the files the model expects to be most problematic. If the number of files identified is too large, the tester could reselect a smaller percentage of faults. This would make the number of files requiring particular scrutiny manageable. We expect both modes to be valuable to professional software testers and developers.",no8462,Dependability of distributed control system fault tolerant units,"We investigate two types of fault tolerant units (FTUs) suitable for dependable distributed control systems and numerically evaluate their reliability and mean time to failure (MTTF). A simple simulation-based methodology to numerically evaluate dependability functions of a wide variety of fault tolerant units is presented. The method is based on simulation of stochastic Petri nets. A set of 15 FTU configurations belonging to five groups is analyzed. Groups 1 and 2 belong to the node oriented category whereas groups 3 through 5 belong to the application oriented category. The methodology allows a quick and accurate evaluation of dependability functions of any distributed control system design in terms of the type of FTU (i.e., node or application), replicas per group, replicas per FTU, and shared replicas.",no8461,Path vs. subpath vs. link restoration for fault management in IP-over-WDM networks: performance comparisons using GMPLS control signaling,"We investigate three restoration techniques (path, subpath, and link restoration) for fault management in an IP-over-WDM network. We have implemented all of these techniques on the ns-2 simulation platform using generalized multiprotocol label switching (GMPLS) control signaling. These techniques can handle practical situations such as simultaneous multiple fiber failures, which are difficult to design for and recover from by nonrestoration techniques. We then present performance measurement results for the three restoration techniques by applying them to a typical nationwide mesh network running IP over WDM. We investigate interesting trade-offs in the performance of the restoration techniques on restoration success rate, average restoration time, availability, and blocking probability.",no8460,Some optimal object-based architectural features for corrective maintenance,"We investigate the relationship between some characteristics of software architecture present at the top-level design stage and the resulting corrective maintainability of five Ada systems. Measures are developed for both internal and external complexity for the subset of packages, within our five projects, that were changed to correct a software fault. These measures involve the context coupling of packages and the number of visible declarations that can be exported by a package. A relationship establishing the optimal number of object couples as a function of the mean number of visible declarations is empirically estimated based on the faults contained within our projects. We find that the optimal number of object couples varies inversely with the mean number of visible declarations. When initially designing a system, or when making modifications to an existing system, this relationship can be used to provide guidance for choosing the most maintainable design among alternative designs.",no8459,The effects of atmospheric correction schemes on the hyperspectral imaging of littoral environments,"We investigate the effects of several atmospheric correction schemes (ACS) on spectral determination, biophysical parameter estimation, and unsupervised classification from hyperspectral data collected over a coastal New Jersey tidal marsh. The ACS examined include: two modes from ACORN 4.0 (modes 1.0 and 1.5), ATREM, FLAASH, Tafkaa-6S, and Tafkaa-tabular. Results from the comparative analysis of derived spectra reveal a high degree of similarity for all methods for terrestrial derived spectra but considerably less so for spectra obtained from aquatic environments. Likewise, the similarities in the terrestrial spectra translate to significant correlations among the ACS for two of four computed biophysical reflectance indices (NDVI and Red Edge Wavelength). Of the remaining two indices, PRI and SIPI, computed values from both ACORN modes were not correlated with other ACS. Results from ISODATA classification for land and water environments, show a consistently high level of agreement (> 90% overall) between both Tafkaa methods. FLAASH also shows a relatively high level of agreement with the Tafkaa methods (> 70%) for land classes, however a relatively low agreement for water classes (< 40%). ACORN mode 1.5 consistently demonstrated the lowest agreement among correction methods",no8458,Error resilience of EZW coder for image transmission in lossy networks,We investigate the effect of network errors on Embedded Zerotree Wavelet (EZW) encoded images and propose modifications to the EZW coder to increase error resilience in bursty packet loss conditions. A hybrid-encoding scheme that uses data interleaving to spread correlated information into independently processed groups and layered encoding to protect significant information within each group is presented. Simulation results for various packet loss percentages show the improved error resiliency of our scheme in random and bursty packet loss environments.,no8457,On the bit-error probability of differentially encoded QPSK and offset QPSK in the presence of carrier synchronization,"We investigate the differences between allowable differential encoding strategies and their associated bit-error probability performances for quadrature phase-shift keying (QPSK) and offset QPSK modulations when the carrier demodulation reference signals are supplied by the optimum (motivated by maximum a posteriori estimation of carrier phase) carrier-tracking loop suitable for that modulation. In particular, we show that in the presence of carrier-synchronization phase ambiguity but an otherwise ideal loop, both the symbol and bit-error probabilities in the presence of differential encoding are identical for the two modulations. On the other hand, when in addition the phase error introduced by the loop's finite signal-to-noise ratio is taken into account, it is shown that the two differentially encoded modulations behave differently, and their performances are no longer equivalent. A similar statement has previously been demonstrated for the same modulations when the phase ambiguity was assumed to have been perfectly resolved by means other than differential encoding.",no8456,On the Correlation between Controller Faults and Instruction-Level Errors in Modern Microprocessors,"We investigate the correlation between register transfer-level faults in the control logic of a modern microprocessor and their instruction-level impact on the execution flow of typical programs. Such information can prove immensely useful in accurately assessing and prioritizing faults with regards to their criticality, as well as commensurately allocating resources to enhance testability, diagnosability, manufacturability and reliability. To this end, we developed an extensive infrastructure which allows injection of stuck-at faults and transient errors of arbitrary starting point and duration, as well as cost-effective simulation and classification of their repercussions into various instruction-level error types. As a test vehicle for our study, we employ a superscalar, dynamically-scheduled, out-of-order, Alpha-like microprocessor, on which we execute SPEC2000 integer benchmarks. Extensive experimentation with faults injected in control logic modules of this microprocessor reveals interesting trends and results, corroborating the utility of this simulation infrastructure and motivating its further development and application to various tasks related to robust design.",no8455,Using software implemented fault inserter in dependability analysis,We investigate program susceptibility to hardware faults in Win32 environment. For this purpose we use the software implemented fault injector FITS. We analyze natural fault resistivity of COTS systems and the effectiveness of various software techniques improving system dependability. The problems of experiment tuning and result interpretation are discussed in context of a wide spectrum of applications.,no8454,Automatic selection of recognition errors by respeaking the intended text,"We investigate how to automatically align spoken corrections with an initial speech recognition result. Such automatic alignment would enable one-step voice-only correction in which users simply respeak their intended text. We present three new models for automatically aligning corrections: a 1-best model, a word confusion network model, and a revision model. The revision model allows users to alter what they intended to write even when the initial recognition was completely correct. We evaluate our models with data gathered from two user studies. We show that providing just a single correct word of context dramatically improves alignment success from 65% to 84%. We find that a majority of users provide such context without being explicitly instructed to do so. We find that the revision model is superior when users modify words in their initial recognition, improving alignment success from 73% to 83%. We show how our models can easily incorporate prior information about correction location and we show that such information aids alignment success. Last, we observe that users speak their intended text faster and with fewer re-recordings than if they are forced to speak misrecognized text.",no8453,Fault recovery in linear systems via intrinsic evolution,"We investigate fault recovery using reconfiguration for analog linear feedback control systems. We assume any faults occur only within the linear system and accessibility to its internal circuitry is impossible. Consequently, the only way to restore service - even degraded service - is by inserting a compensation network into the control loop. System failures are manifested by a change in the original bandwidth. The compensators are evolved intrinsically.",no8452,Task feasibility analysis and dynamic voltage scaling in fault-tolerant real-time embedded systems,"We investigate dynamic voltage scaling (DVS) in real-time embedded systems that use checkpointing for fault tolerance. We present feasibility-of-scheduling tests for checkpointing schemes for a constant processor speed as well as for variable processor speeds. DVS is then carried out on the basis of the feasibility analysis. We incorporate practical issues such as faults during checkpointing and state restoration, rollback recovery time, memory access time and energy, and DVS overhead. Simulation results are presented for real-life checkpointing data and embedded processors.",no8451,Semantic errors in SQL queries: a quite complete list,"We investigate classes of SQL queries which are syntactically correct, but certainly not intended, no matter for which task the query was written. For instance, queries that are contradictory, i.e. always return the empty set, are obviously not intended. However, current database management systems execute such queries without any warning. We give an extensive list of conditions that are strong indications of semantic errors. Of course, questions like the satisfiability are in general undecidable, but a significant subset of SQL queries can actually be checked. We believe that future database management systems perform such checks and that the generated warnings help to develop code with fewer bugs in less time.",no8450,Evaluating the Accuracy of Fault Localization Techniques,"We investigate claims and assumptions made in several recent papers about fault localization (FL) techniques. Most of these claims have to do with evaluating FL accuracy. Our investigation centers on a new subject program having properties useful for FL experiments. We find that Tarantula (Jones et al.) works well on the program, and we show weak support for the assertion that coverage-based test suites help Tarantula to localize faults. Baudry et al. used automatically-generated mutants to evaluate the accuracy of an FL technique that generates many distinct scores for program locations. We find no evidence to suggest that the use of mutants for this purpose is invalid. However, we find evidence that the standard method for evaluating FL accuracy is unfairly biased toward techniques that generate many distinct scores, and we propose a fairer method of accuracy evaluation. Finally, Denmat et al. suggest that data mining techniques may apply to FL. We investigate this suggestion with the data mining tool Weka, using standard techniques for evaluating the accuracy of data mining classifiers. We find that standard classifiers suffer from the class imbalance problem. However, we find that adding cost information improves accuracy.",no8449,Towards Optimal Resource Allocation in Partial-Fault Tolerant Applications,"We introduce Zen, a new resource allocation framework that assigns application components to node clusters to achieve high availability for partial-fault tolerant (PFT) applications. These applications have the characteristic that under partial failures, they can still produce useful output though the output quality may be reduced. Thus, the primary goal of resource allocation for PFT applications is to prevent, delay, or minimize the impact of failures on the application output quality. This paper is the first to approach this resource allocation problem from a theoretical perspective, and obtains a series of results regarding component assignments that provide the highest service availability under the constraints imposed by the application data flow graph and the hosting clusters. We show that (1) even simple versions of this resource allocation problem are NP-Hard, (2) a 2-approximate polynomial-time algorithm works for tree topologies, and (3) a simple greedy component placement performs well in practice for general application topologies. We implement a system prototype to study the application availability achieved by Zen compared to failure-oblivious placement, replication, and Zen+replication. Our experimental results show that three PFT applications achieve significant data output quality and availability benefits using Zen.",no8448,Evaluation of the accuracy and robustness of a motion correction algorithm for PET using a novel phantom approach,"We introduce the use of a novel physical phantom to quantify the performance of a motion-correction algorithm. The goal of the study was to assess a PET-PET image registration, the final output of which is a motion-corrected high-statistics PET image volume, a procedure called Reconstruct, Register and Average (RRA). Methods: A phantom was constructed using 5~2mL Ge-68 filled spheres suspended in a water-filled tank via lightweight fishing line and driven by a periodic motion. Comparison of maximum and mean concentration and sphere volume was performed. Ground truth data were measured using no-motion. With motion, five replicate datasets of 3-minute phase-gated data for each of 3 different periods of motion were acquired. Gated PET images were registered using a multi-resolution level-sets-based non-rigid registration (NRR). The NRR images were then averaged to form a motion-corrected, high-statistics image volume. Spheres from all images were segmented and compared across the imaging conditions. Results: The average center-of-mass range of motion was 7.35, 5.83 and 2.66 mm for the spheres over the three periods of 8, 6 and 4 seconds. The center-of-mass for all spheres in all conditions was corrected to within 1mm on average using NRR as compared to the gated data. For the RRA data, the sphere maximum activity concentration (MAC) was on average 40.2% higher (-4.0% to 116.7%) and sphere volume was on average 12.0% smaller (-8.2% to 28.1%) as compared to the un-gated data with motion. The RRA results for MAC were on average 70% more accurate and for sphere volume 80% more accurate as compared to the un-gated data. Conclusions: The results show that the novel phantom setup and analysis methods are a promising evaluation technique for the assessment of motion correction algorithms. Benefits include the ability to compare against ground truth data without motion but with control of the statistical data quality and background variability. Use of a nonmoving object adjacen- - t to spheres in motion, the spatial extent of the motion correction algorithm was confirmed to be local to the induced motion and to not affect the stationary object. A further benefit of the assessment technique is the use of ground truth data.",no8447,Identifying efficient combinations of error detection mechanisms based on results of fault injection experiments,"We introduce novel performance ratings for error detection mechanisms. Given a proper setup of the fault injection experiments, these ratings can be directly computed from raw readout data. They allow the evaluation of the overall performance of arbitrary combinations of mechanisms without the need for further experiments. With this means we can determine a minimal subset of mechanisms that still provides the required performance",no8446,Scheduling optional computations in fault-tolerant real-time systems,"We introduce an exact schedulability analysis for the optional computation model under a specified failure hypothesis. From this analysis, we propose a solution for determining, before run-time, the degree of fault tolerance allowed in the system. This analysis will allow the system designer to verify if all the tasks in the system meet their deadlines and to decide which optional parts must be discarded if some deadlines would be missed. The identification of feasible options that satisfy some optimality criteria requires the exploration of a potentially large combinatorial space of possible optional parts to discard. Since this complexity is too high to be considered practical in dynamic systems, two heuristic algorithms are proposed for selecting which tasks must be discarded and for guiding the process of searching for feasible options. The performance of the algorithms is measured quantitatively with simulations using synthetic task sets",no8445,Time domain phase noise correction for OFDM signals,"We introduce an algorithm for compensating for carrier phase noise in an OFDM communication system. Through the creation of a linearized parametric model for phase noise, we generate a least squares (LS) estimate of the transmitted symbol. Using digitized DVB-T RF signals created in a laboratory and a DVB-T compliant receiver model, simulation results are presented to evaluate the effectiveness of the algorithm in practical environments.",no8444,Theoretical Lower Error Bound for Comparative Evaluation of Sensor Arrays in Magnetostatic Linear Inverse Problems,"We introduce a theoretical lower error bound for solutions to magnetostatic linear inverse problems and we propose it as a figure of merit for the comparative evaluation of sensor arrays. With the help of the proposed error bound, we demonstrate the superiority of three-axial biomagnetic sensor arrays applying truncated singular value decomposition analysis to a kernel matrix computed from boundary-element-method (BEM) models of the human torso for a biomagnetic application. In simulations, we found that, for a more complex five-compartment BEM model, the advantage of using three-axial measurements is more pronounced, compared to a three-compartment BEM model",no8443,Transition Delay Fault Testing of Microprocessors by Spectral Method,"We introduce a novel spectral method of delay test generation for microprocessors at the register-transfer level (RTL). Vectors are first generated by an available ATPG tool for transition faults on inputs and outputs of the RTL modules of the circuit. These vectors are analyzed using Hadamard matrices to obtain Walsh function components and random noise levels for each primary input. A large number of vector sequences is then generated such that all sequences have the same Walsh spectrum but they differ due to the random noise in them. At the gate-level, a fault simulator and an integer linear program (ILP) compact these vector sequences. The initial RTL vector generation also reveals the hard-to-test parts of the circuit. An XOR observability tree was used to improve the testability of those parts. We give results for an accumulator-based processor named Parwan. The RTL technique produced higher gate-level transition fault coverage in shorter CPU time as compared to a gate-level transition fault ATPG.",no8442,Novel Agent-Based Management for Fault-Tolerance in Network-on-Chip,We introduce a novel agent-based reconfiguring concept for futures network-on-chip (NoC) systems. The necessary properties to increase architecture level fault tolerance are introduced. The system control is modeled as multi-level agent hierarchy that is able to increase application fault-tolerance and performance with autonomous reactions of agents. The agent technology adds a system level intelligence level to the traditional NoC system design. The architecture and functions of this system are described on conceptual level. Communication and reconfiguring data flows are presented as study cases. Principles of reconfiguration of a NoC on faulty environment are demonstrated and simulated. Probability of reconfiguration success is measured with different latency requirements and amount of redundancy by Monte Carlo simulations. The effect of network topology in reconfiguration of a faulty mesh was also under research in the simulations.,no8441,Self-adaptive masking method for automatic shape recognition and motion correction of thallium-201 myocardial perfusion SPECT imaging,"We introduce a new self-adaptive masking method for shape detection of low signal to noise ratio (SNR) images to improve the tracking capabilities of the motion correction in nuclear cardiac imaging. The method is developed using two-dimensional fast Fourier transform, ideal filtering in the frequency domain, recursive thresholding, and region recognition. This method is independent of the correlation between the context of the planar images and has a good tolerance for low SNR images. Also it is robust under the circumstances of significant abrupt motion of the object",no8440,A New Hardware/Software Platform and a New 1/E Neutron Source for Soft Error Studies: Testing FPGAs at the ISIS Facility,"We introduce a new hardware/software platform for testing SRAM-based FPGAs under heavy-ion and neutron beams, capable of tracing the bit-flips in the configuration memory back to the physical resources affected in the FPGA. The validation was performed using, for the first time, the neutron source at the RAL-ISIS facility. The ISIS beam features a 1/E spectrum, which is similar to the terrestrial one with an acceleration between 10<sup>7</sup> and 10<sup>8</sup> in the energy range 10-100 MeV. The results gathered on Xilinx SRAM-based FPGAs are discussed in terms of cross section and circuit-level modifications.",no8439,Fault detection for robot manipulators with parametric uncertainty: a prediction error based approach,"We introduce a new approach to fault detection for robot manipulators. The technique, which is based on the isolation of fault signatures via filtered torque prediction error estimates, does not require measurements or estimates of manipulator acceleration as is the case with some of the previously suggested methods. The method is formally demonstrated to be robust under uncertainty in the robot parameters. Furthermore, an adaptive version of the algorithm is introduced, and shown to both improve coverage and significantly reduce detection times. The effectiveness of the approach is demonstrated by experiments with a two-joint manipulator system",no8438,Nano-scale fault tolerant machine learning for cognitive radio,"We introduce a machine learning based channel state classifier for cognitive radio, designed for nano-scale implementation. The system uses analog computation, and consists of cyclostationary feature extraction and a radial basis function network for classification. The description of the system is partially abstract, but our design choices are motivated by domain knowledge and we believe the system will be feasible for future nanotechnology implementation. We describe an error model for the system, and simulate experimental performance and fault tolerance of the system in recognizing WLAN signals, under different levels of input noise and computational errors. The system performs well under the expected non-ideal manufacturing and operating conditions.",no8437,A Framework for Fault-Tolerant Control of Discrete Event Systems,"We introduce a framework for fault-tolerant supervisory control of discrete-event systems. Given a plant, possessing both faulty and nonfaulty behavior, and a submodel for just the nonfaulty part, the goal of fault-tolerant supervisory control is to enforce a certain specification for the nonfaulty plant and another (perhaps more liberal) specification for the overall plant, and further to ensure that the plant recovers from any fault within a bounded delay so that following the recovery the system state is equivalent to a nonfaulty state (as if no fault ever happened). The specification for the overall plant is more liberal compared to the one for the nonfaulty part since a degraded performance may be allowed after a fault has occurred. We formulate this notion of fault-tolerant supervisory control and provide a necessary and sufficient condition for the existence of such a supervisor. The condition involves the usual notions of controllability, observability and relative-closure, together with the notion of stability. An example of a power system is provided to illustrate the framework. We also propose a weaker notion of fault-tolerance where following the recovery, the system state is simulated by some nonfaulty state, i.e., behaviors following the recovery are also the behaviors from some faulty state. Also, we formulate the corresponding notion of weakly fault-tolerant supervisory control and present a necessary and sufficient condition (involving the notion of language-stability) for the its existence. We also introduce the notion of nonuniformly-bounded fault-tolerance (and its weak version) where the delay-bound for recovery is not uniformly bounded over the set of faulty traces, and show that when the plant model has finitely many states, this more general notion of fault-tolerance coincides with the one in which the delay-bound for recovery is uniformly bounded.",no8436,A hybrid scatter correction for 3D PET based on an estimation of the distribution of unscattered coincidences: implementation on the ECAT EXACT HR+,"We implemented a hybrid scatter correction method for 3D PET that combines two scatter correction methods in a complementary way. The implemented scheme uses a method based on the discrimination of the energy of events (the estimation of trues method, ETM) and an auxiliary method (the single scatter simulation method, or the convolution-subtraction method), in an attempt to increase the accuracy of the correction over a wider range of acquisitions. The ETM takes into account the scatter from outside the field-of-view (FOV), which is not estimated with the auxiliary method. On the other hand, the auxiliary method accounts for events that have scattered with small angles, which have an energy that can not be discriminated from that of unscattered events using the ETM. The ETM uses the data acquired in an upper energy window above the photopeak (550-650 keV), to obtain a noisy estimate of the unscattered events in the standard window (350-650 keV). Our implementation uses the auxiliary method to correct the residual scatter in the upper window. After appropriate scaling, the upper window data is subtracted from the total coincidences acquired in the standard window, resulting in the final scatter estimate, after smoothing. We compare the hybrid method with the corrections used by default in the 2D and 3D modes of the ECAT EXACT HR+, using phantom measurements. Generally, the contrast was better with the hybrid method, although the relative errors of quantification were similar. We conclude that hybrid techniques such as the one implemented in this work can provide an accurate, general-purpose and practical way to correct the scatter in 3D PET, taking into account the scatter from outside the FOV",no8435,Simulating the Multipath Channel With a Reverberation Chamber: Application to Bit Error Rate Measurements,"We illustrate the use of the reverberation chamber to simulate fixed wireless propagation environments including effects such as narrowband fading and Doppler spread. These effects have a strong impact on the quality of the wireless channel and the ability of a receiver to decode a digitally modulated signal. Different channel characteristics such as power delay profile and RMS delay spread are varied inside the chamber by incorporating various amounts of absorbing material. In order to illustrate the impact of the chamber configuration on the quality of a wireless communication channel, bit error rate measurements are performed inside the reverberation chamber for different loadings, symbol rates, and paddle speeds; the results are discussed. Measured results acquired inside a chamber are compared with those obtained both in an actual industrial environment and in an office.",no8434,The effect of nonlinear signal transformations on bias errors in elastography,"We have reported several artifacts in elastography (1991). These include mechanical artifacts, such as stress concentration, and signal processing artifacts, such as zebras, which are caused by bias errors incurred during the estimation of the peak of correlation functions using a curve-fitting method. We investigate the bias errors and show that bias errors in curve-fitting methods are substantially increased because of nonlinear operations on the echo signals that reduce other errors. We also show that, for typical sampling rates, the bias errors can be ignored in the absence of these nonlinear operations.",no8433,Study on the detection and correction of software based on UML,"We have proposed an approach to the correction of anti-patterns; we believe that before attempting such corrections it is important to have confirmation from the developer. Assumptions are made when identifying or correcting certain anti-patterns; however, these assumptions that we recognize as the causes of the anti-pattern may be the behavior the developer actually intended with the design, or may not be the most optimal correction for the anti pattern. We propose these transformations as a guide for the improvement of the design; nevertheless the decision of applying the changes should be left to the user.",no8432,Error recovery for a boiler system with OTS PID controller,"We have previously presented initial results of a case study which illustrated an approach to engineering protective wrappers as a means of detecting errors or unwanted behaviour in systems employing an OTS (off-the-shelf) item. The case study used a Simulink model of a steam boiler system together with an OTS PID (proportional, integral and derivative) controller. The protective wrappers are developed for the model of the system in such a way that they allow detection and tolerance of typical errors caused by unavailability of signals, violations of range limitations, and oscillations. In this paper, we extend the case study to demonstrate how forward error recovery based on exception handling can be systematically incorporated at the level of the protective wrappers.",no8431,A Case Study of Defect-Density and Change-Density and their Progress over Time,"We have performed an empirical case study, investigating defect-density and change-density of a reusable framework compared with one application reusing it over time at a large Oil and Gas company in Norway, Statoil ASA. The framework, called JEF, consists of seven components grouped together, and the application, called DCF, reuses the framework, without modifications to the framework. We analyzed all trouble reports and change requests from three releases of both. Change requests in our study covered any changes (not correcting defects) in the requirements, while trouble reports covered any reported defects. Additionally, we have investigated the relation between defect-density and change-density both for the reusable JEF framework and the application. The results revealed that the defect-density of the reusable framework was lower than the application. The JEF framework had higher change-density in the first release, but lower change-density than the DCF application over the successive releases. For the DCF application, on the other hand, a slow increase in change-density appeared. On the relation between change-density and defect-density for the JEF framework, we found a decreasing defect-density and change-density. The DCF application here showed a decreasing defect-density, with an increasing change-density. The results show that the quality of the reusable framework improves and it becomes more stable over several releases, which is important for reliability of the framework and assigning resources",no8430,Predicting Defects for Eclipse,"We have mapped defects from the bug database of eclipse (one of the largest open-source projects) to source code locations. The resulting data set lists the number of pre- and post-release defects for every package and file in the eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data with common complexity metrics. All data is publicly available and can serve as a benchmark for defect prediction models.",yes8429,Identification of the atomic scale defects involved in radiation damage in HfO<sub>2</sub> based MOS devices,"We have identified the structure of three atomic scale defects which almost certainly play important roles in radiation damage in hafnium oxide based metal oxide silicon technology. We find that electron trapping centers dominate the HfO<sub>2</sub> radiation response. We find two radiation induced trapped electron centers in the HfO<sub>2</sub>: an O<sub>2</sub><sup>-</sup> coupled to a hafnium ion and an HfO<sub>2</sub> oxygen vacancy center which is likely both an electron trap and a hole trap. We find that, under some circumstances, Si/dielectric interface traps similar to the Si/SiO<sub>2</sub> P<sub>b</sub> centers are generated by irradiation. Our results show that there are very great atomic scale differences between radiation damage in conventional Si/SiO<sub>2</sub> devices and the new Si/dielectric devices based upon HfO<sub>2</sub>.",no8428,Thermoreflectance imaging of defects in thin-film solar cells,We have identified and characterized various defects in thin-film a-Si and CIGS solar cells with sub-micron spatial resolution using thermoreflectance imaging. A megapixel silicon-based CCD was used to obtain noncontact thermal images simultaneously with visible electroluminescence (EL) images. EL can be indicative of pre-breakdown sites due to trap assisted tunneling and stress induced leakage currents. Physical defects appear at reverse bias voltages of 8 V in a-Si samples. Linear and nonlinear shunt defects are investigated as well as electroluminescent breakdown regions at reverse biases as low as 4.5 V. Pre-breakdown sites with electroluminescence are investigated.,yes8427,Comparing the effects of standard and segmented attenuation correction,"We have evaluated a segmentation algorithm developed by General Electric for the Advance positron emission scanner (PET). Phantom studies were performed to measure the accuracy in emission scans reconstructed with segmented, attenuation data as a function of transmission scan time. The results indicated errors of less than 2% will be made in emission scan data reconstructed with transmission scan times of 3 minutes. Based on the phantom results, 185 patient data sets were acquired with both long (15 min.) non-segmented and short (3 min.) segmented attenuation scans. Comparisons of scan data in foci of abnormal uptake yielded a correlation coefficient between long and short scan SUV maximum values of 0.99 and a mean absolute difference of 4.6%. The average SUV values in lung between long and short has a correlation coefficient of 0.99 and a mean absolute difference of 3.1%. The corresponding values from the liver had a correlation coefficient of 0.96 and mean absolute difference of 7.4%. Visual review by physicians noted minor differences, but when grading the images on a scale of 1 to 5, 91% of the time there was no difference. In all cases comparing the long and short attenuation and no abnormal sites were missed",no8426,"Argus: Low-Cost, Comprehensive Error Detection in Simple Cores","We have developed Argus, a novel approach for providing low-cost, comprehensive error detection for simple cores. The key to Argus is that the operation of a von Neumann core consists of four fundamental tasks - control flow, dataflow, computation, and memory access - that can be checked separately. We prove that Argus can detect any error by observing whether any of these tasks are performed incorrectly. We describe a prototype implementation, Argus-1, based on a single-issue, 4-stage, in-order processor to illustrate the potential of our approach. Experiments show that Argus-1 detects transient and permanent errors in simple cores with much lower impact on performance (<4% average overhead) and chip area (<17% overhead) than previous techniques.",no8425,On-line sensor calibration and error modeling using single actuator stimulus,"We have developed an on-line in-field nonparametric calibration and error modeling approach. The approach employs a single excitation source as the external stimulus to create differential sensor readings. Under very mild assumptions imposed on the calibration functions, error model and the environment model, the technique utilizes the maximal likelihood principle and a nonlinear function minimization to derive both simultaneously the calibration function and the error model of a specified accuracy. Resubstitution is then used in order to establish the interval of confidence. The approach is intrinsically localized and we present two variants: i) one where only pairs of neighboring sensors communicate in order to conduct calibration and construct error model; ii) one where a provably minimum amount of communication is achieved. While the idea of employing external actuators to conduct calibration is generic in the sense that it can be applied to any sensor modality, in this paper we demonstrate and evaluate the approach using traces from light sensors and acoustic signal-based distance measurements recorded by in-field deployed sensors.",no8424,Automatic Recognition of Defect Signatures and Notification of Tool Malfunctions - IECON'06,"We have developed an automatic method that efficiently detects the defect signatures of substrates and identifies possible problems pertaining to LSI/TFT-LCD manufacturing processes and tools. This system, which has no built-in libraries, can be applied to the mass production line of thin film devices. This method is useful to quickly detect problems that have been overlooked thus far",no8423,Clinical evaluation of real-time phase-aberration correction system [medical ultrasound],"We have developed a phase-aberration correction system that correlates signals in real time. In this paper we evaluate the clinical performance of the system in vivo. This system 1) constructs a cross-sectional image and, 2) calculates the correlation of signals at neighboring sensor elements. Both 1) and 2) are carried out in real time. The system was used for imaging of living tissue. First the beam was formed using initial focus delay settings and a real-time cross-sectional image was constructed. The correlation between neighboring signals was calculated simultaneously with the beam formation and the time difference between the pairs of signals was acquired. The time difference was then used to compensate for the initial delay. The image of the living tissue was substantially improved after the compensation. A further experiment is in progress to collect a statistically significant number of clinical results",no8422,Consistency Error Modeling-based Localization in Sensor Networks,"We have developed a new error modeling and optimization-based localization approach for sensor networks in presence of distance measurement noise. The approach is solely based on the concept of consistency. The error models are constructed using non-parametric statistical techniques; they do not only indicate the most likely error, but also provide the likelihood distribution of particular errors occurring. The models are evaluated using the learn-and-test techniques and serve as the objective functions for the task of localization. The localization problem is formulated as task of maximizing consistency between measurements and calculated distances. We evaluated the approach in (i) both GPS-based and GPS-less scenarios; (ii) 1-D, 2-D and 3-D spaces, on sets of acoustic ranging-based distance measurements recorded by deployed sensor networks. The experimental evaluation indicates that localization of only a few centimeters is consistently achieved when the average and median distance measurement errors are more than a meter, even when the nodes have only a few distance measurements. The relative performance in terms of location accuracy compare favorably with respect to several state-of-the-art localization approaches. Finally, several insightful observations about the required conditions for accurate localization are deduced by analyzing the experimental results",no8421,Prediction models for software fault correction effort,"We have developed a model to explain and predict the effort associated with changes made to software to correct faults while it is undergoing development. Since the effort data available for this study is ordinal in nature, ordinal response models are used to explain the effort in terms of measures of fault locality and the characteristics of the software components being changed. The calibrated ordinal response model is then applied to two projects not used in the calibration to examine predictive validity",no8420,Object-oriented executives and components for fault tolerance,"We have created two kinds of reusable, object-oriented software components to facilitate building fault tolerant applications. Executive components orchestrate familiar software fault tolerance techniques in a data type independent manner. Building block components provide fault tolerance utilities and application-specific functions. We use a three-level class framework (or design pattern) to create data type and application-independent classes at the highest level, define data type-dependent base classes in the middle level, and organize application and data type-specific derived classes at the lowest level. This approach employs polymorphism, pointer conversions and Run-Time Type Information. These techniques have successfully handled applications with dissimilar data types. Reusing these components greatly speeds the development of applications that exploit software fault tolerance techniques",no8419,Using Developer Information as a Factor for Fault Prediction,"We have been investigating different prediction models to identify which files of a large multi-release industrial software system are most likely to contain the largest numbers of faults in the next release. To make predictions we considered a number of different file characteristics and change information about the files, and have built fully- automatable models that do not require that the user have any statistical expertise. We now consider the effect of adding developer information as a prediction factor and assess the extent to which this affects the quality of the predictions.",yes8418,Correctness of a Fault-Tolerant Real-Time Scheduler and its Hardware Implementation,"We formalize the correctness of a fault-tolerant scheduler in a time-triggered architecture. Where previous research elaborated on real-time protocol correctness, we extend this work to gate-level hardware. This requires a sophisticated analysis of analog bit-level synchronization and transmission. Our case-study is a concrete automotive bus controller (ABC), inspired by the FlexRay standard. For a set of interconnected ABCs, vulnerable to sudden failure, we prove at gate-level, that all operating ABCs are synchronized tightly enough such that messages are broadcast correctly. This includes formal arguments for startup, failures, and reintegration of nodes at arbitrary times. To the best of our knowledge, this is the first effort tackling fault-tolerant scheduling correctness at gate-level.",no8417,Reducing human error in simulation in General Motors,"We focus on the steps taken to minimize human error in simulation modeling in General Motors. While errors are costly and undesirable in any field, they are especially harmful in simulation which has been struggling to gain acceptance in the business world for a long time. The solution discussed can be summarized as ""enter the data once and use the best tool for the job"".",no8416,Complexity issues in automated synthesis of failsafe fault-tolerance,"We focus on the problem of synthesizing failsafe fault-tolerance where fault-tolerance is added to an existing (fault-intolerant) program. A failsafe fault-tolerant program satisfies its specification (including safety and liveness) in the absence of faults. However, in the presence of faults, it satisfies its safety specification. We present a somewhat unexpected result that, in general, the problem of synthesizing failsafe fault-tolerant distributed programs from their fault-intolerant version is NP-complete in the state space of the program. We also identify a class of specifications, monotonic specifications, and a class of programs, monotonic programs, for which the synthesis of failsafe fault-tolerance can be done in polynomial time (in program state space). As an illustration, we show that the monotonicity restrictions are met for commonly encountered problems, such as Byzantine agreement, distributed consensus, and atomic commitment. Furthermore, we evaluate the role of these restrictions in the complexity of synthesizing failsafe fault-tolerance. Specifically, we prove that if only one of these conditions is satisfied, the synthesis of failsafe fault-tolerance is still NP-complete. Finally, we demonstrate the application of monotonicity property in enhancing the fault-tolerance of (distributed) nonmasking fault-tolerant programs to masking.",no8415,Execution model for outsourced corrective maintenance,"We focus on corrective maintenance carried out in the outsourced mode under strict service level agreements and present the characteristics of the problem and the activities performed. We detail the information requirements for various maintenance services, such as, emergency maintenance, production support, and corrective maintenance. We present the concept of a system execution model with its constituent nodes and arcs and present the steps to build the same. We present a case study of a large commercial outsourcing project to demonstrate how the execution model can help in making quick decisions that reduces the turn around times of corrective maintenance requests.",no8414,Constraint Based Automated Synthesis of Nonmasking and Stabilizing Fault-Tolerance,"We focus on constraint based automated addition of nonmasking and stabilizing fault-tolerance to hierarchical programs. We specify legitimate states of the program in terms of constraints that should be satisfied in those states. To deal with faults that may violate these constraints, we add recovery actions while ensuring interference freedom among the recovery actions added for satisfying different constraints. Since the constraint based manual design of fault tolerance is well known to be applicable in the manual design of nonmasking fault tolerance, we expect our approach to have a significant benefit in automation of fault tolerant programs. We illustrate our algorithms with three case studies: stabilizing mutual exclusion, stabilizing diffusing computation, and a data dissemination problem in sensor networks. With experimental results,we show that the complexity of synthesis is reasonable and that it can be reduced using the structure of the hierarchical systems. To our knowledge, this is the first instance where automated synthesis has been successfully used in synthesizing programs that are correct under fairness assumptions. Moreover, in two of the case studies considered in this paper, the structure of the recovery paths is too complex to permit existing heuristic based approaches for adding recovery.",no8413,Fault-tolerant high-performance matrix multiplication: theory and practice,"We extend the theory and practice regarding algorithmic fault-tolerant matrix-matrix multiplication, C=AB, in a number of ways. First, we propose low-overhead methods for detecting errors introduced not only in C but also in A and/or B. Second, we show that, theoretically, these methods will detect all errors as long as only one entry, is corrupted. Third we propose a low-overhead roll-back approach to correct errors once detected. Finally, we give a high-performance implementation of matrix-matrix multiplication that incorporates these error detection and correction methods. Empirical results demonstrate that these methods work well in practice while imposing an acceptable level of overhead relative to high-performance implementations without fault-tolerance.",no8412,Compiler-directed instruction duplication for soft error detection,"We experiment with compiler-directed instruction duplication to detect soft errors in VLIW datapaths. In the proposed approach, the compiler determines the instruction schedule by balancing the permissible performance degradation with the required degree of duplication. Our experimental results show that our algorithms allow the designer to perform tradeoff analysis between performance and reliability.",no