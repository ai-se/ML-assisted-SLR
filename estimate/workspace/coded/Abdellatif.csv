Document Title,Abstract,Year,PDF Link,label,code,time
Developer Dashboards: The Need for Qualitative Analytics,"Prominent technology companies including IBM, Microsoft, and Google have embraced an analytics-driven culture to help improve their decision making. Analytics aim to help practitioners answer questions critical to their projects, such as ""Are we on track to deliver the next release on schedule?"" and ""Of the recent features added, which are the most prone to defects?"" by providing fact-based views about projects. Analytic results are often quantitative in nature, presenting data as graphical dashboards with reports and charts. Although current dashboards are often geared toward project managers, they aren't well suited to help individual developers. Mozilla developer interviews show that developers face challenges maintaining a global understanding of the tasks they're working on and that they desire improved support for situational awareness, a form of qualitative analytics that's difficult to achieve with current quantitative tools. This article motivates the need for qualitative dashboards designed to improve developers' situational awareness by providing task tracking and prioritizing capabilities, presenting insights on the workloads of others, listing individual actions, and providing custom views to help manage workload while performing day-to-day development tasks.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509380,yes,yes,1485873347.233786
CrashLocator: locating crashing faults based on crash stacks,"Software crash is common. When a crash occurs, software developers can receive a report upon user permission. A crash report typically includes a call stack at the time of crash. An important step of debugging a crash is to identify faulty functions, which is often a tedious and labor-intensive task. In this paper, we propose CrashLocator, a method to locate faulty functions using the crash stack information in crash reports. It deduces possible crash traces (the failing execution traces that lead to crash) by expanding the crash stack with functions in static call graph. It then calculates the suspiciousness of each function in the approximate crash traces. The functions are then ranked by their suspiciousness scores and are recommended to developers for further investigation. We evaluate our approach using real-world Mozilla crash data. The results show that our approach is effective: we can locate 50.6%, 63.7% and 67.5% of crashing faults by examining top 1, 5 and 10 functions recommended by CrashLocator, respectively. Our approach outperforms the conventional stack-only methods significantly.",2014,http://dl.acm.org/citation.cfm?id=2610386&CFID=696538919&CFTOKEN=83912867,yes,yes,1485873345.516949
AR-miner: mining informative reviews for developers from mobile app marketplace,"With the popularity of smartphones and mobile devices, mobile application (a.k.a. ñappî) markets have been growing exponentially in terms of number of users and downloads. App developers spend considerable effort on collecting and exploiting user feedback to improve user satisfaction, but suffer from the absence of effective user review analytics tools. To facilitate mobile app developers discover the most ñinformativeî user reviews from a large and rapidly increasing pool of user reviews, we present ñAR-Minerî „ a novel computational framework for App Review Mining, which performs comprehensive analytics from raw user reviews by (i) first extracting informative user reviews by filtering noisy and irrelevant ones, (ii) then grouping the informative reviews automatically using topic modeling, (iii) further prioritizing the informative reviews by an effective review ranking scheme, (iv) and finally presenting the groups of most ñinformativeî reviews via an intuitive visualization approach. We conduct extensive experiments and case studies on four popular Android apps to evaluate AR-Miner, from which the encouraging results indicate that AR-Miner is effective, efficient and promising for app developers.",2014,http://dl.acm.org/citation.cfm?id=2568263&CFID=696538919&CFTOKEN=83912867,yes,yes,1485873343.54031
Searching under the Streetlight for Useful Software Analytics,"For more than 15 years, researchers at the Collaborative Software Development Laboratory at the University of Hawaii at Manoa have looked for analytics that help developers understand and improve development processes and products. This article reviews that research and discusses the trade-off between studying easily obtained analytics and studying richer analytics with higher overhead.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509376,yes,yes,1485873343.102514
Performance debugging in the large via mining millions of stack traces,"Given limited resource and time before software release, development-site testing and debugging become more and more insufficient to ensure satisfactory software performance. As a counterpart for debugging in the large pioneered by the Microsoft Windows Error Reporting (WER) system focusing on crashing/hanging bugs, performance debugging in the large has emerged thanks to available infrastructure support to collect execution traces with performance issues from a huge number of users at the deployment sites. However, performance debugging against these numerous and complex traces remains a significant challenge for performance analysts. In this paper, to enable performance debugging in the large in practice, we propose a novel approach, called StackMine, that mines callstack traces to help performance analysts effectively discover highly impactful performance bugs (e.g., bugs impacting many users with long response delay). As a successful technology-transfer effort, since December 2010, StackMine has been applied in performance-debugging activities at a Microsoft team for performance analysis, especially for a large number of execution traces. Based on real-adoption experiences of StackMine in practice, we conducted an evaluation of StackMine on performance debugging in the large for Microsoft Windows 7. We also conducted another evaluation on a third-party application. The results highlight substantial benefits offered by StackMine in performance debugging in the large for large-scale software systems.",2012,http://dl.acm.org/citation.cfm?id=2337241&CFID=696538919&CFTOKEN=83912867,yes,yes,1485873343.102513
BugMap: a topographic map of bugs,"A large and complex software system could contain a large number of bugs. It is desirable for developers to understand how these bugs are distributed across the system, so they could have a better overview of software quality. In this paper, we describe BugMap, a tool we developed for visualizing large-scale bug location information. Taken source code and bug data as the input, BugMap can display bug localizations on a topographic map. By examining the topographic map, developers can understand how the components and files are affected by bugs. We apply this tool to visualize the distribution of Eclipse bugs across components/files. The results show that our tool is effective for understanding the overall quality status of a large-scale system and for identifying the problematic areas of the system.",2013,http://dl.acm.org/citation.cfm?id=2494582&CFID=696538919&CFTOKEN=83912867,yes,yes,1485873342.712801
Towards tool support for analyzing legacy systems in technical domains,"Software in technical domains contains extensive and complex computations in a highly-optimized and unstructured way. Such software systems developed and maintained over years are prone to become legacy code based on old technology and without accurate documentation. We have conducted several industrial projects to reengineer and re-document legacy systems in electrical engineering and steel making domains by means of self-provided techniques and tools. Based on this experience, we derived requirements for a toolkit to analyze legacy code in technical domains and developed a corresponding toolkit including feature location and static analysis on a multi-language level. We have applied our approach and toolkit for software systems implemented in the C++, Fortran, and PL/SQL programming languages and illustrate main benefits of our approach from these experiences.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6747197,yes,yes,1485873340.941482
The Solid* toolset for software visual analytics of program structure and metrics comprehension: From research prototype to product,"Software visual analytics (SVA) tools combine static program analysis and fact extraction with information visualization to support program comprehension. However, building efficient and effective SVA tools is highly challenging, as it involves extensive software development in program analysis, graphics, information visualization, and interaction. We present a SVA toolset for software maintenance, and detail two of its components which target software structure, metrics and code duplication. We illustrate the toolset's usage for constructing software visualizations with examples in education, research, and industrial contexts. We discuss the design evolution from research prototypes to integrated, scalable, and easy-to-use products, and present several guidelines for the development of efficient and effective SVA solutions.",2014,http://dl.acm.org/citation.cfm?id=2537325&CFID=696538919&CFTOKEN=83912867,yes,yes,1485873340.117177
Software Analytics for Mobile Applications--Insights & Lessons Learned,"Mobile applications, known as apps, are software systems running on handheld devices, such as smartphones and tablet PCs. The market of apps has rapidly expanded in the past few years into a multi-billion dollar business. Being a new phenomenon, it is unclear whether approaches to maintain and comprehend traditional software systems can be ported to the context of apps. We present a novel approach to comprehend apps from a structural and historical perspective, leveraging three factors for the analysis: source code, usage of third-party APIs, and historical data. We implemented our approach in a web-based software analytics platform named SAMOA. We detail our approach and the supporting tool, and present a number of findings obtained while investigating a corpus of mobile applications. Our findings reveal that apps differ significantly from traditional software systems in a number of ways, which calls for the development of novel approaches to maintain and comprehend them.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498463,yes,yes,1485873339.853873
SQuAVisiT: A Flexible Tool for Visual Software Analytics,"We present the Software Quality Assessment and Visualization Toolset (SQuAVisiT), a flexible tool for visual software analytics. Visual software analytics supports analytical reasoning about software systems facilitated by interactive visual interfaces. In particular, SQuAVisiT assists software developers, maintainers and assessors in performing quality assurance and maintenance tasks. Flexibility of SQuAVisiT allows for integration of multiple programming languages and variety of analysis and visualization tools.SQuAVisiT has been successfully applied in a number of case studies, ranging from hundreds to thousands KLOC, from homogeneous to heterogeneous systems.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812790,yes,yes,1485873339.853865
Maleku: An evolutionary visual software analysis tool for providing insights into software evolution,"Software maintenance is a complex process that requires the understanding and comprehension of software project details. It involves the understanding of the evolution of the software project, hundreds of software components and the relationships among software items in the form of inheritance, interface implementation, coupling and cohesion. Consequently, the aim of evolutionary visual software analytics is to support software project managers and developers during software maintenance. It takes into account the mining of evolutionary data, the subsequent analysis of the results produced by the mining process for producing evolution facts, the use of visualizations supported by interaction techniques and the active participation of users. Hence, this paper proposes an evolutionary visual software analytics tool for the exploration and comparison of project structural, interface implementation and class hierarchy data, and the correlation of structural data with metrics, as well as socio-technical relationships. Its main contribution is a tool that automatically retrieves evolutionary software facts and represent them using a scalable visualization design.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080838,yes,yes,1485873339.633277
Interactive Exploration of Collaborative Software-Development Data,"Modern collaborative software-development tools generate a rich data record, over the lifecycle of the project, which can be analyzed to provide team members and managers with insights into the performance and contributions of individual members and the overall team dynamic. This data can be analyzed from different perspectives, sliced and diced across different dimensions, and visualized in different ways. Frequently the most useful analysis depends on the actual data, which makes the design of single authoritative visualization a challenge. In this paper we describe an analysis and visualization tool that supports the flexible run-time mapping of such a data record to a number of alternative visualizations. We have used our framework to analyze and gain an understanding of how individuals work within their teams and how teams differ in their work on these term projects.",2013,http://dl.acm.org/citation.cfm?id=2550626&CFID=696538919&CFTOKEN=83912867,yes,yes,1485873339.411825
CODEMINE: Building a Software Development Data Analytics Platform at Microsoft,"The scale and speed of today's software development efforts impose unprecedented constraints on the pace and quality of decisions made during planning, implementation, and postrelease maintenance and support for software. Decisions during the planning process include level of staffing and choosing a development model given the scope of a project and timelines. Tracking progress, course correcting, and identifying and mitigating risks are key in the development phase, as are monitoring aspects of and improving overall customer satisfaction in the maintenance and support phase. Availability of relevant data can greatly increase both the speed and likelihood of making a decision that leads to a successful software system. This article outlines the process Microsoft has gone through developing CODEMINE--a software development data analytics platform for collecting and analyzing engineering process data&mdash;its constraints, and pivotal organizational and technical choices.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509369,yes,yes,1485873339.411824
Process mining software repositories from student projects in an undergraduate software engineering course,"An undergraduate level Software Engineering courses generally consists of a team-based semester long project and emphasizes on both technical and managerial skills. Software Engineering is a practice-oriented and applied discipline and hence there is an emphasis on hands-on development, process, usage of tools in addition to theory and basic concepts. We present an approach for mining the process data (process mining) from software repositories archiving data generated as a result of constructing software by student teams in an educational setting. We present an application of mining three software repositories: team wiki (used during requirement engineering), version control system (development and maintenance) and issue tracking system (corrective and adaptive maintenance) in the context of an undergraduate Software Engineering course. We propose visualizations, metrics and algorithms to provide an insight into practices and procedures followed during various phases of a software development life-cycle. The proposed visualizations and metrics (learning analytics) provide a multi-faceted view to the instructor serving as a feedback tool on development process and quality by students. We mine the event logs produced by software repositories and derive insights such as degree of individual contributions in a team, quality of commit messages, intensity and consistency of commit activities, bug fixing process trend and quality, component and developer entropy, process compliance and verification. We present our empirical analysis on a software repository dataset consisting of 19 teams of 5 members each and discuss challenges, limitations and recommendations.",2014,http://dl.acm.org/citation.cfm?id=2591152&CFID=696538919&CFTOKEN=83912867,yes,yes,1485873339.244741
Estimating development effort in free/open source software projects by mining software repositories: a case study of OpenStack,"Because of the distributed and collaborative nature of free / open source software (FOSS) projects, the development effort invested in a project is usually unknown, even after the software has been released. However, this information is becoming of major interest, especially ---but not only--- because of the growth in the number of companies for which FOSS has become relevant for their business strategy. In this paper we present a novel approach to estimate effort by considering data from source code management repositories. We apply our model to the OpenStack project, a FOSS project with more than 1,000 authors, in which several tens of companies cooperate. Based on data from its repositories and together with the input from a survey answered by more than 100 developers, we show that the model offers a simple, but sound way of obtaining software development estimations with bounded margins of error.",2014,http://dl.acm.org/citation.cfm?id=2597107&CFID=696538919&CFTOKEN=83912867,yes,yes,1485873338.635567
"Effect of temporal collaboration network, maintenance activity, and experience on defect exposure","Context: Number of defects fixed in a given month is used as an input for several project management decisions such as release time, maintenance effort estimation and software quality assessment. Past activity of developers and testers may help us understand the future number of reported defects. Goal: To find a simple and easy to implement solution, predicting defect exposure. Method: We propose a temporal collaboration network model that uses the history of collaboration among developers, testers, and other issue originators to estimate the defect exposure for the next month. Results: Our empirical results show that temporal collaboration model could be used to predict the number of exposed defects in the next month with R2 values of 0.73. We also show that temporality gives a more realistic picture of collaboration network compared to a static one. Conclusions: We believe that our novel approach may be used to better plan for the upcoming releases, helping managers to make evidence based decisions.",2014,http://dl.acm.org/citation.cfm?id=2652586&CFID=696538919&CFTOKEN=83912867,yes,yes,1485873338.635564
Constructing Defect Predictors and Communicating the Outcomes to Practitioners,"Background: An alternative to expert-based decisions is to take data-driven decisions and software analytics is the key enabler for this evidence-based management approach. Defect prediction is one popular application area of software analytics, however with serious challenges to deploy into practice. Goal: We aim at developing and deploying a defect prediction model for guiding practitioners to focus their activities on the most problematic parts of the software and improve the efficiency of the testing process. Method: We present a pilot study, where we developed a defect prediction model and different modes of information representation of the data and the model outcomes, namely: commit hotness ranking, error probability mapping to the source and visualization of interactions among teams through errors. We also share the challenges and lessons learned in the process. Result: In terms of standard performance measures, the constructed defect prediction model performs similar to those reported in earlier studies, e.g. 80% of errors can be detected by inspecting 30% of the source. However, the feedback from practitioners indicates that such performance figures are not useful to have an impact in their daily work. Pointing out most problematic source files, even isolating error-prone sections within files are regarded as stating the obvious by the practitioners, though the latter is found to be helpful for activities such as refactoring. On the other hand, visualizing the interactions among teams, based on the errors introduced and fixed, turns out to be the most helpful representation as it helps pinpointing communication related issues within and across teams. Conclusion: The constructed predictor can give accurate information about the most error prone parts. Creating practical representations from this data is possible, but takes effort. The error prediction research done in Elektrobit Wireless Ltd is concluded to be useful and we will further improve the present- tions made from the error prediction data.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681379,yes,yes,1485873338.447812
Economic Governance of Software Delivery,"Agility without objective governance cannot scale, and governance without agility cannot compete. Agile methods are mainstream, and software enterprises are adopting these practices in diverse delivery contexts and at enterprise scale. IBM's broad industry experience with agile transformations and deep internal know-how point to two key principles to deliver sustained improvements in software business outcomes with higher confidence: measure and streamline change costs, and steer with economic governance and Bayesian analytics. Applying these two principles in context is the crux of measured improvement in continuous delivery of smarter software-intensive systems. This article describes more meaningful measurement and prediction foundations for economic governance. The Web extra at http://youtu.be/ghAM8ifyeVI is a video in which Walker Royce, author, IEEE Software editorial board member, and IBM Chief Software Economist, describes how to reason about software delivery governance with lean principles.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6581776,no,no,1485873347.233787
Smarter bridges through advanced structural health monitoring,"This paper describes an IBM and the University of Miami joint research project providing infrastructure and application components to advance structural health monitoring of civil infrastructures such as bridges. We discuss a newly developed software infrastructure that enables a shift from batch to continuous bridge monitoring, providing continuous real-time sensor data collection and forwarding to a monitoring location where the data is cleansed (e.g., corrected), normalized, and recorded. We also discuss how significant load events are detected and fracture mechanics analytics are applied to assess bridge structural health. Finally, we discuss visualization of the raw and processed data. The University of Miami civil engineers developed the continual analytics techniques based on fracture mechanics and acoustic-emission analytics that together with the software infrastructure make it possible to perform more accurate and real-time monitoring of bridge deterioration. For civil structural health monitoring, this introduces a shift from time- to event-based analytics by exploiting a rolling, potentially complex, significant event-based threshold that triggers the analytics. The system was deployed on a single laboratory test specimen to assess the validity of the messaging and analytical tools in a simulated bridge environment.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697271,no,no,1485873347.233784
An adaptive parameter space-filling algorithm for highly interactive cluster exploration,"For a user to perceive continuous interactive response time in a visualization tool, the rule of thumb is that it must process, deliver, and display rendered results for any given interaction in under 100 milliseconds. In many visualization systems, successive interactions trigger independent queries and caching of results. Consequently, computationally expensive queries like multidimensional clustering cannot keep up with rapid sequences of interactions, precluding visual benefits such as motion parallax. In this paper, we describe a heuristic prefetching technique to improve the interactive response time of KMeans clustering in dynamic query visualizations of multidimensional data. We address the tradeoff between high interaction and intense query computation by observing how related interactions on overlapping data subsets produce similar clustering results, and characterizing these similarities within a parameter space of interaction. We focus on the two-dimensional parameter space defined by the minimum and maximum values of a time range manipulated by dragging and stretching a one-dimensional filtering lens over a plot of time series data. Using calculation of nearest neighbors of interaction points in parameter space, we reuse partial query results from prior interaction sequences to calculate both an immediate best-effort clustering result and to schedule calculation of an exact result. The method adapts to user interaction patterns in the parameter space by reprioritizing the interaction neighbors of visited points in the parameter space. A performance study on Mesonet meteorological data demonstrates that the method is a significant improvement over the baseline scheme in which interaction triggers on-demand, exact-range clustering with LRU caching. We also present initial evidence that approximate, temporary clustering results are sufficiently accurate (compared to exact results) to convey useful cluster structure during rapid and protracted interaction.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400493,no,no,1485873347.233783
P-Tracer: Path-Based Performance Profiling in Cloud Computing Systems,"In large-scale cloud computing systems, the growing scale and complexity of component interactions pose great challenges for operators to understand the characteristics of system performance. Performance profiling has long been proved to be an effective approach to performance analysis; however, existing approaches do not consider two new requirements that emerge in cloud computing systems. First, the efficiency of the profiling becomes of critical concern; second, visual analytics should be utilized to make profiling results more readable. To address the above two issues, in this paper, we present P-Tracer, an online performance profiling approach specifically tailored for large-scale cloud computing systems. P-Tracer constructs a specific search engine that adopts a proactive way to process performance logs and generates particular indices for fast queries; furthermore, PTracer provides users with a suite of web-based interfaces to query statistical information of all kinds of services, which helps them quickly and intuitively understand system behavior. The approach has been successfully applied in Alibaba Cloud Computing Inc. to conduct online performance profiling both in production clusters and test clusters. Experience with one real-world case demonstrates that P-Tracer can effectively and efficiently help users conduct performance profiling and localize the primary causes of performance anomalies.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340205,no,no,1485873347.233782
Visual Analytics on the Financial Market: Pixel-based Analysis and Comparison of Long-Term Investments,"In this paper, we describe solutions how pixel-based visualization techniques can support the decision making process for investors on the financial market. We especially focus on explorative interactive techniques where analysts try to analyze large amounts of financial data for long-term investments, and show how visualization can effectively support an investor to gain insight into large amounts of financial time series data. After presenting methods for improving the traditional performance/risk computation in order to take user-specific regions of interest into account, we present a novel visualization approach that demonstrates how changes in these regions of interest affect the ranking of assets in a long-term investment strategy.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4577961,no,no,1485873347.23378
Pattern Matching in Constrained Sequences,"Constrained sequences find applications in communication, magnetic recording, and biology. In this paper, we restrict our attention to the so-called (d, k) constrained binary sequences in which any run of zeros must be of length at least d and at most k, where 0lesd<k. In some applications one needs to know the number of occurrences of a given pattern w in such sequences, for which we coin the term constrained pattern matching. For a given word w or a set of words W, we estimate the (conditional) probability of the number of occurrences of w in a (d, k) sequence generated by a memoryless source. As a by-product, we enumerate asymptotically the number of (d, k) sequences with exactly r occurrences of a given word w, and compute Shannon entropy of (d, k) sequences with a given number of occurrences of w. Throughout this paper we use techniques of analytic information theory such as combinatorial calculus, generating functions, and complex asymptotics.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4557611,no,no,1485873347.233779
Resource-efficient regular expression matching architecture for text analytics,"Text analytics systems, such as IBM's SystemT software, rely on regular expressions (regexs) and dictionaries for transforming unstructured data into a structured format. Unlike network intrusion detection systems, text analytics systems compute and report precisely where the specific and sensitive information starts and ends in a text document. Therefore, advanced regex matching functions, such as start-offset reporting, capturing groups, and leftmost match computation are heavily used in text analytics systems. We present a novel regex matching architecture that supports such functions in a resource-efficient way. The resource efficiency is achieved by 1) eliminating state replication, 2) avoiding expensive offset comparison operations in leftmost match computation, and 3) minimizing the number of offset registers. Experiments on regex sets from text analytics and network intrusion detection domains, using an Altera Stratix IV FPGA, show that the proposed architecture achieves a more than threefold reduction of the logic resources used and a more than 1.25-fold increase of the clock frequency with respect to a recently proposed architecture that supports identical features.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868623,no,no,1485873347.233778
AngeLA: Putting the teacher in control of student privacy in the online classroom,"Learning analytics (LA) is often considered as a means to improve learning and learning environments by measuring student behaviour, analysing the tracked data and acting upon the results. The use of LA tools implies recording and processing of student activities conducted on software platforms. This paper proposes a flexible, contextual and intuitive way to provide the teacher with full control over student activity tracking in online learning environments. We call this approach AngeLA, inspired by an angel guarding over LA privacy. AngeLA mimics in a virtual space the privacy control mechanism that works well in a physical room: if a person is present in a room, she is able to observe all activities happening in the room. AngeLA serves two main purposes: (1) it increases the awareness of teachers about the activity tracking and (2) provides an intuitive way to manage the activity tracking permissions. This approach can be applied to various learning environments and social media platforms. We have implemented AngeLA in Graasp, a social platform that fosters collaborative activities.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155683,no,no,1485873347.233776
IEEE Visualization and Graphics Technical Committee (VGTC),Provides a listing of current committee members.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677342,no,no,1485873347.233773
Analysis of Bayesian classification-based approaches for Android malware detection,"Mobile malware has been growing in scale and complexity spurred by the unabated uptake of smartphones worldwide. Android is fast becoming the most popular mobile platform resulting in sharp increase in malware targeting the platform. Additionally, Android malware is evolving rapidly to evade detection by traditional signature-based scanning. Despite current detection measures in place, timely discovery of new malware is still a critical issue. This calls for novel approaches to mitigate the growing threat of zero-day Android malware. Hence, the authors develop and analyse proactive machine-learning approaches based on Bayesian classification aimed at uncovering unknown Android malware via static analysis. The study, which is based on a large malware sample set of majority of the existing families, demonstrates detection capabilities with high accuracy. Empirical results and comparative analysis are presented offering useful insight towards development of effective static-analytic Bayesian classification-based solutions for detecting unknown Android malware.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6687155,no,no,1485873346.653107
Challenges for Creating Highly Dependable Service Based Systems,"The paradigm shift from traditional on-premise software to a service based model has gained significant momentum in the past decade. One such concept, Software as a Service (SaaS), delivers the functionality of traditional on-premise software as a service over the web. While a defect or a malfunction in a traditional on-premise application may affect a single user, the affected user base in a SaaS application may span the entire group of customers serviced by the provider. The physical disconnect between end users and the SaaS applications puts onus on service providers to deliver highly dependable systems that are available and reliable at all times. In this paper, we explore the general challenges faced in delivering and analyzing highly dependable service based systems. We quantify the challenges of dependability assessment utilizing a commercial case study. Furthermore, we explore one facet of dependability assessment related to log entries not necessarily related to dependability. We provide a novel approach to log filtering and show that the removal of benign log entries leads to more realistic system dependability analysis. We also show the need to merge multiple types of SaaS logs to support effective analysis.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753536,no,no,1485873346.653106
Broadening Our Collaboration with Design,"Computer graphics researchers have been collaborating successfully with engineers, architects, and artists for decades, focusing on better tools for model and image creation. Graphics researchers have already developed a wide range of procedural (automatic) modeling techniques, but with few exceptions, these focus on modeling natural objects, such as plants, terrains, and water. The next generation of tools must automate modeling of the most common and complex elements of digital content: manmade artifacts such as cities, buildings, vehicles, and furniture. Creating these tools require a new and close collaboration with architects as well as urban and industrial designers",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1683688,no,no,1485873346.653104
A framework f or calculating fundamental DVR performance limits,"This describes an analytic framework that can be used to estimate the performance of any DVR system in terms of both megabits per second and number of simultaneous video streams. The framework also highlights the extent to which the maximum performance is constrained by the disk, by the host hardware, or by the host software.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5012365,no,no,1485873346.653103
Ontology Refactoring,"This paper presents a rule based approach to ontology refactoring. Our method supports large scale instance relationships for support of translation to an improved, functionally equivalent design. By generating new ontology versions on-the-fly we can verify potential updates to our requirements. An example of this technique is presented utilizing a subset of the OWL-DL specification through the implementation of the Jena API. Advantages of this approach include rapid prototyping, versioning support, querying and tool development to support the automated engineering of instance data.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4597228,no,no,1485873346.653101
A Community Information System for Ubiquitous Informal Learning Support,"There are many mobile apps supporting informal learning tasks like sense-making out of multimedia materials. However, regardless how well these have been implemented, their scope and scalability is limited either by over-specialization and consequently limited transferability or by over-generalization - with a lack of informal learning support for most Web 2.0 apps. Our cloud-based mobile Web information system approach is theoretically founded and designed for scalability in informal learning. As a proof of concept we present a semantic video annotation scenario for supporting informal learning at workplace. As the approach has been designed for community learning analytics, we can present results from a preliminary evaluation. We conducted our research with a strong commitment towards open source software development, so that our solutions are already available and can have impact beyond the usual scope of a funded project.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901419,no,no,1485873346.6531
"TDA2X, a SoC optimized for advanced driver assistance systems","TDA2X is an optimized scalable system on chip (SoC) solution from Texas Instruments that spans various application areas of ADAS such as front-camera, surround-view and the emerging area of sensor fusion. It accomplishes this through a focused set of heterogeneous processors, brought together in a scalable architecture with a rich set of integrated peripherals, providing an optimal mix of performance in a low power footprint for Advanced Driver Assistance Systems (ADAS) vision analytics. Computer vision algorithms across the various ADAS application systems have a rich variation and diversity in processing requirements along with the need to run them concurrently within challenging thermal budgets. A heterogeneous architecture with various programmable elements allows system developers to map various portions of the algorithms to the architectures that are best suited for the underlying task allowing maximizing system performance and reducing development time and effort in developing these complex systems. Scalability of the architecture by varying the number of cores and clock speeds of these heterogeneous architectures, allows for scalability in performance and power across low, mid and high end products with one software investment. A critical focus on functional safety across the cores and various memories is particularly essential given the mission critical nature of ADAS applications.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853990,no,no,1485873346.653098
Front Cover,September/October 2013 IEEE Software: The Many Faces of Software Analytics,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6588532,no,no,1485873346.653097
Recovery from Failures Due to Mandelbugs in IT Systems,"Several studies have been carried out on software bugs analysis and classification for life and mission critical systems, which include reproducible bugs called Bohrbugs, and hard to reproduce bugs called Mandelbugs. Although software reliability in IT systems has been studied for years, there are only a few formal analytic models for recovery from Mandelbugs. This paper discusses in detail several real cases of Mandelbugs and presents a simple flowchart which describes the recovery processes implemented in IT systems for a large variety of Mandelbugs. The flowchart is based on more than 10 IT systems that are running in production. The paper then presents a closed-form expression of the mean time to recovery from these bugs. Measures of interest including mean time to recovery and system unavailability are computed. A numerical and parametric sensitivity analysis of the model parameters are carried out. This analysis allows the designer to find out important parameter(s) for the recovery from failures due to Mandelbugs.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133084,no,no,1485873346.653095
Periscopic visualizes symptomatology of pandemic: VAST 2010 Mini Challenge 2 award: Effective visualization of symptoms,"By using affordable, off-the-shelf software and novel visualization methods, characterizing the spread of a pandemic was achieved for the VAST Challenge. Dashboards of the presenting symptomatology, anomalies, and death records all contributed to creating ways for health officials better detect and analyze the spread of the disease.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652678,no,no,1485873346.653092
Protection against remote code execution exploits of popular applications in Windows,"The objective of Malicious Remote Code Execution Exploits is to remotely execute code transparently to the user, and without relying on user interaction, in order to infect targeted machines. This comparative study examines the effectiveness of different proactive exploit mitigation technologies included in popular endpoint security products and specialized anti-exploit tools. The study focuses on exploits of popular applications running on Windows XP SP3 with Internet Explorer (IE8). As such, the Microsoft Enhanced Mitigation Experience Toolkit (MS-EMET) is used as a reference standard for all exploit mitigation solutions. The study compares the effectiveness of endpoint security products and anti-exploit tools by separating measurements of protections in common with MS-EMET from measures of protections supplemental to MS-EMET. This is done in order to understand not just the relative competitive effectiveness of the individual products and tools but also to understand the overall capabilities of the Windows endpoint security solutions to combat the remote code execution exploit capabilities of the overall Windows malware ecosystem.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999416,no,no,1485873346.082817
TIALA äóî Time series alignment analysis,"The analysis of time series expression data is widely employed for investigating biological mechanisms. Microarrays are often used to generate time series for several different experimental conditions. These time series then need to be compared to each other. For a successful comparison it is necessary to perform a time series alignment because the experiments can differ in the number of time points, as well as in the time points themselves. In this work we propose a novel visual analytics approach for the analysis of multiple time series experiments in parallel. Our time series alignment analysis tool Tiala allows one to align multiple time series experiments and to visually explore the aligned expression profiles. A two- and three-dimensional visualization strategy was implemented that is especially designed to enhance the display of multiple aligned time series expression profiles. Tiala is available as a part of the microarray data analysis software Mayday. Mayday itself is open source software distributed under the terms of the GNU General Public License. It is available from http://www.microarray-analysis.org. We apply our approach to time series showing abiotic stress responses of Arabidopsis thaliana and to data sets from two replicates of the antibiotics producing bacterium Streptomyces coelicolor.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6094048,no,no,1485873346.082816
Investigating Graph Algorithms in the BSP Model on the Cray XMT,"Implementing parallel graph algorithms in large, shared memory machines, such as the Cray XMT, can be challenging for programmers. Synchronization, deadlock, hot spotting, and others can be barriers to obtaining linear scalability. Alternative programming models, such as the bulk synchronous parallel programming model used in Google's Pregel, have been proposed for large graph analytics. This model eases programmer complexity by eliminating deadlock and simplifying data sharing. We investigate the algorithmic effects of the BSP model for graph algorithms and compare performance and scalability with hand-tuned, open source software using GraphCT. We analyze the innermost iterations of these algorithms to understand the differences in scalability between BSP and shared memory algorithms. We show that scalable performance can be obtained with graph algorithms in the BSP model on the Cray XMT. These algorithms perform within a factor of 10 of hand-tuned C code.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651060,no,no,1485873346.082814
The Construction of Autonomous-resource Systems in ŒñZero-approachŒî Teaching Mode Based on Web,"With the teaching reform, ""zero-approach""teaching mode has been much more popular than before. The core spirit of the reform is to develop autonomy of students in universities. During the course of development, autonomous-material system is necessary to be built based on Web. The author has investigated the mode and put forward some useful measures to comply with present teaching reform in universities. In the investigation,the author paid much attention to the usages of methods of natural field observation and analytic statistical ones to get some effective conclusions.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5232499,no,no,1485873346.082813
Robust graph traversal: Resiliency techniques for data intensive supercomputing,"Emerging large-scale, data intensive applications that use the graph abstraction to represent problems in a broad spectrum of scientific and analytics applications have radically different features from floating point intensive scientific applications. These complex graph applications, besides having large working datasets, exhibit very low spatial and temporal locality which makes designing algorithmic fault tolerance for these quite challenging. They will run on future exascale-class High Performance Computing (HPC) systems that will contain massive number of components, and will be built from devices far less reliable than those used today. In this paper we propose software based approaches that increase robustness for these data intensive, graph-based applications by managing the resiliency in terms of the data flow progress and validation of pointer computations. Our experimental results show that such a simple approach incurs fairly low execution time overheads while allowing these computations to survive a large number of faults that would otherwise always result in the termination of the computation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6670340,no,no,1485873346.082812
Extracting weblog of Siam University for learning user behavior on MapReduce,"MapReduce is a framework that allows developers to write applications that rapidly process and analyze large volumes of data in a massively parallel scale. Moreover, a clickstream is a record of a user's activity on the Internet. Using a clickstream analysis we can collect, analyze, and report aggregate data about which pages visitors visit in what order - and which are the result of the succession of mouse clicks each visitor makes. Clickstream analysis can reveal usage patterns leading to a heightened understanding of users' behavior. In this paper, we introduced a novel and efficient web log mining model for web users clustering. In general, our model consists of three main steps; 1) Computing the similarity measure of any path in a web page, 2) Defining the k-mean clustering for group customerID 3) Generating the report based on the Hadoop MapReduce Framework. Consequently, our experiments were run on real world data derived from weblogs of Siam University at Bangkok, Thailand (www.siam.edu).",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6306177,no,no,1485873346.08281
Simple and Accurate Approximations for the Two Dimensional Gaussian Q-Function,"The aim of this work is the derivation of two approximated expressions for the two dimensional Gaussian Q-function, Q(x, y, “). These expressions are highly accurate and are expressed in closed-form. Furthermore, their algebraic representation is relatively simple and therefore, convenient to handle both analytically and numerically. This feature is particularly useful for two reasons: firstly because it renders the derived expressions useful mathematical tools that can be utilized in numerous analytic performance evaluation studies in digital communications under fading; secondly because the two dimensional Gaussian Q-function is neither tabulated nor a built-in function in popular mathematical software packages such as Maple, Mathematica and Matlab.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5956733,no,no,1485873346.082808
Evacuation traces mini challenge: User testing to obtain consensus discovering the terrorist,"The adoption of visual analytics methodologies in security applications is an approach that could lead to interesting results. Usually, the data that has to be analyzed finds in a graphical representation its preferred nature, such as spatial or temporal relationships. Due to the nature of these applications, it is very important that key-details are made easy to identify. In the context of the VAST 2008 Challenge, we developed a visualization tool that graphically displays the movement of 82 employees of the Miami Department of Health (USA). We also asked 13 users to identify potential suspects and observe what happened during an evacuation of the building caused by an explosion. In this paper we explain the results of the user testing we conducted and how the users interpreted the event taken into account.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677390,no,no,1485873346.082807
Stress assessment in power systems and its visualization using synchrophasor based metrics,"This paper proposes two metrics for assessing static and dynamic stresses present in a large interconnected power grid. The base loading of the system constitutes the static stress. It refers to the normal/pre-contingency state of the system. The dynamic stress refers to the event/contingency that the system is subjected to and is primarily caused by loss of transmission system or drop in generation. The angle difference between buses located across the network, and the voltage sensitivity of buses lying in the middle are two synchrophasor-based metrics that are found to accurately reflect the system's static loading and its ability to withstand the dynamic stress. The simulations performed using the full WECC system show that by monitoring these metrics in real-time, the ability of the system to withstand a variety of contingencies can be predicted with great accuracy. These metrics can be monitored through analytic and visualization platforms such as RTDMS<sup>Œ¬1</sup>, which is a synchrophasor based software application. The methodology to be followed for integrating with such a platform is also provided. The analysis shows that the proposed metrics can be very effective in aiding system operators for real-time static and dynamic stress monitoring.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6965460,no,no,1485873346.082805
Analytic results for efficient computation of the Nuttall-Q and incomplete Toronto functions,"This work is devoted to the derivation of novel analytic results for special functions which are particularly useful in wireless communication theory. Capitalizing on recently reported series representations for the Nuttall Q-function and the incomplete Toronto function, we derive closed-form upper bounds for the corresponding truncation error of these series as well as closed-form upper bounds that under certain cases become accurate approximations. The derived expressions are tight and their algebraic representation is rather convenient to handle analytically and numerically. Given that the Nuttall-Q and incomplete Toronto functions are not built-in in popular mathematical software packages, the proposed results are particularly useful in computing these functions when employed in applications relating to natural sciences and engineering, such as wireless communication over fading channels.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698148,no,no,1485873346.082802
LiterMiner: A Literature Visual Analytic System,"The data set of scientific literature contains abundant knowledge. Currently, most bibliography search engines provide only keyword search or similarity search functions for retrieving information directly stored in the database but neglect mining hidden information. In this article, we design a visual analytic tool called LiterMiner to extract entities such as article, author, affiliation, and keyword from these literature records, establish the relationships among them and display them to help analyst understand them effectively. Especially, LiterMiner uses community detection algorithm to explore academic teams. Our tool has three principle features. First, a sophisticated data clean process is designed for integration of many different formatted literature records. Second, LiterMiner can find academic teams and analyze the resume of an academic team. Third, an interactive and cooperate visualization solution is used to display the analysis result, which help user understand the result better.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5454639,no,no,1485873345.516959
Multivariate Network Exploration with JauntyNets,"The amount of data produced in the world every day implies a huge challenge in understanding and extracting knowledge from it. Much of this data is of relational nature, such as social networks, metabolic pathways, or links between software components. Traditionally, those networks are represented as node-link diagrams or matrix representations. They help us to understand the structure (topology) of the relational data. However in many real world data sets, additional (often multidimensional) attributes are attached to the network elements. One challenge is to show these attributes in context of the underlying network topology in order to support the user in further analyses. In this paper, we present a novel approach that extends traditional force-based graph layouts to create an attribute-driven layout. In addition, our prototype implementation supports interactive exploration by introducing clustering and multidimensional scaling into the analysis process.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676538,no,no,1485873345.516958
Enabling comprehensive data-driven system management for large computational facilities,"This paper presents a tool chain, based on the open source tool TACC_Stats, for systematic and comprehensive job level resource use measurement for large cluster computers, and its incorporation into XDMoD, a reporting and analytics framework for resource management that targets meeting the information needs of users, application developers, systems administrators, systems management and funding managers. Accounting, scheduler and event logs are integrated with system performance data from TACC_Stats. TACC_Stats periodically records resource use including many hardware counters for each job running on each node. Furthermore, system level metrics are obtained through aggregation of the node (job) level data. Analysis of this data generates many types of standard and custom reports and even a limited predictive capability that has not previously been available for open-source, Linux-based software systems. This paper presents case studies of information that can be applied for effective resource management. We believe this system to be the first fully comprehensive system for supporting the information needs of all stakeholders in open-source software based HPC systems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877519,no,no,1485873345.516956
Analysis of Smartphone User Behavior,"Holistic, objective and precise data on mobile user behavior and experience are needed in today's product development and marketing activities. This article presents a framework for mobile audience measurements, for collecting data at the point of convergence - devices. The paper compares the presented framework to alternative methods of mobile user research, and identifies the unique advantages of on-device measurements along with the key weaknesses. In addition to elaborating on data collection, the paper addresses the related analytics, presenting adoption modeling and stickiness analysis that complement the data collection processes and deliver practical insights. The insights can be provided to device vendors, application developers and carriers, who can use the insights in product portfolio management, product development, and marketing.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494859,no,no,1485873345.516955
Monitoring students performances in French Institutes of Technology using the ScoDoc software,"French University-based Institutes of Technology (IUT1) are a major player in France's superior educational system. IUTs provide technical university education, preparing students to careers in the industry and services. The main diploma is called DUT, Diplome Universitaire de Technologie [2]. Designed to train mid-level technical staff in 2 years, IUT programmes also allow graduated students to pursue their studies with a more advanced degree, such as a licence professionnelle [4]. IUT students have very different profiles: most of them are coming from high school (with scientific or technical majors), but a significant proportion come from foreign countries (mainly Africa), or from other university tracks. Assessment of IUT students relies on continuous evaluation: there are in principle no terminal exams, but series of tests, monitoring the acquisition of competences. At the IUT de Villetaneuse (Universite Paris 13 - Sorbonne-Paris-Cite’), we started more than ten years ago to design a dedicated information system to gather all available information concerning our students: personal data, results at all tests and exams, assiduity, orientation decision. This system, implemented in the open-source free software ScoDoc (https://trac.lipn.univ-paris13.fr/projects/ scodoc) monitors the progression of all students and raises alarms when some special pedagogical action should be taken. It is widely distributed and used in other French universities, and is wellsuited to handle the complex national regulations for semesters validation in IUTs. Prevention of student difficulties requires action from the pedagogical team: it could be as simple as talking to an individual student to discuss his personal situation, or as complex as detecting an homogeneous group of students sharing similar problems and proposing them an ad-hoc remediation plan (such as tutoring, personalised learning plan, or ad-hoc learning module). ScoDoc software system is intended for being used- by the pedagogical team as well as by the students. Indeed, students can access their marks summary at all times, and be informed of new marks as soon as they are entered in the system. They thus can monitor their performances and ask advice from the teachers for a better progress. Teachers can be assigned different roles according to their responsibilities in the organisation of studies. All of them can enter notes on a student in order for the pedagogical team to easily track past difficulties. Those involved in a specific course can enter their marks concerning this course, while the head of studies for a particular year or semester can also prepare summaries for the class of students they are in charge of. This is not only necessary for the validation of semesters, but also at any time to have a synthetic view of the class performance, pointing out difficulties in some subjects or of a group within the class. This paper is organised in three parts: first, we present the French IUT system and describe its peculiarities, insisting on the evaluation of the students. In section 2, the main features of the ScoDoc software are briefly presented. In section 3, we discuss how ScoDoc can be used to prevent student difficulties. Finally, in section 4, we discuss some future perspectives, such as the integration of more sophisticated predictive analytics technologies to enhance the relevance and accuracy of student real-time characterisation.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155707,no,no,1485873345.516954
Prescriptive Analytics System for Improving Research Power,"We introduce a prescriptive analytics system, InSciTe advisory, to provide researchers with advice of their future research direction and strategy. The system analyzes several thousands of heterogeneous types of data sources such as papers, patents, reports, Web news, Web magazines, and collective intelligence data. It consists of two main parts of descriptive analytics and prescriptive analytics. Once given a researcher, the descriptive analytics part provides results from activity history and research power w.r.t the designated researcher. Then, prescriptive analytics part suggests a group of role model researchers to the researcher, as well as how to be like the role model researchers. The prescription for the researcher is provided according to 5W1H questions and their corresponding answers. All of the analytical results and their explanations about the given researcher are automatically generated and saved to a report. This researcher-centric prescriptive analytics has not been introduced before and it is useful tool to understand the designated researcher in the perspective of prescriptive as well as descriptive analytics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755350,no,no,1485873345.516952
Mobile Commerce: A Broader Perspective,"Many organizations are trying to understand how best to leverage the unique combination of content, data, and functionality provided by mobile devices. This is especially true in the retail sector, where significant mobile commerce growth is predicted. According to Internet Retailer, of the 49.6 billion visits to the top 500 e-retailers in 2014, 26.4 billion (53.2 percent) will stem from smartphones. Similarly, the percentage of online retail sales placed via mobile devices will grow from 11 percent in 2012 to 25 percent by 2017. However, ecommerce and transaction statistics are only one part of the picture when it comes to leveraging mobile commerce.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824518,no,no,1485873345.516951
A model-based framework: an approach for profit-driven optimization,"Although optimizations have been applied for a number of years to improve the performance of software, problems that have been long-standing remain, which include knowing what optimizations to apply and how to apply them. To systematically tackle these problems, we need to understand the properties of optimizations. In our current research, we are investigating the profitability property, which is useful for determining the benefit of applying an optimization. Due to the high cost of applying optimizations and then experimentally evaluating their profitability, we use an analytic model framework for predicting the profitability of optimizations. In this paper, we target scalar optimizations, and in particular, describe framework instances for partial redundancy elimination (PRE) and loop invariant code motion (LICM). We implemented the framework for both optimizations and compare profit-driven PRE and LICM with a heuristic-driven approach. Our experiments demonstrate that a model-based approach is effective and efficient in that it can accurately predict the profitability of optimizations with low overhead. By predicting the profitability using models, we can selectively apply optimizations. The model-based approach does not require tuning of parameters used in heuristic approaches and works well across different code contexts and optimizations.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402098,no,no,1485873345.516947
Dynamic process support based on users' behavior,"Nowadays there is a gap between the possibilities and the massively existing data on the one side and the user as main worker on the other side. In different scenarios e.g. search, exploration, analysis and policy-modeling a user has to deal with massive information, but for this work he usually gets a static designed system. So meanwhile data-driven work-processes are increasing in its complexity the support of the users who are working with these data is limited on basic features. Hence this paper describes a concept for a process-supporting approach, which includes relevant aspects of users' behaviors in support him to successfully finish also complex tasks. This will be achieved by a process-based guidance with an automatic tools selection for every process and activity on the one hand. And on the other hand the consideration of expert-level of a user to a single task and process. This expert-level will be classified during each task and process interaction and allow the automatically selection of optimal tools for a concrete task. In final the user gets for every task an automatically initialized user-interface with useful and required tools.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402079,no,no,1485873345.516945
"Large Scale Video Analytics: On-demand, iterative inquiry for moving image research","Video is exploding as a means of communication and expression, and the resultant archives are massive, disconnected datasets. Thus, scholars' ability to research this crucial aspect of contemporary culture is severely hamstrung by limitations in semantic image retrieval, incomplete metadata, and the lack of a precise understanding of the actual content of any given archive. Our aim in the Large Scale Video Analytics (LSVA) project is to address obstacles in both image-retrieval and research that uses extreme-scale archives of video data that employs a human-machine hybrid process for analyzing moving images. We propose an approach that 1) places more interpretive power in the hands of the human user through novel visualizations of video data, and 2) uses a customized on-demand configuration that enables iterative queries.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6404446,no,no,1485873345.003553
Decision metrix: A new approach to voltage quality monitoring on the Irish distribution grid,"Distribution Service Operators (DSO's) have historically ensured voltage quality with a range of strategies. However, DSO's now face tough challenges addressing voltage quality due to a number of emerging drivers. Integration of renewable energy into the distribution grid, increased sensitivity of customers to voltage quality and ongoing regulatory developments are increasing the workload of DSO's. This paper introduces Decision Metrix, a vendor neutral software solution developed to address the needs of ESB Networks and other network operators in regards to grid wide performance analytics. Specifically Decision Metrix continuously evaluates the voltage quality performance of bulk supply points and at the point of connection with generators and demand customers. The system is capable of embedding EN 50160 [2] and internal standards in a flexible rules engine for continuous evaluation. Decision Metrix was deployed in ESB Networks (The Irish distribution system operator) in summer 2012. Significant benefits have been seen in relation to automated compliance monitoring, proactive network management, increased awareness and preventative maintenance through risk identification. Our experience shows that an advanced compliance monitoring tool is an essential component for delivering a secure, sustainable and low carbon electrical network of the future.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714869,no,no,1485873345.003551
Supporting sustainability with software äóî An industrial perspective (Keynote),"Summary form only given. TechnoAware research and develops technologies and solutions for ambient intelligence. Established in 2003 TechnoAware was born from the experiences and competencies of the ISIP40 research group of the University of Genova. This research group is studying and implementing video analytics algorithms since 1985 and is considered nowadays one of the major actors in this filed worldwide. Entirely made up by researchers and experts in the video analytics field, TechnoAware main principles are: proprietary technologies (highly customizable and modular solutions), scientific competencies (high quality level and performances), continuous research and technological innovation (cutting edge products).",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227254,no,no,1485873345.00355
SoftLearn: A Process Mining Platform for the Discovery of Learning Paths,"One of the most challenging issues in learning analytics is the development of techniques and tools that facilitate the evaluation of the learning activities carried out by learners. In this paper, we faced this issue through a process mining-based platform, called Soft Learn, that is able to discover complete, precise and simple learning paths from event logs. This platform has a graphical interface that allows teachers to better understand the real learning paths undertaken by learners.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901485,no,no,1485873345.003548
IP geolocation suspicious email messages,"As the Internet and electronic mail continue to be utilized by an ever increasing number of users, so does fraudulent and criminal activity via the Internet and email increase. The negative effects of cybercrime activities on the use of the Internet for e-business and secure communications increased interest in studying the factors that motivate these criminals, their tactics and what can be done to mitigate their activities. The research in the area of email analysis usually focuses on two areas, email traffic analysis and email content analysis, but very poor in the area of visual analytics of emails. The paper presents the software for visualizing suspicious email messages based on the information provided in the email header (rather than the content of the email). This IP mapping tool, called MIPA, uses a Google Map to display the geographic position and integrates InfoDB, WhoIS databases, and the Google Maps API. Thus, the proposed work can be helpful for identifying and investigating suspicious email messages and also assist the investigators to get the information in time to take effective actions to reduce the criminal activities.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6716371,no,no,1485873345.003547
FDT 2.0: Improving scalability of the fuzzy decision tree induction tool - integrating database storage,"Effective machine-learning handles large datasets efficiently. One key feature of handling large data is the use of databases such as MySQL. The freeware fuzzy decision tree induction tool, FDT, is a scalable supervised-classification software tool implementing fuzzy decision trees. It is based on an optimized fuzzy ID3 (FID3) algorithm. FDT 2.0 improves upon FDT 1.0 by bridging the gap between data science and data engineering: it combines a robust decisioning tool with data retention for future decisions, so that the tool does not need to be recalibrated from scratch every time a new decision is required. In this paper we briefly review the analytical capabilities of the freeware FDT tool and its major features and functionalities; examples of large biological datasets from HIV, microRNAs and sRNAs are included. This work shows how to integrate fuzzy decision algorithms with modern database technology. In addition, we show that integrating the fuzzy decision tree induction tool with database storage allows for optimal user satisfaction in today's Data Analytics world.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7007853,no,no,1485873345.003546
"Advancing process modeling, simulation, and analytics in practice","If you take a broad view, there are many ways ahead for software process modeling and simulation. One way to broaden the view is to include not just software processes but also systems and service processes. Another is to address not just technical processes but also business processes. In any scope, impact can be strengthened by combining modeling and simulation with analytics. A way to assure longevity of the field is to solve problems in practice. These afford a variety of ways to make contributions, including both applied and fundamental. Challenges that must be overcome in achieving practical results are getting access to practitioners, acquiring useful data, and getting your solution adopted (among others). More empirical studies and experience reports are needed. But there are many routes by which contributions may flow to and from the field.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225971,no,no,1485873345.003544
Designing visual analytics systems for disease spread and evolution: VAST 2010 mini challenge 2 and 3 award: Good overall design and analysis,"Using two of the VAST 2010 mini challenges as a case study, we report on the design decisions and software development process used to create visual analytics software for understanding disease spread and mutation. The software we developed and the analysis conducted attempted to help us understand (a) how a fictitious disease may have spread between selected cities around the globe; and (b) how genetic sequences taken from infected patients may be used to chart the evolution of the disease and changes in its severity, drug resistance and other characteristics.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652689,no,no,1485873345.003543
Application of analytic hierarchy process in the selection of transit route for large-scale cargo transportation,"The selection of the optimized transit route is the prime task in the early stage of large-scale cargo transportation. The article first discusses in detail the principles and process of the selection of transit route for large-scale cargo transportation, and then, the analytic hierarchy process is applied, in combination of Matlab mathematical software, to establish a mathematical model for the selection of the optimized transit route of large-scale cargo transportation which can be used to solve the problem of routes optimization of an actual project. This evaluation model is proved to be stable and effective by the project evaluation, and provides a simple practical method for engineering and technical personnel in route optimization.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019526,no,no,1485873345.003541
Optimizing an SPT-tree for visual analytics,"Despite the extensive work done in the scientific visualization community on the creation and optimization of spatial data structures, there has been little adaptation of these structures in visual analytics and information visualization. In this work we present how we modify a space-partioning time (SPT) tree - a structure normally used in direct-volume rendering - for geospatial-temporal visualizations. We also present optimization techniques to improve the traversal speed of our structure through locational codes and bitwise comparisons. Finally, we present the results of an experiment that quantitatively evaluates our modified SPT tree with and without our optimizations. Our results indicate that retrieval was nearly three times faster when using our optimizations, and are consistent across multiple trials. Our finding could have implications for performance in using our modified SPT tree in large-scale geospatial temporal visual analytics software.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400544,no,no,1485873345.003539
A Cost-Effective Approach to Delivering Analytics as a Service,"Analytical solutions are considered as increasingly important for modern enterprises. Currently, systematical adoption of analytical solutions is limited to only a small set of large enterprises, as the deployment cost is high due to high performance hardware requirement and expensive analytics software. Moreover, such on-premises solutions are not suitable for the occasional analytics consumers. In order to accelerate the prevalence of analytical solutions, this paper explores the feasibility of leveraging SaaS (Software-as-a-Service) delivery model to provide analytics capabilities as services in a cost-effective way. The main contributions of our work include: (1) proposing a framework to enable enterprise tenants to consume analytics capabilities as services; (2) developing a method to enhance existing analytics platform to support multi-tenancy so that a single software instance can effectively support multiple concurrent tenants; (3) designing an SLA (Service Level Agreement) customization mechanism to satisfy the diverse analytics capability demands of tenants. A prototype system has been developed to evaluate the feasibility of our approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6257847,no,no,1485873344.500106
Person attribute search for large-area video surveillance,"This paper describes novel video analytics technology which allows an operator to search through large volumes of surveillance video data to find persons that match a particular attribute profile. Since the proposed technique is geared for surveillance of large areas, this profile consists of attributes that are observable at a distance (including clothing information, hair color, gender, etc) rather than identifying information at the face level. The purpose of this tool is to allow security staff or investigators to quickly locate a person-of-interest in real time (e.g., based on witness descriptions) or to speed up the process of video-based forensic investigations. The proposed algorithm consists of two main components: a technique for detecting individual moving persons in large and potentially crowded scenes, and an algorithm for scoring how well each detection matches a given attribute profile based on a generative probabilistic model. The system described in this paper has been implemented as a proof-of-concept interactive software tool and has been applied to different test video datasets, including collections in an airport terminal and collections in an outdoor environment for law enforcement monitoring. This paper discusses performance statistics measured on these datasets, as well as key algorithmic challenges and useful extensions of this work based on end-user feedback.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107847,no,no,1485873344.500105
Analytics for Product Planning: In-Depth Interview Study with SaaS Product Managers,"SaaS cloud computing, in contrast to packaged products, enables permanent contact between users of a software product and the product-owning company. When planning the development and evolution of a software product, a product manager depends on reliable information about feature attractiveness. So far, planning decisions were based on stakeholder opinion and the customer's willingness to buy. Whether or not a feature actually is used was out of consideration. Analytics that measure the interaction between users and the SaaS gives product managers unprecedented access to information about product usage. To understand whether and how SaaS analytics can be used for product planning decision, we performed 17 in-depth interviews with experienced managers of SaaS products and analyzed the results analyzed with a mixed-method strategy. The empirical results characterize the relevance of a broad range of analytics for product planning decisions, and the strengths and limitations of an analytics-based product planning approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6740237,no,no,1485873344.500103
DIVE: A Graph-Based Visual-Analytics Framework for Big Data,"The need for data-centric scientific tools is growing; domains such as biology, chemistry, and physics are increasingly adopting computational approaches. So, scientists must deal with the challenges of big data. To address these challenges, researchers built a visual-analytics platform named DIVE (Data Intensive Visualization Engine). DIVE is a data-agnostic, ontologically expressive software framework that can stream large datasets at interactive speeds. In particular, DIVE makes novel contributions to structured-data-model manipulation and high-throughput streaming of large, structured datasets.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777449,no,no,1485873344.500102
An Approach of Real-Time Team Behavior Control in Games,"The design of NPC (non-player character) is an analytic process. It is relying on assumptions of human game players' behavior. In practice, however, different PCs (player characters) often exhibit variable behavior, making them difficult to predicate and complicating the design process. In this paper, we describe an approach for team AI planning and learning. This approach is based on procedural knowledge and a layered multi-agent architecture. We implement real-time transfer learning and adaptive mechanism for the team of NPCs. The team can react to the human player with the tactical awareness of seasoned team behavior. Results indicate that the approach of using the hybrid of transfer learning and adaptive mechanism can improve NPCs' overall performance in real-time.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366965,no,no,1485873344.500101
"Visual analysis of large graphs using (X,Y)-clustering and hybrid visualizations","Many different approaches have been proposed for the challenging problem of visually analyzing large networks. Clustering is one of the most promising. In this paper we propose a new goal for clustering that is especially tailored to hybrid-visualization tools. Namely, that of producing both intra-cluster graphs and inter-cluster graph that are suitable for highly-readable visualizations within different representation conventions. We formalize this concept in the (X,Y)-clustering framework, where Y is the class that defines the desired topological properties of intra-cluster graphs and X is the class that defines the desired topological properties of the inter-cluster graph. By exploiting this approach hybrid-visualization tools can effectively combine different node-link and matrix-based representations, allowing the users to interactively explore the graph by expansion/contraction of clusters without loosing their mental map. As a proof of concept, we describe the system VHYXY (Visual Hybrid (X,Y)-clustering) that integrates our techniques and we present the results of case studies to the visual analysis of co-authorship networks.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429591,no,no,1485873344.500099
Getting more for less in optimized MapReduce workflows,"Many companies are piloting the use of Hadoop for advanced data analytics over large datasets. Typically, such MapReduce programs represent workflows of MapReduce jobs. Currently, a user must specify the number of reduce tasks for each MapReduce job. The choice of the right number of reduce tasks is non-trivial and depends on the cluster size, input dataset of the job, and the amount of resources available for processing this job. In the workflow of MapReduce jobs, the output of one job becomes the input of the next job, and therefore the number of reduce tasks in the previous job may impact the performance and processing efficiency of the next job. In this work,1 we offer a novel performance evaluation framework for easing the user efforts of tuning the reduce task settings while achieving performance objectives. The proposed framework is based on two performance models: a platform performance model and a workflow performance model. A platform performance model characterizes the execution time of each generic phase in the MapReduce processing pipeline as a function of processed data. The complementary workflow performance model evaluates the completion time of a given workflow as a function of i) input dataset size(s) and ii) the reduce tasks' settings in the jobs that comprise a given workflow. We validate the accuracy, effectiveness, and performance benefits of the proposed framework using a set of realistic MapReduce applications and queries from the TPC-H benchmark.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572974,no,no,1485873344.500097
Roundtable: What's Next in Software Analytics,"For this special issue, the guest editors asked a panel of six established experts in software analytics to highlight what they thought were the most important, or overlooked, aspect of this field. They all pleaded for a much broader view of analytics than seen in current practice: software analytics should go beyond developers (Ahmed Hassan) and numbers (Per Runeson). Analytics should also prove its relevance to practitioners (Abram Hindle, Martin Shepperd). There are now opportunities for ""natural"" software analytics based on statistical natural language processing (Prem Devanbu). Lastly, software analytics needs information analysts and field agents like Chloe O'Brian and Jack Bauer in the TV show 24 (Sung Kim).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547637,no,no,1485873344.500096
Food for Thought: Improving the Market for Assurance,"Better information can improve a marketplace. An evaluation/certification process that leveraged modern programming languages and analytic tools could accelerate both the development and the adoption of less vulnerable and more effective programming practices, products, and systems. *This article also includes a letter to the editor, regarding ""Alien vs. Quine"" by Vanessa Gratzer and David Naccache from the March/April 2007 issue.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218542,no,no,1485873344.500094
HierarchicalTopics: Visually Exploring Large Text Collections Using Topic Hierarchies,"Analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated. Topic-based text summarization methods coupled with interactive visualizations have presented promising approaches to address the challenge of analyzing large text corpora. As the text corpora and vocabulary grow larger, more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora. However, it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible. To facilitate the representation and navigation of a large number of topics, we propose a visual analytics system - HierarchicalTopic (HT). HT integrates a computational algorithm, Topic Rose Tree, with an interactive visual interface. The Topic Rose Tree constructs a topic hierarchy based on a list of topics. The interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion. User interactions are provided for users to make changes to the topic hierarchy based on their mental model of the topic space. To qualitatively evaluate HT, we present a case study that showcases how HierarchicalTopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups. We have also conducted a user study to quantitatively evaluate the effect of hierarchical topic structure. The study results reveal that the HT leads to faster identification of large number of relevant topics. We have also solicited user feedback during the experiments and incorporated some suggestions into the current version of HierarchicalTopics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634160,no,no,1485873344.500092
Clarifying the digital content output formats for mobile learning in higher education,"Mobile learning (m-learning) is an emerging area of distance education and there is a great interest in incorporating it in higher education, due to the mobile devices capabilities which are rapidly increasing and renders mobile devices more efficient and more attractive to students. On the other hand, the educational content plays a significant role in the process of delivering knowledge, especially in the field of distance education. The delivery of the web content for mobile devices depends on heterogeneous software and hardware environments; therefore, web content adaptation to various mobile environments is a challenge for the educational institutions. The Hellenic Open University (HOU), an educational institution which offers distance learning courses, has started to emphasize on the arising importance of m-learning with the aim to involve it in the educational process. Towards this direction, in this paper we aim to define different file formats of the digital educational content that can be reproduced by mobile devices regardless of the various operating systems. The clarification of the digital content output formats for mobile learning will contribute to the creation of a number of analytic guides with detailed technical specifications for the development of educational content for mobile devices.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011121,no,no,1485873344.02111
Towards eliminating configuration errors in cyber infrastructure,"It is well-documented that configuration errors account for 50% to 80% of downtime and vulnerabilities in cyber infrastructure. The ConfigAssure suite of tools has been developed to help eliminate these errors. These tools are for requirement specification, configuration synthesis, diagnosis and repair, verification, reconfiguration planning and visualization. These tools are being made available as a web service that is demonstrated.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111678,no,no,1485873344.021109
A comparison of Fuzzy DNP and SEM in analyzing novel mobile learning technology acceptances by learners,"Mobile learning, the learning method which learners can leverage mobile devices to learn everywhere, has become the one of the most potential learning approach as novel mobile devices emerges. However, how the novel mobile learning technology can be accepted by learners was seldom addressed. Meanwhile, due to the unavailability of large number (more than 100) experts for evaluating novel mobile learning devices, the traditional statistical methods, e.g. the structural equation model (SEM), are not appropriate for evaluating the factors influencing the acceptance of novel techniques, a feasible research framework will be very helpful for achieving the evaluation purposes. In this work, the author proposes a novel technology acceptance modeling (TAM) evaluation framework with the fuzzy Decision Making Trial and Evaluation Laboratory (DEMATEL) based Network Process (DNP) to resolve the above mentioned real world problem. The SEM based analytic results based on learners will be introduced for demonstrating the effectiveness of the Fuzzy DNP. An empirical study based on the analysis of factors influencing Taiwanese mobile learners' acceptances of some specific mobile learning software being installed in iPhone will be introduced for demonstrating the feasibility and effectiveness of the proposed method. Based on the analytic results, the Fuzzy DNP based framework can be used for real world technology acceptance analysis of novel mobile learning techniques. Meanwhile, the derived factors which can influence the mobile learning technology acceptances can serve as the basis for educators and marketers for designing and improving the next generation learning devices.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6409686,no,no,1485873344.021108
Software as an engineering material: How the affordances of programming have changed and what to do about it (Invited industrial talk),"Summary form only given. A huge wealth of various data exist in the practice of software development. Further rich data are produced by modern software and services in operation, many of which tend to be data-driven and/or data-producing in nature. Hidden in the data is information about the quality of software and services and the dynamics of software development. Software analytics is to develop and apply data exploration and analysis technologies, such as pattern recognition, machine learning, and information visualization, on software data to obtain insightful and actionable information for modern software and services. This tutorial presents latest research and practice on principles, techniques, and applications of software analytics in practice, highlighting success stories in industry, research achievements that are transferred to industrial practice, and future research and practice directions in software analytics. The attendees can acquire the skills and knowledge needed to perform industrial research or conduct industrial practice in the field of software analytics and to integrate analytics in their own industrial research, practice, and training.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227251,no,no,1485873344.021093
Chinese Mobile Banking Service Evaluation Based on AHP Method,"In the mobile era with a growing number of mobile phone users, the financial needs of the users should not be underestimated. Mobile financial services market's enormous potential has driven the major banks to expand mobile financial market with Intensive efforts. The 3G era also propelled this trend. Mobile phone banks are beginning to be accepted by more and more customers. This paper establishes an evaluation model for the mobile banking service .The model involves five main factors: system quality, information quality, interface design quality, brand quality and fees and benefits. We applied the analytic hierarchy process (AHP) to determine the weights of different factors. And then, a ranking of eight major banks' mobile phone banking services is given by using the evaluation model. Finally, we provide some suggestions for banks' further optimization with insights into the ranking results.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5661025,no,no,1485873344.021091
An analytics approach to traffic analysis in network virtualization,"Network virtualization has been propounded as a diversifying attribute of the future inter-networking paradigm. However, monitoring and troubleshooting operational virtual networks can be a daunting task, due to their size, distributed state, and additional complexity introduced by network virtualization. We propose an analytics approach for the analysis of network traces collected across hypervisors and switches. To re-organize individual trace events into path-wise slices that represent the life-cycle of individual packets, we first present a trace slicing scheme. Then, we develop a path characterization scheme to extract feature matrices from those trace slices. Using those feature metrics, we develop a set of trace analysis algorithms to cluster, rank, query, and verify packet traces. We have developed the analytics approach in a SDN network management tool, and presented evaluation results to show how it can enable visibility and effective problem diagnosis in a SDN network.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014183,no,no,1485873344.02109
Twelve Years of Visualization Research at Microsoft,"Summary form only given. Microsoft Research has been involved in a variety of visualization research efforts over the last twelve years. In this talk, the author summarizes the various threads of research, which include task management, personal information management, software visualization, business visualization, community visualization, graph and tree visualization, and visual analytics for homeland security. The author also gives demonstrations of key prototypes that have been built. One of the key challenges throughout this work has been developing effective means of evaluation of visualization techniques. The author summarizes what we have learned about evaluation methods. Finally, the author summarizes some basic lessons learned about what visualization techniques are most effective across all of these research efforts.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4351320,no,no,1485873344.021088
App Analytic: A Study on Correlation Analysis of App Ranking Data,"In recent years, app store and android market have experienced a significant growth in terms of app numbers. Since we discover 85% of apps through the ranks, it is important to develop effective app ranking analyzing tools. In this paper, we present a method called App Analytic. In our method, we explore correlations of app ranking data about popular social networking sites. Specifically, we analyze correlations between various characteristics of social networking sites on Internet and android market. The results of correlation analysis reveal that there is a strong positive correlation of the number of app downloads with the number of registered users and page rank. We also provide an in-depth analysis on the major factors that impact the correlations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686087,no,no,1485873344.021086
Applying Systematic Reviews to Diverse Study Types: An Experience Report,"Systematic reviews are one of the key building blocks of evidence-based software engineering. Current guidelines for such reviews are, for a large part, based on standard meta-analytic techniques. However, such quantitative techniques have only limited applicability to software engineering research. In this paper, therefore, we describe our experience with an approach to combine diverse study types in a systematic review of empirical research of agile software development.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343750,no,no,1485873344.021085
Visual Tracing for the Eclipse Java Debugger,"In contrast to stepping, tracing is a debugging technique that does not suspend the execution. This technique is more suitable for debugging programs whose correctness is compromised by the suspension of execution. In this work we present a tool for visually tracing Java programs in Eclipse. Trace point hits are collected on a per-instance basis. This enables finding out which trace points were hit for which objects at which time. The interactive visualization provides detailed information about the hits such as thread, stack trace, and assigned values. We implemented the tool as an Eclipse plug in that integrates with other features of Eclipse Java debugger. In an informal evaluation, developers appreciated the utility of our method as a solution in the middle between full tracing and stop-and-go debugging. They suggested scenarios in which our tool can help them in debugging and understanding their programs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178940,no,no,1485873344.02107
Visual analysis of large-scale network anomalies,"The amount of information flowing across communication networks has rapidly increased. The highly dynamic and complex networks, represented as large graphs, make the analysis of such networks increasingly challenging. In this paper, we provide a brief overview of several useful visualization techniques for the analysis of spatiotemporal anomalies in large-scale networks. We make use of community-based similarity graphs (CSGs), temporal expansion model graphs (TEMGs), correlation graphs (CGs), high-dimension projection graphs (HDPGs), and topology-preserving compressed graphs (TPCGs). CSG is used to detect anomalies based on community membership changes rather than individual nodes and edges and therefore may be more tolerant to the highly dynamic nature of large networks. TEMG transforms network topologies into directed trees so that efficient search is more likely to be performed for anomalous changes in network behavior and routing topology in large dynamic networks. CG and HDPG are used to examine the complex relationship of data dimensions among graph nodes through transformation in a high-dimensional space. TPCG groups nodes with similar neighbor sets into mega-nodes, thus making graph visualization and analysis more scalable to large networks. All the methods target efficient large-graph anomaly visualization from different perspectives and together provide valuable insights.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6517342,no,no,1485873343.540326
r-AnalytiCA: Requirements Analytics for Certification & Accreditation,Numerous interdependent quality requirements imposed by regulatory Certification and Accreditation (C&A) processes enable a rich context to gather compliance evidences for promoting software assurance. The goal of the r-AnalytiCA workbench is to make sense out of the large collection of available evidences for a complex software system though multidimensional requirements-driven problem domain analysis. The requirements analytics employed in the workbench support C&A activities by leveraging the expressiveness of ontologies used to model C&A requirements and their interdependencies.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384213,no,no,1485873343.540325
An improved analytic hierarchy process model on Software Quality Evaluation,"With new global demands for better quality of software products, effective and efficient SQE (Software Quality Evaluation) becomes necessary and indispensible. Existing methods of SQE are as it is, however, subjective, non-quantitative and uncompleted to some extent. In this paper, we introduce a novel method, an improved AHPM (Analytic Hierarchy Process Model) incorporated traditional AHPM and some quality characteristics from ISO/IEC 9126, which attempts to propose a better method of software quality evaluation that is relatively objective, quantitative and completed. On this basis, current software can be well assessed as well as future products can be well predicted.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5690372,no,no,1485873343.540323
A Fuzzy Model for Selecting Software,"A fuzzy AHP model is proposed in this paper for the selection of software. Since the priorities of selection criteria obtained by traditional fuzzy AHP are dependent, in stead of normalizing priorities and aggregating the performance of software by simple weighted average, in this paper we propose a nonadditive aggregation method based on fuzzy integral to aggregate the performance of software.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4406271,no,no,1485873343.540321
The Many Faces of Software Analytics,"Articles regarding the many faces of software analytics highlight the power of analytics for different types of organizations: large organizations and open source projects, as well as small- to medium-sized projects.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6588525,no,no,1485873343.54032
Using a learning analytics tool for evaluation in self-regulated learning,"In self-regulated learning, evaluation is a complex task of the teaching process, but even more if students have social media that allow them to build their personal learning environment in different ways. In these kind of virtual environments a large amount of data that needs to be assessed by teachers is generated, and therefore they require tools that facilitate the assessment task. In this paper, we present an experiment with a process mining-based learning analytics tool, called SoftLearn, that helps teachers to assess the student's activity in self-regulated learning. The subject of this experiment is taught in blended learning mode with weekly classroom sessions, and the students use a social network software, called ELGG, as an e-portfolio in which they reflect their individual knowledge process construction. The results show that the use of this tool reduces significantly the assessment time and helps teachers to understand the learning process of the students.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044400,no,no,1485873343.540319
Performance Testing: Far from Steady State,"The dot com era ushered in a number of industry standard load testing tools. While there is no doubt that these tools have helped improve the quality of IT systems, performance testing in the IT industry is far from steady state. There are still severe gaps between performance test results and production systems performance in IT projects. This paper proposes a number of areas where performance testing needs to improve radically, several of which can be incorporated in to load testing tools. Examples are also provided of simple analytics during single user performance testing to demonstrate the effectiveness of this extra but necessary step in the testing process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5614034,no,no,1485873343.540317
Assisting developers of Big Data Analytics Applications when deploying on Hadoop clouds,"Big data analytics is the process of examining large amounts of data (big data) in an effort to uncover hidden patterns or unknown correlations. Big Data Analytics Applications (BDA Apps) are a new type of software applications, which analyze big data using massive parallel processing frameworks (e.g., Hadoop). Developers of such applications typically develop them using a small sample of data in a pseudo-cloud environment. Afterwards, they deploy the applications in a large-scale cloud environment with considerably more processing power and larger input data (reminiscent of the mainframe days). Working with BDA App developers in industry over the past three years, we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches. In this paper, as a first step in assisting developers of BDA Apps for cloud deployments, we propose a lightweight approach for uncovering differences between pseudo and large-scale cloud deployments. Our approach makes use of the readily-available yet rarely used execution logs from these platforms. Our approach abstracts the execution logs, recovers the execution sequences, and compares the sequences between the pseudo and cloud deployments. Through a case study on three representative Hadoop-based BDA Apps, we show that our approach can rapidly direct the attention of BDA App developers to the major differences between the two deployments. Knowledge of such differences is essential in verifying BDA Apps when analyzing big data in the cloud. Using injected deployment faults, we show that our approach not only significantly reduces the deployment verification effort, but also provides very few false positives when identifying deployment failures.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606586,no,no,1485873343.540315
A method for design and performance modeling of client/server systems,"Designing complex distributed client/server applications that meet performance requirements may prove extremely difficult in practice if software developers are not willing or do not have the time to help software performance analysts. The paper advocates the need to integrate both design and performance modeling activities so that one can help the other. We present a method developed and used by the authors in the design of a fairly large and complex client/server application. The method is based on a software performance engineering language developed by one of the authors. Use cases were developed and mapped to a performance modeling specification using the language. A compiler for the language generates an analytic performance model for the system. Service demand parameters at servers, storage boxes, and networks are derived by the compiler from the system specification. A detailed model of DBMS query optimizers allows the compiler to estimate the number of I/Os and CPU time for SQL statements. The paper concludes with some results of the application that prompted the development of the method and language.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=881718,no,no,1485873343.540313
A novel visual analytics approach for clustering large-scale social data,"Social data refers to data individuals create that is knowingly and voluntarily shared by them and is an exciting avenue into gaining insight into interpersonal behaviors and interaction. However, such data is large, heterogeneous and often incomplete, properties that make the analysis of such data extremely challenging. One common method of exploring such data is through cluster analysis, which can enable analysts to find groups of related users, behaviors and interactions. This paper presents a novel visual analysis approach for detecting clusters within large-scale social networks by utilizing a divide-analyze-recombine scheme that sequentially performs data partitioning, subset clustering and result recombination within an integrated visual interface. A case study on a microblog messaging data (with 4.8 millions users) is used to demonstrate the feasibility of this approach and comparisons are also provided to illustrate the performance benefits of this approach with respect to existing solutions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691718,no,no,1485873343.102517
Improving metadata management for small files in HDFS,"Scientific applications are adapting HDFS/MapReduce to perform large scale data analytics. One of the major challenges is that an overabundance of small files is common in these applications, and HDFS manages all its files through a single server, the Namenode. It is anticipated that small files can significantly impact the performance of Namenode. In this work we propose a mechanism to store small files in HDFS efficiently and improve the space utilization for metadata. Our scheme is based on the assumption that each client is assigned a quota in the file system, for both the space and number of files. In our approach, we utilize the compression method `harballing', provided by Hadoop, to better utilize the HDFS. We provide for new job functionality to allow for in-job archival of directories and files so that running MapReduce programs may complete without being killed by the jobtracker due to quota policies. This approach leads to better functionality of metadata operations and more efficient usage of the HDFS. Our analysis results show that we can reduce the metadata footprint in main memory by a factor of 42.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289133,no,no,1485873343.102516
An Empirical Study of Bugs in Machine Learning Systems,"Many machine learning systems that include various data mining, information retrieval, and natural language processing code and libraries are used in real world applications. Search engines, internet advertising systems, product recommendation systems are sample users of these algorithm-intensive code and libraries. Machine learning code and toolkits have also been used in many recent studies on software mining and analytics that aim to automate various software engineering tasks. With the increasing number of important applications of machine learning systems, the reliability of such systems is also becoming increasingly important. A necessary step for ensuring reliability of such systems is to understand the features and characteristics of bugs occurred in the systems. A number of studies have investigated bugs and fixes in various software systems, but none focuses on machine learning systems. Machine learning systems are unique due to their algorithm-intensive nature and applications to potentially large-scale data, and thus deserve a special consideration. In this study, we fill the research gap by performing an empirical study on the bugs in machine learning systems. We analyze three systems, Apache Mahout, Lucene, and OpenNLP, which are data mining, information retrieval, and natural language processing tools respectively. We look into their bug databases and code repositories, analyze a sample set of bugs and corresponding fixes, and label the bugs into various categories. Our study finds that 22.6% of the bugs belong to the algorithm/method category, 15.6% of the bugs belong to the non-functional category, and 13% of the bugs belong to the assignment/initialization category. We also report the relationship between bug categories and bug severities, the time and effort needed to fix the bugs, and bug impacts. We highlight several bug categories that deserve attention in future research.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405375,no,no,1485873343.102511
Formal Verification of Industrial Software with Dynamic Memory Management,"Tool-based analytic techniques such as formal verification may be used to justify the quality, correctness and dependability of software involved in digital control systems. This paper reports on the development and application of a tool-based methodology, the purpose of which is the formal verification of freedom from intrinsic software faults related to dynamic memory management. The paper introduces the operational and research context in the power generation industry, in which this work takes place. The theoretical framework and the tool at the cornerstone of the methodology are then presented. The paper also presents the practical aspects of the research: software under analysis, experimental results and lessons learned. The results are seen promising, as the methodology scales accurately in identified conditions of analysis, and has a number of perspectives which are currently under study in ongoing work.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703230,no,no,1485873343.10251
VMCAnalytic: Developing a Collaborative Video Analysis Tool for Education Faculty and Practicing Educators,"This paper describes the genesis, design and prototype development of the VMCAnalytic, a repository-based video annotation and analysis tool for education. The VMCAnalytic is a flexible, extensible analytic tool that is unique in its integration into an open source repository architecture to transform a resource discovery environment into an interactive collaborative where practicing teachers and faculty researchers can analyze and annotate videos to support a range of needs from longitudinal research to improving individual teaching performance. This paper presents an overview of the design and functionality of the VMCAnalytic, which is a key component of the NSF-funded Video Mosaic Collaborative (VMC), together with a description of the underlying repository service architecture. The paper also describes the synergistic collaboration between digital library technologists and education researchers to build a research environment that can integrate with the VMCAnalytic tool to create a digital collaboration space. The prototype tool is available in as of January 2010 at the VMC website: www.video-mosaic.org.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5428756,no,no,1485873343.102508
MC2: Map Concurrency Characterization for MapReduce on the Cloud,"MapReduce is now a pervasive analytics engine on the cloud. Hadoop is an open source implementation of MapReduce and is currently enjoying wide popularity. Hadoop offers a high-dimensional space of configuration parameters, which makes it difficult for practitioners to set for efficient and cost-effective execution. In this work we observe that MapReduce application performance is highly influenced by map concurrency. Map concurrency is defined in terms of two configurable parameters, the number of available map slots and the number of map tasks running over the slots. We show that some inherent MapReduce characteristics enable well-informed prediction of map concurrency. We propose Map Concurrency Characterization (MC<sup>2</sup>), a standalone utility program that can predict the best map concurrency for any given MapReduce application. By leveraging the generated predicted information, MC<sup>2</sup> can judiciously guide Map phase configuration and, consequently, improve Hadoop performance. Unlike many of relevant schemes, MC<sup>2</sup> does not employ simulation, dynamic instrumentation, and/or static analysis of unmodified job code to predict map concurrency. In contrast, MC<sup>2</sup> utilizes a simple, yet effective mathematical model, which exploits the MapReduce characteristics that impact map concurrency. We implemented MC<sup>2</sup> and conducted comprehensive experiments on a private cloud and on Amazon MC<sup>2</sup> using Hadoop 0.20.2. Our results show that MC<sup>2</sup> can correctly predict the best map concurrencies for the tested benchmarks and provide up to 2.2X speedup in runtime.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676673,no,no,1485873343.102507
A closer look at bugs,"The evolution of non-trivial software systems is accompanied by unexpected behaviour and side-effects, referred as bugs or defects. These defects are reported to and stored in bug tracking systems, which contain descriptions of the problems that have been encountered. However, bug tracking systems store and present bug reports in textual form, which makes their understanding dispersive and unintuitive. We present an approach to display bug reports through a web-based visual analytics platform, named in*Bug. in*Bug allows users to navigate and inspect the vast information space created by bug tracking systems, with the goal of easing the comprehension of bug reports in detail and also obtain an understanding äóìin the largeäó of how bugs are reported with respect to one system or to an entire software ecosystem.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650542,no,no,1485873343.102505
Coastal online analysis and synthesis tool 2.0 (COAST),"The Coastal Online Assessment and Synthesis Tool (COAST) geobrowser has been developed at NASA Stennis Space Center (SSC) for integration of previously disparate coastal datasets from NASA and other sources into a common desktop client tool. COAST will provide insightful new data visualization and analysis capabilities for the coastal researcher. COAST is built upon the NASA open source 3D geobrowser, World Wind, developed at the NASA Ames Research Center. COAST also integrated some of the value-added modifications and enhancements from the NASA Marshall Space Flight Center version of World Wind, SERVIR-Viz. COAST is being developed to maximize use of open source data access, viewing, and data manipulation software tools, creating a low-cost, widely installable base for potential users. Feedback from preliminary reviewers has led to more robust understanding of the data integration and visual analytic challenges and of the potential solutions that COAST can offer to the broader user community. Improved mode of functionality for these users will lead to a more refined methodology for implementation of COAST as an effective tool for a range of potential users varying from researcher to investigator to potential decision maker. Development of the Temporal Visualization Tool (TVT) plugin for COAST was begun in the 2007 Integrated Approach to Monitoring Hypoxia in the Northern Gulf of Mexico project. The origin of this time-based animated data overlay tool is the Naval Research Laboratory Monterey Weather plugin, which is still distributed with the present World Wind 1.4 package. Modifications to the TVT tool have been targeted to provide users the capability to connect to and map/integrate disparate datasets, located locally and online, into project sessions. The TVT allows direct data listing of accessible raster datasets, subsequent multi-select, temporally animated image overlays in the COAST browser, and transparency control over the animated layer within COAST via - a slider mechanism. The development of the Recursive Online Remote Data - Data Mapper (RECORD-DM) utility was driven by the need for an ability to map and add online remote image-product datasets to the TVT plugin's list of available images as needed. The RECORD-DM tool allows a user to map the current state, structure, and location of online raster data available for viewing in TVT. It also allows geographic position information to be attached and creates an XML file map of the data for immediate use in the TVT as either static or temporally animated overlays in the current COAST session. The Import Data Tool provides the ability to quickly add image and vector datasets in a COAST session without having to be a geospatial or image processing expert. The envisioned COAST end user community can vary from seasoned research scientists wanting to integrate decision model output into their sessions all the way to coastal community managers wanting to review local, state, and federal data products in their areas of interest.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422062,no,no,1485873343.102503
Software-as-a-Service evaluation in cloud paradigm: Primitive cognitive network process approach,"With rapid growth of Software-as-a-Services (SaaS) products in cloud paradigm, evaluation of SaaS product is essential for an enterprise to purchase a software service whilst there are a number of alternatives. This paper proposes the primitive cognitive network process (P-CNP) approach to measure the SaaS products in multi-criteria decision making aspect. The P-CNP is the rectified approach of the Analytic Hierarchy Process (AHP) in the aspects of paired interval scale and the corresponding mathematical development. The proposed approach can support the business decision maker to select the best cloud service product through user experiences and preferences.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6335719,no,no,1485873342.712812
Teaching digital test with BIST analyzer,"This paper describes a new software tool for high quality training/learning in the field of digital microelectronics. Its main purpose is to give insight into reliability and quality assurance technologies based on linear feedback shift registers (LFSR) and other pseudo-random pattern generators (PRPG). Various PRPG types are becoming the mainstream test generation solution used in built-in self-test (BIST) structures. Taking into account complex theoretical concepts behind the microelectronics self-testing (including data coding and compression, cryptography, field theory, linear programming) it is important to effectively educate engineers in this field. The software tool we present in this paper is aimed at facilitating this goal. Unlike other similar systems, this tool facilitates study of various test optimization problems, allows fault coverage analysis for different circuits and with different LFSR parameters. The main didactic aim of the tool is presenting complicated concepts in a comprehensive graphical and analytical way. The multi-platform JAVA runtime environment allows for easy usage of the tool both in the classroom and at home. The BIST Analyzer represents an integrated simulation, training, and research environment that supports both analytic and synthetic way of learning. Due to the above mentioned facts the tool provides a unique training platform to use in courses on electronic testing and design for testability.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4610171,no,no,1485873342.712811
A new software tool to model measured RF-data with optimum circuit topology,"In this paper a new S/W tool is presented to model measured RF data employing immitance interpolation techniques. This S/W tool also employs a recently developed sub-routine, which generates circuit models with least number of circuit elements by means of a numerical approach. Furthermore, an analytic procedure is introduced and implemented within the new S/W package to select proper sample-points subject to interpolation error in the ""near min-max sense"" or the so-called ""Chebysev sense"". Hence optimum circuit topology for the given data is constructed. An example is presented to exhibit the utilization of the new tool. This new S/W package may be utilized as the front end to the commercially available design and analysis package such as ANSOFT, EAGLEWARE etc.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1328361,no,no,1485873342.71281
Vismate: Interactive visual analysis of station-based observation data on climate changes,"We present a new approach to visualizing the climate data of multi-dimensional, time-series, and geo-related characteristics. Our approach integrates three new highly interrelated visualization techniques, and uses the same input data types as in the traditional model-based analysis methods. As the main visualization view, Global Radial Map is used to identify the overall state of climate changes and provide users with a compact and intuitive view for analyzing spatial and temporal patterns at the same time. Other two visualization techniques, providing complementary views, are specialized in analysing time trend and detecting abnormal cases, which are two important analysis tasks in any climate change study. Case studies and expert reviews have been conducted, through which the effectiveness and scalability of the proposed approach has been confirmed.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042489,no,no,1485873342.712808
A function point analysis model based-AHP,"Function point analysis is a method of software size measurement. It has become the mainstream software measurement methods in software engineering. Traditional function point analysis method makes adjustment to the results by generic system characteristics. But it exist limitations. In order to improve it, this article presents an approach based on AHP software function point analysis model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5691239,no,no,1485873342.712807
Keynote talk 2: How to build an industrial R&D center in Vietnam: A case study,"Summary form only given. To be competitive in today's market, the IT industry faces many challenges in the development and maintenance of enterprise information systems. Engineering these largescaled systems efficiently requires making decisions about a number of issues. In addition, customers expectations imply continuous software delivery in predictable quality. The operation such systems demands for transparency of the software in regard to lifecycle, change and incident management as well as cost efficiency. Addressing these challenges, we learned how to benefit from traditional industries. Contrary to the fact that the IT business calls itself gladly an industry, the industrialization of software engineering in most cases moves on a rather modest level. Industrialization means not only to build a solution or product on top of managed and well-defined processes, but also to have access to structured information about the current conditions of manufacturing at any time. Comparably with test series and assembly lines of the automobile industry, each individual component and each step from the beginning of manufacturing up to the final product should be equipped with measuring points for quality and progress. Even one step further the product itself, after it has left the factory, should be able to continuously provide analytic data for diagnostic reasons. Information is automatically collected and builds the basic essentials for process control, optimization and continuous improvement of the software engineering process. This presentation shows by means of a practical experience report how AdNovum managed to build its software engineering based on a well-balanced system of processes, continuous measurement and control äóî as well as a healthy portion of pragmatism. We implemented an efficient and predictable software delivery pipeline",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6027420,no,no,1485873342.712805
Overcoming Limited Collaboration Channels in Distributed Intelligence Analysis: Visualization Tools and Design Seeds,"Military intelligence analysis (IA) support tools are often developed using generalized models of IA that fail to take into consideration the real-world constraints put on analysts by factors such as organizational structures and cultures. IA in domains where distributed collaboration is required because direct communication and coordination is infeasible represents a challenge for generalized models of IA. This paper provides our analysis of distributed IA, which we conducted to support the design of software. We present a resulting set of capabilities that have been developed and deployed in an operational community. Our analysis approach and design focuses on extracting requirements and translating them into ""design seeds"" or guidelines for implementation, which are later used to verify that the resulting system meets the expressed requirements.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975549,no,no,1485873342.712804
Principles of Software-Defined Elastic Systems for Big Data Analytics,"Techniques for big data analytics should support principles of elasticity that are inherent in types of data and data resources being analyzed, computational models and computing units used for analyzing data, and the quality of results expected from the consumer. In this paper, we analyze and present these principles and their consequences for software-defined environments to support data analytics. We will conceptualize software-defined elastic systems for data analytics and present a case study in smart city management, urban mobility and energy systems with our elasticity supports.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903529,no,no,1485873342.712802
Sensation of learning analytics top prevail the software engineering education,"Software pockets an indispensable role in the modern living style to make their work easy. Software engineers are the people who develop the software to comply with the user needs and make them joyful. Software engineering education is the place where the software engineers mold up academically for the society's requirement. As a result, software engineering education grabs the essence in the computer education, albeit it fall shorts to cook up the genius to meet with the industries necessity. To overcome these issues, researchers suggested number of software engineering learning/teaching methods to egg on students to acquire their profundity knowledge in software engineering. Even though the suggestions do not utterly surmount this crucial issue since the suggested approaches did not attract the maj ority of the students and touch their learning style. Learning analytics plays a vital role to improve the students learning activities. Learning style and learning engagement are the key factors for the learning analytics. This paper focuses on learning style and learning engagement of the software engineering students. This work classifies the students in line with their learning style and identifies the needs of the software engineering students using learning engagement finally proposed the model to inspire the students to overcome such issues and motivate them in gathering software engineering knowledge.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6938704,no,no,1485873342.712798
A decision-analytic stopping rule for validation of commercial software systems,"The decision of when to release a software product commercially is not a question of when the software has attained some objectively justifiable degree of correctness. It is, rather, a question of whether the software achieves a reasonable balance among engineering objectives, market demand, customer requirements, and marketing directives of the software organization. We present a rigorous framework for addressing this important decision. Conjugate distributions from statistical decision theory provide an attractive means of modeling the cost and rate of bugs given information acquired during software testing, as well as prior information provided by software engineers about the fidelity of the software before testing begins. In contrast to other methods, the stopping analysis yields a computationally simple rule for deciding when to release a commercial software product based on information revealed to engineers during software testing-complicated numerical procedures are not needed. Our method has the added benefits that it is sequential: it measures explicitly the costs of customer dissatisfaction associated with bugs as well as the costs of declining market position while the testing process continues; and it incorporates a practical framework for cost-criticality assessment that makes sense to professional software developers",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=877849,no,no,1485873342.333743
An analytic model of optimistic Software Transactional Memory,"An analytic model is proposed to assess the performance of optimistic software transactional memory (STM) systems with in-place memory updates for write operations. Based on an absorbing discrete-time Markov chain, closed-form analytic expressions are developed, which are quickly solved iteratively to determine key parameters of the STM system. The model covers complex implementation details such as read/write locking, data consistency checks and conflict management. It provides fundamental insight into the system behavior, when we vary input parameters like number and size of concurrent transactions or the number of the data objects. Numerical results are validated by comparison with a discrete-event simulation.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4919647,no,no,1485873342.333741
Award: Efficient toolkit integration solving the cell phone calls challenge with the Prajna Project,"The Prajna Project is a Java toolkit designed to provide various capabilities for visualization, knowledge representation, geographic displays, semantic reasoning, and data fusion. Rather than attempt to recreate the significant capabilities provided in other tools, Prajna instead provides software bridges to incorporate other toolkits where appropriate. This challenge required the development of a custom application for visual analysis. By applying the utilities within the Prajna project, I developed a robust and diverse set of capabilities to solve the analytical challenge.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677396,no,no,1485873342.333739
A Method to Test the Information Quality of Technical Documentation on Websites,"In software engineering, testing is one of the corner-stones of quality assurance. The idea of software testing can be applied to information quality as well. Technical documentation has a set of intended uses that correspond to use cases in a software system. Documentation is, in many cases, presented via software systems, e.g., web servers and browsers, and contains software, e.g., Javascript for user interaction, animation, and customization, etc. This makes it difficult to find a clear-cut distinction between a software system and technical documentation. However, we can assume that each use case of a technical documentation involves retrieval of some sort of information that helps a user answer a specific questions. To assess information testing as a method, we implemented QAnalytics, a tool to assess the information quality of documentation that is provided by a website. The tool is web-based and allows test managers and site owners to define test cases and success criteria, disseminate the test cases to testers, and to analyze the test results. This way, information testing is easily manageable even for non-technical stakeholders. We applied our testing method and tool in a case study. According to common perception, the website of Linnaeus University needs to be re-engineered. Our method and tool helped the stakeholders identify what information is presented well and which parts of the website that need to change. The test results allowed the design and development effort to prioritize actual quality issues and potentially save time and resources.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958417,no,no,1485873342.333738
Bifurcationanalysis of an LC-tank VCO including the variable capacitance,"An alternative approach for designing fully integrated LC oscillators based on the Andronov-Hopf bifurcation analysis including a nonlinear overall-model for MOS transistors (EKV model) is introduced. In the presented procedure the capacitance of the VCO is described in terms of geometric transistor dimensions. This leads to an extended bifurcation analysis which considers only the widths and lengths of the used transistors. For the description of the MOS capacitance a basic-charge-model, as it was introduced in the work by Enz and Vittoz, was used in combination of an explicit analytic approximation of the surface potential. Our procedure enables the consideration of the oscillator amplitude as an additional design parameter in the VCO design process. Furthermore it allows the designer to receive a more exact estimation of the varactor dimensions in advance. The procedure has been implemented in a software toolbox in Matlab which employs a graphical optimization process that facilitates the analytical determination of the required design parameters.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4600942,no,no,1485873342.333737
Agile way of BI implementation,"The rapidly changing IT economy has influenced the Business Intelligence (BI) systems to look at innovative ways to be equally fast and flexible. There is a need to be more intuitive and quick in implementation so as to adapt to the changing environment. One of the ways by which organizations can achieve these goals is by using Agile based BI development models. There are many components in a successful BI solution which include data integration, analytics, data quality, metadata management, enterprise data warehouse, dashboards and so on. Each of these components are critical for an organization, and stakeholders are ready to invest in these. The only issue is how quickly we can provide these solutions and how flexible these solutions are with the changing demands. Traditionally, we have been using the waterfall SDLC model for BI implementations which encourages getting requirements clarity in the initial phases of the projects and having distinct deliverables for each phase. With time the approach has been customized and enhanced to `iterative waterfall approach' where a chunk of requirements is implemented in one SDLC cycle. Though this approach has been successful in the past, the BI practitioners recognize that business requirements are not static and we must be able to effectively mould the deliverables based on changing requirements. Hence, we cannot continue with the Waterfall (or Iterative waterfall) project management approach that is neither fast nor flexible. Applying the concepts of agile development to BI is the intuitive way forward. The aim of this paper is to provide a background on agile project management & development techniques, and suggest some guidelines and best practices which can help in successful Agile BI implementations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139618,no,no,1485873342.333735
"The Innovation Network as a Complex Adaptive System: Flexible Multi-agent Based Modeling, Simulation, and Evolutionary Decision Making","The literature rarely considers an innovation network as a complex adaptive system. In this paper, theories of complex adaptive systems research are employed to model and analyze intra-organization networks, inter-organization networks as well as their interaction mechanisms in the whole innovation context, with a conceptual framework proposed and presented. Flexible multi-agent based modeling, smart simulation, self-survival and adaptive intelligent software agents, expert systems, analytic hierarchy process, hybrid decision support approach, and statistical methods are integrated to deal with the innovation network problem and support evolutionary decision making in the open and dynamic environments.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977779,no,no,1485873342.333733
3-D Quantification of the Aortic Arch Morphology in 3-D CTA Data for Endovascular Aortic Repair,"We introduce a new model-based approach for the segmentation and quantification of the aortic arch morphology in 3-D computed tomography angiography (CTA) data for thoracic endovascular aortic repair (TEVAR). The approach is based on a model-fitting scheme using a 3-D analytic intensity model for thick vessels in conjunction with a two-step refinement procedure, and allows us to accurately quantify the morphology of the aortic arch. Based on the fitting results, we additionally compute the (local) 3-D vessel curvature and torsion as well as the relevant lengths not only along the 3-D centerline, but particularly also along the inner and outer contour. These measurements are important for preoperative planning in TEVAR applications. We have validated our approach based on 3-D synthetic as well as 3-D MR phantom images. Moreover, we have successfully applied our approach using 3-D CTA datasets of the human thorax and have compared the results with ground truth obtained by a radiologist. We have also performed a quantitative comparison with a commercial vascular analysis software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5491100,no,no,1485873342.333731
A Reference Model for the New Product Development in Medium-Sized Technology-Based Electronics Enterprises,"New Product development (NPD) is getting even more significant to the entrepreneurship competitiveness due to the increasing market internationalization, to the product diversity and variety and the reduction of the product life cycle. This present work has as its main goal to propose a reference model adapted to NPD of technology-based companies (TBC) that produce electronics. This research follows a combined methodology approach, that is, a qualitative-quantitative approach. First, a qualitative approach is applied with the study of multiple cases in order to identify the NPD features based on electronics manufacturing. Then, a qualitative approach is employed with application on multi-criterion decision-making method as to select macro-phases, phases and activities from the proposed model for NPD. Finally, studies of multiple cases are carried out to verify the proposed model adequacy. The results analyses suggests that the proposed model may be considered convenient.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014499,no,no,1485873342.333729
Agile Software Architecture in Advanced Data Analytics,"Requirements evolve over the development lifecycle of a software project. Agile practices are designed specifically to address this challenge while showing early and continuous progress towards project goals. Applying an agile approach allows stakeholders to adapt the scope and capabilities of a development release to changing market needs. More recently, an agile approach has been recommended for developing the architecture of software systems, enabling the design to support current requirements and early releases while evolving to meet future expectations. Our experience defining emergent software systems to build a product line architecture for advanced data analytics demonstrates the benefits that can be gained from prioritizing work activities and delaying architecture decisions. This paper proposes a process and ontology for agile architecture development. Only the necessary aspects for each evolutionary release are designed and prototyped, as determined by expectations of the identified application domain scenarios. Feedback from implementing the scenarios using the architecture extends our understanding of the requirements and provides the backlog for successive design iterations.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6827125,no,no,1485873342.333727
"Extraction, Identification, and Ranking of Network Structures from Data Sets","Networks are widely used to model a variety of complex, often multi-disciplinary, systems in which the relationships between their sub-parts play a significant role. In particular, there is extensive research on the topological properties associated with their structure as this allows the analysis of the overall behaviour of such networks. However, extracting networks from structured and unstructured data sets raises several challenges, including addressing any inconsistency present in the data, as well as the difficulty in investigating their properties especially when the topological structure is not fully determined or not explicitly defined. In this paper, we propose a novel method to address the automated identification, assessment and ranking of the most likely structure associated with networks extracted from a variety of data sets. More specifically, our approach allows to mine data to assess whether their associated networks exhibit properties comparable to well-known structures, namely scale-free, small world and random networks. The main motivation is to provide a toolbox to classify and analyse real-world networks otherwise difficult to fully assess due to their potential lack of structure. This can be used to investigate their dynamical and statistical behaviour which would potentially lead to a better understanding and prediction of the properties of the system (s) they model. Our initial validation shows the potential of our method providing relevant and accurate results.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915536,no,no,1485873341.963465
Visualizing the effects of scale and geography in multivariate comparison,"Our research investigates the sensitivities and complexities of visualizing multivariate data over multiple scales with the consideration of local geography. We investigate this in the context of creating geodemographic classifications, where multivariate comparison for the variable selection process is an important, yet time-consuming and intensive process. We propose a visual interactive approach which allows skewed variables and those with strong correlations to be quickly identified and investigated and the geography of multi-scale correlation to be explored. Our objective is to present comprehensive documentation of the parameter space prior to the development of the visualization tools to help explore it.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042515,no,no,1485873341.963464
Challenges of mapping financial analytics to many-core architecture,"Summary form only given. In the past 20 years there has been an explosive growth of the variety of traded financial instruments, from European and American options to a more complex, alas ill-fated, credit derivatives. The rapid increase in computational power coupled with the use of mathematical tools for valuing these instruments and estimating the risk has given rise to the discipline of computational finance. Multi- and many-core architectures present significant potential for performance gains in financial applications. Recent years have seen an emergence of a variety of such designs, from general-purpose multi-cores, to GPGPU-, ASIC- and FPGA- style accelerates. To efficiently utilize hundreds of gigaflops offered by such systems requires serious optimization and parallelization effort from an application programmer. This can be a significant deterrent to a quant, traditionally focused on development, validation and deployment speed of a given pricing model, rather than optimizing model implementation for higher performance. In this talk I will describe several recent parallel hardware and software platforms for accelerating financial analytics. I will also discuss the impact that many-core era will have on financial industry and implications for both quants as well as parallel platform designers.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4745397,no,no,1485873341.963462
VAST Challenge MC1: An off the shelf approach to messy data,"We describe our approach to the VAST challenge using off the shelf software, for analysis and visualization. This allowed us to gain insight into the data within a short time-frame. We discuss the merits of our approach and set out directions for future research based on our experience.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042575,no,no,1485873341.963461
A survey-based study of the mapping of system properties to ISO/IEC 9126 maintainability characteristics,"The ISO/IEC 9126 international standard for software product quality is a widely accepted reference for terminology regarding the multi-faceted concept of software product quality. Based on this standard, the Software Improvement Group has developed a pragmatic approach for measuring technical quality of software products. This quality model introduces another level below the hierarchy defined by ISO/IEC 9126, which consists of system properties such as volume, duplication, unit complexity and others. A mapping between system properties and ISO/IEC 9126 characteristics is defined in a binary fashion: a property either influences a characteristic or not. This mapping embodies consensus among three experts based, in an informal way, on their experience in software quality assessment. We have conducted a survey-based experiment to study the mapping between system properties and quality characteristics. We used the Analytic Hierarchy Process as a formally structured method to elicit the relative importance of system properties and quality characteristics from a group of 22 software quality experts. We analyzed the results of the experiment with two objectives: (i) to validate the original binary mapping and (ii) to refine the mapping using the elicited relative weights.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306346,no,no,1485873341.96346
Software Performance Estimation in MPSoC Design,"Estimation tools are a key component of system-level methodologies, enabling a fast design space exploration. Estimation of software performance is essential in current software-dominated embedded systems. This work proposes an integrated methodology for system design and performance analysis. An analytic approach based on neural networks is used for high-level software performance estimation. At the functional level, this analytic tool enables a fast evaluation of the performance to be obtained with selected processors, which is an essential task for the definition of a ""golden"" architecture. From this architectural definition, a tool that refines hardware and software interfaces produces a bus-functional model. A virtual prototype is then generated from the bus-functional model, providing a global, cycle-accurate simulation model and offering several features for design validation and detailed performance analysis. Our work thus combines an analytic approach at functional level and a simulation-based approach at bus functional level. This provides an adequate trade-off between estimation time and precision. A multiprocessor platform implementing an MPEG4 encoder is used as case study, and the analytic estimation results in errors only up to 17% when compared to the virtual platform simulation. On the other hand, the analytic estimation takes only 17 seconds, against 10 minutes using the cycle-accurate simulation model.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4195993,no,no,1485873341.963458
The selection of project management software by FAHP and FMCDM in automobile R&D process,"The selection of project management (PM) software is a complex issue and has significant impacts to the efficiency of the automobile research & development (R&D) process. Given two alternatives based on critical path method (CPM) and critical chain project management (CCPM) respectively, decision-makers usually cause confusion. This paper analyzed the characteristic of automobile R&D project and developed an evaluation model based on the fuzzy analytic hierarchy process (FAHP) and fuzzy multiple criteria decision making (FMCDM) methods, which can help the managers in automobile industry to select more appropriate software in the course of product R&D. In particular, the FAHP method is used to obtain the weights of evaluation criteria, and the FMCDM method is used to determine the final rank of the software. The uncertainty and vagueness in evaluation procedure were presented as the fuzzy triangular numbers. The proposed method is applied to the case study of evaluating the performance of PM software based on CPM and CCPM respectively for a car manufacturer. The result of evaluation not only assisted managers in choosing suitable software but also outline the trend in development of CCPM software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5479297,no,no,1485873341.963457
Software Analytics: So What?,"The guest editors of this special issue of IEEE Software invited submissions that reflected the benefits (and drawbacks) of software analytics, an area of explosive growth. They had so many excellent submissions that they had to split this special issue into two volumes--you'll see even more content in the September/October issue. They divided the articles on conceptual grounds, so both volumes will feature equally excellent work. The Web extra at http://youtu.be/nO6X0azR0nw is a video interview in which IEEE Software editor in chief Forrest Shull speaks with Tim Menzies about the growing importance of software analytics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547619,no,no,1485873341.963455
Intelligence Database Creation and Analysis: Network-Based Text Analysis versus Human Cognition,"The 9/11 Commission Report and the National Intelligence Reform Act both state that the development of terrorist network database collection processes is an immediate and pressing requirement. This paper is a study and comparison of two complementary approaches to developing a terror network dataset: Automap, a network text analysis (NTA) tool; and Intelligence Analyst coding, a human process. NTA tools are an emerging branch of software that supports the analysis of quantitative characteristics of large-scale textual data as well as the extraction of meaning from texts. Intelligence Analyst coding is the traditional method that requires a human to read and cognitively process each raw field report. In this study, both approaches were applied to the same one hundred eighty-three open source texts on the Al Qaeda organization. Each approach's process, dataset product, and analytics are compared qualitatively and quantitatively. In terms of process, the Automap-assisted system required less manpower and time resources. In terms of dataset product, both approaches identified unique nodes and relationships that the other missed. Lastly, the differences in the datasets significantly impacted threat analytics and potential course of action selection. These results suggest an integrated human-centered automation support approach to intelligence dataset development.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4438779,no,no,1485873341.963454
Performance indicators in software project monitoring: Balanced scorecard approach,"Balanced scorecard is one of most used methodologies for performance measurement in enterprises and non-profit organizations. It presents a general framework that has been applied more precisely by many scientists and practitioners. It has been adapted to particular performance measurement systems. These monitoring systems are based on collecting data and analysis of these data. This paper presents overview of implementing balanced scorecard in IT project management. Framework for software project success monitoring, based on PRINCE2 methodology and balanced scorecard is proposed. Key performance indicators as measures to be performed upon data collected during software project implementation are defined. This way, analytics to be performed upon data according to specified key performance indicators, needed for decision support is enabled.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6339539,no,no,1485873341.963452
Towards a conceptual framework for visual analytics of time and time-oriented data,"Time is an important data dimension with distinct characteristics that is common across many application domains. This demands specialized methods in order to support proper analysis and visualization to explore trends, patterns, and relationships in different kinds of time-oriented data. The human perceptual system is highly sophisticated and specifically suited to spot visual patterns. For this reason, visualization is successfully applied in aiding these tasks. But facing the huge volumes of data to be analyzed today, applying purely visual techniques is often not sufficient. Visual analytics systems aim to bridge this gap by combining both, interactive visualization and computational analysis. In this paper, we introduce a concept for designing visual analytics frameworks and tailored visual analytics systems for time and time-oriented data. We present a number of relevant design choices and illustrate our concept by example.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419666,no,no,1485873341.608384
Evaluating vendor's performance in outsource software development risks using analytic hierarchy process technique,"Outsource software development approach has been recognized as an important practice in government agencies as well as in private sectors. It includes third parties role and involvement in IT implementation and thus this approach has certain level of risks that need to be identified, measured and managed. The benefits and reasons associated with outsourcing implementation are operation and development cost reduction, operational efficiency and improves service quality. Although there are advantages in this approach, but there are also risks that associated with it. In this study, a survey was conducted to recognize key risks involve in outsource software development and to access and classify risk's level and severity in the software project development and implementation. Then, the identified risks were analyzed using AHP technique to evaluate the performance of the vendors in outsourcing data collected in the organization. The evaluation was carried out with officers in The Rubber Industry Smallholders Development Authority or RISDA in Malaysia because this organization has experience in outsourcing for more than 15 years. The results categorize key risks associated with system software development and the method developed helps the organizations to manage risks based on the importance and harshness of the risk towards organisations. The proposed method is developed and established on risk analysis to measure performance of the vendors using multi-criteria selection method.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6985989,no,no,1485873341.608383
Study of Measurement Approach of Loop Gain of Converter,"In this paper, an approach by using the Agilent 4395A to measure the loop gain of the power supply has been deeply developed. The main issues have been investigated as the following: (1). Measurement techniques about the parasitic parameters of the filtering components will be studied. One is how to test of the electrolyte capacitor by Agilent 4395A; another is how to test dynamic inductance of filter inductor by Tektronix TDS5052 with TDSPWR3 software; (2). A measurement approach for loop gain has been put forward; (3). A prototype has been made up and its loop gain has been tested to verify the proposed approaches; (4). The experimental result has first revealed that the biggest analytic error of the loop gain occurs nearby about the resonance frequency of low pass filter. It can be explained that the error results from the equivalent resistor of the power diode and MOSFET; (5). Based on considering the equivalent resistor, a novel accurate small-signal model of buck converter has been proposed to reduce the theoretic error. The proposed approaches, model and other results have provided a powerful tool to analyze and design a power supply",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777983,no,no,1485873341.608381
Keynote Addresses,These tutorials discuss the following: Business analytics and optimization in software development-experience at IBM Rational; and Measurement impossible: How a measure for value saved NASA JPL's software assurance program.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113035,no,no,1485873341.60838
Improved analytical multiphysical modeling of a surface PMSM,"Permanent Magnet Synchronous Machines (PMSMs) are complex systems where a great amount of physical phenomena are produced simultaneously. Most of the existing PMSMs models are based on empirical formulations and standard design rules which are not suitable for high-performance applications or optimized design processes. The aim of this paper is to present an improved PMSM model which offers a holistic, multiphysic, modular and very fast approach capable of supporting a subsequent optimized design methodology. A complete multiphysic analysis which takes into account a coupled and analytic modeling of the magnetic, electrical, thermal and vibro-acoustics domains will be fully explained and applied to model a 10-poles 12-slots (Q12p5) PMSM. The achieved results are compared with those obtained in commercial software (FLUX2DŒ¬, ANSYSŒ¬ and Motor-CADŒ¬) getting high accuracy.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6960338,no,no,1485873341.608379
DWARF: an approach for requirements definition and management of data warehouse systems,"In the novel domain of data warehouse systems, software engineers are required to define a solution that integrates with a number of heterogeneous sources to extract, transform and aggregate data, as well as to offer flexibility to run adhoc queries that retrieve analytic information. Moreover, these activities should be performed based on a concise dimensional schema. This intricate process with its particular multidimensionality claims for a requirements engineering approach to aid the precise definition of data warehouse applications. We adapt the traditional requirements engineering process and propose DWARF, a data warehouse requirements definition method. A case study demonstrates how the method has been successfully applied in the company wise development of a large-scale data warehouse system that stores hundreds of gigabytes of strategic data for the Brazilian Federal Revenue Service.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232739,no,no,1485873341.608377
A quantitative software quality evaluation model for the artifacts of component based development,"Recently, software quality evaluation based on ISO/IEC 9126 and ISO/IEC 14598 has been used widely. However, these standards for software quality don't provide practical guidelines to apply the quality model and the evaluation process to real projects. Hence, this paper presents a quantitative software quality evaluation model for the artifacts of the component based development (CBD) methodology which is developed by the Ministry of National Defense of the Republic of Korea, Particularly, our model adopts the weights of quality characteristics which are obtained by carefully selected questionnaires for the stakeholders and analytic hierarchical process (AHP) technique. We also present the evaluation process using checklists and the result of a trial evaluation for validation of our model. As a result, we believe that the proposed model helps to acquire high quality software.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1434862,no,no,1485873341.608375
Visual Readability Analysis: How to Make Your Writings Easier to Read,"We present a tool that is specifically designed to support a writer in revising a draft version of a document. In addition to showing which paragraphs and sentences are difficult to read and understand, we assist the reader in understanding why this is the case. This requires features that are expressive predictors of readability, and are also semantically understandable. In the first part of the paper, we, therefore, discuss a semiautomatic feature selection approach that is used to choose appropriate measures from a collection of 141 candidate readability features. In the second part, we present the visual analysis tool VisRA, which allows the user to analyze the feature values across the text and within single sentences. Users can choose between different visual representations accounting for differences in the size of the documents and the availability of information about the physical and logical layout of the documents. We put special emphasis on providing as much transparency as possible to ensure that the user can purposefully improve the readability of a sentence. Several case studies are presented that show the wide range of applicability of our tool. Furthermore, an in-depth evaluation assesses the quality of the measure and investigates how well users do in revising a text with the help of the tool.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051432,no,no,1485873341.608373
Information needs for software development analytics,"Software development is a data rich activity with many sophisticated metrics. Yet engineers often lack the tools and techniques necessary to leverage these potentially powerful information resources toward decision making. In this paper, we present the data and analysis needs of professional software engineers, which we identified among 110 developers and managers in a survey. We asked about their decision making process, their needs for artifacts and indicators, and scenarios in which they would use analytics. The survey responses lead us to propose several guidelines for analytics tools in software development including: Engineers do not necessarily have much expertise in data analysis; thus tools should be easy to use, fast, and produce concise output. Engineers have diverse analysis needs and consider most indicators to be important; thus tools should at the same time support many different types of artifacts and many indicators. In addition, engineers want to drill down into data based on time, organizational structure, and system architecture.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227122,no,no,1485873341.608371
A component and query approach for scientific data exploration applications,"The immense increase of scientific databases, combined with the spread of open standards for service interfaces, has resulted in unprecedented opportunities of data exploration and exploitation for scientists. From a client point of view, scientists often perform data combination and analysis, based on interactive incremental refinement and visualization. While this abstract description applies to virtually any science, the concrete presentation of data and queries offered varies greatly across the individual domains. The result is a repetitive effort in crafting streamlined client interfaces, which is particular tedious in face of interactive graphics. We address this problem by establishing a modular Web client toolkit offering common analysis input and output widgets. The backend generates database queries from the input and displays query results through the output widgets. We present this toolkit, its first implementation version, and an earth science scenario using an Array Database System as retrieval backend.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6166204,no,no,1485873341.608369
Systemic assessment of risks for projects: A systems and Cybernetics approach,"The current and past success rate of software projects is poor. This is mainly due to the reductionist/ analytic methods used in project risk assessments. The risk assessment practices adhered today based on standards or otherwise are non-systemic. These assessments cannot provide or deal with the systemic view of Project risks. They handle the project context and complex issues in Projects inadequately. The issue with reductionist thinking is that it leads to reductionist approach to problem solving and history has shown us Project failures continue to happen. This paper explains the Systemic Assessment of Risks (SAR) methodology that is proposed for assessment of project risks by considering Project as a system. This methodology uses the Cybernetics Risk Influence Diagramming (CRID) technique for identification of probable interconnected, interrelated and emergent risks. SAR's application in a software development project in a telecommunications enterprise demonstrates the methodology with the project risks assessed systemically.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031745,no,no,1485873341.270111
Software engineering for the industrial Internet: Situation-aware smart applications,"Summary form only given. With the rise of the Industrial Internet the world entered a new era of innovation. At the heart of this new industrial revolution is the convergence of the global industrial system with computing power, low-cost sensing, big data, predictive analytics, and ubiquitous connectivity. The growing proliferation of smart devices and applications is accelerating the convergence of the physical and the digital worlds. Smart apps allow users, with the help of sensors and networks, to do a great variety of things, from tracking their friends to controlling remote devices and machines. At the core of such smart systems are self-adaptive systems that optimize their own behaviour according to high-level objectives and constraints to address changes in functional and non-functional requirements as well as environmental conditions. Self-adaptive systems are implemented using four key technologies: runtime models, context management, feedback control theory, and run-time verification and validation. The proliferation of highly dynamic and smart applications challenges the software engineering community in re-thinking the boundary between development time and run time and developing techniques for adapting systems at run time. The key challenge is to automate traditional software engineering, maintenance and evolution techniques to adapt and evolve systems at run time with minimal or no human interference. Hitherto, most developers did not instrument their software with sensors and effectors to observe whether requirements are satisfied in an evolving environment at run time. One way to break out of this mold is to make the four key technologies readily accessible at run time.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6642408,no,no,1485873341.27011
Analytics-Driven Dashboards Enable Leading Indicators for Requirements and Designs of Large-Scale Systems,"Mining software repositories using analytics-driven dashboards provides a unifying mechanism for understanding, evaluating, and predicting the development, management, and economics of large-scale systems and processes. Dashboards enable measurement and interactive graphical displays of complex information and support flexible analytic capabilities for user customizability and extensibility. Dashboards commonly include system requirements and design metrics because they provide leading indicators for project size, growth, and volatility. This article focuses on dashboards that have been used on actual large-scale software projects as well as example empirical relationships revealed by the dashboards. The empirical results focus on leading indicators for requirements and designs of large-scale software systems based on insights from two sets of software projects containing 14 systems and 23 systems.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721182,no,no,1485873341.270108
Visual Analysis of Historic Hotel Visitation Patterns,"Understanding the space and time characteristics of human interaction in complex social networks is a critical component of visual tools for intelligence analysis, consumer behavior analysis, and human geography. Visual identification and comparison of patterns of recurring events is an essential feature of such tools. In this paper, we describe a tool for exploring hotel visitation patterns in and around Rebersburg, Pennsylvania from 1898-1900. The tool uses a wrapping spreadsheet technique, called reruns, to display cyclic patterns of geographic events in multiple overlapping natural and artificial calendars. Implemented as an improvise visualization, the tool is in active development through a iterative process of data collection, hypothesis, design, discovery, and evaluation in close collaboration with historical geographers. Several discoveries have inspired ongoing data collection and plans to expand exploration to include historic weather records and railroad schedules. Distributed online evaluations of usability and usefulness have resulted in numerous feature and design recommendations",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035745,no,no,1485873341.270107
Measurement-driven dashboards enable leading indicators for requirements and design of large-scale systems,"Measurement-driven dashboards provide a unifying mechanism for understanding, evaluating, and predicting the development, management, and economics of large-scale systems and processes. Dashboards enable interactive graphical displays of complex information and support flexible analytic capabilities for user customizability and extensibility. Dashboards commonly include software requirements and design metrics because they provide leading indicators for project size, growth, and stability. This paper focuses on dashboards that have been used on actual large-scale projects as well as example empirical relationships revealed by the dashboards. The empirical results focus on leading indicators for requirements and design of large-scale systems. In the first set of 14 projects focusing on requirements metrics, the ratio of software requirements to source-lines-of code averaged 1:46. Projects that far exceeded the 1:46 requirements-to-code ratio tended to be more effort-intensive and fault-prone during verification. In the second set of 16 projects focusing on design metrics, the components in the top quartile of the number of component internal states had 6.2 times more faults on average than did the components in the bottom quartile, after normalization by size. The components in the top quartile of the number of component interactions had 4.3 times more faults on average than did the components in the bottom quartile, after normalization by size. When the number of component internal states was in the bottom quartile, the component fault-proneness was low even when the number of component interactions was in the upper quartiles, regardless of size normalization. Measurement-driven dashboards reveal insights that increase visibility into large-scale systems and provide feedback to organizations and projects",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509300,no,no,1485873341.270105
The method of quality management software,"Were considered the method of process quality management software in its design. The method includes the development of quality requirements based on ISO 25010, and implementation procedures of communication requirements. This allowed to realize the monitoring process as an intermediate product life-cycle and thus ensure compliance with software quality requirements. Communication requirements of the procedure developed based on the method SQFD (Software Quality Funktion Deployment) and the modified Analytic Hierarchy Process. Were developed also äóìCASEäó method which automatizes implementation of these processes.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5960278,no,no,1485873341.270104
What Use is Verified Software?,"The world at large cares little for verified software; what it cares about are trustworthy and cost-effective systems that do their jobs well. We examine the value of verified software and of verification technology in the systems context from two perspectives, one analytic, the other synthetic. We propose some research opportunities that could enhance the contribution of the verified software initiative to the practices of systems engineering and assurance.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276323,no,no,1485873341.270102
"Wikis, semantics, and collaboration: Symposium on collaboration analysis and reasoning systems, at the 2014 conference on collaboration technologies and systems","Designing software for collaborative sensemaking environments begins with a set of very challenging requirements. At a high level, the software needs to be flexible enough to support multiple lines of inquiry, contradictory hypotheses, and collaborative tasking by multiple analysts. It should also include support for managing evolving human/machine workflows and analytic products at various levels of strictness and formality, processing partial and ambiguous evidence arriving in streams, and developing explanatory scenarios based on both serendipitous and structured discovery. Eventually, it should support the analytic team as they evaluate multiple alternatives and converge on one or more consensus responses, while preserving the history and underlying reasoning. Finally, it should be delightful and simple to use, not require an inordinate degree of precision and exactness, and be quickly and inexpensively deployable in a variety of rapid-response analytic situations. It has not been possible thus far to create a single software architecture that adequately balances all these goals. However, we can shed useful light on this problem by looking at the experience of semantic wiki architectures: an emerging class of software that blends wikis, databases, social tagging systems, and Semantic Web representations.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6867607,no,no,1485873341.270101
How to tackle security issues in large existing/legacy systems while maintaining development priorities,"Legacy software systems represent a large base of software assets that hold significant corporate intellectual properties, along with carrying large opportunity costs and operational risks. There is a growing need to prolong their lifespan through maintenance efforts and enhance them to accommodate new and changing market requirements and governmental regulations. The majority of these systems were developed at a time when security requirements were more relaxed and not well understood, and at a time when being netted did not have the same consequences as exist today. There is a real need for retrofitting security into legacy software systems so they can operate in the current environments. However, over time, as legacy systems became larger and more complex, their design structure eroded which hinders system comprehension, compromises architectural integrity and decreases maintenance productivity. This makes the task of retrofitting security difficult and risky. Since legacy systems are a large part of our nations' critical infrastructure we must retrofit-in security in such way that the level of confidence related to security is substantially increased. This paper will discuss a standards based approach to achieving this goal.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4534443,no,no,1485873341.270099
Reverse Engineering PL/SQL Legacy Code: An Experience Report,"The reengineering of legacy code is a tedious endeavor. Automatic transformation of legacy code from an old technology to a new one preserves potential problems in legacy code with respect to obsolete, changed, and new business cases. On the other hand, manual analysis of legacy code without assistance of original developers is time consuming and error-prone. For the purpose of reengineering PL/SQL legacy code in the steel making domain, we developed tool support for the reverse engineering of PL/SQL code into a more abstract and comprehensive representation. This representation then serves as input for stakeholders to manually analyze legacy code, to identify obsolete and missing business cases, and, finally, to support the re-implementation of a new system. In this paper we briefly introduce the tool and present results of reverse engineering PL/SQL legacy code in the steel making domain. We show how stakeholders are supported in analyzing legacy code by means of general-purpose analysis techniques combined with domain-specific representations and conclude with some of the lessons learned.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976137,no,no,1485873341.270097
Dependency Analysis Framework for Software Service Delivery,"Various phases in the delivery of software services such as solution design, application deployment, and maintenance require analysis of the dependencies of software products that form the solution. As software systems become more complex and involve a large number of software products from multiple vendors, availability of correct and up-to-date system requirement information becomes critical to ensure proper functioning of managed and maintained software solutions. System requirement information, is mostly made available in unstructured formats from sources such as websites or product documents and are not amenable to programmatic analysis. In this paper, we motivate the benefits of capturing this information in a structured format for software service delivery, and present a dependency analysis system that collects and integrates software dependency/interoperability information from multiple unstructured sources using text mining techniques. Information hence collected, is used to support analytics useful in software service delivery. We report the results of our experiments on mining millions of web pages to collect dependency information for more than 700 software products.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284017,no,no,1485873340.941487
Empirical analysis of user data in game software development,"For several years empirical studies have spanned the spectrum of research from software productivity, quality, reliability, performance to human computer interaction. Analyses have involved software systems ranging from desktop software to telecommunication switching systems. But surprising there has been little work done on the emerging digital game industry, one of the fastest growing domains today. To the best of our knowledge, our work is one of the first empirical analysis of a large commercially successful game system. In this paper, we introduce an analysis of the significant user data generated in the gaming industry by using a successful game: Project Gotham Racing 4. More specifically, due to the increasing ubiquity of constantly connected high-speed internet connections for game consoles, developers are able to collect extensive amounts of data about their games following release. The challenge now is to make sense of that data, and from it be able to make recommendations to developers. This paper presents an empirical case study analyzing the data collected from a released game over a three year period. The results of this analysis include a better understanding of the differences between long-term and short-term players, and the extent to which various options in the game are utilized. This led to recommendations for future development ways to reduce development costs and to keep new players engaged. A secondary goal for this paper is to introduce software game development as a topic of importance to the empirical software engineering community and discuss research results on a key difference area: data analytics on user data to customize user and development experiences.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475400,no,no,1485873340.941486
Addressing human bottlenecks in big data,"We live in an era when compute is cheap, data is plentiful, and system software is being given away for free. Today, the critical bottlenecks in data-driven organizations are human bottlenecks, measured in the costs of software developers, IT professionals, and data analysts. How can computer science remain relevant in this context? The Big Data ecosystem presents two archetypal settings for answering this question: NoSQL distributed databases, and analytics on Hadoop. In the case of NoSQL, developers are being asked to build parallel programs for global-scale systems that cannot even guarantee the consistency of a single register of memory. How can this possibly be made to work? I'll talk about what we have seen in the wild in user deployments, and what we've learned from developers and their design patterns. Then I'll present theoretical results - the CALM Theorem - that shed light on what's possible here, and what requires more expensive tools for coordination on top of the typical NoSQL offerings. Finally, I will highlight some new approaches to writing and testing software - exemplified by the Bloom language - that can help developers of distributed software avoid expensive coordination when possible, and have the coordination logic synthesized for them automatically when necessary. In the Hadoop context, the key bottlenecks lie with data analysts and data engineers, who are routinely asked to work with data that cannot possibly be loaded into tools for statistical analytics or visualization. Instead, they have to engage in time-consuming data äóìwranglingäó - to try and figure out what's in their data, whip it into a rectangular shape for analysis, and figure out how to clean and integrate it for use. I'll discuss what we heard talking with data analysts in both academic interviews and commercial engagements. Then I'll talk about how techniques from human-computer interaction, machine learning, and database systems can be brought together - o address this human bottleneck, as exemplified by our work on various systems including the Data Wrangler project and Trifacta's platform for data transformation.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004205,no,no,1485873340.941484
Front Cover,July/August 2013 IEEE Software: Software Analytics: So What?,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547616,no,no,1485873340.941483
Challenges and perspectives in an undergraduate flipped classroom experience: Looking through the lens of learning analytics,"Recent technical and infrastructural developments posit flipped classroom approaches ripe for exploration. Flipped classroom approaches have students use technology to access the lecture and other instructional resources outside the classroom in order to engage them in active learning during in-class time. Scholars and educators have reported a variety of positive outcomes of a flipped (or inverted) approach to instruction. Although, flipped classroom practices have been used in a number of education studies, the detailed framework and data obtained from students' interaction with the technology materials are typically not described. In this paper, we present a flipped classroom framework and the first captured results of such data. The framework incorporates basic e-learning tools and traditional learning practices, making it accessible to anyone wanting to implement a flipped classroom experience in his/her course. The framework is structured on open-source and easy-to-use tools, allowing for the incorporation of any additional specificities of a course. This work-in-progress can provide insights for other scholars and practitioners to further validate, examine, and extend the proposed approach. This approach can be used for those interested in incorporating flipped classroom in their teaching, since it is a flexible procedure that may be adapted to meet their needs.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044449,no,no,1485873340.94148
ProDV äóî A case study in delivering visual analytics,"We present a custom visual analytics system developed in conjunction with the test and evaluation community of the US Army. We designed and implemented a visual programming environment for configuring a variety of interactive visual analysis capabilities. Our abstraction of the visualization process is based on insights gained from interviews conducted with expert users. We show that this model allowed analysts to implement multiple visual analysis capabilities for network performance, anomalous sensor activity, and engagement results. Long-term interaction with expert users led to development of several custom visual analysis techniques. We have conducted training sessions with expert users, and are working to evaluate the success of our work based on performance metrics captured in a semi-automated fashion during these training sessions. We have also integrated collaborative analysis features such as annotations and shared content.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5650219,no,no,1485873340.941479
A Three-Dimensional Model for Software Security Evaluation,"Software security evaluation is considered as a significant and indispensible activity in all phases of software development lifecycle, and there are also many factors that should be taken into account such as the environment, risks, and development documents. Despite the achievements of the past several decades, there is still a lack of methodology in evaluating software security systematically. In this paper, we propose a comprehensive model for evaluating the software security from three different but complementary points of view: technology, management and engineering. The technological dimension is 7 security levels based on Evaluation Assurance Levels (EALs) from ISO/IEC15408, the management dimension mainly concerns the management of software infrastructures, development documents and risks, and the engineering dimension focuses on 5 stages of software development lifecycle. Experts evaluate software security through the evidence items which are collected from these three dimensions and provide their assessments. Relying on Analytic Hierarchy Process (AHP) and Dempster-Shafer Evidence Theory, assessments obtained from the experts can be combined and merged to get a score which presents the security degree of software. A case study illustrates how the evaluators may use the proposed approach to evaluate security of their system.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976565,no,no,1485873340.941477
3D software tool for reliability assessment based on three dimensional wiener process model considering big data on cloud computing,"At present, many software services by using cloud computing are provided because of the unification management of data, low cost. We focus on big data on cloud computing by using open source software such as OpenStack and Eucalyptus. In order to consider the interesting aspect of the big data on cloud computing, we propose a new approach to software reliability assessment based on an AHP and three dimensional stochastic differential equation models. Moreover, we develop the 3D software tool based on our model.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014658,no,no,1485873340.941475
Aperture: An Open Web 2.0 Visualization Framework,"Aperture is an open, adaptable and extensible Web 2.0 visualization framework, designed to produce visualizations for analysts and decision makers in any common web browser. Aperture utilizes a novel layer based approach to visualization assembly, and a data mapping API that simplifies the process of adaptable transformation of data and analytic results into visual forms and properties. This common visual layer and data mapping API, combined with core elements such as contextually derivable color palettes, layout and symbol ontology services is designed to enable highly creative and expressive visual analytics, rapidly and with less effort. This paper introduces the Aperture framework, describing key features of the programming API and reference implementation, presents example use cases, and proposes an approach for measuring technical performance metrics for software development, and operational performance metrics for visualization support of analysis and decision making.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6480018,no,no,1485873340.941473
An analytic software testability model,"Software testability, which has been discussed in the past decade, has been defined as provisions that can be taken into consideration at the early step of software development. This paper gives software testability, previously defined by Voas, a new model and measurement without performing testing with respect to a particular input distribution.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181724,no,no,1485873340.647862
Appscio: A Software Environment for Semantic Multimedia Analysis,"The goal of the Appscio(tm) software platform is to ease the creation of multimedia content analysis applications that consist of components provided from multiple sources, in different programming languages, and for various operating systems. Appscio provides a unified approach that standardizes the entire process of development, deployment, and integration of components into productive applications. In addition, the aim is to facilitate the integration of analytic approaches with traditional sensor output. Therefore the framework allows the combination of multimedia analytics with any other event generating sources, as used for observational systems. A basic concept of the platform is to allow mainstream application developers to create semantic-rich Web applications that integrate components previously only accessible to scientists.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4597225,no,no,1485873340.647861
Guest Editors' Introduction: Studying Professional Software Design,"This special issue sets an agenda for research into early software design, and this introduction outlines drivers and issues for that agenda. It argues that looking at software from a design perspective, understanding software as a designed artifact, and considering how design reaches into the whole software life cycle can bring significant benefits both to our understanding of what works in software design and to our approach to tools and practices. The special issue presents outputs from an NSF-funded workshop on 'Studying Professional Software Design' held in 2010 at UC Irvine in which participants analyzed the same professional design sessions from different analytic perspectives. The workshop dialogues provide an example of what's critically needed to drive this research agenda: empirically grounded dialogues between research and practitioners.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111365,no,no,1485873340.647859
Refinement-Friendly Bigraphs and Spygraphs,"Over the past decade the successful approach to specification and mechanical analysis of correctness and security properties using CSP and its refinement checker FDR has been extended to contexts including mobile ad-hoc networks and pervasive systems. But the more scope for network reconfiguration the system exhibits, the more intricate and less obviously accurate the models must become in order to accommodate such dynamic behaviour in a language with a basically static process and communication graph. Milner's Bigraph framework, on the other hand, and in particular Blackwell's Spygraph specialisation, are ideally suited for describing intuitively such dynamic reconfigurations of a system and support notions of locality and adjacency which fit them well for reasoning, for instance, about the interface between physical and electronic security; but they lack powerful analytic tool support. Our long-term goal is to combine the best of both approaches. Unfortunately the canonical labelled transition system induced by the category-theoretic semantics of a bigraphical reactive system present a number of challenges to the refinement-based approach. Prominent amongst these is the feature that the label on a transition is the 'borrowed context' required to make the redex of some reaction rule appear in the augmented source bigraph; this means that any reaction which can already take place entirely within a given bigraph gives rise to a transition labelled only with the trivial identity context, equivalent to a tau transition in CCS or CSP, with the result that neither the reaction rule nor the agents involved can be distinguished. This makes it quite impossible for an observer of the transition system to determine whether such a reaction was desirable with respect to any specification. We are investigating ways to remedy this situation. Here we present a systematic transformation of a bigraphical reactive system, both its rules and the underlying bigraphs, with the effec- - t that every transition becomes labelled with the specific rule that gave rise to it and the set of agents involved. We also consider how that now possibly over-precise labelling can be restricted through selective hiding and judicious forgetful renaming.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5637430,no,no,1485873340.647858
The design and development of a sales force automation tool using business process management software,"A sales force automation (SFA) tool is a computerized system that provides sales team members and managers with the functionality to track sales leads, manage contacts, control customer relations, monitor sales processes, schedule meetings, forecast sales and analyze employee performance. SFA tools aim to increase the efficiency and effectiveness of a sales team; however many commercially available SFA tools are generically structured solutions that do not accommodate the specific needs of a company. However, because of recent interest and developments in business process management (BPM) software, an emerging technology capable of modeling and automating business processes, it is possible for individual firms to custom-design their own sales force automation tools. In general, BPM software tools provide automated support for tracking tasks across multiple departments as they are completed by different employees. This paper discusses the design and development of an SFA tool using BPM software, including the identification of a detailed sales process, an analysis of the reporting capabilities, a model for determining the probabilistic outcomes of the sales process, and a decision-analytic model for optimizing sales force resource allocation.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1497167,no,no,1485873340.647856
"From Student to Teacher: Transforming Industry Sponsored Student Projects into Relevant, Engaging, and Practical Curricular Materials","Over the past several years we have collaborated with a variety of industrial partners to carry out applied research and capstone design projects in cooperation with our students. Although the projects have varied widely, more often than not, success or failure lies within the students' ability to see beyond the technical challenges into the subtleties of the business and the meaning of value. Looking back at our traditional software engineering curriculum it is not so surprising that gaps in technical skills are not typically the source of problems. With a strong traditional focus on the construction of software, we have been producing graduates who can build relatively complex stand-alone systems. Unfortunately, in today's world, being able to build software is only a small, albeit necessary, skill for software engineers and it is miles away from being sufficient. The challenges inherent in providing a portfolio of innovative, integrated, and strategic IT services are well beyond any of the techniques or conceptual frameworks historically taught in many software engineering curriculums, including our own. To address these shortcomings we have recently begun experimenting with a new curriculum that presents software engineering in its larger context as a strategic business function. We are also beginning to stress the importance of using a set of analytic frameworks to guide the evolution and development of software systems starting with the business and its context, through the architecture and design stages, and finally into implementation and support. To create materials for this curriculum we have gone back to the original voice of the problem and are attempting to assemble learning materials from the projects that industry has championed for us in the past. Our goal is not merely to showcase the software that was built, but rather to expose the reasons behind their conception and the frameworks used to make critical decisions throughout the process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5508872,no,no,1485873340.647855
Video games as a medium for software education,"Educational video games may be used as a medium for software visualisation and visual programming to provide highly enjoyable, self-motivating and inquiry-based pedagogical tools. An educational game has been developed and tested on university-level students in three iterations. Players are required to solve puzzles by programming the solutions; the game introduces syntax, conditional statements and logical operators. An integrated analytics system is used to store the time taken, the number of lines of code, and players' solutions to each level. Qualitative feedback indicates that the tool is very easy to learn because of the help system and user interface. A software quiz was administered before and after participants played the game. When tested on 14 applied computing students (who had formal exposure to programming), there was no increase in the average grade. In a group of 32 electrical engineering students (who had no exposure to programming at university), the game helped about 60% of participants increase their grade, by an average of 11%.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329850,no,no,1485873340.647853
Proceedings 2001 IEEE International Conference on Data Mining,The following topics are dealt with:data mining; intelligent agents with data mining capabilities; multi-modality interfaces and Internet retrieval systems; data mining for marketing; effective data mining and online analytic mining for efficient use of the Internet; agent technology; and collaborative work,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989492,no,no,1485873340.647851
GenAMap: Visualization strategies for structured association mapping,"Association mapping studies promise to link DNA mutations to gene expression data, possibly leading to innovative treatments for diseases. One challenge in large-scale association mapping studies is exploring the results of the computational analysis to find relevant and interesting associations. Although many association mapping studies find associations from a genome-wide collection of genomic data to hundreds or thousands of traits, current visualization software only allow these associations to be explored one trait at a time. The inability to explore the association of a genomic location to multiple traits hides the inherent interaction between traits in the analysis. Additionally, researchers must rely on collections of in-house scripts and multiple tools to perform an analysis, adding time and effort to find interesting associations. In this paper, we present a novel visual analytics system called GenAMap. GenAMap replaces the time-consuming analysis of large-scale association mapping studies with exploratory visualization tools that give geneticists an overview of the data and lead them to relevant information. We present the results of a preliminary evaluation that validated our basic approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6094052,no,no,1485873340.64785
ChronoTwigger: A Visual Analytics Tool for Understanding Source and Test Co-evolution,"Applying visual analytics to large software systems can help users comprehend the wealth of information produced by source repository mining. One concept of interest is the co-evolution of test code with source code, or how source and test files develop together over time. For example, understanding how the testing pace compares to the development pace can help test managers gauge the effectiveness of their testing strategy. A useful concept that has yet to be effectively incorporated into a co-evolution visualization is co-change. Co-change is a quantity that identifies correlations between software artifacts, and we propose using this to organize our visualization in order to enrich the analysis of co-evolution. In this paper, we create, implement, and study an interactive visual analytics tool that displays source and test file changes over time (co-evolution) while grouping files that change together (co-change). Our new technique improves the analyst's ability to infer information about the software development process and its relationship to testing. We discuss the development of our system and the results of a small pilot study with three participants. Our findings show that our visualization can lead to inferences that are not easily made using other techniques alone.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980223,no,no,1485873340.647847
A decision model for agile software release,"Analyzing the agile software development process, combining the evolution laws of agile software project, applying AHP method, an integration assessment model for agile software release is presented. The index system for agile software release plans is built, and its target layer, criterion layer and scheme layer are specified. Comparison matrix and judgment matrix are constructed with three-demarcation analytic hierarchy process (TDAHP). Combining with the views of agile experts about index matrix and weight, the qualitative and quantitative analysis about software release scheme can be carried out, and an optimal scheme can be selected. Finally, a numerical example is given, which shown this model is scientific and feasible.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979368,no,no,1485873340.376422
Migrant boat mini challenge award: Simple and effective integrated display geo-temporal analysis of migrant boats,"We provide a description of the tools and techniques used in our analysis of the VAST 2008 Challenge dealing with mass movement of persons departing Isla Del Sue.no on boats for the United States during 2005-2007. We used visual analytics to explore migration patterns, characterize the choice and evolution of landing sites, characterize the geographical patterns of interdictions and determine the successful landing rate. Our ComVis tool, in connection with some helper applications and Google Earth, allowed us to explore geo-temporal characteristics of the data set and answer the challenge questions. The ComVis project file captures the visual analysis context and facilitates better collaboration among team members.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677387,no,no,1485873340.376421
Analytical Network Process based model to estimate the quality of software components,"Software components are software units designed to interact with other independently developed software components. These components are assembled by third parties into software applications. The success of final software applications largely depends upon the selection of appropriate and easy to fit components in software application according to the need of customer. It is primary requirement to evaluate the quality of components before using them in the final software application system. All the quality characteristics may not be of same significance for a particular software application of a specific domain. Therefore, it is necessary to identify only those characteristics/ sub-characteristics, which may have higher importance over the others. Analytical Network Process (ANP) is used to solve the decision problem, where attributes of decision parameters form dependency networks. The objective of this paper is to propose ANP based model to prioritize the characteristics /sub-characteristics of quality and to o estimate the numeric value of software quality.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6781361,no,no,1485873340.376419
Exploring Approaches of Integration Software Architecture Modeling with Quality Analysis Models,"One of the important benefits of model-to-model transformation is that it allows architects to design iteratively by analyzing and studying alternative or optimal solutions without redesign of the software architecture models or quality analytic models. The main contribution of this work is the presentation of five recently approaches based on the definition of a framework which applies separation of concerns in viewpoints and perspectives. The framework definition identifies viewpoints and their sets of concerns regarding the approaches achieving the goal of integration and interoperability of tools in a model-driven and quality-driven software architecture development. Each approach presentation is a multiple views description, where a view conforms to a viewpoint. The quality of each approach depends on the perspective under which the approach is analyzed. By applying various perspectives on the views composing each approach a software architect or a modeler can select the most appropriate one. Also based on this framework of presentation, the study identifies a current state of the research in the domain defined by these representative approaches, existent limitations and future research directions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959725,no,no,1485873340.376418
Design Considerations for Collaborative Visual Analytics,"Information visualization leverages the human visual system to support the process of sensemaking, in which information is collected, organized, and analyzed to generate knowledge and inform action. Though most research to date assumes a single-user focus on perceptual and cognitive processes, in practice, sensemaking is often a social process involving parallelization of effort, discussion, and consensus building. This suggests that to fully support sensemaking, interactive visualization should also support social interaction. However, the most appropriate collaboration mechanisms for supporting this interaction are not immediately clear. In this article, we present design considerations for asynchronous collaboration in visual analysis environments, highlighting issues of work parallelization, communication, and social organization. These considerations provide a guide for the design and evaluation of collaborative visualization systems.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389011,no,no,1485873340.376416
Analyzing Actors and Their Discussion Topics by Semantic Social Network Analysis,"iQuest is a novel software system to improve understanding of organizational phenomena with greater precision, clarity, and granularity than has previously been possible. It permits to gain new insights into organizational behavior, addressing issues such as tracking information while respecting privacy, comparing different interaction channels, network membership, and correlating organizational performance and creativity. It extends automatic visualization of social networks by mining communication archives such as e-mail and blogs through including analysis of the contents of those archives",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648252,no,no,1485873340.376415
A novel domain-specific language for the robot welding automation domain,"Implementation, fault analysis, and maintenance of robot welding automation solutions are traditionally restricted to professional software developers only. Program code is written in a general purpose programming language and, hence, unmanageable by other stakeholders with limited or no programming skills. To tackle this problem we have implemented a domain-specific language (DSL) specifically designed to the domain of robot welding automation and to be intuitively manageable by all stakeholders. The created DSL supports a textual and visual notation and is embedded within a full featured tool chain which let our customer fully replace the creation and maintenance of welding automation solutions by our DSL-based development approach.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005348,no,no,1485873340.376413
Department course selection problem: The primitive cognitive network process approach,"The program curriculum in tertiary education has to be reviewed and updated regularly to enhance the education quality regarding the student interests, current industry demands and academic trends. To effectively develop a number of reasonable courses, a comprehensive approach to evaluate and select the suitable courses for students is essential for the program curriculum development. This paper proposes the Primitive Cognitive Network Process for the department course selection strategy considering multiple criteria and alternatives.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6360327,no,no,1485873340.376411
Combining iterative analytical reasoning and software development using the visualization language Processing,"Processing is a very powerful visualization language which combines software concepts with principles of visual form and interaction. Artists, designers and architects use it but it is also a very effective programming language in the area of visual analytics. In the following contribution Processing is utilized in order to visually analyze data provided by IEEE VAST 2009 Mini Challenge Badge and Network Traffic. The applied process is iterative and each stage of the analytical reasoning process is accompanied by customized software development. The visual model, the process and the technical solution will be briefly introduced.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5334463,no,no,1485873340.37641
Prototype Implementation of a Goal-Based Software Health Management Service,"The FAILSAFE project is developing concepts and prototype implementations for software health management in mission-critical real-time embedded systems. The project unites features of the industry standard ARINC 653 Avionics Application Software Standard Interface and JPL's Mission Data System (MDS) technology. The ARINC 653 standard establishes requirements for the services provided by partitioned real-time operating systems. The MDS technology provides a state analysis method, canonical architecture, and software framework that facilitates the design and implementation of software-intensive complex systems. We use the MDS technology to provide the health management function for an ARINC 653 application implementation. In particular, we focus on showing how this combination enables reasoning about and recovering from application software problems. Our prototype application software mimics the space shuttle orbiter's abort control sequencer software task, which provides safety-related functions to manage vehicle performance during launch aborts. We turned this task into a goal-based function that, when working in concert with the software health manager, aims to work around software and hardware problems in order to maximize abort performance results. In order to make it a compelling demonstration for current aerospace initiatives, we additionally imposed on our prototype a number of requirements derived from NASA's Constellation Program. Lastly, the ARINC 653 standard imposes a number of requirements on the system integrator for developing the requisite error handler process. Under ARINC 653, the health monitoring (HM) service is invoked by an application calling the application error service or by the operating system or hardware detecting a fault. It is these HM and error process details that we implement with the MDS technology, showing how a state-analytic approach is appropriate for identifying fault determination details, and showing how the framework supp- orts acting upon state estimation and control features in order to achieve safety-related goals. We describe herein the requirements, design, and implementation of our software health manager and the software under control. We provide details of the analysis and design for the phase II prototype, and describe future directions for the remainder of phase II and the new topics we plan to address in phase III.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226840,no,no,1485873340.376407
An Image-Based Approach to Extreme Scale in Situ Visualization and Analysis,"Extreme scale scientific simulations are leading a charge to exascale computation, and data analytics runs the risk of being a bottleneck to scientific discovery. Due to power and I/O constraints, we expect in situ visualization and analysis will be a critical component of these workflows. Options for extreme scale data analysis are often presented as a stark contrast: write large files to disk for interactive, exploratory analysis, or perform in situ analysis to save detailed data about phenomena that a scientists knows about in advance. We present a novel framework for a third option - a highly interactive, image-based approach that promotes exploration of simulation results, and is easily accessed through extensions to widely used open source tools. This in situ approach supports interactive exploration of a wide range of results, while still significantly reducing data movement and storage.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013022,no,no,1485873340.117178
Software Analytics in Practice,"With software analytics, software practitioners explore and analyze data to obtain insightful, actionable information for tasks regarding software development, systems, and users. The StackMine project produced a software analytics system for Microsoft product teams. The project provided lessons on applying software analytics technologies to positively impact software development practice. The lessons include focusing on problems that practitioners care about, using domain knowledge for correct data understanding and problem modeling, building prototypes early to get practitioners' feedback, taking into account scalability and customizability, and evaluating analysis results using criteria related to real tasks.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6559957,no,no,1485873340.117176
Detecting flaws and intruders with visual data analysis,"The task of sifting through large amounts of data to find useful information spawned the field of data mining. Most data mining approaches are based on machine-learning techniques, numerical analysis, or statistical modeling. They use human interaction and visualization only minimally. Such automatic methods can miss some important features of the data. Incorporating human perception into the data mining process through interactive visualization can help us better understand the complex behaviors of computer network systems. This article describes visual-analytics-based solutions and outlines a visual exploration process for log analysis. Three log-file analysis applications demonstrate our approach's effectiveness in discovering flaws and intruders in network systems.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333625,no,no,1485873340.117174
Goldfish bowl panel: Software development analytics,"Gaming companies now routinely apply data mining to their user data in order to plan the next release of their software. We predict that such software development analytics will become commonplace, in the near future. For example, as large software systems migrate to the cloud, they are divided and sold as dozens of smaller apps; when shopping inside the cloud, users are free to mix and match their apps from multiple vendors (e.g. Google Docs' word processor with Zoho's slide manager); to extend, or even retain, market share cloud vendors must mine their user data in order to understand what features best attract their clients. This panel will address the open issues with analytics. Issues addressed will include the following. What is the potential for software development analytics? What are the strengths and weaknesses of the current generation of analytics tools? How best can we mature those tools?",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227117,no,no,1485873340.117173
Are Software Analytics Efforts Worthwhile for Small Companies? The Case of Amisoft,"Amisoft, a Chilean software company with 43 employees, successfully uses software analytics in its projects. These support a variety of strategic and tactical decisions, resulting in less overwork of employees. However, the analytics done at Amisoft are very different from the ones used in larger companies.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6544520,no,no,1485873340.117172
Generating visualization-based analysis scenarios from maintenance task descriptions,"Software visualization is an efficient and flexible tool to inspect and analyze software data at various levels of detail. However, software analysts typically do not have a sufficient background in visualization and cognitive science to select efficient representations and parameters without the help of visualization experts. To overcome this problem, we propose an approach to generate software analysis tasks that use visualization. To this end, we use taxonomies of low-level analytic tasks, high-level interactive tasks, and perceptual rules to design an assistant that proposes analysis scenarios.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336423,no,no,1485873340.11717
Enabling proteomics discovery through visual analysis,"This article presents the motivation for developing visual analysis tools for proteomic data and demonstrates their application to proteomics research with a visualization tool named Peptide Permutation and Protein Prediction, or PQuad, a functioning visual analytic tool for the study of systems biology, is in operation at the Pacific Northwest National Laboratory (PNNL). PQuad supports the exploration of proteins identified by proteomic techniques in the context of supplemental biological information. In particular, PQuad supports differential proteomics by simplifying the comparison of peptide sets from different experimental conditions as well as different proteins identification or confidence scoring techniques. Finally, PQuad supports data validation and quality control by providing a variety of resolutions for huge amounts of data to reveal errors undetected by other methods.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436460,no,no,1485873340.117168
Supporting Distributed Collaborative Work with Multi-versioning,"The multi-version approach is useful in both synchronous and asynchronous groupware systems. This paper discusses the implementation of a real-time group editor that embodies our approaches and algorithms based on multi-versioning, which can preserve individual users' concurrent conflicting intentions in a consistent way. To highlight the distinct contributions of our work, we also present a detailed description of some novel features of the system",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019176,no,no,1485873340.117166
SAMOA -- A Visual Software Analytics Platform for Mobile Applications,"Mobile applications, also known as apps, are dedicated software systems that run on handheld devices, such as smartphones and tablet computers. The apps business has in a few years turned into a multi-billion dollar market. From a software engineering perspective apps represent a new phenomenon, and there is a need for tools and techniques to analyze apps. We present SAMOA, a visual web-based software analytics platform for mobile applications. It mines software repositories of apps and uses a set of visualization techniques to present the mined data. We describe SAMOA, detail the analyses it supports, and describe a methodology to understand apps from a structural and historical perspective. The website of SAMOA, containing the screen cast of the tool demo, is located at http://samoa.inf.usi.ch/about.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676936,no,no,1485873340.117164
The online prediction of the faults for integrated maintenance and reliability,"The objective of this paper is to realize analytic studies and applications in oriented engineering, especially in maintenance, reliability and security of the big technical installations, particularly for the nuclear domain. Software applications were developed to permit the online automatic computation of the reliability, maintenance, availability and predictive maintenance parameters. This paper presents a task of an integrated application that accomplishes structural and dynamic analysis computation, geometric simulation, software for monitoring on-line predictive maintenance for the installation meant to tritium elimination, all based on scientific methods.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588734,no,no,1485873339.85388
UCD-Griffin-MC2 submission summary,"This paper describes the approach and the visual analytics (VA) tool developed to solve the VAST 2014 Challenge: Mini-Challenge 2. For this challenge, a methodical approach was taken by analyzing each GAStech employees daily activities. Open source software, commercial application programming interfaces (APIs), and custom software were combined to create a very robust VA tool used in this challenge. The tool provides visualization and animation of the tracking data, an overview of credit card and loyalty card transactions, and on-demand details of the aforementioned data. Finally, a novel feature was implemented in the VA tool to insert missing location data for credit card and loyalty card transactions in realtime.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042561,no,no,1485873339.853879
Convergence of evolutionary biology and software engineering: Putting practice in action,"This paper presents a project in experiential learning where students put knowledge of software engineering processes into action in a multidisciplinary project combining computer science and biology. Visualization serves as a primary element to bind the concepts of the two disciplines. Students seeking to further their experience and strengthen their skills in software engineering may choose to complete their senior capstone course working on an ongoing project to construct a toolkit for visualization of phylogenies generated from Avida experimental data. Avida provides a complex computational environment in which the evolution of digital organisms is tracked and analyzed to help find answers to a wide range of research questions. Student projects involve extensions of existing analytic and visualization techniques, as well as the addition of new, often novel, techniques. Importantly, to be successful a visualization technique must be appropriate for the domain in which it is to be used, requiring students to also understand elements of biology. It is our premise that exposing computer science students to the convergence of these two disciplines will strengthen their ability to work at different levels of abstraction and develop new conceptual frameworks to address current and future challenges in hardware and software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684847,no,no,1485873339.853877
Case study: Visual analytics in software product assessments,"We present how a combination of static source code analysis, repository analysis, and visualization techniques has been used to effectively get and communicate insight in the development and project management problems of a large industrial code base. This study is an example of how visual analytics can be effectively applied to answer maintenance questions and support decision making in the software industry. We comment on the relevant findings during the study both in terms of used technique and applied methodology and outline the favorable factors that were essential in making this type of assessment successful within tight time and budget constraints.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336417,no,no,1485873339.853876
Shared memory aware MPSoC software deployment,"In this paper we present a novel approach for mapping interconnected software components onto cores of homogenous MPSoC architectures. The analytic mapping process considers shared memory communication as well as the routing algorithm controlling packet-based communication. The software components are mapped with the constraints of avoiding communication conflicts as well as access conflicts to shared memory resources. The core of the elaborated approach consists of an algorithm for software mapping which is inspired by force-directed scheduling from high-level synthesis. Experimental results show that the presented approach increases the overall system performance by 22% while reducing the average communication latency by 35%. For presenting the major advantages of the developed solution, we optimized an advanced driver assistance system on the Tilera TILEPro64 processor.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513802,no,no,1485873339.853874
TimeBench: A Data Model and Software Library for Visual Analytics of Time-Oriented Data,"Time-oriented data play an essential role in many Visual Analytics scenarios such as extracting medical insights from collections of electronic health records or identifying emerging problems and vulnerabilities in network traffic. However, many software libraries for Visual Analytics treat time as a flat numerical data type and insufficiently tackle the complexity of the time domain such as calendar granularities and intervals. Therefore, developers of advanced Visual Analytics designs need to implement temporal foundations in their application code over and over again. We present TimeBench, a software library that provides foundational data structures and algorithms for time-oriented data in Visual Analytics. Its expressiveness and developer accessibility have been evaluated through application examples demonstrating a variety of challenges with time-oriented data and long-term developer studies conducted in the scope of research and student projects.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634096,no,no,1485873339.853871
On Metrics-Driven Software Process,"Metrics can drive software processing, because they found the base of its quantizing management. There are two fundamental requirements in engineering: formal modeling and quantitative modeling. We must emphasize that metrics for software engineering is insufficient in quantification now. In order to improve software and software processing, the factors that affect schedule, cost and quality of software development should be measured. Metrics produces adjustable and iterative motions in software processing. Based on Jaynes' maximum entropy principle, this paper establishes a model to quantify the factors and introduces distance to compare the metric indicators. The authors propose that the metric estimation tree can be used and the nodes that stand for the software attributes in the tree can be marked with their corresponding evaluation values. Dynamic feedback in the software processing will be combined with AHP (analytic hierarchy process), and the project and process of development will be learned and analyzed entirely, concentratedly and dynamically.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4392660,no,no,1485873339.85387
Keynote 1: Visualization for Software Analytics by Margaret-Anne (Peggy) Storey,"The popularity of software visualization research over the past 30 years has led to innovative techniques that are now seeing widespread adoption by professional software practitioners. But this research has barely kept pace with some of the radical changes occurring in software engineering today. In this talk, I explore current trends in software engineering, including the prevalence of software ecosystems and software delivery as a service, and the emergence of the social coder within a participatory development culture. I will also discuss how the field of software analytics has matured and seeks to support practitioners in improving software quality, user experience and developer productivity through data-driven tasks. Finally, I suggest that software visualization should be playing a bigger role in these recent trends, emphasizing that interactive visualizations are poised to play a critical role in the field of software analytics.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980204,no,no,1485873339.853868
1st International workshop on data analysis patterns in software engineering (DAPSE 2013),"Data scientists in software engineering seek insight in data collected from software projects to improve software development. The demand for data scientists with domain knowledge in software development is growing rapidly and there is already a shortage of such data scientists. Data science is a skilled art with a steep learning curve. To shorten that learning curve, this workshop will collect best practices in form of data analysis patterns, that is, analyses of data that leads to meaningful conclusions and can be reused for comparable data. In the workshop we compiled a catalog of such patterns that will help experienced data scientists to better communicate about data analysis. The workshop was targeted at experienced data scientists and researchers and anyone interested in how to analyze data correctly and efficiently in a community accepted way.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606765,no,no,1485873339.633276
Dynamic Evaluation Model for the Prototype Information System Development Process,"The purpose of this paper is to promote the developing efficiency of information system (IS) and to make greatest possible effort to eliminate the information asymmetry between user and developer through constructing the dynamic evaluation model for prototype IS development process. Methods of fuzzy analytic hierarchy process (FAHP) and a new fuzzy matrices combination were employed. By constructing the hierarchy of IS development process, determining the indicators weights using FAHP and calculating the score and level the indicator belongs to, both developer and user can clearly figure out the main problem In accordance with the different IS developing stage, and reduce the lose to the minimum extent at development process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5576575,no,no,1485873339.633274
A closer look at note taking in the co-located collaborative visual analytics process,"This paper highlights the important role that record-keeping (i.e. taking notes and saving charts) plays in collaborative data analysis within the business domain. The discussion of record-keeping is based on observations from a user study in which co-located teams worked on collaborative visual analytics tasks using large interactive wall and tabletop displays. Part of our findings is a collaborative data analysis framework that encompasses note taking as one of the main activities. We observed that record-keeping was a critical activity within the analysis process. Based on our observations, we characterize notes according to their content, scope, and usage, and describe how they fit into a process of collaborative data analysis. We then discuss suggestions for the design of collaborative visual analytics tools.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652879,no,no,1485873339.633273
Applying the balanced score card in the team strategic performance management,"The traditional performance management which uses measure methods by financial indicators can only reflect the past situation not to judge the future development of the team and its profit ability. In the new circumstances, how to establish strategic performance management system and guide organization and personnel's behavior toward the strategic goal to avoid becoming obsolete in the future competition is placed an arduous task in front of the team managers. This article discusses the balanced score card' three aspects: dimension analysis, implementation, system support in the team strategic performance management application by model of grey analytic evaluation, with the purpose of enhancing the team to improve performance management which has an important realistic and profound significance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6010534,no,no,1485873339.633272
The Structure of Sale Teams' Cohesion: Confirmatory Factor Analysis,"The purpose of this study is to explore the dimension structure of sale teams' cohesion. Based on the relevant literature review and semi-structured interview study, an initial questionnaire of sale teams' cohesion was designed. Through pre-investigation, we obtained a 26-subject formal questionnaire, we surveyed 150 salesmen from 100 sales teams of Tianjin or Hebei province by random sampling, and we obtained 16 factors that affect the cohesion of sales team. We used group AHP (Analytic Hierarchy Process) to analyze the Matrix. And the result shows that the different factors play different roles to affect the cohesion.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363599,no,no,1485873339.63327
An integrated visualization on network events VAST 2011 Mini Challenge #2 Award: äóìOutstanding integrated overview displayäó,"To visualize security trends for the data set provided by the VAST 2011 Mini Challenge #2 a custom tool has been developed. Open source tools [1,2], web programming languages [4,7] and an open source database [3] has been used to work with the data and create a visualization for security log files containing network security trends. In this paper, the tools and methods used for the analysis are described. The methods include the log synchronization with different timezone and the development of heat maps and parallel coordinates charts. To develop the visualization, Processing and Canvas [4,7] was used.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102493,no,no,1485873339.633268
How Information Visualization Novices Construct Visualizations,"It remains challenging for information visualization novices to rapidly construct visualizations during exploratory data analysis. We conducted an exploratory laboratory study in which information visualization novices explored fictitious sales data by communicating visualization specifications to a human mediator, who rapidly constructed the visualizations using commercial visualization software. We found that three activities were central to the iterative visualization construction process: data attribute selection, visual template selection, and visual mapping specification. The major barriers faced by the participants were translating questions into data attributes, designing visual mappings, and interpreting the visualizations. Partial specification was common, and the participants used simple heuristics and preferred visualizations they were already familiar with, such as bar, line and pie charts. We derived abstract models from our observations that describe barriers in the data exploration process and uncovered how information visualization novices think about visualization specifications. Our findings support the need for tools that suggest potential visualizations and support iterative refinement, that provide explanations and help with learning, and that are tightly integrated into tool support for the overall visual analytics process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613431,no,no,1485873339.633267
Advanced analysis of dynamic neural control advisories for process optimization and parts maintenance,"This paper details an advanced set of analyses designed to drive specific process variable setpoint adjustments or maintenance actions required for cost effective process control using the Dynamic Neural Controllerä‹¢ (DNC) wafer-to-wafer advisories for semiconductor manufacturing advanced process control. The new analytic displays and metrics are illustrated using data obtained on a LAM 4520XL at STMicroelectronics as part of a SEMATECH SPIT beta test evaluation. The DNC represents a comprehensive modeling environment that uses as its input extensive process chamber information and history of the time since maintenance actions occurred. The DNC uses a neural network to predict multiple quality output metrics and a closed-loop risk-based optimization to maximize process quality performance while minimizing overall cost of tool operation and machine downtime. The software responds in an advisory mode on a wafer-to-wafer basis as to the optimal actions to be taken. In this paper, we present three specific instances of patterns arising during wafer processing over time that signal the process or equipment engineer to the need for corrective action: either a process setpoint adjustment or specific maintenance actions. Based on the controller's recommended corrective action set with the overall risk reduction predicted by such actions, a metric of corrective action ""urgency"" can be created. The tracking of this metric over time yields different pattern types that signify a quantified need for a specific type of corrective action. Three basic urgency patterns are found: 1. a pattern in a given maintenance action over time showing increasing urgency or ""risk reduction"" capability for the action; 2. a pattern in a process variable specific to a given recipe indicating a chronic request over time to only adjust the variable setpoint either above or below the current target; 3. a pattern in a process variable existing over all recipes processed through the chamber indicating chronic request to adjust the variable setpoint in either or both directions over time. This pattern is a pointer to the need for a maintenance action that is either corroborated by the urgency graph for that maintenance action, or if no such action has been previously take- n, a guide to the source of the equipment malfunction.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194514,no,no,1485873339.633265
"Leveraging the Crowd: How 48,000 Users Helped Improve Lync Performance","Performance is a critical component of customer satisfaction with network-based applications. Unfortunately, accurately evaluating the performance of collaborative software that operates in extremely heterogeneous environments is difficult with traditional techniques such as modeling workloads or testing in controlled environments. To evaluate performance of an application in the wild during development, the authors deployed early versions of the software, collecting performance data from application users for key usage scenarios. The analysis package they used produces visualizations to help development teams identify and prioritize performance issues by focusing on performance early in the development cycle, evaluating progress, identifying defects, and estimating timelines.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509371,no,no,1485873339.633262
How software engineering can benefit from traditional industries äóî A practical experience report (Invited industrial talk),"To be competitive in today's market, the IT industry faces many challenges in the development and maintenance of enterprise information systems. Engineering these largescaled systems efficiently requires making decisions about a number of issues. In addition, customers expectations imply continuous software delivery in predictable quality. The operation such systems demands for transparency of the software in regard to lifecycle, change and incident management as well as cost efficiency. Addressing these challenges, we learned how to benefit from traditional industries. Contrary to the fact that the IT business calls itself gladly an industry, the industrialization of software engineering in most cases moves on a rather modest level. Industrialization means not only to build a solution or product on top of managed and well-defined processes, but also to have access to structured information about the current conditions of manufacturing at any time. Comparably with test series and assembly lines of the automobile industry, each individual component and each step from the beginning of manufacturing up to the final product should be equipped with measuring points for quality and progress. Even one step further the product itself, after it has left the factory, should be able to continuously provide analytic data for diagnostic reasons. Information is automatically collected and builds the basic essentials for process control, optimization and continuous improvement of the software engineering process. This presentation shows by means of a practical experience report how AdNovum managed to build its software engineering based on a well-balanced system of processes, continuous measurement and control äóî as well as a healthy portion of pragmatism. We implemented an efficient and predictable software delivery pipeline based on five cornerstones that enables us to ship more than 1500 customer deliveries per year.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227249,no,no,1485873339.411831
FSBD: A Framework for Scheduling of Big Data Mining in Cloud Computing,"Cloud computing is seen as an emerging technology for big data mining and analytics. Cloud computing can provide data mining results in the form of a Software As a Service (SAS). Both performance and quality of mining are fundamentals criteria for the use of a data mining application provided by a Cloud computing environment. In this paper, we propose a Cloud computing framework, which is responsible to distribute and schedule a Cluster-Based data mining application and its data set. The main goal of our proposed framework for scheduling of Big Data Mining (FSBD) is to decrease the overall execution time of the application with minimum loss in mining quality. We consider the Cluster-based data mining technique as a pilot application for our framework. The results show an important speedup with a minimum loss in quality of mining. We obtained a ratio of 2 of the normalized actual makespan vis-a-vis the ideal makespan. The quality of mining scales well with the number of clusters and the increasing size of the dataset. The results are promising, encouraging the adoption of the framework by Cloud providers.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906823,no,no,1485873339.411829
A model and system for applying Lean Six sigma to agile software development using hybrid simulation,"Software quality control and quality assurance have close ties with predictability, speed/time, and cost of software development. Process improvement has essential impact on these factors that drive the quality of software project outcomes. While stochastic design and process improvement methodologies based on the Lean Six Sigma can greatly help with process design and improvement, software development processes are substantially different from the processes in the other disciplines such as manufacturing or service operations that produce same/similar product/services. It's not feasible to quantify software processes in a discrete manner that is required by the Six Sigma methodologies. The discrete simulation that is used in operations such as car manufacturing relies on the fact that system activities change state at discrete time points. However this cannot be applied to software development as the activities are not repetitive and they have time estimates at best. The continuous simulation approach lacks the discrete simulation advantage of identifying inefficiencies and improving the processes along the line. Then the discrete simulation has the shortcoming of detecting consequences of improvements late in the process. The model and system introduced in this paper applies Six Sigma methodologies to software processes using hybrid simulation. It uses the relatively detailed empirical data - which the lean software development and agile methodologies produce - to simulate future activities. Such predictions are used as the baseline measurement data to assess the actual results of the continuous improvement activities. The Monte Carlo simulation is used to eliminate dependency on assumption of a specific distribution function for software development activities. The System also includes a framework for collecting process data and creating the empirical knowledge base that optimizes simulation, analytics and data mining. The System collects empirical data on proce- s actors and uses them in simulation to provide estimations that incorporate the human factor that has substantial role in software processes. Over time the System effectively uses machine learning to improve estimation and in some cases to recommend actions to improve performance. The resulted data can be used not only for process improvement but also for evaluating impacts of factors such as outsourcing, geographical base cost, and time zone difference on the process quality.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918594,no,no,1485873339.411828
"Educational software engineering: Where software engineering, education, and gaming meet","We define and advocate the subfield of educational software engineering (i.e., software engineering for education), which develops software engineering technologies (e.g., software testing and analysis, software analytics) for general educational tasks, going beyond educational tasks for software engineering. In this subfield, gaming technologies often play an important role together with software engineering technologies. We expect that researchers in educational software engineering would be among key players in the education domain and in the coming age of Massive Open Online Courses (MOOCs). Educational software engineering can and will contribute significant solutions to address various critical challenges in education especially MOOCs such as automatic grading, intelligent tutoring, problem generation, and plagiarism detection. In this position paper, we define educational software engineering and illustrate Pex for Fun (in short as Pex4Fun), one of our recent examples on leveraging software engineering and gaming technologies to address educational tasks on teaching and learning programming and software engineering skills.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6632588,no,no,1485873339.411827
Statistical process control: analyzing space shuttle onboard software process,Demand for increased software process efficiency and effectiveness places measurement demands on the software engineering community beyond those traditionally practiced. Statistical- and process-thinking principles lead to the use of statistical process control (SPC) methods to determine the consistency and capability of the processes used to develop software. The authors use data and analysis from a collaborative effort between the Software Engineering Institute (a federally funded research and development center sponsored by the US Department of Defense) and the Space Shuttle Onboard Software Project as a vehicle to illustrate the analytic processes analysts frequently encounter when using SPC,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=854075,no,no,1485873339.411823
Experiences in Global Software Development - A Framework-Based Analysis of Distributed Product Development Projects,"Many authors have reported on various challenges and benefits encountered by teams engaged in global software development (GSD). Previous research has proposed a framework to structure these challenges and benefits within dimensions of distance and process. In this paper, the framework was used as an analytic device to investigate various projects performed by distributed teams in order to explore further the mechanisms used in industry both to overcome obstacles posed by distance and process challenges and also to exploit potential benefits enabled by global software development.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5196938,no,no,1485873339.411821
Leveraging Process-Mining Techniques,"Semi-structured processes are data-driven, human-centric, flexible processes whose execution between instances can vary dramatically. Due to their unpredictability and data-driven nature, it's becoming increasingly important to mine traces of events collected from these processes. This enables the extraction of mined process models that could help users handle new process instances. Process-mining techniques can help facilitate this goal, but it can be daunting for users new to process-aware analytics to sift through the literature and available software to determine which process-mining algorithm to use. The authors compare five process-mining algorithms and present a decision tree to help readers determine which mining algorithm to use for a specific problem. Semi-structured processes, however, present challenges that these mining techniques don't address. So, the authors also identify three key characteristics of semi-structured processes and the mining challenges they present, highlighting a selection of emerging mining approaches that can address these challenges.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6279446,no,no,1485873339.411819
Mining student repositories to gain learning analytics. An experience report,"Engineering students often have to deliver small computer programs in many engineering courses. Instructors have to evaluate these assignments according to the learning goals and their quality, but ensure as well that there is no plagiarism. In this paper, we report the experience of using mining software repositories techniques in a multimedia networks course where students have to submit several software programs. We show how we have proceeded, the tools that we have used and provide some useful links and ideas that other lecturers may use.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6530267,no,no,1485873339.411817
Establishing the expert decision-making strategy to improve the TFT-LCD quality,"At present, TFT-LCD industry demand and expand the development budget, the experts in collaboration design systems leadership and raise the alarm management system (AMS) quality. This study uses the analytic network process (ANP) and cause-effect grey relational analysis (CEGRA) model to establish the expert decision-making strategy to solve he problems in the TFT-LCD industry. Finally, the expert decision-making strategy approach to rank the collaboration design systems' quality of the TFT-LCD industry performance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967473,no,no,1485873339.244747
VAST 2012 Mini-Challenge 2: Chart- and Matrix-based approach to network operations forensics,"We report the approach and results on the VAST 2012 MiniChallenge 2: Bank of Money Regional Office Network Operations Forensics. Using commercial data mining, visualization and database software such as KNIME, Tableau and MySQL as well as a custom-written source vs. destination IP pixel matrix, our team of students identified suspicious IRC traffic, an attack on the firewall, a drop in the firewall connections, an attempt for sensitive information exchange and a possible Distributed Denial-of-Service attack executed partly from a host within the bank network.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400513,no,no,1485873339.244746
KWIVER: An open source cross-platform video exploitation framework,"We introduce KWIVER, a cross-platform video exploitation framework that Kitware has begun releasing as open source. Kitware is utilizing a multi-tiered open-source approach to reach as wide an audience as possible. Kitware's government-funded efforts to develop critical defense technology will be released back to the defense community via Forge.mil, a government open source repository. Infrastructure, algorithms, and systems without release restrictions will be provided to the larger video analytics community via kwiver.org and GitHub. Our goal is to provide a video analytics technology baseline for repeatable and reproducible experiments and to serve as a framework for the development of computer vision and machine learning systems. We hope that KWIVER will provide a focal point for collaboration and contributions from groups across the community.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7041910,no,no,1485873339.244745
Modelling of SACK TCP and application to the HTTP file transfer environment,"It is known that analytic modelling for TCP latency is a non trivial task. Recently, some significant progress has been made, such as the comprehensive result by Sikdar et al. However, models similar to these often rely on trial and error methods such as ""data fitting"". This can lead to a very limited scope for the resulting model and also large estimation error. In this paper, we propose improvements to Sikdar's SACK TCP model. A new delayed acknowledgement slow start model is developed that is analytically derived from the slow start algorithm which provides a novel mechanism to model the relationship between RTT and the delayed acknowledgement timer. We introduce a simple mechanism to include time taken to send an HTTP get request to broaden the scope of our SACK TCP model to Website file transfer. Simulation and live Internet experimentation has validated our scheme",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1550913,no,no,1485873339.244743
Social scheduler: a proposal of collaborative personal task management,"We propose a collaborative approach for personal task management which is modeled as an integration of alliance and human-in-the-loop model. Alliance model is based on information sharing and collaboration of several persons. They disclose their task condition and maintain to be updatable by their friends. To avoid privacy issues we propose emergent group discovery algorithm to control the level of disclosure. Human-in-the-loop model consists of three subsystems to support decision-making activities. Visualizer indicates the attributes associated with each task such as the deadline, the subjective priority, and the workload, which are determined by the user. Optimizer generates executable schedules from these tasks by active scheduler and multiobjective genetic algorithm. Recommender evaluates these alternatives by analytic hierarchy process. We implement client/server system called social scheduler on cell-phones environment. We remark the advantages of our approach with an experiment.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241292,no,no,1485873339.244742
In-Memory Database Support for Source Code Search and Analytics,"Software engineers are coerced to deal with a large amount of information about source code. Appropriate tools could assist to handle it, but existing tools are not capable of processing and presenting such a large amount of information sufficiently. With the advent of in-memory column-oriented databases the performance of some data-intensive applications could be significantly improved. This has resulted in a completely new user experience of those applications and enabled new use-cases. This PhD thesis investigates the applicability of in-memory column-oriented databases for supporting daily software engineering activities. The major research question addressed in this thesis is as follows: does in-memory column-oriented database technology provide the necessary performance advantages for working interactively with large amounts of fine-grained structural information about source code? To investigate this research question two scenarios have been selected that particularly suffer from low performance. The first selected scenario is source code search. Existing source code repositories contain a large amount of structural data. Interface definitions, abstract syntax trees, and call graphs are examples of such structural data. Existing tools have solved the performance problems either by reducing the amount of data because of using a coarse-grained representation, or by preparing answers to developers' questions in advance, or by reducing the scope of search. All currently existing alternatives result in the loss of developers' productivity. The second scenario is source code analytics. To complete reverse engineering tasks software engineers often are required to analyze a number of atomic facts that have been extracted from source code. Examples of such atomic facts are occurrences of certain syntactic patterns in code, software product metrics or violations of development guidelines. Each fact typically has several characteristics, such as the type of the fact,- - the location in code where found, and some attributes. Particularly, analysis of large software systems requires the ability to process a large amount of such facts efficiently. During industrial experiments conducted for this thesis it was evidenced that in-memory technology provides performance gains that improve developers' productivity and enable scenarios previously not possible. This thesis overlaps both software engineering and database technology. From the viewpoint of software engineering, it seeks to find a way to support developers in dealing with a large amount of structural data. From the viewpoint of database technology, source code search and analytics are domains for studying fundamental issues of storing and querying structural data.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079871,no,no,1485873339.244739
Using Analytic Network Process to analyze influencing factors of project complexity,"The management of project complexity has become an important part in the project management, being critical to the success of the large complex project. According to literature review and questionnaire survey, the Analytical Network Process (ANP) method is used to measure the influencing factors of project complexity and Super Decisions (SD) software is used to calculate weights of influencing factors, so as to identify the key influencing factors to manage projects better. Results found that cross-organizational interdependence, multiple stakeholders, number of organizational structure hierarchy, project team's trust and diversity of technology are the five key factors which have the biggest influence on the project complexity. This research provides the scientific support for the practice of project management, which has theory direction significance for mega and complex project management.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6414413,no,no,1485873339.244738
A new soft-computing based framework for project management using game theory,"Software project success or failure depends on the ineffective software project management. Success or failure of any project can be attributed incorrect handling of one or more project variables: people, proper technology, proper project scheduling and selection. Among these attributes proper project selection is one of the most vital part of software project management. There exist many uncertainties in project management and current software engineering techniques are unable to eliminate them. So there is huge scope for developing. The current researchers have developed a unique model which is capable to take decision on the field of software project selection. This model has two embedded sub models namely fuzzy AHP(Analytic Hierarchy Process) and strategic game model. Here in the first case experts opinions are considered under fuzzy environment and in the second case, different decisions makers act as players in the game module. Different criteria are taken into consideration for choosing optimal strategy of the players. An elaborated case study is also analyzed for testing the output of the system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6422193,no,no,1485873339.244736
A novel game theoretic algorithm for project selection under fuzziness,"Software project success or failure depends on the ineffective software project management. Success or failure of any project can be attributed incorrect handling of one or more project variables: people, proper technology, proper project scheduling and selection. Among these attributes proper project selection is one of the most vital part of software project management. There exist many uncertainties in project management and current software engineering techniques are unable to eliminate them. So there is huge scope for developing. The current researchers have developed a unique model which is capable to take decision on the field of software project selection. This model has two embedded sub models namely fuzzy AHP (Analytic Hierarchy Process) and strategic game model. Here in the first case experts opinions are considered under fuzzy environment and in the second case, different decisions makers act as players in the game module. Different criteria are taken into consideration for choosing optimal strategy of the players. An elaborated case study is also analyzed for testing the output of the system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481846,no,no,1485873339.244734
Evaluating conservation voltage reduction: An application of GridLAB-D: An open source software package,"Conservation Voltage Reduction (CVR) is the reduction of energy consumption resulting from a reduction of the service voltage. While there have been numerous CVR deployments in North America, there has been little substantive analytic analysis of the effect; the majority of the published results are based on empirical field measurements. Due to the lack of analytic study, it is difficult to determine the impacts of CVR outside of sites that have conducted demonstration projects. This panel paper will examine a framework for the analysis of CVR using the open source software package GridLAB-D. An open source simulation environment is used to highlight the effectiveness of open source software programs and their ability to be used for evaluating multi-disciplinary smart grid technologies.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6039467,no,no,1485873339.090266
Extracting Dependencies from Software Changes: An Industry Experience Report,"Retrieving and analyzing information from software repositories and detecting dependencies are important tasks supporting software evolution. Dependency information is used for change impact analysis, defect prediction as well as cohesion and coupling measurement. In this paper we report our experience from extracting dependency information from the change history of a commercial software system. We analyzed the software system's evolution of about six years, from the start of development to the transition to product releases and maintenance. Analyzing the co-evolution of software artifacts allows detecting logical dependencies between system parts implemented with heterogeneous technologies as well as between different types of development artifacts such as source code, data models or documentation. However, the quality of the extracted dependencies relies on established development practices and conformance to a defined change process. In this paper we indicate resulting limitations and recommend further processing and filtering steps to prepare the dependency data for subsequent analysis and measurement activities.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000096,no,no,1485873339.090264
Temporal Data Mining in Dynamic Feature Spaces,"Many interesting real-world applications for temporal data mining are hindered by concept drift. One particular form of concept drift is characterized by changes to the underlying feature space. Seemingly little has been done in this area. This paper presents FAE, an incremental ensemble approach to mining data subject to such concept drift. Empirical results on large data streams demonstrate promise.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053168,no,no,1485873339.090263
"On the path to sustainable, scalable, and energy-efficient data analytics: Challenges, promises, and future directions","As scientific data is reaching exascale, scalable and energy efficient data analytics is quickly becoming a top notch priority. Yet, a sustainable solution to this problem is hampered by a number of technical challenges that get exacerbated with the emerging hardware and software technology trends. In this paper, we present a number of recently created äóìsecret saucesäó that promise to address some of these challenges. We discuss transformative approaches to efficient data reduction, analytics-driven query processing, scalable analytical kernels, approximate analytics, among others. We propose a number of future directions that could be pursued on the path to sustainable data analytics at scale.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6322265,no,no,1485873339.090261
Notice of Retraction<BR>Research and Implementation of AHP-Based Method Base - Model Base Application of Hierarchical Model,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>This paper introduces that the representation and storage of hierarchical model via a database table, achieves the dynamic generation and management of model. It main achieves model base based on AHP. That addresses a number of unstructured or semi-structured problems such as assessment and prediction in the modeling. Focusing on the creation of their corresponding AHP-based method base is to solve practical problems. Experiments show that this method base can be well integrated with model base. So it will be applied in common AHP cases.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5383609,no,no,1485873339.09026
Stationary and adaptive replication approach to data availability in structured peer-to-peer overlay networks,"Structured peer-to-peer overlay networks offer a novel infrastructure for large-scale applications. However, replications aimed for high data availability often have too much maintenance cost. In this paper we propose a simple and efficient replication strategy over low available peers, which has stationary replica locations and variation-tolerant data recovery mechanism, so that it can greatly reduce the costs for maintaining replicas, esp. in the circumstance of poor peer availabilities. We also present an analytic model for data availability and replication maintenance costs. Based on our model, we make adaptive mechanism for replicas checking, which can further reduce the cost and guarantee data availability in variable circumstance and conditions for living replicas. The simulation results proved the contribution of our approach and model.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1266201,no,no,1485873339.090258
Quality assessment model for wheat storage warehouse using Analytic Hierarchy Process and BP Neural Network,In India the quality of the wheat storage warehouse is assessed manually by officials and there is no scientific model present for the same. In this paper we have developed a model for the quality assessment using the Analytical Hierarchy Process and the Back Propagation Neural Network. The simulations are carried out in MATLAB software and the results are deduced thereafter. The results and the correlation between actual results and the deduced results show the validity of the developed model. It provides an effective way to assess the quality in short time and with a prescribed scientific model.,2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014734,no,no,1485873339.090257
Adaptive multivariate rational data fitting with applications in electromagnetics,"The behavior of certain electromagnetic devices or components can be simulated with great detail in software. A drawback of these simulation models is that they are very time consuming. Since the accuracy required for the computational electromagnetic analysis is usually only 2-3 significant digits, an approximate analytic model is sometimes used instead, as noted by Lehmensiek and Meyer in 2001. The most complex model we consider here is a multivariate rational function, which interpolates a number of simulation data. The interpolating rational function is constructed in such a way that it minimizes both the truncation error and the number of simulation data since each evaluation of the simulation model is computationally costly.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1629071,no,no,1485873339.090255
Evaluation model of TPL provider of agricultural products basing on Analytic Network Process,"Basing on the present situation and exiting problems, and considering the basic characteristics such as independence, seasonality and regionalism of agricultural logistics in China, this paper set up an evaluation index system, and then construct a multi-attribute comprehensive decision model on Third-Part Logistics (TPL) provider of agricultural products. Finally, with the application of Super Decisions Software, this research offers a case study with ANP method to verify the correctness of the model, and evaluates some different programs. The ultimate results indicate that the model is efficacious on the evaluation of TPL providers based on the practical situation of the TPL enterprises.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6703534,no,no,1485873339.090254
Model-driven software design for smart grid data analytics,"Practical data analytics for the smart grid requires a software platform which enables the distribution companies to better correlate the projects, improve understating of system requirements and simplify system design by decomposing its complexity and large-scale data. This paper proposes the model-driven software design (MDSD) to managing and optimizing the smart grid data. It defines a language for visualizing, specifying, analyzing, and documenting the distributed object-oriented data. An MDSD experience is reported that employs three software environments interfaced with each other in order to create a chain of the desired methods for an energy efficiency program. The aim is to transform the data into actionable decisions accessible and understandable by distribution companies by taking software development into a higher level of abstraction. As a pilot project, a comprehensive data logging were performed for an MV/LV distribution system. Subsequently, the technical losses were studied using the developed software package. A Java-based GUI, MATLAB/Simulink, and an embedded C-based data management and calculation module are used in the proposed MDSD.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6683280,no,no,1485873339.090251
QoS-Aware Service Selection Using QDG for B2B Collaboration,"Collaboration among enterprises through Web service has become a hot topic. Before the collaboration, how to select the most appropriate enterprise to collaborate with, from a set of enterprise candidates that provide similar functions, is an important issue. Existing work focus on proposing evaluation rules, and aggregating these rules to evaluate a service, where subjectiveness is usually involved. In this paper, we propose to utilize í‰ŒËserve, be servedí‰ŒË relationship to evaluate the quality of services. In more detail, we use quality dependency graph (QDG) method model the relationship among enterprises, and then, by traveling the built QDG, an analytic hierarchy process (AHP) model is used to calculate the evaluation result of each candidate organization. Our method provides a more objective way for collaboration on enterprise level.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724337,no,no,1485873338.938893
Heterogeneous Metric Learning with Content-Based Regularization for Software Artifact Retrieval,"The problem of software artifact retrieval has the goal to effectively locate software artifacts, such as a piece of source code, in a large code repository. This problem has been traditionally addressed through the textual query. In other words, information retrieval techniques will be exploited based on the textual similarity between queries and textual representation of software artifacts, which is generated by collecting words from comments, identifiers, and descriptions of programs. However, in addition to these semantic information, there are rich information embedded in source codes themselves. These source codes, if analyzed properly, can be a rich source for enhancing the efforts of software artifact retrieval. To this end, in this paper, we develop a feature extraction method on source codes. Specifically, this method can capture both the inherent information in the source codes and the semantic information hidden in the comments, descriptions, and identifiers of the source codes. Moreover, we design a heterogeneous metric learning approach, which allows to integrate code features and text features into the same latent semantic space. This, in turn, can help to measure the artifact similarity by exploiting the joint power of both code and text features. Finally, extensive experiments on real-world data show that the proposed method can help to improve the performances of software artifact retrieval with a significant margin.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023378,no,no,1485873338.938891
Combining LISREL and Bayesian network to predict tourism loyalty,"This study proposes an analytic approach that combines LISREL and Bayesian networks (BN) to examine factors influencing tourism loyalty and predict a touristpsilas loyalty level. LISREL is used to verify the hypothesized relationships proposed in the research model. Subsequently, the supported relationships are used as the BN network structure for prediction. 452 valid samples were collected from tourists with the tour experience of the Toyugi hot spring resort, Taiwan. Compared with other prediction methods, our approach yielded better results than those of back-propagation neural networks (BPN) or classification and regression trees (CART) for 10-fold cross-validation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4634220,no,no,1485873338.93889
Modelling the performance of CORBA using layered queueing networks,"One-of the typical features of distributed systems is the heterogeneity of its components (e.g. geographical spreading and different platform architectures), leading to interoperability issues. Many of these are handled by generic middleware-based solutions. We present an analytic model of the impact of using such middleware on the overall system performance. Specifically, a layered queueing network is described that models a client/server system using CORBA as a middleware system offering location transparency. The response times estimated from the model are compared to the measured response times for a growing number of clients, in order to assess the accuracy of the model and the values of the parameters in the model. This model can then be used for designing a distributed application, before the entire system is installed or even fully implemented.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231576,no,no,1485873338.938888
Entity-based collaboration tools for intelligence analysis,"Software tools that make it easier for analysts to collaborate as a natural part of their work will lead to better analysis that is informed by more perspectives. We are interested to know if software tools can be designed that support collaboration even as they allow analysts to find documents and organize information (including evidence, schemas, and hypotheses). We have modified the Entity Workspace system, described previously, to test such designs. We have evaluated the resulting design in both a laboratory study and a study where it is situated with an analysis team. In both cases, effects on collaboration appear to be positive. Key aspects of the design include an evidence notebook optimized for organizing entities (rather than text characters), information structures that can be collapsed and expanded, visualization of evidence that emphasizes events and documents (rather than emphasizing the entity graph), and a notification system that finds entities of mutual interest to multiple analysts.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677362,no,no,1485873338.938887
An internally replicated quasi-experimental comparison of checklist and perspective based reading of code documents,"The basic premise of software inspections is that they detect and remove defects before they propagate to subsequent development phases where their detection and correction cost escalates. To exploit their full potential, software inspections must call for a close and strict examination of the inspected artifact. For this, reading techniques for defect detection may be helpful since these techniques tell inspection participants what to look for and, more importantly, how to scrutinize a software artifact in a systematic manner. Recent research efforts investigated the benefits of scenario-based reading techniques. A major finding has been that these techniques help inspection teams find more defects than existing state-of-the-practice approaches, such as, ad-hoc or checklist-based reading (CBR). We experimentally compare one scenario-based reading technique, namely, perspective-based reading (PBR), for defect detection in code documents with the more traditional CBR approach. The comparison was performed in a series of three studies, as a quasi experiment and two internal replications, with a total of 60 professional software developers at Bosch Telecom GmbH. Meta-analytic techniques were applied to analyze the data",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922713,no,no,1485873338.938885
Domain-driven competence assessment in virtual learning environments. Application to planning and time management skills,"The Learning Management Systems provide a set of facilities for the lecturer to manage his courses. Unfortunately, they have limitations when it comes to assessing generic skills. In most of them, every activity is assessable but just with a simple grade and there is not a direct link between activities and generic skills. In this work we present two alternatives to solve this issue: an assisted method based on a Model-driven architecture approach and a Rest Web service that facilitates the assessment of generic skills. We apply both approaches to a case study consisting in a Moodle-based course where we assess the ability of plan and manage time of each student. Results show that the approaches are complementary, the Web service provides more detailed formative feedback, but the Model-driven approach seems more scalable for courses with a high number of students, where it is more difficult to assess their generic skills.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017716,no,no,1485873338.938883
Network Worm Propagating Model Based on Network Topology Unit,"The existing analytic models on network worm propagation rarely consider the effect of topology structure, and it usually considers the probability and spread parameter as fixed values. They are not coinciding with real situation. Here in the paper, a microstructure based propagation model is proposed to inflect the variability with time. In this model, many probability factors are considered, for example the connecting probability from one node to another node, one node's immune probability, which make the model more general and suitable for simulation.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5371035,no,no,1485873338.938882
Prediction of mean arterial blood pressure with linear stochastic models,"A model-based approach that integrates known portion of the cardiovascular system and unknown portion through a parameter estimation to predict evolution of the mean arterial pressure is considered. The unknown portion corresponds to the neural portion that acts like a controller that takes corrective actions to regulate the arterial blood pressure at a constant level. The input to the neural part is the arterial pressure and output is the sympathetic nerve activity. In this model, heart rate is considered a proxy for sympathetic nerve activity. The neural portion is modeled as a linear discrete-time system with random coefficients. The performance of the model is tested on a case study of acute hypotensive episodes (AHEs) on PhysioNet data. TPRs and FPRs improve as more data becomes available during estimation period.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6090161,no,no,1485873338.938881
Determining the Proper Number and Price of Software Licenses,"Software houses sell their products by transferring usage licenses of various software components to the customers. Depending on the kind of software, there are several different license types that allow controlled access of services. The two most popular types are the fixed license, which gives access rights for an identified workstation, and the floating license, which restricts the number of simultaneous users to a certain bound. The latter of these types is advantageous when the users do not demand full-time services and occasional lack of access is bearable. The problem of deciding the number of floating licenses is studied in the present paper. Based on the expected usage profile of the software, we calculate the minimal number of licenses that guarantees that the customers get service better than a given lower bound. The problem is studied by using certain queuing models, known as the Erlang toss system, the Erlang delay system, and the Engset model. None of these analytic models consider, however, the transient period that we analyze by means of simulation and by the so-called modified offered load approximation. We also give simple formulas presenting how the number of software licenses needed to keep the probability of nonaccess below a given blocking level grows as a function of the offered load, which is the proportion of the time used in the case that all requests were successful. Results of the study may be used for setting license prices and for determining the proper number of licenses.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4160969,no,no,1485873338.938878
Risk analysis in project of software development,"The Project Management in the area of IT is becoming hot topic now. The risk problems in IT Projects, especially in software projects become more and more concentration in software project management in China. This paper analyze systematically the characteristics of project management for software itself, then the paper analyses the risk of software development project in following two aspects: one for owners another for contractors. To owners the paper identify and analyze the risk in software development projects according to life cycle of a project. To contractors the paper identifies and assess the risk of project management in software development, and on the bases of investigating of software development project in the IT enterprises of China some conclusions are obtained with the method of AHP.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1252234,no,no,1485873338.800531
A chemical reaction hysteresis model for magnetic materials,"The authors present a new static hysteresis model for magnetic materials based on physical meaning. The idea is original: electronic transformation of the material is compared to a chemical reaction. Then a simple analytic formulation of the B(H) law for GO iron sheet in the rolling direction is obtained. Results are compared with measurements. The model is as simple as the Potter one, its parameters can be easily determined and its accuracy could compete with the Preisach model. Finally an easy computation by finite element software is gained",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=877662,no,no,1485873338.80053
ConfigChecker: A tool for comprehensive security configuration analytics,"Recent studies show that configurations of network access control is one of the most complex and error prone network management tasks. For this reason, network misconfiguration becomes the main source for network unreachablility and vulnerability problems. In this paper, we present a novel approach that models the global end-to-end behavior of access control configurations of the entire network including routers, IPSec, firewalls, and NAT for unicast and multicast packets. Our model represents the network as a state machine where the packet header and location determines the state. The transitions in this model are determined by packet header information, packet location, and policy semantics for the devices being modeled. We encode the semantics of access control policies with Boolean functions using binary decision diagrams (BDDs). We then use computation tree logic (CTL) and symbolic model checking to investigate all future and past states of this packet in the network and verify network reachability and security requirements. Thus, our contributions in this work is the global encoding for network configurations that allows for general reachability and security property-based verification using CTL model checking. We have implemented our approach in a tool called ConfigChecker. While evaluating ConfigChecker, we modeled and verified network configurations with thousands of devices and millions of configuration rules, thus demonstrating the scalability of this approach. We also present a SCAP-based tool on top of ConfigChecker that integrates host and network configuration compliance checking in one model and allows for executing comprehensive analysis queries in order to verify security and risk requirements across the end-to-end network as a single system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111667,no,no,1485873338.800528
Data-driven diffusion modeling to examine deterrence,"The combination of social network extraction from texts, network analytics to identify key actors, and then simulation to assess alternative interventions in terms of their impact on the network is a powerful approach for supporting crisis de-escalation activities. In this paper, we describe how researchers used this approach as part of a scenario-driven modeling effort. We demonstrate the strength of going from data-to-model and the advantages of data-driven simulation. We conclude with a discussion of the limitations of this approach for the chosen policy domain and our anticipated future steps.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004651,no,no,1485873338.800527
Abstract Syntax Trees - and their Role in Model Driven Software Development,Abstract syntax trees (ASTs) are known from compiler construction where they build the intermediate data format which is passed from the analytic front-end to the synthetic back-end. In model driven software development ASTs are used as a model of the source code. The object management group (OMG) has issued a request for proposals for AST models. Various levels of abstraction can be introduced. ASTs can be used for program analysis and for program transformation. In this paper we present an eclipse based representation framework for ASTs.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299919,no,no,1485873338.800525
Comparison of Software Reliability Assessment Methods for Open Source Software,"IT (information technology) advanced with steady steps from 1970's is essential in our daily life. As the results of the advances in high-speed data-transfer network technology, software development environment has been changing into new development paradigm. In this paper, we propose software reliability assessment methods for concurrent distributed system development by using the analytic hierarchy process. Also, we make a comparison between the inflection S-shaped software reliability growth model and the other models based on a nonhomogeneous Poisson process applied to reliability assessment of the entire system composed of several software components. Moreover, we analyze actual software fault count data to show numerical examples of software reliability assessment for the open source project. Furthermore, we investigate an efficient software reliability assessment method for the actual open source system development",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524356,no,no,1485873338.800524
Informing development decisions: From data to information,"Software engineers generate vast quantities of development artifacts such as source code, bug reports, test cases, usage logs, etc., as they create and maintain their projects. The information contained in these artifacts could provide valuable insights into the software quality and adoption, as well as development process. However, very little of it is available in the way that is immediately useful to various stakeholders. This research aims to extract and analyze data from software repositories to provide software practitioners with up-to-date and insightful information that can support informed decisions related to the business, management, design, or development of software systems. This data-centric decision-making is known as analytics. In particular, we demonstrate that by employing software development analytics, we can help developers make informed decisions around user adoption of a software project, code review process, as well as improve developers' awareness of their working context.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606729,no,no,1485873338.800522
Adoption of Free Libre Open Source Software (FLOSS): A Risk Management Perspective,"Free Libre Open Source Software (FLOSS) has become a strategic asset in software development, and open source communities behind FLOSS are a key player in the field. The analysis of open source community dynamics is a key capability in risk management practices focused on the integration of FLOSS in all types of organizations. We are conducting research in developing methodologies for managing risks of FLOSS adoption and deployment in various application domains. This paper is about the ability to systematically capture, filter, analyze, reason about, and build theories upon, the behavior of an open source community in combination with the structured elicitation of expert opinions on potential organizational business risk. The novel methodology presented here blends together qualitative and quantitative information as part of a wider analytics platform. The approach combines big data analytics with automatic scripting of scenarios that permits experts to assess risk indicators and business risks in focused tactical and strategic workshops. These workshops generate data that is used to construct Bayesian networks that map data from community risk drivers into statistical distributions that are feeding the platform risk management dashboard. A special feature of this model is that the dynamics of an open source community are tracked using social network metrics that capture the structure of unstructured chat data. The method is illustrated with a running example based on experience gained in implementing our approach in an academic smart environment setting including Mood bile, a Mobile Learning for Moodle (www.moodbile.org). This example is the first in a series of planned experiences in the domain of smart environments with the ultimate goal of deriving a complete risk model in that field.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6899215,no,no,1485873338.80052
Early effort estimation by AHP: A case study of project metrics in small organizations,"Project effort estimation in the early phase of software development is a significant activity for every software organization to improve the development process. Small organizations need some simple and easily used method to estimate effort because of insufficient resources including human and finance. Analytic Hierarchy Process (AHP), which integrates qualitative and subjective approach with quantitative and objective approach, can meet the need of small organizations. We use the data of three real projects to demonstrate the application of AHP for early project effort estimation. By the analysis of the estimation results, we point that AHP can not only predict the project effort in the early phase of development, but also help managers to find the principal factors that contribute to the project effort in their organizations. In summary, AHP is appropriate for early project effort estimation in a small organization.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272636,no,no,1485873338.800519
Understanding How Companies Interact with Free Software Communities,"When free, open source software development communities work with companies that use their output, it's especially important for both parties to understand how this collaboration is performing. The use of data analytics techniques on software development repositories can improve factual knowledge about performance metrics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6560081,no,no,1485873338.800517
Analytic Model for Web Anomalies Classification,"In this paper, an analytic technique is proposed to improve the dynamic Web application quality and reliability. The technique integrates orthogonal defect classification (ODC), and Markov chain to classify as well as analyze the collective view of Web errors. The error collective view will be built from access logs and defect data. This classification technique will enable viewing the Web errors in page, path, and application contexts. This technique will help in developing reliable Web applications that benefit from the understanding of Web anomalies and past issues. The preliminary results of applying this approach to a case study from telecommunications industry are included to demonstrate its' viability.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404772,no,no,1485873338.635571
Teaching and Training for Software Analytics,"Software analytics is to enable software practitioners to perform data exploration and analysis in order to obtain insightful and actionable information for data-driven tasks around software and services. When applying analytic technologies in practice of software analytics, one should incorporate (1) a broad spectrum of domain knowledge and expertise, e.g., management, machine learning, large-scale data processing and computing, and information visualization; and (2) investigate how practitioners take actions on the produced information, and provide effective support for such information-based action taking. This tutorial instructs materials to equip participants with skills and knowledge of conducting software analytics along with teaching and training students and practitioners for software analytics in university or industrial settings.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245015,no,no,1485873338.63557
An Insight-Based Longitudinal Study of Visual Analytics,"Visualization tools are typically evaluated in controlled studies that observe the short-term usage of these tools by participants on preselected data sets and benchmark tasks. Though such studies provide useful suggestions, they miss the long-term usage of the tools. A longitudinal study of a bioinformatics data set analysis is reported here. The main focus of this work is to capture the entire analysts process that an analyst goes through from a raw data set to the insights sought from the data. The study provides interesting observations about the use of visual representations and interaction mechanisms provided by the tools, and also about the process of insight generation in general. This deepens our understanding of visual analytics, guides visualization developers in creating more effective visualization tools in terms of user requirements, and guides evaluators in designing future studies that are more representative of insights sought by users from their data sets",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703371,no,no,1485873338.635569
Inäš„bug: Visual analytics of bug repositories,"Bug tracking systems are used to track and store the defects reported during the life of software projects. The underlying repositories represent a valuable source of information used for example for defect prediction and program comprehension. However, bug tracking systems present the actual bugs essentially in textual form, which is not only cumbersome to navigate, but also hinders the understanding of the intricate pieces of information that revolve around software bugs. We present in*Bug, a web-based visual analytics platform to navigate and inspect bug repositories. in*Bug provides several interactive views to understand detailed information about the bugs and the people that report them. The tool can be downloaded at http://inbug.inf.usi.ch",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6747208,no,no,1485873338.635566
EdiFlow: Data-intensive interactive workflows for visual analytics,"Visual analytics aims at combining interactive data visualization with data analysis tasks. Given the explosion in volume and complexity of scientific data, e.g., associated to biological or physical processes or social networks, visual analytics is called to play an important role in scientific data management. Most visual analytics platforms, however, are memory-based, and are therefore limited in the volume of data handled. More over, the integration of each new algorithm (e.g. for clustering) requires integrating it by hand into the platform. Finally, they lack the capability to define and deploy well-structured processes where users with different roles interact in a coordinated way sharing the same data and possibly the same visualizations. We have designed and implemented EdiFlow, a workflow platform for visual analytics applications. EdiFlow uses a simple structured process model, and is backed by a persistent database, storing both process information and process instance data. EdiFlow processes provide the usual process features (roles, structured control) and may integrate visual analytics tasks as activities. We present its architecture, deployment on a sample application, and main technical challenges involved.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5767914,no,no,1485873338.635563
Watershed Reanalysis: Towards a National Strategy for Model-Data Integration,"Reanalysis or retrospective analysis is the process of re-analyzing and assimilating climate and weather observations with the current modeling context. Reanalysis is an objective, quantitative method of synthesizing all sources of information (historical and real-time observations) within a unified framework. In this context, we propose a prototype for automated and virtualized web services software using national data products for climate reanalysis, soils, geology, terrain and land cover for the purpose of water resource simulation, prediction, data assimilation, calibration and archival. The prototype for model-data integration focuses on creating tools for fast data storage from selected national databases, as well as the computational resources necessary for a dynamic, distributed watershed prediction anywhere in the continental US. In the future implementation of virtualized services will benefit from the development of a cloud cyber infrastructure as the prototype evolves to data and model intensive computation for continental scale water resource predictions.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130732,no,no,1485873338.635561
The effect of aggregation and defuzzification method selection on the risk level calculation,"In this paper a fuzzy logic-based hierarchical multilevel risk calculation model will be introduced with different model parameters. On each occasion when a fuzzy-based simulation model is constructed, the appropriate aggregation and defuzzification method must be chosen. It is very difficult because it cannot be said generally, which is the best method, its depends on the current application. The model presented in the paper is a model for risk calculation of physical exercise, and it was constructed in Simulink - Fuzzy Logic Toolbox environment with Mamdani-type fuzzy evaluation and different aggregation and defuzzification operators. The test was performed for several typical groups of the patients. The results are compared with a previously implemented Analytic Hierarchy Process with Fuzzy Comprehensive Evaluation based model with similar purposes. The result of the comparison has been analyzed and the best methods have been selected.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6208943,no,no,1485873338.63556
Visual analytical model for educational data,"Current technologies used in learning processes imply the logging of all the performed activities. These data can be exploited to gain insight into the learning process and can be used for the assessment of students, professors and the processes themselves. However, although this wealth of data exists, it is still difficult for the teachers (and interested stakeholders) to verify hypothesis, extract conclusions, or make decisions based on discovered facts or situations. This paper introduces an educational data analysis model based on visual analytics, learning analytics and academic analytics, by means of a software tool that allows performing confirmatory and exploratory data analysis through the interacting with the gathered information from a typical Learning Management System. The main goal thus is to define a model which enable the discovery of knowledge on the specific learning process that, in turn, will permit to improve it.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877098,no,no,1485873338.635557
Failure data analytics to build failure prediction mechanisms,"With ever-growing complexity of computer systems, proactive failure management is turning out to be an effective and essential approach for enhancing availability. Several techniques have been proposed to develop failure prediction models [3]. In this paper we have concentrated on the process to build up a failure prediction model based on the failure reports (service ticket logs) from hardware storage devices.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688883,no,no,1485873338.554021
Uncertainty Boundaries for Complex Objects in Augmented Reality,"Registration errors between the physical world and computer- generated objects are a central problem in Augmented Reality (AR) systems. Some existing AR systems have demonstrated how to dynamically estimate registration errors based on estimates of spatial errors in the system. Using these error estimates, these systems also demonstrated a number of ways of ameliorating the effects of registration error. One central part of this previous work was the creation and use of error regions around objects; unfortunately, the analytic methods used only created accurate regions for simple convex objects. In this paper, we present a simple and stable algorithm for generating the uncertainty regions for complex objects, including non-convex objects and objects with interior holes. We demonstrate how our approach can be used to create a set of more accurate error-based highlights in the presence of registration error, and also be used as a general highlighting mechanism.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4480784,no,no,1485873338.554019
Message from the PROMISE 2013 Chairs,"PROMISE conference is an annual forum for researchers and practitioners to present, discuss and exchange ideas, results, expertise and experiences in construction and/or application of prediction models in software engineering. Such models could be targeted at: planning, design, implementation, testing, maintenance, quality assurance, evaluation, process improvement, management, decision making, and risk assessment in software and systems development. PROMISE is distinguished from similar forums with its public data repository and focus on methodological details, providing a unique interdisciplinary venue for software engineering and machine learning communities, and seeking for verifiable and repeatable prediction models that are useful in practice.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681384,no,no,1485873338.554018
Predicting with sparse data,"It is well-known that effective prediction of project cost related factors is an important aspect of software engineering. Unfortunately, despite extensive research over more than 30 years, this remains a significant problem for many practitioners. A major obstacle is the absence of reliable and systematic historic data, yet this is a sine qua non for almost all proposed methods: statistical, machine learning or calibration of existing models. The authors describe our sparse data method (SDM) based upon a pairwise comparison technique and T.L. Saaty's (1980) Analytic Hierarchy Process (AHP). Our minimum data requirement is a single known point. The technique is supported by a software tool known as DataSalvage. We show, for data from two companies, how our approach, based upon expert judgement, adds value to expert judgement by producing significantly more accurate and less biased results. A sensitivity analysis shows that our approach is robust to pairwise comparison errors. We then describe the results of a small usability trial with a practicing project manager. From this empirical work, we conclude that the technique is promising and may help overcome some of the present barriers to effective project prediction",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=965339,no,no,1485873338.554017
Predicting with sparse data,"It is well known that effective prediction of project cost related factors is an important aspect of software engineering. Unfortunately, despite extensive research over more than 30 years, this remains a significant problem for many practitioners. A major obstacle is the absence of reliable and systematic historic data, yet this is a sine qua non for almost all proposed methods: statistical, machine learning or calibration of existing models. The authors describe their sparse data method (SDM) based upon a pairwise comparison technique and T.L. Saaty's (1980) Analytic Hierarchy Process. Our minimum data requirement is a single known point. The technique is supported by a software tool known as DataSalvage. We show, for data from two companies, how our approach, based upon expert judgement, adds value to expert judgement by producing significantly more accurate and less biased results. A sensitivity analysis shows that our approach is robust to pairwise comparison errors. We then describe the results of a small usability trial with a practising project manager. From this empirical work we conclude that the technique is promising and may help overcome some of the present barriers to effective project prediction",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915513,no,no,1485873338.554015
An Evaluation Model in Software Testing Based on AHP,"In this paper, a novel software testing evaluation model is proposed for specification based software testing. The proposed model uses Analytic Hierarchy Process (AHP) to analyze the weight of influence each individual function unit software, then the model classifies the defect ratio of each function unit based on the weight of influence derived from step one and calculates the final evaluation result of software testing according to the defect ration and influence degree.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211159,no,no,1485873338.554014
Location semantics prediction for living analytics by mining smartphone data,"Automatic location semantics prediction for living analytics based on smartphone data has attracted extensive attention in just recent years. Basically, this task can be formulated as a multi-class classification problem, where different location/places are regarded as different labels. Previous studies were mostly based on common classification techniques directly, neglecting the critical challenging issue of class imbalance in such a problem (e.g., people go to offices much more often than they go to cinemas). It is also noteworthy that in contrast to common multi-class problems where the classes can be treated independently and interchangeably, the places for labeling usually have important correlations, which should be taken account in the classification/labeling process. Moreover, several activities may occur in the same place and thus the same place label might convey different semantics. In this paper, we address the above issues for location semantics prediction by proposing the FS-Mining (Frame-based Semantics Mining) approach. We treat the raw sensor data in the smartphone as a sequence of short and non-overlapping frames, based on which the user behavior at each place can be characterized and the place semantics can be modeled. To deal with the issues of label relation and class imbalance, a multi-level classification model with class-split and class-merge mechanisms was also developed. An ensemble strategy was also employed to further improve the performance. Experiments on the dataset of Nokia Mobile Data Challenge [1] demonstrate promising performances for the FS-Mining approach.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058122,no,no,1485873338.554012
A Retrospective Study of Software Analytics Projects: In-Depth Interviews with Practitioners,"Software analytics guide practitioners in decision making throughout the software development process. In this context, prediction models help managers efficiently organize their resources and identify problems by analyzing patterns on existing project data in an intelligent and meaningful manner. Over the past decade, the authors have worked with software organizations to build metric repositories and predictive models that address process-, product-, and people-related issues in practice. This article shares their experience over the years, reflecting the expectations and outcomes both from practitioner and researcher viewpoints.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547143,no,no,1485873338.554011
On the asymptotic performance analysis of subspace DOA estimation in the presence of modeling errors: case of MUSIC,"This paper provides a new analytic expression of the bias and RMS error (root mean square) error of the estimated direction of arrival (DOA) in the presence of modeling errors. In , first-order approximations of the RMS error are derived, which are accurate for small enough perturbations. However, the previously available expressions are not able to capture the behavior of the estimation algorithm into the threshold region. In order to fill this gap, we provide a second-order performance analysis, which is valid in a larger interval of modeling errors. To this end, it is shown that the DOA estimation error for each signal source can be expressed as a ratio of Hermitian forms, with a stochastic vector containing the modeling error. Then, an analytic expression for the moments of such a Hermitian forms ratio is provided. Finally, a closed-form expression for the performance (bias and RMS error) is derived. Simulation results indicate that the new result is accurate into the region where the algorithm breaks down.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1597557,no,no,1485873338.55401
A non-orthogonal SVD-based decomposition for phase invariant error-related potential estimation,"The estimation of the Error Related Potential from a set of trials is a challenging problem. Indeed, the Error Related Potential is of low amplitude compared to the ongoing electroencephalographic activity. In addition, simple summing over the different trials is prone to errors, since the waveform does not appear at an exact latency with respect to the trigger. In this work, we propose a method to cope with the discrepancy of these latencies of the Error Related Potential waveform and offer a framework in which the estimation of the Error Related Potential waveform reduces to a simple Singular Value Decomposition of an analytic waveform representation of the observed signal. The followed approach is promising, since we are able to explain a higher portion of the variance of the observed signal with fewer components in the expansion.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6091760,no,no,1485873338.554007
"Multi-platform strategies, approaches and challenges for developing mobile applications","Developing applications for mobile platforms is challenging because of multiple proprietary environments. Abundant material has been published discussing three kinds of mobile app development - Native, Web, and Hybrid, where Hybrid apps are preferred due to their usability. In this paper we discuss a strategy and approach for developing and delivering existing Web and Desktop applications as mobile apps. This proposal is a variant of Hybrid development model that utilizes code translators to translate existing Web or Desktop applications for the target mobile platforms. Our goal is to validate if investments made by an enterprise in developing Web or Desktop applications are still relevant when the same are to be re-deployed as mobile apps.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839274,no,no,1485873338.447819
Performance evaluation and failure rate prediction for the soft implemented error detection technique,"This paper presents two error models to evaluate safety of a software error detection method. The proposed models analyze the impact on program overhead in terms of memory code area and increased execution time when the studied error detection technique is applied. For faults affecting the processor's registers, analytic formulas are derived to estimate the failure rate before program execution. These formulas are based on probabilistic methods and use statistics of the program, which are collected during compilation. The studied error detection technique was applied to several benchmark programs and then program overhead and failure rate was estimated. Experimental results validate the estimated performances and show the effectiveness of the proposed evaluation formulas.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319693,no,no,1485873338.447818
Parallel techniques for improving three-dimensional models storing and accessing performance,"Nowadays, the volume of multimedia and unstructured data has grown rapidly. More and more three-dimensional (3D) models are created for ever increasing applications. New storage and processing technologies are needed to keep pace with the continuous growth of big data. Hadoop is an attractive and open-source platform for large-scale data storage and analytics. Our previous research work has applied Hadoop distributed file system to efficiently manage 3D data for a 3D model retrieval system. To take better advantages of Hadoop, in this paper we propose two parallel strategies to improve the storing and accessing performance of 3D models. The MapReduce paradigm is adopted to provide a coarse grained parallelism for data loading, and a lightweight multithreaded algorithm is presented for data accesses. We conduct an extensive performance study on a cluster and the results show that significant performance increase can be gained for the parallel techniques.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818156,no,no,1485873338.447817
A review of the research on quantitative reliability Prediction and Assessment for electronic components,"A review is carried out on how quantitative approaches have been applied so far to the Reliability Prediction and Assessment (RPA) for computer and communication systems. A series of the reliability evaluation technology based on analytic models and computer simulations are developed for use in product design and test, shape, system operation and maintenance, during a research initiative towards understanding the quantitative characteristics of hardware and software systems. The implementation of these techniques guarantees that the sophisticated system satisfying the high standard on reliability, availability, performability, maintainability and safety. Such approaches conduct quantitative assessment for reliability at the system level. Such reliability quantitative assessments are effectively used in the system decision-making for fault detection, failures elimination, optimization, maintenance and safety.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5939553,no,no,1485873338.447816
Study on the Improvement of Analytic Hierarchy Process under College Course Evaluation System,"Under the guidance of analytic hierarchy process and genetic algorithms, establish three evaluation index systems for courses, by means of analytic hierarchy process, which improves accelerating genetic algorithm by real-coding, to weigh the evaluation index systems, and you can well evaluate the rationality of courses set up by colleges.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5522889,no,no,1485873338.447815
General Julia Sets of Non-analytic Families z^n+c,"In this paper we present general Julia sets of non-analytic families zí‰ŒË<sup>n</sup>+ c (n <sup>3</sup> 2), we also propose some properties of these general Julia sets. Moreover, by iterated function systems theory, we give out two estimations of Hausdorff dimension of these general Julia sets when |c| is sufficient large or sufficient small.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362042,no,no,1485873338.447814
On evaluation of a multiscale-based CT image analysis and visualisation algorithm,"Development of computed tomography (CT) protocols that minimise radiation dose for specific clinical treatments continues to be a major research focus. Building on the success of an earlier collaborative case study concerning prostate cancer diagnosis with pelvis CT images, this paper presents our evaluation results of a multi-scale texture analytic procedure developed to aid detection of specific anatomical features (abnormality, lesions, etc) in such images. This is a critical step in realising an intelligent and integrated image visualisation platform which will facilitate the construction of highly customised and personalised treatment plans. The ultimate aim is to provide in a single framework optimal software techniques for treatment planning, CT image-guided positioning and treatment delivery.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6746924,no,no,1485873338.447813
Design Principles for Effective Knowledge Discovery from Big Data,"Big data phenomenon refers to the practice of collection and processing of very large data sets and associated systems and algorithms used to analyze these massive datasets. Architectures for big data usually range across multiple machines and clusters, and they commonly consist of multiple special purpose sub-systems. Coupled with the knowledge discovery process, big data movement offers many unique opportunities for organizations to benefit (with respect to new insights, business optimizations, etc.). However, due to the difficulty of analyzing such large datasets, big data presents unique systems engineering and architectural challenges. In this paper, we present three system design principles that can inform organizations on effective analytic and data collection processes, system organization, and data dissemination practices. The principles presented derive from our own research and development experiences with big data problems from various federal agencies, and we illustrate each principle with our own experiences and recommendations.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337722,no,no,1485873338.44781
A Manager Framework of Supply Chain Operational Reference Model,"It is very necessary for supply chain operational managers to adopt robust and practicable design and evaluation tools. Many researchers focused on high level strategic aspects of supply chain design whose results are usually generic guidelines for business executives rather than specific tools for operational managers. In this paper, a decision-based framework for supply chain operational managers is presented and can be used to select suppliers, and so on. The methodology combines the techniques of analytic hierarchy process and preemptive goal programming. The performance metrics based on supply chain operational reference model level I are incorporated into the proposed model as the decision criteria. Additionally, a set of performance metrics is developed to evaluate the overall supply chain effectiveness, allowing direct comparison of different supply chain designs for operational managers.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4340945,no,no,1485873338.447809
Stock Market Volatility Prediction: A Service-Oriented Multi-kernel Learning Approach,"Stock market is an important and active part of nowadays financial markets. Stock time series volatility analysis is regarded as one of the most challenging time series forecasting due to the hard-to-predict volatility observed in worldwide stock markets. In this paper we argue that the stock market state is dynamic and invisible but it will be influenced by some visible stock market information. Existing research on financial time series analysis and stock market volatility prediction can be classified into two categories: in depth study of one market factor on the stock market volatility prediction or prediction by combining historical price fluctuations with either trading volume or news. In this paper we present a service-oriented multi-kernel based learning framework (MKL) for stock volatility analysis. Our MKL service framework promotes a two-tier learning architecture. In the top tier, we develop a suite of data preparation and data transformation techniques to provide a source-specific modeling, which transforms and normalizes a source specific input dataset into the MKL ready data representation. Then we apply data alignment techniques to prepare the datasets from multiple information sources based on the classification model we choose for cross-source correlation analysis. In the next tier, we develop model integration methods to perform three analytic tasks: (i) building one sub-kernel per source, (ii) learning and tuning the weights for sub-kernels through weight adjustment methods and (iii) performing multi-kernel based cross-correlation analysis of market volatility. To validate the effectiveness of our service oriented MKL approach, we performed experiments on HKEx 2001 stock market datasets with three important market information sources: historical prices, trading volumes and stock related news articles. Our experiments show that 1) multi-kernel learning method has a higher degree of accuracy and a lower degree of false prediction, compared to exis- ing single kernel methods; and 2) integrating both news and trading volume data with historical stock price information can significantly improve the effectiveness of stock market volatility prediction, compared to many existing prediction methods.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6274126,no,no,1485873338.445941
An overview of cloud middleware services for interconnection of healthcare platforms,"Using heterogeneous clouds has been considered to improve performance of big-data analytics for healthcare platforms. However, the problem of the delay when transferring big-data over the network needs to be addressed. The purpose of this paper is to analyze and compare existing cloud computing environments (PaaS, IaaS) in order to implement middleware services. Understanding the differences and similarities between cloud technologies will help in the interconnection of healthcare platforms. The paper provides a general overview of the techniques and interfaces for cloud computing middleware services, and proposes a cloud architecture for healthcare. Cloud middleware enables heterogeneous devices to act as data sources and to integrate data from other healthcare platforms, but specific APIs need to be developed. Furthermore, security and management problems need to be addressed, given the heterogeneous nature of the communication and computing environment. The present paper fills a gap in the electronic healthcare register literature by providing an overview of cloud computing middleware services and standardized interfaces for the integration with medical devices.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6866753,no,no,1485873338.44594
Efficient People Counting with Limited Manual Interferences,"People counting is a topic with various practical applications. Over the last decade, two general approaches have been proposed to tackle this problem: (a) counting based on individual human detection; (b)counting by measuring regression relation between the crowd density and number of people. Because the regression based method can avoid explicit people detection which faces several well-known challenges, it has been considered as a robust method particularly on a complicated environments. An efficient regression based method is proposed in this paper, which can be well adopted into any existing video surveillance system. It adopts color based segmentation to extract foreground regions in images. Regression is established based on the foreground density and the number of people. This method is fast and can deal with lighting condition changes. Experiments on public datasets and one captured dataset have shown the effectiveness and robustness of the method.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008106,no,no,1485873338.445939
The multi-dimension component quality evaluation,"Recently, software quality evaluation based on ISO/IEC 9126 and ISO/IEC 14598 has been used widely. However, these standards for software quality don't provide practical guidelines to apply the quality model and the evaluation process to component based software. This paper presents a multi-dimension quality model and a quantitative quality evaluation model for components and component based software. Particularly, our model provides the weights of quality characteristics and sub-characteristics using analytic hierarchical process technique. We also present the evaluation process using different methods for component developers, reusers and the third party. As a result, we believe that the proposed model will be helpful to evaluate component quality.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541423,no,no,1485873338.445938
Network-wide traffic visibility in OF@TEIN SDN testbed using sFlow,"This paper provides insights into the traffic flow monitoring system of OF@TEIN (OpenFlow@Trans Eurasia Information Network) testbed. OF@TEIN is software defined networking (SDN) testbed adapted by KOREN (KOrea advanced REsearch Network) and integrated with the research and education networks of several Asian nations including Japan, Malaysia, Thailand, Vietnam, Philippine, etc. Traditional traffic monitoring solutions such as NetFlow, RSPAN ports, Network Packet Brokers (NPBs) can't provide network-wide visibility in OF@TEIN because OF@TEIN is a large multi-tenant testbed deployed over high speed research networks across several countries. Therefore, we have implemented a new sFlow-based flow monitoring system that is tailored to OF@TEIN requirements and can provide real-time L2 to L7 network wide visibility. OF@TEIN uses SDN-based network virtualization to slice the network among multiple concurrent experimenters. Machines in a network slice (or VLAN) communicate with each other using GRE tunnels. Our traffic monitoring system enables monitoring flow spaces of VLANs as well as physical provider network. It utilizes northbound interfaces (NBIs) exposed by sFlow-RT analytics engine and FloodLight SDN Controller. It periodically fetches flows statistics from sFlow-RT and stores them in time-series format in Whisper RRD database. Graphite real-time charting tool is used to plot the statistics stored in Whisper.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996541,no,no,1485873338.445937
Performance appraisal of Student Affairs Management based on group analytic hierarchy process,"Student Affairs Management is a key component of College Management and one of important factors affecting the long-term development of colleges. Furthermore, the management quality of student affairs directly determines the core competitiveness of a college. This paper has built the index system of Student Affairs Management performance appraisal in College A. This index system applies the Group AHP, which is based on cluster analysis, to determine index weights and fuzzy comprehensive evaluation, which aims at collected data, to carry on a comprehensive evaluation about the Student Affairs Management level of College A. Evaluation results verify and demonstrate that the combination of group AHP cluster analysis and fuzzy comprehensive evaluation is more scientific and effective, and that it can be better applied to the performance appraisal of Student Affairs Management of this college and other colleges in the similar level.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013808,no,no,1485873338.445936
"A Dynamic Software Product Line Architecture for Prepackaged Expert Analytics: Enabling Efficient Capture, Reuse and Adaptation of Operational Knowledge","Advanced asset health management solutions blend business intelligence with analytics that incorporate expert operational knowledge of industrial equipment and systems. Key challenges in developing these solutions include: streamlining the capture and prepackaging of operational experts' knowledge as analytic modules, efficiently evolving the modules as knowledge grows, adapting the analytics in the field for diverse operating circumstances and industries, and executing the analytics with high performance in industrial and enterprise software systems. A Quality Attribute Workshop (QAW) was used to elicit and analyze variability at development time and runtime for creating, integrating, evolving, and tailoring reusable analytic modules for ABB/Ventyx asset health solution offerings. Dynamic software product line (DSPL) architecture approaches were then applied in designing an analytics plug in architecture for asset health solutions. This paper describes our approach and experiences in designing the analytics product line architecture and its SME Workbench toolset, and how we achieved significant improvements in speed and flexibility of deploying industrial analytics.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6827120,no,no,1485873338.445935
A New Coupled Metric Learning for Real-time Anomalies Detection with High-Frequency Field Programmable Gate Arrays,"Billions of internet end-users and device to device connections contribute to the significant data growth in recent years, large scale, unstructured, heterogeneous data and the corresponding complexity present challenges to the conventional real-time online fraud detection system security. With the advent of big data era, it is expected the data analytic techniques to be much faster and more efficient than ever before. Moreover, one of the challenges with many modern algorithms is that they run too slowly in software to have any practical value. This paper proposes a Field Programmable Gate Array (FPGA) -based intrusion detection system (IDS), driven by a new coupled metric learning to discover the inter- and intra-coupling relationships against the growth of data volumes and item relationship to provide a new approach for efficient anomaly detections. This work is experimented on our previously published NetFlow-based IDS dataset, which is further processed into the categorical data for coupled metric learning purpose. The overall performance of the new hardware system has been further compared with the presence of conventional Bayesian classifier and Support Vector Machines classifier. The experimental results show the very promising performance by considering the coupled metric learning scheme in the FPGA implementation. The false alarm rate is successfully reduced down to 5% while the high detection rate (=99.9%) is maintained.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7022747,no,no,1485873338.445933
"Orion: A system for modeling, transformation and visualization of multidimensional heterogeneous networks","The study of complex activities such as scientific production and software development often require modeling connections among heterogeneous entities including people, institutions and artifacts. Despite numerous advances in algorithms and visualization techniques for understanding such social networks, the process of constructing network models and performing exploratory analysis remains difficult and time-consuming. In this paper we present Orion, a system for interactive modeling, transformation and visualization of network data. Orion's interface enables the rapid manipulation of large graphs-including the specification of complex linking relationships-using simple drag-and-drop operations with desired node types. Orion maps these user interactions to statements in a declarative workflow language that incorporates both relational operators (e.g., selection, aggregation and joins) and network analytics (e.g., centrality measures). We demonstrate how these features enable analysts to flexibly construct and compare networks in domains such as online health communities, academic collaboration and distributed software development.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102441,no,no,1485873338.445932
An analytic model and optimization technique based methods for fault diagnosis in power systems,"An analytic model for fault diagnosis of power system using optimization technique is expressed as unconstrained 0-1 integer programming problem, and consequently faulty equipment identification can be solved by refined mathematical operation. Considering the configuration of automatic devices in modern power systems, such as protective relays and reclosing relays, an improved analytic model and optimization technique-based method for fault diagnosis of power system is proposed in this paper. The evaluation criteria of the presented model is improved considering the relationship of multiple main protective relays, backup protective relays, malfunctioning protective relays and reclosing relays. Improvements of analytic model for fault diagnosis of electric power system based on optimization techniques are presented firstly. A brief description about the modulars and functions of the online fault diagnosis software which is developed by the authors for Jiangsu Provincial Power Company is given. The adopted EMS data acquisition method and simulated online test results for the power system of Jiangsu Power Company are described.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4523623,no,no,1485873338.445931
An analytic approach to measuring the overall effectiveness of R&D-a case study in the telecom sector,R&D managers need to measure the overall performance of R&D activities in order to improve the activities and prove their effectiveness. This paper aims at systematizing the measurement of the overall effectiveness of R&D. An example of the utilization of our analytical approach is presented in the form of a case study in the telecom sector.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1038516,no,no,1485873338.444075
Do Rapid Releases Affect Bug Reopening? A Case Study of Firefox,"Large software organizations have been adopting rapid release cycles to deliver features and bug fixes earlier to their users. Because this approach reduces time for testing, it raises concerns about the effectiveness of quality assurance in this setting. In this paper, we study how the adoption of rapid release cycles impacts bug reopening rate, an indicator for the quality of the bug fixing process. To this end, we analyze thousands of bug reports from Mozilla Firefox, both before and after their adoption of rapid releases. Results suggest that the bug reopening rate of versions developed in rapid cycles was about 7% higher. Also, as a warning to the software analytics community, we report contradictory results from three attempts to answer our research question, performed with varying degrees of knowledge about the Firefox release process.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6943480,no,no,1485873338.444074
SCAP based configuration analytics for comprehensive compliance checking,"Computing systems today have large number of security configuration settings that are designed to offer flexible and robust services. However, incorrect configuration increases the potential of vulnerability and attacks. Security Content Automation Protocol provides a unified mean to automate the process of checking the desktop system compliance using standard interfaces. However, misconfiguration can be identified only if global checking that includes network and desktop configuration is performed, as many of these configurations are highly interdependent. In this work we present a SCAP-based tool that integrates host and network configuration compliance checking in one model and allows for executing comprehensive analysis queries in order to verify security and risk requirements across the end-to-end network as a single system. Our proposed tool translates XCCDF reports generated from SCAP tools into logical objects that can be further composed to create global logical analysis using more advanced security analytic tools such as ConfigChecker and PROLOG-based tools. This project also shows the value of building on the effort of standard tools to improve the state of the art.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111674,no,no,1485873338.444073
Log File Analysis of E-commerce Systems in Rich Internet Web 2.0 Applications,"This paper describes the implications of the new trends in web development languages on log file analysis for e-commerce. The new trends in Software Development and the available Web Analytics technologies are explained. The focus is placed on the diversifications introduced to the traces left on the system that by Rich Internet Applications (RIAs). Finally a novel hybrid solution is proposed that is based on the junction of log files with operational data and page tagging, which allows even exacter measurements of customer behavior. It allows a customization of the Analysis Tool and survives the shift of the technologies.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065092,no,no,1485873338.444072
On computing the L<inf>2</inf> norm of a generalized discrete-time system,"We give explicit analytic formulas for computing the L<sub>2</sub> norm of a discrete-time generalised system whose rational transfer matrix function may be improper or polynomial. The norm is expressed in terms of solutions of generalized Lyapunov equations, written down with coefficients from a special type of realisation of the underlying transfer function matrix, much in the same spirit of the standard (proper) case. The main result hints to a numerically-sound prototype algorithm that relies on standard reliable software for computing solutions of generalised Lyapunov equations.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982451,no,no,1485873338.444071
Optimal Preventive Maintenance Scheduling in Semiconductor Manufacturing Systems: Software Tool and Simulation Case Studies,"This paper presents the architecture and implementation of a preventive maintenance optimization software tool (PMOST), based on algorithms for the optimal scheduling of preventive maintenance (PM) tasks in semiconductor manufacturing operations. We also present results from four complex simulation case studies, based on real industrial data and employing full fab models, to illustrate the use, data needs and outcomes produced by PMOST. These results demonstrate significant improvements in tool production and consolidation of PM tasks. We give a description of the different software modules that compose PMOST, to provide guidelines as well as a template for other implementations of the PM optimization algorithms utilized by PMOST.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5475204,no,no,1485873338.444069
Towards Quality Aware Collaborative Video Analytic Cloud,"As cloud diversifies into different application fields, understanding and characterizing the specific work load sand application requirements play important roles in the design of efficient cloud infrastructure and system software support. Video analytic is a rapidly advancing field and it is widely used in many application domains (i.e., health, medical care, surveillance, and defense). To support video analytic applications efficiently in cloud, one has to overcome many challenges such as lack of understanding of the relationship and trade off between analytic performance metrics and resource requirements. Furthermore, cloud computing has grown from the early model of resource sharing to data sharing and workflow sharing. To address the challenges and to lever age emerging trends, we propose and experiment with a domain specific cloud environment for video analytic applications. We design a cloud infrastructure framework for sharing video data, analytic software, and workflow. In addition, we create a video analytic quality aware resource plan model to guarantee users QoS and optimize usage of resources based on predictive knowledge of video analytic softwares performance metrics and a resource planning model that optimizes the overall analytic service quality under users constraints (i.e., time and cost).The predictive knowledge is represented as input and analytic software specific predictors. The experimental results show that the video analytic quality aware resource planning model can balance the tradeoff between analytic quality and resource requirements, and achieve optimal or near-optimal planning for video analytic workloads with constraints in a resource shared environment. Simulation studies show that resource planning results using ground truth and video analytic performance predictions are very similar, which indicates that our analytic quality/resource predictors are very accurate.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6253500,no,no,1485873338.444068
Modeling of Efficiency and Uniformity of Different Pumping Structures of Slab Lasers,"Modeling for end-pumping and edged-pumping high power solid-state slab lasers. Both analytic and ray tracing methods were used to analyze the distributions of absorbed pumping power using laser-diode-array pump sources. The pumping light field profiles of the different pumping structures mentioned above are acquired, according to which the absorption power density and uniformity of the pumping light are analyzed. In order to compare the pumping uniformity and efficiency of the two pumping structures, cylinder lens are adopted for beam focus and shaping. Each cylinder lens gets different size, but coupling efficiency is about 91% as the same. The irradiance profiles of these two pumping structure are simulated via ray trace method of non-sequential component of ZEMAX software, each is confirmed finally, i.e. the absorbed pumping power density distribution is calculated. The edge-pumped and end-pumped slab lasers allow for a longer absorption path than in face-pumped slabs. However, the tradeoff made to reach higher pump efficiency is lower absorbed pump power distribution uniformity due to the long absorption path. By comparing simulation results of the different pumping structures, it was found that uniformity of absorbed pumping distribution on width-thickness plane in doped region of crystal with end-pumping structure is the best, and the absorbed peak power density is 0.0168 W/mm<sup>3</sup> which is the highest among three pumping structures. It is more uniformity distribution for pumping light in the Z direction of crystal axis.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5230307,no,no,1485873338.444067
The Current State of Business Intelligence,"Business intelligence (BI) is now widely used, especially in the world of practice, to describe analytic applications. BI is currently the top-most priority of many chief information officers. BI has become a strategic initiative and is now recognized by CIOs and business leaders as instrumental in driving business effectiveness and innovation. BI is a process that includes two primary activities: getting data in and getting data out. Getting data in, traditionally referred to as data warehousing, involves moving data from a set of source systems into an integrated data warehouse. Getting data in delivers limited value to an enterprise; only when users and applications access the data and use it to make decisions does the organization realize the full value from its data warehouse. Thus, getting data out receives most attention from organizations. This second activity, which is commonly referred to as BI, consists of business users and applications accessing data from the data warehouse to perform enterprise reporting, OLAP, querying, and predictive analytics.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4302625,no,no,1485873338.444066
Mobile Audience Measurements in User Experience Research,"Holistic, objective and precise data on mobile user behavior and experience are needed in today's product development and marketing activities. This article presents a framework for mobile audience measurements, for collecting data at the point of convergence-devices. The paper compares the presented framework to alternative methods of mobile user research, and identifies the unique advantages of on-device measurements along with the key weaknesses. In addition to elaborating on data collection, the paper addresses the related analytics, presenting adoption modeling and stickiness analysis that complement the data collection processes and deliver practical insights. The insights can be provided to device vendors, application developers and carriers, who can use the insights in product portfolio management, product development, and marketing.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5506573,no,no,1485873338.444065
ERP Effort Estimation Based on Expert Judgments,"A new technology shift brings to the ERP domain a change in the industry and a new platform build on in-memory optimized databases, introduced and known as SAP HANA [1]. This technology shift in the ERP domain led to SAP's ERP on HANA, the solution where the ERP suite is offered on the same platform as ERP Services such as Business Analytics. The integration of ERP Services and the ERP suite brings to the industry new opportunities to ""fine tune"" customer and industry specific business processes. This radical shift in innovation brings with it new challenges in terms of ERP effort estimation. No longer can we rely on a single method such as functional size measurement methods, due to the wide range of customization possibilities. This shift from a typical predefined solution scope to a highly customizable landscape poses a challenge to project estimation practitioners as the functional size estimation techniques used in the past for ERP solutions address a fixed scope deployable in multiple landscapes, and hence are no longer suitable for dynamically definable scope. Today's highly volatile and customized ERP landscape demands a new approach to estimate effort by leveraging ERP professionals' tacit knowledge and expert judgments. This paper presents the ERP Service estimation method that leverages the strengths of expert-judgment-based estimation techniques while using a more structured approach to reduce the effects of expert bias and avoid common pitfalls associated with judgment-based estimation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693229,no,no,1485873338.442207
A Novel Blind Recognition Algorithm for Modulated M-QAM Signals,"In this paper, we develop an algorithm using Hilbert transform for the blind recognition of QAM signals. Without requiring any prior knowledge of signal parameters, the proposed method employs incoming signal and its Hilbert transform to calculate the instantaneous amplitude of the analytic signal, and then uses subtractive clustering algorithm to find the clustering centers of the instantaneous amplitude. The modulation order (M) of QAM signal is determined according to the number of clustering centers obtained. Computer simulation shows that the proposed method has strong capability for recognition of higher order modulation signals in the presence of additive white Gaussian noise (AWGN).",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4797040,no,no,1485873338.442206
Automated biofilm region recognition and morphology quantification from confocal laser scanning microscopy imaging,"Staphylococcus aureus is an opportunistic human pathogen and a primary cause of nosocomial infections. Its biofilm forming capability is an adaptation strategy utilized by many species of bacteria to overcome stressful environmental conditions and provides both resistance to antimicrobial treatments and protection from the host immune system. This paper addresses a growing demand for an objective, fully automated method of biofilm structure description with standardized parameters that are independent of user input. In this study, we used watershed segmentation to analyze and compare confocal laser scanning microscopy (CLSM) images of two S. aureus strains with different biofilm-forming capabilities. Results are compared with manual calculations as well as the commonly used COMSTAT software.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872327,no,no,1485873338.442205
Robust Decision Engineering: Collaborative Big Data and its application to international development/aid,"Much of the research that goes into Big Data, and specifically on Collaborative Big Data, is focused upon questions, such as: how to get more of it? (e.g., Œ‡ participatory mechanisms, social media, geo-coded data from personal electronic devices) and Œ‡ how to handle it? (e.g., how to ingest, sort, store, and link up disparate data sets). A question that receives far less attention is that of Collaborative analysis of Big Data; how can a multi-disciplinary layered analysis of Big Data be used to support robust decisions, especially in a collaborative setting, and especially under time pressure? The robust Decision Engineering required can be achieved by employing an approach related to Network Science, that we call Relationship Science. In Relationship Science, our methodological framework, karassian netchain analysis (KNA), is utilized to ascertain islands of stability or positive influence dominating sets (PIDS), so that a form of annealed resiliency or latent stability is achieved, thereby mitigating against unintended consequences, elements of instability, and äóìperfect stormäó crises lurking within the network.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6450957,no,no,1485873338.442204
TreePlus: Interactive Exploration of Networks with Enhanced Tree Layouts,"Despite extensive research, it is still difficult to produce effective interactive layouts for large graphs. Dense layout and occlusion make food Webs, ontologies and social networks difficult to understand and interact with. We propose a new interactive visual analytics component called TreePlus that is based on a tree-style layout. TreePlus reveals the missing graph structure with visualization and interaction while maintaining good readability. To support exploration of the local structure of the graph and gathering of information from the extensive reading of labels, we use a guiding metaphor of ""plant a seed and watch it grow."" It allows users to start with a node and expand the graph as needed, which complements the classic overview techniques that can be effective at (but often limited to) revealing clusters. We describe our design goals, describe the interface and report on a controlled user study with 28 participants comparing TreePlus with a traditional graph interface for six tasks. In general, the advantage of TreePlus over the traditional interface increased as the density of the displayed data increased. Participants also reported higher levels of confidence in their answers with TreePlus and most of them preferred TreePlus",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703363,no,no,1485873338.442203
Architecture-Based Adaptivity Support for Service Oriented Scientific Workflows,"Adaptivity is the ability of a program to change its behavior automatically according to its context. Programs over multiple scientific workflows and analytic domains have similar needs of adaptivity to handle data intensive computing. These include dynamically selecting analytical models or processes according to data sets at runtime, handling exceptions to make long running workflows reliable, and reducing large volumes of data to a suitable form for visualization. Architecture support to reuse these adaptive techniques across different scientific domains helps to enhance the workflows' efficiency, extensibility and reliability. In this paper, a service oriented architecture framework is presented to compose adaptive scientific workflows. This framework has a core of service components and mechanisms that eases the interoperation between disparate workflows and programs that encapsulate the adaptive control logic. The uses of this framework are demonstrated by scientific workflow scenarios.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6525537,no,no,1485873338.442202
Moment-closure approximations for mass-action models,"Although stochastic population models have proved to be a powerful tool in the study of process generating mechanisms across a wide range of disciplines, all too often the associated mathematical development involves nonlinear mathematics, which immediately raises difficult and challenging analytic problems that need to be solved if useful progress is to be made. One approximation that is often employed to estimate the moments of a stochastic process is moment closure. This approximation essentially truncates the moment equations of the stochastic process. A general expression for the marginal- and joint-moment equations for a large class of stochastic population models is presented. The generalisation of the moment equations allows this approximation to be applied easily to a wide range of models. Software is available from http://pysbml.googlecode.com/ to implement the techniques presented here.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4762250,no,no,1485873338.4422
Learning Analytics on federated remote laboratories: Tips and techniques,"A remote laboratory is a software and hardware tool which enables students to use real equipment -located in an educational institution- through the Internet. This way, students can experiment as if they were using the laboratories with their own hands. And, depending on the design, instructors can later see the results of these students. During the last decade, federation protocols to share remote laboratories have emerged. The focus of these protocols is to be make remote laboratories of one institution available in other in an automated manner, through institutional contracts. And these federation protocols usually rely on existing Remote Laboratory Management Systems (RLMS), which usually provide APIs for tracking student usage. At the same time, the interest on Learning Analytics is increasing. Learning Analytics focuses on the measurement and analysis of data about learners in their context. In the particular context of federated remote laboratories, new challenges arise: on the one hand, remote laboratories must be prepared to track insightful information from the student session so as to extract patterns, and on the other hand, the usage of a federated environment requires different degrees of anonymity. This contribution describes the new Learning Analytics dashboard of WebLab-Deusto, detailing what information can be extracted and how the usage of a RLMS simplifies the development of such tools in a federated environment.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6826107,no,no,1485873338.442199
A computer based strategy design for automobile spare part logistics network optimization,"This paper discusses the problem of the automobile spare part logistics warehouse network strategy, a quite important problem in logistics. On the basis of the integration of quantitative technology and qualitative technology, a mathematic model and algorithm of automobile spare part logistics network optimization are presented. In the quantitative analysis part we propose a novel hybrid approach through crossing over the PSO and GA, called hybrid PSO-GA based algorithm. Among the mixed algorithm, GA is embedded to solve the difficulties of updating the particles in the binary code system; the roulette algorithm is embedded to eliminate worse particles; SA is embed to control convergence of particles. Then in the qualitative part, we propose a selection model along with the AHP methodology that project selection would be easier and more accurate than before. In the end we apply the above approach in an automobile spare part logistics company. Computational simulation is carried out to evaluate the performance of the algorithm and the results show that this approach can indeed find effective solutions for the automobile spare part logistics optimization problem.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5402534,no,no,1485873338.442198
Needs Assessment for the Design of Information Synthesis Visual Analytics Tools,"Information synthesis is a key portion of the analysis process with visual analytics tools. This stage of work requires users to collect, organize, and add meaning to individual analytical results. This paper reports the results of a needs assessment study with technical and bio/chemical security analysts intended to characterize the ways in which users currently synthesize information,and to elicit ideas for future tools to support information synthesis. Our work used structured interviews to obtain knowledge from analysts. Responses indicate that synthesis is currently supported through the use of office productivity software, and current tools do not provide adequate support for the task of information synthesis.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190786,no,no,1485873338.442197
Combining commercial consensus and community crowd-sourced categorization of web sites for integrity against phishing and other web fraud,"Traditionally, the protection provided by 3rd party anti-Malware endpoint security products is measured using a sample set that is representative of the prevalent universe of attacks at that point in time (malicious URLs and/or malicious files in the world). The methodology used for such a selection of the Malware attack samples, the so-called Stimulus Workload (SW), has been a matter of controversy for a number of years. The reason is simple. Given a carefully crafted selection of such files or URLs, then, the results of the measurements can varied drastically favoring one vendor versus the other. In [1], Colon Osorio, et.al. argued that the selection process must be strictly regulated, and further, that such a selection must take into account the fact that amongst the samples selected, some pose a greater threat to users than others, as they are more widespread, and hence are more likely to affect a given user. Further, some Malware attack samples may only be found on specific websites, affect specific countries/regions, or only be relevant to a particular operating system version or interface languages (English, German, Chinese, and so forth). In [1], [2], the idea of a Customizable Stimulus Workloads, (CSW) was first suggested, whereas, the collection of samples selected as the Stimulus Workload is required to take into account all the elements described above. Within this context, CSWs are created by filtering attack samples base on prevalence, geographic regions, customer application environments, and other factors. Within the context of this methodology, in this manuscript we will pay special attention to one such specific application environment, primarily, Social Networks. With such a target environment in mind, a CSW was created and used to evaluate the performance of end-point security products. Basically, we examine the protection provided against Malware that uses internet Social Networks as part of the attack vector. When Social Network CSWs are used,- together with differential metrics of effectiveness, we found that amongst the Social Networks studied (Facebook, Google+, and Twitter) the amount of inherent protection provided ranged from negligible to a level that we will call modest self-protection (0% to 18% prevention rate). Further, results of our evaluation showed that the supplemental protection provided by 3rd party anti-Malware products was erratic, ranging from a low of 0% to a high of 93% depending on the product and/or Social Network combination.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999407,no,no,1485873338.440259
The 8<sup>th</sup> international conference on communications systems: system level buffer size estimation on QoS demands,"Buffer size estimation on QoS demands of a multimedia communication system is an important and challenging issue. Because the QoS is often related with many aspects, it is hard to resolve this problem with an analytic method based on a mathematical model. We propose a simulation method based on an operational Distributed Co-Design Model (DCDM). This method is used in the ROCS, and the operating process is shown by taking the MPEG video transmission over a wireless channel.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183281,no,no,1485873338.440258
"Analysis Engine for Automated Health Checks, Early Problem Detection and Advanced Problem Determination","With current trends in software industry toward increased complexity of modern software, tight integration of multiple software products, emphasis on software reliability and high-level availability, software support and maintenance costs increase dramatically. It is imperative for businesses to be able to monitor health of their systems making sure that they are performing at top levels, quickly respond to any problems and timely fix them and also be able to perform advanced problem determination to reduce total time for outages that already occurred. Equally important is to prevent problems from occurring based on best practices and knowledge of known problems/issues for specific software products. To achieve these goals, a powerful analysis engine capable of performing comprehensive health checks of customer systems and advanced problem determination based on analysis of customers' data is proposed. It can be used for both proactive and reactive customer support. Such an engine works as a virtual consultant for the end users. It detects potential problems related to customer systems and installed products and provides notifications or alerts proactively, i.e. could be considered as an early detection system. It is also capable of analyzing FFDC (First Failure Data Capture) data after a problem has occurred, comparing the data with well known problems and related symptoms from relevant knowledge databases and providing customers with the results of analysis, found matches of previously recorded problems and recommendations on how to fix the problem at hand. The engine proposed utilizes up to date analytics from subject matter experts and best practices encoded in it. In the present work, a system architecture and design of such an analysis engine is presented. The proposed engine has a low bar of adoption, flexible extensible design and could be easily adopted for any software product. It is able to analyze encoded human knowledge, compare collected customer dat- with available historical data and report problems and issues found along with the relevant recommendations and suggested fixes. More specifically, the engine provides a comprehensive analysis in terms of health checks, best practices compliance check, prerequisites check, end-of-service product check, operating environment and configuration setup check, outage prevention, state comparison, problem determination and others. A case study based on the proposed engine design is presented and discussed in more detail.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311040,no,no,1485873338.440257
A Study of Factors Influencing Customers' Online Shopping Behavior Based on ANP,This research explores which factors affect the online shopping behavior of Internet customer. This paper adopts the Analytic Network Process (ANP) as the main analytical tool. ANP was an effective method for considering the feed back and the relations between factors. The research results point out that the security of network transactions was the most important factors for customer's online shopping.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578473,no,no,1485873338.440256
The Berkeley Data Analytics Stack: Present and future,"The Berkeley AMPLab was founded on the idea that the challenges of emerging Big Data applications requires a new approach to analytics systems. Launching in early 2011, the project set out to rethink the traditional analytics stack, breaking down technical and intellectual barriers that had arisen during decades of evolutionary development. The vision of the lab is to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (such as machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and en masse, as with crowdsourced human computation). To pursue this goal, we assembled a research team with diverse interests across computer science, forged relationships with domain experts on campus and elsewhere, and obtained the support of leading industry partners and major government sponsors. The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the nearly three years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark inmemory computation framework, and the Shark query processing system. BDAS shows up prominently in many industry discussions of the future of the Big Data analytics ecosystem - a rare degree of impact for an ongoing academic project. Given this initial success, the lab is continuing on its research path, moving ""up the stack"" to better integrate and support deep machine learning and to make people a full-fledged resource for making sense of Big Data. In this talk, I'll first outline the motivation and insights behind our research approach and describe how we have organized to address the cross-disciplinary nature of Big Data challenges. I will then describe- the current state of BDAS with an emphasis on the key components listed above and will address our current efforts on machine learning scalability and ease of use, and hybrid human/computer processing. Finally I will present our current views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691545,no,no,1485873338.440254
The research of optimal design of heat exchanger in heat exchanger heat meter,"In heat exchanger heat meter, the heat exchanger is equivalent to a flowmeter. The flow and heat can be calculated by measuring the inlet and outlet temperature of heat exchanger. It greatly prevents the flowmeter from the problem of inaccuracy and damage which are caused by the quality of water. However, due to the complexity of the heat exchanger model, it is difficult to get an expression between flow and temperature through analytic methods. This makes a great difficulty to design and optimize the structure of heat exchanger. To solve the problem above, we establish a three-dimensional model of heat exchanger by means of CFD numerical simulation software. By solving it in the parallel computing platform, we get the relationship between flow and temperature difference ratio, and optimize the structure of the heat exchanger by means of pattern search method. Simulation results show that after using this optimization method, the performance of heat exchanger has been effectively improved.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5973729,no,no,1485873338.440253
Exploring big data in small forms: A multi-layered knowledge extraction of social networks,"Big data poses great challenges for social network analysts in both the data volume and the latent dimensions hidden in the unstructured data. In this paper, we propose a comprehensive knowledge extraction approach for social networks to guide latent dimensions analysis. An improved hypergraph model of social behaviors was then proposed for conveniently conducting multi-faceted analytics in relationships inherent to social media. A real life case study based on Twitter's data was also presented to illustrate the multi-dimensional relations between users based on the categories they co-join and the tweets they co-spread with three orthogonal dimensions of affect analyzed simultaneously, i.e. valence, activation, and intention.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691784,no,no,1485873338.440252
The Study of Test Evaluation Method Based on Bank Assessment System,"In this paper, the personal financial assessment system of Industrial and Commercial Bank of China branch in Suzhou was used for a example, during the designing and testing in parallel, according to test results, we propose a method to calculate the satisfaction of the software system, to judge the performance of the system and to analysis the extent of the various stages of testing changes, then evaluate the quality of the system changes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676846,no,no,1485873338.440251
The Software Quality Evaluation Method Based on Software Testing,"In order to improve the effectiveness, visibility and specification of the TT&C software testing, this paper made an in-depth study according to its characteristics. At first, this paper presented a quality assessment model with high reliability and real-time demands getting idea from analytic hierarchy process (AHP). Then, the paper brought forward a dedicated simulation test environment and a generating method of software testing cases based on fault tree analysis (FTA). Next, the paper defined the software testing procedure learning from CMMI demands. At last, the paper gave the performance of quantitative assessment results in the form of a radar chart. Practice has proved that the given model of this paper can represent the software quality objectively, the generating method can improve the sufficiency of software testing cases effectively, and the defined procedure can ensure the specification of software testing availably.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394607,no,no,1485873338.44025
Computer-Supported Assessment of Software Verification Proofs,"Most conventional e-assessment systems are not suited for examining analytic, creative and constructive skills and the few existing ones have too limited functionality to appropriately support Computer Science assessments. On this account the e-assessment system EASy has been developed which e.g. provides relevant exercise modules for Computer Science tasks. Recently the system has been extended by a module for the computer-supported examination of software verification proofs based on the Hoare Logic. In this work we discuss this module and evaluate its applicability, usability and acceptance in terms of a lecture on Formal Specification.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5607766,no,no,1485873338.440248
Facing scalability issues in requirements prioritization with machine learning techniques,"Case-based driven approaches to requirements prioritization proved to be much more effective than first-principle methods in being tailored to a specific problem, that is they take advantage of the implicit knowledge that is available, given a problem representation. In these approaches, first-principle prioritization criteria are replaced by a pairwise preference elicitation process. Nevertheless case-based approaches, using the analytic hierarchy process (AHP) technique, become impractical when the size of the collection of requirements is greater than about twenty since the elicitation effort grows as the square of the number of requirements. We adopt a case-based framework for requirements prioritization, called case-based ranking, which exploits machine learning techniques to overcome the scalability problem. This method reduces the acquisition effort by combining human preference elicitation and automatic preference approximation. Our goal in this paper is to describe the framework in details and to present empirical evaluations which aim at showing its effectiveness in overcoming the scalability problem. The results prove that on average our approach outperforms AHP with respect to the trade-off between expert elicitation effort and the requirement prioritization accuracy.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531050,no,no,1485873338.438386
"More ""normal"" than normal: scaling distributions and complex systems","One feature of many naturally occurring or engineered complex systems is tremendous variability in event sizes. To account for it, the behavior of these systems is often described using power law relationships or scaling distributions, which tend to be viewed as ""exotic"" because of their unusual properties (e.g., infinite moments). An alternate view is based on mathematical, statistical, and data-analytic arguments and suggests that scaling distributions should be viewed as ""more normal than normal"". In support of this latter view that has been advocated by Mandelbrot for the last 40 years, we review in this paper some relevant results from probability theory and illustrate a powerful statistical approach for deciding whether the variability associated with observed event sizes is consistent with an underlying Gaussian-type (finite variance) or scaling-type (infinite variance) distribution. We contrast this approach with traditional model fitting techniques and discuss its implications for future modeling of complex systems.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371310,no,no,1485873338.438385
Parameters Optimization of VSC-HVDC Control System Based on Simplex Algorithm,"The performance of VSC-HVDC system depends on the parameters of control system, and usually PI controllers have been used to adjust system to fulfill desired objectives. However, the optimization methods for PI controllers' parameters of VSC-HVDC system are very few up to now. A control strategy based on direct analytic expression for VSC- HVDC is presented and the corresponding control system is designed. Simplex algorithm and system objective function are adopted to optimize the PI parameters for single- and multi- objective VSC-HVDC system on the basis of the control strategy. Simulation results in PSCAD/EMTDC software testify the performance of VSC-HVDC control system with optimized PI parameters and show that the controllers with the optimized PI parameters can effectively control VSC-HVDC system. Advantages of the control system with optimized parameters, such as precise control, quickly responding time and strong robustness have been testified by step response.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275851,no,no,1485873338.438384
An analytic approach for estimation of maximum power point in solar cars,"In this paper a new maximum power point tracker (MPPT) for a moving solar module is proposed. In these cases, multiple peaks are introduced in the PV array curves (due to the shadow effect) and it is difficult to find and track the new MPP, in order to extract maximum PV power. Hence, a simple analytical MPPT approach is introduced which is easy to implement and could overcome the mentioned problem. The proposed method is also fast enough to accurately track the MPP in different environmental circumstances. The feasibility and effectiveness of the proposed method is confirmed using modeling in MATLAB environment.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6292422,no,no,1485873338.438382
Resource allocation contracts for Open Analytic Runtime models,"Open Analytic Runtime (OAR) Models embed analysis algorithms into runtime architectural models, thus integrating the model and its analytic interpretations. Such an integration is critical for Cyber-Physical Systems (CPS) when model parts are independently developed by different teams as it is the case in multi-tier industries, e.g. avionics and automotive. Analysis algorithms play a central role augmenting the designer's capacity to automatically verify properties of interest in systems at the scale and complexity required by these industries. Unfortunately, the verification results are valid only if the assumptions of the different analysis algorithms (analytic assumptions) are consistent with each other. This paper presents our work on the automatic verification of one important class of analytic assumptions in OAR models: resource allocation assumptions. These assumptions are modeled as Resource Allocation (RA) contracts. RA contract constructs include not only the typical assumes and guarantees but also runtime facts and implications. Finally, we automatically determine the correct sequence of execution of the analysis algorithms based on the contract input/output dependencies described in our models. Together these characteristics enable the automatic assumption verification that preserves the scalability of analytic models. We illustrate our approach using an example model with analysis algorithms for security, schedulability, and energy efficiency.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064507,no,no,1485873338.438381
Software rejuvenation policies for cluster systems under varying workload,"We analyze two software rejuvenation policies of cluster server systems under varying workload, called fixed rejuvenation and delayed rejuvenation. In order to achieve a higher average throughput, we propose the delayed rejuvenation policy, which postpones the rejuvenation of individual nodes until off-peak hours. Analytic models using the well known paradigm of Markov chains are used. Since the size of the Markov model is nontrivial, automated specification generation, and the solution via stochastic Petri nets is utilized. Deterministic time to trigger rejuvenation is approximated by a 20-stage Erlangian distribution. Based on the numerical solutions of the models, we find that under the given context, although the fixed rejuvenation occasionally yields a higher throughput, the delayed rejuvenation policy seems to outperform fixed rejuvenation policy by up to 11%. We also compare the steady-state system availabilities of these two rejuvenation policies.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276563,no,no,1485873338.43838
Structure and Kinematics Decoupling Analysis of a Novel 3D Translations Spatial Parallel Robot Mechanism,"In this paper, a novel spatial parallel robot mechanism that can carry out three-dimensional translations was proposed. Based on topology structure design theory of robot mechanism, the structure and motion output were analyzed. The DOF and the coupling coefficient were calculated. According to the characteristics of the mechanism, the forward and inverse position models of mechanism were presented by using of the coordinates transformation theory and projection theory of analytic geometry. Depend on the conclusions of the forward solution, the decoupling of the input and output was analyzed. The kinematics decoupling characteristic of the mechanism was simulated and verified by ADAMS software. The research provided theory foundation for future study and industrial application.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5375757,no,no,1485873338.438379
A Survey and Empirical Study of Employee Satisfaction in Growing Small and Micro Businesses,"This paper, on the basis of the previous studies and the features of the growing small and micro businesses, presents the evaluation index system of employee satisfaction in these types of companies. It uses an anonymous questionnaire survey approach to survey the enterprise employee satisfaction, and adopts the AHP-Fuzzy method to get statistical analysis on samples, drawing an integrated staff satisfaction of 3.19 points in the growing small and micro businesses, namely"" almost satisfaction"", in which the"" work group satisfaction"" ranks the highest score (4.016 points), and the ""job reward satisfaction"" the lowest (2.571 points). In addition, using the same method to classify and calculate the employee satisfaction, it finds that in the employee age category, staff satisfaction of 30-year-old or over is the highest (3.451), In the light of the working seniority in the enterprise, namely the length of service category, the satisfaction of employees working 2-3 years in the company is the lowest(2.501), according to the staff qualifications category, the satisfaction of employees with lower College education is the lowest (2.932), in family status category, unmarried staff satisfaction is the lowest (2.941), according to the position in hierarchy category, general staff satisfaction is the lowest (2.901 points).",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382457,no,no,1485873338.438378
Can CEM software ever be validated?,"Validation of computational electromagnetics software has been a key topic for code developers for many years. The question arises: can software ever be validated meaningfully? Two cases are examined. Case 1: an electromagnetic code has been written to solve Maxwell's equations using the finite element method, including effects of eddy current losses and displacement currents (i.e. the full set of equations). The software is compared against a number of test problems, each having an analytic solution. It is shown that any required level of accuracy can be achieved by refining the mesh. Is this software therefore fully tested? Case 2: an electromagnetic code has been written to solve a subset of Maxwell's equations, neglecting displacement currents. It is used to model an induction heater and comparison is made with measured results. The results are found to be close to measurement but there are some differences. Is this software therefore in error? This paper discusses these two approaches to software validation in more detail and makes some recommendations.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1514247,no,no,1485873338.438377
Using Pregel-like Large Scale Graph Processing Frameworks for Social Network Analysis,"Pregel is a system for large scale graph processing developed at Google. It provides a scalable framework for running graph analytics on clusters of commodity machines. In this paper, we present several important undirected graph algorithms for social network analysis which fit within this framework. We discuss various graph componentisation methods, diameter estimation, degrees of separations, along with triangle, k-core and k-truss finding and computing clustering coefficients. Finally we present some experimental results using our own implementation of the Pregel framework, and examine key features of the general framework and algorithmic design.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6425724,no,no,1485873338.438375
Data Burst Statistics and Performance Analysis of Optical Burst Switching Networks with Self-Similar Traffic,"The self-similar model can reflect the Internet traffic property more precisely than Poisson model, the theoretical and simulation results of burst length distribution based on the time threshold assembly mechanism are shown in this paper. Moreover, the simulation result of OBS schedule algorithm under self-similar traffic model is reported. Both the analytic and simulation results show that burst length distribution is related to the length of assembly period; secondly, the assembly mechanism based on time period can reduce the self-similarity of traffic effectively; at last, the schedule algorithms in OBS have different performance based on the self-similar traffic.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287990,no,no,1485873338.436512
A Trust Model in P2P E-Commerce Systems,"In Peer-to-Peer (P2P) e-commerce systems, peers' features such as heterogeneity, anonymity and autonomy lead to some security problems, such as forging, slandering and collective cheating, which affect the quality of service a lot. A trust model in P2P e-commerce systems based on the recommendation is proposed, each peer in the system has a unique credibility of recommendation, two trust parameters for updating the credibility of recommendation are introduced, namely updating range and updating strength. The trust model proposes an algorithm to update the credibility of recommendation; a peer selects recommendation peers whose evaluation criteria are similar, evaluation criteria of peers are determined through the AHP method(Analytic Hierarchy Process). Simulations show that, the trust model can identify malicious peers, and improve the quality of service in P2P e-commerce systems effectively.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6478380,no,no,1485873338.436511
Analytic approximations for multilayer substrate coplanar-plate capacitors,"Closed form analytic approximations are proposed for complex impedance (capacitance and Q-factor) of capacitors formed by two coplanar rectangular conducting patches sandwiched between dielectric layers. The computations using proposed formulas are at least an order of magnitude faster in comparison with the commercially available software's, and can be used in optimisation procedures. The formulas are reversible, i.e. from measured impedance one can compute permittivity and losses of one of the dielectric layers.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1516957,no,no,1485873338.43651
Research on Method of Key Spare Parts Estimation and its Application,"Key spare parts management plays an important role in equipment management. To ensure the quality of equipment maintenance, the quality of spare parts must be guaranteed. So the evaluation problem of similar spare parts supplier must be solved effectively, by which the management and optimization of key spare parts can be realized. Firstly, the mathematic model of key spare parts evaluation is introduced in detail. Then based on the principle of AHP, the algorithm of key spare parts evaluation is established, which consists with four step: (1)establish AHP model; (2)construct comparative judgment matrix; (3)calculate the weight ranking and matrix consistency judgment under single rule; (4)calculate synthetic weight relative to target layer of element at each layer. It is a method of qualitative analysis and quantitative analysis on complex decision-making problems. Finally the specific example was given, and the result showed its effectiveness.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319606,no,no,1485873338.436509
The computation and application of random consistency index,"Analytic hierarchy process (AHP) is a method frequently used in mathematical modeling. Its consistency must be tested, after the positive reciprocal matrix of paired comparison is constructed. The formal test results of random consistency indices were of lower order. In practical application, for various factors are often involved, the computation of the test results of random indices of higher order is often complicated. Using Matlab software, we resolved the problem that positive reciprocal Matrix of any order be generated at random, thus we solved the problem that how to calculate random consistency index of high-order the better. The method is also applied to äóìthe mathematical modeling of coal-mining safety alarm systemäó, with ideal results.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5691977,no,no,1485873338.436508
The Application of the System Parameter Fusion Principle to Assessing University Electronic Library Performance,"Modern technology provides a great amount of information. But for computer monitoring systems or computer control systems, in order to have the situation in hand, we need to reduce the number of variables to one or two parameters, which express the quality and/or security of the whole system. In this paper, the authors introduce the system parameter fusion principle put forward by the third author and present how to apply it to assessing university electronic library performance combining with the Delphi technique and AHP",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4216535,no,no,1485873338.436507
Data-driven computer graphics,"Summary form only given. The field of computer graphics abounds with modeling and simulation problems. Among these are the representation of surface shape, the description of surface reflectance, the probabilistic modeling of small-scale variations, and the application of physics for simulating the dynamics of rigid and elastic materials. During its formative years, computer graphics has focused largely on developing algorithms and systems for performing efficient simulations that transform these analytic representations into images and animations. At present, the simulation framework for computer graphics is very mature. In last ten years, we have also witnessed significant technological developments in the areas of high-quality sensors and measurement devices. However, the data provided from these devices are frequently incompatible with the representations assumed by most computer graphics systems. The author explores new approaches to computer graphics that attempt to bridge the dichotomy between parametric and empirical modeling.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238244,no,no,1485873338.436506
Harnessing the Information Ecosystem with Wiki-based Visualization Dashboards,"We describe the design and deployment of Dashiki, a public Website where users may collaboratively build visualization dashboards through a combination of a wiki-like syntax and interactive editors. Our goals are to extend existing research on social data analysis into presentation and organization of data from multiple sources, explore new metaphors for these activities, and participate more fully in the Web's information ecology by providing tighter integration with real-time data. To support these goals, our design includes novel and low-barrier mechanisms for editing and layout of dashboard pages and visualizations, connection to data sources, and coordinating interaction between visualizations. In addition to describing these technologies, we provide a preliminary report on the public launch of a prototype based on this design, including a description of the activities of our users derived from observation and interviews.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290715,no,no,1485873338.436504
äóìAdvanced Modeling Grid Research: An overview of DOE's activitiesäó,"Abstract form only given. Shifting operational data analytics from a traditionally off-line environment to real-time situational awareness (e.g., visibility) to measurement-based, fast control will require significant advancements in algorithms and computational approaches. The Advanced Modeling Grid Research Program at the U.S. Department of Energy leverages scientific research in mathematics for application to power system models and software tools. This will result in a new class of decision support tools that will simulate dynamic events and help inform operators on real-time conditions to maintain stability.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6939841,no,no,1485873338.436503
New direction in project management success: Base on smart methodology selection,"Modern project management is a well-understood discipline that can produce predictable, repeatable results. The methodologies of modern project management are highly analytic, usually requiring automated tools to support them on large projects. Like most other disciplines, it is learned through both practice and study. Project management encompasses many different skills, such as understanding the interdependencies among people, technologies, budgets, and expectations; planning the project to maximize productivity; motivating others to execute the plan; analyzing the actual results; and reworking and tuning the plan to deal with the realities of what really happens as the project is executed. In order to manage a project and bring it to a successful completion, its project manager must have a complete understanding of the methodologies being used for the management of different parts of the project. Managers prefer specific project methodology, while resist and face difficulties for an opportunity to manage another project with different methodology as they donäó»t know how much commonality exists between the preferred and the new required methodology.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631556,no,no,1485873338.436502
Sunfall: A Collaborative Visual Analytics System for Astrophysics,"Computational and experimental sciences produce and collect ever- larger and complex datasets, often in large-scale, multi-institution projects. The inability to gain insight into complex scientific phenomena using current software tools is a bottleneck facing virtually all endeavors of science. In this paper, we introduce Sunfall, a collaborative visual analytics system developed for the Nearby Supernova Factory, an international astrophysics experiment and the largest data volume supernova search currently in operation. Sunfall utilizes novel interactive visualization and analysis techniques to facilitate deeper scientific insight into complex, noisy, high-dimensional, high-volume, time-critical data. The system combines novel image processing algorithms, statistical analysis, and machine learning with highly interactive visual interfaces to enable collaborative, user-driven scientific exploration of supernova image and spectral data. Sunfall is currently in operation at the Nearby Supernova Factory; it is the first visual analytics system in production use at a major astrophysics project.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389026,no,no,1485873338.434529
A data access framework for integration to facilitate efficient building operation,"Building Automation and Control Systems (BACs) are used to manage the day-to-day functions, operation and maintenance of a huge diversity of equipment within facilities of varying size and function. These systems are developed by a large number of hardware and software manufacturers who produce proprietary products designed to solve specific problems. As a result a number of different BACS can be operating within a single facility controlling various devices and producing significant quantities of data. This data can prove difficult to access due to the proprietary nature of the individual applications, thereby limiting the potential for a holistic view of a facilities operation and limiting the scope for big data analytics. This paper proposes the implementation of a Data Access Framework to coalesce the disparate information sources that can exist within a facility via OPC into a tagged database with particular focus on Heating Ventilation and Air Conditioning (HVAC) as a test case.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005265,no,no,1485873338.434528
A three step process to design visualisations for GeoTemporal analysis (VAST 2014 Mini Challenge 2),"Given vehicle tracking data, loyalty and credit card logs of employees from a fictitious company, GAStech, participants of VAST 2014 mini challenge 2 were tasked to extract the common daily routine of employees and identify any suspicious activities that may be present in the data. In this paper, we reflect on our analysis procedure focusing on each step of the process that contributed to problem solving. Accordingly, we describe the features incorporated into our software at each stage of the process and justify the design decisions that were made. Inspired by DiBiase's approach to visual analysis [1], our procedure consists of three stages (Fig. 1). With off-the-shelf software, such as R and QGIS, we conducted exploratory data analysis [2] to generate a diverse range of insights. The insights were evaluated based on their relevance to the given task. They were used to formulate hypotheses and data task abstraction [3] resulting in a set of complementary tools comprising of an origin-destination map, a timeline and a flow diagram that we developed in processing. These tools were not designed to function as an integrated software package but were treated as rapid prototypes that would afford more flexibility in the design and development cycle [4].",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042560,no,no,1485873338.434527
Tradeoff and Sensitivity Analysis of a Hybrid Model for Ranking Commercial Off-the-Shelf Products,"Despite its popularity, The COTS-based development still faces some challenges, in particular the evaluation and selection process in which uncertainty plays a major role. A hybrid model, composed of the analytic hierarchy process (AHP) and Bayesian belief network (BBN), is proposed to evaluate and rank various COTS candidates while explicitly considering uncertainty. Several input parameters such as weights assigned to evaluation criteria, relative scores for various COTS candidates, and prior belief about the satisfaction of various attributes associated with the evaluation criteria need to be estimated. The estimation process of these input parameters is subject to uncertainty that limits the applicability of the modelpsilas results. In this paper, we apply sensitivity analysis to check the validity and robustness of the model. Further, we apply tradeoff analysis to explore the impact of relaxing one criterion in order to achieve an increase in another criterion that is considered as more desirable in a particular project context. A digital library system is used as a case study to illustrate how the proposed tradeoff and sensitivity analysis was performed.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4839238,no,no,1485873338.434526
Monitoring and Control System used in Microalgae Crop,"This paper presents a monitoring and control system developed with the purpose to increase the reproducibility of microalgae production processes, achieving more effectiveness of casual-analytic physiologic research, the verification of process models and the development of bioprocess. It also controls of form web the following variables: temperature and photoperiod, the equipment was tested on a Haematococcus pluvialis crop. The settings of experimentation were: photoperiod 18/6 (light hours/dark) given by the extensions of blue SMD LEDs (380<;‘é<;470 nm), the agitation speed of 720 RPM and temperature of 24ŒÁC. The settings for the controlled crop were the same except the light that, for this case, was natural. It was found that the system is able to control automatically the process variables permitting the continuous monitoring of the experiments, removing barriers of time and place. At an institutional level, the system has brought benefits allowing the decongestion of places as saving time for both students and researchers. At a level of bioprocesses of researching, it was determined that both kinetic of microalgae growth and quantity of astaxanthin that is produced are incremented when the crop is illuminating by short wavelength, blue light.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272485,no,no,1485873338.434525
RF energy harvesting design using high Q resonators,"This paper presents a new method for rectifying low ambient radiation sources to supply autonomous measurement systems such as microsystems. An impedance transformation with high quality factor (Q) in front of a Schottky-diode using a quartz resonator at 24 MHz is presented. In addition to an analytic computation and nonlinear simulation of the rectifier circuit with advanced design system (ADS) software, the circuit was built and the efficiency of the RF energy harvesting system was measured. The design has been optimized to achieve maximum sensitivity. The measurement results show a sensitivity of -30 dBm (1 muW) for DC output voltage of 1 V and an efficiency of more than 22 %.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5307869,no,no,1485873338.434523
An architecture for efficiently establishing the classroom cloud,"Cloud computing is the key for Big Data analytics. Apache Hadoop makes it feasible to process the immense data set. An experimental environment with flexibility is required to do education and research with Big Data. However, this solution brings with the challenges that are how to manage large number of computers in the Hadoop cluster, and how to reuse the idle computer resources. In this paper, we propose the classroom cloud infrastructure to overcome the challenges. With this infrastructure, we can reuse the idle computer resources in the classroom for education and research. The case study of such a classroom cloud implemented in YZU is presented and discussed to help reader who can follow architecture to set up their personal Hadoop experimental environment in the classroom.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890805,no,no,1485873338.434522
Prioritisation and Selection of Software Security Activities,"Software security is accomplished by introducing security-related activities into the software development process or by altering existing activities so that security is taken into account. Since the importance of software security has only relatively recently received the recognition it deserves, security is not ingrained into the development processes in common use today. A variety of approaches to software security have been proposed, but they rarely support developers in determining which security activities are appropriate for them and which they should choose to implement. An exception to this rule is the sustainable software security process (S<sup>3</sup>P). This paper describes the final step of the S<sup>3</sup>P, which helps developers estimate the cost of security-related activities and select the combination of security activities that best suits their needs. This is accomplished by applying the analytic hierarchy process and an automated search heuristic, scatter search, to the models created as part of the S<sup>3</sup>P.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5066474,no,no,1485873338.434521
A Systematic Approach to Model Checking HumanäóñAutomation Interaction Using Task Analytic Models,"Formal methods are typically used in the analysis of complex system components that can be described as äóìautomatedäó (digital circuits, devices, protocols, and software). Human-automation interaction has been linked to system failure, where problems stem from human operators interacting with an automated system via its controls and information displays. As part of the process of designing and analyzing human-automation interaction, human factors engineers use task analytic models to capture the descriptive and normative human operator behavior. In order to support the integration of task analyses into the formal verification of larger system models, we have developed the enhanced operator function model (EOFM) as an Extensible Markup Language-based, platform- and analysis-independent language for describing task analytic models. We present the formal syntax and semantics of the EOFM and an automated process for translating an instantiated EOFM into the model checking language Symbolic Analysis Laboratory. We present an evaluation of the scalability of the translation algorithm. We then present an automobile cruise control example to illustrate how an instantiated EOFM can be integrated into a larger system model that includes environmental features and the human operator's mission. The system model is verified using model checking in order to analyze a potentially hazardous situation related to the human-automation interaction.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5735232,no,no,1485873338.43452
Data analytics for game development: NIER track,"The software engineering community has had seminal papers on data analysis for software productivity, quality, reliability, performance etc. Analyses have involved software systems ranging from desktop software to telecommunication switching systems. Little work has been done on the emerging digital game industry. In this paper we explore how data can drive game design and production decisions in game development. We define a mixture of qualitative and quantitative data sources, broken down into three broad categories: internal testing, external testing, and subjective evaluations. We present preliminary results of a case study of how data collected from users of a released game can inform subsequent development.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032557,no,no,1485873338.434519
Real-time autonomous video enhancement system (RAVE),"The ability to autonomously enhance low-quality or corrupted streaming video data is essential in a number of important civilian and defense scenarios. Applications include visual surveillance, motion picture restoration, and remote control of unmanned aerial vehicles. We have developed a prototype of RAVE: real-time autonomous video enhancement system. It consists of a suite of video artifact detection algorithms and corresponding correction algorithms. The system is autonomously controlled by an intelligent software agent. Our prototype has been successfully validated on several video sequences from different application domains and is being matured into a fully-functional, real-time embedded system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1039951,no,no,1485873338.432592
Extending Always Best Connected Paradigm for Voice Communications in Next Generation Wireless Network,"Selecting transparently a proper network connection for voice communication will be a fundamental requirement in future multi-mode heterogeneous wireless network. This paper extends always best connected (ABC) to a fine-grain paradigm called always best network connection (ABNC) to address this issue. Instead of selecting a best access network as in conventional ABC, ABNC enable users to select a best network connection, which consists of source and destination access network pair, to satisfy quality constraint and users' preference. To support ABNC, we develop a user profile to specify network connection priority. Meanwhile we extend SIP and propose a network selection information service (NSIS) based on MIH (media independent handover) to collect information of both source and destination access networks for decision making. Finally, analytic hierarchy process (AHP) is used to recommend a network connection with assistance of user profile and NSIS. An example is illustrated to show that AHP can successfully select a good network connection that fulfill the requirement of users.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4482788,no,no,1485873338.432591
InterJoin: Exploiting Indexes and Materialized Views in XPath Evaluation,"XML has become the standard for data exchange for a wide variety of applications, particularly in the scientific community. In order to efficiently process queries on XML representations of scientific data, we require specialized techniques for evaluating XPath expressions. Exploiting materialized views in query processing significantly enhances query processing performance. We propose a novel view definition that allows for intermediate (structural) join results to be stored and reused in XML query evaluation. Unlike current XML view proposals, our views do not require navigation in the original document or path-based pattern matching. Hence, they are evaluated significantly faster and are easily costed as part of a query plan. In general, current structural joins cannot exploit views efficiently when the view definition is not a prefix (or a suffix) of the XPath query. To increase the applicability of our proposed view definition, we propose a novel physical structural join operator called InterJoin. The InterJoin operator allows for joining interleaving XPath expressions, e.g., joining //A//C with //B to evaluate //A//B//C. InterJoin allows for more join alternatives in XML query plans. We propose several physical implementations for InterJoin, including a technique to exploit spatial indexes on the inputs. We give analytic cost models for the implementations so they can be costed in an existing XML query optimizer. Experiments on real and synthetic XML data show significant speed-ups of up to 200% using InterJoin, and speed-ups of up to 400% using our materialized views",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1644293,no,no,1485873338.43259
Software analytics: Achievements and challenges,"A huge wealth of various data exist in the practice of software development. Further rich data are produced by modern software and services in operation, many of which tend to be data-driven and/or data-producing in nature. Hidden in the data is information about the quality of software and services or the dynamics of software development. Software analytics is to utilize a data-driven approach to enable software practitioners to perform data exploration and analysis in order to obtain insightful and actionable information; such information is used for completing various tasks around software systems, software users, and software development process. This tutorial presents achievements and challenges of research and practice on principles, techniques, and applications of software analytics, highlighting success stories in industry, research achievements that are transferred to industrial practice, and future research and practice directions in software analytics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606753,no,no,1485873338.432589
Information seeking in academic learning environments: an exploratory factor analytic approach to understanding design features,"Traditional information retrieval (IR) systems perform retrieval by ""closely matching"" a query to a set of documents objectively without considering users' contexts. We aim to enhance objective relevance and address its limitations by taking a quantitative, subjective relevance (SR) approach. SR provides suitable theoretical underpinnings as it focuses on a document's relevance for users' needs. Our present work builds on an initial study where features supporting students' evaluations of subjective relevance of documents were elicited. Using digital libraries as an example of IR systems, we designed a survey form based on elicited SR features and conducted a quantitative, pilot study with 465 university students. The pilot study was exploratory and aimed to understand students' preferences for features as they completed a task in a digital library. Exploratory factor analysis (EFA) was used to analyze the data as it removed redundant features and identified relationships so that groups of important features supporting students' IR interactions could be discovered to provide rationale for designing IR interfaces supporting SR",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4119151,no,no,1485873338.432588
AHP-based prioritization of microgrid generation plans considering resource uncertainties,"The growing focus on the energy efficiency promotion, deployment of renewable energy resources, and serving the electricity in more reliable ways have recently resulted in a remarkable attention on microgrids. Among different electricity generation options available for a microgrid, the one compromising both technical and economical aspects is of a great desire. Taking into account the existing uncertainties associated with energy resources, this paper evaluates various distributed generation (DG) configuration plans for a microgrid using analytical hierarchical process (AHP) approach. A variety of DG resources including photovoltaic and wind generation are incorporated in the analysis. Meanwhile, the uncertainties associated with solar radiation, wind speed, and fuel prices are considered. For the sake of numerical analysis, DG configuration plans are generated with the HOMER software; thereafter, the plans are assessed with the Expert Choice software that supports the AHP approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733800,no,no,1485873338.432587
Information as a Service in a Data Analytics Scenario - A Case Study,"In this work we present a case study of a SOA realization exercise at a business information provider firm, which deals with disparate sources of data in-order to provide reliable reports to its clients. Unlike typical enterprise scenarios, where applications are required to be service enabled, the key requirement here was to service enable its data acquisition, quality check, reporting and other processes which are either mostly manual or ETL based workflows. This paper also addresses how shared services, business processes, rules, and semantics are used to provide quality and agility to the internal processes many of which are entirely dependent on the type of data received. The case and the scenario are chosen specifically to emphasize the fact, that mere web-services implementation does not lead to service oriented architecture, but it is the appropriate usage of them.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670228,no,no,1485873338.432586
Mining Twitter using cloud computing,"Today Social Networks have an undeniable impact on how users communicate search and share data online. This data has opened up new venues of research on sociology. Unfortunately mining and analyzing this large amount of data can be a difficult task and expensive in terms of computational resources. With the advent of cloud computing, data mining and analysis can be made more accessible due to its cost-effective computational resources. As a source of large data, we propose to use data from Twitter considering both its popularity and the open nature of its API. A large study of information propagation within Twitter reveals that a majority of users are passive information consumers and do not forward the content to even their own network. In this paper, we propose and demonstrate the usage of cloud computing platforms as a possible solution for mining and analyzing large amounts of data, while inspecting the Retweet Graph of a user to calculate their influence within their network.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141241,no,no,1485873338.432584
An intelligent approach to CAM software selection,"In this paper, an intelligent approach is presented to help the companies select most suitable CAM software for their current and future needs. For this purpose, fuzzy AHP method is used to carry out CAM selection process more effectively, easily and applicable for a company. Shortly, the objectives of the research are; to define a step-by-step approach for an effective CAM software selection. The proposed approach is also illustrated on a case study.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417855,no,no,1485873338.432583
The improvement of the performance of DS-UWB system based on a switched chaotic sequence,"This paper proposes a chaotic PN (pseudo-noise) sequence, which is generated by a novel switched chaotic system. Its fifteen performances are analyzed via current international standard NIST, which is working in the Linux operating system. The analytic results indicate that the switched chaotic sequence can pass NIST test and satisfy the command of DS-UWB system. Also the model of chaos-based DS-UWB communication system is established in the purpose of analyzing the performance of its reception. Using the switched chaotic sequence as spread spectrum address code and simulating its performance with the MATLAB software, at the same time assuming ideal synchronization at the receivers. Simulation results show that the new chaotic sequence is a good PN sequence for bit error rate (BER) performance of the DS-UWB system and more excellent than the conventional sequences such as the m sequence.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581879,no,no,1485873338.432582
On the detection probability of parallel code phase search algorithms in GPS receivers,"The first stage of the signal processing chain in a Global Positioning System (GPS) receiver is the acquisition, which provides for a desired satellite coarse code phase and Doppler frequency estimates to subsequent stages. Thus, acquisition is a two-dimensional search, implemented as demodulation and non-coherent correlation. For a certain Doppler estimate, software-defined GPS receivers typically compute the correlation for all time lags in parallel. One way to detect the presence of a signal is by comparing the ratio between the largest and the second largest correlation peak against a threshold. For this type of receivers, the detection and false alarm probabilities are derived. Interestingly, the false alarm probability is independent of the noise power spectral density, which allows a fixed threshold setting. The analytic results are verified by a series of simulations.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5672040,no,undetermined,0
Carbon management in assembly manufacturing logistics,"In this paper, we present the IBM Carbon Analyzer Tool, a software solution that models and quantifies carbon emissions and explores ways to reduce emissions through advanced analytics. The tool is designed to manage carbon emissions associated with the support logistics for an assembly manufacturing operation. The tool has four analytical modules. A shipment analysis module calculates carbon emissions from transportation activities and analyzes opportunities for reducing emissions by changing fuel types of vehicles and using larger vehicles that permit consolidated shipments. A sourcing analysis module compares sourcing alternatives, including changes to supplier locations, routing of shipments, frequency of orders, and transportation modes. A scenario analysis module explores various consolidation policies to minimize transportation, inventory, and carbon costs, subject to inventory availability requirements. A sensitivity analysis module quantifies the effects of changes to uncontrollable and uncertain inputs, such as manufacturing demand for components, carbon prices, and supplier reliability. The tool makes use of a Javaä‹¢-based graphical user interface and an IBM DB2Œ¬ (Database 2ä‹¢) platform to manage input and output data. A pilot implementation of the solution, using actual customer data, showed that emissions and transportation costs can be reduced simultaneously by optimizing vehicle use, fuel types, and shipment consolidation. Achieving a 20%äóñ30% reduction in emission was possible with minimal cost increase.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429021,no,undetermined,0
On the Processor Sharing Properties of File Transfers in a WLAN Testbed,"802.11-based WLAN deployments have become a commodity to provide today's wireless Internet access. In this paper, we conduct a practical study on the performance of FTP file transfers over real WLAN equipment. To this end, we propose a new analytic model that translates the highly complex dynamics of the FTP/TCP/IP/MAC-stack, and their interactions, into a single parameter, which will be called the effective load. The effective load is used to describe the flow-level behavior of FTP-based file transfers over WLANs without admission control as a Processor-Sharing (PS)-model. Next, despite the fact that PS models are heavily used in modeling flow-level performance in WLAN networks, an extensive validation of such models with real equipment has not been conducted. Motivated by this, we validate in the present paper our analytic model by comparing the model-based response times against the outcomes obtained from a testbed environment. The results show (a) that the obtained mean download times are fairly insensitive to the file-size distribution, as suggested by the PS-model, and (b) that the model leads to accurate predictions over a broad range of parameters combinations, including different file-size distributions and light- and heavy-load scenarios.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615513,no,undetermined,0
Calibrating Resource Allocation for Parallel Processing of Analytic Tasks,"Cloud Computing, the long-held dream of computing as a utility, has the potential to transform a large part of the IT industry, making software even more attractive as a service and shaping the way IT hardware is designed and purchased. In this environment, any application needs a model of to achieve elasticity and the illusion of infinite capacity requires each of these resources to be virtualized to hide the implementation of how they are multiplexed and shared. Given the nature of parallel processing dynamic, how to assign numbers of servers, CPUs, cores to the tasks have great impacts to the resource utilization of a PaaS (Platform as a Service) provider. In this paper, we face the challenge in automated calibration of resource allocation for parallel processing of analytic tasks. The proposed framework does not assume availability of data statistics and application semantics but probeable tradeoff between parallelism benefits and overheads. To implement it, a Sampling-then-Calibrating algorithm is presented to sample the runtime statistic information and calibrate the resource allocation accordingly. The experiments validate effectiveness of our approach.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342096,no,undetermined,0
Predictive Analytics Using a Blackboard-Based Reasoning Agent,"Significant increase in collected data for investigative tasks and the increased complexity of the reasoning process itself have made investigative analytical tasks more challenging. These tasks are time critical and typically involve identifying and tracking multiple hypotheses; gathering evidence to validate the correct hypotheses and eliminating the incorrect ones. In this paper we specifically address predictive tasks that are concerned with predicting future trends. We describe RESIN, an AI blackboard-based agent that leverages interactive visualizations and mixed-initiative problem solving to enable analysts to explore and pre-process large amounts of data in order to perform predictive analytics. Our empirical evaluation discusses the advantages and challenges of predictive analytics in a complex domain like intelligence analysis.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5616323,no,undetermined,0
Priority Queueing with Finite Buffer Size and Randomized Push-Out Mechanism,The non-preemptive priority queuing with a finite buffer is considered. We introduce a randomized push-out buffer management mechanism which allows to control very efficiently the loss probability of priority packets. The packet loss probabilities for priority and non-priority traffic are calculated using the generating function approach. For the particular case of the standard non-randomized push-out scheme we obtain explicit analytic expressions. The theoretical results are illustrated by a numerical example.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473996,no,undetermined,0
Proposal of Transmission Control Methods with Multihopped Environments in Cognitive Wireless Networks,"Remarkable wireless networks technology developments have made us to expect the realization of new applications like the advanced traffic system, the disaster prevention system, and the adhoc network system. However the resources of wireless bandwidths are not enough to use for such new applications because it is not efficient usage. Therefore, it is necessary to develop with new efficient wireless transmission methods like cognitive wireless network. In this paper, the transmission control methods in cognitive wireless network considering with multihopped environments are discussed. First, at the observation stage, the physical data such as user policy, electric field strength, bit error rate, jitter, latency, packet error rate, and throughput are observed. Then, at the decision stage, extended AHP (Analytic hierarchy process) is applied for decision making process with those parameters, and the suitable routing is decided. Finally, the action stage, one of the suitable link and route are chosen and changed links and networks. In the simulation, ns2 are used for the computational results to the effectiveness of the suggested transmission methods in cognitive wireless networks.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480838,no,undetermined,0
On stability of KIII model based on nonlinear dynamics,"As a bionic system simulating biologic olfactory structure and characteristics, KIII model is different from traditional artificial neural networks on pattern recognition. But there is no quantificational index to judge the stability of KIII model. Based on nonlinear dynamics index, the problem is researched in this paper and a quantificational index, Lyapunov exponent, is used to judge the stability of KIII model. Three stages in pattern recognition process of KIII model are analyzed quantificationally using wolf method. The calculational results show that KIII model can change from chaotic stage to stable stage quickly and presents obvious synchronization stage, which is consistent with the analytic result drawn from phase graph. It is also shown that Lyapunov exponent is an effective method to judge the stability of KIII model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572225,no,undetermined,0
Using the Multi-Attribute Global Inference of Quality (MAGIQ) Technique for Software Testing,"The Multi-Attribute Global Inference of Quality (MAGIQ) technique is a simple way to assign a single measure of overall quality to each of a set of similar software systems. Software testing activities can produce a wide range of useful information such as bug counts, performance metrics, and mean time to failure data. However, techniques to aggregate quality and testing metrics into a single quality meta-value are not widely known or used. The MAGIQ technique uses rank order centroids to convert system comparison attributes into normalized numeric weights, and then computes an overall measure of quality as a weighted (by comparison attributes) sum of system ratings. MAGIQ was originally developed to validate the results of analytic hierarchy process (AHP) analyses. Although MAGIQ has not been subjected to extensive research, the technique has proven highly useful in practice.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070708,no,undetermined,0
Notice of Retraction<BR>University network security risk assessment based On fuzzy analytic hierarchy process,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>In this paper, university network security evaluation system based on fuzzy AHP evaluation model, with examples given using the evaluation model for network security assessment procedures and methods. Evaluation results show that the proposed approach is feasible and guidance.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5623049,no,undetermined,0
Dependable integrated surveillance systems for the physical security of metro railways,"Rail-based mass transit systems are vulnerable to many criminal acts, ranging from vandalism to terrorism. In this paper, we present the architecture, the main functionalities and the dependability related issues of a security system specifically tailored to metro railways. Heterogeneous intrusion detection, access control, intelligent video-surveillance and sound detection devices are integrated in a cohesive Security Management System (SMS). In case of emergencies, the procedural actions required to the operators involved are orchestrated by the SMS. Redundancy both in sensor dislocation and hardware apparels (e.g. by local or geographical clustering) improve detection reliability, through alarm correlation, and overall system resiliency against both random and malicious threats. Video-analytics is essential, since a small number of operators would be unable to visually control a large number of cameras. Therefore, the visualization of video streams is activated automatically when an alarm is generated by smart-cameras or other sensors, according to an event-driven approach. The system is able to protect stations (accesses, technical rooms, platforms, etc.), tunnels (portals, ventilation shafts, etc.), trains and depots. Presently, the system is being installed in the Metrocampania underground regional railway. To the best of our knowledge, this is the first subway security system featuring artificial intelligence algorithms both for video and audio surveillance. The security system is highly heterogeneous in terms not only of detection technologies but also of embedded computing power and communication facilities. In fact, sensors can differ in their inner hardware-software architecture and thus in the capacity of providing information security and dependability. The focus of this paper is on the development of novel solutions to achieve a measurable level of dependability for the security system in order to fulfill the requirements of the specific application.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289385,no,undetermined,0
Logistics system performance evaluation of the fresh and live agricultural products with an application of analytic network process,"In the first place, this thesis, based on an analysis of the features of the logistics system of the fresh and live agriculture products, the affecting factors of its performance and influencing mechanism, considering the specificity of the whole fresh and live agriculture products logistics system, constructs an indicating system on the evaluation of logistics system performance of the fresh and live agriculture products. Secondly, based on an analysis of the applicability of the analytic network process, this paper, analyses the dependent network process relationship between different indicators in the evaluation system of performance of the fresh and live agriculture products logistics system establishes a network construction of the fresh and live agriculture products and confirms partial weight and overall weight. Practically, through a survey of the relative departments and professors in Chang Tai in Fujian province, this paper, with an empirical analysis of the software Super Decision, raised some managerial suggestions which can the improve the performance.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461452,no,undetermined,0
Measurement-based underwater acoustic physical layer simulation,"Simulation is important in the study of underwater networking because of the difficulty and expense of performing experiments. Underwater acoustic propagation is influenced by a wide variety of environmental factors, rendering analytic models complex, inaccurate, or both. Therefore, simulations based on models are of uncertain utility. In contrast, this simulator uses measured impulse response, CTD, noise, and transmission loss data in an effort to more realistically simulate the channel. The application layer generates data packets whose modulated waveforms are äóìmixedäó with the channel's properties and sent to a receiver implemented fully in software, where the simulated bit error rate (BER) is measured. For a shallow time-invariant test channel, this process results in a simulated BER that is, on average, within 3.34% of the true BER.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5664378,no,undetermined,0
Elastic scaling of data parallel operators in stream processing,"We describe an approach to elastically scale the performance of a data analytics operator that is part of a streaming application. Our techniques focus on dynamically adjusting the amount of computation an operator can carry out in response to changes in incoming workload and the availability of processing cycles. We show that our elastic approach is beneficial in light of the dynamic aspects of streaming workloads and stream processing environments. Addressing another recent trend, we show the importance of our approach as a means to providing computational elasticity in multicore processor-based environments such that operators can automatically find their best operating point. Finally, we present experiments driven by synthetic workloads, showing the space where the optimizing efforts are most beneficial and a radioastronomy imaging application, where we observe substantial improvements in its performance-critical section.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161036,no,undetermined,0
Mechanical stress analysis of port crane based on the condition of temperature field,"This paper systematically study the calculative theory of of large-scale port crane's the temperature field and thermal stress, Use the finite element software ANSYS to mode and analyse the large-scale port crane under the temperature field and temperature stress. Carry out a detailed study to results of the analysis and determine the impact of temperature stress on port crane structure. The analytic results show the temperature impact has a non-negligible effect on the structural strength of the port crane, which is likely to result in the main factors of the local structure's cracking.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5538057,no,undetermined,0
Migrating from Per-Job Analysis to Per-Resource Analysis for Tighter Bounds of End-to-End Response Times,"As the software complexity drastically increases for multiresource real-time systems, industries have great needs for analytically validating real-time behaviors of their complex software systems. Possible candidates for such analytic validations are the end-to-end response time analysis techniques that can analytically find the worst-case response times of real-time transactions over multiple resources. The existing techniques, however, exhibit severe overestimation when real-time transactions visit the same resource multiple times, which we call a multiple visit problem. To address the problem, this paper proposes a novel analysis that completely changes its analysis viewpoint from classical per-job basis-aggregation of per-job response times-to per-resource basis-aggregation of per-resource total delays. Our experiments show that the proposed analysis can find significantly tighter bounds of end-to-end response times compared with the existing per-job-based analysis.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342409,no,undetermined,0
Min-Max Based AHP Method for Route Selection in Cognitive Wireless Network,"Cognitive wireless network is one of efficient wireless transmission methods to solve today's wireless network problems like a lack of wireless radio resource or congestion of wireless bands. However, Cognitive wireless network still has some problems to realize like efficient algorisms, control methods, and technical problems to attain efficient transmission. In this paper, the Min-Max based AHP method for route selection in cognitive wireless network considering is especially discussed. First, at the observation stage, the physical data such as user policy, electric field strength, bit error rate, jitter, latency, packet error rate, and throughput are observed. Then, at the decision stage, AHP (Analytic hierarchy process) is applied for decision making process with those parameters. Finally, the action stage, one of the suitable links and routes is chosen and changed links and networks by the results of the proposal methods. In the simulation, ns2 are used for the computational results to the effectiveness of the suggested transmission methods in cognitive wireless networks.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635691,no,undetermined,0
E-Learning Platform Evaluation Using Fuzzy AHP,"E-learning platform evaluation is one of the most important issues in e-learning platform management. It is a multi-criteria decision problem including various factors. Traditional methods have many shortcomings. Fuzzy AHP model is established to solve this problem. A general evaluation method is provided and steps are presented. As a case study, the model is implemented and a detailed example is given to illustrate the effectiveness of this approach.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366686,no,undetermined,0
Miniaturized Implantable Wireless Sensor System for Realtime Measurement of Well-Being of Fishes,A miniaturized and implantable wireless underwater sensor system for realtime monitoring of vital parameters for sweet water fishes is presented. The wireless communication between the nodes and the gateway is realized by a RF radio link with a datarate of 250 kbps. The sensor nodes are directly implanted under the membrane of the fish and sample eight different sensors every second. With a lifetime of nearly one month with a coin cell battery the system delivers fine granulate data about the well-fare of the fish. Complex analytic-software can use the data for dedicated diagnostics.,2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558112,no,undetermined,0
Design Quality Analytics of Traceability Enablement in Service-Oriented Solution Design Environment,"This paper provides an artifact-pattern-matching framework and mathematical model to analyze the dynamic behaviors of the SOA solution design in model driven fashion and provide recommendations for optimal solution pattern enablement for solution artifacts. The artifact-pattern-matching system can be dynamically tuned based on the practitionerspsila final selections of there commendations. Specifically, we propose a set of solution patterns to guide SOA solution architects through the process of consuming and configuring SOA artifacts for composing SOA solutions. The resulting multi-dimensional cascading flagging method is also presented in this paper. As an example, impact analysis patterns are used as solution patterns to support traceability enablement. We present some future directions of leveraging reinforcement learning algorithms to enrich the design quality analytics of SOA solution.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5175917,no,undetermined,0
Modeling and analysis of software rejuvenation in a server virtualized system,"As server virtualization is used as an essential software infrastructure of various software services such as cloud computing, availability management of server virtualized system is becoming more significant. Although time-based software rejuvenation is useful to postpone/prevent failures due to software aging in a server virtualized system, the rejuvenation schedules for virtual machine (VM) and virtual machine monitor (VMM) need to be determined in a proper way for the VM availability, since VMM rejuvenation affects VMs running on the VMM. This paper presents analytic models using stochastic reward nets for three time-based rejuvenation techniques of VMM; (i) Cold-VM rejuvenation in which all VMs are shut down before the VMM rejuvenation, (ii) Warm-VM rejuvenation in which all VMs are suspended before the VMM rejuvenation and (iii) Migrate-VM rejuvenation in which all VMs are moved to the other host server during the VMM rejuvenation. We compare the three techniques in terms of steady-state availability and the number of transactions lost in a year. We find the optimal combination of rejuvenation trigger intervals for each rejuvenation technique by a gradient search method. The numerical analysis shows the interesting result that Warm-VM rejuvenation does not always outperform Cold-VM rejuvenation in terms of steady-state availability depending on rejuvenation trigger intervals. Migrate-VM rejuvenation is better than the other two as long as live VM migration rate is large enough and the other host server has a capacity to accept the migrated VM.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5722098,no,undetermined,0
Design for configurability: rethinking interdomain routing policies from the ground up,"Giving ISPs more fine-grain control over interdomain routing policies would help them better manage their networks and offer value-added services to their customers. Unfortunately, the current BGP route-selection process imposes inherent restrictions on the policies an ISP can configure, making many useful policies infeasible. In this paper, we present Morpheus, a routing control platform that is designed for configurability. Morpheus enables a single ISP to safely realize a much broader range of routing policies without requiring changes to the underlying routers or the BGP protocol itself. Morpheus allows network operators to: (1) make flexible trade-offs between policy objectives through a weighted-sum based decision process, (2) realize customer-specific policies by supporting multiple route-selection processes in parallel, and allowing customers to influence the decision processes, and (3) configure the decision processes through a simple and intuitive configuration interface based on the Analytic Hierarchy Process, a decision-theoretic technique for balancing conflicting objectives. We also present the design, implementation, and evaluation of Morpheus as an extension to the XORP software router.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4808477,no,undetermined,0
Modeling SW to HW task migration for MPSOC performance analysis,"Codesign choices of a system differ in terms of different hardware/software partitions, different types of architectural components, different communication architectures, etc. This paper presents an analytic method to estimate the gain on a system throughput when a software task is selected to be moved to hardware during the codesign process. The method is based on formal transformations of a Synchronous Data Flow Graph that models the application as well as its mapping to architecture. The proposed method is applied to the MJPEG decoder using the predictable MPSOC design tool SDF<sup>3</sup>.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5487581,no,undetermined,0
Demystifying Visual Analytics,"Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces. Technologies based on visual analysis are moving from research into widespread use, driven by the increased power of analytical databases and computer graphics hardware.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4797520,no,undetermined,0
Notice of Retraction<BR>Study of the distribution of initial stress for mechanical properties of guyed transmission tower,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>With the purpose of studying the mechanical properties of guyed transmission tower which is effected by initial stress of guy. The paper takes a guyed transmission tower as an example, building corresponding nonlinear finite element model by using general finite analytic software ANSYS, making comparison on mechanical properties of transmission tower structure which under different states of initial stress of guy, summing up the law which between the distribution of initial stress and the mechanical properties of the structure, providing the reasonable value for the design and constriction of tower structure, and ensuring self-reliance and stability of transmission tower structure.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5619290,no,undetermined,0
Demo VI Emergent analytics for enterprise management,"This talk will present the results of an Army sponsored project. The objective of the project was to develop a semantic knowledgebase designed to offer increased visibility into the Army portfolio. The first step in this project was to develop a seed ontology of the Army's enterprise architecture. The process used to develop the ontology was driven by a set of analytic questions that needed to be answered by queries to the knowledgebase. It was a collaborative, bottom-up effort from start to finish. The members of the team, ontologists and subject matter experts, worked together using a semantic wiki to gain consensus on the structure and meaning of the ontology. Once a suitable seed ontology was created, the project was increased in scale and the team began to collect large amounts of data, primarily about programs, software services, and capabilities within the Army's portfolio, as well as common business processes, missions and operational environments. This data came from many different systems within the Army and other DoD agencies, it was integrated using the enterprise architecture ontology. As the knowledgebase was populated with data, the ontology continued to evolve within the collaborative environment to respond to new data sets and data needs. The knowledgebase has been used to analyze the contents of the Army portfolio enabling the Army to make more informed budgetary decisions and also gain a vastly better understanding of the software services that they have and how they can be combined and used to offer new, vital capabilities to soldiers at the tactical edge.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5067441,no,undetermined,0
Deformation Analytic Computation of Throttle Slice of Shock Absorber,"Based on the governing differential equation for deformation of throttle slice, while satisfying required boundary conditions, an analytical formula for computing the deformation of throttle slice was presented through equivalency transformation, which is a concise, accurate and practical method for throttle slice design. The deformation at any radius was computed, compared with ANSYS FEA software by the simulation analysis. The result shows that the new method is an accurate analytical method for computing the deformation of throttle slice at any radius, suitable to use in the design of valves parameters.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5230954,no,undetermined,0
Defending Mobile Phones from Proximity Malware,"As mobile phones increasingly become the target of propagating malware, their use of direct pair-wise communication mechanisms, such as Bluetooth and WiFi, pose considerable challenges to malware detection and mitigation. Unlike malware that propagates using the network, where the provider can employ centralized defenses, proximity malware can propagate in an entirely distributed fashion. In this paper we consider the dynamics of mobile phone malware that propagates by proximity contact, and we evaluate potential defenses against it. Defending against proximity malware is particularly challenging since it is difficult to piece together global dynamics from just pair-wise device interactions. Whereas traditional network defenses depend upon observing aggregated network activity to detect correlated or anomalous behavior, proximity malware detection must begin at the device. As a result, we explore three strategies for detecting and mitigating proximity malware that span the spectrum from simple local detection to a globally coordinated defense. Using insight from a combination of real-world traces, analytic epidemic models, and synthetic mobility models, we simulate proximity malware propagation and defense at the scale of a university campus. We find that local proximity-based dissemination of signatures can limit malware propagation. Globally coordinated strategies with broadcast dissemination are substantially more effective, but rely upon more demanding infrastructure within the provider.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5062067,no,undetermined,0
Multi-core Video Analytics Engine (MVE) for security and surveillance applications,"The Multi-core Video Analytics Engine (MVE??) is an easily configurable, compact, high-performance processing architecture that can be used to implement complete video analytics solutions in a single FPGA embedded in intelligent surveillance cameras. With the steadily growing demand for increased processing power at lower costs for video analytics systems, especially intelligent cameras with embedded Digital Signal Processors (DSPs), traditional -software-only approaches are breaking down under the heavy burden of sheer computational complexity. MVE addresses this problem right at the heart, combining an inherently parallel multi-core processing architecture with embedded complex video analytics algorithms in a configurable SoC (System-on-Chip) solution. This initial commercial implementation of MVE is on the Xilinx Spartan-3A DSP 3400A FPGA chip. MVE is based on Eutecus' Cellular Multi-core Video Analytics (C-MVA??) processor - containing specialized image processing IP cores - which has been developed based on substantial research into cellular architectures that mimics the processing found in human vision. The performance of the MVE?? will be demonstrated in both indoor and outdoor security and surveillance applications.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5430331,no,undetermined,0
Decision of Air Conditioning Cold and Heat Sources Based on Analytic Hierarchy Process,"The optimization model of analytic hierarchy process (AHP) is introduced firstly, and then using it, air conditioning cold and heat sources are decided in a project of Qingdao City. Considering economy, technical condition, environmental impact and social benefit, each of four schemes of air conditioning cold and heat sources is analyzed quantitatively by Excel software. Finally sea water resource heat pump system is selected. The results show that the AHP Excel algorithm is convenience in applying, but the application of this method should be based on specific cases.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366343,no,undetermined,0
Proposal of Transmission Control Methods with User Oriented Environments in Cognitive Wireless Networks,"Remarkable wireless networks technology developments have made us to expect the realization of new applications like the advanced traffic system, the disaster prevention system, and the adhoc network system. However the resources of wireless bandwidths are not enough to use for such new applications because it is not efficient usage. Therefore, it is necessary to develop with new efficient wireless transmission methods like cognitive wireless network. In this paper, the transmission control methods in cognitive wireless network considering with cross layers including user policies are discussed. First, at the observation stage, the physical data such as user policy, electric field strength, bit error rate, jitter, latency, packet error rate, and throughput are observed. Then, at the decision stage, AHP (Analytic hierarchy process) is applied for decision making process with those parameters. Finally, the action stage, one of the suitable link is chosen and changed links and networks. In the simulation, ns2 are used for the computational results to the effectiveness of the suggested transmission methods in cognitive wireless networks.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447425,no,undetermined,0
Multidimensional data dissection using attribute relationship graphs,"Visual exploration and analysis is a process of discovering and dissecting the abundant and complex attribute relationships that pervade multidimensional data. Recent research has identified and characterized patterns of multiple coordinated views, such as cross-filtered views, in which rapid sequences of simple interactions can be used to express queries on subsets of attribute values. In visualizations designed around these patterns, for the most part, distinct views serve to visually isolate each attribute from the others. Although the brush-and-click simplicity of visual isolation facilitates discovery of many-to-many relationships between attributes, dissecting these relationships into more fine-grained one-to-many relationships is interactively tedious and, worse, visually fragmented over prolonged sequences of queries. This paper describes: (1) a method for interactively dissecting multidimensional data by iteratively slicing and manipulating a multigraph representation of data values and value co-occurrences; and (2) design strategies for extending the construction of coordinated multiple view interfaces for dissection as well as discovery of attribute relationships in multidimensional data sets. Using examples from different domains, we describe how attribute relationship graphs can be combined with cross-filtered views, modularized for reuse across designs, and integrated into broader visual analysis tools. The exploratory and analytic utility of these examples suggests that an attribute relationship graph would be a useful addition to a wide variety of visual analysis tools.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652520,no,undetermined,0
Comparative Analysis of System Evaluation Methods,"This paper analyzes the classifications, foundations of the theory, principles, advantages, disadvantages, and the main application scope of common system evaluation methods including fuzzy comprehensive evaluation, grey forecasting method, technology-economy analytical method, principal component analysis, discrimination analysis, analytic hierarchy process and intelligent evaluation. Then these system evaluation methods are analyzed comparatively from several different viewpoints. Finally some problems needing attention in choosing system evaluation methods are proposed and the conclusions are given.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5455497,no,undetermined,0
Notice of Retraction<BR>A study on chemical process control and optimization system development based on web services,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>In process system engineering field, plant-wide optimization becomes an important research issue in relation to model based control strategy and software aided solution integration. A platform with the function of simulation, data analysis, and fault diagnosis for operation system optimal control is proposed. The function of five levels in the proposed platform is discussed. Web services technology is employed to integration the different process analytic software. With the key web services application, the dynamic integration and optimization control for chemical product development are realized. Finally, a case study is carried out on TE process to verify the proposed platform performance.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553104,no,undetermined,0
Notice of Retraction<BR>Currency Structure Optimization in China's External Debt Management,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>Unceasing deepening along with the financial liberalization and integration of the world, the scale and speed of international capital flowing are also enlarging, which becomes a reduced factor to result in the regional economical and financial crises. Therefore, introducing the foreign capital with certain scale and structure safely and effectively to debtor countries appears very important. On the assumption that the debtor country only borrows 5 kinds of money at the beginning of the loan period, i.e., U.S. dollar, Japanese yen, HK. dollar, pound and euro and with the loan period of five years, we set up the multi-objective linear programming model of the optimal currency structure. Then Matlab mathematical software is used to solve the model. Finally, the analytic conclusions are given.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448814,no,undetermined,0
Notice of Retraction<BR>Designing and formulating organization performance evaluation model in AHP method based on EFQM criteria (case study),"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>Designing and formulating a comprehensive organization performance evaluation model based on European Foundation for Quality Management (EFQM) in AHP (Analytic Hierarchy Process) method is the main aim of the present research study. Evaluation is considered as one of the most important activities in each organization in a way that reformation of processes and procedures of doing activity without evaluation of results will be impossible. At the present research activity, AHP (Analytic Hierarchy Process) has been used as one of MADM (Multi-Attribute Decision Making) methods for the evaluation of performance of organizations through the application of EFQM (European Foundation for Quality Management) excellence model criteria. Also, Municipality of City of Tabriz has been selected as subjects for testing the presented model. In the same direction, seven districts of this municipality were selected as sample model. Necessary and required information were accumulated through questionnaire, interview and also taking advantage of data and library resources, details of which were analyzed and studied through the application of advanced Excel and Expert Choice 11.5 software package system. Eventually, various districts of this organization were evaluated and prioritized in terms of performance.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553098,no,undetermined,0
Chaos phenomenon in UC3842 current-programmed flyback converters,"This study studies the chaos phenomenon in a UC3842 current-programmed flyback converter. The primary side magnetizing inductance current of a transformer during the conversion operation is considered in continuous-conduction mode (CCM). The nonlinear behaviors of system operation from period-1 to doubling-bifurcation and to chaos is studied by varying the input voltage of the converter. First, mathematical models and discrete-time analytic solutions of the flyback converter are derived. Second, MATLAB/SIMULINK software to simulate the nonlinear behaviors of the converter. Furthermore, the actual circuit is implemented, and experiments are performed. The consistency of simulation and experimental results confirm that the proposed mathematical models and discrete-time analytic solutions accurately model the nonlinear dynamical behaviors of this converter.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5138190,no,undetermined,0
"Congestion location detection: Methodology, algorithm, and performance","We address the following question in this study: Can a network application detect not only the occurrence, but also the location of congestion? Answering this question will not only help the diagnostic of network failure and monitor server's QoS, but also help developers to engineer transport protocols with more desirable congestion avoidance behavior. The paper answers this question through new analytic results on the two underlying technical difficulties: 1) synchronization effects of loss and delay in TCP, and 2) distributed hypothesis testing using only local loss and delay data. We present a practical congestion location detection (CLD) algorithm that effectively allows an end host to distributively detect whether congestion happens in the local access link or in more remote links. We validate the effectiveness of CLD algorithm with extensive experiments.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5201404,no,undetermined,0
Analytic Scoring of Landscape CAD Course Based on BP Neural Network,"Course scoring is the major criteria for measuring the teaching effect and a basis for improving the education quality. Analytic scoring of landscape CAD (computer-aided design) course is influenced by various factors. Relationships among these factors are complex, some are nonlinear, even some are random and fuzzy. It is difficult to explain their internal relationships with traditional method. This research combines back-propagation neural network and DPS software to establish a three-layer BP neural network model, which took 60 examination papers of landscape CAD course as samples and made predictions on the score in accordance with five factors, including landscape design standard, landscape design innovation, computer cartography standard, drawing effect and workload. The results show that BP neural network model has strong nonlinear approximation ability, could truly reflects the nonlinear relationships between global score of landscape CAD course and main controlling factors of analytic scoring, with small error between predicted values and the measured values, relative error lower than 5%. In the future, when analytic scoring of the landscape CAD course obtains from the teachers, the global scoring can be calculated by BPNN model automatically. This method showed wide application prospect to the courses need analytic scoring.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5169443,no,undetermined,0
CAE software of twin-tubes shock absorber outer-characteristic,"The pathway throttle and local throttle loss of oil flow in telescopic shock absorber were analyzed, and the pathway loss coefficient and equivalent length of piston holes were studied. According to the thickness and the pre-deformation of throttle slice, and the throttle holes area, the velocity points of valve opening were researched and the analytic formulas of shock absorber velocity when valve opening were given. With the velocity points of valve opening, the model of shock absorber outer characteristic was established by piecewise linear math function. Based on this, the CAE software for shock absorber outer characteristic was developed. A practical example of simulation of shock absorber outer characteristic was given with this CAE software, and the performance test was conducted to verify the CAE software. The results show that the CAE software is reliable, and the values simulated is close to that tested.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5246863,no,undetermined,0
Research on Wavelet Threshold De-noising Method of Remainder Detection for Stand-Alone Electronic Equipments in Satellite,"Stand-alone electronic equipments are important parts of a satellite system, inside which remainders will bring great harm to reliability of the satellite system. Particle shock noise detection (PIND) is an effective method to detect the remainders in stand-alone electronic equipments. Signal to noise ratio (SNR) of detection signal is a bottleneck to improve processing accuracy and robustness of testing software. In this paper, key factors of wavelet threshold de-noising method were investigated and a wavelet threshold algorithm was proposed to improve the SNR of remainder detection signal of the stand-alone electronic equipments. The analytic results proved that the wavelet threshold method efficiently improved the de-noising effect.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635857,no,undetermined,0
Revisiting the Determination of Busway Impedance Using First Principles,"An approach using a combination of well-known circuit principles is used to convert the impedance matrix-determined for three-phase single lamination per phase busway-to industry-specified äóìaverageäó impedance values as specified by the National Electrical Manufacturers Association. This paper details how this approach can be used to predict average busway impedance with reasonable accuracy, and therefore help in reducing the number of physical tests and related costs in the design process. Four cases are provided for validation purposes. The results show good correlation with published data.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438810,no,undetermined,0
Adaptive Intra-Symbol SMSE Waveform Design Amidst Coexistent Primary Users,"An analytic approach is presented for optimizing spectrally modulated, spectrally encoded (SMSE) waveforms using independent selection of intra-symbol (within a symbol) subcarrier power and modulation order. The SMSE framework is well-suited for cognition-based, software defined radio (SDR) applications. By exploiting statistical knowledge about the spectral and temporal behavior of interfering signals, the inherent SMSE framework flexibility is leveraged to substantially increase system throughput while limiting coexistent interference. Results for a coexistent scenario are provided in which the analytic optimization of the SMSE waveform is demonstrated in the presence of multiple direct sequence spread spectrum (DSSS) signals. The results reveal significant performance benefits that demonstrate the potential of the SMSE framework to dynamically adapt to changing environmental conditions-key functionality required for future SDR implementations.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5198702,no,undetermined,0
Risk evaluation of EPC project based on ANP fuzzy comprehensive evaluation,"In order to assess and analyze the importance of the elements which have a significant impact on the risk, analytic network process is applied to establish the network structure model of risk evaluation index system for EPC project. And hypermatrix is utilized to compute and determine the weight of all the indexes. An EPC project risk evaluation model is constructed based on ANP (Analytic Network Process) -Fuzzy Comprehensive Evaluation to evaluate the integrated risk level. Validity of such EPC project risk evaluation model would be proved by a demonstration case analysis with help of Super Decision software. Results are shown at the last part of this paper.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5646427,no,undetermined,0
Scale and rotation invariant recognition of cursive Pashto script using SIFT features,"Cursive scripts such as Urdu, Pashto and Arabic contain large number of unique shapes called ligatures. Recognition of thousands of ligatures is challenging due to variations of various kinds including scaling, orientation, font style, spatial location/registration of ligatures and limited number of samples available for training. Accurate segmentation is a key challenge for analytic approaches, whereas holistic approaches suffer due to limitations of various feature representation schemes. In this paper, the use of SIFT descriptor has been proposed to evaluate its effectiveness for representing Pashto ligatures while overcoming above mentioned challenges in a holistic framework. The proposed approach is script independent and can be easily adapted to other cursive languages. A comparison of recognition results against classical methods such as PCA is provided to test the effectiveness of feature representation. Our research shows that SIFT descriptor perform better than classical feature representation methods such as PCA. The proposed recognition is holistic using ligature (word) based classification. We have tested 1000 unique ligatures (images) with 4 different sizes, along with their rotated images; and average recognition rate that is obtained is 74%.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5638470,no,undetermined,0
A Study on E-Government System Security Based on Fuzzy Analytic Hierarchy Process,"On the basis of analyzing various possible external threats and internal vulnerability in our country, this paper constructed a security index system of e-government system, introduced the algorithm of quantitative evaluation by using Fuzzy Analytic Hierarchy Process, and then proposed solutions to these threats.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5305680,no,undetermined,0
Selective Routing Protocol for Cognitive Wireless Networks Based on User's Policy,"Cognitive Wireless Network (CWN) is expected as one of the most efficient transmission methods to solve today's wireless problems like the lack of wireless bands or the efficient usage of limited wireless resources. However, CWN have not established effective transmission methods considered with QoS or upper layers protocols yet. In this paper, selective transmission control method in cognitive wireless network considering with cross layers including user policies is proposed. First, at the observation stage, the physical data such as user policy, electric field strength, bit error rate, jitter, latency, packet error rate, and throughput are observed. Then, at the decision stage, AHP (Analytic hierarchy process) is applied for decision making process with those parameters. Finally, the action stage, one of the suitable link is chosen and changed links and networks. Also, the route selecting method is proposed based on AODV protocols. When route changes come to need, considerable wireless routes are listed by AODV protocol including network conditions. Then, the following three methods are used for route selection; 1)Route selection so that Maximizing End-to-End throughput among all of the routes for CBR. 2) Route selection so that Minimizing End-to-End delay time among all of the routes for VoIP. 3) Route selection so that Optimize the policy based AHP for Web service. In our simulation, ns2 is used for the computational results to the effectiveness of the suggested transmission methods in cognitive wireless networks. The results showed the effectiveness of the proposed methods.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5628746,no,undetermined,0
A simulation setup to optimize particle flow velocimetry,"Particle Flow Velocimetry (PFV) has been introduced as a new ultrasound methodology to measure two-dimensional intraventricular flow patterns. It can potentially provide important new information on cardiac hemodynamics and function but how to optimize the whole process (such as frame rate, line density, contrast concentration) is still not clear. The aim of this study was therefore to build a simulation environment allowing to optimize this methodology by combining computational fluid dynamics (CFD) and ultrasound simulations. A 2D model of the left ventricular (LV) geometry was generated and meshed in order to be used as input to commercially available CFD software. An analytic description of a typical ventricular inflow velocity profile (showing an early and atrial filling phase) was used as a boundary condition at the inlet of the LV model and the dynamic flow field was simulated. Point scatterers were subsequently put at random positions within the model and their positions were updated over time based on the simulated flow field. From this dynamic scatterer field, ultrasound data could subsequently be obtained using a convolution-based model previously introduced by our lab. In order to test the simulation setup, RF signals from both a PW Doppler acquisition in the inlet portion of the model and a 2D color Doppler image sequence were simulated. For the PW Doppler spectrogram, the normalized RMSE of the estimated velocities relative to the CFD reference was 3.09%. Moreover, good qualitative agreement was found between the CFD and the color Doppler measurement. In conclusion, a simulation setup was constructed and shown to work correctly. It will be useful for optimizing PFV and for developing flow tracking methods in echocardiography further.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5441655,no,undetermined,0
Sensitivity of circular coils to conductivity changes: Analytical and numerical solutions,"The method of contactless conductivity imaging is based on the magnetic-induction magnetic-measurement principle. In this study, the sensitivity of the circular coil used for measuring magnetic fields, to conductivity changes is investigated. To do so, the electrical circuit model of the receiver coil and that of the conductive object is developed. The conductive object is modelled as a thin disk (cylinder) coaxially placed inside the circular coil. Another cylindrical object is assumed to be coaxially placed within the conductive object. It is assumed that, the conductive object is placed within a time varying but spatially constant magnetic field. An analytic formulation is developed for the determination of the sensitivity of the circular coil to the variations of the radius and conductivity of the second object (inhomogeneity). The validity of the model is tested with numerical simulations employing the ANSYS magnetic simulation software. The nonlinearity error between the analytic and numerical solutions is calculated as 1.38% of the full scale for ‘_=0.1, and 11.2% of the full scale for ‘_=0.2.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5479791,no,undetermined,0
A self-maintenance clustering algorithm based on decision model for space information networks,"Space information networks, which have become a popular research focus, are a new type of self-organizing networks. With fully considering characteristics distinguishing space information networks from common self-organizing networks such as terrestrial ad hoc networks and wireless sensor networks, the proposed clustering algorithm introduces a decision model based on analytic hierarchy process (AHP) to select cluster heads, and then forms non-overlapping k-hop clusters. The dynamical self-maintenance mechanisms take node mobility and cluster equalization into account. Besides of cluster merger/partition disposal, reaffiliation management and adaptive adjustment of information update period, mobile agents are used to migrate and duplicate functions of cluster heads in a recruiting way. Simulation results show that our clustering algorithm improves network scalability and is suitable for use in space information networks.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349087,no,undetermined,0
Simulation of parallel-plate pulsed plasma TeflonŒ¬ thruster based on the electromechanical model,"Electric propulsion devices have been used widely in space flight mission for its superior performance. As a kind of electromagnetic thruster, the parallel-plate pulsed plasma thruster (PPT) as well as its operation process is theoretically analyzed and discussed. Based on the electromechanical model of PPT-an idealized, quasi-steady, analytic model including the ablated mass process of Teflon, an interactive intelligent computer-aided software which is used to predict the influences of variations of several electric parameters (capacitor capacitance, initial voltage, circuit resistance and electron temperature) and configuration parameters (electrodes distance, electrode width) on PPT performance is designed and implemented in MATLAB environment. And an application example of Lincoln Experimental Satellite VI (LES-6) PPT shows that this software has characteristics of easy operation, friendly human-computer interaction interface, high display speed and precision for PPT preliminary design. Finally, certain deficiencies in the presented model are identified and addressed with considerations for future improvement.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5487276,no,undetermined,0
Skills evaluation of junior-level software talents,"Many software industrial association established in all provincial capital cities in China. Providing service for talents and company is their main responsibilities. The evaluation of software talents skills is multi-factor, multi-objective and multi-criteria decision-making problems. The current selection and evaluation method which the company used commonly is single, non-objective, non-universal and low input-output ratio. Base on a lot of investigation and research in typical software industry, the method of AHP (analytic hierarchy process) and FCEM (fuzzy comprehensive evaluation method) are used to evaluate the skills level of junior-level software talents. It makes evaluation results more objective and reasonable.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569577,no,undetermined,0
A Post Evaluation Technique for Engineering Project Investment Based on ANP-ENTROPY-TOPSIS,"Mutual influence always exists among the indices in engineering project investment evaluation system. Since ANP (Analytic Network Process) technique can scientifically reflect the mutual influence among the indices, it is introduced into this paper instead of AHP to overcome the defect that traditional AHP technique can not reflect the relationship among the indices. And in the weight determining process, entropy method is adopted in combination with ANP to avoid the respective unilateralism of subjective weight method and objective weight method. On this basis, TOPSIS method is applied in the comprehensive evaluation for the engineering project investment schemes to quantitatively rank these schemes. Finally, by means of Super Decision software, an example is provided to show the effectiveness and scientificity of this evaluation technique.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5305005,no,undetermined,0
Soft Decision Design of Spectrally Partitioned CI-SMSE Waveforms for Coexistent Applications,"Applicability of Spectrally Modulated, Spectrally Encoded (SMSE) waveform design has been expanded for future Cognitive Radio (CR)-based Software Defined Radio (SDR) applications. As previously demonstrated, the SMSE waveform design process can exploit statistical knowledge of PU spectral and temporal behavior to maximize SMSE system throughput (bits/second) while adhering to SMSE and Primary User (PU) spectral constraints. The capacity of SMSE systems is extended here using spectral partitioning with carrier-interferometry (CI) coding to increase SMSE waveform agility in the presence of a spectrally diverse transmission channel. By adaptively varying the modulation order and optimally allocating power within each spectral partition, inherent SMSE flexibility is more fully exploited and substantially increases system throughput while meeting Power Spectral Density (PSD) constraints. A coexistent scenario is provided in which the analytic optimization of the SMSE waveform is demonstrated while meeting spectral mask requirements. Results show that spectrally partitioned CI-SMSE waveforms have a significantly greater ability to adapt to varying spectral requirements.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5502446,no,undetermined,0
Soil water division by SPSS statistics analysis software in Hebei Province,"Water resource in Hebei Province is extremely deficient; however, there is plenty of precipitation so that the soil water is abundance. If the soil water there can be taken full advantage of, part of the water resource can be saved. In order to scientifically and rationally evaluate and develop the technology of exploiting the soil water in Hebei Province, to utilize the soil water adjusting measures to local conditions in different regions, but to adopt unified irrigation regime and irrigation requirement in the same region, and to be unified planning and management, the soil water should be divided into different kinds of soil water regions so that to utilize soil water sufficiently. In this paper, making reference to the measure of the water resource regionalization, basing on the information of the soil water in Hebei Province, the topography and physiognomy of the earth's surface, drought index, soil texture and vegetation were selected to make up of the indices system, the weight of each index was calculated by utilizing the analytic hierarchy process, and in the end SPSS (Statistical Product and Service Solutions) statistics analysis software was used to divide the soil water resource. Finally, the soil water resource in Hebei Province was divided into 8 subdivisions. It is necessary to utilize the soil water reasonably and efficiently.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580517,no,undetermined,0
Study of Influencing Factor Index System of tourism development for Henan based on AHP method,"An analytical way to reach the best decision is more preferable in many business platforms. In this study, Analytic Hierarchy Process has been applied for evaluating the impact factors of foreign exchange income by tourism market in Henan province. Moreover, the score methodology by specialists and MATLAB software are also used for getting the impact factors' weight scores. The results show that the top four among the evaluated impact factors are Signtseeing, Long-distance, Arrivals and Accommodation, which are the key factors influencing foreign exchange income by tourism market of Henan province.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655798,no,undetermined,0
Study of modeling and simulation of Flexsim-based inventory management system,"This paper examines the modeling and simulation process of an assembly line of a certain computer manufacturer. It employs the powerful re-develop technology of the Flexsim software to perform advanced simulation modeling, thereby eliminating the difficulties in the modeling of inventory management system and in its simulation optimization. This paper also presents the codes for advanced simulation. It has practical significance for facilitating the complex modeling of inventory management system, and provides managers with substantial analytic data and close-to-reality visual scenarios to make sound decisions.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5646120,no,undetermined,0
A Novel Visualization Technique for Electric Power Grid Analytics,"The application of information visualization holds tremendous promise for the electric power industry, but its potential has so far not been sufficiently exploited by the visualization community. Prior work on visualizing electric power systems has been limited to depicting raw or processed information on top of a geographic layout. Little effort has been devoted to visualizing the physics of the power grids, which ultimately determines the condition and stability of the electricity infrastructure. Based on this assessment, we developed a novel visualization system prototype, GreenGrid, to explore the planning and monitoring of the North American Electricity Infrastructure. The paper discusses the rationale underlying the GreenGrid design, describes its implementation and performance details, and assesses its strengths and weaknesses against the current geographic-based power grid visualization. We also present a case study using GreenGrid to analyze the information collected moments before the last major electric blackout in the Western United States and Canada, and a usability study to evaluate the practical significance of our design in simulated real-life situations. Our result indicates that many of the disturbance characteristics can be readily identified with the proper form of visualization.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4695829,no,undetermined,0
Supplier selection and evaluation models for a design change scheme in collaborative manufacturing environment,"A design change is commonly applied to change a portion of an existing product for the purpose of adding some functional values or reducing certain costs. In a global supply chain, a design change not only changes a portion of a product, but also changes the selection of suppliers. Therefore, it is important to evaluate a design change and its effect on the selection of suppliers in a collaborative manufacturing environment. In this research, a fuzzy analytic hierarchy process evaluation model is presented for evaluating the different design change alternative cases. First, a component relational matrix is modeled. If an initial component change is made, the other components that are affected by the initial component are identified and represented using a component relational matrix. A component relational matrix is used to obtain the relational values of the relational factors for the different suppliers. Second, a fuzzy analytic hierarchy process evaluation method is presented to establish the hierarchy of the indices and analyze the final total relational values of the relational factors for the different suppliers. The total relational values are compared to select the most suitable suppliers. Implementation and test results of a mobile example product are presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5668206,no,undetermined,0
Test Case Reusability Metrics Model,"As organizations implement systematic reuse test cases in software testing to improve productivity and quality, they must be able to measure reusability of test case and identify the most effective reuse strategies. This is done with Test Case Reusability Metrics Models(TCRMM). In this article we survey metrics of test case reusability, and provide reusability factors division that will help establish reusability assessment model. It combines four metrics factors into reusability metrics based on analytic hierarchy process. The prominent characteristic of this model is that the process of reusability metric is automated when the judgment matrix has been constructed.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645869,no,undetermined,0
A new method of performance evaluation for search engine,"In this paper, a user-oriented hierarchical analysis model is constructed so as to discuss briefly about the evaluation of search engine. According to the property that the ratio of corresponding elements is a constant in any two rows of consistency matrix, the indirect judgment information of the judgment matrix is comprehensively used to construct a constrained programming model in order to determine the ranking weight of judgment matrix. And the Genetic Algorithm is used to solve the model. Finally, an example is given to demonstrate this method. The results showed that weight dispersion got by this method is higher, and it is also much easier to identify the optimal plan.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5267949,no,undetermined,0
A Multiple-Criteria Approach to Ranking Computer Operating Systems,"To select an operating system, an organization must consider several essential characteristics during its initial evaluation process. The analytic hierarchy process (AHP) offers an appropriate solution; the author illustrates it with a realistic case study in which an organization evaluates and ranks Windows XP, Linux, and Mac OS X 10.4.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5173034,no,undetermined,0
Testing of a spatial impulse response algorithm for double curved transducers,"The spatial impulse response (SIR) method for solving the Rayleigh integral is a well known method for fast time response simulation of acoustic waves. Several analytical expressions have been found for simple transducer geometries such as rectangles and discs. However, no analytical solution is known for double curved transducers (DCT), i.e. transducers with both concave and convex radius. To calculate the SIR from such transducers Field II uses a far-field approximation by dividing the surface into smaller flat elements and then performs a summation of the response from all the elements using Huygen's principle. This calculation method involves several summations, and it relies on exact phase calculation to avoid numerical noise in the response. A stable analytical expression for the SIR would thus be beneficial to the Field II software as an alternative solver. A semi-analytic algorithm (SAA) has been developed, and it is the objective of this work to validate an analytical approximation of the algorithm as an alternative solver for Field II. Two approximations of a SAA that efficiently finds the SIR for DCT have been implemented into a MATLAB and a C-code environment. The root mean square (RMS) error of calculating the SIR using Field II and the C-implemented approximation are calculated relative to a high resolution solution obtained with MATLAB on a DCT, a linear concave, and a flat transducer. The computation time for solving a point 400 times is also found. Calculations are performed at sampling frequencies ranging from 100 MHz to 15 GHz in steps of 100 MHz. The transducer width is 250 ‘_m and the height is 10 mm. The C-implementation exhibits errors ranging from 4.9-10<sup>-1</sup> % to 0.91 % and Field II 0.0117 % to 0.94 %. A slight trade off between accuracy and computation time is found. Field II outperforms the SAA in computation time if high accuracy is not needed. However, if a higher accuracy is required, the SAA is the best model choice.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5935558,no,undetermined,0
A Multiple Criteria Decision Making Model for CNO Attack Scheme Evaluation,"Computer network operation (CNO) decision making activity includes three sub-activities: mission definition, scheme design and scheme evaluation. Scheme design produces many attack schemes of the same target. During scheme evaluation, attack schemes are evaluated and the comparatively optimal one is selected as course of action (COA) based on certain criterions. In this study, a multiple criteria decision making model is proposed to evaluate CNO attack schemes. Attack effects, costs and risks of attack tasks and attack paths are considered and evaluated to select an optimal scheme. As a multiple criteria decision making method that combines both qualitative and quantitative analysis, the analytic hierarchy process (AHP) is introduced and applied in the model. An experiment is conducted to illustrate the availability and effectiveness of the decision making model.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363559,no,undetermined,0
A model proposal for usability scoring of websites,"Usability has become a very critical factor for the success of a website. The study focuses on constructing a scoring model of websites that is based on observed usability test data. In the proposed model, the first usability test is designed including 4 important tasks. These tasks include finding information in the website about MMS regulations, calling fees, searching for IMEI numbers, and listing shop addresses. The usability test is followed by multi-attribute decision making model for the purpose of obtaining a usability score of the websites.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5223495,no,undetermined,0
The evaluation of the integrated ecological environment status of Shizuishan City based on Analytic Hierarchy Process,"Distinguish the importance degree of the elements affecting the ecological environmental status of ShiZuishan City by expert scoring, establish the evaluation index system and calculate the weight of each element contributing to affect the quality of the ecological environmental status based on the Analytic Hierarchy Process. The analysis result shows that the most important four elements affecting the ecological environmental status of ShiZuishan City are as follows: vegetation cover condition, geomorphological Characteristics, soil erodibility and pollution status. Sketch out the integrated geographical union using the remote sensing image, endow the attribute to every integrated geographical union according to the basic materials, and then, make the attribute values being dimensionless, assort the attributes into several classifications, finally, use the modeling tool in Arctoolbox module of the ArcGIS9.2 software to build weighted comprehensive evaluation model, draw the final evaluation map of the integrated ecological environment status of Shizuishan city by stacking the four elements weighted. The ecological environment status of Shizuishan City is graded into four ranks, excellent, good, general and worse. Based on the comprehensive evaluation result, putting forward some suggestions to Shizuishan City on promoting sustainable development of economy and society, for example, suggest Shizuishan City to accelerate the closing of the high water consumption, high energy consumption and high pollution enterprises, lay the development emphases on PingLuo country, construct tourism facilities around the lakes and wetlands within limited, prohibit exploiting the nature reserve on the Ho-lan Mountain and cancel the coal mines out in the nature reserve.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5690663,no,undetermined,0
The implementation of general evaluation model in the e-government,"At first, in this paper AHP and Fuzzy Synthetic Evaluation Method are abstracted with Mathematics tools and packaged then evaluation index is filled by the system administrator with semi-automatic mode applying open database technology the filled data are related to the packaged algorithm. Finally the paper implements the evaluation based on B/S.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5579167,no,undetermined,0
A meta-analytic review of current CALL research on second language learning,"To access the research methods and data analysis procedures employed by research studies investigating the applications of computer-assisted language learning (CALL) to enhance second/foreign language learning, the study made extensive search for scholarly articles published from 2000 to 2008. The contributions of quantitative research on computer-assisted second/foreign language learning were reviewed under the categories of research designs, data analysis techniques, skill areas/language taught and the trends of CALL types investigated. Results of the study showed that frequently used research methods were pre- post-test control group experimental and within subject experimental research designs. Findings of the meta-analytic review also indicated that new data analysis techniques were introduced into the field of computer-assisted second/foreign language learning research. The overall interest in implementing technology in second language learning was reflected by the increasing number of research studies on computer mediated communication (CMC) and Web-based tools. The study concluded that there was obviously still a pressing need among the research community for valid research designs and statistical analyses. It is recommended that to meet professional standards, more inferential statistics be applied to provide generalized and significant research findings.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5236335,no,undetermined,0
The Measurement Model for Enterprise Informationization Capability Maturity,"As a dynamic development process, informationization involves a development process from immaturity to maturity and from imperfection to perfection. Constant improvement of the informationization process is a must to make the final achievement. This paper, which focuses on the extension and expansion of the content and measurement system of the Enterprise Informationization Capability Maturity Model (EICMM), discusses the measurement analysis from the dimensions of the development level, development quality, development capacity and suitability of enterprise informationization based on the hierarchical framework for the enterprise informationization maturity. This paper analyzes in detail the concrete concepts and contents of the EICMM, provides corresponding measurement paths, and helps enterprises identify the problems behind informationization construction to achieve the process and level upgrade of informationized enterprises.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533051,no,undetermined,0
A Highway Express Passenger Transportation Company Evaluation System Based on the Evidential Reasoning Approach,"The existing researches on highway express passenger transportation (HEPT) have been studied, and an improved evaluation attribute system of the assessment to HEPT company is constructed. Considering the disagreement in group of experts and the majority of uncertainties in the process of assessment to HEPT company, an evidential reasoning (ER) approach based HEPT company assessment model is proposed. In this assessment model, the weight of each attribute is calculated through group analytic hierarchy process (GAHP), and the values of all quantitative and qualitative attributes are aggregated by the ER approach. At last, intelligence decision system (IDS) software is applied for the simulation of assessment to HEPT company, the result of which makes great contribution to the decision making by management department of passenger transportation.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5209703,no,undetermined,0
Amplifying performance of lever-type flexure hinge amplifying mechanism,"Two kinds of layout model of lever-type flexure hinge amplifying mechanism are presented. Then the forces of flexure hinge in the two layouts are discussed in detail and the displacement losses are calculated. Finally, the displacement losses and the amplifying factors of flexure hinge amplifying mechanism are analyzed by using the finite element software ANSYS. The analytic results demonstrate that the theoretical results are agreement with the finite element analytic results well and that the amplifying performance of layout A is better than that of layout B.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5274870,no,undetermined,0
An Analytic Network Process approach to the planning and managing of the energy politics,"The obtainment of electricity in a cheap, reliable and stable way on time with good quality is of high priority for the management of countries. In order to do so, in our country where the need to electrical energy is rapidly increasing, the planning of production and development of electrical energy and its execution is very important from the economical point of view. However, the analysis of the final consumption of the country, the determination of all the factors affecting the final demand, is a multicriteria decision making (MCDM) problem which requires taking several different factors in account. In this study, the determination of the scenarios of the socio-economic and technique development and the problem of evaluating the values calculated for these scenarios will be solved with analytic network process (ANP)'s software superdecisions which will provide more accurate results by taking the relations between the evaluation factors into account. The results will be evaluated and their conformity to this process will be researched.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5223855,no,undetermined,0
Quality Attributes Assessment for Feature-Based Product Configuration in Software Product Line,"Product configuration based on a feature model in software product lines is the process of selecting the desired features based on customers' requirements. In most cases, application engineers focus on the functionalities of the target product during product configuration process whereas the quality attributes are handled until the final product is produced. However, it is costly to fix the problem if the quality attributes have not been considered in the product configuration stage. The key issue of assessing a quality attribute of a product configuration is to measure the impact on a quality attribute made by the set of functional variable features selected in a configuration. Current existing approaches have several limitations, such as no quantitative measurements provided or requiring existing valid products and heavy human effort for the assessment. To overcome theses limitations, we propose an Analytic Hierarchical Process (AHP) based approach to estimate the relative importance of each functional variable feature on a quality attribute. Based on the relative importance value of each functional variable feature on a quality attribute, the level of quality attributes of a product configuration in software product lines can be assessed. An illustrative example based on the Computer Aided Dispatch (CAD) software product line is presented to demonstrate how the proposed approach works.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693189,no,undetermined,0
An Application of the AHP in Supplier Selection of Maintenance and Repair Parts,"The supplier selection of maintenance and repair parts is very important for the maintenance of complicated equipments, as it is a multi-item, multi-person and multi-criteria decision problem. Among the recorded literature, the analytic hierarchy process (AHP) has been recognized as an appropriate approach to solve the above problem, which helps several decision-makers with different conflicting objectives to arrive at a consensus decision. In this paper, an innovative model which based on the AHP method is formulated and it can be applied to provide a framework for the organization to select a supplier that satisfies the customer specifications. We also could change the assessment indicator system a little to apply the framework supplied by our selection model in any other industry.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5455238,no,undetermined,0
Reducing the digital divide of the electronic government of the 921 reconstruction areas in Taiwan,"With great efforts over the past two decades, Taiwan has become one of leading countries in E-government practice. People have benefited from the efficiency of E-government services and Taiwan government will develop the next stage E-government, which is integrated, innovative, real time, interactive, and personalized, to establish a virtually trusted society that connects each citizen via the Internet. At 1:47 a.m. on September 21, 1999, a massive earthquake measuring 7.3 on the Richter scale struck central Taiwan. This earthquake is a terrible disaster that causes tragic loss of life, severe property damage, and sharp decline in living standards and the regional prosperity. The earthquakes of the last decade also brought to light the importance of Earthquake Disaster Management (EDM) operations. As a result, there is an urgent call for applying digital services, broadening geographical service scope, enriching service options, and lowering costs for the reconstruction areas. This study employed the Analytic Hierarchy Process (AHP) to analyze the Taiwan government's supply and demand of the information services in the reconstruction areas to heed the call for reducing uneven opportunities on availability of information and telecommunication technology.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5603448,no,undetermined,0
Batch on-line analytics for every user,"Process Analytical Technologies (PAT), in particular Principal Component Analysis (PCA) for fault detection and Projection to Latent Structures (PLS) for end of batch quality prediction, are seen as pivotal techniques for improving process operation. A number of software packages are available today for off-line analysis. The difficulty for control or production engineers is that these tools are not designed for on-line operation. Many PAT issues may be addressed by tightly integrating analytic tools with the production and control system. This special session examines basic design requirements associated with batch analytics applied for on-line operation. The presentation delivers an in-depth look at the data processing requirements, calculations and limitations for the application of on-line analytics to a batch process. It will show how to achieve proper data alignment for different batches, a key requirement for building good statistical models, and how to use the model for on-line analysis. A typical batch operation in the chemical industry and a running simulation will be used to illustrate the advantages of on-line process analytics over traditional monitoring and control techniques. Details will be presented on the approach taken to integrate analytic tools and results into a commercial control system with examples of control and production operation screens.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5159786,no,undetermined,0
Automatic fault detection and diagnosis in complex software systems by information-theoretic monitoring,"Management metrics of complex software systems exhibit stable correlations which can enable fault detection and diagnosis. Current approaches use specific analytic forms, typically linear, for modeling correlations. In this paper we use normalized mutual information as a similarity measure to identify clusters of correlated metrics, without knowing the specific form. We show how we can apply the Wilcoxon rank-sum test to identify anomalous behaviour. We present two diagnosis algorithms to locate faulty components: RatioScore, based on the Jaccard coefficient, and SigScore, which incorporates knowledge of component dependencies. We evaluate our mechanisms in the context of a complex enterprise application. Through fault injection experiments, we show that we can detect 17 out of 22 faults without any false positives. We diagnose the faulty component in the top five anomaly scores 7 times out of 17 using SigScore, which is 40% better than when system structure is ignored.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5270324,no,undetermined,0
Applying the AHP Approach to Evaluate the Competition of Telecommunications Companies,"The rigorous competition has become a crucial challenge for Chinese telecommunications companies. This paper employs the analytic hierarchy process (AHP) approach to evaluate the competition of telecom companies. Specifically, the model proposed in the paper can be used to establish related factors and consequently their weights that are applied to calculate the competition index of telecommunications companies. In the case study, the paper employs the data from a questionnaire survey and the results indicate that the market shares have the highest weight and management is very important to build up the competition of telecommunications companies. Meanwhile, the results also find that using the AHP model can reduce time in evaluating competition of telecommunications companies.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364849,no,undetermined,0
Application of incomplete linguistic preference relations in predicting the success of ERP implementation,"This study applies an analytic hierarchical prediction model based on Multi-Criteria Decision Making with Incomplete Linguistic Preference Relations(InLinPreRa) to help the organizations become aware of the essential factors affecting the Enterprise Resource Planning(ERP), as well as identify the actions necessary before implementing ERP. The subjectivity and vagueness in the prediction procedures are dealt with using linguistic terms quantified in an interval scale [-t, t] . Then predicted success/failure values are obtained to enable organizations to decide whether to initiate ERP, inhibit adoption or take remedial actions to increase the possibility of successful ERP. The empirical results not only demonstrate the senior manager support degree, organizational and coordination are the three most important influential factors in the ERP initiative process, but also reveal the applicability and feasibility of reciprocal Incomplete Linguistic Preference Relation(InLinPreRa) for solving complicated hierarchical multiple attribute prediction problems.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277255,no,undetermined,0
Application of AHP-TOPSIS to the Evaluation and Classification of Provincial Landscape Construction Level of China,"Urban green system plays a vital role in the urban sustainable development. This study, using analytic hierarchy process (AHP) and technique for order preference by similarity to ideal solution (TOPSIS) models, taking provinces in China as unit, has made analysis and thus carried out a classification of the landscape construction level of provinces in China from a macroscopic view. The results show that the provincial landscape construction level is positively correlated with the economic development level. But due to the big difference of landscape construction level of each province, enough importance should be attached. The AHP-TOPSIS model has good discrimination in the evaluation on Chinapsilas provincial landscape construction level and it is also applicable to the evaluation and classification of urban landscape construction level among different cities.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5189813,no,undetermined,0
Application of AHP to tobacco enterprise performance appraisal,"In order to achieve better economic benefit and social impression, tobacco enterprise must establish effective and operable performance appraisal system. It is very important to select indexes and their weight for performance appraisal. In the paper, weight for AHP-based (analytic hierarchy process) performance appraisal is studied by case. The result indicates that sales volume, inventory and purchasing plan are important indexes.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5192294,no,undetermined,0
Analyzing Alternatives in Collaborative Technology of Software Contractor by Analytic Network Process,"Collaborative software technology management point of view to assist the industry in the use of collaborative enterprise to create competitive advantage, into the network era of economic and information systems to integrate control of the development platform to build synergies and reduce the occurrence of system customization. When the majority of the common problems faced by enterprises, will be faced with the choice of partners, to strengthen the demand for collaborative management industry, and decided to technical evaluation criteria. This study collect the advice of experts and scholars, see the relevant literature that the standard of their choice and analytic network process (ANP) to choose computer integrated manufacturing (CIM) software standards and weights. The structure of these advantages in this study under a more objective measure of the evaluation criteria forms the different types of coordination and technical guidance. Eventually, the study is a useful reference for enterprises to cooperate in the choice of technology development partners.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5412473,no,undetermined,0
Research and Realization of WEB Security Auto-Testing Tool Based on AHP,"In the process of software production, testing is the premise to guarantee the quality of software. With the extensive application of network software, Web security test has become a key point that can not neglect. Based on the Analytic Hierarchy Process (AHP) algorithm, a new kind of Web security testing programme was introduced in this paper. According to which it realized the Web Security auto-Testing Tool including Black-box testing and White-box testing. Through the experiments on the B/S mode software platform, its validity was verified.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676792,no,undetermined,0
Research of ship oil spill risk monitoring & evaluation system in Shanghai Yangshan Port water area,"The paper studies on ship oil spill risk monitoring and evaluation of Shanghai Yangshan Port. Through the analysis on general situation of water and transportation, it discusses reasons of ship oil spill accidents happened in Yangshan Port and summarizes the regular pattern. Single-vessel oil spill evaluation module of Yangshan Port was set up using gray analytic hierarchy process and corresponding software tool was developed using Visual Basic computer programming language, providing scientific and swift risk assessment user interface, which made the ship oil spill monitoring and evaluation of Shanghai Yangshan Port quantitative.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572067,no,undetermined,0
Analytics on historical data using a clustered insert-only in-memory column database,"In the field of OLAP and data warehousing, column stores and compressed main-memory data storage technology have successfully been implemented in products that enable a significant speed improvement of analytical queries with special performance requirements. We could soon see the majority of analytical workloads move to such main-memory based systems. Having one specialized OLAP DBMS explicitly aimed at performing ad-hoc queries on an ever-growing database requires the capability of an in-memory database to retain historical states so that applications can calculate consistent values based on previous states of the database, a requirement often found in financial and production planning analytical applications. This paper describes Rock, an in-memory analytics cluster based on a column store database, and proposes an architecture for historical query support as well as the prototypical implementation in Rock.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5344497,no,undetermined,0
Log-Based Reliability Analysis of Software as a Service (SaaS),"Software as a Service (SaaS) has gained momentum in the past few years and businesses have been increasingly moving to SaaS model for their IT solutions. SaaS is a newer and transformed model where software is delivered to customers as a service over the web. With the SaaS model, there is a need for service providers to ensure that the services are available and reliable for end users at all times, which introduces significant pressure on the service provider to ensure right test processes and methodologies to minimize any impact to the provisions in Service Level Agreements (SLA). There is lack of research on the unique approaches to reliability analysis of SaaS suites. In this paper, we expand traditional approaches to reliability analysis of traditional web servers and propose methods tailored towards assessing the workload and reliability of SaaS applications. In addition we show the importance of data filtration when assessing SaaS reliability from log files. Finally, we discuss the suitability of reliability measures with respect to their relevance in the context of SLAs.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635046,no,undetermined,0
Research of the organizational changes model in maritime companies,"After theoretical considerations of organizational changes, the research findings were presented in relation to the following: the selection of the optimal model for implementation of organizational changes in the maritime company that is the subject of analysis and establishing of the character of the correlation between the organizational changes, the productivity level and the ship service time (components related to cargo handling operations). Through the implementation of AHP (Analytic Hierarchy Process) method, it has been identified that the optimal model for implementation of organizational changes in the maritime company being the research subject is restructuring.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5647061,no,undetermined,0
"Analytic Hierarchy Process (AHP), Weighted Scoring Method (WSM), and Hybrid Knowledge Based System (HKBS) for Software Selection: A Comparative Study","Multi criteria decision making (MCDM) methods help decision makers to make preference decision over the available alternatives. Evaluation and selection of the software packages is multi criteria decision making problem. Analytical hierarchy process (AHP) and weighted scoring method (WSM) have widely been used for evaluation and selection of the software packages. Hybrid knowledge based system (HKBS) approach for evaluation and selection of the software packages has been proposed recently. Therefore, there is need to compare HKBS, AHP and WSM. This paper studies and compares these approaches by applying for evaluation and selection of the software components. The comparison shows that HKBS approach for evaluation and selection of the software packages is comparatively better than AHP and WSM with regard to (i) computational efficiency (ii) flexibility in problem solving (iii) knowledge reuse and (iv) consistency and presentation of the evaluation results.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395484,no,undetermined,0
Analytic calculation of stress of multi-throttle-slices for twin tubes shock absorber,"In this paper, the deformation, internal forces and stress of single throttle slice of shock absorber was analyzed, the formula of the stresses of its were established. Studied the pressure and the stresses on each slice, the analytic stresses formulas of multi-slices were established. Followed a practical example for stresses computation of multi-slices with this new analytic method, and it was simulated and verified by ANSYS software. The results show that the stress computation method for multi-slices is accurate enough.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5357930,no,undetermined,0
Analytic Architecture Assessment in SOA Solution Design and its Engineering Application,"In this paper, we present an architecture-centric assessment approach for model evaluation over reference architecture to quantitatively estimate architecture maturity and quality. Such assessment is essential to support design-level refinement for an enterprise solution. To achieve this analytic goal, we select a nine-layer SOA solution stack (S3) as reference architecture, and introduce the necessary mathematical definitions and formulation. The baseline for such assessment is a model template composed of S3 solution patterns. A template is the starting point of creating a design model. The selection of such template will largely determine the architecture properties of the final SOA solution.The maturity analysis is carried out at different granularity levels (architecture building block, architecture layer, and architecture model) to justify the 'completeness' of a design. The quality assessment is accomplished through a set of quality-indicators to justify the 'goodness' of an architecture based on the relationships of architecture building block instances. Finally, using UML 2.0 to capture the model of S3, we provide a real assessment prototype developed over IBM RSA platform.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5175900,no,undetermined,0
Analysis of influencing factors on performance evaluation of agricultural products network marketing based on AHM,"Agricultural products network marketing means fully introducing e-commerce systems into the sale process of agricultural products, using information technologies to publish and collect the demand and price information, and relying on agricultural production bases and logistics distribution systems to enhance brand images, improve customer services, develop online marketing channels and ultimately expand marketing activities. So, it is very important to evaluate the performance of agricultural products network marketing. According to the principles of purposefulness, scientific, practicality and comprehensiveness, the paper firstly established an evaluation index system of agricultural products network marketing performance, and then applied analytic hierarchical model (AHM) to analyze these factors. This can provide the basis for researchers to study how to improve the performance of agricultural products network marketing.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406815,no,undetermined,0
Research on a Software Trustworthy Measure Model,"Through analyzing the factors affecting software trustworthy, established the index system of trustworthy software estimation. Apply Analytic Hierarchy Process (AHP) to determine the relative importance of factors and indicator items affecting software trustworthy, and then determine the score of estimation index system through fuzzy estimation model, use the combination of both to measure software trustworthy. Finally, this paper presents an example to validate the validity and objectivity of this model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480976,no,undetermined,0
Research on e-government security risk assessment based on improved D-S evidence theory and entropy weight AHP,"With the application of e-government becomes more and more extensive, the security issues become increasingly important. The paper firstly presents a relatively complete e-government security risk evaluation index system basing on comprehensive analysis of risk factors which affects e-government security. Then we conduct orthogonal transformation to the indexes for eliminating duplication between them. After that, we combine entropy weight coefficient method and AHP to determine the weight of risk factor index, and the improved D_S evidence theory is introduced to deal with the uncertainty in risk assessment. Finally, an example is cited to verify the validity of this method, compared with the result of fuzzy comprehensive evaluation, we can see the effectiveness of this method in e-government security risk assessment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5610536,no,undetermined,0
Analysis and evaluation of human factors in aviation maintenance based on fuzzy and AHP method,"This paper aimed to develop a quantitative and objective method to analyze and evaluate human factors in aviation maintenance process. Firstly a detailed classification and causing analysis on human errors in aviation maintenance was carried out basing on SHEL and Reason Model; then an integrated evaluation solution was proposed out by using fuzzy and AHP method together. Then the solution was applied into practice for an evaluation case. Finally it concluded that Noise & vibration, Professional ethics & responsibility, Safety information sharing and Completeness of software & document took the four largest weights to the four first-level indices respectively. This comprehensive evaluation method was expected to be using in practice and provide effective support to prevent human errors and improve safety in maintenance organizations.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372887,no,undetermined,0
Research on Online Static Risk Assessment for Urban Power System,"With the rapid development of urbanization, the importance of city power grids safety has been gradually recognized. Given a full consideration for the characteristics of city power grids, we design a complete set of risk evaluation index system based on probability theory by employing risk theory and analytic hierarchy process (AHP) in power system online static security risk assessment. Further, we have developed city grid online risk assessment software, and have provided a clear description of some key technologies of implementation and calculation process after installation. Finally, the test result shows the functionality and applicability of our software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5448732,no,undetermined,0
Research on service quality of campus vehicles based on SPSS,"The quality of service is an important aspect of current social concern, and to meet the transport needs of teachers and students on campus is an important prerequisite for quality services. In the paper the evaluation index system of campus vehicle service quality was established based on AHP, and the Delphi method was used to determine the relative importance of individual index. The quality of campus vehicle service in Nanchang University was taken as an example, through multi-disciplinary and multi-level sampling survey of 120 people, 11 evaluation indexes were analyzed with SPSS statistical software. The analysis shows that the method proposed in the paper can accurately measure the current service quality of campus vehicles. The corresponding software and hardware improvement methods can be taken to improve the service quality for the emerging problems, which provides a fundamental guarantee for the scientific and standardized service management.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5479215,no,undetermined,0
An Integrated Supplier Selection Approach Based on Fuzzy Analytic Network Process,This paper develops a supplier evaluation approach based on the analytic network process (ANP) and fuzzy synthetic evaluation under a fuzzy environment. The importance weights of various criteria are considered as linguistic variables. These linguistic ratings can be expressed in triangular fuzzy numbers by using the fuzzy extent analysis. Fuzzy synthetic evaluation is used to select a supplier alternative and the Fuzzy ANP (FANP) method is applied to calculate the important of the criteria weights. Then an integrated FANP and fuzzy synthetic evaluation methodology is proposed for evaluating and selecting the most suitable suppliers. A hypothetical example is presented and the results indicated that the combination of ANP and fuzzy synthetic evaluation provided useful tool to select the optimal supplier.,2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362809,no,undetermined,0
An Information System Security Risk Assessment Model Based on Fuzzy Analytic Hierarchy Process,"Information system is a large-scale complex system. It includes many uncertain factors, as software, hardware, people and so on. As a result, information systems security risk is related to many ambiguous factors, what are difficult to measure, with ambiguity. This paper introduces the information system security risk generating mechanism, and based on the risk assessment of factors, builds information system security risk assessment model based on fuzzy analytic hierarchy process, which could be used to evaluate the security situation of information system.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5137926,no,undetermined,0
Research on ship welding process planning with case-based reasoning,"In order to solve the problem that the judgment for the necessity of the welding procedure qualification (WPQ) depends excessively on the experience and the accumulated process knowledge of enterprise can't be unitized effectively in the traditional welding ship process planning, according to analyzing influencing factors of the ship welding procedure, an advanced planning approach based on case-based reasoning (CBR) was proposed. With this approach combining analytic hierarchy process (AHP) and similarity system theory, the weight and similarity of each factor were calculated, and then the process-similarity was presented to assist the process designer to judge. Adopting the result of judgment, the process planning would be achieved efficiently. Finally, a case study was demonstrated to show the rationality and adaptation of this approach.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478182,no,undetermined,0
Research on the Evaluation of Performance of Integration Emergency Supply Chain with an Application of the Analytic Network Process,"In the first place, this thesis, based on an analysis of the features of the Integration Emergency Supply Chain and the affecting factors of its performance, considering the specificity of Integration Emergency Supply Chain, constructs an indicating system on the evaluation of overall performance of Integration Emergency Supply Chain. Secondly, based on an analysis of the applicability of the analytic network process, this thesis, analyses the dependent network process relationship between different indicators in the evaluation system of overall performance of the Integration Emergency Supply Chain, establishes a network construction of Performance Evaluation and confirms partial weight and overall weight. Meanwhile, through a survey of experts, this paper, with an empirical analysis of the software Super Decision, raised some managerial suggestions which can improve the performance.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5566459,no,undetermined,0
An Approach for Selecting Software-as-a-Service (SaaS) Product,"Software-as-a-Service (SaaS) helps organizations avoid capital expenditure and pay for the functionality as an operational expenditure. Though enterprises are unlikely to use SaaS model for all their information systems needs, certain business functionalities such as Sales Force Automation (SFA), are more seen to be implemented using SaaS model. Such demand has prompted quite a few vendors to offer SFA functionality as SaaS. Enterprises need to adopt an objective approach to ensure they select the most appropriate SaaS product for their needs. This paper presents an approach that makes use of Analytic Hierarchy Process (AHP) technique for prioritizing the product features and also for expert-led scoring of the products.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284286,no,undetermined,0
Research on the simulation problem of support vehicle requirement among the Multi-Aircraft Flight Support Process,"Among the Multi-Aircraft Flight Support Process, as it refers to much influence factors for determining the requirement of the support vehicles, and the process of vehicle scheduling is complicated and changeful, so it is difficult to establish the analytic model for the above-mentioned NP hard problem. But the simulation model about the process of support vehicle scheduling could be found by making use of the discrete event dynamic system's modeling and simulation technology, through setting the different kinds of aircraft' flight batch interval in the simulation software Arena, Simulates the flight support process. Until the output of simulation which satisfies the requirement of flight ready model is obtained by continually adjusting the number of different kinds of support vehicle, whose scheduled utilization rate is higher than the other support vehicle in the software's process analyzer tool. The simulation result shows that the proposed method provides a new way to deal with the requirement problem of Multi-Aircraft flight support vehicles.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5622559,no,undetermined,0
An Approach for Eliciting Software Requirements and its Prioritization Using Analytic Hierarchy Process,"Most software engineering methods presume that requirements are explicitly and completely stated; however, experience shows that requirements are rarely complete and usually contain implicit requirements. The failure or success of a software system depends on the quality of the requirements. The quality of the requirements is influenced by the techniques employed during requirements elicitation. Requirements elicitation is most critical part of the software development because errors at this beginning stage propagate through the development process and the hardest to repair later. In this paper we have proposed an algorithmic approach to elicit the software requirements and its prioritization of the requirements using analytic hierarchy process (AHP).",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328151,no,undetermined,0
Embedded systems design - Scientific challenges and work directions,"Summary form only given. The development of a satisfactory Embedded Systems Design Science provides a timely challenge and opportunity for reinvigorating Computer Science. Embedded systems are components integrating software and hardware jointly and specifically designed to provide given functionalities, which are often critical. They are used in many applications areas including transport, consumer electronics and electrical appliances, energy distribution, manufacturing systems etc. Embedded systems design requires techniques taking into account extra-functional requirements regarding optimal use of resources such as time, memory and energy while ensuring autonomy, reactivity and robustness. Jointly taking into account these requirements raises a grand scientific and technical challenge extending Computer Science with paradigms and methods from Control Theory and Electrical Engineering. Computer Science is based on discrete computation models not encompassing physical time and resources which are by their nature very different from analytic models used by other engineering disciplines. We summarise some current trends in embedded systems design and point out some of their characteristics, such as the chasm between analytical and computational models and the gap between safety critical and best-effort engineering practices. We call for a coherent scientific foundation for embedded systems design, and we discuss a few key demands on such a foundation: the need for encompassing several manifestations of heterogeneity, and the need for design paradigms ensuring constructivity and adaptivity. We discuss main aspects of this challenge and associated research directions for different areas such as modelling, programming, compilers, operating systems and networks.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090623,no,undetermined,0
Imaging and reconstruction of a 3D complex target using downward-looking step-frequency radar,"A technique of imaging and reconstruction for a three-dimensional (3D) complex-shaped Perfect Electric Conductor (PEC) target is developed using step-frequency radar observation synthesizing a two-dimensional aperture. The radar works in downward-looking spotlight mode moving within a 2D circular arc aperture to eliminate geometric disortions and shadowing effect, and the backscattered electrical fields in both the amplitude and phase are obtained. The three-dimensional fast Fourier transform (3D-FFT) algorithm is adopted for uniformly resampling data, which are aquired by interpolating the collected backscattering fields to quickly form a focus image. The bidirectional analytic ray tracing (BART) method [1] is applied to fast calculate the backscattering fields from the 3D complex target, e.g. a tank. Automatic reconstruction of the target is well demonstrated. As a validation, the scattering fields are also computed and compared using widely accepted software FEKO based on Physical Optics.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5696614,no,undetermined,0
Evaluating the risk of cyber attacks on SCADA systems via Petri net analysis with application to hazardous liquid loading operations,"This paper develops an analytic technique for quantifying the risk of computer network operations (CNO) against supervisory control and data acquisition (SCADA) systems. We measure risk in terms of the extent to which an attacker can manipulate process control elements, the consequences due to disruption of the controlled physical process, and the vulnerability of the SCADA system to malicious intrusion. The technique constitutes a novel application of Petri net state coverability analysis coupled with process simulation. As such, this framework permits a formal assessment of candidate policies to manage risk by diminishing aspects of the network vulnerability to intrusion, where the objective is to prevent malicious induction of catastrophic process failure modes. We extend earlier work on Petri nets for attack analysis by developing a detailed methodology including: a new algorithm for the automatic generation of Petri nets from the description of a SCADA network and its vulnerabilities; metrics for quantifying risk as a function of a Petri net's state; techniques for evaluating these metrics based on a Petri net's minimal coverability set; and a method for coupling the Petri net representation of the SCADA network to the controlled processes for failure mode and effects assessment. The paper concludes by presenting an example application of the analysis technique to evaluate the security of a hazardous liquid loading process.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5168093,no,undetermined,0
An Analytic Network Process Model for Engineering Device Procurement Evaluation,"The ANP method is used in this paper to evaluate engineering device procurement. The evaluation criteria are given, their control hierarchy is built and weights are calculated. The evaluation can be carried out in four perspectives: the requirements, the finance, the supplier and the procurement procedure. The weights can be used in a semi-automatic evaluation process in which a group of dropdown lists is to be selected and the final evaluation score is the addition of the weighted score. The weight calculation is carried out in Excel and the evaluation can be carried out manually or in software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575835,no,undetermined,0
AC Loss Analysis of HTS Power Cable With RABiTS Coated Conductor,"Numerical analysis of AC loss for a HTS power cable is investigated using commercial FEM software package. AC loss of the HTS power cable, which is made by 2 G conductor, is hard to experimentally measure due to very small signal compared to that made by 1G conductor. The FEM model describes current distribution and AC loss inside the HTS conductor for the AC transport current through nonlinear E-J correlation. For the verification of the AC loss analysis model, the results were compared with the well known analytic solution of a single strip HTS conductor and experiments. Unlike IBAD substrate, magnetization of the RABiTS has influence on the precise estimation of the AC loss and it is also considered in the FEM model. Moreover, several conductors should be stacked to meet the large transport current because of the small critical current at present and the effect of stacking configuration is also investigated. In this paper, AC loss analysis results are presented for various HTS power cable configurations such as stacking directions. The results are compared with the experimental results of a model HTS power cable and the best configuration to minimize AC loss is suggested.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439739,no,undetermined,0
Simulation research of the Mixed-Model production line based on Flexsim,"To meet product diversification and individuation, the paper reconstructed the original single object production line using Flexsim simulation software. Reconstructed model was optimized using the ECRS (eliminate, combine, rearrange, simple) analytic method. The study indicate that the mixed-model production line making up three production line can raise the comprehensive utilization ratio of equipment.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5344303,no,undetermined,0
Simulation of Cutting Force Based on Software Deform,"Using analytic method to analyze the change rule of cutting force fall across hardness, because it relate to non-linear problem that is perplexing. Base on software deform, infection factors were simulated in the process of cutting.The factors cutting include deepness and reamer angle and enter measure. The simulation get the result that include the influence of the factors and the change rule of cutting force. To The analysis and the simulation of the cuts process through the finite element software that is helpful in optimizes the geometry parameter of the cutting tool and the processing craft parameter choice, exert the efficiency of machine tool farthest. Has saved the cutting tool, the material and laboratory uses experimental expense and so on, reduced the experimental cycle. It has found the new more effective modernized tool for the cutting rule.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5288127,no,undetermined,0
Agent-based modeling of mems fluidic self-assembly,"The dynamics of MEMS 3D fluidic self-assembly (FSA) was modeled using interactive software agents, i.e. by agent-based modeling (ABM). ABM enables realistic simulations of 3D FSA dynamics taking into account spatial parameters - hard to include in analytic models. Our ABM model was tested by reproducing the experimental data of Zheng and Jacobs's 3D FSA process, and it was used to investigate the influence of design parameters and assembly strategies on FSA yield. The ABM model is a significant advance in the modeling of FSA and may represent the natural framework to explore open issues in this promising field.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5442463,no,undetermined,0
Simulation model to investigate flexible workload management for healthcare and servicescape environment,"High demand and poor staffing conditions cause avoidable pressure and stress among healthcare personnel which results in burnout symptoms and unplanned absenteeism which are hidden cost drivers. The work environment within an emergency department is commonly arranged in a flexible workload which is highly dynamic and complex for the outside observer. Using detailed simulation modeling within structured modeling methods, a comprehensive model to characterize the nurses' time utilization in such flexible dynamic workload environment was investigated. The results have been used to derive a generalized analytic expression that describes certain settings that lead to an instable queuing system with serious consequences for the healthcare facility. Thus decision makers are hence equipped with a tool which allows identifying and preventing such conditions that affect service quality level.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429210,no,undetermined,0
An Analytic Framework for Detailed Resource Profiling in Large and Parallel Programs and Its Application for Memory Use,"Profiling is an essential and widely used technique to understand the resource use of applications. For example, the memory use of large applications is becoming an important cost factor. Very large systems are typically sized to accommodate designated tasks, and thus, the price, as well as cache and TLB efficiency, depends significantly on the memory footprint of the target applications. Importantly, the increasing use of multicore systems magnifies the problem since memory use grows with the number of parallel tasks. Additionally, the presence of multiple tasks or threads makes the problem of correlating resource use to the program structure harder. Thus, tools that correlate resource use with program structure with quantitative error margins are essential for optimizing the resource use of complex software applications. While efficient tools for the profiling of execution time are available, the choices for detailed profiling of memory use or other hardware resources are very limited. We were unable to find tools that provided sufficiently accurate insight into, e.g., memory use without adding unacceptable overhead in memory use and execution time for the performance analysis of very large applications. In this paper, we present a highly efficient probabilistic method for profiling that provides detailed resource usage information R<sub>?</sub>(t) indexed by the full location descriptor ? (e.g., process id, thread id, and call chain) and time t. Importantly, we provide an analytical framework, which provides error estimates and allows to analyze and quantitatively optimize a wide variety of profiling scenarios. We employed the probabilistic approach to implement a memory profiling tool that adds minimal overhead and does not require recompilation or relinking. The tool provides the memory use M<sub>?</sub> (t) for all location descriptors ? over the execution time for single and multithreaded programs. Experimental results confirm that execution time and memory o- - verhead are less than 10 percent of the unprofiled, optimized execution. Importantly, the technique is sufficiently general to be applicable to profiling of other hardware resources as cache or TLB misses over time for all location descriptors with similarly low overhead and across multiple processes, threads, and processors.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5276794,no,undetermined,0
Simulation Based on Matlab Software for Vehicle Transmission Shaft,"A method in this paper, which the theory of planar analytic geometry is used to model and simulate in vehicle transmission shaft, is firstly presented. This method is clear, uncomplicated and lucid, which offers a more convenient way to analyze vehicle transmission shaft kinematics characteristic so that it is suitable for project designs. The Matlab software is used to make the simulation in vehicle transmission shaft, through which the kinematics characteristic will be gained and the conclusion will provide theoretic reference for the disposal of the vehicle transmission shaft and mechanics in the future.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190255,no,undetermined,0
"RPM-A: Who will win the battle for the Gigabit Wireless in your home: WirelessHD, 802.11n, wireless USB, or UWB?","The extraordinary growth in the HD multimedia market, during the last few years, has created an eminent need for truly seamless interconnectivity of the various home entertainment appliances, broadband content streaming machines and personal computing devices. According to DisplaySearch, the worldwide sales of HDTV topped US$100B for the first time in 2007 with 200M units. Meanwhile, set-top box revenue reached 41M units in 2007 according to Instat. IDC projected a large leap in worldwide laptop shipments, from 108 million in 2007 to 148.2 million in 2008. Strategy Analytics estimated that 30M units of Blu-ray players will be sold in 2008, out of which 20M units are Play Stations 3 by Sony. Apple reported that since the launch of iStore, between July and September, more than 100M iPhone software applications (US$40M sales) have been downloaded - an unprecedented adoption rate by any measure. The common theme that all these trends share is a massive increase in the amount of digital content and the desire to share them seamlessly.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5135458,no,undetermined,0
An Analytic Model for Fault Diagnosis in Power Systems Considering Malfunctions of Protective Relays and Circuit Breakers,"When a fault occurs on a section or a component in a given power system, if one or more protective relays (PRs) and/or circuit breakers (CBs) associated do not work properly, or in other words, a malfunction or malfunctions happen with these PRs and/or CBs, the outage area could be extended. As a result, the complexity of the fault diagnosis could be greatly increased. The existing analytic models for power system fault diagnosis do not systematically address the possible malfunctions of PRs and/or CBs, and hence may lead to incorrect diagnosis results if such malfunctions do occur. Given this background, based on the existing analytic models, an effort is made to develop a new analytic model to well take into account of the possible malfunctions of PRs and/or CBs, and further to improve the accuracy of fault diagnosis results. The developed model does not only estimate the faulted section(s), but also identify the malfunctioned PRs and/or CBs as well as the missing and/or false alarms. A software system is developed for practical applications, and realistic fault scenarios from an actual power system are served for demonstrating the correctness of the presented model and the efficiency of the developed software system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5471043,no,undetermined,0
An Analytic Model for Impedance Calculation of an RFID Metal Tag,"Since low-profile metal tags are infamous for their narrow bandwidth, obtaining fast and accurate impedance computation is therefore an important issue in the antenna design stage. This letter presents a method for computing antenna impedance of a patch-type radio frequency identification (RFID) tag. In order to perform impedance calculation in a very short time, an analytic model is proposed, and the result is highly agreeable with that obtained by the commercial software. Two important parameters are also identified for the purpose of independent impedance tuning of the real and imaginary parts. A prototype was designed and manufactured based on the proposed method, showing a dimension of 118 í„ 43 í„ 1.5 mm<sup>3</sup> and a reading range of 6 m.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5491048,no,undetermined,0
An Analytic Model of Atomic Service for Services Descriptions,"The key question with Service Science is to be how to formulate a theoretic foundation for service description, service discovery and service composition in the context of basic service system development and the building of service architecture. However, the current research and development in the field of Service Science assumes that the mapping between the service requesters' requirements and the formal and executable services provided by the service providers (or developers) is straightforward, without a clear view of unbalance or even semantic conflict between a requirement very likely in NL and a service as a programming module. In this paper, we attempt to address this issue by proposing an analytic model of atomic service for service composition based on our initial analysis of service structures and manipulations.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494281,no,undetermined,0
Resilience in computer systems and networks,"The term resilience is used differently by different communities. In general engineering systems, fast recovery from a degraded system state is often termed as resilience. Computer networking community defines it as the combination of trustworthiness (dependability, security, performability) and tolerance (survivability, disruption tolerance, and traffic tolerance). Dependable computing community defined resilience as the persistence of service delivery that can justifiably be trusted, when facing changes. In this paper, resilience definitions of systems and networks will be presented. Metrics for resilience will be compared with dependability metrics such as availability, performance, performability. Simple examples will be used to show quantification of resilience via probabilistic analytic models.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5361311,no,undetermined,0
Research on the Performance of xVM Virtual Machine Based on HPCC,"The virtual machine (VM) technology has received an increasing interest a spotlight both in the industry and the research communities. Although the potential advantages of virtualization in HPC workloads have been documented, the potential impact to application performance in HPC environments is not clearly understood. This paper presents a study on performance evaluation of virtual HPC systems using High Performance Computing Challenge (HPCC) benchmark suite and xVM as the workload representative and VM technology, respectively. Based on the extended AHP (Analytic Hierarchy Process) method, we propose an efficient performance evaluation model based on extended AHP and analyze the results and quantify the performance overhead of xVM in terms of compute, memory, and network overhead. Our analysis shows that the computational and network performance in HVM is slightly better and the memory performance is significantly better compared to paravirtualization.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328965,no,undetermined,0
Research on task allocation for Product Cooperative Development,"Based on analysis of the requirements of task allocation for product cooperative development (PCD), an evaluation index system was established for task allocation. This paper introduced the mathematical method of fuzzy analytic hierarchy process (FAHP) firstly, and established a hierarchy evaluation model of task allocation for PCD, which uses time, quality, cost as the first class evaluation indexes and uses work efficiency, technical merit, cooperation ability, work experience, personnel interest, work attitude, personnel wages and cooperation expenses as the secondary class evaluation indexes. Based on FAHP method, this paper realized task allocation for complicated product cooperative development. A software prototype of task allocation tool for PCD is implemented, which has features of friendly interface, easy operation and rapid calculation, etc.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234428,no,undetermined,0
An Application of Six Sigma and Simulation in Software Testing Risk Assessment,"The conventional approach to Risk Assessment in Software Testing is based on analytic models and statistical analysis. The analytic models are static, so they don't account for the inherent variability and uncertainty of the testing process, which is an apparent deficiency. This paper presents an application of Six Sigma and Simulation in Software Testing. DMAIC and simulation are applied to a testing process to assess and mitigate the risk to deliver the product on time, achieving the quality goals. DMAIC is used to improve the process and achieve required (higher) capability. Simulation is used to predict the quality (reliability) and considers the uncertainty and variability, which, in comparison with the analytic models, more accurately models the testing process. Presented experiments are applied on a real project using published data. The results are satisfactorily verified. This enhanced approach is compliant with CMMI<sup>Œ¬</sup> and provides for substantial Software Testing performance-driven improvements.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477075,no,undetermined,0
Evaluation Indicator System and Weights Research of Communication Effects in ERP Implementation Project,"Based on communication theory and characteristics of communication in ERP implementation project, this paper focuses on communication influencing factors analysis and proposes a new two level evaluation indicator system of communication effects for ERP implementation project. Besides, this paper carries on a weights research of the indicator system by means of the analytic hierarchy process (AHP) method, questionnaire survey and expert scoring method. The indicator system and weights research can serve as a basis to evaluate communication effects in the context of ERP implementation project and help management team to prioritize their attentions according to the weights of elements. Furthermore, this paper is an exploratory research about communication in ERP implementation project and provides rudimental idea for later research.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384911,no,undetermined,0
An Architecture for a Task-Oriented Surveillance System: A Service- and Event-Based Approach,"Due to the increasing threat posed by crime, industrial espionage and even terrorism, video surveillance systems have become more important and powerful during the last years. While most commercially available surveillance systems have to be managed by human operators, who constantly monitor all video streams, several experimental systems from different research groups already include robust video processing approaches for (semi-) automated surveillance. Still, most research activities focus on a sensor-oriented approach to video analytics of large and distributed camera networks, aiming to extract, analyze and store all extractable information from the video streams. In real-life applications, however, only a limited set of specific threats needs to be covered. Accordingly, only a small subset of potentially extractable information has to be monitored. Besides the huge amount of raw video data, modern surveillance systems are also extended with other sensors that deliver even more data. As a consequence, a new paradigm is introduced, called task-oriented information and data processing for surveillance systems. In the proposed system NEST (Network Enabled Surveillance and Tracking) following the task-oriented approach, every resource allocation, data acquisition, and analysis process is assigned to a specific surveillance task. In order to meet the requirements of task-oriented surveillance, the proposed architecture combines a Service-Oriented Architecture with an Event-Driven Architecture (Event-driven SOA).",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5464144,no,undetermined,0
An Effect Evaluation Model for Vulnerability Testing of Web Application,"In this paper, we propose a security evaluation model for the web application and define a security evaluation function based on the Analytic Hierarchy Process (AHP) to describe the model. We use the evaluation method proposed by this paper to evaluate the vulnerability test effect of a BBS application named IPB. The experiment result reveals that the evaluation value calculated by the security evaluation function is positively correlated with the number of vulnerabilities found in the security test. It proves that the security evaluation method proposed by this paper is practical and reliable.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5480968,no,undetermined,0
Research on logistics distribution center location problem based on genetic algorithm and AHP,"Logistics distribution center is crucial to the whole logistics system. The location of the distribution is the key of the logistics system analysis. In this paper, numerous elements are taken into consideration to establish logistics distribution centers; to economic factors, best solutions are found by improved genetic algorithm; environmental and service factors are also involved by using AHP. The combination of both methods solves this problem both quantitatively and qualitatively, which makes final solution better in accordance with practical demands.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5228492,no,undetermined,0
"An Insuanrance Model for Guranteeing Service Assurance, Integrity and QoS in Cloud Computing","SOA and cloud computing have brought new opportunities for the long expected agility, reuse and the adaptive capability of IT to the ever changing business requirements and environments. But due to the immature nature of the rapidly evolving technologies, especially in the areas of security, service or information integrity, privacy, quality of service and their possible detrimental consequences, many enterprises have been hesitating to make the shift. This paper adopts the concept of insurance and establishes a framework and the supporting reference model for cloud computing. We utilize the value-at-risk (VAR) approach to establish several appropriate mechanisms, and use a set of measurable metrics. Those quantitative or qualitative metrics can be applied as the basis for the business value and risk assessment, and eventually for insurance premium and compensation calculation for the failures of the services offered in Cloud environment. This model can also establish a potential new innovative market branch for the insurance industry.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552738,no,undetermined,0
Research of Secure Multicast Key Management Protocol Based on Fault-Tolerant Mechanism,"As multicasting is increasingly used as an efficient communication mechanism for group-oriented applications in the Internet, the research of the multicast key management is becoming a hot issue. Firstly, we analyze the <i>n</i>-party GDH.2 multicast key management protocol and point out that it has the following flaws: lack of certification, vulnerability to man-in-the-middle attacks, and a single-point failure. In order to settle the issues mentioned above, a fault-tolerant and secure multicast key management protocol (FTS, for short) with using the fault-tolerant algorithm and the password authentication mechanism is proposed in this paper. In our protocol, legal members are able to agree on a key despite failures of other members. The protocol can also prevent man-in-the-middle attacks. Finally, we evaluate the security of FTS, and compare our protocol with the FTKM through performance analysis. The analytic results show that the protocol not only avoids the single-point failure but also improves the comprehensive performance.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4908530,no,undetermined,0
Analysis of substation availability,"The paper discusses the modeling of substations for reliability, dealing with each equipment and its influence in the topology. The study consider the power availability for each load, using either an analytic and a simulation model. Some results and a software implementation are presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762916,no,undetermined,0
Research and Implementation of Wushu Video Retrieval Based on Semantic,"This paper put forward a semantic-based video analytic approach and it's two algorithms in the Wushu video retrieval. Firstly, it can be taken a edge detection to the special-temporal slices by using the edge detection operator based on bilateral filtering, and then extracting and training the classes used the key frame semantic based on SVM. The system which is completed by the two algorithms implement the segmentation and retrieval of the Wushu video. The experiment show that it is better to retrieval according to the characters of the Wushu video by using this system.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5073035,no,undetermined,0
Research and Design of Computer-Aided English Textbook Evaluation System,"To break through the traditional one-sided qualitative evaluation of English textbook, the authors put forward a new model of evaluation system which includes three parts: teacherpsilas evaluation, studentpsilas evaluation, and the objective evaluation based on the corpus linguistics. By the analysis of evaluation theory of English textbook, 70 evaluation criteria are set out of 127 items using Delphi method. The proposed fuzzy analytic hierarchy process (AHP) is applied to eliminate subjective elements in weighting the evaluation criteria based on expertspsila knowledge and subjective judgments of textbook users. . The statistic data for objective evaluation criteria are obtained by using computer software to analyze the material in English textbook corpus. The quantitative analysis based on these data makes the evaluation results more objective and veracity. A detailed case study, illustrating the application of the proposed evaluation system to College English textbook evaluation is given.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959456,no,undetermined,0
Application of AHP to Evaluation on Failure Causes Analysis for Lithography Machine,"This study conducted hierarchical analysis on the evaluation item of the stability index of the lithography machine, and established a set of evaluation mechanism for failure prediction, in order to provide references and indicators of troubleshooting for lithography machine. The results showed, when the lithography machine is out of order, the possible failure causes are mainly be found based on the past experiences. This study also found that, under the good configuration of maintenance system, adequate information is closely associated a good system. As for lithography process in semiconductor industry, the complexity of broken Wafer is first considered. Thus, the overall lithography process of semiconductor relies on engineers' experience. More specifically, a quick error interpretation and repair are required in field maintenance. As in a competitive market of semiconductor processing with high-tech and high-cost, a timely maintenance in the lithography machine is urgent and requested.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474433,no,undetermined,0
Application of Non-superconducting Fault Current Limiter to improve transient stability,"In this paper, enhancement of transient stability of Single Machine Infinite Bus (SMIB) system with a double circuit transmission line using a Non-superconducting Fault Current Limiter (NSFCL) is proposed. Stability analysis for such system is discussed in detail. It is shown that, the stability depends on the resistance of NSFCL in fault condition. To effective improvement of stability, the optimum value of NSFCL resistance is calculated. Simulation results by PSCAD/EMTDC software are presented to confirm the analytic analysis accuracy.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697660,no,undetermined,0
Applying excel in the weight calculation of software maintainability evaluation based on AHP,"In order to assure the quality of software maintainability evaluation, setting up a reasonable index system and distribute the index weights with reason is the most important thing. In this situation, this paper set up an index system of software maintainability evaluation firstly, based on two principles such as giving prominence to main indexes and that indexes are usually required be measurable. Then, it did calculation upon the index weights with the application of analytic hierarchy process (AHP). Finally, the implementation of AHP in Excel validates fully that AHP is an available and efficient solution to calculate the index weights during software maintainability evaluation. In conclusion, using Excel to calculate the weights of software maintainability evaluation based on AHP is original.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609649,no,undetermined,0
Applying fuzzy AHP to select an automatic container number identification system in port terminals,"Container traffic passing through terminals has increased dramatically. The massive flows of trucks and containers typically cause traffic jams when processing efficiency at a gate is insufficient. Therefore, automatic gate systems that recognize container numbers and truck license plates are needed in container terminals. However, several systems, such as radio frequency identification (RFID) and optical character recognition (OCR), can be used for gate automation. How to choose a suitable system is an important planning problem for terminal operators. This study identified the determinants influencing terminal operators while choosing an automated gate system. An objective structure was hierarchically constructed by exploratory factor analysis (EFA) to aid decision-making when choosing an automated gate system. An empirical study based on a dedicated terminal at the Keelung Port was conducted to assess the suitability of the RFID and OCR systems. The fuzzy analytical hierarchy process was applied to calculate the relative weights of criteria and the alternatives. Analytical results demonstrate that the RFID system is the most suitable system for the Keelung Port terminal.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5681704,no,undetermined,0
Axial flux machines modelling with the combination of 2D FEM and analytic tools,"This paper deals with the development of performance analysis tools for axial flux permanent magnet machines. Modeling with 3D Finite Element Method (FEM3D) software could take too much time, and both the definition and the problem solving may be very arduous. In this work an analysis method for axial flux machines is proposed. This method consists in the combination of FEM2D simulations in the average radius plane with analytical models. The obtained results prove that the proposed method could be a very interesting option in terms of time and accuracy.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5608115,no,undetermined,0
ProcessLine: Visualizing time-series data in process industry,"In modern process industry, it is often difficult to analyze a manufacture process due to its numerous time-series data. Analysts wish to not only interpret the evolution of data over time in a working procedure, but also examine the changes in the whole production process through time. To meet such analytic requirements, we have developed ProcessLine, an interactive visualization tool for a large amount of time-series data in process industry. The data are displayed in a fisheye timeline. ProcessLine provides good overviews for the whole production process and details for the focused working procedure. A preliminary user study using beer industry production data has shown that the tool is effective.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333421,no,undetermined,0
A visualization analysis tool for DNS amplification attack,"This paper presents a visualization analysis tool for detecting, analyzing and responding to the Distributed Denial of Service attack termed the Domain Name Service (DNS) amplification attack. The tool integrates agent technology, visual analytics and interactive visualization techniques to allow users to interact with the system in real-time, to monitor the network traffic, to analyze traffic information, to detect abnormal behaviors, and to respond to the DNS amplification attack. Three algorithms that are the filter algorithm, the mapping and visualizing algorithm, and the response algorithm have been developed to detect and respond to the DNS amplification attack automatically or manually. A set of experiments have been conducted in an isolated laboratory and produced expected results.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5639324,no,undetermined,0
Software Maintainability Metrics Based on the Index System and Fuzzy Method,"An index system is proposed to solve the problem of software maintainability metrics. On the basis of quantitative procedural information, this paper uses Analytic Hierarchy Process (AHP) to determine the weight of the evaluation indices, uses the fuzzy evaluation method to deal with the quality of the indices, gets the quantitative result of software maintainability evaluation and solves the multi-index evaluation problems effectively. A representative example is applied to prove the index system and the evaluation methods feasibility.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5454554,no,undetermined,0
Solving the traffic and flitter challenges with tulip,"We present our visualization systems and findings for the badge and network traffic as well as the social network and geospatial challenges of the 2009 VAST contest. The summary starts by presenting an overview of our time series encoding of badge information and network traffic. Our findings suggest that employee 30 may be of interest. In the second part of the paper, we describe our system for finding subgraphs in the social network subject to degree constraints. Subsequently, we present our most likely candidate network which is similar to scenario B.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5334456,no,undetermined,0
A Study on Flexibility of ERP System Based on Grey Evaluation Model,"The competitive environment and changing business processes propose new requirements to ERP system's flexibility and its evaluation becomes the first and critical step to solve these problems. This paper begins with the analysis of flexibility's definition.Basing on the core idea of three-tier software architecture, the author refines the index system from the aspects of presentation flexibility, business flexibility and design flexibility. Then, treating the grey theory as a guide, the evaluation system is established by integrating AHP and Grey Evaluation Model. Finally, the corresponding example is given to illustrate the model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5659109,no,undetermined,0
Users Take a Close Look at Visual Analytics,The need to more easily and effectively analyze the mountains of data that organizations are gathering and also to see their findings in ways that are simple to understand and work with has created a growing interest in visual analytics.,2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781964,no,undetermined,0
To Score or Not to Score? Tripling Insights for Participatory Design,"For evaluating visual-analytics tools, many studies confine to scoring user insights into data. For participatory design of those tools, we propose a three-level methodology to make more out of users' insights. The relational insight organizer (RIO) helps to understand how insights emerge and build on one each other. In recent years, computers have also been used to develop visual methods and tools that further support the data analysis process. With the advent of the emerging field of visual analytics (VA), the underlying concept of visual tools is taken a step further. In essence, VA combines human analytical capabilities with computer processing capacities. In the human-computer interaction process, the user generates new knowledge and gains insights.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4909116,no,undetermined,0
Throughput-Based MAC Layer Handoff in WLAN,"We propose a MAC layer handoff mechanism for IEEE 802.11 WLAN to give benefit to bandwidth-greedy applications at STAs. In order to find an optimal AP, we investigate a new measurement method called TFC (Transient Frame Capture) to estimate the achievable throughput from APs. Since TFC is employed under promiscuous mode, it avoids service degradation through the current associated AP. In addition, the mechanism is a client-only solution which does not require any modification on APs. We develop an analytic model for the throughput estimation and demonstrate it through experimental studies.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5072197,no,undetermined,0
Utility analysis for Internet-oriented server consolidation in VM-based data centers,"Server consolidation based on virtualization technology will simplify system administration, reduce the cost of power and physical infrastructure, and improve utilization in today's Internet-service-oriented enterprise data centers. How much power and how many servers for the underlying physical infrastructure are saved via server consolidation in VM-based data centers is of great interest to administrators and designers of those data centers. Various workload consolidations differ in saving power and physical servers for the infrastructure. The impacts caused by virtualization to those concurrent services are fluctuating considerably which may have a great effect on server consolidation. This paper proposes a utility analytic model for Internet-oriented server consolidation in VM-based data centers, modelling the interaction between server arrival requests with several QoS requirements, and capability flowing amongst concurrent services, based on the queuing theory. According to features of those services' workloads, this model can provide the upper bound of consolidated physical servers needed to guarantee QoS with the same loss probability of requests as in dedicated servers. At the same time, it can also evaluate the server consolidation in terms of power and utility of physical servers. Finally, we verify the model via a case study comprised of one e-book database service and one e-commerce Web service, simulated respectively by TPC-W and SPECweb2005 benchmarks. Our experiments show that the model is simple but accurate enough. The VM-based server consolidation saves up to 50% physical infrastructure, up to 53% power, and improves 1.7 times in CPU resource utilization, without any degradation of concurrent services' performance, running on Rainbow - our virtual computing platform.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289190,no,undetermined,0
Value Measurement Model for Technology-Based Knowledge Products,"This paper summarized the major current value measurement methods for technology-based knowledge products (TKP), and then made an analysis to these methods in the value measuring application, and we further exploited the main difficulties and established the value measurement model for the TKP value measuring, and on this basis, we built the value measuring model for the TKP. First, we establish the value measurement index system, and then got the weight distribution by the analytic hierarchy process(AHP). As for the comprehensive weights calculation, the model designs to suggest the experts to give monetary value directly in all sub-indicators according their experience, which is also the data acquisition method. Finally, we used a case analysis to prove the scientificity and feasibility of the model.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363627,no,undetermined,0
The service architecture of real-time video analytic system,"Videos are an important source of information and require extremely computation expensive analysis in order to understand the high level semantics. The computational difficulties in extracting embedded information and bridging the semantic gap present the major challenges in interoperability, scalability and real-time response of video analytic systems. In this paper, we propose an intelligent video analytic system VIP (Video Intelligence Platform) that provides a distributed scalable infrastructure for supporting near real-time video stream analysis. VIP is based on service-orient architecture (SOA), in which the video analysis computation modules are wrapped as services and composed in a directed acrylic graph (DAG) structure to represent the application requirements. VIP leverages UIMA (Unstructured Information Management Architecture) framework as the data flow control engine and multiple commodity databases as the storage and computation resources. The actual executions of video analysis have been pushed down into database engine to minimize the data movement cost. We initially choose video surveillance in retail store as a representative application domain. As a case study, a prototype system has been developed to achieve fundamental functions of real-time video surveillance, including video capture/store, human detection/tracking and customer shopping trajectory analysis.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5410267,no,undetermined,0
VAST contest dataset use in education,"The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333245,no,undetermined,0
VIDI surveillance - embassy monitoring and oversight system,"We hypothesized that potential spies would try to use other employees' terminals in order to not draw attention to themselves. We define one type of suspicious activity as IP use on a terminal when the owner is inside the classified area. We created a timeline visualization of IP usage, overlaid with classified area entrances and exits. The vertical axis divides the timelines into 31 rows, one for each day of the month. The horizontal axis represents the time of day from early morning to late evening. A single employee's entire month is viewed all at once using this visualization. The employee being viewed can be changed using the arrow keys. Every IP event is represented by a vertical bar positioned at the exact time of its appearance. We color the IP events by port number, which is either intranet, HTTP, tomcat, or email, and size the bar based on the outgoing data size. Whenever an employee enters the classified area, a semi-transparent yellow region is drawn until that user exits the classified area. In rare cases when the user double enters, the region is twice as opaque, and in the other rare case where a user leaves the exits without entering, a red region is drawn until the next time the employee enters. The legend key and office diagram showing the current selected employee, highlighted in red, can be seen in the top left-hand corner.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333950,no,undetermined,0
Visual Knowledge Discovery in Dynamic Enterprise Text Repositories,"Knowledge discovery involves data driven processes where data is transformed and processed by various algorithms to identify new knowledge. KnowMiner is a service oriented framework providing a rich set of knowledge discovery functionalities with focus on text data sets. Complementing results of automatic machine analysis with the immense processing power of human visual apparatus has the potential of significantly improving the process of acquiring new knowledge. VisTools is a lightweight visual analytics framework based on multiple coordinated views (MCV) paradigm designed for deployment atop the KnowMinerpsilas service architecture. In this paper we briefly present both frameworks and, driven by real-world customer requirements, describe how visual techniques can be synergistically combined with machine processing for effective analysis of dynamically changing, metadata-rich text documents sets.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190787,no,undetermined,0
Visualizing cyber security: Usable workspaces,"The goal of cyber security visualization is to help analysts increase the safety and soundness of our digital infrastructures by providing effective tools and workspaces. Visualization researchers must make visual tools more usable and compelling than the text-based tools that currently dominate cyber analysts' tool chests. A cyber analytics work environment should enable multiple, simultaneous investigations and information foraging, as well as provide a solution space for organizing data. We describe our study of cyber-security professionals and visualizations in a large, high-resolution display work environment and the analytic tasks this environment can support. We articulate a set of design principles for usable cyber analytic workspaces that our studies have brought to light. Finally, we present prototypes designed to meet our guidelines and a usability evaluation of the environment.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5375542,no,undetermined,0
Web opinions analysis with scalable distance-based clustering,"Due to the advance of Web 2.0 technologies, a large volume of Web opinions are available in computer-mediated communication sites such as forums and blogs. Many of these Web opinions involve terrorism and crime related issues. For instances, some terrorist groups may use Web forums to propagandize their ideology, some may post threaten messages, and some criminals may recruit members or identify victims through Web social networks. Analyzing and clustering Web opinions are extremely challenging. Unlike regular documents, Web opinions usually appear as short and sparse text messages. Using typical document clustering techniques on Web opinions produce unsatisfying result. In this work, we propose the scalable distance-based clustering technique for Web opinions clustering. We have conducted experiments and benchmarked with the density-based algorithm. It shows that it obtains higher micro and macro accuracy. This Web opinions clustering technique is useful in identifying the themes of discussions in Web social networks and studying their development as well as the interactions of active participants.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5137273,no,undetermined,0
Web semantics in action: Web 3.0 in e-Science,"Semantic Web technologies have moved beyond the point of being promising futuristic technologies and demonstration projects, to being technologies in action in realistic contexts and conditions. Semantic Web applications are being developed for many aspects of scientific research, from experimental data management, discovery and retrieval, to analytic workflows, hypothesis development and testing, to research publishing and dissemination. This workshop intends to explore the questions that arise as Semantic Web applications are increasingly grounded within the actual lifecycle of scientific research, from observation and hypothesis formulation to publication, dissemination and criticism. We aim to bring together researchers across the disciplines, to discuss the use, development and embedding of these technologies in varied research domains and contexts. We will discuss the actuality of Semantic Web technologies in use and the emergent practices through which they are being developed and deployed. We aim to encourage vigorous discussion around aims, methods, applications and pragmatics. This workshop will look at the theoretical, methodological and pragmatic issues of grounding the development, deployment and evolution of ontologies and applications in Semantic e-Science in practical scientific problems and activity. How do we ground deductive Semantic Web information management and retrieval in the practical conditions of evolving sciences based on experiment, observation and induction? How do we bridge gaps and conflicts in approach between computer scientists developing research tools, and research practitioners using those tools? Is it possible to develop Semantic Web practices in e-Science that deal explicitly with hypothesis formulation, testing, challenge and refinement? Semantic Web applications have the potential to substantially accelerate research. Are all domains of research equally promising for the development of targeted semantic web applications? Are- äóìsemanticäó domains defined by particular areas of research, by a particular form of scientific question, by discipline or sub-discipline or by particular aspects or stages of the scientific process? Are there types of enquiry which are intractable to the solutions offered by the Semantic Web, and if so, why? What are the specific challenges of Semantic Web applications in different disciplines, and how might Semantic Web applications shape and be shaped by them? The incorporation of semantic technologies with existing social web practices äóî äóìWeb 3.0äó äóî promises to change the scientific research, publication and discussion model we now have to a much more fluid, äóìhigher-velocityäó model. It also poses many questions for technologists and researchers alike. Can abstract and formal understandings of the underlying ontologies be supplemented or even replaced by more informal social conceptions of ontologies? What are the advantages and disadvantages of top-down or bottom-up development processes? What are best practices regarding user engagement and usability; what are the different roles of stakeholders in the process of development and deployment? What if any changes in scientific practice may be required to exploit the promise of semantic e-Science?",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5407976,no,undetermined,0
The optimization of the fluid machinery design parameter,"The method of CFD integrated with CAD is presented to optimize fluid machinery design parameter in the paper. Firstly, the parameter to be optimized is determined. Secondly, the CAD technology is applied to build three-dimensional fluid machinery parameterization models by the parameter in its range, which is used to calculate three-dimensional flow field in the fluid machinery model by CFD software. Then, with the evaluation indexes established, the calculation results of the inner flow field in the each parametric model are evaluated by synthesis of Analytic Hierarchy Process and Fuzzy Comprehensive Evaluation. Finally, by comparing the evaluation value of every parametric model, the optimal design parameter of the fluid machinery model is achieved.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5246567,no,undetermined,0
The management of crowdsourcing in business processes,"The Centre for Next Generation localization [CNGL] is developing a number of systems in order to investigate the issues that arise in integrating centralized workflows with community-based value creation in the form of crowd- sourced localization. Consistent with current workflow and system integration practice these developed studies adopt a service oriented architecture, using Web service and Web service orchestration standards [bpel], integrated with data interchange standards from the localization industry, e.g. XLIFF. Idiom Worldserver, a leading commercial localization workflow management system was used to implement the segmentation, TM reuse, and reassembly portions of the workflow. A BPEL Glassflsh Web service orchestration engine was used to support several configurations of academic, open source and commercial text analytics and machine translation. Translation crowdsourcing was implemented through a plugin in the Drupal CMS. This integration provides us with an experimental platform for further investigating the integration of industrial workflows in this knowledge-based industry with the potential for crowdsourcing certain activities while being able to manage the end-to-end quality of the overall workflow.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5195939,no,undetermined,0
The impact of priority generations in a multi-priority queueing system äóî A simulation approach,"In this paper, we consider a preemptive (multiple) priority queueing model in which arrivals occur according to a Markovian arrival process (MAP). An arriving customer belongs to priority type i, 1 í‰ŒË i í‰ŒË m + 1, with probability p<sub>i</sub>. The highest priority, labeled as 0, is generated by other priority customers while waiting in the system and not otherwise. Also, a customer of priority i can turn into a priority j, j í‰ŒË i, 1 í‰ŒË i, j í‰ŒË m + 1, customer, after a random amount of time that is assumed to be exponentially distributed with parameter depending on the priority type. The waiting spaces for all but priority type m + 1 are assumed to be finite. The (m + 1)-st priority customers have unlimited waiting space. At any given time, the system can have at most one highest priority customer. Thus, all priority customers except the (m +1) - st are subject to loss. Customers are served on a first-come-first-served basis within their priority by a single server and the service times are assumed to follow a phase type distribution that may depend on the customer priority type. This queueing model, which is a level-dependent quasi-birth-and-death process, is amenable for investigation algorithmically through the well-known matrix-analytic methodology. However, here we propose to study through simulation using ARENA, a powerful simulation software as some key measures such as the waiting time distributions are highly complex to characterize analytically. The simulated results for a few scenarios are presented.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429269,no,undetermined,0
The evaluation system of programming curriculums,"In view of the paperless teaching and the test situation of programming curriculums, this paper employed the analytic hierarchy process (AHP) to construct achievement evaluation indicator system and the examination paper evaluation indicator system for programming curriculums group. This evaluation indicator system made full use of the wealth of information of the process of paperless teaching and examination. Finally, an evaluation software system was formed based on this evaluation indicator system. This evaluation software system can be used as a basis of the teaching evaluation for the teaching management department as well as a method of analyzing student's study situation and teacher's teaching situation.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5236336,no,undetermined,0
A computational selection strategy of dynamic knowledge alliance members on collaborative product design under the distributed knowledge resource environments,"As products collaborative design has an essential effect on the creativity of complex product, products collaborative design will need many design resources in the distributed environments. How to quantify and use the approximate and even uncertain information to evaluate design resources on collaborative design under the distributed knowledge resource environments is a crucial issue. This paper focuses on the collaborative design system reconfiguration of design resources. A collaborative design unit model is proposed. The evaluation system of dynamic knowledge alliance members in collaborative product design is then built up. A computational selection strategy method called Multi-objective Weighted Fuzzy Analytic Hierarchy Process is then proposed to realize the computational selection of dynamic knowledge alliance members on collaborative product design under the distributed knowledge resource environments. This paper also proposes a model of collaborative design based on distributed knowledge resources. The framework of distributed knowledge resource is then put forward to support collaborative design. A computer-aided collaborative design software platform based on distributed knowledge resources is developed. The collaborative design of new devices for construction site personnel wireless location system is given as an example, which demonstrates that the methodology is obviously helpful to reuse and reorganize distributed knowledge for collaborative design and product innovation.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541210,no,undetermined,0
The Evaluation of Software Trustworthiness with FAHP and FTOPSIS Methods,"Trustworthy software has attracted increasing concern both in academia and industry. How to effectively evaluate the trustworthiness is becoming a closed question. This paper models the software trustworthiness evaluation (STE) problem as a multi-criteria decision-making (MCDM) problem, and proposes both an evaluation framework and a practical approach to evaluate the software trustworthiness based on the fuzzy analytic hierarchy process (FAHP) and fuzzy technique for order preference by similarity to ideal solution (FTOPSIS) methods. FAHP method is utilized to obtain the weights of evaluation criteria. The FTOPSIS method is used to determine the final ranking of the software alternatives. The uncertainty and vagueness included in evaluation procedure are represented as fuzzy triangular numbers. Finally, the proposed method is applied to the case study of evaluating the trustworthiness of project management (PM) software alternatives for a car manufacturer in China.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365827,no,undetermined,0
The Evaluation of College Teacher's Ability of Using Information Technology Based on Analytic Hierarchy Process Method,"This thesis discusses the evaluation function of college teacher's ability of using information technology including oriented function, inspiring function and ensuring function, and establishes the evaluation index system including information ideals, information circumstances, basic knowledge, using ability and effects. This thesis studies its index sequence and weight with Analytic Hierarchy Process method and provides a quantitative analyzing method for colleges to practice the evaluation.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384735,no,undetermined,0
A Fuzzy Synthetic Evaluation Method for Software Quality,"The software quality evaluation problem is becoming increasingly important for software providers and their users. Existing most methods suitable for software quality evaluation are provided based on multiple-attribute decision-making (MADM) model. In this paper, an integration method of a fuzzy matter element (FME) and analytic hierarchy process (AHP) is used to consider quantitative and qualitative factors in evaluating the software quality. The fuzzy synthetic evaluation method is proposed to build the multiple software quality characteristics evaluation model based on the theory of fuzzy mathematics and matter element theory. Then, the AHP method is applied to calculate the weight of each evaluation index. Finally, a case study show that the new method is applicable for both theoretical and practical purposes.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473544,no,undetermined,0
A High-Performance Multi-user Service System for Financial Analytics Based on Web Service and GPU Computation,"In finance, securities, such as stocks, funds, warrants and bonds, are actively traded in financial markets. Abundance of market data and accurate pricing of a security can help the practitioners arbitrage or hedge their position. It can also help researhers and traders design better trading strategies. In this work, we develop a pricing and data/information service system for financial analytics with the following goals: (1) supporting fast pricing and data/information services for massive multiple users, (2) saving cost in hardware equipments and reducing energy consumption, and (3) easy maintenance and service expansion of the system. To achieve the first two goals, we use one traditional server paired with one Tesla C1060 and a set of PCs each paired with an Nvidia x275, and enhance GPU performance using a set of simple yet very effective optimization techniques. For the third goal, we develop our service system based on Web Service and the Service-Oriented-Architecture design principles. Our initial experiment results show that our GPU-based service system can deliver 4.8 tera flops computing speed, which achieves over 6000% performance increase compared to a cluster of eight Intel i7 quad-core servers. Cost-wise, the GPU-based service system costs 40% of the i7 server cluster, and consumes 50% of energy that is required by the i7 cluster. We also gives an overview of our system architecture and describe the workflow for processing pricing and information service requests.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634351,no,undetermined,0
A qualitative and quantitative assessment method for software process model,"Aimed at the problems of high-cost, non-completeness and ambiguity existed in the traditional assessment methods for Software Process Model (SPM), this paper proposes a qualitative and quantitative assessment method. On the basis of assessment theory and domain experience of SPM, the unclear goals in the project start-up phase are qualitatively described in the form of problem set and expert problem domain is constructed as the assessment criteria on the implementation capability of SPM in the proposed method. With the integration of qualitative analysis and quantitative analysis by using a multi-index synthetic assessment algorithm of AHP (Analytic Hierarchy Process), the weight vector of goals and the reciprocal comparative matrix of problem domain are calculated and then the assessment result in dimensionless index is obtained. In order to reduce the cost of assessment, questionnaires instead of project tracking and audit are adopted to extract project goals in the method. Meanwhile, SPM is comprehensively assessed from four aspects including personnel, method, product and process, and the assessment result can intuitively reflect the capability of different SPM in a numerical form. Finally, an example of how to assess and choose four classical SPMs in the initial stage of a practical project is given out to show the validity of this assessment method.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552434,no,undetermined,0
The Architecture and Implementation of the New Generation Business System in a Commercial Bank,"At present, there are many problems in domestic commercial banks. This paper describes the fundamental solution to these problems through the presentation of a domestic commercial bank, which integrates various business channels by centralizing business platforms and altering the business model based on accounting treatment into the operation model of customer-oriented. It builds a new generation of business system that is divided into access layer, support layer, application service layer and business system layer which respectively implements the functions of docking client, unifying data format, exchanging data, database services and etc. This paper describes the new architecture, technical support and the physical configuration of the new system. The new system includes eight application subsystems: Core and auxiliary systems; EAI-synthetical front-end system; plug-business system; agency business system; managed application system; analytic applications system; Payment, UnionPay and other external front-end system; Fore-end service channel and their support system. Meanwhile, take the business process of core business inquiring and reports printing for example to show the effectiveness of the new system.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5208834,no,undetermined,0
A Reliable Service Quality Evaluation Model in the Web Services Community,"As the development of Web service, reliable service quality evaluation has become the key in the service optimization. This paper addresses this problem by proposing a reliable service quality evaluation model in the Web services community. The model initially imports Web service community aiming at evaluation unreliable when involved criterions are beyond service domain. The assistant domain Ontology besides the domain Ontology is defined, and criterion trees and criterion forest are built up to evaluate service reliably. Then in the model a new TFAHP which has fuzzy evaluation adjustment attribute is proposed, and approach of fuzzy comprehensive evaluation is improved. At the same time based on combination of them, a new evaluation algorithm to the model is created. Finally, an experiment is designed and performed to prove the feasibility and effectiveness of the model.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5696321,no,undetermined,0
The Analysis of the Hindering Factors of Implementation of Green Credit Policy Based on Fuzzy Comprehensive Evaluation,"Green credit policy as a new standard for bank loan has been known by more and more people. It is not only a new method to avoid bank's risks, but also a new measure to promote energy-saving, emission reduction and sustainable development. More qualitative analysis of green credit has been done. Yet, through the method of analytic hierarchy process (AHP) to ensure the weights of each index and setting up membership function by use of statistical methods, this paper evaluates the extent of obstacle of green credit and the size of the impact of various factors by fuzzy comprehensive evaluation. This paper has been designed to provide advice for decision-making.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5304083,no,undetermined,0
Tentative Application of Computer Simulation Technique to the Risk Evaluation of Large Concrete Dams,"Failure probability calculation is the scientific basis for the quantitative analysis of large dam risk and thus, probing the method of calculating large dams' failure probability is of considerable scientific significance and is urgently required by engineering application. While risk evaluation is required for individual large dams, no country can afford the failure calamity test on actual large dams in view of the vast investment in the large dam construction, and it is only possible to reveal the mechanism of the failure by means of computer simulation of process of failure occurrence, on the basis of both indoor model experiment and large-dam operation monitoring data. At present, in the field of large dam engineering, JC method and Monte Carlo method are generally recommended for the quantitative calculation of the failure probability. Due to the complexity of large dams' structure and their boundary conditions, the randomicity of the load and resistance force and the fact that there exists no analytic expression between the output and input quantities but the usual implicit function, the calculating of large dam's failure probability by means of JC method or Monte Carlo method cannot do without computer numerical simulation to some extent. A tentative approach is made in making use of computer simulation technique to generate the random number of random variables of both the load and the resistance force, to establish the state function of the failure modes and to conduct independent transformation of relevant random variables. Based on the computer simulation results, the failure probability of a large dam is calculated out by means of JC method and Monte Carlo method respectively.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384784,no,undetermined,0
Tactical cross-domain solutions: Current status and the need for change,"The rapid migration of system high information sharing to the tactical edge has made it imperative that the DoD reexamine tactical Cross Domain Solutions/Enterprise Services (CDS/ES). Prior to Operation Iraqi Freedom (OIF), information sharing requirements at the tactical edge were relatively few in number and nominal in terms of data throughput, data types, and users. Cross Domain Solutions (CDS) deployed back then were specialized, hardened, and resistant to hacking in the event of enemy overrun. Since OIF, both the volume of battle field system high tactical networks as well as the operational requirements to support each of these networks (i.e., increased data throughput, data types and variety of users) have significantly increased [On Point]. When combined with additional constraints inherent to the battle space such as low latency, Size, Weight and Power (SWaP), the current approach to addressing information sharing requirements in a tactical network breaks down. Taking the traditional, point-to-point approach by making a CDS smaller and more robust in the tactical environment may be adequate in the near term. However, this approach will not support the requirements levied upon next generation warfighting systems. In the future, interdependent tactical networks will be required to exhibit a dynamic (self organizing) nature, supporting adaptability and quick response to data ingress and egress. Nodes on these future networks will also need to operate with severely limited bandwidth and other operational and/or environment constraints. Therefore it is necessary to examine current and future information sharing requirements at the tactical edge from both the CDS/ES developer as well as user perspective. This position paper will discuss both perspectives in order to allow a better understanding of the current CD problem space, as well as gain insight into building the next generation CDS/ES.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5379749,no,undetermined,0
A research on the selection and evaluation of supplier for laptop,"According to the characteristics of laptop, a system of supplier selection's indicators could be drawn up on the basis of references and experts queries. EXPERT CHOICE is adopted in calculating data which is from AHP. Using EXPORT CHOICE tends to be easy and correct. The paper also discusses the application of AHP at the evaluation and selection of suppliers for laptop.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461333,no,undetermined,0
SpRay: A visual analytics approach for gene expression data,"We present a new application, SpRay, designed for the visual exploration of gene expression data. It is based on an extension and adaption of parallel coordinates to support the visual exploration of large and high-dimensional datasets. In particular, we investigate the visual analysis of gene expression data as generated by micro-array experiments; We combine refined visual exploration with statistical methods to a visual analytics approach that proved to be particularly successful in this application domain. We will demonstrate the usefulness on several multidimensional gene expression datasets from different bioinformatics applications.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333911,no,undetermined,0
Preconditioned Electric Field Integral Equation Using Calderon Identities and Dual Loop/Star Basis Functions,"An analytic preconditioner for the electric field integral equation, based on the Calderon identities, is considered. It is shown, based on physical reasoning, that RWG elements are not suitable for discretizing the electric field integral operator appearing in the preconditioner. Instead, the geometrically dual basis functions proposed by Buffa and Christiansen are used. However, it is found that this preconditioner is vulnerable to roundoff errors at low frequencies. A loop/star decomposition of the Buffa-Christiansen basis functions is presented, along with numerical results demonstrating its effectiveness.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812227,no,undetermined,0
Playing roles in design patterns: An empirical descriptive and analytic study,"This work presents a descriptive and analytic study of classes playing zero, one, or two roles in six different design patterns (and combinations thereof). First, we answer three research questions showing that (1) classes playing one or two roles do exist in programs and are not negligible and that there are significant differences among the (2) internal (class metrics) and (3) external (change-proneness) characteristics of classes playing zero, one, or two roles. Second, we revisit a previous work on design patterns and changeability and show that its results were, in a great part, due to classes playing two roles. Third, we exemplify the use of the study results to provide a ranking of the occurrences of the design patterns identified in a program. The ranking allows developers to balance precision and recall.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306327,no,undetermined,0
"BioExtract ServeräóîAn Integrated Workflow-Enabling System to Access and Analyze Heterogeneous, Distributed Biomolecular Data","Many in silico investigations in bioinformatics require access to multiple, distributed data sources and analytic tools. The requisite data sources may include large public data repositories, community databases, and project databases for use in domain-specific research. Different data sources frequently utilize distinct query languages and return results in unique formats, and therefore researchers must either rely upon a small number of primary data sources or become familiar with multiple query languages and formats. Similarly, the associated analytic tools often require specific input formats and produce unique outputs which make it difficult to utilize the output from one tool as input to another. The BioExtract Server (http://bioextract.org) is a Web-based data integration application designed to consolidate, analyze, and serve data from heterogeneous biomolecular databases in the form of a mash-up. The basic operations of the BioExtract Server allow researchers, via their Web browsers, to specify data sources, flexibly query data sources, apply analytic tools, download result sets, and store query results for later reuse. As a researcher works with the system, their ??steps?? are saved in the background. At any time, these steps can be preserved long-term as a workflow simply by providing a workflow name and description.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4626945,no,undetermined,0
Evaluation on the factors affecting the quality of social audit based on FAHP,"In recent years, the exposure of the mansions of Silver, Enron and other major financial scandals led an unprecedented credit crisis and moral crisis to the audit, and they also raises people's researching interest in the quality of audit and its impact factors. At present, domestic and foreign scholars have done lots of work on the factors affecting the quality of audit, but the literature is mostly qualitative not quantitative. This paper uses fuzzy analytic hierarchy process to determine the weight of main factors affecting the quality of auditing.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552257,no,undetermined,0
Flood Hazard Evaluation and GIS in Guangzhou,"The author used both the analytic hierarchy process (AHP) and the factor superposition method to evaluate flood risk and vulnerability of environments subject to continuous hazards in areas and districts of Guangzhou. Natural environmental flood risk factors were evaluated as was the subsequent level of vulnerability to human activities in each area in Guangzhou. To express its spatial relationships, a Flood Hazard Evaluation map was created using GIS software. Analysis of the results produced the following conclusions: flood hazard intensity in Guangzhou is higher in the south and lower in the north; considered from west to east, western and eastern areas have a higher flood hazard intensity while the city's center is low. These findings are related to central Guangzhou being dominated by mountains and no major rivers, while the western and eastern areas consist of low-lying areas with many rivers and streams. Results from this study should serve as baseline information both for flood prevention and for its mitigation in Guangzhou and similar cities.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631139,no,undetermined,0
Grid-Based Region Management System Based on SOA Architecture Reseach,"Region management information system is a three-dimensional structure which contains management dimension, system architecture dimension and data dimension. In this article, the author introduced the grid management model into region management information system as management dimension, and designed the region management system using SOA as system architecture dimension, and used grid-based region's spatial data and other business data as data dimension. This paper took the workflow engine service platform as the system's core, combined with the natural semantic analytic location serve and mobile client system based on SOA architecture, formed a complete grid-based region management system designed based on SOA architecture.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5175176,no,undetermined,0
Fuzzy Comprehensive Evaluation of Corrosion-Resistant Properties of PTFE Compounded Fibers,"PTFE compounded fiber is the raw material of the industrial fabrics which have been applied for separating particles from industrial exhaust under harsh industrial conditions such as the oxidant and high temperature environments. It is a decision process with multiple factor analysis to select an optimal PTFE compounded fiber with good corrosion-resistant property among several samples. The study applied Analytic Hierarchy Process and a new fuzzy comprehensive evaluation method to evaluate the corrosion-resistant properties of four tested PTFE compounded fibers, which are PTFE fiber with chemical coat and PTFE fiber without chemical coat and PTFE/glass compounded fibers with membrane laminating and PTFE/glass compounded fibers with chemical coat. The results show that the compounded technology of PTFE/glass fibers and chemical coating can all improve the corrosion-resistant properties of the industrial fabrics against the oxidant and high temperature. It also can be concluded that the new fuzzy comprehensive evaluation method have advantages in the selection of the optimal one among many PTFE compounded fiber samples.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677055,no,undetermined,0
Generation and calibration of compositional performance analysis models for multi-processor systems,"The performance analysis of heterogeneous multi-processor systems is becoming increasingly difficult due to the steadily growing complexity of software and hardware components. To cope with these increasing requirements, analytic methods have been proposed. The automatic generation of analytic system models that faithfully represent real system implementations has received relatively little attention, however. In this paper, an approach is presented in which an analytic system model is automatically generated from the same specification that is also used for system synthesis. Analytic methods for performance analysis of a system can thus be seamlessly integrated into the multi-processor design flow which lays a sound foundation for designing systems with a predictable performance.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5289246,no,undetermined,0
Grey Analytic Hierarchy Process Applied to Effectiveness Evaluation for Crime Prevention System,"With the rapid development of industrialization and urbanization, crime prevention issues become serious in China each day. Many cities have established a number of crime prevention systems in order to maintain social stability. However, crime prevention system is a complex system, which is controlled by a number of interrelated factors and is difficult to estimate. To overcome the obstacles of conventional effectiveness evaluation for crime prevention system, this paper proposed an effectiveness evaluation method for crime prevention system which combined the advantages of the analytic hierarchy process (AHP) and a grey clustering method to guarantee the accuracy and objectivity of weight coefficients. After constructing an index system of crime prevention system effectiveness evaluation based on correlated factors, we calculated the weight of every index with AHP and gave an evaluation result by means of a grey clustering method. A case study was given to validate the design of the underlying grey analytic hierarchy process model. Results show the feasibility and reliability of the model, which will be helpful to realize the quantitative analysis in crime prevention system effectiveness evaluation and provide a decision support tool for decision makers.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5462408,no,undetermined,0
"Hierarchical Aggregation for Information Visualization: Overview, Techniques, and Design Guidelines","We present a model for building, visualizing, and interacting with multiscale representations of information visualization techniques using hierarchical aggregation. The motivation for this work is to make visual representations more visually scalable and less cluttered. The model allows for augmenting existing techniques with multiscale functionality, as well as for designing new visualization and interaction techniques that conform to this new class of visual representations. We give some examples of how to use the model for standard information visualization techniques such as scatterplots, parallel coordinates, and node-link diagrams, and discuss existing techniques that are based on hierarchical aggregation. This yields a set of design guidelines for aggregated visualizations. We also present a basic vocabulary of interaction techniques suitable for navigating these multiscale visualizations.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184827,no,undetermined,0
HLA-based parallel test grid simulation,"Reconfigured Parallel Test Grid (RPTG) technology advanced a novel theory to improve test and ATE using efficiency. In order to analyze the function and efficiency of RPTG, it is important to simulate the RPTG environment authentically. Based on parallel and reconfigured characteristic of RPTG, HLA-based layered analytic simulation architecture was put forward. This solution can exploit the parallelism of the analytic simulation applications and support them well; also test tasks could be distributed effectively and dynamically.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613631,no,undetermined,0
I/Q imbalance effects in quadrature ‘£‘î modulators äóî Analysis and signal processing,"This article focuses on the in-phase/quadrature (I/Q) imbalance problem in quadrature ‘£‘î modulators (Q‘£‘îM). In the literature, such quadrature modulators have been seen as a promising possibility for software defined radio (SDR) receivers, but matching the quadrature circuits is an inevitable problem for this kind of systems. In this article, an analytic closed-form model will be derived for a mismatched first order Q‘£‘îM by means of the modulator transfer functions and the underlying implementation components. In addition, notching the signal transfer function (STF) on the desired signal mirror frequency inside the modulator is proposed for cancelling the mirror frequency interference originating from the blocking signal on that band. This is shown to be effective in case of the modulator feedback branch mismatches and can be implemented with simple additional feedforward coefficient(s) compared to ordinary modulator.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440963,no,undetermined,0
Game Theoretical Formulation of Network Selection in Competing Wireless Networks: An Analytic Hierarchy Process Model,Network Selection mechanisms play an important role in ensuring quality of service for users in a multi-network environment. These mechanisms handle the selection of an optimal wireless network to satisfy a user request. This paper proposes a radio resource management framework for integrated network selection mechanism control in multi- network environment as an interaction game between the service providers and customers in non-cooperative manner to maximize their rewards. The proposed scheme comprises two steps. The first applies the analytic hierarchy process (AHP) to determine the relative weights of the evaluative criteria according to customer preferences and network condition. The second calculates the payoffs based on the relative weights calculated in the previous step and a utility function evaluation by each wireless network of each customer. Analytical and simulation results demonstrate the effectiveness of proposed model to achieve optimum network utility for the wireless networks along with optimizing the customer's satisfaction. The proposed model is preliminary and its contribution is to create an admission policy that can adapt to different coverage areas of a wireless network and depends on the priority of customers and their requirements.,2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337435,no,undetermined,0
G-MicroRNA: A New Tool for MicroRNA Genomics,"With the post-genome era coming, it is interesting to establish a new bioinformatics database, further integrate related information of the sequences and develop new methods to reveal the mechanism of life based on databases of sequence. We have built a professional bioinformatics database of non-coding miRNA named G-microRNA by means of PHP, MYSQL and so on. The database integrates more than 2,000 miRNA sequences including several model organisms-human, arabidopsis thaliana and virus. The pre-calculated Z curves and characteristics of each sequence are displayed in the database. Simultaneously, we have improved analytic softwares, selected famous predicting tools and hundreds of outstanding references, and shared our database in the internet. The construction of the database will provide much useful information and operation platform for bioinformatics further research.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5455020,no,undetermined,0
Full-Information Lookups for Peer-to-Peer Overlays,"Most peer-to-peer lookup schemes keep a small amount of routing state per node, typically logarithmic in the number of overlay nodes. This design assumes that routing information at each member node must be kept small so that the bookkeeping required to respond to system membership changes is also small, given that aggressive membership dynamics are expected. As a consequence, lookups have high latency as each lookup requires contacting several nodes in sequence. In this paper, we question these assumptions by presenting a peer-to-peer routing algorithm with small lookup paths. Our algorithm, called ldquoOneHop,rdquo maintains full information about the system membership at each node, routing in a single hop whenever that information is up to date and in a small number of hops otherwise. We show how to disseminate information about membership changes quickly enough so that nodes maintain accurate complete membership information. We also present analytic bandwidth requirements for our scheme that demonstrate that it could be deployed in systems with hundreds of thousands of nodes and high churn. We validate our analytic model using a simulated environment and a real implementation. Our results confirm that OneHop is able to achieve high efficiency, usually reaching the correct node directly 99 percent of the time.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641918,no,undetermined,0
Indicator-system based adaptive information assurance evaluation for aeronautical information system,"With the consideration of the system features, an indicator system of information assurance for aeronautical information system is established. The indicator system is based on SSE (Systems Security Engineering) theory, and DSC (Dynamic System Control) method. The indicator system is established from four aspects, stratagem, technique, management and engineering. Fuzzy-AHP (Analytic Hierarchy Process) is used to evaluate the static indicators. A quite good result is got.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5658434,no,undetermined,0
Influence factor analysis of product development process,"In the product development process considering design iteration, it's more difficult to reflect interrelationship between variables with simulation analysis. Especially there is no certain function expressing the effect of parameter interrelationship on the product development time. The existence of iteration complicates the structure of analytic model, and the metamodel can simplify the problem and reduce the costs. The paper analyzed the impact of the parameters on average completion time of product development in design structure maxim by building the metamodel, and identified the main impact factors.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674356,no,undetermined,0
Information Security Risk Assessment Methodology Research: Group Decision Making and Analytic Hierarchy Process,"Information security risk can be measured by probability of the potential risk incident and its impact. Various quantitative methodologies are given to compute information security risks, but among the existed research, seldom of them considered the difficulties of obtaining data of risk probability and risk impact. Considering the efficiency and operability of collecting data, as well as the effectiveness of output for risk management support, this paper presents a risk assessment methodology for information systems security with the application of group decision making and analytic hierarchy process methods. Procedure of this methodology is provided, and a test case is given to illustrate the effectiveness of this methodology.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718368,no,undetermined,0
Information support of corporate governance and strategic management using analytical software,"In the paper an information aspect of corporate governance and strategic management is considered. Relying on analysis of merits and limitations of current developments the concept of performance management information support system and the appropriate modeling approach are proposed. Finally, the possibilities of use of analytical information systems for corporate governance and strategic management are discussed.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5658550,no,undetermined,0
Foreword,Presents the welcome message from the conference proceedings.,2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431692,no,undetermined,0
Innovative video analytics for maritime surveillance,"In this paper, we are presenting a new software dedicated to maritime surveillance and security named AUTOMATIC SEA VISION<sup>Œ¬</sup> (ASV) (Waquet, 2007) ASV is the first entirely automatic optical software processing (video analytics) providing a smart solution to maritime safety and security issues, whether the intended use is for applications on board ships, or for protection of critical coastal areas and port facilities. The software processes in real time the output from multiple sensors (mainly IR camera, but also GPS, Inertial Navigation Unit...) in order to automatically detect any boat or object for a 24/7 surveillance. We rely on maritime environment and IR sensor data specificities to develop tailored algorithms to perform automatic object detection. These algorithms have been packaged into a complete solution which performs from data acquisition through a user friendly graphical user interface (GUI). Extensive tests have been carried out to validate our solution. We present a quantitative evaluation done on ground truth data. System performance has been calculated using Detection/False Alarm rates over multiple sequences acquired from numerous cameras. Moreover, qualitative evaluations have demonstrated the robustness of the system in numerous different weather conditions. The last section of this paper tackles algorithms enhancements as well as future evolutions of the system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5730280,no,undetermined,0
First Steps to Netviz Nirvana: Evaluating Social Network Analysis with NodeXL,"Social Network Analysis (SNA) has evolved as a popular, standard method for modeling meaningful, often hidden structural relationships in communities. Existing SNA tools often involve extensive pre-processing or intensive programming skills that can challenge practitioners and students alike. NodeXL, an open-source template for Microsoft Excel, integrates a library of common network metrics and graph layout algorithms within the familiar spreadsheet format, offering a potentially low-barrier-to-entry framework for teaching and learning SNA. We present the preliminary findings of 2 user studies of 21 graduate students who engaged in SNA using NodeXL. The majority of students, while information professionals, had little technical background or experience with SNA techniques. Six of the participants had more technical backgrounds and were chosen specifically for their experience with graph drawing and information visualization. Our primary objectives were (1) to evaluate NodeXL as an SNA tool for a broad base of users and (2) to explore methods for teaching SNA. Our complementary dual case-study format demonstrates the usability of NodeXL for a diverse set of users, and significantly, the power of a tightly integrated metrics/visualization tool to spark insight and facilitate sense-making for students of SNA.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284040,no,undetermined,0
FinVis: Applied visual analytics for personal financial planning,"FinVis is a visual analytics tool that allows the non-expert casual user to interpret the return, risk and correlation aspects of financial data and make personal finance decisions. This interactive exploratory tool helps the casual decision-maker quickly choose between various financial portfolio options and view possible outcomes. FinVis allows for exploration of inter-temporal data to analyze outcomes of short-term or long-term investment decisions. FinVis helps the user overcome cognitive limitations and understand the impact of correlation between financial instruments in order to reap the benefits of portfolio diversification. Because this software is accessible by non-expert users, decision-makers from the general population can benefit greatly from using FinVis in practical applications. We quantify the value of FinVis using experimental economics methods and find that subjects using the FinVis software make better financial portfolio decisions as compared to subjects using a tabular version with the same information. We also find that FinVis engages the user, which results in greater exploration of the dataset and increased learning as compared to a tabular display. Further, participants using FinVis reported increased confidence in financial decision-making and noted that they were likely to use this tool in practical application.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333920,no,undetermined,0
Fault-Tolerant Algorithm for Distributed Primary Detection in Cognitive Radio Networks,"This paper attempts to identify the reliability of detection of licensed primary transmission based on cooperative sensing in cognitive radio networks. With a parallel fusion network model, the correlation issue of the received signals between the nodes in the worst case is derived. Leveraging the property of false sensing data due to malfunctioning or malicious software, the optimizing strategy, namely fault-tolerant algorithm for distributed detection (FTDD) is proposed, and quantitative analysis of false alarm reliability and detection probability under the scheme is presented. In particular, the tradeoff between licensed transmissions and user cooperation among nodes is discussed. Simulation experiments are also used to evaluate the fusion performance under practical settings. The model and analytic results provide useful tools for reliability analysis for other wireless decentralization-based applications (e.g., those involving robust spectrum sensing).",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4908281,no,undetermined,0
Exploring Temporal Egocentric Networks in Mobile Call Graphs,"The structure of customer communication network provides us the insights into the function of customers' relationships. In this paper, we use egocentric social network to explore how people manage their personal and group communications over time. Our primary goal is that our findings can provide business insights and help devise strategies for telecom service providers. We are interested in tracking changes in large-scale mobile networks and examining the evolution processes of customer egocentric networks. By defining several statistical metrics, we can investigate the egocentric networks' evolution trends and their communication patterns. We explore several temporal real-world mobile call graphs and find an interesting phenomenon in these temporal networks which is the neighboring vertices' egocentric networks have assortiative evolution trends. By taking a visual analytics approach, we track the changes in the customer egocentric networks and explore some highly correlated customers' egocentric networks visually. We detect several interesting communication patterns by visualizing the egocentric networks which may give us more hints on customers' communication trends in their egocentric networks.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5359482,no,undetermined,0
Intelligent Sensor Information System For Public Transport Œ_ To Safely GoŒƒ,"The Intelligent Sensor Information System (ISIS) is described. ISIS is an active CCTV approach to reducing crime and anti-social behavior on public transport systems such as buses. Key to the system is the idea of event composition, in which directly detected atomic events are combined to infer higher-level events with semantic meaning. Video analytics are described that profile the gender of passengers and track them as they move about a 3-D space. The overall system architecture is described which integrates the on-board event recognition with the control room software over a wireless network to generate a real-time alert. Data from preliminary data-gathering trial is presented.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597103,no,undetermined,0
Evaluation on Sustainable Development of Rapid Transit System Based on the Improved Entropy Method,"Bus rapid transit system in the application of China's urban transportation is in the ascendant. With the technical advantages of rail transportation, it's economical, beneficial and having less impact on the environment. And the significance of sustainable development determines its high research value. This article establishes a comprehensive evaluation index system of sustainable development, using AHP to determine the target indicator system and improving entropy proportion means, experts scoring, qualitative and quantitative analysis to evaluate this new mode of urban transportation and its sustainable development. This article innovatively introduced the application of entropy right method in the choosing of urban transport mode on its sustainable development. To verify the reliability and simplicity of the method through empirical analysis, we have concluded that bus rapid transit system is relative superior.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5288250,no,undetermined,0
Kalman filtering with multiple nonlinear-linear mixing state constraints,"This paper is devoted to the problem of state estimation for a class of dynamic system with multiple nonlinear-linear (NL) mixing state constraints. A computational algorithm was developed by C. Yang and E. Blasch to incorporate a positive definite quadratic form equality constraint into the Kalman filtering for the ground vehicle tracking. However, when the target tracked in multidimensional space, a single positive definite quadratic form constraint is not enough due to the involvement of other surface such as hyperboloid, paraboloid, or even plane. A analytic method of incorporating multiple nonlinear-linear mixing state constraints into the Kalman filter is presented here, and a sufficient condition is obtained that guarantee the existence of solution. The constraints may be time varying. At each time step the unconstrained Kalman filter solution is projected onto the state constraints surface. This significantly improves the prediction accuracy of the filter. The use of this algorithm is demonstrated on a nonlinear-linear mixing spacecraft positioning problem.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718030,no,undetermined,0
Kinematic Analysis of a Novel 4-UPS-UPU PCMM,"A novel 4-UPS-UPU PCMM (parallel coordinate measuring machine) with three translations and two rotations is proposed, and its kinematic analysis is studied systematically. First, the structure characteristics of the 4-UPS-UPU PCMM is analyzed. Second, a new solution for position analysis of 4-UPS-UPU PCMM, which without the construction of mathematical model, is put forward. Third, The curves of the length of actuating limbs respecting with motion of pose was obtained by Matlab software, the curves of velocity and acceleration were generated from the curve of the actuating limbs. The analytic results are verified by its simulation mechanism.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460237,no,undetermined,0
Evaluation of the Current Situation and Planning of the Greenspace System in Huaibei City Using GIS Based Network Analysis,"In rapid urbanization, how to construct the rational landscape network have great significances in keeping the urban ecosystem healthy and improving the quantity and quality of the current greenspace system. The basic research materials are urban current land use, greenspace current situation, green space system planning and satellite images of Huaibei in 2006. Supported by GIS , the research chooses the main urban area in Huaibei with an area greater than 10 hm<sup>2</sup> of afforestation space as the node, according to the analytic approach of the network, the research establishes the idealized ecological networks, through index alpha , index beta , index gamma, compare their network structure integrality, choose the best network. The paper analyzes with the landscape analysis software Fragstats, uses the method of landscape pattern analytical. It selects the indexes of many kinds of analysis landscape for use, analyzes the systematic current situation of greenspace, greenspace system planning and the idealized ecological network scheme of Huaibei in terms of landscape ecology. It is an effective route to evaluate and improve the greenspace system planning with the ecological network analysis combines with the pattern analysis of the landscape, and the ecological network scheme planning to be made on the greenspace system planning can further improve main quantity and quality with systematic current greenspace of urban area.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5301262,no,undetermined,0
Evaluation of Management Information System Based on Fuzzy AHP and Multiple Matter Elements,"It is a difficult task to do evaluation of management information systems, because the information systems have a lot of characteristics. According to the function requirements of management information systems (MIS), and in combination with the characteristics of MIS, the evaluation index system for MIS was established. We combine of fuzzy analytic hierarchy process and multiple matter elements to evaluate the management information system. In the end a case is used to test the validate atonality and reliability of the method. This electronic document is a ldquoliverdquo template.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231968,no,undetermined,0
Evaluation of Image Spam Classification System Based on AHP,"The ever increasing volumes of image-based spam e-mails are bringing more annoyance to Internet users, and how to filter it has become a pressing problem. This paper discussed the evaluation problem of image spam classification system using analytic hierarchy process (AHP). First propose the evaluating index system and build hierarchy structure model; Then build the comparative matrixes according to the rate of contribution the variant factors applied to the target; Finally calculate the index weight of each layer. At the end of this paper, the experiment results verify the correctness of our methods.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366905,no,undetermined,0
Evaluation Model for Computer Network Information Security Based on Analytic Hierarchy Process,"Evaluation for computer network information security is helpful for taking corresponding preventive measures. In order to obtain a comprehensive assessment of network security, analytic hierarchy process (AHP) model is proposed to assess the computer network information security. As the criteria and the relevant factors are decomposed hierarchically corresponding to evaluation and judgment of the problem, all kinds of factors of influencing network security are researched and the evaluation indexes for computer network information security are constructed, analytic hierarchy process (AHP) evaluation model for computer network information security is constructed on the basis of the evaluation indexes. The experimental results indicate that the evaluation of computer network information security by analytic hierarchy process is effective.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5369486,no,undetermined,0
How interactive visualization can assist investigative analysis: Views and perspectives from domain experts,"Interactive visualization could become an essential tool in the work of investigative analysts. Visualization could help analysts to explore large collections of data and documents, supporting the analysts investigative sense-making processes. This panel gathers recognized leaders from three important domains, investigative reporting, biosciences (genomics), and intelligence analysis, that all include a fundamental investigative analysis component. The panelists will provide a glimpse into their worlds, describing and illustrating the data they examine, the goals and methods of their analysts, and the culture of their respective professions. In particular, the panelists will explore how visualization could potentially benefit investigators from their domain and they will provide guidance for visualization researchers seeking to collaborate with their colleagues.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332411,no,undetermined,0
ICE--visual analytics for transportation incident datasets,"Transportation systems are being monitored at an unprecedented scope resulting in tremendously detailed traffic and incident databases. While the transportation community emphasizes developing standards for storing this incident data, little effort has been made to design appropriate visual analytics tools to explore the data, extract meaningful knowledge, and represent results. Analyzing these large multivariate geospatial datasets is a non-trivial task. A novel, web-based, visual analytics tool called ICE (incident cluster explorer) is proposed as an application that affords sophisticated yet user-friendly analysis of transportation incident datasets. Interactive maps, histograms, two-dimensional plots and parallel coordinates plots are four visualizations that are integrated together to allow users to simultaneously interact with and see relationships between multiple visualizations. Accompanied by a rich set of filters, users can create custom conditions to filter data and focus on a smaller dataset. Due to the multivariate nature of the data, a rank-by-feature framework has been expanded to quantify the strength of relationships between the different fields.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5211551,no,undetermined,0
Business Intelligence analytics without conventional data warehousing,"The implementation of a Business Intelligence (BI) solution in an environment where traditional barriers prohibit incorporating analytics in operational and strategic decision making remains challenging. The solution must overcome a very tight budget, severe restrictions on data access due to security concerns, and a tradition of using conventional legacy tools for primary reporting. We propose a method that minimizes both the ETL (Extract Transform Load) and data warehousing components of the solution and allows for agile development and incremental adoption. This is achieved by taking advantage of the organization's current legacy reporting structure as the basis and then building a BI reporting layer on top for a high level view of the information. Our methodology has demonstrated benefits in the form of reduced complexity, reduced risk, reduced cost in both time and money, and a solution that provides a greater return on investment.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6018712,no,undetermined,0
Evaluation of Teacher's Performance in Independent Colleges Based on AHP and Multi-Level Matter Element Extension Measurement Models,"Evaluation of Teacher's Performance is the important instrument of carrying out teaching management, and is important assurance of inspiring teachers to further efforts, improving teaching quality and carrying out scientific teaching management and decision. In this article we studied target system of Independent Colleges' teacher performance evaluation setting up, how to count target weight based on AHP, multi-level matter element extension measurement evaluation model and arithmetic realization. Considering the deficiency of evaluation methods of teacher's performance, the evaluation index system was established by analyzing the major factors affecting evaluation of teacher's Performance y. Applied the multilevel matter element theory into the multilevel matter element evaluation model for teacher's performance and expatiated the analyzing method and process. Aiming at the qualification requirements that are proposed to teachers by universities, considering the three tasks of teaching work, Scientific Research and Working Attitude, following up the principles that are integrated with the transition from the qualitative to quantitative, a multi-level matter element extension measurement performance evaluation model for teachers is raised; it can make the performance evaluation for teachers more scientific, objective and reasonable.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676753,no,undetermined,0
One Method to Evaluate the Safety of Train Control Center,"The train control center which plays a central role of message communication is a pivotal sub-system of the china train control system. On the basis of developing and debugging the engineering prototype of the train control center, an index system for evaluating the safety of the train control center was set up. Based on the analytic hierarchy process, a safety evaluation method for the train control center was proposed. Using the technology of verification, validation and accreditation, comprehensive analysis to the safety evaluation index of the train control center was carried on. At last, the safety evaluation index of the whole train control center was obtained, which is useful to the nationalized train control center.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777186,no,undetermined,0
On new screw propeller driven by a flexible shaft consisted of spheral gears and gimbals,"Taking the new screw propeller driven by a flexible transmission shaft into consideration, some analytic work has been brought out on the relationship between its input and output motions with its math model. With the help of the CFD software FLUENT, the paper also presented some research work on the hydro-resistance force and pressure distribution of the propeller, which is set into three stances with a aberrancy of 0deg, 45deg, 90deg . Under each stance, three kinds of flow velocities were given, they were 0.5 m/s, 1 m/s, 2 m/s respectively. The hydrokinetic simulation showed the screw propeller driven by a controllable flexible had better hydrokinetics than the one driven by a traditional rigid shaft. A prototype was made and some experiments had been carried out to validate the motion and function of the new screw propeller.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5173829,no,undetermined,0
C-NEDAT: A cognitive network engineering design analytic toolset for MANETs,"Future force networks of the types envisioned for the network centric warfare (NCW) paradigm will be highly diverse, with the diversity spanning a wide range of (a) requirements (e.g., need for capacity, connectivity, survivability), (b) resources (e.g., radios with widely different capabilities and `smart' (e.g., Software Defined Radios (SDRs)), and (c) environments (e.g., urban, rural). The need to facilitate robust and adaptable communications in such networks has in turn triggered research in the area of cognitive networks that have the ability to `learn' and generate real-time control actions to adapt to the wide diversity of requirements, resources and environments. However, the combination of diversity and äóìsmartäó networking exacerbates the problem of generating reliable and robust network designs. We present in this paper, our work on the use of cognitive mechanisms to assist with the design and analysis of robust NCW-like networks. Based on formal network-science based approaches, our Cognitive Network Engineering Design Analytic Toolset (C-NEDAT) provides for a systematic way to design, analyze and maintain robustness of future force MANETs. We provide in this paper an overview of the key functional modules and design capabilities of C-NEDAT and present example results.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680350,no,undetermined,0
Novel overlay/underlay cognitive radio waveforms using SD-SMSE framework to enhance spectrum efficiency- part i: theoretical framework and analysis in AWGN channel,"Recent studies suggest that spectrum congestion is primarily due to inefficient spectrum usage rather than spectrum availability. Dynamic spectrum access (DSA) and cognitive radio (CR) are two techniques being considered to improve spectrum efficiency and utilization. The advent of CR has created a paradigm shift in wireless communications and instigated a change in FCC policy towards spectrum regulations. Within the hierarchical DSA model, spectrum overlay and underlay techniques are employed to enable primary and secondary users to coexist while improving overall spectrum efficiency. As employed here, spectrum overlay exploits unused (white) spectral regions while spectrum underlay exploits underused (gray) spectral regions. In general, underlay approaches use more spectrum than overlay approaches and operate below the noise floor of primary users. Spectrally modulated, spectrally encoded (SMSE) signals, to include orthogonal frequency domain multiplexing (OFDM) and multi-carrier code division multiple access (MC-CDMA), are candidate CR waveforms. The SMSE structure supports and is well suited for CR-based software defined radio (SDR) applications. This paper provides a general soft decision SMSE (SDSMSE) framework that extends the original SMSE framework to achieve synergistic CR benefits of overlay and underlay techniques. This extended framework provides considerable flexibility to design overlay, underlay and hybrid overlay/underlay waveforms that are scenario dependent. Overlay/underlay framework flexibility is demonstrated herein for a family of SMSE signals, including OFDM and MC-CDMA. Analytic derivation of CR error probability for overlay and underlay applications is presented. Simulated performance analysis of overlay, underlay and hybrid overlay/underlay waveforms is also presented and benefits discussed, to include improved spectrum efficiency and channel capacity maximization. Performance analysis of overlay/underlay CR waveform in fading channels wil- l be discussed in Part II of the paper.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5351675,no,undetermined,0
Clear and Precise Specification of Ecological Data Management Processes and Dataset Provenance,"With the availability of powerful computational and communication systems, scientists now readily access large, complicated derived datasets and build on those results to produce, through further processing, yet other derived datasets of interest. The scientific processes used to create such datasets must be clearly documented so that scientists can evaluate their soundness, reproduce the results, and build upon them in responsible and appropriate ways. Here, we present the concept of an <i>analytic web</i>, which defines the scientific processes employed and details the exact application of those processes in creating derived datasets. The work described here is similar to work often referred to as ??scientific workflow,?? but emphasizes the need for a semantically rich, rigorously defined process definition language. We illustrate the information that comprises an analytic web for a scientific process that measures and analyzes the flux of water through a forested watershed. This is a complex and demanding scientific process that illustrates the benefits of using a semantically rich, executable language for defining processes and for supporting automatic creation of process provenance metadata.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5191091,no,undetermined,0
Convergency and Error Estimate of Nonlinear Fredholm Fuzzy Integral Equations of the Second Kind by Homotopy Analysis Method,"A powerful, easy-to-use analytic tool for nonlinear problems in general, namely the homotopy analysis method, is further improved and systematically described through a typical nonlinear problem. In this paper, a Nonlinear Fredholm fuzzy integral equations is solved by using the homotopy analysis method(HAM). The approximate solution of this equation is calculated in the form of a series which its components are computed easily. The convergence and error estimate of the proposed method are proved.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445598,no,undetermined,0
Notice of Retraction<BR>An Evaluation System of Finance Method Bases on AHP,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>First, this paper constructs a financial evaluation system, and then directly against the various relative importance of evaluation system at all levels and indicates, and the difficulty to determine scientifically to set up determine matrix of all levels, and use AHP to determine the financial evaluation index weight and conduct a strict consistency test of the level of single-sort, and a total sort. The result of the evaluation is objective and practical, and has the characteristics of stability and accuracy, and has the value of popularization and application.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5197239,no,undetermined,0
Cross Enterprise Improvements Delivered via a Cloud Platform: A Game Changer for the Consumer Product and Retail Industry,"Gaining visibility into their retail supply chain has become a top priority for the Consumer Product (CP) industry. However, taking a äóìdo-it-yourselfäó approach to the problem is proving to be both expensive and complex. Cloud Computing, with its on-demand provisioning capability on shared resources, has emerged as a new paradigm to address the challenges of the CP industry. In this paper, we describe a framework for deployment of business analytic solutions on a Cloud platform. We illustrate the benefits of the approach in context of the Demand Driven Business Analytic solution that provides demand signals to CP manufacturers.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557274,no,undetermined,0
Database architecture (R)evolution: New hardware vs. new software,"The last few years have been exciting for data management system designers. The explosion in user and enterprise data coupled with the availability of newer, cheaper, and more capable hardware have lead system designers and researchers to rethink and, in some cases, reinvent the traditional DBMS architecture. In the space of data warehousing and analytics alone, more than a dozen new database product offerings have recently appeared, and dozens of research system papers are routinely published each year. In this panel, we will ask our panelists, a mix of industry and academic experts, which of those trends will have lasting effects on database system design, and which directions hold the biggest potential for future research. We are particularly interested in the differences in views and approaches between academic and industrial research.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447682,no,undetermined,0
New Techniques for Visualising Web Navigational Data,"Understanding Web navigational data is a crucial task for Web analysts as it may influence the Web site improvement process. However, major Web analytics tools do not provide enough insight, taking into account the vast amount of data available in Web serverspsila log files. Moreover, some analysts argue that those tools have a lack of rich visualisations that enable the exploration of such data. In this paper we introduce new techniques applied to a highly interactive and exploratory tool, to allow drilling down through Web usage data. The system uses a set of coordinated visual abstractions from Web site structure and userspsila navigation to provide different perspectives and hence, to assist the conversion of data into knowledge.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190803,no,undetermined,0
DataStorm An Ontology-Driven Framework for Cloud-Based Data Analytic Systems,"Cloud-based systems have proven to be a powerful technology for building data-intensive applications. However, the process of designing and deploying such applications is still primarily a manual one. There is a need for mechanisms and tools to help automate the required development steps. Using the Semantic Web ontology language OWL and the Hadoop platform we have developed a number of models and associated software tools that provide an end-to-end solution for designing and deploying cloud-based systems. This solution supports the construction of detailed models of data dependencies and their validation. It also enables generation and deployment of cloud-based data flows from those models. We illustrate its use for detecting alarm scenarios using data from vast underwater sensor-network.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575575,no,undetermined,0
nCompass Service Oriented Architecture for Tacit Collaboration Services,"nCompass is a flexible, Service Oriented Architecture (SOA) designed to support the research and deployment of advanced tacit collaboration technology services for analysts. nCompass allows a significantly larger number of individual analytic capabilities, applications and services to be integrated together quickly and effectively. Service integration results are described from several computational tacit collaboration experiments conducted with open source intelligence analysts working with open source data. Key to nCompass is the technical framework and unique analytic event logging schema that supports context sharing across diverse applications and services. It is by combining the analyst with shared context across multiple advanced computational capabilities in a system of systems that a breakthrough in collaborative open source analysis can be achieved. This paper introduces the nCompass framework and integration platform, describes key nCompass core services, and provides results on functional synergies achieved through technology service integration with nCompass.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190860,no,undetermined,0
Multiple step social structure analysis with Cytoscape,Cytoscape is a popular open source tool for biologists to visualize interaction networks. We find that it offers most of the desired functionality for visual analytics on graph data to guide us in the identification of the underlying social structure. We demonstrate its utility in the identification of the social structure in the VAST 2009 Flitter Mini Challenge.,2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333961,no,undetermined,0
Modeling and Performance Analysis of Software Rejuvenation Policies for Multiple Degradation Systems,"Software rejuvenation is a preventive and proactive technology to counteract the phenomenon of software aging and system failures and to improve the system reliability. In this paper we present and analyze three software rejuvenation policies for an operational software system with multiple degradations, called preemptive rejuvenation, delayed rejuvenation and mixed rejuvenation. These policies consider both history data and current running state, and the rejuvenation action is triggered on the basis of predetermined performance threshold and rejuvenation interval respectively. Continuous-time Markov chains are used to describe the analytic models. To evaluate these polices expediently, we utilize deterministic and stochastic Petri nets to solve the models. Numerical results show that the deployment of software rejuvenation in the system leads to significant improvement in availability and throughput. These three rejuvenation policies are better than the standard rejuvenation policy, and the mixed policy is the best one.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254254,no,undetermined,0
Metric-based Evolution Analysis and Evaluation of Software Core Assets Library,"The core assets library is the most important component of any software product line; it evolved responding to organization businesses, origination object, technology renovation and time. From the metric of the core assets library, this paper gives several measurements and their definitions which correlate with the evolution of a core assets library. It is combing with data processing, putting forward an evaluation model with the evolution of a core assets library,using analytic hierarchy process to estimate the core assets librarypsilas evolution level, illuminating with an example at last.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959431,no,undetermined,0
Deductive method with Sample Problems on Computational Object Knowledge Base and construct to intelligent educational softwares,"In computer science and information technology, ontology has been researched and developed in application for knowledge representation. Ontology COKB-ONT (Computational Object Knowledge Base) is an ontology was researched and applied in designing knowledge base systems, such as domain of knowledge about analytic geometry, linear algebra. However, when dealing with a practical problem, we often do not immediately find a new solution, but we search related problems which have been solved before and then proposing an appropriate solution for the problem. In this paper, the extension model of ontology COKB-ONT has been presented. In this model, Sample Problems, which are related problems, will be used like the experience of human about practical problem, simulate the way of human thinking about finding solution of problem. This extension model is applied to construct a program for solving plane geometry problems in middle school.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5641419,no,undetermined,0
Merging visual analysis with automated reasoning: Using Prajna to solve the traffic challenge,"The Internet traffic challenge required the development of a custom application to analyze internet traffic patterns coupled with building access records. To solve this challenge, the author applied the Prajna Project, an open-source Java toolkit designed to provide various capabilities for visualization, knowledge representation, semantic reasoning, and data fusion. By applying some of the capabilities of Prajna to this challenge, the author could quickly develop a custom application for visual analysis. The author determined that he could solve some of the analytical components of this challenge using automated reasoning techniques. Prajna includes interfaces to incorporate automated reasoners into visual applications. By blending the automated reasoning processes with visual analysis, the author could design a flexible, useful application to solve this challenge.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332481,no,undetermined,0
LSAView: A tool for visual exploration of latent semantic modeling,"Latent Semantic Analysis (LSA) is a commonly-used method for automated processing, modeling, and analysis of unstructured text data. One of the biggest challenges in using LSA is determining the appropriate model parameters to use for different data domains and types of analyses. Although automated methods have been developed to make rank and scaling parameter choices, these approaches often make choices with respect to noise in the data, without an understanding of how those choices impact analysis and problem solving. Further, no tools currently exist to explore the relationships between an LSA model and analysis methods. Our work focuses on how parameter choices impact analysis and problem solving. In this paper, we present LSAView, a system for interactively exploring parameter choices for LSA models. We illustrate the use of LSAView's small multiple views, linked matrix-graph views, and data views to analyze parameter selection and application in the context of graph layout and clustering.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333428,no,undetermined,0
Design considerations for the gate circuit in distributed amplifiers,"An analytic model for the design of the elementary cell of the gate circuit in distributed amplifiers is discussed. The used approach is based on the image parameter representation of two-port networks. A closed form expression for the input impedance of the amplifier is provided. Design considerations for the gate circuit are presented based on this analytical result. The simulation of the gate network is performed by means of a commercial software package, including the physical models of the microstrip transmission lines and discontinuities present in the actual structure. Moreover, an hybrid circuit with the same configuration of the gate network has been manufactured. The agreement between experimental numerical and analytical data is remarkable.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5471255,no,undetermined,0
Design optimization of meta-material transmission lines for linear and non-linear microwave signal processing,"The possibility to use CRLH (Composite Right/Left-Handed) cells to realize both distributed wide-band filters for linear signal processing and non-linear devices like frequency doublers is investigated analytically and numerically. Full-wave electromagnetic simulations are performed for the filtering structure by means of a commercial software package and confirm the validity of the analytic results. Numerical results for CRLH NLTL (Non-Linear Transmission Line) obtained by using the Microwave Office are discussed, providing design considerations about the synthesis of such a component.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5612958,no,undetermined,0
Investment risks evaluation on high-tech projects based on analytic hierarchy process and BP neural network,"In view of the existing problems of investment risks evaluation on high-tech industry projects such as a lack of systematic, with too much subjectivity and from the point to improve evaluation efficiency and effectiveness, the paper combined analytic hierarchy process (AHP) with BP neural network to establish a new and suitable risk evaluation model of high-tech projects. Firstly, we applied AHP to construct a comprehensive risk evaluation index system and screened the evaluation indexes according to their weights. On this basis, using MATLAB software with BP neural network model, we carried out example simulations and results were analyzed. The results showed that the combination model of analytic hierarchy process with BP neural network model (AHP-BPNN) is effective.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5268119,no,undetermined,0
Interactive poster: A proposal for sharing user requirements for visual analytic tools,"Although many in the community have advocated user-centered evaluations for visual analytic environments, a significant barrier exists. The users targeted by the visual analytics community (law enforcement personnel, professional information analysts, financial analysts, health care analysts, etc.) are often inaccessible to researchers. These analysts are extremely busy and their work environments and data are often classified or at least confidential. Furthermore, their tasks often last weeks or even months. It is simply not feasible to do such long-term observations to understand their jobs. How then can we hope to gather enough information about the diverse user populations to understand their needs? Some researchers, including the author, have been successful in getting access to specific end-users. A reasonable approach, therefore, would be to find a way to share user information. This work outlines a proposal for developing a handbook of user profiles for use by researchers, developers, and evaluators.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333474,no,undetermined,0
Dynamic application and model updating service,"Army Battle Command (BC) applications routinely collaborate to perform BC operations, particularly information collection, management, and analysis. Successful collaboration is highly dependent on timely and efficient access to common authoritative sources of information. These sources must satisfy the dynamic needs of active units, incorporating features that arise only after the initial deployment of the software infrastructure. Therefore, the data models for these sources must also be dynamic and allow for continuous modification while robustly propagating changes to affected applications. The applications then need to remain online (i.e., no recompilation or redeployment) to support ongoing operations while seamlessly adapting to any changes. To support these requirements, we are developing a Dynamic Application and Model Updating System (DYNAMUS). DYNAMUS provides a dynamic data model that includes the metadata required to track and visualize changes to the data model, a Software Development Kit (SDK) that supports the development of dynamic applications, and editing tools that support model architects while preventing application disruptions. These capabilities support timely and non-disruptive distribution of updates while maintaining operational effectiveness, usefulness, interoperability, and multilevel security.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5478481,no,undetermined,0
Information Seeking Can Be Social,"For reasons ranging from obligation to curiosity, users have a strong inclination to seek information from others during the search process. Search systems using statistical analytics over traces left behind by others can help support the search experience.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803887,no,undetermined,0
IEEE International symposium on performance analysis of systems and software,The following topics are dealt with: differentiating the roles of IR measurement and simulation for power and temperature-aware design; user- and process-driven dynamic voltage and frequency scaling; accuracy of performance counter measurements; GARNET: a detailed on-chip network model inside a full-system simulator; Cetra: a trace and analysis framework for the evaluation of Cell BE; Zesto: a cycle-level simulator for highly detailed microarchitecture exploration; Lonestar: a suite of parallel irregular program; exploring speculative parallelism in SPEC2006; machine learning based online performance prediction for runtime parallelization and task scheduling; WARP: enabling fast CPU scheduler development and evaluation; CMPSchedSim: evaluating OS/CMP interaction on shared cache management; understanding the cost of thread migration for multi-threaded Java applications running on a multicore platform; the data-centricity of Web 2.0 workloads and its impact on server performance; characterizing and optimizing the memory footprint of De Novo Short Read DNA sequence assembly; analytic model of optimistic software transactional memory; analyzing CUDA workloads using a detailed GPU simulator; evaluating GPUs for network packet signature matching; online compression of cache-filtered address traces; analysis of the TRIPS prototype block predictor; experiment flows and microbenchmarks for reverse engineering of branch predictor structures; analyzing the impact of on-chip network traffic on program phases for CMPs; SuiteSpecks and SuiteSpots: a methodology for the automatic conversion of benchmarking programs into intrinsically checkpointed assembly code; accurately approximating superscalar processor performance from traces; and QUICK: a flexible full-system functional model.,2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4919624,no,undetermined,0
Dynamic Programming Based Hybrid Strategy for Offline Cursive Script Recognition,"In domain of analytic cursive word recognition, there are two main approaches: explicit segmentation based and implicit segmentation based. However, both approaches have their own shortcomings. To overcome individual weaknesses, this paper presents a hybrid strategy for recognition of strings of characters (words or numerals). In a two stage dynamic programming based, lexicon driven approach, first an explicit segmentation is applied to segment either cursive handwritten words or numeric strings. However, at this stage, segmentation points are not finalized. In the second verification stage, statistical features are extracted from each segmented area to recognize characters using a trained neural network. To enhance segmentation and recognition accuracy, lexicon is consulted using existing dynamic programming matching techniques. Accordingly, segmentation points are altered to decide true character boundaries by using lexicon feedback. A rigorous experimental protocol shows high performance of the proposed method for cursive handwritten words and numeral strings.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445714,no,undetermined,0
E-business system integrations of enterprise post merger and acquisition based on continuous assurance - A case study,"Recently, because of the competitive environment, Taiwan's high-technology industries have undergone enormous change in a series of enterprise mergers and acquisitions. The enterprise mergers and acquisitions led us to study the e-business integration impact issues on continuous assurance. Our study found that the information technology integration strategies and assurance process integration methods had an impact on their continuous assurance performances and effectiveness. We used case study research methodology that including data collection, case attendant interview, data analysis and summary finding steps for creating new model applications of information technology integrations. The information technology integration assurance (ITIS) model steps include information technology infrastructure building, assurance process integrations, database immigration and analytic and alarm methods. Our findings close the literature theory gaps between IT integrations and continuous assurance; it will be applicable for information technology integration and continuous assurance about post-enterprise mergers and acquisitions.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5544452,no,undetermined,0
Efficient procedure to evaluate electromagnetic transients on three-phase transmission lines,"This paper presents a hybrid way mixing time and frequency domain for transmission lines modelling. The proposed methodology handles steady fundamental signal mixed with fast and slow transients, including impulsive and oscillatory behaviour. A transmission line model is developed based on lumped elements representation and state-space techniques. The proposed methodology represents an easy and practical procedure to model a three-phase transmission line directly in time domain, without the explicit use of inverse transforms. The proposed methodology takes into account the frequency-dependent parameters of the line, considering the soil and skin effects. In order to include this effect in the state matrices, a fitting method is applied. Furthermore the accuracy of proposed the developed model is verified, in frequency domain, by a simple methodology based on line distributed parameters and transfer function related to the input/output signals of the lumped parameters representation. In addition, this article proposes the use of a fast and robust analytic integration procedure to solve the state equations, enabling transient and steady-state simulations. The results are compared with those obtained by the commercial software Microtran (EMTP), taking into account a three-phase transmission line, typical in the Brazilian transmission system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551072,no,undetermined,0
Embedded systems design äóî Scientific challenges and work directions,"Summary form only given. The development of a satisfactory Embedded Systems Design Science provides a timely challenge and opportunity for reinvigorating Computer Science. Embedded systems are components integrating software and hardware jointly and specifically designed to provide given functionalities, which are often critical. They are used in many applications areas including transport, consumer electronics and electrical appliances, energy distribution, manufacturing systems, etc. Embedded systems design requires techniques taking into account extra-functional requirements regarding optimal use of resources such as time, memory and energy while ensuring autonomy, reactivity and robustness. Jointly taking into account these requirements raises a grand scientific and technical challenge: extending Computer Science with paradigms and methods from Control Theory and Electrical Engineering. Computer Science is based on discrete computation models not encompassing physical time and resources which are by their nature very different from analytic models used by other engineering disciplines. We summarize some current trends in embedded systems design and point out some of their characteristics, such as the chasm between analytical and computational models, and the gap between safety critical and best-effort engineering practices. We call for a coherent scientific foundation for embedded systems design, and we discuss a few key demands on such a foundation: the need for encompassing several manifestations of heterogeneity, and the need for design paradigms ensuring constructivity and adaptivity. We discuss main aspects of this challenge and associated research directions for different areas such as modelling, programming, compilers, operating systems and networks.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770926,no,undetermined,0
A high performance pair trading application,"This paper describes a high-frequency pair trading strategy that exploits the power of MarketMiner, a high-performance analytics platform that enables a real-time, market-wide search for short-term correlation breakdowns across multiple markets and asset classes. The main theme of this paper is to discuss the computational requirements of model formulation and back-testing, and how a scalable solution built using a modular, MPI-based infrastructure can assist quantitative model and strategy developers by increasing the scale of their experiments or decreasing the time it takes to thoroughly test different parameters. We describe our work to date which is the design of a canonical pair trading algorithm, illustrating how fast and efficient backtesting can be performed using MarketMiner. Preliminary results are given based on a small set of stocks, parameter sets and correlation measures.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161147,no,undetermined,0
XDB - A Novel Database Architecture for Data Analytics as a Service,"Parallel shared-nothing database systems are major platforms for efficiently analyzing large amounts of structured data. However, in order to offer SQL-like services for data analytics in the cloud, providers such as Amazon and Google do not use these systems as a basis. A major reason for this trend is that existing parallel shared-nothing database systems are expensive and that they do not fulfill many of the requirements such as elasticity and fault-tolerance needed for providing a service for data analytics in the cloud. In this paper, we present an overview of an elastic and fault-tolerant database system called XDB, which supports complex analytics. XDB builds on the following novel concepts: (1) a partitioning scheme that supports elasticity with regard to data and queries, (2) a cost-based fault-tolerance scheme that allows to recover from mid-query faults, and (3) adaptive parallelization techniques to better support complex analytical queries. XDB is implemented using a middleware approach on top of multiple nodes each hosting an instance of a single node database system (MySQL in our prototype). Initial experiments show that our novel concepts effectively support elasticity, fault-tolerance and complex analytics when compared to the traditional behavior of existing databases.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906766,no,undetermined,0
A Hardware Accelerated Semi Analytic Approach for Fault Trees with Repairable Components,"Fault tree analysis of complex systems with repairable components can easily be quite complicated and usually requires significant computer time and power despite significant simplifications. Invariably, software-based solutions, particularly those involving Monte Carlo simulation methods, have been used in practice to compute the top event probability. However, these methods require significant computer power and time. In this paper, a hardware-based solution is presented for solving fault trees. The methodology developed uses a new semi analytic approach embedded in a Field Programmable Gate Array (FPGA) using accelerators. Unlike previous attempts, the methodology developed properly handles repairable components in fault trees. Results from a specially written software-based simulation program confirm the accuracy and validate the efficacy of the hardware-oriented approach.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4809753,no,undetermined,0
QoS-based dynamic scheduling for manufacturing grid workflow,"Manufacturing grid (MG) workflow can be defined as the composition of manufacturing activities which execute on heterogeneous and distributed manufacturing resources in virtual organization to accomplish a specific goal. Uncertainties within MG environment pose new challenges for MG workflow. Dynamic scheduling is one of the most critical components in MG workflow. Through analyzing the QoS properties in MG, a scheduling architecture based on QoS for MG workflow is presented. Workflow engine can dynamically schedule the resources according the activity requirements and resource QoS capabilities. If multiple resources that meet the QoS requirements are available to an activity at a time, workflow scheduling can select the 'best match' resource according to scheduling algorithms based on AHP (analytic hierarchy process).",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1504254,no,undetermined,0
Generalized preview and delayed H<sup>äš_</sup> control: output feedback case,"A generalized H<sup>äš_</sup>control problem, which covers preview and delayed control strategies, is discussed in the output feedback setting. By introducing analytic solutions to the corresponding control/filtering operator Riccati equations, the solvability is clarified based on the fundamental solutions to ordinary differential equations. The solutions obtained here enable us to deal with important class of H<sup>äš_</sup>control problems, which include multiple input/output delays and the preview tracking strategies.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1583083,no,undetermined,0
Image up-sampling using total-variation regularization with a new observation model,"This paper presents a new formulation of the regularized image up-sampling problem that incorporates models of the image acquisition and display processes. We give a new analytic perspective that justifies the use of total-variation regularization from a signal processing perspective, based on an analysis that specifies the requirements of edge-directed filtering. This approach leads to a new data fidelity term that has been coupled with a total-variation regularizer to yield our objective function. This objective function is minimized using a level-sets motion that is based on the level-set method, with two types of motion that interact simultaneously. A new choice of these motions leads to a stable solution scheme that has a unique minimum. One aspect of the human visual system, perceptual uniformity, is treated in accordance with the linear nature of the data fidelity term. The method was implemented and has been verified to provide improved results, yielding crisp edges without introducing ringing or other artifacts.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510697,no,undetermined,0
Implementing Large-Scale Autonomic Server Monitoring Using Process Query Systems,"In this paper we present a new server monitoring method based on a new and powerful approach to dynamic data analysis: process query systems (PQS). PQS enables user-space monitoring of servers and, by using advanced behavioral models, makes accurate and fast decisions regarding server and service state. Data to support state estimation come from multiple sensor feeds located within a server network. By post-processing a system's state estimates, it becomes possible to identify, isolate and/or restart anomalous systems, thus avoiding cross-infection or prolonging performance degradation. The PQS system we use is a generic process detection software platform. It builds on the wide variety of system-level information that past autonomic computing research has studied by implementing a highly flexible, scalable and efficient process-based analytic engine for turning raw system information into actionable system and service state estimates",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498058,no,undetermined,0
Load Balancing using Grid-based Peer-to-Peer Parallel I/O,"In the area of grid computing, there is a growing need to process large amounts of data. To support this trend, we need to develop efficient parallel storage systems that can provide for high performance for data-intensive applications. In order to overcome I/O bottlenecks and to increase I/O parallelism, data streams need to be parallelized at both the application level and the storage device level. In this paper, we propose a novel peer-to-peer (P2P) storage architecture for MPI applications on grid systems. We first present an analytic model of our P2P storage architecture. Next, we describe a profile-guided data allocation algorithm that can increase the degree of I/O parallelism present in the system, as well as to balance I/O in a heterogeneous system. We present results on an actual implementation. Our experimental results show that by partitioning data across all available storage devices and carefully tuning I/O workloads in the grid system, our peer-to-peer scheme can deliver scalable high performance I/O that can address I/O-intensive workloads",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4154083,no,undetermined,0
Optimal control with unreliable communication: the TCP case,"The paper considers the linear quadratic Gaussian (LQG) optimal control problem in the discrete time setting and when data loss may occur between the sensors and the estimation-control unit and between the latter and the actuation points. We consider the case where the arrival of the control packet is acknowledged at the receiving actuator, as it happens with the common transfer control protocol (TCP). We start by showing that the separation principle holds. Additionally, we can prove that the optimal LQG control is a linear function of the state. Finally, building upon our previous results on estimation with unreliable communication, the paper shows the existence of critical arrival probabilities below which the optimal controller fails to stabilize the system. This is done by providing analytic upper and lower bounds on the cost functional.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1470488,no,undetermined,0
Paint deposition modeling for trajectory planning on automotive surfaces,"This research is focused on developing trajectory planning tools for the automotive painting industry. The geometric complexity of automotive surfaces and the complexity of the spray patterns produced by modern paint atomizers combine to make this a challenging and interesting problem. This paper documents our efforts to develop computationally tractable analytic deposition models for electrostatic rotating bell (ESRB) atomizers, which have recently become widely used in the automotive painting industry. The models presented in this paper account for both the effects of surface curvature as well as the deposition pattern of ESRB atomizers in a computationally tractable form, enabling the development of automated trajectory generation tools. We present experimental results used to develop and validate the models, and verify the interaction between the deposition pattern, the atomizer trajectory, and the surface curvature. Limitations of the deposition model with respect to predictions of paint deposition on highly curved surfaces are discussed. Note to Practitioners-The empirical paint deposition models developed herein, which are fit to experimental data, offer a significant improvement over models that are typically used in industrial robot simulations. The improved simulation results come without the computational cost and complexity of finite element methods. The models could be incorporated, as is, into existing industrial simulation tools, provided the users are cognizant of the model limitations with respect to highly curved surfaces. Although the models are based on readily available information, incorporating the models into existing robot simulation software would likely require support from the software vendor.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1514457,no,undetermined,0
Performance analysis of binary code protection,"Software protection technology seeks to prevent unauthorized observation or use of applications. Cryptography can be used to provide such protection, but imposes a potentially significant additional computation load. This paper examines the performance impact of two software protection techniques. We develop an analytic model and validate it using a detailed discrete-event simulator applied to memory reference traces of well-known benchmark programs. We find that even though the added workload may be large, that impact is often dominated by inherent costs of disk activity.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1574300,no,undetermined,0
Practical Performance Model for Optimizing Dynamic Load Balancing of Adaptive Applications,"Optimizing the performance of dynamic load balancing toolkits and applications requires the adjustment of several runtime parameters; however, determining sufficiently good values for these parameters through repeated experimentation can be an expensive and prohibitive process. We describe an analytic modeling method which allows developers to study and optimize adaptive application performance in the presence of dynamic load balancing. To aid tractibility, we first derive a ""bi-modal"" step function which simplifies and approximates task execution behavior. This allows for the creation of an analytic modeling function which captures the dynamic behavior of adaptive and asynchronous applications, enabling accurate predictions of runtime performance. We validate our technique using synthetic microbenchmarks and a parallel mesh generation application and demonstrate that this technique, when used in conjunction with the PREMA runtime toolkit, can offer users significant performance improvements over several well-known load balancing tools used in practice today.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1419848,no,undetermined,0
PULSATINGSTORE: An Analytic Framework for Automated Storage Management,"Self-management of large information technology components, such as DBMSs, has emerged as one important problem in the area of autonomic computing. In particular, automated storage management is critical for most data-intensive applications. The reason is that the storage maintenance cost manifests one of the biggest factors in the overall operational cost. At the same time, due to the interactive nature of most applications, users typically pose the QoS constraints on IO access performance. Hence it is crucial to ensure that the applications are not underprovisioned (giving rise to the risk of QoS violation) or over-provisioned (resulting in high operational costs). Such issue gets further complicated when the application workload keeps changing. In this paper, we present a novel analytic framework, PULSATINGSTORE, for autonomically managing the storage to balance the cost and performance in an online manner. In particular, given the workload characteristics of an application and storage QoS requirement, our PULSATINGSTORE framework is capable of scheduling the up-migration (in the case of under-provisioning) or down-migration (in the case of over-provisioning) with the optimal or near-optimal cost while still maintaining the QoS constraint.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1647830,no,undetermined,0
"Quasi-static solutions of multilayer elliptical, cylindrical coplanar striplines and multilayer coplanar striplines with finite dielectric dimensions - asymmetrical case","In this paper, fast, accurate, and simple analytic closed-form expressions are presented in order to calculate the quasi-TEM parameters of multilayer elliptical coplanar stripline (CPS), multilayer cylindrical CPS, and multilayer CPS, with finite dielectric dimensions by using conformal mapping techniques. The obtained computer-aided-design-oriented expressions are quite accurate and easy to apply in designing microwave integrated circuits and antenna applications. The results have also been compared with the ones of another conformal mapping method available in the literature and also confirmed analytically.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1550016,no,undetermined,0
A Longitudinal Study of the Use of a Collaboration Tool: A Breadth and Depth Analysis,"In this paper we present both a broad and deep look at the use of a collaboration tool in the intelligence community. Through an experimental program, intelligence analysts are given the opportunity to explore and use tools to determine if the tools provide sufficient value to be certified and moved into the analytic work environment. The goal of this program is to bring advanced technologies to the intelligence community through research and experimentation. New tools are evaluated using a metrics based assessment. Tools that successfully pass these evaluations are then introduced on an experimental network. Analysts employed by the experimental program work along side analysts in the intelligence community and look for opportunities where the experimental tools could be useful in current analytic processes. These uses are also evaluated to determine the value of the tools in the analytic environment.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1644110,no,undetermined,0
Reliability evaluation of Web-based software applications,"In diverse industrial and academic environments, the quality of software has been evaluated (validated) using different analytic studies. It is common practice in these environments to use statistical models for the assurance, control and evaluation of the quality of a software product or process. A number of industries in the safety-critical sector are forced nowadays to use such processes by industry-specific standards (e.g., the DO-178B standard for airborne software systems). The contribution of the present work is focused on the development of a methodology for the evaluation and analysis of the reliability of Web-based software applications. We tested our methodology in a Web-based software system and used statistical modeling theory for the analysis and evaluation of the reliability. The behavior of the system under ideal conditions was evaluated and compared against the operation of the system executing under real conditions. The results obtained demonstrated the effectiveness and applicability of our methodology.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1592207,no,undetermined,0
Requirements on worm mitigation technologies in MANETS,"This study presents an analysis of the impact of mitigation on computer worm propagation in mobile ad-hoc networks (MANETS). According to the recent DARPA BAA - defense against cyber attacks on MANETS (A.K. Ghosh 2004), ""one of the most severe cyber threats is expected to be worms with arbitrary payload that can infect and saturate MANET-based networks on the order of seconds"". Critical to the design of effective worm counter measures in MANET environments is an understanding of the propagation mechanisms and the performance of the mitigation technologies. This work aims to advance the security of these critical systems through increased knowledge of propagation mechanisms, performance and the effect of mitigation technologies. We present both analytic and simulation analysis of mitigation effectiveness. The ultimate goal of these studies is to develop an accurate set of performance requirements on mitigation techniques to minimize worm propagation in tactical, battlefield MANETS.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1443326,no,undetermined,0
Resource Allocation for Autonomic Data Centers using Analytic Performance Models,"Large data centers host several application environments (AEs) that are subject to workloads whose intensity varies widely and unpredictably. Therefore, the servers of the data center may need to be dynamically redeployed among the various AEs in order to optimize some global utility function. Previous approaches to solving this problem suffer from scalability limitations and cannot easily address the fact that there may be multiple classes of workloads executing on the same AE. This paper presents a solution that addresses these limitations. This solution is based on the use of analytic queuing network models combined with combinatorial search techniques. The paper demonstrates the effectiveness of the approach through simulation experiments. Both online and batch workloads are considered",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498067,no,undetermined,0
Search-based amorphous slicing,"Amorphous slicing is an automated source code extraction technique with applications in many areas of software engineering, including comprehension, reuse, testing and reverse engineering. Algorithms for syntax-preserving slicing are well established, but amorphous slicing is harder because it requires arbitrary transformation; finding good general purpose amorphous slicing algorithms therefore remains as hard as general program transformation. In this paper we show how amorphous slices can be computed using search techniques. The paper presents results from a set of experiments designed to explore the application of genetic algorithms, hill climbing, random search and systematic search to a set of six subject programs. As a benchmark, the results are compared to those from an existing analytical algorithm for amorphous slicing, which was written specifically to perform well with the sorts of program under consideration. The results, while tentative at this stage, do give grounds for optimism. The search techniques proved able to reduce the size of the programs under consideration in all cases, sometimes equaling the performance of the specifically-tailored analytic algorithm. In one case, the search techniques performed better, highlighting a fault in the existing algorithm",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566141,no,undetermined,0
Sentiment mining in WebFountain,"WebFountain is a platform for very large-scale text analytics applications that allows uniform access to a wide variety of sources. It enables the deployment of a variety of document-level and corpus-level miners in a scalable manner, and feeds information that drives end-user applications through a set of hosted Web services. Sentiment (or opinion) mining is one of the most useful analyses for various end-user applications, such as reputation management. Instead of classifying the sentiment of an entire document about a subject, our sentiment miner determines sentiment of each subject reference using natural language processing techniques. In this paper, we describe the fully functional system environment and the algorithms, and report the performance of the sentiment miner. The performance of the algorithms was verified on online product review articles, and more general documents including Web pages and news articles.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410217,no,undetermined,0
Sequential correction of perspective warp in camera-based documents,"Documents captured with hand-held devices, such as digital cameras often exhibit perspective warp artifacts. These artifacts pose problems for OCR systems which at best can only handle in-plane rotation. We propose a method for recovering the planar appearance of an input document image by examining the vertical rate of change in scale of features in the document. Our method makes fewer assumptions about the document structure than do previously published algorithms.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575576,no,undetermined,0
Smooth adaptive fitting of 3D models using hierarchical triangular splines,"The recent ability to measure quickly and inexpensively dense sets of points on physical objects has deeply influenced the way engineers represent shapes in CAD systems, animation software or in the game industry. Many researchers advocated to completely bypass smooth surface representations, and to stick to a dense mesh model throughout the design process. Yet smooth analytic representations are still required in standard CAD systems and animation software, for reasons of compactness, control, appearance and manufacturability. In this paper we present a new method for fitting a smooth adoptively refinable triangular spline surface of arbitrary topology to an arbitrary dense triangular mesh. The key ingredient in our solution is that adaptive fitting is achieved by 4-splitting triangular surface patches locally therefore no particular attention has to be paid the validity of an underlying subdivided mesh. Furthermore, the final surface is composed of low-degree polynomial patches that always join with Gl-continuity. The ability to adoptively refine the model allows to achieve a given approximation error with a minimal number of patches.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563206,no,undetermined,0
The essential components of software architecture design and analysis,"Summary form only given. Architecture analysis and design methods such as ATAM, QAW, ADD and CBAM have enjoyed modest success in recent years and are being adopted by many companies as part of their standard software development processes. They are used in the software lifecycle, as a means of understanding business goals and stakeholder concerns, mapping these onto an architectural representation, and assessing the risks associated with this mapping. These methods have evolved a set of shared component techniques. In this article the author describes how these techniques can be combined in countless ways to create needs-specific methods. The author demonstrates the generality of these techniques by describing a new architecture improvement method called APTIA (analytic principles and tools for the improvement of architectures). APTIA almost entirely reuses pre-existing techniques but in a new combination, with new goals and results. Lastly, the author exemplifies APTIA's use in improving the architecture of a commercial information system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607129,no,undetermined,0
Wordlength determination algorithms for hardware implementation of linear time invariant systems with prescribed output accuracy,"This paper proposes two novel algorithms for optimizing the hardware resources in finite wordlength implementation of linear time invariant systems. The hardware complexity is measured by the exact internal wordlength used for each intermediate data. The first algorithm formulates the design problem as a constrained optimization, from which an analytic closed-form solution of the internal wordlengths subject to a prescribed output accuracy can be determined by the Lagrange multiplier method. The second algorithm is based on a discrete optimization method called the marginal analysis method, and it yields the desired wordlengths in integer values. Both approaches are found to be very effective and they are well-suited to large scale systems such as software radio receivers. Design examples show that the proposed algorithms offer better results and a lower design complexity than conventional methods.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1465160,no,undetermined,0
Field Modeling for the CESR-C Superconducting Wiggler Magnets,"Superconducting wiggler magnets for operation of the CESR electron-storage ring at energies as low as 1.5 GeV have been designed, built and installed in the years 2000 to 2004. Finite-element models of field quality have been developed and various sources of field errors have been investigated and compared to field measurements. Minimization algorithms providing accurate analytic representations of the wiggler fields have been established. We present quantitative descriptions of field modeling, of measured field quality and of the accuracy achieved in the analytic functions of the field.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1591102,no,undetermined,0
Earth return path impedances of underground cables for the two-layer earth case,"The influence of earth stratification on underground power cable impedances is investigated in this paper. A rigorous solution of the electromagnetic field for the case of underground conductors and a two-layer earth is presented. Analytic expressions for the self and mutual impedances of the cable are derived. The involved semi-infinite integrals are calculated by a novel, numerically stable, and efficient integration scheme. Typical single-core cable arrangements are examined for a combination of layer depths and earth resistivities, based on actual measurements. The accuracy of the results over a wide frequency range is justified by a proper finite-element method formulation. The differences in cable impedances due to earth stratification are presented. Finally, a simple switching transient simulation is examined to evaluate the influence of the earth stratification on transient voltages and currents.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1458895,no,undetermined,0
Design comparison of two rotating electrical machines for 42 V electric power steering,"The proposed paper presents two design procedures of rotating electrical machines for 42 V embedded application. Particularly, for an electrical power steering, a three-phase interior permanent magnet synchronous machine (PMSM) fed by a switch redundant power converter and a six-phase induction machine (IM6) fed by a new type of six switches converter are designed for future 42 V DC system. For the PMSM, the magnetic circuit has been fully designed using the optimization from analytic and finite-element based software. For the IM6, a classical magnetic circuit coming from a traditional three-phase squirrel-cage low power induction machine has been used. The final design results are compared on the basis of the power-to-weight ratio",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531377,no,undetermined,0
Boundary Case of Pulse Propagation Analytic Solution in the Presence of Interference and Higher Order Dispersion,"Higher order dispersion is one of the most important limiting factors in high-speed optical transmission systems. In this paper the pulse propagation problem under the influence of higher order dispersion is considered by treating the fiber as a linear medium. The influence of interference is also considered and analytic solution of pulse propagation in the presence of interference and higher order dispersion is given. Interference is time and phase shifted in regard to useful signal. We consider the worst case, i.e. case when phase shift is pi",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1572172,no,undetermined,0
Browsers to support awareness and social interaction,"Information sharing and social interaction are the Web's main features that have enabled online communities to abound and flourish. However, the Web is lacking cues and browsing mechanisms for the online social spaces. The challenge of creating social browsing tools to access such social information and patterns is of interest as a visual analytic problem for two reasons. Browsers that combine social visualizations and tools let newcomers and visitors explore information and patterns. Here we present social browsers for two Web communities. In addition to the novel visualizations and representation of two facets of a group's identity, our work has three other notable contributions.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333629,no,undetermined,0
COTS acquisition process: incorporating business factors in COTS vendor evaluation taxonomy,"The increasingly prevalent use of COTS components has attracted a huge capital pool to the industry. The result is an industry that is characterized by strong change forces and weak resistance. Under such environment, weaker players are constantly replaced by stronger players, and older technologies are constantly replaced by emerging technologies. This phenomenon has brought about a new class of risk to the COTS acquirers. These risk factors include the vendor's financial stability and technology capability. However, the existing COTS vendor evaluation taxonomies remain product centric, focusing only on product functionality and costs. We extend the taxonomies to incorporate business factors in the vendor evaluation process, and the resulting process is called VERPRO. The VERPRO decision making tool, which is based on the analytic hierarchy process, allows the acquirers to incorporate vendor business factors into the selection criteria.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357893,no,undetermined,0
Exact analysis of a class of GI/G/1-type performability models,"We present an exact decomposition algorithm for the analysis of Markov chains with a GI/G/1-type repetitive structure. Such processes exhibit both M/G/1-type & GI/M/1-type patterns, and cannot be solved using existing techniques. Markov chains with a GI/G/1 pattern result when modeling open systems which accept jobs from multiple exogenous sources, and are subject to failures & repairs; a single failure can empty the system of jobs, while a single batch arrival can add many jobs to the system. Our method provides exact computation of the stationary probabilities, which can then be used to obtain performance measures such as the average queue length or any of its higher moments, as well as the probability of the system being in various failure states, thus performability measures. We formulate the conditions under which our approach is applicable, and illustrate it via the performability analysis of a parallel computer system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1308668,no,undetermined,0
Finding trading patterns in stock market data,"This article describes our design and evaluation of a multisensory human perceptual tool for the real-world task domain of stock market trading. The tool is complementary in that it displays different information to different senses - our design incorporates both a 3D visual and a 2D sound display. The results of evaluating the tool in a formal experiment are complex. The data mined in this case study is bid-and-ask data - also called depth-of-market data - from the Australian Stock Exchange. Our visual-auditory display is the bid-ask-land-scape, which we developed over much iteration with the close collaboration of an expert in the stock market domain. From this domain's perspective, the project's principal goal was to develop a tool to help traders uncover new trading patterns in depth-of-market data. In this article, we not only describe the design of the bid-ask-landscape but also report on a formal evaluation of this visual-auditory display. We tested nonexperts on their ability to use the tool to predict the future direction of stock prices.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333627,no,undetermined,0
IEEE Computer Graphics and Applications,,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317926,no,undetermined,0
Information retrieval with distributed databases: analytic models of performance,"The major emphasis is on analytical techniques for predicting the performance of various collection fusion scenarios. Knowledge of analytical models of information retrieval system performance, both with single processors and with multiple processors, increases our understanding of the parameters (e.g., number of documents, ranking algorithms, stemming algorithms, stop word lists, etc.) affecting system behavior. While there is a growing literature on the implementation of distributed information retrieval systems and digital libraries, little research has focused on analytic models of performance. We analytically describe the performance for single and multiple processors, both when different processors have the same parameter values and when they have different values. The use of different ranking algorithms and parameter values at different sites is examined.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1264782,no,undetermined,0
Object dependency of resolution in reconstruction algorithms with interiteration filtering applied to PET data,"In this paper, we study the resolution properties of those algorithms where a filtering step is applied after every iteration. As concrete examples we take filtered preconditioned gradient descent algorithms for the Poisson log likelihood for PET emission data. For nonlinear estimators, resolution can be characterized in terms of the linearized local impulse response (LLIR). We provide analytic approximations for the LLIR for the class of algorithms mentioned above. Our expressions clearly show that when interiteration filtering (with linear filters) is used, the resolution properties are, in most cases, spatially varying, object dependent and asymmetric. These nonuniformities are solely due to the interaction between the filtering step and the Poisson noise model. This situation is similar to penalized likelihood reconstructions as studied previously in the literature. In contrast, nonregularized and postfiltered maximum-likelihood expectation maximization (MLEM) produce images with nearly ""perfect"" uniform resolution when convergence is reached. We use the analytic expressions for the LLIR to propose three different approaches to obtain nearly object independent and uniform resolution. Two of them are based on calculating filter coefficients on a pixel basis, whereas the third one chooses an appropriate preconditioner. These three approaches are tested on simulated data for the filtered MLEM algorithm or the filtered separable paraboloidal surrogates algorithm. The evaluation confirms that images obtained using our proposed regularization methods have nearly object independent and uniform resolution.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281997,no,undetermined,0
Optimal object state transfer - recovery policies for fault tolerant distributed systems,"Recent developments in the field of object-based fault tolerance and the advent of the first OMG FT-CORBA compliant middleware raise new requirements for the design process of distributed fault-tolerant systems. In this work, we introduce a simulation-based design approach based on the optimum effectiveness of the compared fault tolerance schemes. Each scheme is defined as a set of fault tolerance properties for the objects that compose the system. Its optimum effectiveness is determined by the tightest effective checkpoint intervals, for the passively replicated objects. Our approach allows mixing miscellaneous fault tolerance policies, as opposed to the published analytic models, which are best suited in the evaluation of single-server process replication schemes. Special emphasis has been given to the accuracy of the generated estimates using an appropriate simulation output analysis procedure. We provide showcase results and compare two characteristic warm passive replication schemes: one with periodic and another one with load-dependent object state checkpoints. Finally, a trade-off analysis is applied, for determining appropriate checkpoint properties, in respect to a specified design goal.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311947,no,undetermined,0
Power laws for monkeys typing randomly: the case of unequal probabilities,"An early result in the history of power laws, due to Miller, concerned the following experiment. A monkey types randomly on a keyboard with N letters (N>1) and a space bar, where a space separates words. A space is hit with probability p; all other letters are hit with equal probability (1-p)/N. Miller proved that in this experiment, the rank-frequency distribution of words follows a power law. The case where letters are hit with unequal probability has been the subject of recent confusion, with some suggesting that in this case the rank-frequency distribution follows a lognormal distribution. We prove that the rank-frequency distribution follows a power law for assignments of probabilities that have rational log-ratios for any pair of keys, and we present an argument of Montgomery that settles the remaining cases, also yielding a power law. The key to both arguments is the use of complex analysis. The method of proof produces simple explicit formulas for the coefficient in the power law in cases with rational log-ratios for the assigned probabilities of keys. Our formula in these cases suggests an exact asymptotic formula in the cases with an irrational log-ratio, and this formula is exactly what was proved by Montgomery.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1306541,no,undetermined,0
Reliability improvement of Web-based software applications,"In diverse industrial and academic environments, the quality of the software has been evaluated (validated) using different analytic studies. It is a common practice on these environments to use statistical models for the assurance, control and evaluation of the quality of a software product or process. A number of industries in the safety-critical sector are forced nowadays to use such processes by industry-specific standards (e.g., the DO-178B standard for airborne software systems). The contribution of the present work is focused on the development of a methodology for the evaluation and analysis of the reliability of Web-based software applications. We tested our methodology in a Web-based software system and used statistical modeling theory for the analysis and evaluation of the reliability. The behavior of the system under ideal conditions was evaluated and compared against the operation of the system executing under real conditions. The personal software process (PSP) was introduced in our methodology for improving the quality of the process and the product. The evaluation + improvement (Ei) process is performed in our methodology to evaluate and improve the quality of the software system. The results obtained demonstrated the effectiveness and applicability of our methodology.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357959,no,undetermined,0
Route throughput analysis for mobile multi-rate wireless ad hoc networks,"The mobile ad hoc networks (MANETs) have received a lot of attention recently. While many routing protocols have been proposed for MANETs based on different criteria, few have considered the impact of multi-rate communication capability that is supported by many current WLAN products. Given a routing path, this paper provides an analytic tool to evaluate the expected throughput of the route, assuming that hosts move following the discrete-time, random-walk model. The derived result can be added as another metric for route selection. Simulation results are also presented.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1363835,no,undetermined,0
The impact of free-riding on peer-to-peer networks,"Peer-to-peer networking is gaining popularity as a architecture for sharing information goods and other computing resources. However, these networks suffer from a high level of free-riding, whereby some users consume network resources without providing any network resources. The high levels of free-riding observed by several recent studies have led some to suggest the imminent collapse of these communities as a viable information sharing mechanism. Our research develops analytic models to analyze the behavior of P2P networks in the presence of free-riding. In contrast to previous predictions, we find that P2P networks can operate effectively in the presence of significant free-riding. However, we also show that without external incentives, the level of free-riding in P2P networks is higher than socially optimal. Our research also explores the implications of these findings for entrepreneurs, network designers, and copyright holders.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265472,no,undetermined,0
The multi-agent rendezvous problem - the asynchronous case,"This paper is concerned with the collective behavior of a group of n > 1 mobile autonomous agents, labelled 1 through n, which can all move in the plane. Each agent is able to continuously track the positions of all other agents currently within its ""sensing region"" where by an agent's sensing region is meant a closed disk of positive radius r centered at the agent's current position. The multi-agent rendezvous problem is to devise ""local"" control strategies, one for each agent, which without any active communication between agents, cause all members of the group to eventually rendezvous at single unspecified location. This paper describes a family of asynchronously functioning strategies for solving the problem. Correctness is established appealing to the concept of ""analytic synchronization"".",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1430329,no,undetermined,0
The operation of the ITS architecture - a case study of public bus service in Taiwan,"The paper presents an analytic approach to generate the market packages for the operation via the intelligent transportation system (ITS) architecture. This analytic approach uses the concepts of the ""use case diagram"" and ""sequence diagram"" defined by the unified modeling language (UML). By following the steps, we take Taiwan's public bus service as an example and generate the market package effectually, public bus monitoring and public bus scheduling and dispatching, which is corresponding to the real world situation. We also point out the academic models in market packages that could be the patent products. Consequently, driving the establishment of ITS market package not only conduces to ITS development but also promotes the knowledge economic in our society.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1297107,no,undetermined,0
Visual analytics in the pharmaceutical industry,"With the flood of data across all aspects of the pharmaceutical industry, information visualization is emerging as a critical component of discovery, development, and business. But it's a new class of visualizations that is having the greatest impact. Higher-level summaries that can provide a framework for understanding the immense volumes of data and that reveal unexpected relationships have come to the forefront. And, the ability to use these visualizations to cross-domains and data types provides the ability to integrate analyses and support fast, effective decisions.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333621,no,undetermined,0
A generic toolkit for multivariate fitting designed with template metaprogramming,We present a toolkit developed to perform Unbinned Maximum Likelihood and Chi-square fits for parameters and yields estimates. The design of the toolkit is based on template metaprogramming in order to provide users with a generic interface to minimization tools. The default implementation is based on ROOT wrapper of MINUIT. Commonly used Probability Density Functions (PDFs) are provided with the toolkit and users may add custom PDFs for their own analyses. Description of fit models can be done using a package for symbolic manipulation of analytic functions.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1546479,no,undetermined,0
A time-dependent Hamilton-Jacobi formulation of reachable sets for continuous dynamic games,"We describe and implement an algorithm for computing the set of reachable states of a continuous dynamic game. The algorithm is based on a proof that the reachable set is the zero sublevel set of the viscosity solution of a particular time-dependent Hamilton-Jacobi-Isaacs partial differential equation. While alternative techniques for computing the reachable set have been proposed, the differential game formulation allows treatment of nonlinear systems with inputs and uncertain parameters. Because the time-dependent equation's solution is continuous and defined throughout the state space, methods from the level set literature can be used to generate more accurate approximations than are possible for formulations with potentially discontinuous solutions. A numerical implementation of our formulation is described and has been released on the web. Its correctness is verified through a two vehicle, three dimensional collision avoidance example for which an analytic solution is available.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1463302,no,undetermined,0
"Advanced system modeling for fast, iterative, fully 3D positron-emission-tomography reconstruction","Iterative algorithms for PET reconstruction, e.g. maximum-likelihood-expectation-maximization (MLEM), are well established and often yield superior image quality compared to conventional analytic reconstruction methods. Especially, the incorporation of detector blurring within the modeling of the scanner response can significantly improve the image quality. However, the blurring modeling is accompanied by a reduction of the rate of convergence. Therefore, the statistical accuracy of the acquired data limits the improvements depending on the specific system sampling. We have developed a fully 3D MLEM-reconstruction software for a Siemens/CTI HR+ PET scanner including an appropriate modeling. Here, the true parameters of the detector resolution appear not adequate for the modeling, because strong artefacts in the images are observed. Therefore, we have adapted the parameters of the detector resolution model so that the artefacts disappeared, but that the image resolution was still better than without using the detector resolution model.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1507853,no,undetermined,0
An LQG Optimal Linear Controller for Control Systems with Packet Losses,"Motivated by control applications over lossy packet networks, this paper considers the Linear Quadratic Gaussian (LQG) optimal control problem in the discrete time setting and when packet losses may occur between the sensors and the estimation-control unit and between the latter and the actuation points. Previous work [1] shows that, for protocols where packets are acknowledged at the receiver (e.g. TCP- like protocols), the separation principle holds. Moreover, in this case the optimal LQG control is a linear function of the estimated state and there exist critical probabilities for the successful delivery of both observation and control packets, below which the optimal controller fails to stabilize the system. The existence of such critical values is determined by providing analytic upper and lower bounds on the cost functional, and stochastically characterizing their convergence properties in the infinite horizon. Finally, it turns out that when there is no feedback on whether a control packet has been delivered or not (e.g. UDP-like protocols), the LQG optimal controller is in general nonlinear, as shown in [2]. There exists a special case, i.e. the observation matrix C is invertible and there is no output noise. In this case this paper shows that the optimal control is linear and critical values for arrival probabilities exist and can be computed analytically.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1582198,no,undetermined,0
"""GeoAnalytics"" - Exploring spatio-temporal and multivariate data","The voluminous nature of social scientific, spatial-temporal statistical databases calls for high interactive performance and creative integrated information and geo-visualization tools. A solution to this challenge can be found in the emerging visual analytics (VA), a science of analytical reasoning facilitated by interactive visual interfaces and innovative visualization and is now actively pursued by research groups worldwide. In this paper, we present a tool called ""GeoAnalytics"", based on the principles behind VA. Our objective is to define new suitable approaches and tools for exploring time variant and multivariate attributes simultaneous including a spatial dimension. We introduce parallel coordinates integrated with time series and trend graph that serves as the visual control panel for the application. Multivariate attribute dynamic queries can express simultaneously queries involving time varying spatial data. VA encourages the need to build a bridge between the advantages of both human perception and computer science technologies. The sense of immediacy and speed-of-thought interaction is achieved in our dynamically linked components and maximum allocation of screen area for visual displays that helps users stay focused on their work and shortens their time to enlightenment",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648237,no,undetermined,0
A Matlab-Based Simulation of System Stability In Frequency-Field Analysis,"The frequency-field analysis to a control system says the system's steady-state response when it has a sine signal input. Use this analytic method, control system's specification can be found directly and the method is very simple. Under this we can solve many questions such as avoiding resonance, restraining molest, improving system stability and so on. Matlab (Matrix Laboratory) is software designed for researching of matrix theory, linear algebra and numerical value accounting by Ph.D Clever Moler, the doyen scientist of MathWorks. Now Matlab software has developed into an edition of 7.1 and has been adopted widely by more and more researchers and engineers for its wide account ability and OOP GUI. The application of system stability simulation in frequency-field analysis to a linear system is described. We share our experiences and intuitions gained and encourage other control process to develop simulation-based system analysis",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691854,no,undetermined,0
An improved AHP method in performance assessment,"In order to reduce subjective errors in the traditional analytic hierarchy process (AHP), an improved AHP method integrated with orthogonal experimental design was presented in this paper. The new method combined both AHP and orthogonal design principles to make decisions, objectives in assessment system and values computed through mapping and satisfying function respectively as the corresponding factors, levels and results in orthogonal experimental design. With software SAS analysis, priorities on performance were to be ranked out accordingly. Application has showed that the method can improve the accuracy of AHP in performance assessment.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1340551,no,undetermined,0
Manufacturing Execution Systems (MES) assessment and investment decision study,"Manufacturing execution systems (MES), a category of industrial software for the manufacturing environment, is grasping increasing attention, due to its unique ability to improve manufacturing performance, and then to have a positive impact on the financial performance of the company. For many executives, whether to implement MES to improve manufacturing and financial performance is an issue they must encounter. Since what MES brings about are not only benefits and potential opportunities, but also costs and potential risks, a comprehensive and systematic assessment is absolutely necessary before making such a critical investment decision. This paper proposes a decision methodology for MES investment decision making. In addition, a general decision model with respect to Benefits, Opportunities, Costs and Risks merits is provided. The decision methodology and decision model are validated by an undershirt manufacturer in China, by applying analytic network process (ANP) method.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274757,no,undetermined,0
"Electronics at Nanoscale: Fundamental and Practical Challenges, and Emerging Directions","In electronics, i.e. when using charge transport and change of electromagnetic fields in devices and systems, non-linearity, collective effects, and a hierarchy of design across length and time scales is central to efficient information processing through manipulation and transmission of bits. Silicon-based electronics brings together a systematic interdependent framework that connects software and hardware to reproducibility, speed, power, noise margin, reliability, signal restoration and communication, low defect count, and an ability to do predictive design across the scales. In the limits of nanometer scale, the dominant practical constraints arise from power dissipation in ever smaller volumes and of efficient signal interconnectivity commensurate with the large density of devices. These limitations are tied to the physical basis in charge transport and changes of fields, and equally apply to other materials äóñ hard, soft or molecular. At the largest scale, the limitations arise from partitioning and hierarchical apportionment for system performance, ease of design and manufacturing. Power management, behavioral encapsulation, fault tolerance, congestion avoidance, timing, placement, routing, electromagnetic cross-talk, etc. all need to be addressed from the perspective of centimeter scale. We take a hierarchical view of the underlying fundamental and practical challenges of the conventional and unconventional approaches using the analytic framework appropriate to the length scale to distinguish between fact and fantasy, and to point to practical emerging directions with a system-scale perspective.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609776,no,undetermined,0
Evaluation Models for E-Learning Platform: an AHP approach,"Our ""information-oriented"" society shows an increasing exigency of life-long learning. In such framework, the e-learning approach is becoming an important tool to allow the flexibility and quality requested by such a kind of learning process. In the recent past, a great number of online platforms have been introduced on the market showing different characteristics and services. With a plethora of e-learning providers and solutions available in the market, there is a new kind of problem faced by organizations consisting in the selection of the most suitable e-learning suite. This paper proposes a model for describing, characterizing and selecting e-learning platform. The e-learning solution selection is a multiple criteria decision-making problem that needs to be addressed objectively taking into consideration the relative weights of the criteria for any organization. We formulate the quoted multi criteria problem as a decision hierarchy to be solved using the analytic hierarchy process (AHP). In this paper we will show the general evaluation strategy and some obtained results using our model to evaluate some existing commercial platforms",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4116906,no,undetermined,0
Games Theory and Software Defined Radios,"The use of game theory in software designed radio networks is studied using OPNET. In order to model the spectrum usage, radio frequency interference avoidance, and distributed radio resource management, the behavior of software defined radios are predicted using game theory in an analytic mathematical framework. The wireless networks consist of devices that dynamically reconfigure themselves to respond to any air-interface or data format. Interfering radios however affect the adaptation schemes used on a link by link basis. Cognitive radio is an enhancement on traditional software radio design that can help establish a design model. Cognitive radios employ a cognition cycle to alter their actions in response to changes in the environment through the use of state machines. A smart network presents particularly difficult challenges to the analysis of radio resource management, as changes that one node makes may influence the decisions that other nodes make, so network planning remains a difficult task. The radios are the players in the game; the strategies in the game are based on the transmitter choosing the value of the adaptive link characteristic that maximizes the spectral efficiency, while meeting a BER constraint. The goal of the game is to maximize your winnings. Here the winnings would be for a radio to accrue the most bandwidth, or in our case, to achieve a particular performance target. Additionally, these approaches rest upon the assumption of higher-order rationality, i.e. the ability of a node to independently and recursively analyze a best response to other network nodes OPNET's pipeline processing can provide a solution path unavailable with other tools. We use OPNET to model changes in waveform appearance due to the interfering effects of neighboring communication links. In order to successfully model these networks, it will be necessary to determine if the network will eventually reach a steady state. With a game theoretic analysis, these network s- - teady states can be identified from the Nash Equilibriums of its associated game. In other traditional approaches this is not possible and demonstrates the more rapid convergence of the game results to modeling and simulation",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4086382,no,undetermined,0
Have Green - A Visual Analytics Framework for Large Semantic Graphs,"A semantic graph is a network of heterogeneous nodes and links annotated with a domain ontology. In intelligence analysis, investigators use semantic graphs to organize concepts and relationships as graph nodes and links in hopes of discovering key trends, patterns, and insights. However, as new information continues to arrive from a multitude of sources, the size and complexity of the semantic graphs will soon overwhelm an investigator's cognitive capacity to carry out significant analyses. We introduce a powerful visual analytics framework designed to enhance investigators' natural analytical capabilities to comprehend and analyze large semantic graphs. The paper describes the overall framework design, presents major development accomplishments to date, and discusses future directions of a new visual analytics system known as Have Green",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035749,no,undetermined,0
Identification of Software Specifications through Quality Function Deployment,"The present paper describes the procedure that has been used to determine the software specifications for a simulation component of an information platform. The procedure has been designed and implemented within the GRailChem project. The initiative was funded under a French-German scheme of cooperation and aimed to determine whether the intended information platform could actually improve the effectiveness and the efficiency of the freight transportation between Germany and France. The suggested procedure follows the general steps of the quality function deployment to translate user requirements into system specifications, and later on, into simulation specifications. It also uses the analytic hierarchy process to decrease the subjectivity of the decisions made during the systems analysis process",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022926,no,undetermined,0
Kinematic scheme design based on design catalogue,"First of all the essential procedure of design catalogue is expatriated on. And the methods of function coding and mechanism coding are defined according to the features of function and mechanism. On this base, the function table and mechanism table including evaluation criteria are constructed. Then because different users have different interests in the importance of various evaluation criteria of function, a user's function table is set up to describe the user's function on one hand and the different authority of each evaluation criteria for different functions is acquired with AHP(analysis of hierarchy process) on the other hand. In addition the user's functions may be searched in mechanism solution table and the set of feasible solutions is obtained. At last the optimal solution is achieved with a fuzzy evaluation method. In terms of idea expressed above the software system kinematic based design catalogue is developed for kinematic design. Accordingly, a software system kinematic based on design catalogue is developed",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127054,no,undetermined,0
Knowledge discovery in high-dimensional data: case studies and a user survey for the rank-by-feature framework,"Knowledge discovery in high-dimensional data is a challenging enterprise, but new visual analytic tools appear to offer users remarkable powers if they are ready to learn new concepts and interfaces. Our three-year effort to develop versions of the hierarchical clustering explorer (HCE) began with building an interactive tool for exploring clustering results. It expanded, based on user needs, to include other potent analytic and visualization tools for multivariate data, especially the rank-by-feature framework. Our own successes using HCE provided some testimonial evidence of its utility, but we felt it necessary to get beyond our subjective impressions. This paper presents an evaluation of the hierarchical clustering explorer (HCE) using three case studies and an e-mail user survey (n=57) to focus on skill acquisition with the novel concepts and interface for the rank-by-feature framework. Knowledgeable and motivated users in diverse fields provided multiple perspectives that refined our understanding of strengths and weaknesses. A user survey confirmed the benefits of HCE, but gave less guidance about improvements. Both evaluations suggested improved training methods",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608018,no,undetermined,0
Low-cost static performance prediction of parallel stochastic task compositions,"Current analytic solutions to the execution time distribution of a parallel composition of tasks having stochastic execution times are computationally complex, except for a limited number of distributions. In this paper, we present an analytical solution based on approximating execution time distributions in terms of the first four statistical moments. This low-cost approach allows the parallel execution time distribution to be approximated at ultra-low solution complexity for a wide range of execution time distributions. The accuracy of our method is experimentally evaluated for synthetic distributions as well as for task execution time distributions found in real parallel programs and kernels (NAS-EP, SSSP, APSP, Splash2-Barnes, PSRS, and WATOR). Our experiments show that the prediction error of the mean value of the parallel execution time for N-ary parallel composition is in the order of percents, provided the task execution time distributions are sufficiently independent and unimodal.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1549817,no,undetermined,0
Low-power buffer management using hybrid control,"The power management problem is studied for a pipeline of streaming data consisting of multiple stages with buffer memories inserted between adjacent stages. The objective is to find the optimal strategy for dynamically changing the power states of the stages so that the power consumption of the overall system is minimized. By modeling the pipeline as a hybrid system, we derive various necessary conditions for the optimal solutions. In particular, an operation called folding is introduced that can improve the performance of an existing strategy. Analytic conditions are derived for determining whether folding can save power and the optimal number of folds",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656626,no,undetermined,0
Modelling and Improving Group Communication in Server Operating Systems,"virtual environment (DVE) have become increasingly popular. Many DVE implementations use a client-server architecture that requires the server to send the same data to all members of a collaborating or interacting group. This type of group communication operation is often implemented by sending data from the server to each recipient in a unicast fashion. The problem with this approach is that the cost of communication at the server does not scale very well with the number of participants because the application requires significant interaction with the operating system, network stack and drivers for each individual send. In this paper, we first propose a general analytic framework for predicting how group communication performance impacts DVE server capacity. We then conduct an experimental evaluation to determine the extent to which using a kernel-based group communication mechanism reduces the cost of group send operations. Lastly, we use the measurements obtained from these experiments to demonstrate how to apply the analytic framework by determining the extent to which the kernel-based group communication mechanism permits example applications to scale to more users.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1698552,no,undetermined,0
A New System for Frequency Monitoring and Fault Analysis,"The new system studied in this paper is made of late-model single chip microcomputer and personal computer. It not only can monitor networks' frequency in real-time but also can calculate frequency variable then distinguish and record frequency fault course automatically. The whole recording time can come to 30 minutes. It is also equipped with data analytic software under WINDOWS circumstance. The new system can be used in power plants, substations and all levels dispatching station. It will play an important role in monitoring and recording networks' frequency.The recorded messages will be used in analyzing system frequency characteristic and under- frequency load shedding devices' performance. It has been used in power system and the effect is very good.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4116176,no,undetermined,0
NetLens: Iterative Exploration of Content-Actor Network Data,"Networks have remained a challenge for information retrieval and visualization because of the rich set of tasks that users want to accomplish. This paper offers an abstract content-actor network data model, a classification of tasks, and a tool to support them. The NetLens interface was designed around the abstract content-actor network data model to allow users to pose a series of elementary queries and iteratively refine visual overviews and sorted lists. This enables the support of complex queries that are traditionally hard to specify. NetLens is general and scalable in that it applies to any dataset that can be represented with our abstract data model. This paper describes NetLens applying a subset of the ACM Digital Library consisting of about 4,000 papers from the CM I conference written by about 6,000 authors. In addition, we are now working on a collection of half a million emails, and a dataset of legal cases",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035752,no,undetermined,0
Numerical Studies of Curved-walled Micro Nozzle/Diffuser,"In this study, commercially available software CFD was adopted for analyzing the performance of straight-walled and curved-walled micro nozzle/diffuser. Such nozzle/diffuser was used in valve-less micro-pumps. This model tested different types of nozzle/diffuser and the results showed that the pressure loss coefficient for nozzle/diffuser decreases with the increase of Reynolds number. At the same Reynolds number, the pressure loss coefficient for nozzle is higher than that of the diffuser. At a given volumetric flow rate, the pressure loss coefficient and ratio of pressure loss coefficient for curved-walled nozzle/diffuser are slightly higher than that of the straight-walled nozzle/diffuser. In this study, the numerical data was found good agreement with previous analytic solution and experimental results.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4135081,no,undetermined,0
On discretizing linear passive controllers,"In this work a new methodology which allows to discretize linear continuous-time passivity based controller is presented. This methodology is based on choosing a proper output, which preserves the passivity structure, while keeping the continuous-time energy function. Analytic formulation and a numerical example are provided in the paper",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1692715,no,undetermined,0
Partner Selection and Evaluation in Agile Virtual Enterprise Based upon TFN-AHP Model,"To solve some problems caused by the analytic hierarchy process (AHP) or other fuzzy-AHP approaches to partner selection and evaluation in the formation of agile virtual enterprises, a model based on the AHP and basic theory of the trigonometrical fuzzy number (TFN) was given and corresponding evaluation process was described. Compared with the AHP, the proposed model combines subjective analysis with quantitative analysis more reasonably, synthesizes group opinions more adequately. The model derives priorities from TFN-based judgment matrices effectively, regardless of their inconsistency. In addition, the model can be programmed easily due to its simple, normative arithmetic formulas. The results of a case study on selecting suppliers in an agile virtual enterprise indicate that, by applying the model-based software system, not only fair and reasonable conclusions, but also the consistent degree of opinions provided by different experts can be acquired clearly",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019045,no,undetermined,0
Performance analysis of the FastICA algorithm and CrameŒÇr-rao bounds for linear independent component analysis,"The FastICA or fixed-point algorithm is one of the most successful algorithms for linear independent component analysis (ICA) in terms of accuracy and computational complexity. Two versions of the algorithm are available in literature and software: a one-unit (deflation) algorithm and a symmetric algorithm. The main result of this paper are analytic closed-form expressions that characterize the separating ability of both versions of the algorithm in a local sense, assuming a ""good"" initialization of the algorithms and long data records. Based on the analysis, it is possible to combine the advantages of the symmetric and one-unit version algorithms and predict their performance. To validate the analysis, a simple check of saddle points of the cost function is proposed that allows to find a global minimum of the cost function in almost 100% simulation runs. Second, the CrameŒÇr-Rao lower bound for linear ICA is derived as an algorithm independent limit of the achievable separation quality. The FastICA algorithm is shown to approach this limit in certain scenarios. Extensive computer simulations supporting the theoretical findings are included.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608537,no,undetermined,0
Performance evaluation and modeling in Grid Resource Management,"Grid resource management is regarded as an important component of a grid computing system. The resources in the grid are geographically distributed, heterogeneous in nature, owned by different individuals/organizations each having their own resource management policies and different access-and-cost models. Based on abstract owner model, this article proposes a new resource storage model called resource pool which couples geographically distributed resource together. Meanwhile, this paper uses queuing and global optimization theory as tools to emphasize on quantitative analysis for resource pool capacity. Consequently, some new analytic formula for practical computation is obtained. Furthermore, an algorithm for analytic formulas is introduced into global optimization to evaluate the performance",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031478,no,undetermined,0
Quantitative system design,"Summary form only given. This talk provides a 20-year perspective on the use of analytic models to design of a wide range of commercially important architectures and systems with complex behavior. These systems include resources with highly bursty and/or correlated packet arrivals, communication protocols with complex routing and blocking of messages, resources that are configured for a very high probability (e.g., 0.9999) of providing immediate service to each arriving client, and complex large-scale grid/Internet applications. The examples illustrate some guiding principles for model development, and show that the models can be relatively easy to develop. More importantly, the models can be highly accurate - often more accurate than simulation, and sometimes more accurate than the system implementation! The examples also illustrate that the models can provide unique insight into system design as well as significant new system functionality. In other words, analytic models are a key tool for competitive systems engineering. Time permitting, the talk includes some important observations about workload models, and some ways to avoid key pitfalls in simulation.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1620797,no,undetermined,0
Repetitive Controller Design for Track Following of Hard Disk Drive Servo System,"Recently repetitive controller has been applied in servo systems of hard disk drives (HDD) to remove the repeatable errors caused by rotating mechanism of spindle motors. However, most results of published articles are too complex to be implemented in industrial applications. In this paper, we present an improved design strategy for repetitive control of HDD by minimizing the power spectral density (PSD) of PES. An analytic solution is given with tolerable computation requirements and can be evaluated by software, even assembly languages. Simulations of track following of HDD servo are carried out based upon the theoretical derivations. The result shows that compared to single PID control, the proposed controller improves the performance of steady state response",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4150381,no,undetermined,0
Requirements elicitation through model-driven evaluation of software components,"The use of software components is perceived to significantly shorten development time and cost, while improving quality, in developing a large, complex software system. A key premise to this perception seems to be the ability to effectively search, match, rank, and select software components, during the requirements engineering process. In this paper, we present a technique for eliciting requirements by using model-driven evaluation of software components, where the evaluation revolves around ""models"" of software components and ""models"" of the component-based application (CBA). As part of our ongoing project, component-aware requirements engineering (CARE), this model-driven evaluation technique is intended to match the models of the stakeholders' needs for the component-based application against the models of the capabilities of the set of components that are currently available. More specifically, this technique allows for an integrated use of several searching/matching techniques, such as keyword-based search, case-based reasoning (CBR) and analytic hierarchy process (AHP), in evaluating models of components' requirements against models of the requirements of the stakeholders of the CBA being elicited incrementally. The model-driven evaluation technique is illustrated using a home appliance control system (HACS) example.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1595762,no,undetermined,0
Efficient Data Analysis with Modern Analytical System Developed for Use in Photovoltaic (PV) Monitoring System,"Outdoor data acquisition systems (DAS) designed for long term monitoring of photovoltaic (PV) modules provide wide range of measurement conditions. Measured data contain modules' electrical parameters extracted from I-V curves and meteorological conditions occurring during measurements. Every year measurement volume of data systematically increases making analysis more complex and difficult. To cope with such large amount of data efficient database together with analytical tools is essential. This paper presents a set of analytic tools implemented with The SAS Institute Software (USA) as online analytical processing (OLAP) website. Data are available for analysis via SolarLab's website for authorized users. Software provides multidimensional data filtering, data visualization and different types of regression. Additionally, summarized data, like periodical energy gain or insolation, can be viewed and drilled down to single measurements. Efficient data integration and data warehouse tools enable efficient analysis of data measured in different laboratories. Internet web browser interface simplifies multilaboratory cooperation in data analysis and does not require any additional software to be installed",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4149602,no,undetermined,0
Efficient continuous collision detection for bounding boxes under rational motion,"This paper presents a simple yet precise and efficient algorithm for collision prediction of two oriented bounding boxes under univariate (piecewise) rational motion. We present an analytic solution to the problem of finding the time of collision and the feature involved, or declaring that no collision should occur. Our solution can be applied to boxes of any size, under arbitrary rational rigid motion. The algorithm is based on the efficient examination of the Minkowski sum (MS) of the two boxes, using a spherical Gauss map dual representation, and a precise extraction of the collision time, if any, as a solution to a set of rational equations that are automatically derived",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642160,no,undetermined,0
Dynamic Test Composition In Hierarchical Software Testing,"Testing of large software systems is essential to assure the desired system reliability, but it involves large amounts of computer resources. It turns out that working in mid-range test granularity obtains very regular hierarchical test graphs, implying significant savings of testing-time, by dynamic generation of the strictly necessary tests. We formulate general analytic expressions for time savings and obtain the important result that savings increases with system size. The approach was demonstrated by actually implementing a testing system for large bundles of content files.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4115241,no,undetermined,0
Designing a Genetic Algorithm for Function Approximation for Embedded and ASIC Applications,"In embedded systems and application specific integrated circuits (ASICs) that typically do not have a floating-point processor, measured data or function-sampled data is commonly described by an analytic function derived using standard numerical methods. The resultant errors are not caused by rounding but by translating a real solution to a restricted fixed-point environment. We have previously described a genetic algorithm that discovers a superior piece-wise polynomial approximation with coefficients restricted to the integer target space. In this paper we discuss details of the genetic algorithm implementation.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4267414,no,undetermined,0
A Process Diagnosis Technique for the Problems of Business Process,"One process diagnosis technique is proposed to solve the problems in business process. Firstly, on the base of objective view and analytic hierarchy process (AHP), a process diagnosis model is founded and the limits of the causes are given by using process business objective view. Then, objective view is converted into a hierarchical structure of AHP to quantitatively calculate the effects of these causes on the problems. Finally, a software package to support the proposed technique was developed and, as a case study, applied to a process diagnosis in a manufacturing enterprise. The operations of the system in the enterprise have shown that the performance is fine and have got better economic profit and social benefit",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1714153,no,undetermined,0
A Quality Evaluation Technique of RFID Middleware in Ubiquitous Computing,"With ubiquitous computing system, users can access information through a computer network at any time and in any place. The basic infrastructure of ubiquitous computing system is wireless network environment, and a RFID (Radio Frequency Identification) system is composed tags, readers, middleware, application services, etc. and uses networks. RFID middleware is system software that collects a large volume of raw data generated in RFID environment, filters the data, summarizes them into meaningful information and delivers the information to application services. RFID middleware links hardware to conventional middleware. Previous researches on RFID middleware have covered middleware from SUN, SAP, IBM, Microsoft, Oracle, etc. These products attach importance to different quality characteristics, and there have been few researches on the quality properties of RFID middleware. The present study examined functionality, reliability, usability, efficiency and portability among the quality characteristics of software in international standard ISO/IEC 9126 as well as the quality elements of standard RFID middleware of EPC Global, and based on them we extracted and analyzed items for evaluating the quality of RFID middleware in ubiquitous computing systems. Using the AHP (Analytic hierarchy process) that enables rational decision making by simplifying complicated problems, we evaluated the subjective characteristics of stakeholders in an objective way and proposed a selection method that evaluates quality using quality evaluation criteria. The proposed evaluation selection method is useful for developers who are going to develop RFID middleware in areas such as distribution and logistics to select RFID middleware suitable for their environment.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021295,no,undetermined,0
A Visualization System for Space-Time and Multivariate Patterns (VIS-STAMP),"The research reported here integrates computational, visual and cartographic methods to develop a geovisual analytic approach for exploring and understanding spatio-temporal and multivariate patterns. The developed methodology and tools can help analysts investigate complex patterns across multivariate, spatial and temporal dimensions via clustering, sorting and visualization. Specifically, the approach involves a self-organizing map, a parallel coordinate plot, several forms of reorderable matrices (including several ordering methods), a geographic small multiple display and a 2-dimensional cartographic color design method. The coupling among these methods leverages their independent strengths and facilitates a visual exploration of patterns that are difficult to discover otherwise. The visualization system we developed supports overview of complex patterns and through a variety of interactions, enables users to focus on specific patterns and examine detailed views. We demonstrate the system with an application to the IEEE InfoVis 2005 contest data set, which contains time-varying, geographically referenced and multivariate data for technology companies in the US",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703367,no,undetermined,0
About the Time Complexity of EAs Based on Finite Search Space,"Some results about the computation time of evolutionary algorithms are obtained in this paper. First, some exact analytic expressions of the mean first hitting times of evolutionary algorithms infinite search spaces are acquired theoretically by using the properties of Markov chain associated with evolutionary algorithms considered here. Then, by introducing drift analysis and applying Dynkin's formula, the general upper and lower bounds of the mean first hitting times of evolutionary algorithms are estimated",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4072097,no,undetermined,0
Activity based high level modeling of dynamic switching currents in digital IC modules,"Electromagnetic compatibility (EMC) becomes an increasingly important subject within the IC design process, because more and more market segments demand for low electromagnetic emission (EME) of integrated circuits. Therefore automatic emission model generation tools need to become part of the design flow. In this paper we present an automatic generation procedure of equivalent current sources (ECS) from chip netlists, based on an analytic approach. The ECSs describe the dynamic switching currents of digital function blocks of complex ICs. Worst case or typical current profiles are not generated by pattern simulation, but by pre-characterized logic cells and a set of configuration parameters like switching activity",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1629695,no,undetermined,0
An Analytic Solution of a Linear Camera Self-Calibration,"We propose an analytic solution for the self-calibration of five-parameter linear camera from three pairs of views of a scene, of four-parameter linear camera from two pairs of views and of two-parameter linear camera from one pairs of views based on Kruppa equations, respectively. The approach does not need any initial value, and can calculate accurate solution. The analysis was performed first by writing a suitable set of equations under introducing unknown scale factors from Kruppa's equations. Then, by an elimination scheme and symbolic computation, only one unknown scale factor was retained, so the equation set was reduced to a 30th order univariate polynomial equation for five-parameter or four-parameter camera, and a 3rd order univariate polynomial equation for two-parameter camera. After scale factors were attained, the camera intrinsic parameters were computed by a linear method. A numerical example was reported that confirms the theoretical results by ""Mathematica"" software",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713938,no,undetermined,0
An Approach for Intersubject Analysis of 3D Brain Images Based on Conformal Geometry,"Recent advances in imaging technologies, such as magnetic resonance imaging (MRI), positron emission tomography (PET) and diffusion tensor imaging (DTI) have accelerated brain research in many aspects. In order to better understand the synergy of the many processes involved in normal brain function, integrated modeling and analysis of MRI, PET, and DTI across subjects is highly desirable. The current state-of-art computational tools fall short in offering an analytic approach for intersubject brain registration and analysis. In this paper we present an approach which is based on landmark constrained conformal parameterization of a brain surface from high-resolution structural MRI data to a canonical spherical domain. This model allows natural integration of information from co-registered PET as well as DTI data and lays a foundation for the quantitative analysis of the relationship among diverse datasets across subjects. Consequently, the approach can be extended to provide a software environment able to facilitate detection of abnormal functional brain patterns in patients with neurological disorder.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4106749,no,undetermined,0
An Audio Information Hiding Algorithm Based on ICA,"A novel audio information hiding technology combined with ICA (independent component analysis) and QIM (quantized index modulation) is proposed. ICA method is applied to audio signal processing to obtain the statistical independent sources as hiding channels. As ICA processing is sensitive to the only dividing matrix, security is guaranteed. A piece of meaningful speech signal is hidden in the carrier audio. Through analytic comparisons and experimental results, this algorithm is found to have a larger hiding capacity and a lower distortion to carrier audio, and also robust against a variety of common signal processing manipulations",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713161,no,undetermined,0
An infrastructure for automating information sharing in analytic collaboration,"In many parts of the intelligence community data gathers at rates that preclude the ability of analysts and policy makers to ingest, let alone comprehend, the complex information confronting them without working together in teams. These teams often form ad-hoc, mission-oriented communities of practice, and sometimes larger multi-mission communities of interest. These communities can be distributed across the globe. Within these communities analysts try to share their insights but often end up simply passing around partially processed information. This paper reports on developments toward a next generation collaboration supporting small groups of analysts (or policy makers) engaged in joint analysis and problem solving. The work reported in this paper ""automates what ought to be automated,"" differentiating between what machines do well and what humans do well. It describes an infrastructure that automates much of the information exchange between analysts while at the same time creating an environment that facilitates insight sharing among analysts in a community of interest",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656048,no,undetermined,0
Analyses of elliptical coplanar coupled waveguides and coplanar coupled waveguides with finite ground width,"In this paper, the quasi-TEM characteristic parameters of elliptical coplanar coupled waveguides and coupled coplanar waveguide with a finite ground width configuration are presented and analyzed. Computer-aided design (CAD)-oriented fast, simple, and accurate analytic formulas are derived by using conformal mapping techniques, which provide satisfactory accuracy at microwave frequencies and lead to closed-form analytical solutions suitable for CAD software packages. The results for the odd- and even-mode characteristic impedance, effective dielectric constant, and coupling coefficient have been computed by these formulas. Good agreement between the present results and published results is observed. For the planar case, simulations have also been undertaken with Sonnet electromagnetic-circuit solver software. The computed results agree well with those of the simulation ones.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1618555,no,undetermined,0
Applying Multiobjective Compromise Approach to Exploring the Strategic Plan of Fuel Cell Technology,"The fuel cell is one of the most important energy products in the 21st century, and most industrial-advanced countries in the world are placing high expectation on its development. The applications of fuel cells in Taiwan has spread broadly in many products and it takes high global market share, such as notebook computers, PDAs and digital cameras, along with items like automobiles, motorcycles and power generation equipment. In this study we introduce a multi-objective compromise optimization to solve the optimal strategic plans of fuel cell industry, which is emerging industry in Taiwan. In the first step, we introduce popular analytic hierarchy process with fuzzy set theory to establish the hierarchy system to determine the relative importance of evaluated criteria. Secondly, fuzzy geometric mean method was utilized to aggregate the performance score by individual judgment of evaluator, which scores express the measurement of strategic plan proposed by participated experts. Thirdly, the synthetic value of each strategic plan was integrated by criteria weights with performance score, and positive ideal solution (PIS) and negative ideal solution (NIS) were defined by Minkowski's distance metric function. Fourthly, the optimal compromise solution will be derived which should have the shortest distance from the positive ideal solution as well as the farthest distance from the negative ideal solution. Along with the results of this research, we successfully demonstrate that the multi-objective compromise optimization is a good alternative for evaluation of multiple criteria decision making problems",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077435,no,undetermined,0
Autonomic Replication of Management Data Evaluation of a Market-based Approach,"This paper describes work in progress whereby a dynamic data replication scheme, under market-based control is applied to a proposed autonomic distributed data layer for managing configuration management data. The scope of the proposed autonomic system is described and also some experimental work are presented. Analytic approximations of the performance achieved for management requests under various static data replication schemes are compared with event-based simulations of the same system under dynamic market-based replication control. The purpose of this comparison is to evaluate the performance and suitability of a market-based control approach for such autonomic replication systems",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1687631,no,undetermined,0
Avian Flu Case Study with nSpace and GeoTime,"GeoTime and nSpace are new analysis tools that provide innovative visual analytic capabilities. This paper uses an epidemiology analysis scenario to illustrate and discuss these new investigative methods and techniques. In addition, this case study is an exploration and demonstration of the analytical synergy achieved by combining GeoTime's geo-temporal analysis capabilities, with the rapid information triage, scanning and sense-making provided by nSpace. A fictional analyst works through the scenario from the initial brainstorming through to a final collaboration and report. With the efficient knowledge acquisition and insights into large amounts of documents, there is more time for the analyst to reason about the problem and imagine ways to mitigate threats. The use of both nSpace and GeoTime initiated a synergistic exchange of ideas, where hypotheses generated in either software tool could be cross-referenced, refuted, and supported by the other tool",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035744,no,undetermined,0
Case Retrieving Based on Grey Incidence Degree in CBR,"Based on the characteristic of grey system theory, which is an effective method to solve uncertain problems, we used grey incidence analytic method into case retrieving of CBR. We took grey incidence degree as a tool for measuring the similarity degree of case. Grey predominance analysis was applied for choosing the most suitable case. If there was no strict suitable case, AHP (analytic hierarchy process) method was used to calculate weight of different indices, then we got synthetical grey incidence degree, ascertain synthetical most similar case. A CBR framework based on grey incidence degree was built based on grey incidence analytic method. Finally, an example was presented. The results show that the method is simple and practical; the model provides an effective method to case retrieving",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713206,no,undetermined,0
Challenges in Visual Data Analysis,"In today's applications data is produced at unprecedented rates. While the capacity to collect and store new data grows rapidly, the ability to analyze these data volumes increases at much lower pace. This gap leads to new challenges in the analysis process, since analysts, decision makers, engineers, or emergency response teams depend on information ""concealed"" in the data. The emerging field of visual analytics focuses on handling massive, heterogenous, and dynamic volumes of information through integration of human judgement by means of visual representations and interaction techniques in the analysis process. Furthermore, it is the combination of related research areas including visualization, data mining, and statistics that turns visual analytics into a promising field of research. This paper aims at providing an overview of visual analytics, its scope and concepts, and details the most important technical research challenges in the field",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648235,no,undetermined,0
Checkpoint Placement Algorithms for Mobile Agent System,"Checkpointing is a fault tolerance technique widely used in various types of computer systems. In checkpointing, an important issue is how to achieve a good trade-off between the recovery cost and the system performance. Excessive checkpointing would result in the performance degradation due to the high costly I/O operations during checkpointing. Equidistant and equicost are two well-known checkpointing strategies for addressing this issue. However, there is no study on these strategies catering for a mobile agent (MA) system, which has different characteristics with conventional systems. In this paper, based on an analysis of the behaviours of an MA system, we find that it can be modelled as a homogeneous discrete-parameter Markov chain, which is different from the models used in conventional systems. Therefore, the analytic methods and corresponding results for conventional systems cannot be adopted directly for an MA system. Based on our proposed model, we study the equidistant and equicost checkpointing strategies and propose checkpoint placement algorithms for MA systems. Through simulations we evaluate the performance of our proposed algorithms and the result shows that the equicost strategy based algorithm is most suitable for an MA system",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031479,no,undetermined,0
Connectionless Quality-of-Service Routing Framework,"Next generation networks call for novel network architectures in order to support QoS-oriented applications. However, existing QoS architectures mostly suffer from the unscalability problem. In this paper we propose a novel connectionless QoS routing framework for high-speed core networks. Unlike traditional QoS architectures, neither resource reservation nor per-flow management exists in the proposed routing framework, so it has good scalability. In this framework, QoS constraints are carried by every packet and routing decisions are made hop by hop at the packet level. We also establish an analytic QoS model and a simulation model to acquire performance evaluation for the connectionless QoS routing framework. Both the analysis and simulations show that this framework achieves satisfying performance for any underlying type of QoS-oriented services",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4281786,no,undetermined,0
Container Terminal Operation Emulation system and application in Optimizing of Terminal Plane Layout Scheme,"Aimed at the complicated system engineering of plane layout scheme design of container terminal, and based on the eM-plant distribute software have set up container terminal operating emulation modules, such as berth module, gate entrance module, storage module and statistic module. Have set up the comprehensive evaluation system of plane layout, and the right of every index was confirmed by analytic hierarchy process method and expert consult method synthetically. By analyses the level of every index value that got from the emulation results, can get the comprehensive valuation result. Have carried on intelligence optimized research by using VR-UC GA algorithm on the overall arrangement scheme, and applied to a certain container terminal program project, have gotten relatively satisfactory result, offered strong support for decision of container terminal's plane layout scheme.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4060397,no,undetermined,0
Controlling Quality of Service in Multi-Tier Web Applications,"The need for service differentiation in Internet services has motivated interest in controlling multi-tier web applications. This paper describes a tier-to-tier (T2T) management architecture that supports decentralized actuator management in multi-tier systems, and a testbed implementation of this architecture using commercial software products. Based on testbed experiments and analytic models, we gain insight into the value of coordinated exploitation of actuators on multiple tiers, especially considerations for control efficiency and control granularity. For control efficiency, we show that more effective utilization of tiers can be achieved by using actuators on the bottleneck tier rather than only using actuators on the entry tier. For granularity of control (the ability to achieve a wide range of service level objectives) we show that a fine granularity of control can be achieved through a coordinated, cross-tier exploitation of coarse grained actuators (e.g., multiprogramming level), an approach that can greatly reduce controllerinduced variability.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648812,no,undetermined,0
BRIX: meeting the requirements for online second language learning,"This paper describes the design and evaluation of BRIX, an environment for online learning of second languages. A needs analysis identified specific requirements of online language learning. Commercial course management systems were determined to be inadequate with respect to these requirements. BRIX was developed to address the need for a generic language learning environment that fulfils language educators' requirements focusing on reading, writing, and listening activities and can easily be customized for different language courses. The design of BRIX is based on pedagogic approaches and theories of teaching and learning second languages and on the results of analytic and empirical evaluation of test versions of the software. In this paper, we describe the needs analysis and the design of BRIX, and present an evaluation that compares the use and usability of a Chinese course in BRIX to a previous handcrafted version of the same course.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265047,no,undetermined,0
An evaluation of various analytic reconstruction algorithms and implementations for 2D and 3D PET,"Effects of the application, and implementation, of various reconstruction algorithms to 2D and 3D PET data were studied. Results were compared based on the performance of each method when subjected to the NEMA performance tests for resolution, scatter correction accuracy, and uniformity. An independent measurement of the coefficient of variation for a uniform cylinder was also made. Resulting data was reconstructed using each of the ECAT 7.1, STIR, and ECAT 7.2 software packages. Two implementations of the STIR software were investigated, STIR-INT1 which backprojects the original sinograms, and STIR-INT2 which first interpolates the sinograms to have equal bin and voxel sizes. Behaviour of the spatial resolution and CV in terms of varying pixel size was attributed to both the method of backprojector implementation and the reconstruction algorithm (FORE vs. 3DRP). An understanding of the method for reconstruction is important in understanding image quality, and selecting a pixel size for clinical use",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1466782,no,undetermined,0
Self-Motion Analysis on A Redundant Robot with A Parallel/Series Hybrid Configuration,"A redundant robot configured with a parallel base and a series manipulator is proposed in this paper. The position and orientation of end manipulator are considered as the result of the joints' ordinal motion according to the equivalence principle, so the close analytic solution for self-motion of this redundant robot is obtained via geometrical method. The Jacobian matrices of the parallel base and the whole robot system are solved respectively. Based on this, the solution for self-motion in two classes of singular configuration is also obtained. Finally a self-motion simulation of the redundant robot is performed via software MATLAB and ADAMS to validate the algorithm",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4150083,no,undetermined,0
On the syllogistic structure of object-oriented programming,"Recent works by J.F. Sowa (2000) and D. Rayside and G.T. Campbell (2000) demonstrate that there is a strong connection between object-oriented programming and the logical formalism of the syllogism, first set down by Aristotle in the Prior Analytics (1928). In this paper, we develop an understanding of polymorphic method invocations in terms of the syllogism, and apply this understanding to the design of a novel editor for object-oriented programs. This editor is able to display a polymorphic call graph, which is a substantially more difficult problem than displaying a non-polymorphic call graph. We also explore the design space of program analyses related to the syllogism, and find that this space includes Unique Name, Class Hierarchy Analysis, Class Hierarchy Slicing, Class Hierarchy Specialization, and Rapid Type Analysis.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919086,no,undetermined,0
Application of visual specifications for verification of distributed controllers,"In a search for an appropriate visual specification language to be applied in control engineering the timing diagram specification language is suggested. It is applied to the verification of distributed controllers following the standard IEC 61499. Specification of inputs and outputs of the controller are given in the graphical form of signal diagrams. The inputs are then converted into finite-state models, while the diagram of outputs is used to build equivalent analytic expressions in extended CTL. These two parts are used in formal verification of the control system",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969925,no,undetermined,0
Automating the analysis of option pricing algorithms through intelligent knowledge acquisition approaches,"The traditional approach for estimating the performance of numerical methods is to combine an operation's count with an asymptotic error analysis. This analytic approach gives a general feel of the comparative efficiency of methods, but it rarely leads to very precise results. It is now recognized that accurate performance evaluation can be made only with actual measurements on working software. Given that such an approach requires an enormous amount of performance data related to actual measurements, the development of novel approaches and systems that intelligently and efficiently analyze these data is of great importance to scientists and engineers. The paper presents intelligent knowledge acquisition approaches and an integrated prototype system, which enables the automatic and systematic analysis of performance data. The system analyzes the performance data which is usually stored in a database with statistical, and inductive learning techniques and generates knowledge which can be incorporated in a knowledge base incrementally. We demonstrate the use of the system in the context of a case study, covering the analysis of numerical algorithms for the pricing of American vanilla options in a Black and Scholes modeling framework. We also present a qualitative and quantitative comparison of two techniques used for the automated knowledge acquisition phase",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=983414,no,undetermined,0
Building dependable software for critical applications: multi-version software versus one good version,"An increasing range of industries have a growing dependence on software based systems, many of which are safety-critical, real-time applications that require extremely high dependability. Multi-version programming has been proposed as a method for increasing the overall dependability of such systems. We describe an experiment to establish whether or not the multi-version method can offer increased dependability over the traditional single-version development approach when given the same level of resources. Three programs were developed independently to control a real-time, safety-critical system, and were put together to form a decentralized multi-version system. Three functionally equivalent single-version systems. were also implemented, each using the same amount of development resources as the combined resources of the multi-version system. The analytic results from this experiment show that 1) a single-version system is much more dependable than any individual version of the multi-version system, and 2) despite the poor quality of individual versions, the multi-version method still results in a safer system than the single-version solution. It is evident that regarding the single-version method as a ""seem-to-be"" safer design decision for critical applications is not generally justifiable. We conclude by describing plans for a follow up study based on our initial findings",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=945120,no,undetermined,0
Comparison using AHP Web-based learning with classroom learning,"We have developed a Web-base learning system for teaching ""software design methodology"". We also have developed a method using analytic hierarchy process diagrams to evaluate the effectiveness of Web-based learning. Using the method, we evaluated the effectiveness of Web-based learning compared with classroom learning. Based on the results, we are improving our Web-based learning system",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998212,no,undetermined,0
Computation of the electric field around a wet polluted insulator by analytic and numerical techniques,"A benchmark problem of a cylindrical insulator with a wet and dry band is formulated. An analytic solution was obtained and numerically evaluated, and compared with the solution obtained from: (1) a finite-differencing technique; and (2) a commercially available finite-element software. The agreement between the solutions obtained from the three methods was found to be within 10 to 20%",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=963606,no,undetermined,0
Denormalization effects on performance of RDBMS,"Presents a practical view of denormalization, and provides fundamental guidelines for incorporating denormalization. We have suggested using denormalization as an intermediate step between logical and physical modeling, to be used as an analytic procedure for the design of the applications requirements criteria. Relational algebra and query trees are used to examine the effect on the performance of relational database management systems (RDBMS). The guidelines and methodology presented are sufficiently general, and they can be applicable to most databases. It is concluded that denormalization can enhance query performance when it is deployed with a complete understanding of application requirements.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=926306,no,undetermined,0
Extended specifications and test data sets for data level comparisons of direct volume rendering algorithms,"Direct volume rendering (DVR) algorithms do not generate intermediate geometry to create a visualization, yet they produce countless variations in the resulting images. Therefore, comparative studies are essential for objective interpretation. Even though image and data level comparison metrics are available, it is still difficult to compare results because of the numerous rendering parameters and algorithm specifications involved. Most of the previous comparison methods use information from the final rendered images only. We overcome limitations of image level comparisons with our data level approach using intermediate rendering information. We provide a list of rendering parameters and algorithm specifications to guide comparison studies. We extend Williams and Uselton's rendering parameter list with algorithm specification items and provide guidance on how to compare algorithms. Real data are often too complex to study algorithm variations with confidence. Most of the analytic test data sets reported are often useful only for a limited feature of DVR algorithms. We provide simple and easily reproducible test data sets, a checkerboard and a ramp, that can make clear differences in a wide range of algorithm variations. With data level metrics, our test data sets make it possible to perform detailed comparison studies. A number of examples illustrate how to use these tools",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=965345,no,undetermined,0
Hypercomplex signals-a novel extension of the analytic signal to the multidimensional case,"The construction of Gabor's (1946) complex signal-which is also known as the analytic signal-provides direct access to a real one-dimensional (1-D) signal's local amplitude and phase. The complex signal is built from a real signal by adding its Hilbert transform-which is a phase-shifted version of the signal-as an imaginary part to the signal. Since its introduction, the complex signal has become an important tool in signal processing, with applications, for example, in narrowband communication. Different approaches to an n-D analytic or complex signal have been proposed in the past. We review these approaches and propose the hypercomplex signal as a novel extension of the complex signal to n-D. This extension leads to a new definition of local phase, which reveals information on the intrinsic dimensionality of the signal. The different approaches are unified by expressing all of them as combinations of the signal and its partial and total Hilbert transforms. Examples that clarify how the approaches differ in their definitions of local phase and amplitude are shown. An example is provided for the two-dimensional (2-D) hypercomplex signal, which shows how the novel phase concept can be used in texture segmentation",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960432,no,undetermined,0
"Modeling, design, virtual and physical prototyping, testing, and verification of a multifunctional processor queue for a single-chip multiprocessor architecture","Critical to run-time processor resource allocation, reconfiguration, and control of a reconfigurable heterogeneous single-chip multiprocessor architecture is a defined multifunctional queue required by each processor of the architecture. The multifunctional queue implements six functions required for control, resource allocation, and reconfiguration within the architecture. In addition to normal queue functionality of First In First Out (FIFO) operation and empty/full indicator, the multifunctional queue implements the additional non-common functions of indicating when queue depth has reached a programmable threshold level, it indicates queue occupancy level at all times, it continually indicates queue input rate over a programmable time interval, it continually indicates queue input rate change over a programmable time interval and it can implement a pseudo-RAM function. An analytic functional model of the queue is first presented then an organization, architecture and design is developed followed by the development of appropriate analytic real-time performance metrics for the queue. Both virtual and Field Programmable Gate array (FPGA) based prototypes of the queue are then developed and used for functional, maximum frequency, and/or performance model testing resulting in verification of desired queue functionality and performance. A contribution of the queue is its functional versatility which would allow its use in computer architectures or processors other than the described target architecture",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=933850,no,undetermined,0
Optimization of quadratic performance indexes for nonlinear control systems,"The proposed approach aims at the development of a systematic method to optimally choose the controller tunable parameters in a nonlinear control system, where in addition to the traditional set of closed-loop performance specifications, optimality is also requested with respect to the physically meaningful quadratic performance index. In particular, the value of the performance index can be calculated exactly by solving the Zubov partial differential equation (PDE). It can be shown that the Zubov PDE admits a unique and locally analytic solution that is endowed with the properties of a Lyapunov function for the closed-loop system. Moreover, the analyticity property of the solution of Zubov PDE enables the development of a series solution method that can be easily implemented with the aid of a symbolic software package. It can be shown that the evaluation of the above Lyapunov function at the initial conditions leads to a direct calculation of the value of the performance index which now explicitly depends on the controller parameters. Therefore, the employment of static optimization techniques can provide the optimal values of the finite-set of controller parameters. Finally, it shown that an explicit estimate of the size of the closed-loop stability region can be provided by using results from the Zubov stability theory",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=980690,no,undetermined,0
Automated software robustness testing - static and adaptive test case design methods,"Testing is essential in the development of any software system. Testing is required to assess a system's functionality and quality of operation in its final environment. This is especially of importance for systems being assembled from many self-contained software components. In this article, we focus on automated testing of software component robustness, which is a component's ability to handle invalid input data or environmental conditions. We describe how large numbers of test cases can effectively and automatically be generated from small sets of test values. However, there is a great demand on ways to efficiently reduce this mass of test cases as actually executing them on a data processing machine would be too time consuming and expensive. We discuss static analytic methods for test case reduction and some of the disadvantages they bring. Finally a more intelligent and efficient approach is introduced, the Adaptive Test Procedure for Software Robustness Testing developed at ABB Corporate Research in Ladenburg. Along with these discussions the need for intelligent test approaches is illustrated by the Ballista methodology for automated robustness testing of software component interfaces. An object-oriented approach based on parameter data types rather than component functionality essentially eliminates the need for function-specific test scaffolding.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046134,no,undetermined,0
Performance and scalability analysis on client-server workflow architecture,"We excogitate a performance analytic model and describe its analysis results conceived to be helpful in the understanding of the spectrum of possibilities for large-scale workflow architecture. The analytic model is extended to represent several types of client-server workflow architectures. Especially, we focus on performance estimates of the conventional workflow management systems that are characterized by the client-server workflow architectures. The development of a workflow management system is typically a large and complex task. Decisions need to be made about the hardware and software platforms, the data structures, the algorithms, and the interconnection of various modules utilized by various users and administrators. These design decisions are further complicated by the requirements, such as scalability, flexibility, robustness, speed, and usability. We are particularly concerned about issues of scalability to see how well the client-server workflow architecture is dealing with the large amount of workcases. Finally, we graphically show the comparisons of performance evaluation results for several types of client-server workflow architectures on behalf of the single-server, and the multiple-server workflow systems on the distribution environment",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=934817,no,undetermined,0
Proceedings IEEE International Symposium on Network Computing and Applications. NCA 2001,The following topics are dealt with: scalability; system area networking; ad hoc networks; NetSolve; the quadrics network; deadline missing; reconfiguration protocols; free software distribution; firewalls; malicious agreements; application load balancing; multicast; replicated database recovery; an event-based communication framework; the analytic n-burst model; equivalent paths; IP router architectures; high-speed switch fabrics; distributed storage; mobile environment protocols; data logistics; Web cache sharing; IP storage; video transcoding; the Internet; computer clusters; fault-tolerant security; replica divergence; heterogeneous networks; Java cluster management; local scheduling; reconfigurable algorithms; IF environment; mobile Internet; TCP streams; service carrier-grade quality; database state machines; analytic performance; timeout prediction; storage area networks; a rapid I/O-based architecture; mobile agent monitoring; Ethernet; limited scope probing; jitter and delay; wireless MAN; static load balancing; and the PKI trust model,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962508,no,undetermined,0
Requirements engineering for complex collaborative systems,"A method for analysing requirements for complex sociotechnical systems is described. The method builds on the I* family of models by explicitly modelling communication between agents by discourse act types. System (i*) models and use cases are developed which describe the dependencies between human and computer agents in terms of a set of discourse acts that characterise the obligations on agents to respond and act. For human-computer communication, the discourse acts indicate functional requirements to support operators. For human agents, the acts specify their obligation to act and constraints on action. The method provides analytic techniques and heuristics to assess agent workloads in terms of the tasks and communication they have to perform. Scenarios are run against the system model by walking through the chain of operator tasks and communication links to produce time estimates and failure probabilities, where the demands of scenarios impose excessive loads on human operators. The method is illustrated with a case study of a naval command and control system",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948550,no,undetermined,0
Revisiting adaptively sampled distance fields,"Implicit surfaces are a powerful shape description for many applications in computer graphics. An implicit surface is defined by a function f:R<sup>3</sup>äÊÍR as the set of points satisfying f(p)=0. Implicit representation becomes more effective when f is a signed distance function, i.e., when |f| gives the distance to the closest point on the surface and f is negative inside the object and positive outside the object bounded by the surface. The distance function to an arbitrary surface does not have a simple analytic description, and we must resort to approximations. One simple solution is to use a volumetric representation, constructed by sampling f uniformly, but such models are very large and their resolution is limited by the sampling rate. Frisken et al. (2000) proposed adaptively sampled distance fields (ADFs) as a way to overcome these problems. The authors revisit the ADFs and make two contributions to the original framework. First, we analyse the ADF representation and discuss some possible improvements. Second, we show how to compute ADFs more efficiently",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=963083,no,undetermined,0
Selecting the best e-commerce product for a retail chain-the analytic hierarchy process model,Retail firms have been in the forefront of the business-to-consumer (B2C) initiatives as they run the whole gamut of transformation from branding and information-centered activities to online sales and innovation-driven marketing. The paradigm shift in business objective from mere selling of products and services to 'owning a share of life of guests' can also be observed from the new networks and alliances between retail firms and other businesses with complementing business goals. A US-based retail chain has been exploring avenues for enhancing its B2C e-commerce capabilities by embarking on a series of initiatives that inter alia includes a comprehensive study on related software products currently available in the market. This paper focuses on evaluation of the e-commerce products in terms of their technical features and services offered as well as exploring their suitability to meet the specific business priorities of the firm. The analytic hierarchy process (AHP) model has been employed for selecting the best product,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=951870,no,undetermined,0
Singular PDES and the assignment of zero dynamics in nonlinear systems,"The present research work aims at the development of a systematic method to arbitrarily assign the zero dynamics of a nonlinear system by constructing the requisite synthetic output maps. The minimum-phase synthetic output maps constructed can be made statically equivalent to the original output maps, and therefore, they could be directly used for nonminimum-phase compensation purposes. Specifically, the mathematical formulation of the problem is realized via a system of first-order nonlinear singular PDEs and a rather general set of necessary and sufficient conditions for solvability is derived. The solution to the above system of singular PDEs can be proven to be locally analytic and this enables the development of a series solution method that is easily programmable with the aid of a symbolic software package. The minimum-phase synthetic output maps that induce the prescribed zero dynamics for the original nonlinear system can be computed on the basis of the solution of the aforementioned system of singular PDEs. Moreover, statical equivalence to the original output map can be readily established by a simple algebraic construction.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7076514,no,undetermined,0
A flexible accelerator for Layer 7 networking applications,"In this paper, we present a flexible accelerator designed for networking applications. The accelerator can be utilized efficiently by a variety of Network Processor designs. Most Network Processors employ hardware accelerators for implementing key tasks. New applications require new tasks, such as pattern matching, to be performed on the packets in real-time. Using our proposed accelerator, we have implemented several such tasks and measured their performance. Specifically, the accelerator achieves 25-fold improvement on the performance of pattern matching, and 10-fold improvement for tree lookup, over optimized software solutions. Since the accelerator is used for different tasks, the hardware requirements are small compared to an accelerator group that implements the same set of tasks. We also present accurate analytic models to estimate the execution time of these networking tasks.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1012704,no,undetermined,0
An integrated service architecture for managing capital market systems,"This article studies current developments and trends in the area of capital market systems. In particular, it defines the trading lifecycle and the activities associated with it. The article then investigates opportunities for the integration of legacy systems and existing communication protocols through distributed integrated services that correspond to established business processes. These integrated services link to basic services such as an exchange, a settlement, or a registry service. Examples of such integrated services include pre-trade services (e.g., analytics) or post-trade services (e.g., surveillance). The article then presents the various levels of integration in capital market systems and discusses the standards in place. It establishes that most interactions occur at a low level of abstraction such as the network (e.g., TCP/IP), data format (e.g., FIX, XML), and middleware levels (e.g., CORBA). Finally, the article discusses a software development methodology based on the use of design patterns. These design patterns address the essential aspects of managing integrated services in a technology-independent fashion. These aspects are service wrapping, service composition, service contracting, service discovery, and service execution. The objective of the methodology is to facilitate the rapid development of new integrated services that correspond to emerging business opportunities",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=980540,no,undetermined,0
Analytic hierarchy process model for global competitiveness of local companies a case for Thai banking business,"Advances in information technology radically impact all organizations, especially, in providing effective tools to integrate their operations more effectively, respond to market needs more flexibly, and serve their customers globally. This research aims to answer how the local companies survive in the global competitiveness. Using analytic hierarchy process (AHP) method, variables relevant to strategic management and information technology has been developed and with the application-based model, based on the Mc Kinsey framework. Results show that the long term vision of the leader gives the highest contribution to the success of local companies. Appropriate management of leadership and information technology significantly enhances the competitiveness of local companies. Result of evaluation of the proposed model with four local Thai Banks is in accordance with those of international credit rating agencies including Standard & Poors and Moody's Invertors Service.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1038548,no,undetermined,0
Analytic calculation of leakage inductances of dual frequency concentric windings transformers,"Static power electronic converters supplying railways DC or AC motors, require series inductances which can simply be the leakage short-circuit inductances of the traction transformer. This paper describes a one-dimensional analytical method which permits to calculate leakage inductances between primary and secondary windings of dual frequency transformers. A comparison to a two-dimensional analytical method as well as to measurement results on existing units is made. Furthermore, illustration software developed in Visual Basic on Microsoft Excel is presented",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=970642,no,undetermined,0
An analytic throughput model for TCP Reno over wireless networks,"Many wireless network applications are built on TCP, and will continue to be in the foreseeable future. It is important to study TCP performance in wireless networks scenario with high loss rate. In this paper, we develop a simple analytic characterization of the steady state throughput of a bulk transfer TCP Reno flow over wireless networks, as a function of loss rate and round trip time. Our model incorporates many important aspects, such as slow start, congestion avoidance, fast retransmit, fast recovery and timeout mechanism. Our simulation results demonstrate that our model is able to accurately predict TCP throughput over wireless networks and is accurate over a wide range of loss rate",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962584,no,undetermined,0
A weighted distribution of residual failure data using Weibull model,"In analyzing the distribution of residual failure data using the Weibull distribution from the first detection of a failure to the completion of repairing failures, we have proposed the method of applying a weight to each failure. To put it concretely, we have suggested the method of applying a weight according to the importance of each failure within the analytic area in order to manage and measure efficiently the failure data found in the course of confirmation, verification, debugging, adding, and repairing functions collectively. The procedure is carried out after constituting a model switching system and installing a synthesis software package on the HANbit ACE ATM switching system under the development phase, which is the main node in building the B-ISDN for multimedia services",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=905438,no,undetermined,0
A cost framework for evaluating integrated restructuring optimizations,"Loop transformations and array restructuring optimizations usually improve performance by increasing the memory locality of applications, but not always. For instance, loop and array restructuring can either complement or compete with one another. Previous research has proposed integrating loop and array restructuring, but there existed no analytic framework for determining how best to combine the optimizations for a given program. Since the choice of which optimizations to apply, alone or in combination, is highly application and input-dependent, a cost framework is needed if integrated restructuring is to be automated by an optimizing compiler. To this end, we develop a cost model that considers standard loop optimizations along with two potential forms of array restructuring: conventional copying-based restructuring and remapping-based restructuring that exploits a smart memory controller. We simulate eight applications on a variety of input sizes and with a variety of hand-applied restructuring optimizations. We find that employing a fixed strategy does not always deliver the best performance. Finally; our cost model accurately predicts the best combination of restructuring optimizations among those we examine, and yields performance within a geometric mean of 5% of the best combination across all benchmarks and input sizes",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953294,no,undetermined,0
A frequency spectrum model of the intermodulation products generated by paging transmitters,"The present paper presents a frequency spectrum model analytic method of the intermodulation products generated by paging transmitters. A frequency spectrum model and its simulation frequency spectrum of nineteen intermodulation products, which are generated by different kinds of paging transmitters, are shown. All these above could be used by the spectrum manager to identify the intermodulation interference. They could also be used to distinguish the interfering signals with the help of computer assisted software of searching interference",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=853950,no,undetermined,0
A method for 3D analysis of upper extremity kinematics applied to a case study of brachial plexus birth palsy,"Kinematic analysis of the upper extremity has been conducted using a wide variety of techniques, philosophies, and analytic methods. The authors propose a simple, surface marker model, using three-dimensional video recording, that borrows concepts from lower extremity kinematic analysis. A sequential order about orthogonal axes is described (Eulerian) to generate sagittal, coronal and transverse plane motion. The technique is applied to a child with brachial plexus birth palsy pre- and postoperatively",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=858899,no,undetermined,0
An analytic approach to pheromone-based coordination,"Stigmergetic coordination of large numbers of simple agents in natural agent systems yields complex and stable coordinated system-level behavior. The Pheromone Infrastructure, an enhancement of the runtime environment of software agent systems, supports the use of stigmergetic coordination mechanisms in synthetic ecosystems. This paper discusses how such mechanisms may be tuned and evaluated, employing a formal representation of the processes in the Pheromone Infrastructure",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=858479,no,undetermined,0
"Analytic performance model for speculative, synchronous, discrete-event simulation","Performance models exist that reliably describe the execution time and efficiency of parallel discrete-event simulations executed in a synchronous iterative fashion. These performance models incorporate the effects of processor heterogeneity, other processor load due to shared computational resources, application workload imbalance, and the use of speculative computation. This includes modeling the effects of predictive optimism, a technique for improving the accuracy of speculative assumptions. We extend these models to incorporate correlated workloads across the set of processors and validate the models with two different applications",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=847142,no,undetermined,0
Cephalometric Downs' analysis. A mathematical framework,Cephalometric procedure for craniofacial analysis is a very important procedure in orthodontics. Downs' analysis involves tedious geometric and analytic manual calculations to be performed on the X-ray or tracing of the profile of a skull. In this paper we have attempted to emulate this manual operation by modeling the process and creating a software that automatically calculates the required measurements. The orthodontist loads the lateral cephalogram (ceph) and the co-ordinates or landmarks are identified using a mouse interface. The identified landmarks are passed as inputs for the diagnosis module. The standard values are maintained as a table. The calculated values are compared with the standards and appropriate inferences are displayed. The program is also a user friendly one,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894454,no,undetermined,0
Design of discrete-time nonlinear observers,"Proposes an approach to the discrete-time nonlinear observer design problem. Based on the early ideas that influenced the development of the linear Luenberger observer, the proposed approach develops a nonlinear analogue. The formulation of the discrete-time nonlinear observer design problem is realized via a system of first-order linear functional equations, and a rather general set of necessary and sufficient conditions for solvability is derived using results from linear functional equation theory. The solution to the above system of linear functional equations can be proven to be locally analytic and this enables the development of a series solution method, that is easily programmable with the aid of a symbolic software package",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=878591,no,undetermined,0
Designing process replication and activation: a quantitative approach,"Distributed application systems are composed of classes of objects with instances that interact to accomplish common goals. Such systems can have many classes of users with many types of requests. Furthermore, the relative load of these classes can shift throughout the day, causing changes to system behavior and bottlenecks. When designing and deploying such systems, it is necessary to determine a process replication and threading policy for the server processes that contain the objects, as well as process activation policies. To avoid bottlenecks, the policy must support all possible workload conditions. Licensing, implementation or resource constraints can limit the number of permitted replicas or threads of a server process. Process activation policies determine whether a server is persistent or should be created and terminated with each call. This paper describes quantitative techniques for choosing process replication or threading levels and process activation policies. Inappropriate policies can lead to unnecessary queuing delays for callers or unnecessarily high consumption of memory resources. The algorithms presented consider all workload conditions, are iterative in nature and are hybrid mathematical programming and analytic performance evaluation methods. An example is given to demonstrate the technique and describe how the results can be applied during software design and deployment",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888630,no,undetermined,0
E-representative: a scalability scheme for e-commerce,"In order to meet the quality of service demanded by a growing number of online customers, e-commerce services need to use scalability techniques. This paper introduces the concept of e-commerce representatives, a means of scaling the performance of e-commerce services. E-representatives are programs that execute on a cache server or at nearby machines. E-representatives can be implemented using redirection, a mechanism available in popular cache servers. Using analytical and simulation models, we show the potential performance gains obtained by e-commerce sites that distribute their services among e-representatives and contribute to reduce bandwidth consumption and network latency",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=853872,no,undetermined,0
On the stability of a Tank and Hopfield type neural network in the general case of complex eigenvalues,"The stability of a Tank and Hopfield-type neural network is investigated for the general case of practically encountered complex eigenvalues s<sub>D</sub> of the matrix product D<sub>g</sub><sup>T</sup> D<sub>f</sub>, where D<sub>g</sub> and D<sub>f </sub> are approximations of the connection matrix D on the signal and constraint sides of the neural net, respectively. A stability criterion in the form of an analytic expression is derived, thus generalizing the results obtained by Yan (1991) for the special case of purely real eigenvalues",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=815505,no,undetermined,0
Parametric design synthesis of distributed embedded systems,"This paper presents a design synthesis method for distributed embedded systems. In such systems, computations can flow through long pipelines of interacting software components, hosted on a variety of resources, each of which is managed by a local scheduler. Our method automatically calibrates the local resource schedulers to achieve the system's global end-to-end performance requirements. A system is modeled as a set of distributed task chains (or pipelines), where each task represents an activity requiring nonzero load from some CPU or network resource. Task load requirements can vary stochastically due to second-order effects like cache memory behavior, DMA interference, pipeline stalls, bus arbitration delays, transient head-of-line blocking, etc. We aggregate these effects-along with a task's per-service load demand and model them via a single random variable, ranging over an arbitrary discrete probability distribution. Load models can be obtained via profiling tasks in isolation or simply by using an engineer's hypothesis about the system's projected behavior. The end-to-end performance requirements are posited in terms of throughput and delay constraints. Specifically, a pipeline's delay constraint is an upper bound on the total latency a computatation can accumulate, from input to output. The corresponding throughput constraint mandates the pipeline's minimum acceptable output rate-counting only outputs which meet their delay constraints. Since per-component loads can be generally distributed; and since resources host stages from multiple pipelines, meeting all of the system's end-to-end constraints is a nontrivial problem. Our approach involves solving two subproblems in tandem: 1) finding an optimal proportion of load to allocate to each task and channel and 2) deriving the best combination of service intervals over which all load proportions can be guaranteed. The design algorithms use analytic approximations to quickly estimate output rates and propagation delays for candidate solutions. When all parameters are synthesized, the estimated end-to-end performance metrics are rechecked by simulation. The per-component load reservations can then be increased, with the synthesis algorithms rerun to improve performance. At that point, the - system can be configured according to the synthesized scheduling parameters-and then revalidated via on-line profiling. In this paper, we demonstrate our technique on an example system, and compare the estimated performance to its simulated on-line behavior.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895934,no,undetermined,0
Performance scalability in multiprocessor systems with resource contention,"Multiple processes may contend for shared resources such as variables stored in the shared memory of a multiprocessor system. Mechanisms required to preserve data consistency on such systems often lead do a decrease in system performance. This research focuses on controlling shared resource contention for achieving high capacity and scalability in multiprocessor based applications that include telephone switches and real time databases. Both reengineering of existing code as well as appropriate scheduling of the processes are two viable methods for controlling memory contention. Emphasis is placed on the second approach. Based on analytic models, three different scheduling approaches are compared. The numerical results obtained from the model provide insights into system behavior and highlight the important attributes of each strategy. A hybrid approach that combines the good attributes of a number of these strategies is proposed and analyzed. The results of this research are useful mainly to designers of software for multiprocessor based telecommunication and other embedded systems",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842292,no,undetermined,0
Phase-locked loop for grid-connected three-phase power conversion systems,"Analysis and design of a phase-locked loop (PLL) is presented for the power factor control of grid-connected three-phase power conversion systems. The dynamic characteristics of the closed loop PLL system with a second order are investigated in both continuous and discrete-time domains, and the optimisation method is discussed. In particular, the performance of the PLL in the three-phase system is analysed under the distorted utility conditions such as the phase unbalancing harmonics, and offset caused by nonlinear loads and measurement errors. The PLL technique for the three-phase system is implemented in software of a digital signal processor to verify the analytic results, and the experiments are carried out for various utility conditions. This technique is finally applied to the grid-connected photovoltaic power generation system with the current-controlled PWM inverter as a subpart for generating the current reference of the inverter. The experimental results demonstrate its phase tracking capability in the three-phase grid-connected operation",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=848556,no,undetermined,0
PhysioNet: a research resource for studies of complex physiologic and biomedical signals,"PhysioNet (http://www.physionet.org/) is a web-based resource supplying well-characterized physiologic signals and related open-source software to the biomedical research community. Inaugurated in September 1999 under the auspices of the NIH's National Center for Research Resources (NCRR), PhysioNet provides an on-line forum for free dissemination and exchange of research data and software, with facilities for cooperative analysis of data and evaluation of new analytic methods. As of September 2000, PhysioBank, the data archive made available via PhysioNet, contained roughly 35 gigabytes of recorded signals and annotations. PhysioNet is a public service of the Research Resource for Complex Physiologic Signals, a cooperative project initiated by researchers at Boston's Beth Israel Deaconess Medical Center/Harvard Medical School, Boston University, McGill University, and MIT",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=898485,no,undetermined,0
Simulation of hardware support for OpenGL graphics architecture,"The growing number of applications for 3D graphics and imaging systems in the mass market requires the customized approach to the design of high-performance 3D graphics and imaging system architectures. That fact coupled with the strong trends to industrial standardization of graphics API, such as OpenGL, leads to enhanced portability for imaging and graphics applications. The progress on the hardware support of API functionality gives a real possibility to use a top-down approach in the design of custom graphics/imaging systems based on the mixed hardware/software implementation of the OpenGL architecture. Software simulation is a relatively cheap and fast way for initial graphics architecture template development and evaluation to acquire the structure and parameters that can be used later for HDL elaboration and simulation. This paper discusses a 3D graphics and imaging system architecture model implementation based on OpenGL API hardware support simulation. The object-oriented approach has been used for this development of analytic and software models simulating hardware units on different stages of the graphics pipeline. The Interactive Imaging System Architecture Composer has been developed in this project for fast run-time simulation management, data acquisition and interpretation. The expandable set of architecture templates uses a reconfigurable shared library of äóìvirtual hardware unitsäó simulating the different hardware structures for graphics and image processing",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=844241,no,undetermined,0
Spreading sequence sets with zero and low correlation zone for quasi-synchronous CDMA communication systems,"This paper considers the application of sequences with low correlation zone (LCZ) and zero correlation zone (ZCZ) to quasi-synchronous DS/CDMA communication systems. Several classes of sequences with LCZ and ZCZ are described. Numerical results show that the BER performances of systems using ZCZ sequences and LCZ sequences are comparable with the same number of users and almost the same processing gain. In addition, when the timing error is large, the ZCZ sequences perform better than the LCZ sequences. Analytic results are verified with simulations",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=886114,no,undetermined,0
The design and implementation of a WWW traffic generator,"With the growing importance of the World Wide Web, Web managers are facing the problem of how to accurately measure the performance of their Web servers. This paper addresses this problem by introducing a traffic generator which is developed by using the Java language. The generator generates Web workload by using an analytic model which models the arrivals at the user level. It is both self-scaling and self-configuring, meaning that it can scale the traffic to any intensities and configure the arrival characteristics to conform to those of any specific Web site. The latter is done by analyzing a site's access log for adjusting the model parameters being used during traffic generation. In contrast to other traffic generator being used in common Web benchmark software, the real content of the target Web server is used as the testing file set instead of just using a reduced standard file set. Experimental results on using the generator to test two Web servers are also presented. The results show that the generator is both functional and useful to measure the performance of Web servers under real situations",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857736,no,undetermined,0
The effect of average parallelism and CPU-I/O overlap on application speedup,Parallelization of I/O using multiple disk drives is warranted for achieving high performance on systems running CPU and I/O intensive applications. This paper is concerned with the characterization of applications with parallel I/O and the derivation of analytic bounds on speedup. New characteristics for capturing parallelism in I/O are introduced. These characteristics are used in conjunction with other characteristics available in the literature for the derivation of the bounds. Both parallelism in CPU and I/O operations in applications as well as the degree of overlap between CPU and I/O are observed to have a strong impact on speedup,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884646,no,undetermined,0
Using intelligent agents to identify missing and exploited children,"Recent high-profile stings and convictions in the USA have amply demonstrated the Internet's rapid expansion and its increasing use by pedophiles and other sexual predators. Unfortunately, inadequate manpower and fiscal support often mar federal and state investigative operations, and the Internet's ever-expanding growth will only make these efforts more difficult in the future. However, modern computer technology, especially the Internet and agent based software, might help ongoing and future investigations. As part of a National Institute of Justice grant, Analytic Services (Anser) is developing an intelligent-software-agent system that uses the Internet to locate missing and exploited children. We focused our agents in two areas: knowledge discovery and predictive data mining (KD&PDM) for intranet data sources and Web analytics, and intelligent search and discovery (IS&D) for the Internet. Our system is, so far, only an alpha system. We haven't used it with a customer, even though several have expressed interest in its usage after viewing some of its findings and demonstrations. We still need to provide a user interface (the system is completely customizable) and we continue to test the various agents over the Internet, in chat rooms, and in intranets",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=850824,no,undetermined,0
WebSifter: an ontology-based personalizable search agent for the Web,"The World Wide Web provides access to a great deal of information on a vast array of subjects. In a typical Web search a vast amount of information is retrieved. The quantity can be overwhelming, and much of the information may be marginally relevant or completely irrelevant to the user's request. We present a methodology, architecture, and proof-of-concept prototype for query construction and results analysis that provides the user with a ranking of choices based on the user's determination of importance. The user initially designs the query with assistance from the user's profile, a thesaurus, and previously constructed queries acting as a taxonomy of the information requirements. After the query has returned its results, decision analytic methods and information source reliability information are used in conjunction with the expanded taxonomy to rank the solution candidates",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942176,no,undetermined,0
Application of simulation and mean value analysis to a repair facility model for finding optimal staffing levels,"Staffing problems arise in a wide range of applications including job shops, call centres, and hospital emergency departments. They are characterised by the need to allocate shift workers with varying skills to handle an arrival stream of tasks having different sub-task routings and (sub-task) skill requirements. The Manitoba Telecom Service Trouble Diagnosis and Repair System (TDRS) has 3 skill-levels of staff handling multiple types of faults occurring in telephone switching equipment. TDRS is a pure staffing problem having no equipment constraints: the only resource constraint is staff itself. The object of this study is to show how this can be modelled as an open network of queues with feedback and allowing for temporal and fault-class heterogeneity. Analytic mean value analysis then facilitates validation and selecting feasible staffing strategies for closer examination by simulation. The purpose of experiments using simulation is to find effective performance visualisations and ""optimal"" staffing allocations.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166482,no,undetermined,0
Bit-level allocation of multiple-precision specifications,"This paper proposes an allocation algorithm able to perform the combined resource selection and operation binding of multiple-precision specifications that maximizes the bit-level reuse of hardware resources. Additionally, it presents an analytic method to estimate the amount of area that our approach could save in comparison with traditional allocation algorithms. In order to minimize the cost of the implementations obtained, the proposed algorithm produces circuits only influenced by the maximum number of bits calculated per cycle. This approach contrasts with the cost of implementations designed by traditional algorithms, which also depends on the number and widths of the operations executed in every cycle.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115396,no,undetermined,0
"An analysis on the process from ""play"" to ""learning""","It has been recognized especially in preschool that ""play"" is very useful for educating children since early times. Infants play just for enjoyment and learn many things such as scientific concept, communication and physical function through play. Many schoolteachers and educators had tried to make great efforts to apply ""play"" to schooling. To date many educational practices have been reported. But the mechanism of the transition process from ""play"" to ""learning"" as well as important factors for it has not been made apparent yet. We introduce an agent-model with several parameters, based on the probabilistic game theory and obtain analytic solution. The result shows that our model exhibits the existence of the mode transition from play-mode to learning-mode in a certain parameter regime.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1358169,no,undetermined,0
Rate distortion optimization in the scalable video coding,"This paper discusses the rate distortion optimization (RDO) issue in the scalable video coding. It is different from that in the non-scalable video coding because a scalable codec usually presents the decoded video within a certain range of the quality, frame rate and resolution. Therefore, this paper first analyzes the problem from a general model and gives a common description by the Lagrangian formula. Secondly, an algorithm is proposed to solve the above optimization problem at a specific scalable codec: the H.26L-based PFGS. At the same time, two analytic formulae are derived to ascertain the parameters ‘é in this algorithm. The experimental results show that the proposed algorithm significantly improves the coding efficiency of the H.26L-based PFGS. Further studies on this problem will be quite significant and challenging.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1206116,no,undetermined,0
Digitally controlled converter with dynamic change of control law and power throughput,"With the continuous development of faster and cheaper microprocessors the field of applications for digital control is constantly expanding. Based on this trend we describe the analysis and implementation of multiple control laws within the same controller. Also, implemented within the control algorithm is a thermal monitoring scheme used for assessment of safe converter power throughput. An added benefit of this thermal monitoring is the possibility of software implemented analytic redundancy, which improves system fault resilience. Finally, reliability issues concerning the substitution of analog controllers with their digital counterparts are considered. The outline of the paper is divided into two segments - the first being an experimental analysis of the timing behavior by means of code optimization - the second being an examination of the dynamics of incorporating two control laws using multiple control parameters.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1216613,no,undetermined,0
Evaluation of several nonparametric bootstrap methods to estimate confidence intervals for software metrics,"Sample statistics and model parameters can be used to infer the properties, or characteristics, of the underlying population in typical data-analytic situations. Confidence intervals can provide an estimate of the range within which the true value of the statistic lies. A narrow confidence interval implies low variability of the statistic, justifying a strong conclusion made from the analysis. Many statistics used in software metrics analysis do not come with theoretical formulas to allow such accuracy assessment. The Efron bootstrap statistical analysis appears to address this weakness. In this paper, we present an empirical analysis of the reliability of several Efron nonparametric bootstrap methods in assessing the accuracy of sample statistics in the context of software metrics. A brief review on the basic concept of various methods available for the estimation of statistical errors is provided, with the stated advantages of the Efron bootstrap discussed. Validations of several different bootstrap algorithms are performed across basic software metrics in both simulated and industrial software engineering contexts. It was found that the 90 percent confidence intervals for mean, median, and Spearman correlation coefficients were accurately predicted. The 90 percent confidence intervals for the variance and Pearson correlation coefficients were typically underestimated (60-70 percent confidence interval), and those for skewness and kurtosis overestimated (98-100 percent confidence interval). It was found that the Bias-corrected and accelerated bootstrap approach gave the most consistent confidence intervals, but its accuracy depended on the metric examined. A method for correcting the under-/ overestimation of bootstrap confidence intervals for small data sets is suggested, but the success of the approach was found to be inconsistent across the tested metrics.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245301,no,undetermined,0
Exact probability of erasure and a decoding algorithm for convolutional codes on the binary erasure channel,"Analytic expressions for the exact probability of erasure for systematic, rate- 1/2 convolutional codes used to communicate over the binary erasure channel and decoded using the soft-input, soft-output (SISO) and a posteriori probability (APP) algorithms are given. An alternative forward-backward algorithm which produces the same result as the SISO algorithm is also given. This low-complexity implementation, based upon lookup tables, is of interest for systems which use convolutional codes, such as turbo codes.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1258535,no,undetermined,0
Fractional cut: improved recursive bisection placement,"In this paper, we present improvements to recursive bisection based placement. In contrast to prior work, our horizontal cut lines are not restricted to row boundaries; this avoids a ""narrow region"" problem. To support these new cut line positions, a dynamic programming based legalization algorithm has been developed. The combination of these has improved the stability and lowered the wire lengths produced by our Feng Shui placement tool. On benchmarks derived from industry partitioning examples, our results are close to those of the annealing based tool Dragon, while taking only a fraction of the run time. On synthetic benchmarks, our wire lengths are nearly 23% better than those of Dragon. For both benchmark suites, our results are substantially better than those of the recursive bisection based tool Capo and the analytic placement tool Kraftwerk.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1257685,no,undetermined,0
Internet quarantine: requirements for containing self-propagating code,"It has been clear since 1988 that self-propagating code can quickly spread across a network by exploiting homogeneous security vulnerabilities. However, the last few years have seen a dramatic increase in the frequency and virulence of such ""worm"" outbreaks. For example, the Code-Red worm epidemics of 2001 infected hundreds of thousands of Internet hosts in a very short period - incurring enormous operational expense to track down, contain, and repair each infected machine. In response to this threat, there is considerable effort focused on developing technical means for detecting and containing worm infections before they can cause such damage. This paper does not propose a particular technology to address this problem, but instead focuses on a more basic question: How well will any such approach contain a worm epidemic on the Internet? We describe the design space of worm containment systems using three key parameters - reaction time, containment strategy and deployment scenario. Using a combination of analytic modeling and simulation, we describe how each of these design factors impacts the dynamics of a worm epidemic and, conversely, the minimum engineering requirements necessary to contain the spread of a given worm. While our analysis cannot provide definitive guidance for engineering defenses against all future threats, we demonstrate the lower bounds that any such system must exceed to be useful today. Unfortunately, our results suggest that there are significant technological and administrative gaps to be bridged before an effective defense can be provided in today's Internet.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1209212,no,undetermined,0
MACH3: A CHSSI code for computational magnetohydrodynamics of general materials in generalized coordinates,"We were funded via the Common HPC Software Support Initiative (CHSSI) from 2000-2002 to develop a fully 3D arbitrary-coordinate parallel time-domain magnetohydrodynamic (MHD) simulation code - the CEA-10 project. CEA-10 built upon the single-fluid MHD, arbitrary Lagrangian-Eulerian multiblock, multitemperature, simulation environment called MACH3 (Multibloch Arbitrary Coordinate Hydromagnetics in 3D). The CEA-10 software underwent successful beta testing and review in December 2002. The beta tests consisted of five simulations: 1D magnetic diffusion, 2D Alfvenic shock, 2D flow over a wedge, 3D gas-filled rod pinch, and 3D laser-target interactions, run on various HPC platforms. The first three have analytic solutions and last two have experimental data and qualitative models to which simulated results can be compared. We obtained different scaled efficiency and parallel speedup results on different platforms and for different test problems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253397,no,undetermined,0
Modeling malware spreading dynamics,"In this paper we present analytical techniques that can be used to better understand the behavior of malware, a generic term that refers to all kinds of malicious software programs propagating on the Internet, such as e-mail viruses and worms. We develop a modeling methodology based on Interactive Markov Chains that is able to capture many aspects of the problem, especially the impact of the underlying topology on the spreading characteristics of malware. We propose numerical methods to obtain useful bounds and approximations in the case of very large systems, validating our results through simulation. An analytic methodology represents a fundamentally important step in the development of effective countermeasures for future malware activity. Furthermore, we believe our approach can help to understand a wide range of ""dynamic interactions on networks"", such as routing protocols and peer-to-peer applications.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1209209,no,undetermined,0
Multi-version attack recovery for workflow systems,"Workflow systems are popular in daily business processing. Since vulnerabilities cannot be totally removed from a system, recovery from successful attacks is unavoidable. We focus on attacks that inject malicious tasks into workflow management systems. We introduce practical techniques for on-line attack recovery, which include rules for locating damage and rules for execution order. In our system, an independent intrusion detection system reports identified malicious tasks periodically. The recovery system detects all damage caused by the malicious tasks and automatically repairs the damage according to dependency relations. Without multiple versions of data objects, recovery tasks may be corrupted by executing normal tasks when we try to run damage analysis and normal tasks concurrently. We address the problem by introducing multiversion data objects to reduce unnecessary blocking of normal task execution and improve the performance of the whole system. We analyze the integrity level and performance of our system. The analytic results demonstrate guidelines for designing such kinds of systems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254319,no,undetermined,0
On the accuracy of a cellular location system based on RSS measurements,"We discuss the performance of a cellular location system based on received signal strength (RSS) measurements. Each mobile station (MS) collects RSS measurements of the downlink control channels transmitted by the surrounding base stations. It is assumed that there is one-to-one mapping between the RSS and the MS location. Hence, these measurements can be used to obtain the MS location. We examine the accuracy of this method by deriving the Cramer-Rao bound, the concentration ellipse, and the circular error probability (CEP) of this method. In addition, we obtain an analytic expression that predicts the point at which accuracy deviates significantly from the bound (the threshold point). The accuracy of the method does not meet the FCC E911 requirement, but it is an attractive solution for less-demanding location-based services.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1247811,no,undetermined,0
"Semantically reliable multicast: definition, implementation, and performance evaluation","Semantic reliability is a novel correctness criterion for multicast protocols based on the concept of message obsolescence: A message becomes obsolete when its content or purpose is superseded by a subsequent message. By exploiting obsolescence, a reliable multicast protocol may drop irrelevant messages to find additional buffer space for new messages. This makes the multicast protocol more resilient to transient performance perturbations of group members, thus improving throughput stability. This paper describes our experience in developing a suite of semantically reliable protocols. It summarizes the motivation, definition, and algorithmic issues and presents performance figures obtained with a running implementation. The data obtained experimentally is compared with analytic and simulation models. This comparison allows us to confirm the validity of these models and the usefulness of the approach. Finally, the paper reports the application of our prototype to distributed multiplayer games.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1176983,no,undetermined,0
Clinical engineering technology assessment decision support: a case study using the analytic hierarchy process (AHP),"This case study showed how the AHP decision support technique can be applied to clinical engineering health technology assessment projects. AHP provides a structured method of organizing and documenting the decision process and takes into consideration the many tradeoffs that exist between alternate choices. When an AHP model is properly designed and implemented, it facilitates interdepartmental and interdisciplinary communication and results in a decision support tool that represents a consensus model. The AHP model can then be used to compare health technology alternatives and delivers a composite score for each alternative that identifies the best choice. AHP produces a clinical engineering decision support tool for the hospital that identifies the best technology alternative for their specific need. Further, the model can be updated or adapted to different medical technologies whenever needed, so the development investment is not wasted.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1053110,no,undetermined,0
Simulation and analytical calculation of the noise figure in HEMT gate mixers,"A simplified analytic expression is developed to predict the noise performance of HEMT gate mixers. A nonlinear model of noise was given for theoretical study and implanted in ADS simulator. A contribution of each noise source was presented in this paper. This study is applied to a millimeter-wave HEMT gate mixer. The LO, RF and IF frequencies chosen for this test were 24.5, 28.5 and 4 GHz respectively. Good agreement is obtained between analytical calculation, simulation and experimental noise figure in single side band NF<sub>SSB</sub>,.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1262291,no,undetermined,0
Smooth and efficient zooming and panning,"Large 2D information spaces, such as maps, images, or abstract visualizations, require views at various level of detail: close ups to inspect details, overviews to maintain (literally) an overview. Users often switch between these views. We discuss how smooth animations from one view to another can be defined. To this end, a metric on the effect of simultaneous zooming and panning is defined, based on an estimate of the perceived velocity. Optimal is defined as smooth and efficient. Given the metric, these terms can be translated into a computational model, which is used to calculate an analytic solution for optimal animations. The model has two free parameters: animation speed and zoom/pan trade off. A user experiment to find good values for these is described.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1249004,no,undetermined,0
Technology Management for Reshaping the World. PICMET'03. Portland International Conference on Management of Engineering and Technology (IEEE Cat. No.03CH37455),"The following topics are dealt with: strategic and policy issues in technology management; R&D and innovation management; knowledge management; information and communication management; project and product management; software process management; technological entrepreneurship; technology roadmapping; and technology transfer, commercialization and marketing.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222771,no,undetermined,0
Ultrasound simulation for 3D-axisymmetric models,"Development of ultrasound nondestructive evaluation testing (NDT) techniques has involved a combination of both analytic and experimental methods. In contrast, relatively little software exists for ultrasound simulation in NDT, particularly in comparison to that existing for stress-strain and electromagnetic areas. This paper describes new software for simulating the full (longitudinal and shear) solution to the three-dimensional (3D) axisymmetric wave equation. The simulation software is able to model both liquids and solids, and also accounts for losses using a classic viscoelastic model. The program computes the solution using a finite difference time domain algorithm, and evaluates the displacement vector at each (discrete grid) point of the object. Sources and receivers may be placed anywhere in or on the object, which is assumed to be cylindrical. A comparison of the on-axis diffraction pattern results obtained with the simulation software show excellent agreement with analytic results. Results using a cladded rod are also presented. This software should help to broaden the use of computational methods in ultrasonic NDT and in ultrasonics in general.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1293325,no,undetermined,0
"Usage of OLAP-means of the system ""analytics"" for the problem of health protection","The health care sector has been experiencing some difficulty as a result of the implementation of some new principles in planning and management and the continuous accumulation of electronic information. To help ease the situation, the use of an analytical modeling software, called OLAP (online analytical processing), is suggested. With the aid of OLAP, the capability of business analysis is significantly improved in terms of increased operation speed, presentation of the results and operative design of analytical reports.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1438173,no,undetermined,0
Web tool opens up power system visualization,"This article presents a flexible architecture of a Web-based tool that serves as a platform for power system visualization with open data structures. Different from regular applications, this platform may be viewed as a semicompleted application. It implements common features of a power system application such as GIS-like drawing and visualization, built-in topology processor, and so on. Meanwhile, the platform defines open data structures for system components with the consideration of efficiency and flexibility. It also provides a mechanism to link itself with external engines, instead of implementing the engines directly. This architecture may achieve considerable flexibility. For instance, users may extend and customize data structures of power system components. Users may also develop their own analytic engines based on their specific needs and link them with the platform. To maximize the benefits to users, the proposed platform is Web-enabled with Java client-side technology. It is universally accessible, instantaneously upgradable, and operating-system independent.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1213525,no,undetermined,0
Worst cases and lattice reduction,"We propose a new algorithm to find worst cases for correct rounding of an analytic function. We first reduce this problem to the real small value problem - i.e. for polynomials with real coefficients. Then we show that this second problem can be solved efficiently, by extending Coppersmith's work on the integer small value problem - for polynomials with integer coefficients - using lattice reduction (D. Coppersmith, 1996; 2001). For floating-point numbers with a mantissa less than N, and a polynomial approximation of degree d, our algorithm finds all worst cases at distance < N<sup>-d2</sup>/(2d+1) from a machine number in time O(N<sup>((d+1)</sup>(2d+1))+‘µ/). For d=2, this improves on the O(N<sup>2</sup>(3+‘µ)/) complexity from Lefevre's algorithm (V. Lefevre, 2000; V. Lefevre et al., 2001) to O(N<sup>3</sup>(5+‘µ)/). We exhibit some new worst cases found using our algorithm, for double-extended and quadruple precision. For larger d, our algorithm can be used to check that there exist no worst cases at distance < N<sup>-k</sup> in time O(N<sup>(1</sup>2)+O(1/k)/).",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207672,no,undetermined,0
A case study of organizational effects in a distributed sensor network,"We describe how a system employing different types of organizational techniques addresses the challenges posed by a large-scale distributed sensor network environment. The high-level multiagent architecture of real-world system is given in detail, and empirical and analytic results are provided showing the various effects that organizational characteristics have on the system's performance. We show how partitioning of the environment can lead to better locality and more constrained communication, as well as disproportionate load on individuals or increased load on the population as a whole. The presence of such tradeoffs motivates the need for a better understanding of organizational effects.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342923,no,undetermined,0
Active performance management in supply chains,"The importance of performance management in supply chains has long been recognized from a variety of functional disciplines. But much of the work has focused on designing performance measures with less concern for the other stages of the entire performance management process. In this paper, an integrated, efficient and effective performance management system, ""active performance management system"", is presented. The system covers the entire performance management process including measures design, analysis, and dynamic update. The analysis of performance measures using causal loop diagrams, qualitative inference and analytic network process is mainly discussed in this paper. A real world case study is carried out throughout the paper to explain how the framework works. A software tool, supply chain performance analyzer, is also introduced. Some related topics and further research directions are discussed as concluding remarks.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1401345,no,undetermined,0
Challenges in computer architecture evaluation,"We focus on problems suited to the current evaluation infrastructure. The current limitation and trends in evaluation techniques are troublesome and could noticeably slow the rate of computer system innovation. New research has been recommended to help and make quantitative evaluations of computer systems manageable. We support research in the areas of simulation frameworks, benchmarking methodologies, analytic methods, and validation techniques.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1220579,no,undetermined,0
Case studies using graphically enhanced computer software to improve technology assessments and enhance clinical decisions,"This paper describes how clinical engineers and healthcare CIOs, CTOs, and IT/IS specialists can use the Analytic Hierarchy Process (AHP) to improve the quality of diverse and important decisions that hospitals face today. AHP is a versatile and proven decision support tool that allows the user to design a hierarchical structure for decision-making and weighs the trade-offs between decision criteria and alternatives (Saaty 1977, Saaty 1996).",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1280942,no,undetermined,0
An agent-based architecture for analyzing business processes of real-time enterprises,"As the desire for business intelligence capabilities for e-business processes expands, existing workflow management systems and decision support systems are not able to provide continuous, real-time analytics for decision makers. Business intelligence requirements may appear to be different across the various industries, but the underlying requirements are similar nformation that is integrated, current, detailed, and immediately accessible. In this paper we introduce an agent-based architecture that supports a complete business intelligence process to sense, interpret, predict, automate and respond to business processes and aims to decrease the time it takes to make business decisions. In fact, there should be almost zero-latency between the cause and effect of a business decision. Our architecture enables analysis across corporate business processes notifies the business of auctionable recommendations or automatically triggers business operations, effectively closing the gap between business intelligence systems and business processes.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1233840,no,undetermined,0
"Acquisition, representation, query and analysis of spatial data: a demonstration 3D digital library","The increasing power of techniques to model complex geometry and extract meaning from 3D information create complex data that must be described, stored, and displayed to be useful to researchers. Responding to the limitations of two-dimensional (2D) data representations perceived by discipline scientists, the Partnership for Research in Spatial Modeling (PRISM) project at Arizona State University (ASU) developed modeling and analytic tools that raise the level of abstraction and add semantic value to 3D data. The goals are to improve scientific communication, and to assist in generating new knowledge, particularly for natural objects whose asymmetry limit study using 2D representations. The tools simplify analysis of surface and volume using curvature and topology to help researchers understand and interact with 3D data. The tools produced automatically extract information about features and regions of interest to researchers, calculate quantifiable, replicable metric data, and generate metadata about the object being studied. To help researchers interact with the information, the project developed prototype interactive, sketch-based interfaces that permit researchers to remotely search, identify and interact with the detailed, highly accurate 3D models of the objects. The results support comparative analysis of contextual and spatial information, and extend research about asymmetric man-made and natural objects.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1204855,no,undetermined,0
Combination of high resolution analytically computed uncollided flux images with low resolution Monte Carlo computed scattered flux images,"A new computing model has been implemented in Sindbad, a simulation software dedicated to radiographic systems, to provide quite rapidly realistic radiographs of complex objects, including the scatter component of the photon flux. The new computing model consists of a coupling of two previous models : an analytic approach and a Monte Carlo one. The analytic simulation is used to compute a high resolution uncollided photon flux image whereas the Monte Carlo provides the scattered flux image. This latter image is computed as an extrapolation of a reduced dose Monte Carlo scattered flux image. Extrapolation is possible thanks to the low resolution feature of the scattered flux image. After a detailed description of the method developed to perform the combination, we present a validation on a medical radiograph simulation. In spite of the quite complex geometry of the examined part and the important emitted dose, this new model provides a realistic radiograph with a good estimation of the scattered flux in a reasonable computation time.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1239452,no,undetermined,0
Comparison of analytic and numerical models with commercially available simulation tools for the prediction of semiconductor freeze-out and exhaustion,"Currently, commercial software packages, such as available through Silvaco International, are well designed to solve the electron/hole transport problem. This type of calculation is usually required to predict the device IV characteristic. Surprisingly, using the same package, to obtain a temperature dependent plot for majority carrier concentration for a uniform semiconductor requires a somewhat complicated procedure. Our paper will present an efficient novel way of obtaining this curve from the Silvaco International software and compare the results with a proposed one dimensional single-equation analytic model and a numerical model that predict the temperature dependence for majority concentration in all regimes.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1187148,no,undetermined,0
ControlWare: a middleware architecture for feedback control of software performance,"Attainment of software performance assurances in open, largely unpredictable environments has recently become an important focus for real-time research. Unlike closed embedded systems, many contemporary distributed real-time applications operate in environments where offered load and available resources suffer considerable random fluctuations, thereby complicating the performance assurance problem. Feedback control theory has recently been identified as a promising analytic foundation for controlling performance of such unpredictable, poorly modeled software systems, the same way other engineering disciplines have used this theory for physical process control. In this paper we describe the design and implementation of ControlWare, a middleware QoS-control architecture based on control theory, motivated by the needs of performance-assured Internet services. It offers a new type of guarantees we call convergence guarantees that lie between hard and probabilistic guarantees. The efficacy of the architecture in achieving its QoS goals under realistic load conditions is demonstrated in the context of web server and proxy QoS management.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1022267,no,undetermined,0
Design of adaptive and reliable mobile agent communication protocols,"This paper presents a mailbox-based scheme for designing flexible and adaptive message delivery protocols in mobile agent (MA) systems. The scheme associates each mobile agent with a mailbox while allowing the decoupling between them, i.e., a mobile agent can migrate to a new site without bringing its mailbox. By separating the concerns of locating the mailbox of a mobile agent and delivering a message to the agent, we obtain a large space of protocol design with flexibility. Using a three-dimensional model based on the scheme, we have developed a taxonomy of MA communication protocols, which not only covers, as special cases, several known MA message delivery protocols, but also allows for the design of new ones well suited for various application requirements. We describe such an efficient and adaptive protocol derived front the model. The protocol guarantees reliable delivery of messages to mobile agents. We analyze the design trade-offs and performance of the protocol, using an analytic model as well as extensive simulation experiments.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1022295,no,undetermined,0
Image parameter modeling analog traveling-wave phase shifters,The aim of this work is to present a modeling procedure for the traveling-wave analog phase shifter. The proposed approach is based on the image representation of two-port networks. Design considerations to optimize the electric performance of the component reducing also the size of the structure are discussed. A commercial software package is used for the final electric simulation of the actual phase shifters to check the validity of the analytic model presented.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1039504,no,undetermined,0
Integrating GSS and AHP: experiences from benchmarking of buyer-supplier relationships,"The increasing overall global competition forces requirements for companies to improve their business performance continuously in every sector. The utilization of external resources, including a supplier network, has become one of the most critical development areas of business, needing a lot of attention. The performance of buyer-supplier relationships will be enhanced by using a benchmarking method in aiding the identification and implementation of development actions required for reaching the world-class level. The benchmarking process will be led by two decision support systems: AHP and GSS, to ensure valuable outcomes of the benchmarking with less resources.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=993906,no,undetermined,0
New method for dynamic modeling of hybrid stepping motors,"To be able to calculate the performances of a hybrid stepping motor or to design it in a short time, it is essential to apply an efficient model, accepting a slight loss of precision. Using software based on finite elements and executing calculations are difficult and time consuming, but normally offers increased precision. Finite elements may be used to better understand some phenomena and help establishing an analytic modeling based on magnetic equivalent scheme. Torque calculation is therefore much faster and permits to easily optimize a given structure. The dynamic modeling permits an estimation of dynamic behavior of the hybrid stepping motor in stationary and transient states.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044060,no,undetermined,0
On the Migration of the Scientific Code Dyana from SMPs to Clusters of PCs and on to the Grid,"Dyana is a molecular biology code used in the study of infectious prion proteins. Like many other scientific codes, Dyana was migrated successfully from vector supercomputers to some more cost-effective cluster of commodity PCs. A further migration to a widely distributed grid computing platform looks very tempting because many of these platforms promise the use of nearly free compute-cycles on the Internet. Not all codes are equally suited for all platforms. Even embarrassingly parallel codes might require a significant re-engineering effort for a migration from one platform to another. A better understanding of the performance characteristics of a code is required before a migration is attempted. To address this problem, we present a systematic method to study the viability of a code migration from one platform to another, before it is actually undertaken. We construct an analytic performance model of the application. We use the previous migration from SMPs to commodity clusters of PCs to validate and calibrate the model. Finally, we extrapolate the performance of Dyana widely distributed computing on the grid and we suggest optimizations in the process of migration. Our general model predicts that Dyana can efficiently use up to 42000 processors with its current workload and is therefore well suited for grid computing on the Internet.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1540445,no,undetermined,0
Optical character recognition for cursive handwriting,"A new analytic scheme, which uses a sequence of image segmentation and recognition algorithms, is proposed for the off-line cursive handwriting recognition problem. First, some global parameters, such as slant angle, baselines, stroke width and height, are estimated. Second, a segmentation method finds character segmentation paths by combining gray-scale and binary information. Third, a hidden Markov model (HMM) is employed for shape recognition to label and rank the character candidates. For this purpose, a string of codes is extracted from each segment to represent the character candidates. The estimation of feature space parameters is embedded in the HMM training stage together with the estimation of the HMM model parameters. Finally, information from a lexicon and from the HMM ranks is combined in a graph optimization problem for word-level recognition. This method corrects most of the errors produced by the segmentation and HMM ranking stages by maximizing an information measure in an efficient graph search algorithm. The experiments indicate higher recognition rates compared to the available methods reported in the literature",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008386,no,undetermined,0
Performance and implementation of distributed data CPHF and SCF algorithms,"This paper describes a novel distributed data parallel self consistent field (SCF) algorithm and the distributed data coupled perturbed Hartree-Fock (CPHF) step of an analytic Hessian algorithm. The distinguishing features of these algorithms are: (a) columns of density and Fock matrices are distributed among processors, (b) pairwise dynamic load balancing and an efficient static load balancer were developed to achieve a good workload, and (c) network communication time is minimized via careful analysis of data flow in the SCF and CPHF algorithms. By using a shared memory model, novel work load balancers, and improved analytic Hessian steps, we have developed codes that achieve superb performance. The performance of the CPHF code is demonstrated on a large biological system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137738,no,undetermined,0
Predicting TCP throughput from non-invasive network sampling,"In this paper, we wish to derive analytic models that predict the performance of TCP flows between specified endpoints using routinely observed network characteristics such as loss and delay. The ultimate goal of our approach is to convert network observables into representative user and application relevant performance metrics. The main contributions of this paper are in studying which network performance data sources are most reflective of session characteristics, and then in thoroughly investigating a new TCP model based on Padhye et al. (2000) that uses non-invasive network samples to predict the throughput of representative TCP flows between given end-points.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1019259,no,undetermined,0
Reliability prediction models to support conceptual design,"During the early stages of conceptual design, the ability to predict reliability is very limited. Without a prototype to test in a lab environment or without field data, component failure rates and system reliability performance are usually unknown. A popular method for early reliability prediction is to develop a computer model for the system. However, most of these models are extremely specific to an individual system or industry. This paper presents three general procedures (using both simulation and analytic solution techniques) for predicting system reliability and average mission cost. The procedures consider both known and unknown failure rates and component-level and subsystem-level analyzes. The estimates are based on the number of series subsystems and redundant (active or stand-by) components for each subsystem. The result is a set of approaches that engineers can use to predict system reliability early in the system-design process. Software was developed (and is discussed in this paper) that facilitates the application of the simulation-based techniques. For the specific type of system and mission addressed in this paper, the analytic approach is superior to the simulation-based prediction models. However, all three approaches are presented for two reasons: (1) to convey the development process involved with building these prediction tools; and (2) the simulation-based approaches are of greater value as the research is extended to consider more complex systems and scenarios",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011519,no,undetermined,0
RR interval time series modeling: the PhysioNet/Computers in Cardiology Challenge 2002,"Cardiac inter-beat (RR) interval time series contain fluctuations at time scales ranging from a few seconds to many hours. Realistic models of these series are potentially useful to researchers, not only as sources of surrogate data with known properties for evaluating novel analytic methods, but also as sources of insight into the diverse mechanisms underlying heart rate variability. PhysioNet and Computers in Cardiology have sponsored an open on-line competition aimed at stimulating the creation and exchange of high-quality models of RR interval variability, the third in an annual series of challenges for the research community. Participants first created software that was used to generate 24-hour synthetic time series, then attempted to identify the synthetic series within an unlabeled data set that included roughly equal amounts of real and synthetic data. All of the software models and the data used in the challenge are available at http://www.physionet.org/challenge/2002/.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166723,no,undetermined,0
Situation assessment via Bayesian belief networks,"We present here an approach to battlefield situation assessment based on a level 2 fusion processing of incoming information via probabilistic Bayesian Belief Network technology. A belief network (BN) can be thought of as a graphical program script representing causal relationships among various battlefield concepts represented as nodes to which observed significant events are posted as evidence. In our approach, each BN can be constructed in real-time from a library of smaller component-like BNs to assess a specific high-level situation or address mission-specific high-level intelligence requirements. Furthermore, by distributing components of a BN across a set of networked computers, we enhance inferencing efficiency and allow computation at various levels of abstraction suitable for military hierarchical organizations. We demonstrate them effectiveness of our approach by modeling the situation assessment tasks in the context of a battlefield scenario and implementing on our in-house software engine BNet 2000.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021218,no,undetermined,0
Stability analysis for reconfigurable systems with actuator saturation,"Discusses a combined analytic and simulation-based approach to assessing the stability of a control law in a system that may be subject to actuator saturation due to failures and subsequent reconfiguration. The analysis is based on linearized plant dynamics, a linearized state-feedback description of the nonlinear controller dynamics, and a nonlinear actuator model. For systems of this type, a method has previously been developed that provides less conservative estimates of the domain of attraction than other available methods. The domain of attraction estimates are used to guide simulation based stability analysis. The combined analytic and simulation based stability assessment approach is implemented in RASCLE, a software package designed to interface with an arbitrary C, C++, or FORTRAN simulation. Through the combination of analytic stability estimates and automated simulation-based analysis, RASCLE can efficiently provide information about the stability of the full nonlinear system under a wide range of conditions for the purpose of validating a reconfigurable controller.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1025415,no,undetermined,0
Task structuring a brainstorming group activity with an AHP-based group support system,"The use of group support systems has been researched repeatedly, using many task types. For brainstorming tasks, the system process support has been restricted to ensuring member anonymity and allowing the simultaneous entry of ideas. Little work has been done on investigating other process improvements for idea-generating groups. In this paper, we investigate the effect of decomposing a brainstorming task on idea quality and quantity in a South African setting. Using Team Expert Choice, which is an analytic hierarchy process (AHP)-based group support system, we conducted an experiment with two groups. We hypothesized that task decomposition would generate more and better-quality ideas. Our findings showed that task decomposition resulted in 40% more ideas than with no decomposition; the effect on decision quality is statistically significant only when the decision quality is measured as the number of good ideas.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994271,no,undetermined,0
The EFTOS approach to dependability in embedded supercomputing,"Industrial embedded supercomputing applications benefit from a systematic approach to fault tolerance. The EFTOS (embedded fault-tolerant supercomputing) framework provides a flexible and adaptable set of fault-tolerance tools from which the application developer can choose to make an embedded application on a parallel or distributed system more dependable. A high-level description (recovery language) helps the developer specify the fault-tolerance strategies of the application as a second application layer; this separates functional from fault-tolerance aspects of an application, thus shortening the development cycle and improving maintainability. The framework incorporates a backbone (to hook a set of fault-tolerance tools onto, and to coordinate the fault-tolerance actions) and a presentation layer (to monitor and test the fault tolerance behavior). A practical implementation is described with its performance evaluation, using an industrial case study from the energy-transport area, as well as an analytic deduction of the appropriateness of fault-tolerance techniques for various application profiles",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994916,no,undetermined,0
The fireworks effect: exploring trajectory sets in time,"Many students in physics and engineering courses often grapple with the mathematics they encounter and struggle to extract meaning from the analytic material they are learning, Much can be done computationally at this level to ask questions about and explore analytic functions or results in a numeric environment. Much can be done to help explain the analytic material that students must master if they gain some facility with software that provides a rich numeric environment, such as Mathcad or MATLAB. Often, approaching the same subject from different angles (here, analytic and numeric) helps the bridge-building process of learning. As an example of such an approach, where the opportunity for exploration is always present, I consider here two basic problems: trajectories under a constant force (where the behavior of a trajectory set illuminates the collective motion of a fireworks display) or under a Hooke's law force. The solutions for these examples are well known, yet the material is sufficiently rich so as to offer new insights when new questions are asked and explored.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=976441,no,undetermined,0
A new approach to the zero-dynamics assignment problem for nonlinear discrete-time systems using functional equations,"The present paper work aims at the development of a systematic method to arbitrarily assign the zero dynamics of a nonlinear discrete-time system by constructing the requisite synthetic output maps. The problem under consideration is motivated by the need to address the control problem of nonminimum-phase nonlinear discrete-time systems, since the latter represent a rather broad class of systems due to the well-known effect of sampling on the stability of zero-dynamics. In the proposed approach, the above objective can be attained through: a systematic computation of synthetic output maps that induce minimum-phase behavior, and the subsequent integration into the methodological framework of currently available nonminimum-phase compensation schemes that rely on output redefinition. The mathematical formulation of the zero-dynamics assignment problem is realized via a system of nonlinear functional equations (NFEs), a rather general set of necessary and sufficient conditions for solvability is derived. The solution to the above system of NFEs can be proven to be locally analytic, and this enables the development of a solution method that is easily programmable with the aid of a symbolic software package. The synthetic output maps that induce the prescribed zero dynamics for the original nonlinear discrete-time systems can be explicitly computed on the basis of the solution to the aforementioned system of NFEs.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1243774,no,undetermined,0
Resource management with stateful support for analytic applications,"Analytic applications from various industrial sectors have specific attributes and requirements including relatively long processing time, parallelization, multiple interactive invocations, Web services, and expected quality of service objectives. Current parallel resource management systems for batch-oriented jobs lack the effective support for multiple interactive invocations with consideration in quality of service objectives, while transaction processing systems do not support dynamic creation of parallel application instances. To better serve the analytic applications, a set of additional resource management services, defined as stateful support, introduces the concept of service instance and service instance management. This set of stateful support services can be implemented as extension to existing parallel resource management to serve these analytic applications that rapidly increase in the demand of computing power",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639695,no,undetermined,0
Simulation Study of TCP/IP Communication Based on Networked Control Systems,"This paper studies the TCP/IP communication issue of data stream in networked control systems (NCS). The character of the NCS is analyzed and the data stream type is discussed. Analytic model of TCP/IP is presented according to slow start theory. The simulation model is established based on OPNET software. The performance result of different type of data stream TCP/IP transmission is presented. Finally, the simulation result is analyzed, and defects and improved method of TCP/IP protocol is discussed",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713226,no,undetermined,0
A Fuzzy AHP and BSC Approach for Evaluating Performance of a Software Company Based on Knowledge Management,"The objective of this study is to construct an balanced scorecard (BSC) based on knowledge management and the fuzzy analytic hierarchy process (FAHP) for evaluating an software companies. The BSC concept is applied to define the hierarchy with four major perspectives (i.e. financial, customer, internal business process, and learning and growth), and performance indicators are selected for each perspective. A fuzzy AHP (FAHP) approach is then proposed in order to determine weight. The results provide guidance to software companies regarding strategies for improving department performance.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5455616,no,undetermined,0
Finite element analysis for local stability of thin-walled box section,"In view of the problem how to calculate the local stability on the structure of thin-walled box section, the finite element analysis was carried out with ANSYS package as tool in this paper. Considering the cross-section as a whole, a parametric finite element model and the parametric optimization model for the local stability calculation of the box section were established. Taking a rectangular cross-section as an example, its critical stress was calculated, and the result coincides with that from the analytic method. The increase degree in the critical stress that the rectangular cross-section is transformed into the hexagonal cross-section was obtained by another calculation. The local stability of a dodecagonal cross-section structure was also analyzed. The method given by this paper can applied to the calculation of the local stability on the many sorts of cross-section shape, which has practical reference for the design of the thin-walled box section structure.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4675403,no,undetermined,0
Design Method of UI of AV Remote Controller Based on AHP,"This paper proposes a new model for designing the user interface of an audio-visual remote controller based on an analytic hierarchy process. This model has the goal of switching the UI of the controller to one optimized for each user. The following four criteria were defined; demographic, geographic, psychographic and activity, sensor and I/O, Visual, Functional and Interaction interfaces were selected as the alternatives. Next, the method of selecting the most suitable UI for each user by using the proposed model based on AHP is described and shown by an example. Finally, the implementation of the proposal into a verification system for an AV remote controller using an embedded microprocessor for consumer use is described.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4587863,no,undetermined,0
Draw-lines algorithm in 2D for iris image,"Personal computers have become in a powerful tool to produce image, to interpret information or to improve the quality of the visualization of the same ones in a quick and a economic form. The algorithms used for graphics by computer, are used to create one or more images. In this work we are going to approach on the application of an algorithm for drawing lines in 2-D on images of iris in computers, which is based on concepts of analytic geometry. The implemented method uses the basic definitions of the polar coordinate equations and its transformation to the rectangular coordinates. In essence what is shown is the operation of the algorithm and that the drawing of lines in 2-D is exact and more efficient that the one developed by Bresenham to draw lines in 2-D on images of the iris. The proposed algorithm allows to make segmentation of the human eye with high accuracy; This is of much aid for the individual analysis of the eye, reason why it is an important tool in the case of surgical operations of the eye.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4542666,no,undetermined,0
Effect of automotive headlamp modeling on automotive aerodynamic drag,"Automotive headlamp design, combining science with art, is essential in automotive modeling design. Headlamp modeling design should consider harmonizing with automotive modeling design as well as meeting the national standards of structural design and lighting property. The research aims to present an approach for headlamp modeling design considering automotive aerodynamic drag. The effect of different headlamp modeling design on aerodynamic drag and accordingly three sorts of parameters of headlamp modeling design combining with aesthetic demand are studied. An analytic model of headlamp on automotive aerodynamic drag is established. Numerical simulation of 3-D flow field around the automobile with CFD software is performed. The change of drag coefficient with parameters is analyzed. Finally the design rules of headlamp aerodynamic modeling optimization are concluded. The research effort will enrich the design means of automotive modeling and automotive headlamp, so as to improve the technical property.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4730637,no,undetermined,0
Effect of resonance-mode order on mass-sensing resolution of microcantilever sensors,"The paper reports a systematic study of the resonance-mode effect on mass-sensing performance of resonant microcantilevers by both analytic and experimental methods. Firstly the research reveals that the in-air mass-sensing resolution of resonant microcantilevers is dominated by air-drag loss limited Q-factor. Using theoretic analysis and software simulation, we conclude that a higher order mode is with the resolution superior to a lower order mode and a torsion-mode is generally better than a flexure-mode. 4 types of cantilevers resonating in 4 practically realizable resonance-modes, i.e. the 1st, 2nd flexural modes and the 1st, 2nd torsional modes, are designed with identical length, thickness and the main-stem width. Micromachining techniques are used to fabricate the 4 silicon cantilevers with the actuating and signal-readout elements integrated. Biotin-avidin detecting experiment is performed, with the results well verifying the analytic conclusion for sensor optimal design guidance.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4716505,no,undetermined,0
EIGERä‹¢ development and application to an IR frequency-selective surface,"As presented in (Crruz-Cabera et al., 2008), our experimental counterparts have recently obtained exciting results for a frequency-selective surface (FSS) operating in the mid-wave to long-wave infrared frequency range. While that paper emphasizes fabrication and experimental results, it also includes a numerical validation check based on the EIGER electromagnetics simulation tool (the comparison shows favorable agreement). EIGER is a general purpose frequency-domain integral equation code that supports a variety of Greenpsilas functions (GFs), including 2D and 3D free space GFs, symmetry-plane GFs, periodic GFs, and layered media GFs. While the choice of integral equations as a modeling tool may first appear to be the most complex choice, the strength of this method lies in the fact that code generality is realized on the development of the corresponding Greenpsilas functions. In other words, the capability of integral-equation-based code to handle a wide variety of problems is obtained by incorporating more Greenpsilas functions into the software and furthermore, to complement the generality, code speedup can be obtained by taking advantage of the amenability of GFs to analytic techniques. For example, applications of EIGER at Sandia have addressed EMC and EMI problems including thin-slot coupling, periodic diffraction gratings for a petawatt laser, photonic band-gap structures, and electro- and magnetostatic problems for pulsed power and micromachine designs.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4619723,no,undetermined,0
Enabling Streaming Remoting on Embedded Dual-Core Processors,"Dual-core processors (and, to an extent, multicore processors) have been adopted in recent years to provide platforms that satisfy the performance requirements of popular multimedia applications. This architecture comprises groups of processing units connected by various interprocess communication mechanisms such as shared memory, memory mapping interrupts, mailboxes, and channel-based protocols. The associated challenges include how to provide programming models and environments for developing streaming applications for such platforms. In this paper, we present middleware called streaming RPC for supporting a streaming-function remoting mechanism on asymmetric dual-core architectures. This middleware has been implemented both on an experimental platform known as the PAC dual-core platform and in TI OMAP dual-core environments. We also present an analytic model of streaming equations to optimize the internal handshaking for our proposed streaming RPC. The usage and efficiency of the proposed methodology are demonstrated in a JPEG decoder, MP3 decoder, and QCIF H.264 decoder. The experimental results show that our approach improves the performance of the decoders of JPEG, MP3, and H.264 by 24%, 38%, and 32% on PAC, respectively. The communication load of internal handshaking has also been reduced compared to the naive use of RPC over embedded dual-core systems. The experiments also show that the performance improvement can also be achieved on OMAP dual-core platforms.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4625830,no,undetermined,0
Estimating the energy cost of communication on portable wireless devices,"Software applications running on portable wireless devices communicate with the rest of the network over a wireless link. In these portable devices, the communication cost is a large fraction of the total energy consumption. The amount of energy consumed by the communication component of a portable device mostly depends on different parameters such as packet size and packet rate (or, bit rate). In this paper, we present the results of our investigation of the impacts of these communication parameters on energy consumption. First we build a simple analytic model to estimate the energy consumption due to receiving and transmitting data packets, and then we validate our model by conducting experiments. Results show that the analytical model is effective and gives accurate results. By varying data packet lengths, a communication device consumes different levels of energy to achieve the same data rate. When the packet size is very small compared to the maximum transmission unit (MTU), the device consumes more energy. However, large packets do not necessarily save energy. They rather add some other types of overheads, such as segmentation, recombination, and packet drop. Thus, for a given set of network parameters, an application can choose a suitable data packet length to minimize energy consumption. We also present the impact of data rate and packet delays on energy consumption. These results help us in understanding the energy consumption behavior of a communication device. They also facilitate us in optimizing the energy cost while designing a wireless application.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812890,no,undetermined,0
Expanding the Criteria for Evaluating Socio-Technical Software,"This paper compares two evaluation criterion frameworks for sociotechnical software. Research on the technology acceptance model (TAM) confirms that perceived usefulness and perceived ease of use are relevant criteria for users evaluating organizational software. However, information technology has changed considerably since TAM's 1989 inception, so an upgraded evaluation framework may apply. The web of system performance (WOSP) model suggests eight evaluation criteria, based on a systems theory definition of performance. This paper compares WOSP and TAM criterion frameworks in a performance evaluation experiment using the analytic hierarchy process method. Subjects who used both TAM and WOSP criteria preferred the WOSP criteria, were more satisfied with its decision outcomes, and found the WOSP evaluation more accurate and complete. As sociotechnical software becomes more complex, users may need (or prefer) more comprehensive evaluation criterion frameworks.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544888,no,undetermined,0
Experimental validation of grid algorithms: A comparison of methodologies,"The increasing complexity of available infrastructures with specific features (caches, hyper- threading, dual core, etc.) or with complex architectures (hierarchical, parallel, distributed, etc.) makes models either extremely difficult to build or intractable. Hence, it raises the question: how to validate algorithms if a realistic analytic analysis is not possible any longer? As for some other sciences (physics, chemistry, biology, etc.), the answer partly falls in experimental validation. Nevertheless, experiment in computer science is a difficult subject that opens many questions: what an experiment is able to validate? What is a ""good experiments""? How to build an experimental environment that allows for ""good experiments""? etc. In this paper we will provide some hints on this subject and show how some tools can help in performing ""good experiments"". More precisely we will focus on three main experimental methodologies, namely real-scale experiments (with an emphasis on PlanetLab and Grid'5000), Emulation (with an emphasis on Wrekavoc: http://wrekavoc.gforge.inria.fr) and simulation (with an emphasis on SimGRID and Grid-Sim). We will provide a comparison of these tools and methodologies from a quantitative but also qualitative point of view.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536210,no,undetermined,0
Global Exponential Stability of Fuzzy Cellular Neural Networks with Mixed Delays,"In this paper, a class of fuzzy cellular neural networks with mixed delays is studied. By using the fixed point theorem, M-matrix theory and some analytic techniques, sufficient conditions for the existence and global exponential stability of the unique equilibrium point are obtained. For illustration, an example is given to show the effectiveness of the obtained results.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722756,no,undetermined,0
Knowledge management performance evaluation based on ANP,"Knowledge management is considered as a critical process in organizations. With the scope of this paper, 4 first-level indices and 16 second-level indices were chosen to construct a knowledge management evaluation index system on the basis of research both domestic and international on knowledge management. A new appraisal method - analytic network process (ANP) has been adopted to model a multi-criteria knowledge management performance evaluation feedback system. Theoretical foundations and application processes of ANP were discussed and dependence and feedback among indices were analyzed. The result of ANP model for the index system was obtained with super decisions software solution. ANP can solve problems with dependent indices effectively rather than analytic hierarchy process (AHP).",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620414,no,undetermined,0
Hermitian and Skew-Hermitian splitting methods for streamline upwind Petrov-Galerkin approximations of a grid-aligned flow problem,"In this paper, we study the convergence of two-step iterative methods based on Hermitian and skew-Hermitian splitting of the coefficient matrix for solving the linear systems obtained from the bilinear finite element discretisation of a model two-dimensional convection-diffusion problem. Analytic expressions for the optimal convergence factors are derived. The inexact and preconditioned versions of the methods have been analyzed via an extensive set of computational experiments.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781713,no,undetermined,0
iFAO: Spatial Decision Support Services for Facility Network Transformation,"Facility network transformation (FNT) is a strategic approach involving assessing and optimizing the industrial facility networks such as new site selection, demand forecasting, performance evaluation in banking, retailing, etc. In practice, FNT requirements are often diverse, dynamic and industry specific, it's often difficult to implement a generic FNT service fully integrated with legacy systems. The heterogeneity of spatial information further calls for a loosely coupled architecture. An innovative spatial decision support system, iFAO (intelligent facility network analytics and optimization), is therefore developed based on service oriented architecture for FNT problems. In this paper, key FNT service patterns are identified and modeled to develop an industrial independent solution, and an SOA-based framework for iFAO is proposed correspondingly. Implementation of iFAO services is presented with a model-driven approach. With a real case in banking, it's illustrated how the SOA based iFAO services are integrated to solve the real industrial problems, especially for quick decisions on business strategy in the competitive and ever-changing marketplaces.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578554,no,undetermined,0
Implementing a quantitative-based methodology for project risk assessment DSS,"Project risk assessment is a critical activity adopted in project risk management process to prevent risks and to enhance the success rate of projects. But so far it is a big challenge for project managers and experts to combine their expertise with intelligent technology to evaluate project risks due to insufficient risk related data. Based on this, a novel attempt to integrate analytic hierarchy process (AHP) and support vector regression (SVR) is proposed to build the assessment decision model. A prototype system called project risk assessment decision support system (PRADSS) which consists of risk index system, AHP evaluation model, intelligent evaluation system, knowledge base and man-machine interaction is brought forward to reduce negative risk factors and assist in decision making. Software project risk assessment is conducted to show the efficiency and feasibility of the prototype system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4605452,no,undetermined,0
Improving maritime anomaly detection and situation awareness through interactive visualization,"Surveillance of large land, air or sea areas with a multitude of sensor and sensor types typically generates huge amounts of data. Human operators trying to establish individual or collective maritime situation awareness are often overloaded by this information. In order to help them cope with this information overload, we have developed a combined methodology of data visualization, interaction and mining techniques that allows filtering out anomalous vessels, by building a model over normal behavior from which the user can detect deviations. The methodology includes a set of interactive visual representations that support the insertion of the userpsilas knowledge and experience in the creation, validation and continuous update of the normal model. Additionally, this paper presents a software prototype that implements the suggested methodology.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4632191,no,undetermined,0
Index Weight Technology in Threat Evaluation Based on Improved Grey Theory,"A scientific evaluation of network's threat degree is an important part of network risk assessment. It's main basis to perform active defense for supervisory systems. And itpsilas an important function of network risk evaluation system. It's necessary to create a credible and general evaluating index model in network threat assessment. Many indices affect the evaluation of network threat and generally they also affect each other. Thus it can not reflect the real situation of network threat with index weights determined by single method. According to the network indicespsila effect, this paper proposed an algorithm which combines grey correlation degree in grey theory with traditional analytic hierarchy process (AHP) to determine the indices. It considered both objective and subjective factors. Experiments showed that this method had a strict mathematical theory basis, a definite meaning and a high practical value. It improved the evaluation methods' credibility of network threat assessment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731939,no,undetermined,0
Information Security Risk Assessment Method Based on CORAS Frame,"This paper first carry out the summary to the information security risk assessment's present situation and the correlation criterion, then introduced in detail to the risk which possibly exists carry out the quantification based on the CORAS frame's information security risk assessment method and using the analytic hierarchy process, finally uses on-line Electronic bank system's example, proved this method to be possible very well suitable in the information security risk assessment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722408,no,undetermined,0
Integrated Evaluation Model for Software Process Modeling Methods,"In order to help developers choose suitable modeling method according to specific modeling environment and requirement for achieving the best modeling effect, it is important to make reasonable assessment for software process modeling methods. An evaluation system for software process modeling methods is presented, and an evaluation model for evaluating modeling methods is established using method that combines uncertain analytic hierarchy process (AHP) with fuzzy integrated evaluation technology. Further, by an example it proves that using the model can evaluate modeling methods soundly, so it has practical value in software project development.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4548290,no,undetermined,0
Intelligent Video Surveillance Networks: Data Protection Challenges,"Video surveillance techniques are evolving from static and passive cameras documenting events to dynamic and preventive networks. Two trends lead this change: the shift towards wireless IP systems and the emergence of video analytics. The former allows for flexible networks, massive customization whereas the later comes to solve the problem of increase network complexity. This evolution brings however new threats for individual freedoms, challenging in particular the application of data protection safeguards. This paper takes the prototype developed by DYVINE project as example for the next generation of video surveillance networks and analyses its potential threats from a data protection standpoint.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529449,no,undetermined,0
Introduction to the smoothed particle hydrodynamics method in electromagnetics,"The smoothed particle hydrodynamics (SPH) method is a mesh-free method that is characterized by an easy adaptability and an ability to track moving particles. In this paper, the fundamentals of SPH and its corrective smoothed particle method (CSPM) are introduced in detail and those two are compared in terms of consistency. The CSPM formulation for electromagnetics in time domain (SPEM) is presented and an investigation of the effect of particle distribution on the accuracy of SPEM is carried out. The results are compared to those of a finite difference time domain solution and analytic one. It is found that SPEM has a high potential in computational electromagnetics, and that consistency restoring is required for irregular particle distributions.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4559942,no,undetermined,0
The Mouse HRV Analysis Based on Radius of FOD,"Environment change induces a great influence on mouse heartbeat. The Poincare plot and the first-order differential (FOD) plot are usually introduced to analyze heart rate variability (HRV), while both of them are qualitative methods to the research object. A quantitative approach is suggested to analyze abnormal HRV based on the FOD radius in this paper. Based on FOD sequence between the RR intervals, we get the FOD plot. FOD radius is estimated which divide FOD plot into two different regions: normal and abnormal region. So we get the information containing classification of RR intervals. Our research results show that, Concentrated Ambient Particles (CAPs) have significant adverse health effects on both healthy and unhealthy mice, and it's much more sensitive to unhealthy mice.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677088,no,undetermined,0
Decision Support for User Interface Design: Usability Diagnosis by Time Analysis of the User Activity,"This paper presents a methodology for setting up a decision support system for user interface design (DSUID). We first motivate the role and contributions of DSUID and then demonstrate its implementation in the case of usability diagnosis of Web pages, based on time analysis of clickstream data. The resulting DSUID diagnostic reports enable website managers to learn about possible sources of usability barriers. The proposed DSUID analytic method is based on the integration of stochastic Bayesian and Markov models with models for estimating and analyzing the visitors' mental activities during their interaction with a Website. Based on this approach, a seven-layer model for data analysis is suggested and an example of a log analyzer that implements this model is presented. We demonstrate the approach with an example of a Bayesian network applied to clickstream data and conclude with general observations on the generic role of DSUID and the implementation framework we propose.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591677,no,undetermined,0
Comprehensive Assessment Model of Network Vulnerability Based upon Refined Mealy Automata,"To evaluate the vulnerability of a network of hosts comprehensively, a security analyst must take into account the effects of interactions between these vulnerabilities. This paper proposes Comprehensive Assessment Model (CAM) to illustrate the multi-stage attack action and evaluate network vulnerability in context. We make use of first-order predicate calculus to formally refine the Mealy Automata based CAM model. And then we in detail demonstrate the application of this model, in which we have applied the Analytic Hierarchy Process (AHP) to the measurement of vulnerability degree and conclude the security solution.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722414,no,undetermined,0
Comparison with Parametric Optimization in Credit Card Fraud Detection,"We apply five classification methods, neural nets (NN), Bayesian nets (BN), naive Bayes (NB), artificial immune systems (AIS) and decision trees (DT), to credit card fraud detection. For a fair comparison, we fine adjust the parameters for each method either through exhaustive search, or through genetic algorithm (GA). Furthermore, we compare these classification methods in two training modes: a cost sensitive training mode where different costs for false positives and false negatives are considered in the training phase; and a plain training mode. The exploration of possible cost-sensitive metaheuristics to be applied is not in the scope of this work and all executions are run using Weka, a publicly available software. Although NN is claimed to be widely used in the market today, the evaluated implementation of NN in plain training leads to quite poor results. Our experiments are consistent with the early result of Maes et al. (2002) which concludes that BN is better than NN. Cost sensitive training substantially improves the performance of all classification methods apart from NB and, independently of the training mode, DT and AIS with, optimized parameters, are the best methods in our experiments.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724987,no,undetermined,0
An AHP-Based Evaluation Index System of Coding Standards,"Compliance with coding standards is becoming an important evaluating index of a software engineerí‰ŒËs teamwork capability and international cooperation capability. Unreadable and non-reusable program code, high defect density, and expensive maintenance cost are changing the mindset of managers working in software companies. The best place to solve this problem should be in an educational environment such as college where students begin to write their introductory programs. In order to propose an effective and quantitative evaluation index system, and to gradually improve studentsí‰ŒË ability to comply with coding standards, a simplified version of coding standards was presented. By utilizing questionnaire surveys and analytical hierarchy process (AHP), weights of the indices in the evaluation system were determined. This evaluation index system was proved quite practical in the teaching process of PSP course.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722978,no,undetermined,0
An Enhanced Autonomic Multiclass Multithreaded Web Server: A Performance Model Approach,"Computer systems are becoming more complex continuously and managing them is quite challenging even to the most skilled IT professionals. A promising way to address obstacles to efficient management and re-organization of computer systems relies on the automation of these tasks. An approach has been introduced that in which analytic performance models are combined with combinatorial search techniques to design a QoS controller that runs periodically to determine the best possible configuration for the system given its workload. The results of applying this approach have been shown to QoS control of many systems. In this paper we considered a multithreaded web server with multiple classes of request and proposed that the QoS controller re-configure it in each controller interval distinguishing different workload classes. We illustrate that this mechanism enhances the self-optimizing and self-managing capability of the system, and improves global QoS value of it using simulated multithreaded web server. In addition, we show that applying this mechanism make QoS controller to become fairer and more flexible.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591735,no,undetermined,0
An Improved Particle Filtering Algorithm Based on Consensus Fusion Sampling,"Particle filtering is briefly introduced first. Because the depletion of particle diversity resulted from re-sampling causes the decline of filtering precision, an improved particle filtering algorithm based on consensus fusion sampling is proposed. After the re-sampling process, the new algorithm extracts candidate particles based on Markov Chain Monte Carlo (MCMC) principle and combines the re-sampling particles to construct a candidate particle set. Then according to the principle of analytic hierarchy process (AHP), consensus matrix is established, and the complementary and redundancy information of the candidate particles is fully used. Finally, the optimal selection of particles is realized by calculating consensus matrix. Simulation results show the method can effectively reduce the phenomenon of particle impoverishment and improve the state estimation precision.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723157,no,undetermined,0
An Ontology-Driven Framework for Deploying JADE Agent Systems,"Multi-agent systems have proven to be a powerful technology for building distributed applications. However, the process of designing, configuring and deploying agent-based applications is still primarily a manual one. There is a need for mechanisms and tools to help automate the many development steps required when building these applications. Using the Semantic Web ontology language OWL and the JADE platform we have developed a number of models and software tools that provide an end-to-end solution for designing and deploying agent-based systems. This solution supports the construction of detailed models of agent behavior and the automatic deployment of agents from those models. We illustrate its use in the construction of a multi-agent system that supports the configuration, deployment, and evaluation of analytic methods for detecting disease outbreaks.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740686,no,undetermined,0
An Open Community Approach to Emergency Information Services during a Disaster,"This paper discusses the application of open community for the emergency information services during a disaster with the lessons learned from the 2008 Sichuan earthquake. An open community approach enables open participation for information collection and sharing. The suggested ldquodual modelrdquo allows a two-way-traffic from citizens to officials and from officials to citizens. The latest Web services based mashup technology can play a critical role in implementing an OC with the dual model in a cellphone network. This paper further addresses the issues of information sharing, security, and service quality for an open community.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732299,no,undetermined,0
Analysis and design of novel directional ultra wide-band antennas,"Ultra wide-band is one of the hottest issues in the current researches on antennas. The circular monopole antenna is a kind of ultra wide-band antennas with simple structure. Firstly a novel antenna was researched. Four circular disc monopoles were placed across each other. They were considered as the feedings, and the reflection plane was planar. It could achieve dual polarization characteristics. On this basis, further study was applied to two other novel antennas. The first was changing the planar reflector to parabolic one, and the purpose was to realize wide beam, high gain and directed radiation characteristic. The other was changing the circular disc shape, and the purpose was to realize wider lower frequency bandwidth. The novel antennas were theoretic analyzed, and the S- parameter and radiation patterns of the antennas were discussed by electromagnetism simulation software. The analytic results verify that the characteristic of the traditional monopole antenna is effective improved by the novel antennas. Dual-polarization, high gain, wide beam and directed radiation characteristics of the antenna are realized. The novel antennas are suitable for the satellite and mobile communication systems with good prospects.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4540785,no,undetermined,0
Analysis of the Virtual Enterprise Partner Selection Based on Multi-agent System,"Todaypsilas world, virtual enterprise is regarded as the most competitive management model of enterprises that face the resource of the globe. How to select the best partner enterprise is a key section of the whole process of virtual enterprise creation. This paper proposed an instructor of remote manufacturing system supporting dynamic alliance for virtual enterprises with the multi-agent technology to solve the problem of partner enterprise selections.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722105,no,undetermined,0
Analytic performance models for bounded queueing systems,"Pipelined computing applications often have their performance modeled using queueing techniques. While networks with infinite capacity queues have well understood properties, networks with finite capacity queues and blocking between servers have resisted closed-form solutions and are typically analyzed with approximate solutions. It is this latter case that more closely represents the circumstances present for pipelined computation. In this paper, we extend an existing approximate solution technique and, more importantly, provide guidance as to when the approximate solutions work well and when they fail.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536123,no,undetermined,0
Analytic Programming Powered by Distributed Self-Organizing Migrating Algorithm Application,"This paper presents an idea of new algorithm combining advantages of evolutionary algorithm and simple distributed computing to perform tasks which required many re-runs of the same program. Computing time is shorted due to elementary distribution within a number of common computers via the Internet. Progressive .NET framework technology allowing this algorithm to run effectively and examples of possible usage are also described. The algorithm deals with a problem of synthesis of the artificial neural networks using the evolutional scanning method. The basic task to be solved is to create a symbolic regression algorithm on principles of analytic programming, which will be capable of performing a convenient neural network synthesis. The main motivation here is the computerization of such synthesis and discovering so far unknown solutions.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4557842,no,undetermined,0
ANSIGäóîAn analytic signature for permutation-invariant two-dimensional shape representation,"Many applications require a computer representation of 2D shape, usually described by a set of 2D points. The challenge of this representation is that it must not only capture the characteristics of the shape but also be invariant to relevant transformations. Invariance to geometric transformations, such as translation, rotation and scale, has received attention in the past, usually under the assumption that the points are previously labeled, i.e., that the shape is characterized by an ordered set of landmarks. However, in many practical scenarios the landmarks are obtained from an automatic process, e.g., edge/corner detection, thus without natural ordering. In this paper, we represent 2D shapes in a way that is invariant to the permutation of the landmarks. Within our framework, a shape is mapped to an analytic function on the complex plane, leading to what we call its analytic signature (ANSIG). We show that different shapes lead to different ANSIGs but that shapes that differ by a permutation of the landmarks lead to the same ANSIG, i.e., that our representation is a maximal invariant with respect to the permutation group. To store an ANSIG, it suffices to sample it along a closed contour in the complex plane. We further show how easy it is to factor out geometric transformations when comparing shapes using the ANSIG representation. We illustrate the ANSIG capabilities in shape-based image classification.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4587612,no,undetermined,0
Application of analytic network process in knowledge management performance evaluation,"Knowledge management (KM) has become an effective strategic tool to improve the organizational competition capacity in the age of knowledge economy. Knowledge management performance evaluation is an important way to know the knowledge management level of an enterprise. In this study, a knowledge management performance evaluation indices system was established based on knowledge management theory and analytic network process (ANP) performance evaluation methodology. Theoretical foundations and application process of ANP were discussed; dependence and feedback among indices were analyzed. Super Decisions software was used to obtain the result of ANP model for the indices system. The case study in this paper demonstrated that the ANP method can solve problems with dependent indices effectively.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4686511,no,undetermined,0
Application of Schwarz-Christoffel Mapping to Permanent-Magnet Linear Motor Analysis,"Several well-known analytical techniques exist for the force profile analysis of permanent-magnet linear synchronous motors. These techniques, however, make significant simplifications in order to obtain the magnetic field distribution in the air gap. From the field distribution, the force profile can be found. These widely used techniques provide a reasonable approximation for force profile analysis, but fail to give really accurate results in the sense of the exact shape of the force profile caused by effects that due to simplification are not fully included. To obtain the exact shape for the force profile in these cases, the computationally expensive finite-element method (FEM) is often applied. In this paper, an elegant semianalytical approach is presented to acquire the force profile. First, the magnetic field distribution in the air gap is determined by means of Schwarz-Christoffel (SC) mapping. The SC mapping allows a slotted structure of the machine to be mapped to a geometrically simpler domain for which analytic solutions are available. Subsequently, the field solution in the slotted structure can be determined by applying the mapping function to the field distribution in the simplified domain. From the resulting field distribution, the force profile is calculated by means of the Maxwell stress tensor. The results are compared with those from the commonly used equivalent magnetic circuit modeling and 2-D FEM software to demonstrate the accuracy which can be reached by application of the SC method.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4455703,no,undetermined,0
Applied visual analytics for economic decision-making,"This paper introduces the application of visual analytics techniques as a novel approach for improving economic decision making. Particularly, we focus on two known problems where subjectspsila behavior consistently deviates from the optimal, the Winnerpsilas and Loserpsilas Curse. According to economists, subjects fail to recognize the profit-maximizing decision strategy in both the Winnerpsilas and Loserpsilas curse because they are unable to properly consider all the available information. As such, we have created a visual analytics tool to aid subjects in decision making under the Acquiring a Company framework common in many economic experiments. We demonstrate the added value of visual analytics in the decision making process through a series of user studies comparing standard visualization methods with interactive visual analytics techniques. Our work presents not only a basis for development and evaluation of economic visual analytic research, but also empirical evidence demonstrating the added value of applying visual analytics to general decision making tasks.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677363,no,undetermined,0
Approximate Joins for Data-Centric XML,"In data integration applications, a join matches elements that are common to two data sources. Often, however, elements are represented slightly different in each source, so an approximate join must be used. For XML data, most approximate join strategies are based on some ordered tree matching technique. But in data-centric XML the order is irrelevant: two elements should match even if their subelement order varies. In this paper we give a solution for the approximate join of unordered trees. Our solution is based on windowed pq-grams. We develop an efficient technique to systematically generate windowed pq-grams in a three-step process: sorting the unordered tree, extending the sorted tree with dummy nodes, and computing the windowed pq-grams on the extended tree. The windowed pq-gram distance between two sorted trees approximates the tree edit distance between the respective unordered trees. The approximate join algorithm based on windowed pq-grams is implemented as an equality join on strings which avoids the costly computation of the distance between every pair of input trees. Our experiments with synthetic and real world data confirm the analytic results and suggest that our technique is both useful and scalable.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497490,no,undetermined,0
Asset Reliability Modeling and Simulation,"Traditionally in manufacturing discrete event simulation, delay occurrence and duration are represented by static distributions. For many problems and industries, this assumption may be appropriate, rendering them essentially independent. The manufacturing of soft, disposable consumer products, however, may involve the assembly of multi-component products with flexible materials that pose significant challenges to process reliability and thus make the independence assumption invalid. In fact, these non-linear interactions between coupled-delays can result in substantial financial opportunities from seemingly minor contributors. We describe an asset reliability modeling and simulation (ARMS) framework developed at Kimberly-Clark that uses discrete event simulation and dynamic reliability modeling of assetsí‰ŒË process event databases. Models are essentially built í‰ŒËon the flyí‰ŒË and survival simulations are validated against asset process history. Analytics and reports provide a means for identifying the biggest, overall improvement opportunities in performance metrics including production, delays, uptime, and waste.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736424,no,undetermined,0
Business 2.0: A novel model for delivery of business services,"Web 2.0, regardless of the exact definition, has proven to bring about significant changes to the way the Internet was used. Evident by key innovations such as Wikipedia, FaceBook, YouTube, and Blog sites, these community-based Website in which contents are generated and consumed by the same group of users are changing the way businesses operate. Advertisements are no longer dasiaforcedpsila upon the viewers but are instead dasiaintelligentlypsila targeted based on the contents of interest. In this paper, we investigate the concept of Web 2.0 in the context of business entities. We asked if Web 2.0 concepts could potentially lead to a change of paradigm or the way businesses operate today. We conclude with a discussion of a Web 2.0 application we recently developed that we think is an indication that businesses will ultimately be affected by these community-based technologies; thus bringing about Business 2.0 - a paradigm for businesses to cooperate with one another to deliver improved products and services to their own customers.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4598499,no,undetermined,0
Cell phone mini challenge award: Social network accuracyäóî exploring temporal communication in mobile call graphs,"In the mobile call mini challenge of VAST 2008 contest, we explored the temporal communication patterns of Catalano/Vidro social network which is reflected in the mobile call data. We focus on detecting the hierarchy of the social network and try to get the important actors in it. We present our tools and methods in this summary. By using the visual analytic approaches, we can find out not only the temporal communication patterns in the social network but also the hierarchy of it.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677389,no,undetermined,0
ChargeView: An integrated tool for implementing chargeback in IT systems,"Most organizations are becoming increasingly reliant on IT product and services to manage their daily operations. The total cost of ownership (TCO), which includes the hardware and software purchase cost, management cost, etc., has significantly increased and forms one of the major portions of the total expenditure of the company. CIOs have been struggling to justify the increased costs and at the same time fulfill the IT needs of their organizations. For businesses to be successful, these costs need to be carefully accounted and attributed to specific processes or user groups/departments responsible for the consumption of IT resources. This process is called IT chargeback and although desirable, is hard to implement because of the increased consolidation of IT resources via technologies like virtualization. Current IT chargeback methods are either too complex or too adhoc, and often a times lead to unnecessary tensions between IT and business departments and fail to achieve the goal for which chargeback was implemented. This paper presents a new tool called ChargeView that automates the process of IT costing and chargeback. First, it provides a flexible hierarchical framework that encapsulates the cost of IT operations at different level of granularity. Second, it provides an easy way to account for different kind of hardware and management costs. Third, it permits implementation of multiple chargeback policies that fit the organization goals and establishes relationship between the cost and the usage by different users and departments within an organization. Finally, its advanced analytics functions can keep track of usage and cost trends, measure unused resources and aid in determining service pricing. We discuss the prototype implementation of ChargeView and show how it has been used for managing complex systems and storage networks.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4575157,no,undetermined,0
Combining the Power of Taverna and caGrid: Scientific Workflows that Enable Web-Scale Collaboration,"Service-oriented architecture represents a promising approach to integrating data and software across different institutional and disciplinary sources, thus facilitating Web-scale collaboration while avoiding the need to convert different data and software to common formats. The US National Cancer Institute's Biomedical Information Grid program seeks to create both a service-oriented infrastructure (caGrid) and a suite of data and analytic services. Workflow tools in caGrid facilitate both the use and creation of services by accelerating service discovery, composition, and orchestration tasks. The authors present caGrid's workflow requirements and explain how they met these requirements by adopting and extending the Taverna system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670120,no,undetermined,0
Keynote address Practical applications of visual analytics: On the cusp of widespread adoption,"Summary form only given. As practitioners and educators in the field of Visual Analytics Science and Technology, youpsilave seen the power of visual analytics for scientific and technical applications including Homeland Security. But visual analytics is spreading to the general business population solving unexpected problems and challenges. In this talk, Tableau Software CEO Christian Chabot will highlight the areas of opportunity for visual analytics and demonstrate real examples of practical problems being solved by visual analytics. Hepsilall share his vision for the future of this industry - how everyday people can and are using visual analytics to solve some of businesspsilas and societypsilas most challenging issues. Hepsilall also identify whatpsilas needed to bring visual analytics to the forefront of main-stream data analysis and how the industry is helping to deliver on those needs.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677347,no,undetermined,0
Large deviations for constrained pattern matching,"In the constrained pattern matching one searches for a given pattern in a constrained sequence, which finds applications in communication, magnetic recording, and biology. We concentrate on the so-called (d, k) constrained binary sequences in which any run of zeros must be of length at least d and at most k, where 0 les d Lt k. In our previous paper [2] we established the central limit theorem (CLT) for the number of occurrences of a given pattern in such sequences. Here, we present precise large deviations results, often used in diverse applications. In particular, we apply our results to detect under- and over-represented patterns in neuronal data (spike trains), which satisfy structural constraints that match the framework of (d, k) binary sequences. Among others, we obtain justifiably accurate statistical inferences about their biological properties and functions. Throughout, we use techniques of analytic information theory such as combinatorial calculus, generating functions, and complex asymptotics.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595368,no,undetermined,0
Aeroengine Health Assessment Using a Web-Based Grey Analytic Hierarchy Process,"This paper proposed a Web-based evaluation method for aeroengine health assessment which combined the advantages of the analytic hierarchy process (AHP) and a grey clustering method to guarantee the accuracy and objectivity of weight coefficients. The Web-based framework is convenient for data collection, distributed management and processing. After constructing an index system of aeroengine health assessment based on correlated factors involved in aeroengine health status, we confirmed the weight of every index with AHP and gave a general health assessment by means of a grey clustering method. A case study was conducted in a fleet with ten aeroengines to validate the design of the underlying grey analytic hierarchy process (GAHP) model. Running results show the feasibility and reliability of the model, which will be helpful to realize the quantitative analysis in aeroengine health assessment and provide a decision support tool for decision makers.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721774,no,undetermined,0
The Role of Blackboard-Based Reasoning and Visual Analytics in RESIN's Predictive Analysis,"Knowledge gathering and investigative tasks in open environments can be very complex because the problem-solving context is constantly evolving, and the data may be incomplete, unreliable and/or conflicting. This paper significantly extends our previous work on a mixed-initiative agent by making it capable of assisting humans in foraging task analysis using AI blackboard-based reasoning, visualizations and a mix-initiative user interface. The agent is equipped with the ability to adapt its processing to available resources, deadlines and its current problem-solving context.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740675,no,undetermined,0
Statistical Machine Learning in Natural Language Understanding: Object Constraint Language Translator for Business Process,"Natural language is used to represent human thoughts and human actions. Business rules described by natural language are very hard for machine to understand. In order to let machine know the business rules, parts of business process, we need to translate them into a language which machine can understand. Object constraint language is one of those languages. In this paper we present a statistical machine learning method to understand the natural business rules and then translate them into object constraint language. Subsequently a translation algorithm for business process modeling is also provided. A real case, air cargo load planning process is proposed to illustrate the efficiency and effective of the method and the algorithm. The result has shown that this method and algorithm enrich business process modeling technology and enhance the efficiency of software developers in business process modeling.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4810674,no,undetermined,0
Study of Index Weight in Network Threat Evaluation Based on Improved Grey Theory,"It's necessary to create a credible and general evaluating index model in network threat assessment. According to the network indices' effect, this paper proposed an algorithm which combines grey correlation degree in grey theory with traditional analytic hierarchy process (AHP) to determine the indices. It considered both objective and subjective factors. Experiments showed that this method had a strict mathematical theory basis, a definite meaning and a high practical value. It improved the evaluation methods' credibility of network threat assessment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756724,no,undetermined,0
Study of the Method of Evaluating Color Digital Image,"This paper makes a lot of investigation, analysis and research about the image quality control and AHP modeling theory. Based on the theory and principle of color reproduction in color image reproductive process, a index system evaluating the quality of color image reproduction is presented. Referring to the national and industry standards, the relative evaluation standards of various indicators is given. Using the analytic hierarchy process in mathematics, it establishes a mathematics modeling of synthetical evaluation of color image reproductive quality, and designs a set of mathematics methods of synthetical evaluating image processing and color reproduction. Finally,using VB to design panels and programmes, it basically realizes the objective of simulated evaluating color images reproduction quality in computer.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723237,no,undetermined,0
Study on Selection Model of CPC System Partner in Execution and Education,"Collaborative Product Commerce (CPC) is the outcome of the latest development of E-commerce. This paper addresses a CPC partner (in execution and education) selection model by applying AHP (analytic hierarchy process) method and fuzzy system theory. This model is practical, effective and useful for enterprises when they select CPC partners, and it enhances the chance of success in the application of CPC system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722960,no,undetermined,0
Teaching Model of Coding Standards Based on Evaluation Index System and Evaluating Platform,"In order to increase students' capacity of complying with coding standard and strengthen their competitiveness in software industry, an AHP-based evaluation index system and Web-based evaluating platform were established. A student-centered teaching model was discussed, in which both teachers and evaluating platform support the students. After implementing the preliminary teaching model in one academic year, some problems were found, so that the improved teaching model was proposed.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722131,no,undetermined,0
The Evaluation of B2C E-Commerce Web Sites Based on Fuzzy AHP,"E-commerce is one of the most important developments in Internet application. To be successful in the e-commerce marketplace organizations will need to provide high quality web sites that attract and retain users. Hence, evaluation methods for the effectiveness of e-commerce Web sites are a critical issue in both practice and research. The evaluation process involves human subjectivity and it is a multiple-criteria decision making (MCDM) problem in the presence of many quantitative and qualitative attributes. This paper presents fuzzy analytic hierarchy process model to measure the e-commerce web sitesí‰ŒË performance. The study has investigated three web sites with the proposed method.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731740,no,undetermined,0
The kinematics analysis on single cross universal joint,"A new method of space analytic geometry is presented, by which the kinematics characteristics of single cross universal joint is analyzed. Compared with traditional method such as descriptive geometry, three element complex and multi-body system dynamics, space analytic geometry has characteristics of brachylogy and understandability. It is a more convenient way to analyze the kinematics characteristics of single cross universal joint and suitable for project design. The Matlab software is used for simulation to analyze kinematics characteristics of rotate speed and angular acceleration of cross universal joint driven fork shaft.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677771,no,undetermined,0
The Research on Affecting Factors of E-learning Training Effect,"This research found the key affecting factors on e-learning training effect with the use of survey method and analytic hierarchy process (AHP). Through literature research and survey method, we got the factors that affect e-learning training effect from three aspects: organizational factors, E-learning training factors and personal characteristics factors. Then we designed a questionnaire on importance of these factors and took trainees of XX Company as the sample. We analysis the data with AHP software YAAHP0.4.1 and finally got the key factors on E-learning training effects. Managers should pay attention to these factors and control them effectively in order to improve E-learning training effects. So this research can provide guidance and reference value for government department and enterprises in their E-learning training process.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722895,no,undetermined,0
The Research on Building Enterprise Knowledge Management Performance Evaluating Indicator System,"The enterprise's knowledge management performance evaluating indicator system is an important basis for the evaluation of enterprise knowledge management performance. Only if we establish a set of scientific and feasible enterprise knowledge management performance evaluating indicator system, the enterprise's knowledge management level could be reflected, and the gap and shortage could be found and corrected, so that the enterprise could adapt to the complex and changeable circumstances in the age of knowledge-based economy. The thesis proposes the principle for building enterprise knowledge management performance evaluating indicator system. Taking into account of the practical condition of the enterprise, the thesis select the enterprise knowledge management performance evaluating indicator set by using Delphi method. Then the evaluating indicator system hierarchy chart is established by AHP method. The thesis quotes Saaty 1<sub>~</sub>9 scaling procedure and average random coincidence indicator RI value, proceeds the quantifying and weighting calculation for the evaluating indicator, and establishes enterprise knowledge management performance evaluating indicator system model by coincidence test and other integrated methods for judgment matrix. Through the simulated experiment, the scientificalness and feasibility of the enterprise knowledge management performance evaluating indicator system model is proved.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756949,no,undetermined,0
The technical risk assessment for electronic commerce,"A technical risk assessment method for electronic commerce is proposed. On the basis of the analysis of the technical risk of electronic commerce, the method, which combines fuzzy comprehensive judgment method and AHP method, is applied to evaluate risks by qualitative analysis and calculation. The study of the case shows that the method can be easily used to the technical risk assessment for electronic commerce and the results are in accord with the reality.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620646,no,undetermined,0
Linking Business Intelligence into Your Business,IT departments are under pressure to serve their enterprises by professionalizing their business intelligence (BI) operation. Companies can only be effective when their systematic and structured approach to BI is linked into the business itself.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4747653,no,undetermined,0
Towards Analysing Information Management Requirements in New Zealand Genetic Services,"The development of genetic services within healthcare systems is a global phenomenon that raises challenges for managing genetic information. This paper describes an ongoing qualitative study to collect stakeholder perspectives of New Zealand (NZ) genetic services concerning genetic information management. We are conducting semi-structured interviews to build an understanding of their experiences, expectations, and concerns. The data analysis takes a general inductive approach with an analytic comparison strategy and evaluation research techniques. This study draws on past social and health science theories, on our experience in the NZ genetics context, and on emerging issues in the domain. The study result will provide deeper insights on how the genetic service system works and where it should go. The project deliverables will include a structured synthesis of stakeholder requirements, NZ genetic information management principles and goals.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724555,no,undetermined,0
Using Web 2.0 applications to deliver innovative services on the internet,"The emergence of Web 2.0 has brought about new Web applications being developed. Represented chiefly by Web applications such as YouTube, MySpace, blogs and Google applications, these community-based technologies are changing the way we use the Internet. One interesting result of these innovations is the extensibility of these applications. For example, YouTubepsilas content can be displayed on other Websites and hence, are popularly dasiaextendedpsila to be displayed on individual blogs and other organization Websites. In this paper, we discussed two applications that were a result of extending Google Earth and Google Maps. These two applications illustrate how new solutions can be quickly built from these extensible applications thus suggesting the future of application development, one that is built upon applications rather than object-oriented components.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4598500,no,undetermined,0
Versatile models of systems using map queueing networks,"Analyzing the performance impact of temporal dependent workloads on hardware and software systems is a challenging task that yet must be addressed to enhance performance of real applications. For instance, existing matrix-analytic queueing models can capture temporal dependence only in systems that can be described by one or two queues, but the capacity planning of real multi-tier architectures requires larger models with arbitrary topology. To address the lack of a proper modeling technique for systems subject to temporal dependent workloads, we introduce a class of closed queueing networks where service times can have non-exponential distribution and accurately approximate temporal dependent features such as short or long range dependence. We describe these service processes using Markovian arrival processes (MAPs), which include the popular Markov-modulated Poisson processes (MMPPs) as special cases. Using a linear programming approach, we obtain for MAP closed networks tight upper and lower bounds for arbitrary performance indexes (e.g., throughput, response time, utilization). Numerical experiments indicate that our bounds achieve a mean accuracy error of 2% and promote our modeling approach for the accurate performance analysis of real multi-tier architectures.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536387,no,undetermined,0
Visual Analysis of a Co-authorship Network and Its Underlying Structure,"An interesting property of network is that the information is not only contained in the entities, but also in the links between them. As the structure of the co-authorship network can greatly influence its function and reflect how the internal information is exchanged. We attempt to get deep insight of the features in a co-authorship network at a university. This is done by the following two steps. First, we will explore the basic statistical properties of the co-authorship network and try to present these properties by different graph drawing techniques. Second, to gain more insight of the co-authorship network, we will use a community detecting algorithm to find the clusters of the network. By filtering the unstable links and detecting the clusters, we find there are many stable links in these clusters and many unstable links take the role of ""bridges""' between these clusters. By labeling these communities with different department names, we can get an overview of the main research fields of the university and their relations.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666472,no,undetermined,0
Visualizing Time-Dependent Data in Multivariate Hierarchic Plots - Design and Evaluation of an Economic Application,"For successfully competing in a modern economy, large amounts of hierarchic time-dependent data need to be analyzed. As an example, one could consider the geographic composition of inflation in the European Union, or the revenue by product (sub) categories of a firm in the last month. Analysts wish to interpret the structure of the data not only at a single point in time, but examine the changes in the data categories through time. The analysts may need to consider additional dimensions to composition and time, such as the growth rate or profit rate. To reflect such analytic requirements, we have developed an interactive visualization of multi-dimensional, structured data taking the time dimension into account. The data are displayed in a three dimensional hierarchic circular or column plot. The time dimension of the data is represented by animation. Our system provides interactive tools for the visual data analysis and variable set-up of the data display. For better orientation in the data space, we have enhanced the visualization with smooth transitions between different data selections in case of 3D hierarchic plots. The techniques presented can be applied to various data domains. A user study using European inflation data has shown the usefulness for effective economic analysis.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4577939,no,undetermined,0
Viz-A-Vis: Toward Visualizing Video through Computer Vision,"In the established procedural model of information visualization, the first operation is to transform raw data into data tables. The transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human, user or programmer. The theme of this paper is that for video, data transforms should be supported by low level computer vision. High level reasoning still resides in the human analyst, while part of the low level perception is handled by the computer. To illustrate this approach, we present Viz-A-Vis, an overhead video capture and access system for activity analysis in natural settings over variable periods of time. Overhead video provides rich opportunities for long-term behavioral and occupancy analysis, but it poses considerable challenges. We present initial steps addressing two challenges. First, overhead video generates overwhelmingly large volumes of video impractical to analyze manually. Second, automatic video analysis remains an open problem for computer vision.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658139,no,undetermined,0
A computational method for the determination of attraction regions,"The region of attraction of nonlinear dynamical system can be considered using an analytical R-function that can be written like an infinite series where each term of the series has the homogeneous form of degree n ŒËŒË 2 this function allows to determine and to come near to the region of attraction of a nonlinear system around the point of equilibrium located in the origin. The analytical function and the sequence of this Taylor polynomials are constructed by a recurrence formula using the coefficients of the power series expansion of f at 0. This paper describes a novel computational method using the Software MATHEMATICA for obtaining a solution to this problem, which was proposed by the Russian mathematician, V. I. Zubov. In order to evaluate the method, two examples are treated in which the exact attraction region is found in analytic closed form. Since the construction procedure requires the solution of a linear partial differential equation, there are many cases for which an exact analytic solution is not possible. In some of these cases, however, it is possible to construct an approximate series solution which is always at least as good approximation of the usual quadratic form of Lyapunov functions. The ""trajectory reversing method"" is presented as a powerful numerical technique for low order systems. Then an analytical procedure based on the same topological approach is developed, and a comparison is made with the classical Zubov method.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5393394,no,undetermined,0
A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets,"We present a new lock-free parallel algorithm for computing betweenness centrality of massive complex networks that achieves better spatial locality compared with previous approaches. Betweenness centrality is a key kernel in analyzing the importance of vertices (or edges) in applications ranging from social networks, to power grids, to the influence of jazz musicians, and is also incorporated into the DARPA HPCS SSCA#2, a benchmark extensively used to evaluate the performance of emerging high-performance computing architectures for graph analytics. We design an optimized implementation of betweenness centrality for the massively multithreaded Cray XMT system with the Thread-storm processor. For a small-world network of 268 million vertices and 2.147 billion edges, the 16-processor XMT system achieves a TEPS rate (an algorithmic performance count for the number of edges traversed per second) of 160 million per second, which corresponds to more than a 2times performance improvement over the previous parallel implementation. We demonstrate the applicability of our implementation to analyze massive real-world datasets by computing approximate betweenness centrality for the large IMDb movie-actor network.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161100,no,undetermined,0
A framework for calculating fundamental DVR performance limits,"New systems have emerged that deliver hundreds of megabits per second of video to the home. This, coupled with deeper penetration of in-home video networking, will create demand for digital video recorder (DVR) performance to scale up dramatically as well. This paper outlines a simple analytic framework that can be used to estimate the performance of any DVR system, using either hard-disk drives or solid-state disk drives, in terms of both megabits per second and number of video streams. The framework also highlights the extent to which the maximum performance is constrained by the disk, by the host hardware, or by the host software.",2009,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814425,no,undetermined,0
SMILE Visualization with Flash Technologies,"The principles of decision-analytic decision support, implemented in GeNIe (Graphical Network Interface) and SMILE (Structural Modeling, Inference, and Learning Engine) can be applied in practical decision support systems (DSSs). GeNIe plays the role of a development environment and SMILE plays the role of a reasoning engine. A decision support system based on SMILE can be equipped with a customized user interface. GeNIe's name and its uncommon capitalization originate from the name Graphical Network Interface, given to the original simple interface to SMILE which is a library of functions for graphical probabilistic and decision-theoretic models. GeNIe only runs under one of the most popular computing platform,: the Windows operating systems, which makes it not easily portable. GeNIe is therefore limited in its graphical representation across multiple system platforms. This paper is composed of two parts. The first part discusses a development environment for building graphical decision-theoretic models, an influence diagram, on a website by using an newly developed engine called ""SMILE"". The second part of the paper discusses the visualization of SMILE decision-theoretic models on a website using Flash technologies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617430,no,undetermined,0
Simulation Analysis for the Nonlinear Seismic Response of Seismically Isolated Continuous Bridge,"According to the behavior of seismically isolated continuous bridges, the bidirectional nonlinear characteristics of lead rubber bearing are taken into account by using two orthogonal nonlinear level spring elements. Based on the FEA software, the analysis models of seismically isolated and non-isolated continuous bridges are established. And the nonlinear seismic response for these analysis models is carried out under the function of the reasonably chosen seismic motion. The analytic results indicate that the natural period of seismically isolated bridge can be prolonged to avoid the principal period of ground and the seismic energy of structure can be efficiently consumed by the hysteretic energy dissipation of lead rubber bearing. So the response of bridge structure can be reduced to make sure most of the components work in the elastic phase and the structure can be well protected.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4757026,no,undetermined,0
Service analytics framework for web-delivered services,"Web-delivered service is an emerging approach for IT service by leveraging the partnership and Web technology to reduce IT service cost and improve delivery efficiency. To make it more effective, business design is also very important besides technology innovation. This paper proposes a three-tier analytical framework to improve business design based on the centralized information from the platform. At business model design tier, a role-based service ecosystem modeling method is presented to contain the duplication and evolvement of participants' functionalities within the same model. Through the model, we can analyze the impact of business model on the business performance. At service analytics tier, we summarize the available data and categorize the operational analysis by roles. At self-transform tier, several potential functionalities are listed to help end client to do business transformation by themselves. Based on such framework, a case study in mini-ERP services is presented to illustrate how the analytics technology can help in business design of Web-delivered service.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4686475,no,undetermined,0
Semi-blind channel estimation schemes based on a cooperative form of the cross relation criterion,"In this paper, two channel estimation schemes are derived for specific cooperative scenarios. Both schemes are based on the cross-relation criterion that has been extensively studied in the (semi-) blind literature. As shown in the paper, in a cooperative system, the channel estimator can be constructed in a natural way either by fractionally-spaced or symbol-spaced samples. We investigate the performance of these two schemes using semi-analytic arguments accompanied by corresponding experimental results.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556290,no,undetermined,0
Maintaining interactivity while exploring massive time series,"The speed of data retrieval qualitatively affects how analysts visually explore and analyze their data. To ensure smooth interactions in massive time series datasets, one needs to address the challenges of computing <i>ad</i> <i>hoc</i> queries, distributing query load, and hiding system latency. In this paper, we present ATLAS, a visualization tool for temporal data that addresses these issues using a combination of high performance database technology, predictive caching, and level of detail management. We demonstrate ATLAS using commodity hardware on a network traffic dataset of more than a billion records.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677357,no,undetermined,0
Methodology of the Correct Plate-Making to Keep Consistence of Tone Reproduction,"The process of offset plate copy is controlled through the correct reproduction at the highest and the lowest tone. We propose that we should control the whole tone, and we need the correct control method. Based on the offset control strap, this paper proposes a new detected method of plate resolution and analytic method of the correct copy range of plate. Using these methods and through the experiment, we analyze the resolution and the correct copy range of three types of offset plates. By working in the correct copy range, we can guarantee the consistence of tone reproduction of different types of plates. This is the precondition for color consistence in color printing and convenient to adjust the press.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723278,no,undetermined,0
Migrant boat mini challenge award: Analysis summary a geo-temporal analysis of the migrant boat dataset,"The SPADAC team used various visual analytics tools and methods to find geo-temporal patterns of migration from a Caribbean island from 2005-2007. In this paper, we describe the tools and methods used in the analysis. These methods included generating temporal variograms, dendrograms, and proportionally weighted migration maps, using tools such as the R statistical software package and Signature Analysttrade. We found that there is a significant positive space-time correlation with the boat encounters (especially the landings), with a migratory shift further away from the point of departure over time.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677394,no,undetermined,0
MIS Implementing Approaches Choose Based on Analytic Hierarchy Process,"Choose of suitable approaches for implementing management information system is a complex problem involves many intangibles that need to be trades off. Based on analytic hierarchy process, this paper structures the problem of choose as a hierarchy. There are four alternatives and four criteria for the decision. The alternatives are end-user development, joint development, outsourcing development and purchasing software packages. The criteria are performance and quality of the system, and costs and benefits of the implementation. Each criterion is broken down into subcriteria to be more comprehensible and measurable. The elements are compared one by one to construct a set of pairwise comparison, and then obtain the local and global priority of each element. The final priorities of alternatives can derive a better decision. The case study demonstrated that an optimal implementing approach of MIS could be effectively selected based on AHP.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731673,no,undetermined,0
Modelling quality of service in IEEE 802.16 networks,"While only relatively recently standardized, IEEE 802.16 orWiMAX networks are receiving a great deal of attention in both industry and research. This is so because with the increased emphasis on multimedia data, apart from the general advantage of wireless, 802.16 promises wider bandwidth and QoS as part of the standard. As a back haul network for other networks, in particular the 802.11a/b/g/e or WiFi networks, it is well suited. As for any new technology, there are many open questions of which Transmission Scheduling and Connection Admission Control (CAC) are the most prominent. The standard intentionally makes no statement about either function. Different from other performance models we have seen, we consider an analytical framework which takes into account the close relationship between the CAC algorithms and the Scheduler algorithms and is applicable to each mode of operation and admission control paradigm specified by the standard. The long term objective of this work is to present a hybrid analytic and simulation model, based on the proposed framework, for modelling QoS metrics in 802.16 networks.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669466,no,undetermined,0
"Multi-criteria, context-enabled B2B partner selection","B2B collaborations today are typically short-term and dynamic, so it is imperative that business processes be quickly and accurately forged. This paper details research that combines both the context-aware (CA) approach and analytic hierarchy process (AHP) technique to select and rank potential partners in context-enabled B2B collaborations. The approach was validated by a case study. A review of context-aware applications and the criteria for the selection of potential partners are also presented.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4811533,no,undetermined,0
Multidimensional visual analysis using cross-filtered views,"Analysis of multidimensional data often requires careful examination of relationships across dimensions. Coordinated multiple view approaches have become commonplace in visual analysis tools because they directly support expression of complex multidimensional queries using simple interactions. However, generating such tools remains difficult because of the need to map domain-specific data structures and semantics into the idiosyncratic combinations of interdependent data and visual abstractions needed to reveal particular patterns and distributions in cross-dimensional relationships. This paper describes: (1) a method for interactively expressing sequences of multidimensional set queries by cross-filtering data values across pairs of views, and (2) design strategies for constructing coordinated multiple view interfaces for cross-filtered visual analysis of multidimensional data sets. Using examples of cross-filtered visualizations of data from several different domains, we describe how cross-filtering can be modularized and reused across designs, flexibly customized with respect to data types across multiple dimensions, and incorporated into more wide-ranging multiple view designs. The demonstrated analytic utility of these examples suggest that cross-filtering is a suitable design pattern for instantiation in a wide variety of visual analysis tools.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677370,no,undetermined,0
nAble Adaptive Scaffolding Agent Œ_ Intelligent Support for Novices,"Scaffolding techniques allow human instructors to support novice learners in critical early stages, and to remove that support as expertise grows. This paper describes nAble, an adaptive scaffolding agent designed to guide new users through the use of an analytic software tool in the 'nSpace Sandbox' for visual sense-making. nAble adapts the interface and instructional content based on user expertise, learning style and subtask. Bayesian Networks and Hidden Markov task models provide the agent reasoning engine. An experiment was conducted in which participants were provided with one of: an adaptive scaffold, an indexed help file or a human guide. Users of the adaptive scaffold outperformed users of the indexed help and more quickly converged with the performance of users with the human guide.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740799,no,undetermined,0
Narratives: A visualization to track narrative events as they develop,"Analyzing unstructured text streams can be challenging. One popular approach is to isolate specific themes in the text, and to visualize the connections between them. Some existing systems, like ThemeRiver, provide a temporal view of changes in themes; other systems, like In-Spire, use clustering techniques to help an analyst identify the themes at a single point in time. Narratives combines both of these techniques; it uses a temporal axis to visualize ways that concepts have changed over time, and introduces several methods to explore how those concepts relate to each other. Narratives is designed to help the user place news stories in their historical and social context by understanding how the major topics associated with them have changed over time. Users can relate articles through time by examining the topical keywords that summarize a specific news event. By tracking the attention to a news article in the form of references in social media (such as weblogs), a user discovers both important events and measures the social relevance of these stories.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677364,no,undetermined,0
Networking concepts comparison for avionics architecture,"The purpose of this paper is to evaluate alternative networking concepts (standards and protocols), with a particular emphasis on comparing ARINC 664 network standard with legacy avionics networks. The conclusions of this comparison are reinforced with an example network solution for the avionics architecture using the Avionics Full- Duplex Switched Ethernet (AFDX) protocol. The networking of modules (hardware and software) and applications on an aircraft is crucial, and often new designs and upgrades rely on legacy network architectures. In the past, such systems in defense applications have been successfully integrated around the 1553B bus architecture, while commercial applications have satisfied FAA requirements with systems integrated around the ARINC 429 bus architecture. The new applications and capabilities being requested by stakeholders for Avionics systems of the future require increased bandwidth and latency requirements that suggest likely inadequacies in legacy bus architectures. There is continuing pressure by pilots for more information displayed on increasingly more intuitive graphical displays and interfaces; while the ground control and logistics teams want additional and more timely airplane status data - this data is consolidated from a multitude of sub-systems and sensors on board, including event logs and trend data. These new demands may require us to leverage new technologies to keep pace with stakeholder expectations today and in the future. A recent advancement in networking technology is ARINC 664, which defines a deterministic version of an Ethernet network. Boeing and Airbus have adopted Avionic Full- Duplex Switched Ethernet for their newer airplanes [1-3], and NASA is considering ARINC 664 for the new Crew Exploration Vehicle [4]. The B787 is slated to accommodate 100 applications in part due to the availability of larger network bandwidth [5]. NASA hopes to benefit from commercial-off-the-shelf (COTS) Ethernet components which includ- - e reduced overall costs, faster system development and less-costly maintenance for the system network. Both found ARINC 664 to be the best fit in ARINC 653 based systems. Integrating an Avionics Full-Duplex Switched Ethernet may benefit the defense industry avionics customer by lowering life cycle cost and accommodating increasing requirements. This paper addresses the speed, reliability, and flexibility of modern network protocols and explores new options for avionics architectures. This applies to either a complete redesign or a phased avionics upgrade in legacy airplanes. For a historical context, this paper also summarizes the value and utility of legacy networking protocols, together with their downsides. Networking standards and protocols are evaluated as a function of critical requirements pertaining to performance, certification, security, reliability, evolvability, cost, and flexibility to meet changing requirements. Methods such as Quality Function Deployment (QFD) and Analytic Hierarchy Process (AHP) are used to evaluate the three network architectures. The impacts on security and reliability are explored, and additional aspects are highlighted for future research.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4702761,no,undetermined,0
Numerical simulation and experimental study of liquid-solid two-phase flow in nozzle of DIA Jet,The velocity of abrasive particles at the nozzle exit of Direct Injection Abrasive (DIA) Jet is a key factor affecting cutting capacity of jet. The powerful Computational Fluid Dynamics (CFD) analysis software Fluent is applied to numerical simulation of liquid-solid two-phase flow in the hard alloy nozzle of different cylindrical section length under a certain conditions. The optimum ratio of diameter to length is obtained when the particle velocities are the largest at the nozzle exit. The rule of velocity distribution of liquid-solid two-phase flow of the optimum nozzle is analyzed. The numerical control cutting machine tool of DIA Jet is adopted to finish cutting experiments on different variety of materials. The analytic results of experiments verify the results of numerical simulation.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4618377,no,undetermined,0
Orchestrating caGrid Services in Taverna,"caBIGtrade (the cancer Biomedical Informatics Gridtrade) is an open-source, open-access information network enabling cancer researchers to share tools, data, applications, and technologies. caGrid is the underlying service-based grid software infrastructure for caBIG, integrating distributed data and analytic resources into a virtual collaborative platform for cancer research. Within caGrid, many cancer-related data analysis and aggregation tasks can make use of ""canned"" sets of service invocations, or workflows. As a result, there is a need to orchestrate the invocation of caGrid services through the use of both a workflow language and tooling. In this paper, we first explain why we select Taverna as a candidate for workflow authoring and invocation. We then review the development of Taverna plug-ins in general, and describe how we extend Taverna to use caGrid services. We then detail a real-world example and the lessons learned from our research. Finally we conclude with a summary and a description of potential next steps.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670154,no,undetermined,0
Partner Selection for car industry Logistics Alliance in China,"The objectives to form car logistics alliance are confirmed and the factors affecting the partner selection for Chinese car industry logistics alliance have been analyzed. Considering the interdependence and feedback between the factors, we build an ANP model of partner selection for car industry logistics alliance. The elements priorities of the model are obtained by applying the super decisions system software. The preferable partners can be selected by fuzzy evaluating method. The numeric computation proves that the partner selection for car logistics alliance can be solved efficiently with the method.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4636504,no,undetermined,0
Poverty Grade Evaluation Model Based on Multilevel Fuzzy System,"Poverty, a permanent problem in the society, is listed top-class global problem of social development by the United Nations. Grading the needy situation of impoverished families helps the government establish better policies, distribute resources more reasonably, and therefore provide aid more effectively. However, the traditional single-factor mode is not adequate, for poverty grade evaluation involves various factors of different weights, and some factors cannot be analyzed by classical algorithm. To overcome such problems, in this paper we establish a model applying the theories and methods of fuzzy mathematics and comprehensive evaluation. Based on fuzzy inference, we perform evaluations which are both qualitative and quantitative, and include exact and inexact factors. We determine the indexes of poverty grade according to maximum membership degree, and assign their weight using Analytic Hierarchy Process (AHP) -- in this way we quantify the qualitative problems. Finally, we verify our model with instances; the test result indicated that this technologically-advanced model provides a higher reliability to poverty grade evaluation, and is practically applicable.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4737553,no,undetermined,0
Preliminary Considerations on ADC Standard Harmonization,"In this paper, an analytic comparison of the dynamic parameters that are used for qualifying analog-to-digital converters (ADCs) in the frequency domain reported in the most diffused standards is provided. This could be the first step toward their harmonization.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4427397,no,undetermined,0
Research on the Performance of ITS Information Publishing Measures,"This paper explores the performance of traffic information publishing measures (TIPM). By applying the model and theory of the Analytic Network Process (ANP), a comprehensive multiple-attribute evaluation framework was proposed with the consideration of both the relationships of feedback and dependence among the criterion in five dimensions, such as cost, technique, capability, quality and charger. Weights were calculated to emphasize the interdependent relationships by using special software Super Decision and used in evaluating the six main intelligent transportation system (ITS) information publishing measures in Beijing. Finally, the evaluation results were ranked based on the total scores in descending order. It can be used as a guide for TIPMs proprietors to review, improve, and enhance service qualities in the future.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659767,no,undetermined,0
Research on Web services selection model based on AHP,"This paper focuses on the problem that how to select the optimal service among many Web services which all meet the functional needs, establishes an index system for Web services products selection from four aspects, namely the supply side, the user, product and environment. Based on this, we collect the views of 30 experts by Analytic Hierarchy Process (AHP) method and calculate the weight of each index at all levels based on the data collected from questionnaire survey. In the overall sample data analysis, we put two types of sample data namely business operation experts and academics for comparative analysis. The Web services selection model proposed in this article can provide the reference to Web services managers when they selecting Web services, and also contributes to in-depth research on the adoption of Web services based information system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4683004,no,undetermined,0
RFM Value and Grey Relation Based Customer Segmentation Model in the Logistics Market Segmentation,"In CRM (customer relationship management), the importance of a segmentation method for identifying good customers has been increasing. The paper recalls the development of domestic and foreign markets subdivision process and trends. This study presents a novel approach that combines customer targeting and customer segmentation for marketing strategies. This investigation identifies customer behavior using a recency, frequency and monetary (RFM) model and grey correlation model to evaluate proposed segmented customers. Models have taken into account the customerí‰ŒËs value for enterprises and logistics services that the customers are concerned about. The AHP (Analytic Hierarchy Process) algorithm is used to computer the weights of indicators. To demonstrate the efficiency of the proposed method, this work performs an empirical study of a logistics enterprise to segment 10 customers. The result shows the way to segment customer is effective. So it is easy to find high added customers based for enterprises to develop effective marketing strategies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723147,no,undetermined,0
Robust reconfigurable filter design using analytic variability quantification techniques,"In this paper, we develop a variability-aware design methodology for reconfigurable filters used in multi-standard wireless systems. To model the impact of statistical circuit component variations on the predicted manufacturing yield, we implement several different analytic variability quantification techniques based on a double-sided implementation of the first and second order reliability methods (FORM and SORM), which provide several orders of magnitude improvement in computational complexity over statistical sampling methods. Leveraging these efficient analytic variability quantification techniques, we employ an optimization approach using Sequential Quadratic Programming to simultaneously determine the fixed and tunable/switchable circuit element values in an arbitrary-order canonical filter to improve the overall robustness of the filter design when statistical variations are present. The results indicate that reconfigurable filters and impedance matching networks designed using the proposed methodology meet the specified performance requirements with a 26% average absolute yield improvement over circuits designed using deterministic techniques.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4681662,no,undetermined,0
An adaptive planning model based on multi-agent,"Quantitative factors are usually taken into account, but qualitative factors are neglected when a retailer in distribution chain decides to order. For this reason, a planning model based on multi-agent is supposed to solve adaptive problems and then analyze the problems by using quantitative and qualitative factors to improve the retailerspsila ordering efficiency. Using AHP approach, the multi-attribute analysis method is developed to evaluate the influence factors for ordering quantities. The weight for each retailer is calculated via Super Decisions software. In addition, According to the weights and qualitative data in ERP, a multi-objective decision-making method is given to determine the ordering quantities. The simulation results show that the proposed model based on multi-agent increases ordering flexibility and offers a good planning method with cooperation between retailers and a DC.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620572,no,undetermined,0
Activity Recognition Using a Web 3.0 Database,"Web 3.0 envisages software agents that know how to reason over activities, events, locations, people, companies, and their inter-relationships. Learning more about customers through behavioral and activity recognition is here today through currently available Semantic Technologies and is a showcase for how these technologies will evolve. The demonstration shows real world examples of activity recognition using a combination of industry standard RDF and OWL, reasoning with basic Geotemporal primitives and some well-known Social Network Analytics.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4597233,no,undetermined,0
Software Reliability Metrics Selecting Method Based on Analytic Hierarchy Process,"It is very important to select and use appropriate software reliability metrics in software reliability engineering. This paper proposes a framework for selecting software reliability metrics based on analytic hierarchy process (AHP) and expert judgment. Selecting criteria and the metrics for selection are identified. In each development phase, the grading of metrics according to every criterion are given by experts qualitatively, and then analyzed synthetically to calculate the weights of metrics using AHP. A preliminary application is practised, and the metrics whose weights are top-ranked are recommended and analyzed. Sensitivity and consistency of this method are also analyzed. Compared with general selecting criteria, the method studied in this paper can be used to select appropriate metrics correctly, stably and systemically. Furthermore, the final selection results are accordant with engineering experience, and using the metrics recommended will make software reliability evaluation more reliable and effective",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032303,no,undetermined,0
"Communication Waveform Design Using an Adaptive Spectrally Modulated, Spectrally Encoded (SMSE) Framework","Fourth-generation (4G) communication systems will likely support multiple capabilities while providing universal, high-speed access. One potential enabler for these capabilities is software-defined radio (SDR). When controlled by cognitive radio (CR) principles, the required waveform diversity is achieved through a synergistic union called CR-based SDR. This paper introduces a general framework for analyzing, characterizing, and implementing spectrally modulated, spectrally encoded (SMSE) signals within CR-based SDR architectures. Given orthogonal frequency division multiplexing (OFDM) is one 4G candidate signal, OFDM-based signals are collectively classified as SMSE since data modulation and encoding are applied in the spectral domain. The proposed framework provides analytic commonality and unification of multiple SMSE signals. Framework applicability and flexibility is demonstrated for candidate 4G signals by: 1) showing that resultant analytic expressions are consistent with published results and 2) presenting representative modeling and simulation results to reinforce practical utility",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4200710,no,undetermined,0
Analytic Hierarchy Process Based on Fuzzy Logic,The issue of analytic hierarchy process for decision making problems is the long-standing question under study. The improved analytic hierarchy process based on fuzzy logic is considered.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297520,no,undetermined,0
Analytic Hierarchy Process for Technology Policy: Case Study the Costa Rican Digital Divide,"This article consists of presentation slides on the following subject. This paper presents an application of the analytic hierarchy process (AHP) for technology policy. Developing countries face the great challenge to bridge their internal digital divide. However, most studies seeking to bridge this gap offer only recommendations at the policy level. With the use of AHP as a methodology we can provide appropriate information about which technologies will have the greatest impact on bridging this gap.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349443,no,undetermined,0
Analytics-driven solutions for customer targeting and sales-force allocation,"Sales professionals need to identify new sales prospects, and sales executives need to deploy the sales force against the sales accounts with the best potential for future revenue. We describe two analytics-based solutions developed within IBM to address these related issues. The Web-based tool OnTARGET provides a set of analytical models to identify new sales opportunities at existing client accounts and noncustomer companies. The models estimate the probability of purchase at the product-brand level. They use training examples drawn from historical transactions and extract explanatory features from transactional data joined with company firmographic data (e.g., revenue and number of employees). The second initiative, the Market Alignment Program, supports sales-force allocation based on field-validated analytical estimates of future revenue opportunity in each operational market segment. Revenue opportunity estimates are generated by defining the opportunity as a high percentile of a conditional distribution of the customer's spending, that is, what we could realistically hope to sell to this customer. We describe the development of both sets of analytical models, the underlying data models, and the Web sites used to deliver the overall solution. We conclude with a discussion of the business impact of both initiatives.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5386583,no,undetermined,0
Analyzing Large-Scale News Video Databases to Support Knowledge Visualization and Intuitive Retrieval,"In this paper, we have developed a novel framework to enable more effective investigation of large-scale news video database via knowledge visualization. To relieve users from the burdensome exploration of well-known and uninteresting knowledge of news reports, a novel interestingness measurement for video news reports is presented to enable users to find news stories of interest at first glance and capture the relevant knowledge in large-scale video news databases efficiently. Our framework takes advantage of both automatic semantic video analysis and human intelligence by integrating with visualization techniques on semantic video retrieval systems. Our techniques on intelligent news video analysis and knowledge discovery have the capacity to enable more effective visualization and exploration of large-scale news video collections. In addition, news video visualization and exploration can provide valuable feedback to improve our techniques for intelligent news video analysis and knowledge discovery.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389003,no,undetermined,0
Appliance-Based Autonomic Provisioning Framework for Virtualized Outsourcing Data Center,"As outsourcing data centers emerge to host applications or services from many different organizations and companies, it is critical for data center owners to isolate different applications while dynamically and optimally allocate resources among them. To address this problem, we propose a virtual-appliance-based autonomic resource provisioning framework for large virtualized data centers. Firstly, we present the architecture of the data center with enriched autonomic features. Secondly, we define a non-linear constrained optimization model for dynamic resource provisioning and present its novel analytic solution. Key factors including virtualization overhead and reconfiguration delay are incorporated into the model. Experimental results based on a prototype system demonstrate that system-level performance has been greatly improved by taking advantage of fine-grained server consolidation. Experiments with the impact of switching delay also show the efficiency of the framework through significantly reducing provisioning time.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273123,no,undetermined,0
Applying Incomplete Linguistic Preference Relations to a Selection of ERP System Suppliers,"This study provides a method to solve the incomplete linguistic preference relations under multi-criteria decision making. The method uses simple calculation and can speed up the process of comparison and selection of alternative. Decision-making experts obtain the matrix by choosing a finite and fixed set of alternatives and set a pairwise comparison based on their different preferences and knowledge. The method considers only n-1 judgments, whereas the traditional analytic hierarchy approach takes n(n -1)/2 judgments in a preference matrix with n elements. By using the method described above, this study evaluates the ERP system suppliers.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341268,no,undetermined,0
Atmospheric Composition Processing System (ACPS): Evolution from instrument-based to measurement-based processing,"NASA's ACCESS Program is sponsoring an evolution of a mission-based ozone data processing system into a measurement-based Atmospheric Composition Processing System (ACPS). Services currently available only to mission science team members will be extended to members of the Atmospheric Composition Community. Data and information technologies developed for the BUV, TOMS, OMI, and OMPS science teams will be made available to a wider base of science users. Community scientists will have access to current processing algorithms and resources. They will be able to run modified forms of existing algorithms or develop and run new algorithms on the ACPS. Development costs will be minimized by utilizing elements of existing and proven systems. Implementation costs for external scientists will also be minimized by utilizing Linux-based commodity processors, open standards, and open source software. This presentation introduces the architecture of the ACPS and describes how external scientists will be able to interact with it.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4423064,no,undetermined,0
Canny Operator Based Level Set Segmentation Algorithm for Medical Images,"The segmentation and extraction of tissues and organs are fundamental work in 3D reconstruction and visualization of medical images. Considering the features of virtual human images, a Canny operator based level set algorithm and the analytic expression of Level set equation is deduced. The algorithm is implemented using narrow band method numerically. The algorithm combines the advantages of Canny operator which can orient the boundary accurately and the idea that Level set method continuously evolves the boundary in image space. Experiments show that the algorithm calculates the result precisely with good anti-jamming property and fast computing speed. Fine results can be obtained by using this algorithm.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272716,no,undetermined,0
Classifying facial expressions using point-based analytic face model and Support Vector Machines,"This paper describes a fully automated method of classifying facial expressions using support vector machines (SVM). Facial expressions are communicated by subtle changes in one or more discrete features such as tightening the lips, raising the eyebrows, opening and closing of the eyes or certain combination of them, which can be identified through monitoring the changes in muscles movement located near about the regions of mouth, eyes and eyebrows. In this work, we have applied an analytic face model using eleven feature points that represent and identify the principle muscle actions as well as provide measurements of the discrete features responsible for each of the six basic human emotions. A multi-detector approach of facial feature point localization has been utilized for identifying these points of interest from the contours of facial components such as eyes, eyebrows and mouth. Feature vectors composed of eleven features are then obtained by calculating the degree of displacement of these eleven feature points from a non-changeable rigid point. Finally, the obtained feature sets are used to train a SVM classifier so that it can classify facial expressions when given to it in the form of a feature set. The method has been tested on two different publicly available facial expression databases and on average, 89.44% and 84.86% of successful recognition rates have been achieved.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4413713,no,undetermined,0
Deciding on the Ideal Channel Coefficients in Multi-Channel Manufacturing,"This paper provides a methodology to determine ideal channel coefficients in multi-channel manufacturing (MCM). MCM enhances the advantages of cellular manufacturing by expanding the capabilities of the cells to handle multiple products. The ideal channel coefficients are needed as input for MCM design techniques. While determining ideal channel coefficients (so channel coefficients), we want to assign more profitable parts to more channels. In some cases, this may require additional investment (as extra machines) for some of the channels. These two conflicted goals must be compromised. To do this, the analytic network process (ANP) approach, which is one of the systematic decision-aid tools, is used. The developed model is solved by Super Decisions software. Results showed that ANP is a powerful methodology to determine ideal channel coefficients in MCM design.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222991,no,undetermined,0
Literature Fingerprinting: A New Method for Visual Literary Analysis,"In computer-based literary analysis different types of features are used to characterize a text. Usually, only a single feature value or vector is calculated for the whole text. In this paper, we combine automatic literature analysis methods with an effective visualization technique to analyze the behavior of the feature values across the text. For an interactive visual analysis, we calculate a sequence of feature values per text and present them to the user as a characteristic fingerprint. The feature values may be calculated on different hierarchy levels, allowing the analysis to be done on different resolution levels. A case study shows several successful applications of our new method to known literature problems and demonstrates the advantage of our new visual literature fingerprinting.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389004,no,undetermined,0
Discrete event simulation modeling of resource planning and service order execution for service businesses,"In this paper, we present a framework for developing discrete-event simulation models for resource-intensive service businesses. The models simulate interactions of activities of demand planning of service engagements, supply planning of human resources, attrition of resources, termination of resources and execution of service orders to estimate business performance of resource-intensive service businesses. The models estimate serviceability, costs, revenue, profit and quality of service businesses. The models are also used in evaluating effectiveness of various resource management analytics and policies. The framework is aided by an information meta-model, which componentizes modeling objects of service businesses and allows effective integration of the components.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419858,no,undetermined,0
Drop Dynamic Responses and Modal Analysis for Board Level TFBGA,"Along with more and more use of thin-profile fine-pitch ball grid array (TFBGA) in portable electronic products, such as mobile phones and personal digital assistant (PDA), etc., the drop impact reliability of solder joints for TFBGA becomes a critical concern. This problem is more serious with the application of lead free solder because lead free solder alloy have higher rigidity and lower ductibility compared with the traditional SnPb solder alloy. Whereas the drop test is high cost and long cycle, and the traditional board level drop simulation poses severe challenge to the computer resource, the theory analysis and modal superposition method are combined in this paper. Firstly, the drop vibration differential equation of simplified PCB assembly is deduced and the analytic solution is solved based on the given boundary condition according to the dynamics theory. Secondly, the ANSYS/LS-DYNA software is employed, a quarter 3D model with 15 representatives lead free TFBGA mounted on PCB is developed, and the dynamic responses of solder joint is analyzed under the drop impact. The critical solder joint with the maximal stress caused by deflection of PCB is identified by the model. Finally, the relationship between the number of screws which support the PCB and the PCB deflection under drop impact is studied through the modal superposition, and the influence of every stage mode to the PCB bending deflection is discussed. The results show that the outermost corner solder joint called critical solder joint is aptest to failure. More deflection of PCB causes larger stress of solder joint. The first bend mode is the main factor which influences the bending deflection of PCB. Adopting more screws to support PCB as well as reducing the influence of the first mode is good to improve the drop impact performance of solder joint. Compared with time history explicit dynamic analysis, modal superposition harmonic response analysis can save the solving time greatly.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283578,no,undetermined,0
Evaluation on the Industrialization Potential of Emerging Technologies Using the Analytic Network Process,"Due to uncertainties and complexities give birth to the future of emerging technologies, conventional methods for technology evaluation are subjected to many drawbacks and limitations. Even though we can divide a complex system into subsystems, the relative weights of the subsystems are also a crucial problem, because of these subsystems usually exist interdependence and feedback. This paper discusses the industrialization potential evaluation system based on Delphi survey, and evaluates the industrialization potential of emerging technologies using the analytic network process (ANP). A complete ANP model and pairwise comparisons are generated in this paper though the Super Decision software, and supermatrix calculation and sensitivity analysis are discussed at last. Benefits ofthe approach are detailed through illustrative example.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349444,no,undetermined,0
ExView: A Real-time Collaboration Environment for Multi-ship Experiments,"New challenges in the area of experimental logistics, data visualization and data fusion are encountered in oceanographic research when the need to keep track of the location of multiple ships, moorings, gliders, drifters, and other platforms is combined with assimilating supporting data gathered off the Internet and inserted into the experimental framework. Showing that this can be done well is a start towards our being able to think of scientific expeditions on research vessels as deployable ocean observatories. Researchers at the Woods Hole Oceanographic Institution recently collaborated with the Rutgers University Coastal Ocean Observing Lab (COOL) and other members of the Shallow Water '06 experiment (sponsored by the US Office of Naval Research) in the creation of a new software tool called ExView. This experiment viewer software is a Web-based application that runs on ships and on shore. It enables coordinated, real-time collaboration between researchers employing a number of different research platforms involved in a large-scale experiment. During the SW06 experiment, logistics information and scientific reports associated with twenty-five principal investigators, six ships, eight gliders, three REMUS class AUVs, sixty-two moorings, two aircraft, and four drifting moorings were all made available to researchers in near-real-time over a three month time-period during the summer of 2006. A primarily wireless communications network comprising of HiSeasNet (satellite), SWAP (shipboard WiFi), SeaNet (INMARSAT-B), and the Global Internet was used to synchronize Websites (5 on ships, 1 on shore) so that all participants of the experiment could contribute and monitor platform locations, ship tracks, glider tracks, aircraft tracks, daily reports, weather information, CODAR imagery, satellite imagery, and ocean model results. A dynamic website was mirrored between all of the ships involved in the SW06 experiment. A map at the center of the web display showed location- and tracks of all platforms (ships, moorings, planes, gliders, etc.) and the logistics-related information available from each of them. As ships wandered in and out of wireless range of each other, they updated each others' Websites (even though Internet access might not have been available to the ship at that time). A shore-based website was also updated regularly by ships that had satellite connections back to the Internet. Participants on shore (and on each ship) were able to use the website to browse back in time to see the location and status of mobile platforms, science reports that were submitted each day, and data from a number of standard data sensors collected by each of the ships throughout the experiment. A shore-based team at the Rutgers University Cool Lab provided daily reports with graphics such as water temperature profiles, hurricane reports, satellite imagery, weather reports, wind speed profiles, etc. They also provided an analytic analysis of these elements and how they related to the current experiment plans. In addition, Rutger's staff used the ExView application to monitor the locations of their fleet of gliders and steer them to avoid moorings and other fixed and mobile assets in the area. An emerging technology called delay-tolerant networking (DTN) is being examined for inclusion in the ExView software suite. The current DTN design promises tighter integration of wireless technologies and the development of new algorithms capable of routing data via mobile platforms based on available bandwidth, remaining battery power, platform location, data priority, etc. These characteristics will be incorporated into new optional routing algorithms that will be developed in the future as part of DTN. The successful 3-month use of ExView shows how a novel, near-real-time, Web-based application can be used to improve access to logistics information about a collection of ships, other research platforms, investigators, data sensors, and related data so",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4302345,no,undetermined,0
Forecasting Final/Class Yield Based on Fabrication Process E-Test and Sort Data,"This paper presents an application of data mining using gradient boosting trees to predict class test yield performance at high volume manufacturing (HVM) based on e-test and sort ancestry parameters. The paper also presents a framework for the predictive capability system and highlights some of the techniques and implementation details. Modeling at wafer level was found to give the best accuracy and the analytic model provides wafer level yield accuracy at 97% within plusmn2% accuracy level for Intel chipset products. Certain functional bins that correlate to e-test and sort can also be modeled for identification of possible high fallouts. In addition, the modeling process also produced Pareto analysis reports that lists dominant influencers and the dependency plots, enabling Assembly and Test (ATM) engineers to feedback to upstream operation engineers. The overall predictive capability has set a new standard for proactive yield monitoring and excursion management at Intel ATM factories and it is useful to various functions including the yield engineering, product engineering, manufacturing and planning.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341700,no,undetermined,0
Hydrodynamic transport parameters of wurtzite ZnO from analytic- and full-band Monte Carlo Simulation,"Zinc oxide (ZnO) has seen practical applications much earlier than most wide band gap semiconductors, but only recently it has received renewed attention for electronic and optoelectronic applications because of its potential advantages over III-nitrides, including commercial availability of bulk single crystals, amenability to wet chemical etching, a larger exciton binding energy, and excellent radiation-hard characteristics (Ozgur et al., 2005).",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4422373,no,undetermined,0
Intelligent Web Services Selection based on AHP and Wiki,"Web service selection is an essential element in service-oriented computing. How to wisely select appropriate Web services for the benefits of service consumers is a key issue in service discovery. In this paper, we approach QoS-based service selection using a decision making model - the analytic hierarchy process (AHP). In our solution, both subjective and objective criteria are supported by the AHP engine in a context-specific manner. We also provide a flexible Wiki platform to collaboratively form the initial QoS model within a service community. The software prototype is evaluated against the system scalability.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4427186,no,undetermined,0
Knowledge management based on SOA in aerospace industry,"The paper discusses the characteristics of the knowledge management in Chinese aerospace industry. With a study on the process that the standard converts from the self-dependent intellectual property to the industry, national, international ones, the paper further studies on the design and construction of the SOA-based enterprise knowledge management in the aerospace industry. The main research of the design is on the capability of platform-free and interoperability of the heterogeneous system with the integration model of the agent analytic system. The system is supposed to be with an open and unified environment.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4443469,no,undetermined,0
Leveraging Resource Prediction for Anticipatory Dynamic Configuration,"Self-adapting systems based on multiple concurrent applications must decide how to allocate scarce resources to applications and how to set the quality parameters of each application to best satisfy the user. Past work has made those decisions with analytic models that used current resource availability information: they react to recent changes in resource availability as they occur, rather than anticipating future availability. These reactive techniques may model each local decision optimally, but the accumulation of decisions over time nearly always becomes less than optimal. In this paper, we propose an approach to self- adaptation, called anticipatory configuration that leverages predictions of future resource availability to improve utility for the user over the duration of the task. The approach solves the following technical challenges: (1) how to express resource availability prediction, (2) how to combine prediction from multiple sources, and (3) how to leverage predictions continuously while improving utility to the user. Our experiments show that when certain adaptation operations are costly, anticipatory configuration provides better utility to the user than reactive configuration, while being comparable in resource demand.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274905,no,undetermined,0
An SMSE Implementation of CDMA with Partial Band Interference Suppression,"A spectrally modulated, spectrally encoded (SMSE) framework is adopted for implementing code division multiple access (CDMA) with partial band interference suppression. SMSE signals are constructed within an architecture governed by cognitive radio (CR) principles and supported by software defined radio (SDR) implementation, a union referred to as CR- based SDR. Orthogonal frequency division multiplexing (OFDM) signals, a foundational part of future 4G systems, are collectively classified as SMSE because data modulation and encoding are applied in the spectral domain. Framework applicability was demonstrated for realistic 4G signals by illustrating consistency between resultant analytic SMSE expressions and published results. Framework implementability and flexibility is further demonstrated herein using more complex CDMA signal structures. Modeling and simulation results are presented for a form of multi-carrier CDMA using polyphase codes with adaptive spectral notching (interference avoidance). Collectively, the SMSE implementation of this system correlates very well with theoretical predictions for multiple access scenarios.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4411751,no,undetermined,0
AHP-based Master Device Selection Scheme for Pervasive Personal Network,"The advancement of mobile communications and small computers bring pervasive personal network closer to reality. To reduce user intervention in the pervasive environment, we proposed a framework to compose a virtual terminal over distributed devices among the pervasive personal network, called UST Since Master Device performs the framework functionality in UST, selecting proper master device plays a vital role in ensuring quality of service especially. In this article we develop a master device selection scheme for UST system based on the Analytic Hierarchy Process (AHP). It also describes the state transitions and message sequences for the scheme. Experimental results show that the proposed scheme can effectively choose the optimum master device under diverse pervasive personal network conditions.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4469531,no,undetermined,0
Agent-based Human-computer-interaction for Real-time Monitoring Systems in the Trucking Industry,"Auto ID systems can replace time-consuming, costly and error-prone processes of human data entry and produce detailed real time information. However, they would add value only to the extent that data is presented in a user-friendly manner. As model-based decision support is not always adequate, an agent-based approach is often chosen. Real life entities such as orders and trucks are represented by agents, which negotiate in order to solve planning problems. For the respective data representation at least two forms can be distinguished, focusing either on (1) resources (account-based) or (2) orders (order-centric). Applying cognitive fit theory we describe how the different interfaces affect decision making. The hypotheses would be tested in a laboratory experiment. The intended contribution should support that order-centric interfaces have higher user-friendliness and are especially beneficial to low-analytics and planners working under time pressure",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076424,no,undetermined,0
Adoption of e-Commerce: A decision theoretic framework and an illustrative application,"Electronic commerce (e-Commerce or EC), in some form or other, is changing the way organizations do their business. Many organizations (banks etc) are forcing their customers to adopt e-Commerce. Others are adopting e-Commerce for competitive necessity. This raises the obvious question: How can organizations adopt appropriate e-Commerce model judiciously? This paper addresses the above research question. We use a decision theoretic framework based on multiple attributes of e-Commerce. Extensive literature review revealed a number of factors or attributes that either act as drivers or barriers of e-Commerce success. A well known decision theoretic approach based on multiple attribute, called analytic hierarchy process (AHP), is used to develop a comprehensive model of e-Commerce adoption. Our framework can be used as a guide to select the appropriate e-Commerce model. Real world data, from EC consultants, have been collected for a hypothetical SME which is embarking on adopting an e-Commerce model. The paper presents the application of the framework for this SME.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579406,no,undetermined,0
Systematic Development of Requirements Documentation for General Purpose Scientific Computing Software,"This paper presents a methodology for developing the requirements for general purpose scientific computing software. The first step in the methodology is to determine the general purpose scientific software of interest. The second step consists of a commonality analysis on this identified family of general purpose tools to document the terminology, commonalities and variabilities. The commonality analysis is then refined in the third step into a family of specific requirements documents. Besides fixing the variabilities and their binding times, each specific requirements document also shows the relative importance of the different nonfunctional requirements, for instance using the analytic hierarchy process (AHP). The new methodology addresses the challenge of writing validatable requirements by including solution validation strategies as part of the requirements documentation. To illustrate the methodology an example is shown of a solver for a linear system of equations",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1704064,no,undetermined,0
Time Synchronization Simulator and Its Application,"Time synchronization is a critical middleware service of wireless sensor networks. Since the performance of time synchronization algorithm is greatly influenced by many factors, benchmark for evaluating time synchronization algorithm is not only difficult but also urgently needed. Software simulation is a good solution especially in the comparison between similar algorithms. In this paper, we presented a time synchronization simulator, Simsync, for wireless sensor networks. Simsync models the distribution of packet delay and the frequency of crystal oscillator as Gaussian. Based on it, we realized reference broadcast synchronization algorithm (RBS) and broadcast time synchronization algorithm (BTS) in 3 different scenarios. Simulated results are compared with the analytic results to advocate its effectiveness",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026017,no,undetermined,0
Understanding representational sensitivity in the iterated prisoner's dilemma with fingerprints,"The iterated prisoner's dilemma is a widely used computational model of cooperation and conflict. Many studies report emergent cooperation in populations of agents trained to play prisoner's dilemma with an evolutionary algorithm. This study varies the representation of the evolving agents resulting in levels of emergent cooperation ranging from 0% to over 90%. The representations used in this study are directly encoded finite-state machines, cellularly encoded finite-state machines, feedforward neural networks, if-skip-action lists, parse trees storing two types of Boolean functions, lookup tables, Boolean function stacks, and Markov chains. An analytic tool for rapidly identifying agent strategies and comparing across representations called a fingerprint is used to compare the more complex representations. Fingerprints provide functional signatures of an agent's strategy in a manner that is independent of the agent's representation. This study demonstrates conclusively that choice of a representation dominates agent behavior in evolutionary prisoner's dilemma. This in turn suggests that any soft computing system intended to simulate behavior must be concerned with the representation issue",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1643837,no,undetermined,0
Usage of the Analytic Hierarchy Process for Production Optimization,The issue of Analytic Hierarchy Process for group decision making problems is the long-standing question under study. The multicriterion optimization task of the manufacturing process based on Analytic Hierarchy Process method is solved in the article.,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404638,no,undetermined,0
Visual Analytics Education,"Visual analytics is a newly evolving field that spans across several more established disciplines. This panel discusses how VA system developers and researchers are best educated at the MS and PhD levels. This paper describes several ways in which VA can be characterized - with the goal of using these characterizations to identify knowledge domains that can be used to define VA curricula. Also, a digital library of VA educational resources is described",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035767,no,undetermined,0
Visualizing the Performance of Computational Linguistics Algorithms,"We have built a visualization system and analysis portal for evaluating the performance of computational linguistics algorithms. Our system focuses on algorithms that classify and cluster documents by assigning weights to words and scoring each document against high dimensional reference concept vectors. The visualization and algorithm analysis techniques include confusion matrices, ROC curves, document visualizations showing word importance, and interactive reports. One of the unique aspects of our system is that the visualizations are thin-client Web-based components built using SVG visualization components",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035760,no,undetermined,0
A Decision model for Selecting an Offshore Outsourcing Location: Using a Multicriteria Method,"As the offshoring becomes more widespread, business practitioners outsource their activities to overseas countries. Although there is a wealth of academic literature examining outsourcing and offshoring, there is little academic literature that addresses the current outsourcing decision most firms facing, which is where to outsource. Given multi-attribute nature of offshore location selection, this paper argues that five factors should be considered for decisions, and proposes the use of analytic hierarchy process (AHP) and PROMETHEE as aids in making offshore location selection decisions. AHP is used to analyze the structure of the location selection problem and determine weights of the criteria, and PROMETHEE method is used for final ranking, together with changing weights for a sensitivity analysis. It shows by means of an application that the hybrid method is very well suited as a decision-making tool for the offshore location selection decision. Finally, potential issues for future research are presented.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383936,no,undetermined,0
A Discrete Differential Operator for Direction-based Surface Morphometry,"This paper presents a novel directional morphometry method for surfaces using first order derivatives. Non-directional surface morphometry has been previously used to detect regions of cortical atrophy using brain MRI data. However, evaluating directional changes on surfaces requires computing gradients to obtain a full metric tensor. Non-directionality reduces the sensitivity of deformation-based morphometry to area-preserving deformations. By proposing a method to compute directional derivatives, this paper enables analysis of directional deformations on surfaces. Moreover, the proposed method exhibits improved numerical accuracy when evaluating mean curvature, compared to the so-called cotangent formula. The directional deformation of folding patterns was measured in two groups of surfaces and the proposed methodology allowed to defect morphological differences that were not detected using previous non-directional morphometry. The methodology uses a closed-form analytic formalism rather than numerical approximation and is readily generalizable to any application involving surface deformation.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4408886,no,undetermined,0
"A distributed-memory, parallel MLFMA for the solution of radiation and scattering problems in complex environments","A distributed-memory parallel MLFMA has been designed to handle simulation of electromagnetic radiation and scattering in the presence of complex objects. The code has been validated against previously-validated software, analytic solutions and physical optics approximations. For more complex problems, where reference data is not available, the resulting solutions conform to expectations based on physical reasoning.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4396279,no,undetermined,0
A function modulation method for digital communications,"We present a novel modulation method for digital communication systems. The method uses orthogonal and/or non-orthogonal analytic functions as symbols. It is shown that, in the case of orthogonal functions, to transmit M bits using one symbol, the new receiver needs to search over only M functions as opposed to 2 symbols. A numerical method for generating the analytic symbols, called Constrained Gram Schmidt method, is presented. Experimental results demonstrating the proof of concept of the approach are also discussed.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4563308,no,undetermined,0
A Fuzzy AHP approach to evaluating e-commerce websites,"As e-commerce is playing a more and more important role in our daily life, there is no wonder that significant attention is being focused on the evaluation of e-commerce Web sites in recent years. In this paper, a fuzzy analytic hierarchy process (FAHP) approach is used to evaluating e-commerce Web sites, which can tolerate vagueness and uncertainty of judgment. Therefore, the insufficiency and imprecision problems associated with the conventional AHP can be solved. Hence, Web sites can be evaluated more reasonably. In the end, a case study is presented to make this approach more understandable for a decision-maker(s).",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296926,no,undetermined,0
A Lightweight Value-based Software Architecture Evaluation,"Current software engineering practice is focused on value-neutral processes. Value-based architecting, one of value-based software engineering agendas, involves the further consideration of the system objectives associated with different stakeholder values in selecting an optimal architectural alternative. There are several value-based architectural evaluation techniques and cost benefit analysis method (CBAM) is a widely used, established technique based on return on investment (ROI). The weaknesses of the existing techniques are uncertainties from several subjective errors and the heavyweight process, which requires many steps and participation of stakeholders. This paper proposes a lightweight value-based architecture evaluation technique, called LiVASAE, using analytic hierarchy process (AHP), which can support a multi-criteria decision-making process. The proposed technique can help overcome the major weakness of the existing techniques such as the uncertainties caused by subjective decision making and heavy-weight process for architecture evaluations. LiVASAE provides a way to measure the uncertainty level using AHP's consistency rate (CR) and It also provides three simplified evaluation steps. In addition, the LiVASAE presents a framework for decision makers to make technical decisions associated with business goals (or values) such as cost, time-to-market, and integration with legacy system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287763,no,undetermined,0
A New Modulation Method for Digital Communication,"We present the design of a transmitter and a receiver for digital communication system that uses function modulation (fm) method. The design covers both orthogonal and non-orthogonal analytic functions. The design is based on the concept of software radio on a general purpose Digital Signal Processor (DSP). A numerical method for generating the waveforms required for the fm system is presented. Experimental results on real environment, demonstrating the feasibility of the concept, are also discussed.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4381583,no,undetermined,0
A New Performance Evaluation Model and AHP-Based Analysis Method in Service-Oriented Workflow,"In service-oriented architecture, services and workflows are closely related so that the research on service-oriented workflow attracts the attention of academia. Because of the loosely-coupled, autonomic and dynamic nature of service, the operation and performance evaluation of workflow meet some challenges, such as how to judge the quality of service (QoS) and what is the relation between QoS and workflow performance. In this paper we are going to address these challenges. First the definition of service is proposed, and the characteristics and operation mechanism of service-oriented workflow are presented. Then a service-oriented workflow performance evaluation model is described which combines the performance of the business system and IT system. The key performance indicators (KPI) are also depicted with their formal representation. Finally the improved Analytic Hierarchy Process is brought forward to analyze the correlation between different KPIs and select services.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293848,no,undetermined,0
A one-dimensional semi-analytic model of the microwave heating effect in verification of numerical hybrid modeling software,In this paper a semi-analytic model of the microwave heating effect has been briefly presented. The paper illustrates also an application of the model in verification of the computational accuracy of a commercially available electromagnetic simulator coupled with an FDTD-based thermal solver prepared by the author. In a series of simulations the behavior of the numerical model has been verified against the semi-analytic model for media of various losses as well as different relationships between temperature and the medium properties. The obtained results confirm the expectations based on the known properties of the computational methods employed in the numerical model.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4400464,no,undetermined,0
A Service-oriented Architecture for Business Intelligence,"Business intelligence is a business management term used to describe applications and technologies which are used to gather, provide access to and analyze data and information about the organization, to help make better business decisions. In other words, the purpose of business intelligence is to provide actionable insight Business intelligence technologies include traditional data warehousing technologies such as reporting, ad-hoc querying, online analytical processing (OLAP). More advanced business intelligence tools - such as HP Openview DecisionCenter - also include data-mining, predictive analysis using rule-based simulations, Web services and advanced visualization capabilities. In this paper we describe a service-oriented architecture for business intelligence that makes possible a seamless integration of technologies into a coherent business intelligence environment, thus enabling simplified data delivery and low-latency analytics. We compare our service-oriented approach with traditional BI architectures, illustrate the advantages of the service oriented paradigm and share our experience and the lessons learned in architecting and implementing the framework.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273437,no,undetermined,0
A Study on Performance Measurement of a Plastic Packaging Organization's Manufacturing System by AHP Modeling,"By the effect of globalization, products, services, capital, technology, and people began to circulate more freely in the world. As a conclusion, in order to achieve and gain an advantage against competitors, manufacturing firms had to adopt themselves to changing conditions and evaluate their critical performance criteria. In this study, the aim is to determine general performance criteria and their characteristics and classifications from previous studies and evaluate performance criteria for a plastic packaging organization by utilizing analytic hierarchy process (AHP) modeling. A specific manufacturing organization, operating in the Turkish plastic packaging sector has been selected and the manufacturing performance criteria have been determined for that specific organization. Finally, the selected criteria have been assessed according to their relative importance by utilizing AHP approach and expert choice (EC) software program. As a result of this study, operating managers chose cost, quality, customer satisfaction and time factors as criteria for this organization. As the findings of the study indicate, the manufacturing organization operating in the plastic packaging sector, overviews its operations and measures its manufacturing performance basically on those four criteria and their sub criteria. Finally, relative importance of those main measures and their sub criteria are determined in consideration to plastic packaging sector.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349449,no,undetermined,0
Activity Analysis Using Spatio-Temporal Trajectory Volumes in Surveillance Applications,"In this paper, we present a system to analyze activities and detect anomalies in a surveillance application, which exploits the intuition and experience of security and surveillance experts through an easy- to-use visual feedback loop. The multi-scale and location specific nature of behavior patterns in space and time is captured using a wavelet-based feature descriptor. The system learns the fundamental descriptions of the behavior patterns in a semi-supervised fashion by the higher order singular value decomposition of the space described by the training data. This training process is guided and refined by the users in an intuitive fashion. Anomalies are detected by projecting the test data into this multi-linear space and are visualized by the system to direct the attention of the user to potential problem spots. We tested our system on real-world surveillance data, and it satisfied the security concerns of the environment.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4388990,no,undetermined,0
Admission control in data transfers over lightpaths,"The availability of optical network infrastructure and appropriate user control software has recently made it possible for scientists to establish end-to-end circuits across multiple management domains in support of large data transfers. These high-performance data paths are typically provisioned over 10 Gigabit optical links, and accessed using ethernet encapsulation at Gigabit and 10 Gigabit rates. The resulting mixture of circuit sizes gives rise to resource conflicts whereby requests to allocate bandwidth partitions are blocked despite vast underutilization of the optical link. In an attempt to remedy this problem, we investigate intelligent admission control policies that consider the long-term effects of admission decisions. Using analytic techniques we show that the greedy policy, which accepts requests to allocate bandwidth partitions whenever sufficient bandwidth exists, is suboptimal in a pertinent scenario. We then consider dynamic online computation of the optimal admission control policy and show that the acceptance ratio of requests to establish end-to-end circuits can be improved by up to 19% on a fifteen-node network where the behaviour of each link is governed by a local optimization effort.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285335,no,undetermined,0
Linear Current Division Principles,"This paper presents the study of a well established linear current division circuit, which has been referenced many times in published works as a basic building block in variety applications up to date. Understanding the advantages and, even more important, the limitations of this technique would benefit the further exploiting of potential applications. Analytic study with close form expressions shows highly agreements with the original statement that this structure is inherently linear and the linearity is independent of the electrical variables and only relies on the transistors geometrical match performance. However, the more accurate study with simulation results suggests that there are limitations for using this technique in terms of high linearity in the most popular semiconductor processes. Thereby more general circuit principles behind reported high linearity is of particular interested.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4253267,no,undetermined,0
Low order radiation forces by analytic interpolation with degree constraint,"The positive real modeling of a floating body is considered, whereas the main focus is on the radiation forces and moments. The radiation forces and moments describe the interaction of a floating body with the surrounding fluid. This type of mathematical model is of interest in among control and simulation of dynamical positioned vessels (i.e. ships and offshore platforms) and wave power plants. It has been proven that the radiation forces are passive, but very little attention have been drawn towards low order passive identification of these forces. Traditionally high order models have been obtained, and subsequently model order reduction have been applied to obtain low order models. Here, a direct approach for obtaining low order passive models using analytic interpolation with a degree constraint is applied. A case study involving a 3 degrees of freedom surface vessel is shown to illustrate the features of the proposed approach.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4434656,no,undetermined,0
A scheme of telemetering and preventing electric larceny system based on GPRS communication system,"For the power supply department, the key problem is hard to find a suitable and cheap way to not only collect huge number of the vastly distributed meters, but also calculate the balance of electric quantity and detect the nontechnical loss (such as electric larceny) automatically. This paper analyzes the structure and principle of the telemetering and preventing electric larceny system based on the public mobile data communication network GPRS. This system realizes the real time and accuracy of reading meter. Meanwhile, for the unique addressing mode, the monitoring software of this system can monitor the process of power provision and consumption, compare and analyze the data in real-time to find the nontechnical loss and measure the power or water supply system weak points; The power supply department can calculate the line loss according to the real-time data; The strong monitoring software also provides some analytic information about the real-time measurements of electrical energy quantities and alarms of electric larceny. The application shows that this system has the character of cheapness, high efficiency, practicability and so on.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4593767,no,undetermined,0
Utility-based QoS Brokering in Service Oriented Architectures,"Quality of service (QoS) is an important consideration in the dynamic service selection in the context of service oriented architectures. This paper extends previous work on QoS brokering for SOAs by designing, implementing, and experimentally evaluating a service selection QoS broker that maximizes a utility function for service consumers. Utility functions allow stakeholders to ascribe a value to the usefulness of a system as a function of several attributes such as response time, throughput, and availability. This work assumes that consumers of services provide to a QoS broker their utility functions and their cost constraints on the requested services. Service providers register with the broker by providing service demands for each of the resources used by the services provided and cost functions for each of the services. Consumers request services from the QoS broker, which selects a service provider that maximizes the consumer's utility function subject to its cost constraint. The QoS broker uses analytic queuing models to predict the QoS values of the various services that could be selected under varying workload conditions. The broker and services were implemented using a J2EE/Weblogic platform and experiments were conducted to evaluate the broker's efficacy. Results showed that the broker adequately adapts its selection of service providers according to cost constraints.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279627,no,undetermined,0
Study on The Analytic Calculation Method of Throttle Slice Deformation,"In this paper, the governing differential equation for deformation of a single throttle-slice is introduced first, and based on its solution satisfying required boundary conditions, the formula and coefficient for deformation of a single slice is obtained through equivalency transformation. The G<sub>r</sub> and it's physics meaning at different radius are studied, the deformation at any radius r is researched. At last, compared with other methods, such as machine design and ANSYS FEA software, the compared results show that G<sub>r</sub> method is the most precise computation method way of throttle-slice deformation, and has very important meaning to the throttle-slice design, analysis, and verification, at the same time, it provides an effective method for mathematics theory better used in practical application.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4303982,no,undetermined,0
The History of Applications of Analytic Signals in Electrical and Radio Engineering,"Analytic signals have been originally described in 1946 by Dennis Gabor. The paper describes briefly the mathematical background created in the past by prominent mathematicians, physicists and engineers, especially the complex notation of functions, the theory of analytic functions and the Fourier spectral analysis and the theory of distributions. Due to the personal experience of the author, the extension of the Gabor's notion of the analytic signal for N-dimensional signals is briefly described As well, problems of analytic spectra of causal signals with extension for N dimensions and 2N-dimensional Wigner distributions are shortly presented.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4400463,no,undetermined,0
The selection of CRM systems in financial institutes using the analytic hierarchy,"Recently, by changing business circumstances, financial institutes have become interested in customer relationship management (CRM) in order to obtain continuous profit and to keep long term relationships with customers using information technology. CRM in the Korean financial market has more important role for the competitive advantage of financial institutes as the financial service sector has grown into the key industry in korean economy. Thus, selection criteria and evaluation for CRM systems are a necessity when financial institutes introduce the CRM systems . In this paper, we presented the selection of CRM systems by the suggested criteria and relative weights using the analytic hierarchy process (AHP). First of all. selection criteria state, make a model for CRM packages, and survey three groups; researcher group, developer group, and user group. In this paper, weights of selection criteria were confirmed through various experts and a necessity element was suggested for financial institutes with weight of element.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4444256,no,undetermined,0
Thin Client Visualization,"We have developed a Web 2.0 thin client visualization framework called GeoBoosttrade. Our framework focuses on geospatial visualization and using scalable vector graphics (SVG), AJAX, RSS and GeoRSS we have built a complete thin client component set. Our component set provides a rich user experience that is completely browser based. It includes maps, standard business charts, graphs, and time-oriented components. The components are live, interactive, linked, and support real time collaboration.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4388996,no,undetermined,0
Topological Landscapes: A Terrain Metaphor for Scientific Data,"Scientific visualization and illustration tools are designed to help people understand the structure and complexity of scientific data with images that are as informative and intuitive as possible. In this context the use of metaphors plays an important role since they make complex information easily accessible by using commonly known concepts. In this paper we propose a new metaphor, called ""topological landscapes,"" which facilitates understanding the topological structure of scalar functions. The basic idea is to construct a terrain with the same topology as a given dataset and to display the terrain as an easily understood representation of the actual input data. In this projection from an n-dimensional scalar function to a two-dimensional (2D) model we preserve function values of critical points, the persistence (function span) of topological features, and one possible additional metric property (in our examples volume). By displaying this topologically equivalent landscape together with the original data we harness the natural human proficiency in understanding terrain topography and make complex topological information easily accessible.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376169,no,undetermined,0
Toward an Integration of the Fuzzy Logic and MCDA to GIS: Application to the Project of the Localization of a Site for the Implantation of Chemical Products Factory,"The implementation of a new urban infrastructure causes more and more conflicts mainly because of the correlation of actors coming often from opposed different domains, such as the political, the social, the economic, and the spatial one. To help the decision makers in the territory management, several actors have shown the adequacy of the association of the geographical information systems (GIS) and the multicriterion decision aid (MCDA) methods. This association not only permits to manage the spatial reference information but also to apply new analysis methods permitting to have the most pertinent and most profitable information. However, most problems of MCDA take into account not only the traditional quantitative criteria, but also the qualitative and imprecise criteria, which make their use difficult for the decision and analysis. We propose in this paper, a hybrid model of MCDA for the localization of a site for the implantation of a chemical products factory. This model enables us to combine the use of the multi-representation geographical information systems (MRGIS) and the MCDA methods, such as AHP (analytic hierarchy process). Then, to facilitate the interpretation of the qualitative data in our model, we propose an approach of data modelling based on the fuzzy set theory, which makes it possible to interpret the qualitative values by precise values rather than by classical intervals.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218399,no,undetermined,0
Toward the Realization of Policy-Oriented Enterprise Management,"A goal-driven approach to business process composition uses generic, logic-based strategies, descriptions of Web services, and formalized business policies to generate business processes that satisfy the stated business goals. The approach is based on an enterprise physics metaphor, in which business objects are analogous to physical objects and policies are analogous to physical laws. Medium and large businesses use traditional enterprise software systems to manage diverse operations in modules that are part of a unified software reflection (that is, how the software represents the enterprise's organizational structure). Each of these modules typically provides support for an entire business department. Examples of such modules include 1) a business intelligence suite (information warehouse and analytics), 2) customer relationship management, 3) supply chain management, and 4) enterprise resource planning.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385258,no,undetermined,0
Unified Volumes for Light Shaft and Shadow with Scattering,"It is a challenge work to render natural lighting phenomena in real-time. A major reason is due to high computational expense to simulate the physical model of atmosphere scattering. Another is due to the lack of power and programmability in the graphic hardware. In this paper, we propose unified volumes representation for light shaft and shadow, which is an efficient method of simulating natural light shafts and shadows with atmospheric scattering effect. We give the analytic formula of light shaft without numerical integration and then make use of the current graphic hardware to implement the integral computation on each volume surface for scattering. Our approach can not only simulate the lighting effect with single light source but also multiple parallel light sources according to the physical model of skylight and sunlight. With acceleration of the GPU, we can generate realistic appearance with high frame rate satisfying real time application. It can possibly be used in current commercial game or other virtual reality systems.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407874,no,undetermined,0
Using genetic algorithms for Spectrally Modulated Spectrally Encoded waveform design,"A genetic algorithm (GA) is used to design Spectrally Modulated, Spectrally Encoded (SMSE) waveforms while characterizing the impact of parametric variation on coexistence. As recently proposed, the SMSE framework supports cognition-based, software defined radio (SDR) applications and is well-suited for coexistence analysis. For initial proof-of-concept, two SMSE waveform parameters (number of carriers and carrier bandwidth) are optimized in a coexistent scenario to characterize SMSE impact on Direct Sequence Spread Spectrum (DSSS) bit error performance. Given optimization via GA techniques have been successfully applied in many engineering fields, as well as operations research, they are viable candidates for robust SMSE waveform design. As demonstrated, the analytic SMSE framework is well-suited for parametric optimization via GA techniques.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4339423,no,undetermined,0
VAST 2007 Contest Interactive Poster: Data Analysis Using NdCore and REGGAE,"ATS intelligent discovery analyzed the VAST 2007 contest data set using two of its proprietary applications, NdCore and REGGAE (relationship generating graph analysis engine). The paper describes these tools and how they were used to discover the contest's scenarios of wildlife law enforcement, endangered species issues, and ecoterrorism.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389016,no,undetermined,0
Mapping Applications to Tiled Multiprocessor Embedded Systems,"Modern multiprocessor embedded systems execute a large number of tasks on shared processors and handle their complex communications on shared communication networks. Traditional methods from the HW /SW codesign or general purpose computing domain cannot be applied any more to cope with this new class of complex systems. To overcome this problem, a framework called Distributed Operation Layer (DOL) is proposed that enables the efficient execution of parallel applications on multiprocessor platforms. Two main services are offered by the DOL: systemlevel performance analysis and multi-objective algorithmarchitecture mapping. This paper presents the basic principles of the DOL, the specification mechanisms for applications, platform and mapping as well as its internal analytic performance evaluation framework. To illustrate the presented concepts, an MPEG -2 decoder case study is presented.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276262,no,undetermined,0
Video verification of point of sale transactions,"Loss prevention is a significant challenge in retail enterprises. A significant percentage of this loss occurs at point of sale (POS) terminals. POS data mining tools known collectively as exception based reporting (EBR) are helping retailers, but they have limitations as they can only work statistically on trends and anomalies in digital POS data. By applying video analytics techniques to POS transactions, it is possible to detect fraudulent or anomalous activity at the level of individual transactions. Very specific fraudulent behaviors that cannot be detected via POS data alone become clear when combined with video-derived data. ObjectVideo, a provider of intelligent video software, has produced a system called RetailWatch that combines POS information with video data to create a unique loss prevention tool. This paper describes the system architecture, algorithmic approach, and capabilities of the system, together with a customer case-study illustrating the results and effectiveness of the system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425346,no,undetermined,0
Visual Analytics for Requirements-driven Risk Assessment,"Risk assessment is a complex decision making process during certification and accreditation (C&A) activities. It requires to understand the multidimensional correlations among numerous C&A requirements to reason about their collective and adequate behavior to minimize risks to a software system. Also, diverse stakeholders in the organizational hierarchy should be able to comprehend and utilize the risk assessment artifacts to agree upon an acceptable level of risks and justify the criticality and cost of mitigation strategies related to C&A requirements. We believe requirements visualization plays an important role in providing rich contextual information for understanding and analyzing risk assessment artifacts and present our initial experiences in using intuitive visual metaphors and their explanations for requirements-driven risk assessment.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4473006,no,undetermined,0
Wireless Temporal-Spatial Human Mobility Analysis Using Real-Time Three Dimensional Acceleration Data,"It is estimated that by the year 2010, the number of people over 65 years of age will reach 39 million. By 2030, that number is expected to over 70 million individuals in the 85 and older age group making them the fastest growing group of older individuals. Their growth rate is three times more than all of the 65 and older age groups put together. Falls and mobility issues are very common among older individuals and can have severe consequences. In older individuals, falls and mobility issues can occur as a result of normal age related changes such as changes in vision, gait, strength, disease progression, and medication. We introduce wireless microcontroller hardware and software that leverages micro electro mechanical system (MEMS) transducers which communicate with a server-based heuristics analytics system. The real-time heuristics analytics system performs correlation analysis which will allow for measuring and detecting of mobility related events correlated with, for example, specific disease progression and/or changes in medication dosing and scheduling. If there is excessive inactivity detected within a selected time period, notification will be sent by the heuristics analytics system to a response center.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4137056,no,undetermined,0
<?Pub Dtl?>A Consideration of the Use of Plagiarism Tools for Automated Student Assessment,"In this paper, the authors evaluate the flexibility and richness of two well-established text analysis plagiarism tools, through a consideration of the use of plagiarism detection software as a mechanism for the automated assessment of student-created narrative in a virtual learning environment (VLE). The authors are currently engaged in a project creating a prototype VLE, using technologies for multilevel and multiplayer games, based on the inherent support such an environment would provide for constructivist learning, engagement, and contextual socialization. Progress between levels in the VLE will be based on the creation, by the student, of a narrative linking together a number of conceptual elements obtained through game-play at that level. Support for the narrative creation process will help the student to contextualize the conceptual elements, providing the necessary linking elements or themes to enable the student to produce a coherent description of their understanding of the concepts. A particular challenge in such environments is the need for fast, real-time feedback to students to maintain the level of engagement and to support the game-play metaphor. Additionally, the student must be able to make as many attempts to progress as they need and it will be their decision when and how often to submit for assessment. Since the student narrative will be in a textual form and can therefore be related to a sample solution narrative, generated by the author of the level within the learning environment, the idea of using plagiarism detection software as the mechanism for automated comparison and assessment was considered appropriate for investigation. While the limitation of such tools would appear to be that they are seeking direct copies of text elements, the authors wanted to investigate whether they offered sufficient richness and fuzziness to detect common conceptually-linked texts. The initial decision was to experiment with text-analytic tools, since they a- re both widely used and readily available. The tools chosen were TurnItIn, a commercial tool provided to the U.K. higher education community by the U.K. Joint Information Systems Committee (JISC), and VALT/VAST, a set of tools created at the Centre for Interactive Systems Engineering at London South Bank University, London, U.K., the workings of which are based on recognized and well-published research. An experiment using a small group of students in a traditional assessment situation was carried out, and is described in detail. The rationale for this approach was that there is not yet a fully working prototype of the VLE in which to carry out such an experiment, but that the conditions necessary to test the hypothesis that plagiarism tools could be utilized for such a purpose could be replicated sufficiently to make such an experiment viable. The results of the experiment demonstrated neither a correlation between the sample solution and student solutions, nor any correlation between the individual student solutions, proving the null hypothesis. This result demonstrates that these tools are not useful for the development of automated assessment within the VLE, and the authors are now giving consideration to the use of lexical analysis/tokenizer and other tools. However, it also suggests that these text-analytic plagiarism tools are too firmly focused on direct copy, which does raise the question of whether or not they offer enough richness and fuzziness to detect a sophisticated plagiarism attempt using, for example, text replacement tools. An ongoing close relationship between research in automated assessment and plagiarism detection is also proposed, to achieve mutual benefit.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4472096,no,undetermined,0
A Case Study of Hardware/Software Partitioning of Traffic Simulation on the Cray XD1,"Scientific application kernels mapped to reconfigurable hardware have been reported to have 10times to 100times speedup over equivalent software. These promising results suggest that reconfigurable logic might offer significant speedup on applications in science and engineering. To accurately assess the benefit of hardware acceleration on scientific applications, however, it is necessary to consider the entire application including software components as well as the accelerated kernels. Aspects to be considered include alternative methods of hardware/software partitioning, communications costs, and opportunities for concurrent computation between software and hardware. Analysis of these factors is beyond the scope of current automatic parallelizing compilers. In this paper, a case study is presented in which a simulation of metropolitan road traffic networks is mapped onto a reconfigurable supercomputer, the Cray XD1. Five different methods are presented for mapping the application onto the combined hardware/software system. An approach for approximating the performance of each method is derived through analytic equations. Our results, both analytically and empirically, show that key predictors of performance (which are often not considered in reported speedup of kernel operations) are not necessarily maximum parallelism, but must account for the fraction of the problem that runs on the reconfigurable logic and the amount data flow between software and hardware.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407544,no,undetermined,0
A Description of a Highly Modular System for the Emergent Flood Prediction,"The main goal of our system is to provide the end user with information about an approaching disaster. The concept is to ensure information access to adequate data for all potential users, including citizens, local mayors, governments, and specialists, within one system. It is obvious that there is a knowledge gap between the lay user and specialist. Therefore, the system must be able to provide this information in a simple format for the less informed user while providing more complete information with computation adjustment and parameterization options to more qualified users. Important feature is the open structure and modular architecture that enables the usage of different modules. Modules can contain different functions, alternative simulations or additional features. Since the architectural structure is open, modules can be combined in any way to achieve any desired function in the system. One of many important modules is our own analytic solution to the flood waves for a small basin to our system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4557866,no,undetermined,0
A Framework to Evaluate and Predict Performances in Virtual Machines Environment,"Virtualization technology becomes more and more important in area of compute science, such as data center and server consolidation. A large number of hypervisors are available to manage the virtualization either on bare hardware or on host operating systems. One of the important task for the designer is to measure and compare the performance overhead of given virtual machines. In this paper, we provide an analytic framework for the performance analyzing either without running a system or in a runnable real system. Meanwhile, analytic performance models that are based on the queue network theory are developed to study the designs of virtual machines. At the end, a case study of the mathematical models is given to illustrate the performance evaluation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4755255,no,undetermined,0
A Hilbert warping method for camera-based finger-writing recognition,"We propose a time-warping algorithm for recognizing finger actions by a camera. In the proposed method, an input image sequence is aligned to the reference sequences by phase-synchronization of the analytic signals, and then classified by comparing the cumulative distances. A major benefit of this method is that over-fitting to sequences of incorrect categories is restricted. The proposed method exhibited high recognition accuracy in finger-writing character recognition.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761380,no,undetermined,0
A Parallel Algorithm for Block Tridiagonal Systems,"A parallel algorithm, namely parallel block diagonal dominant (PBDD) algorithm, is proposed to solve block tridiagonal linear systems on multi-computers. This algorithm is based on divided-and-conquer idea of the PDD method. When the systems is strictly block diagonal dominant, the PBDD is highly parallel and provides approximate solutions that equals to the exact solutions within machine accuracy. The PBDD method has been implemented on a 64-node multi-computer. The analytic results match closely with the results measured from the numerical experiments.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4710962,no,undetermined,0
Study on Performance of Flexure Hinge Mechanism Based on the EBM,"The basic principle of the equivalent beam methodology (EBM) is introduced and the performance of a double parallel four-bar flexure hinge mechanism is discussed. First, the displacement of this mechanism is theoretically analyzed. Then the displacements of mechanism are studied with the EBM and the conventional finite element method (CFEM) in software ANSYS. Compared with the analytic result of the CFEM, the EBM has the advantage of high efficiency with a low number of elements, which is very important for the simulations of micro-positioning stage with complex flexure hinge mechanism.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4351206,no,undetermined,0
Stochastic Analysis and Improvement of the Reliability of DHT-Based Multicast,"This paper investigates the reliability of application-level multicast based on a distributed hash table (DHT) in a highly dynamic network. Using a node residual lifetime model, we derive the stationary end-to-end delivery ratio of data streaming between a pair of nodes in the worst case, and show through numerical examples that in a practical DHT network, this ratio can be very low (e.g., less than 50%). Leveraging the property of heavy-tailed lifetime distribution, we then consider three optimizing techniques, namely senior member overlay (SMO), longer-lived neighbor selection (LNS), and reliable route selection (RRS), and present quantitative analysis of data delivery reliability under these schemes. In particular, we discuss the tradeoff between delivery ratio and the load imbalance among nodes. Simulation experiments are also used to evaluate the multicast performance under practical settings. Our model and analytic results provide useful tools for reliability analysis for other overlay-based applications (e.g., those involving persistent data transfers).",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4215836,no,undetermined,0
Solving Constraints on the Intermediate Result of Decimal Floating-Point Operations,"The draft revision of the IEEE Standard for Floating- Point Arithmetic (IEEE P754) includes a definition for decimal floating-point (FP) in addition to the widely used binary FP specification. The decimal standard raises new concerns with regard to the verification of hardware- and software-based designs. The verification process normally emphasizes intricate corner cases and uncommon events. The decimal format introduces several new classes of such events in addition to those characteristic of binary FP. Our work addresses the following problem: Given a decimal floating-point operation, a constraint on the intermediate result, and a constraint on the representation selected for the result, find random inputs for the operation that yield an intermediate result compatible with these specifications. The paper supplies efficient analytic solutions for addition and for some cases of multiplication and division. We provide probabilistic algorithms for the remaining cases. These algorithms prove to be efficient in the actual implementation.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272849,no,undetermined,0
Simulation 101 software: Workshop and beyond,"The C source code associated with the Simulation 101 pre-conference workshop (offered at the 2006 and 2007 Winter Simulation Conferences) is presented here. This paper begins with general instructions for downloading, compiling, and executing the software. This is followed by sections on four groups of the software, categorized by functionality: libraries, Monte Carlo simulations, discrete-event simulations, and utilities. The libraries contain code to generate random numbers, code to generate random variates, and code to evaluate probability density functions, cumulative distribution functions, and inverse distribution functions. The Monte Carlo simulations consist of six programs that estimate various probabilities associated with simple probability problems, some with known analytic solutions and others without analytic solutions. The discrete-event simulations consist of various applications from queueing and inventory systems. Finally, the utilities are used to calculate various point and interval estimators from data sets.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419606,no,undetermined,0
Middleware and Performance Issues for Computational Finance Applications on Blue Gene/L,"We discuss real-world case studies involving the implementation of a Web services middleware tier for the IBM Blue Gene/L supercomputer to support financial business applications. These programs that are representative of a class of modern financial analytics that take part in distributed business workflows and are heavily database-centric with input and output data stored in external SQL data warehouses. We describe the design issues related to the development of our middleware tier that provides a number of core features, including an automated SQL data extraction and staging gateway, a standardized high-level job specification schema, a well-defined Web services (SOAP) API for interoperability with other applications, and a secure HTML/JSP Web-based interface suitable for general users. Further, we provide observations on performance optimizations to support the relevant data movement requirements.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228289,no,undetermined,0
NEDAT - A Network Engineering Design Analytic Toolset to Design and Analyze Large Scale MANETs,"Future battlefield networks such as FCS and WIN-T will rely on mobile ad hoc networks (MANETs) to satisfy their communications requirements. Thus there is a critical need for systematic techniques based on formal methodologies to design MANETs to meet mission requirements. However, given the complexity of MANETs, the variety of applications they need to support, and the associated performance measures, it is extremely challenging to perform such an engineering design of MANETs. The challenges stem both from the large number of design choices that can be made for any given mission, and, more importantly, from the lack of a systematic body of principles that a network designer can rely upon to guide these choices. To address this critical need, a joint effort has been initiated between Telcordia and CERDEC to formulate a Network Engineering Design Analytic Toolset (NEDAT) that applies formal network science-based approaches to systematically and accurately design, analyze and predict the performance of MANETs. In this paper, we present the results from the first phase of this joint effort.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4454956,no,undetermined,0
News: A Structure for Unstructured Data Search,"The Unstructured Information Management Architecture is a software development framework developed at IBM to help realize the value of unstructured data search. IBM made UIMA open source in mid-2005 to encourage development of domain analytics. In early December 2006, IBM joined some of the technology industry's other leading governmental and academic sector players to form the UIMA Technical Committee at the Organization for the Advancement of Structured Information Standards. The UIMA framework was transferred to the Apache Foundation's incubator concurrent with the formation of the OASIS committee.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069218,no,undetermined,0
Notice of Retraction<BR>A Digital Diagnosis Instrument of Hess Screen for Paralytic Strabismus,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>The diagnose instrument of Hess screen is an important instrument for ophthalmic diagnoses. It is mainly used for the measurement of paralytic strabismus. This article is about a digital diagnosis instrument of Hess screen for paralytic strabismus, using image manipulation technology and software technology, based on method of making a traditional Hess screen, medical theory of Hess screen diagnosis and oculomotor muscle constitution. This instrument is composed by diagnosis system of Hess screen for strabismus, LCD screen, computer, and red/blue goggles. The diagnosis system of Hess screen for strabismus is consist of automated Hess screen check up and assistant Hess screen check up, the management system of medical record and the result analytic system. It provides integration function of diagnosis of strabismus, data record and analyze. The instrument uses 20 in LCD screen to display and JAVA to implement the function. The clinical test in some hospitals shows: this instrument is convenience, nicety, quick, no hurt, and provides contrast diagnose medical record and after operation medical record. In the end of the article shows analysis of instrument's error based on the clinical data.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272802,no,undetermined,0
Observations on new developments in composability and multi-resolution modeling,"MRM (MRM) and Composability are two of the most challenging topics in M&S. They are also related. In this paper, which was written to set the stage for conference discussion of related papers, we discuss how addressing the MRM challenge is sometimes a necessary - although not sufficient - step towards solving the composability challenge. This paper summarizes recent developments in theory drawing distinctions among issues of syntax, semantics, pragmatics, assumptions, and validity. The paper then discusses how technology for ontology development may be useful in improving both composability and MRM. Two examples illustrate how some of the issues arise. One involves a large analytic war gaming system from the past; the other involves current counter-terrorism modeling in which many of the complications are due to the social- science nature of the problem area.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419682,no,undetermined,0
On Determinism in Event-Triggered Distributed Systems with Time Synchronization,"We study event processing in locally distributed realtime systems. The objective is to use event-triggered communication together with a time-synchronization protocol, in particular. IEEE 1588 over Ethernet, to achieve the similar level of determinism as in statically scheduled time-triggered systems. Given a distributed application with component properties and input event rate characterization, we discuss an analytic procedure that bounds performance parameters. These parameters are also necessary for deterministic implementation of the application. The procedure is experimentally evaluated on a setup with standard software and networking components.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383774,no,undetermined,0
On the relative value of local scheduling versus routing in parallel server systems,"We consider a system with a dispatcher and several identical servers in parallel. Task processing times are known upon arrival. We first study the impact of the local scheduling policy at a server. To this end, we study random routing followed by a priority scheme at each server. Our numerical results show that the performance (mean waiting time) of such a policy could be significantly better than the best known suggested policies that use FCFS at each server. We then propose to use multi-layered round robin routing, which is shown to further improve system performance. Our analysis involves a combination of comparing analytic models, heavy traffic asymptotic and numerical work.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4447751,no,undetermined,0
Open Architecture Software Design for Online Spindle Health Monitoring,"This paper presents an open systems architecture-based software design for an online spindle health monitoring system. The software is implemented using the graphical programming language of LabVIEW, and presents the spindle health status in two types of windows: simplified spindle condition display and warning window for standard machine operators (operator window) and advanced diagnosis window for machine experts (expert window). The capability of effective and efficient spindle defect detection and localization has been realized using the analytic wavelet-based envelope spectrum algorithm. The software provides a user-friendly human-machine interface and contributes directly to the development of a new generation of smart machine tools.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258268,no,undetermined,0
Partial Discharge Pulse Propagation in Shielded Power Cable and Implications for Detection Sensitivity,"Boggs and Stone (1982) defined the fundamental limits to the electrical detection of corona and partial discharge (PD), i.e., wideband detection of a PD-induced pulse in the presence of thermal noise. This paper treated the effect of frequency-dependent attenuation in shielded power cable in that context. However, most of the plots in that paper were the result of numerical computations. In the same year, Stone and Boggs set out a theory for high-frequency attenuation of shielded power cable. They showed good agreement between attenuation predicted from measured material properties and measured, high-frequency attenuation of shielded power cable. Since 1982, measurements of high-frequency cable attenuation have been reported by a number of authors for a variety of cables. In addition, software tools have become available that facilitate an analytic solution for the parameters of interest. This article summarizes the theory for PD propagation in shielded power cable for both symmetric (Gaussian) and asymmetric PD-pulse waveforms, based on the assumption that the attenuation constant (dB/m or Nepers/m) of the cable is proportional to frequency. This appears to be the most complete possible analytic exposition of PD attenuation in shielded-power cable, which has obvious applications to field PD measurements of such cable.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389974,no,undetermined,0
Performability Models for Multi-Server Systems with High-Variance Repair Durations,"We consider cluster systems with multiple nodes where each server is prone to run tasks at a degraded level of service due to some software or hardware fault. The cluster serves tasks generated by remote clients, which are potentially queued at a dispatcher. We present an analytic queueing model of such systems, represented as an M/MMPP/1 queue, and derive and analyze exact numerical solutions for the mean and tail-probabilities of the queue-length distribution. The analysis shows that the distribution of the repair time is critical for these performability metrics. Additionally, in the case of high-variance repair times, the model reveals so-called blow-up points, at which the performance characteristics change dramatically. Since this blowup behavior is sensitive to a change in model parameters, it is critical for system designers to be aware of the conditions under which it occurs. Finally, we present simulation results that demonstrate the robustness of this qualitative blow-up behavior towards several model variations.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273028,no,undetermined,0
Performance Modeling of Integrated Mobile Prepaid Services,"Prepaid personal communication service users have outnumbered postpaid users. This paper studies the charging issues of an integrated Global System for Mobile Communications (GSM) and General Packet Radio Service (GPRS) prepaid service, where a single prepaid account provides a user both voice and data services. The call setup and charging procedures for GSM and GPRS are presented using the Customized Applications for Mobile network Enhanced Logic network architecture. To reduce the probability of terminating both ongoing voice and data calls, we suggest that no new call be admitted when the user credit is below a threshold. An analytic model has been developed to evaluate the performance of the approach. Computer simulations have also been used to verify the results. The numeric results indicate that the force-termination probability can be significantly reduced by choosing an appropriate threshold of the user credit",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4138052,no,undetermined,0
Research of AHP-SOM Model and Its Application,"Decision-making in the field of information systems has become more complex due to a larger number of alternatives, multiple and sometimes conflicting goals. In this paper we explore the appropriateness of Analytic Hierarchy Process (AHP) to support construction project bid decision-making. Since AHP can be applied if the decision problem includes multiple objectives, conflicting criteria, incommensurable units, and aims at selecting an alternative from a known set of alternatives. And Self-organizing feature map network (SOM) network can classify input models automatically by learning rules. That is, under the circumstance of no teachers, through repeated studies, it can capture all features of input data and organize them automatically. This paper attempts to establish a link between these two approaches. We combine AHP with SOM approach, explore and establish a new kind of evaluation model. The proposed model is applied to construction project bid system and a proof - of - concept prototype has been developed to demonstrate our model.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4365418,no,undetermined,0
Research of Project Evaluation Decision Model Based on AHP-SOM,"Firstly, to construct project bidding decision-making of complex indexes, simplifying indexes system and preprocessing of sample data are performed through analytic hierarchy process to normal data. Secondly, a decision-making model is constructed using SOFM and trained by the sample data. The preliminary experimental result demonstrates that the AHP-SOFM model obtains better visual results and the accuracy attains up to 84.6%.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370842,no,undetermined,0
Research of Software User Interface Evaluation Method based on Subjective Expectation,"The artificial neural network based subjective expectation evaluation method on software user interface is proposed. The analytic hierarchy process and radial basis functions neural network based subjective expectation evaluation models are established. The two kinds of models are used to evaluate the control system software user interface of power station. Evaluation results show that the radial basis function neural network based evaluation method can be used to evaluate the software user interface. Because the proposed method has the advantages of deciding the subjective expectation evaluation weights automatically, so it can observably enhance the objectivity of subjective expectation evaluation of software user interface.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4304072,no,undetermined,0
Research on project evaluation system based on äóìblack boxäó technology,"To assure justice and science of scientific and technological project evaluation, avoiding the corrupt transaction in the process of project evaluation, scientific and technological project evaluation management model based on ""black box"" technology is presented, and the architecture of evaluation ""black box"" is established based on artificial intelligence, experts' selection based on knowledge reasoning is analyzed, system architecture and work workflow are studied, and evaluation model of experts' performance based on analytic hierarchy process and fuzzy comprehensive evaluation is established. Based on these a prototype system is developed, results of the system running prove the correctness of theory study and feasibility of technology research. The study works provides a scientific and reliable method of scientific and technological project evaluation.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4522520,no,undetermined,0
Scalable visual reasoning: Supporting collaboration through distributed analysis,"We present a visualization environment called the scalable reasoning system (SRS) that provides a suite of tools for the collection, analysis, and dissemination of reasoning products. This environment is designed to function across multiple platforms, bringing the display of visual information and the capture of reasoning associated with that information to both mobile and desktop clients. The service-oriented architecture of SRS facilitates collaboration and interaction between users regardless of their location or platform. Visualization services allow data processing to be centralized and analysis results to be collected from distributed clients in real time. We use the concept of ldquoreasoning artifactsrdquo to capture the analytic value attached to individual pieces of information and collections thereof, helping to fuse the foraging and sense-making loops in information analysis. Reasoning structures composed of these artifacts can be shared across platforms while maintaining references to the analytic activity (such as interactive visualization) that produced them.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4621734,no,undetermined,0
Self-consistent analytic scattering theory for apertureless THz near-field microscope,"We propose a self-consistent analytic scattering theory for apertureless THz near-field microscope (NFM) which is based on an exact image theory. In this new scattering theory, a quasi-electrostatic image theory is adopted to include the effects of the specular reflection of the incident field and the exact image dipoles induced in the probe tip and the sample substrate. The analytic results from our self-consistent theory are in good agreement with the simulation results obtained by using HFSS, a commercial software based on finite element method.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4516599,no,undetermined,0
Sense-and-respond supply chain using model-driven techniques,"The results of an effort to build a sense-and-respond solution for a supply chain by using a model-driven development framework are described in this paper. One of the components of the framework is the IBM Research-developed model-driven business-transformation (MDBT) toolkit, a set of formal models, methods, and tools. The inventory optimization analytics used to improve supply chain performance are also described. This approach is illustrated through a case study involving the IBM System xä‹¢ supply chain.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5386582,no,undetermined,0
SIFT - A Component-Based Integration Architecture for Enterprise Analytics,"Architectures and technologies for enterprise application integration are relatively mature, resulting in a range of standards-based and proprietary COTS middleware technologies. However, in the domain of complex analytical applications, integration architectures are not so well understood. Analytical applications such as those used in scientific discovery and financial and intelligence analysis exert unique demands on their underlying architectures. These demands make existing COTS integration middleware less suitable for use in enterprise analytics environments. In this paper we describe SIFT (Scalable Information Fusion and Triage), an application architecture designed for integrating the various components that comprise enterprise analytics applications. SIFT exploits a common pattern for composing analytical components, and extends an existing messaging platform with dynamic configuration mechanisms and scaling capabilities. We demonstrate the use of SIFT to create a decision support platform for quality control based on large volumes of incoming delivery data. The strengths and weaknesses of the SIFT solution are discussed, and we conclude by describing where further work is required to create a complete solution applicable to a wide range of analytical application domains",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127303,no,undetermined,0
Design and implementation of honeypot systems based on open-source software,"A honeypot is a type of information system that is used to obtain information on intruders in a network. When a honeypot is deployed in front of a firewall, it can serve as an early warning system. When deployed behind the firewall, it can serve as part of a defense-in-depth system and can be used to detect attackers who bypass the firewall and the intrusion detection system (IDS) or threats from insiders. Honeyd is an open-source honeypot; however, it uses a command-line interface and its configuration is difficult for beginners. The purpose of this study is to use the open-source tool to construct a graphic user interface (GUI) for honeyd. For the sake of portability and easy deployment, the whole system will be installed in a live USB stick. The end user can create a honeyd template by using the GUI or the result of the Nmap scan of a target computer. Moreover, the system will provide a log-review interface and real-time SMS functionality. Finally, we deployed the designed system in a campus network and presented an analytic result of a 60-day period with a Web-based data analysis system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4565077,no,undetermined,0
Interactive data analysis with nSpace2<sup>Œ¬</sup>,"nSpace2 is an innovative visual analytics tool that was the primary platform used to search, evaluate, and organize the data in the VAST 2011 Mini Challenge #3 dataset. nSpace2 is a web-based tool that is designed to facilitate the back-and-forth flow of the multiple steps of an analysis process, including search, data triage, organization, sense-making, and reporting. This paper describes how nSpace2 was used to assist every step of the analysis process for this VAST challenge.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102497,no,undetermined,0
"The Practitioner's Cycles, Part 2: Solving Envisioned World Problems","Previous Human-Centered Computing department articles have reflected on the mismatch that can occur between the promise of intelligent technology and the results of technological interventions. Part 1 on the Practitioner's Cycles illustrated ways in which actual world problems-the forces and constraints of procurement-are at odds with the goals of human centering. This article culminated in a practitioner's tale, in which individuals acted on their own initiative and at their own risk, short-circuiting the rules and constraints that limit success at procurement. This paper presents a model based on the tale and focuses on how the model applies to envisioned world problems-the creation of intelligent technologies for new work systems.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5475081,no,undetermined,0
A comparison of alerting strategies for hemorrhage identification during prehospital emergency transport,"Early and accurate identification of physiological abnormalities is one feature of intelligent decision support. The ideal analytic strategy for identifying pathological states would be highly sensitive and highly specific, with minimal latency. In the field of manufacturing, there are well-established analytic strategies for statistical process control, whereby aberrancies in a manufacturing process are detected by monitoring and analyzing the process output. These include simple thresholding, the sequential probability ratio test (SPRT), risk-adjusted SPRT, and the cumulative sum method. In this report, we applied these strategies to continuously monitored prehospital vital-sign data from trauma patients during their helicopter transport to level I trauma centers, seeking to determine whether one strategy would be superior. We found that different configurations of each alerting strategy yielded widely different performances in terms of sensitivity, specificity, and average time to alert. Yet, comparing the different investigational analytic strategies, we observed substantial overlap among their different configurations, without any one analytic strategy yielding distinctly superior performance. In conclusion, performance did not depend as much on the specific analytic strategy as much as the configuration of each strategy. This implies that any analytic strategy must be carefully configured to yield the optimal performance (i.e., the optimal balance between sensitivity, specificity, and latency) for a specific use case. Conversely, this also implies that an alerting strategy optimized for one use case (e.g., long prehospital transport times) may not necessarily yield performance data that are optimized for another clinical application (e.g., short prehospital transport times, intensive care units, etc.).",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944172,no,undetermined,0
Visual Analytics for Big Data Using R,"The growth in volumes of data has affected today's large organization, where commonly used software tools to capture, manage, and process the data cannot handle big data effectively. The main challenge is that organizations must analyze a large amount of big data and extract useful information or knowledge for future actions in a short time. This type of demands has produced the markets for various innovative big data control mechanisms, such as visual analytics for big data. In this paper, we propose to visually analyze the big data using R statistical software. The proposed method is composed of three steps. In the first step, we extract the data set from the target Web site. In the second step, we parse the extracted raw data according to the types, and store in a database. In the third, we perform visual analysis from the stored data in database using R statistical software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686088,no,undetermined,0
Visual Analytics for Model Selection in Time Series Analysis,"Model selection in time series analysis is a challenging task for domain experts in many application areas such as epidemiology, economy, or environmental sciences. The methodology used for this task demands a close combination of human judgement and automated computation. However, statistical software tools do not adequately support this combination through interactive visual interfaces. We propose a Visual Analytics process to guide domain experts in this task. For this purpose, we developed the TiMoVA prototype that implements this process based on user stories and iterative expert feedback on user experience. The prototype was evaluated by usage scenarios with an example dataset from epidemiology and interviews with two external domain experts in statistics. The insights from the experts' feedback and the usage scenarios show that TiMoVA is able to support domain experts in model selection tasks through interactive visual interfaces with short feedback cycles.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634112,no,undetermined,0
Visual analytics for software requirements engineering,"The research on visual analytics for requirements engineering has noticeably advanced in the past few years. For many software projects, requirements management needs an effective and efficient path from data to decision. Visual analytics (VA) creates such a path that enables the user to extract insights by interacting with the relevant information. While various requirements visualization techniques exist, only few have produced end-to-end values to practitioners. In this research proposal, we advance the literature on visual requirements analytics by characterizing its key components and relationships. Such a characterization allows us to not only assess existing approaches, but also develop tool enhancements in a principled manner. We describe our ongoing work on VA and outline future research plans.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636762,no,undetermined,0
Visual Analytics Infrastructures: From Data Management to Exploration,"Analysts exploring big data require more from information visualization, data analysis, and data management than these components can now deliver. New infrastructures must address the nature of exploration as well as data scale. The Web extra at http://youtu.be/K9PvskathGI is a video segment that gives an overview of how research in visual analytics can help tackle the challenges of managing and interpreting big data in various domains.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6488679,no,undetermined,0
Visual Analytics Model for Intrusion Detection in Flood Attack,"Flood attacks are common forms of Distributed Denial-of-Service (DDoS) attack threats on internet in nature. This has necessitated the need for visual analysis within an intrusion detection system to identify these attacks. The challenges are how to increase the accuracy of detection and how to visualize and present flood attacks in networks for early detection. In this paper, we introduce three coefficients, which not only classify the behaviors of flood attacks, but also measure the system performance under those flood attacks: a) attack-density that patterns the characters of flood attack, b) system workload which represents the system capability in handling flood attack and c) the scalability to classify the impact level of the flood attack at victim site. A visual clustered method is used to display the DDoS flood attacks. The experimentation results are presented to demonstrate our new model significantly improves the accuracy of the detection of DDoS attacks and provides a better understanding of the nature of flood attacks on networks.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680852,no,undetermined,0
Visualizing geolocation of spam email,"With the recent surge in cyber attacks, there is a growing demand for effective security analytics tools. Though, there are advanced data collection techniques in the form of honeypots and malware collectors, the value of data are only as useful as the analysis technique used. One of the primary drawbacks of current security analytic tools is the lack of visualization controls to effectively analyze the data. In this paper, we develop a visualization tool to analyze the geographical locations of spammers based on the integration of MaxMind and WhoIS databases with Google Maps API. The visualization tool provides an insight into spam origins, along with patterns of spammers identified from spam activity. A key component in the development of this tool is its extensible framework allowing for the addition of resources to retrieve more information about a spammer and analyze additional patterns of spammers for spam analysis.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6533610,no,undetermined,0
Web analytics in e-learning: Agent-based and neural network approaches,"Current research explores web analytics tools in the field of electronic learning using agent-based and neural network approaches. A hybrid agent-based model with built-in artificial neural networks is proposed. The model aims to support the computer simulation experiment assessment of knowledge production and knowledge dissemination trends among the agents of three types: authors, tutors and students of on-line courses. In the research we study the effectiveness of using the software to implement this model as an example in one of the higher education institutions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6663030,no,undetermined,0
Where is the business logic?,"One of the challenges in maintaining legacy systems is to be able to locate business logic in the code, and isolate it for different purposes, including implementing requested changes, refactoring, eliminating duplication, unit testing, and extracting business logic into a rule engine. Our new idea is an iterative method to identify the business logic in the code and visualize this information to gain better understanding of the logic distribution in the code, as well as developing a domain-specific business vocabulary. This new method combines and extends several existing technologies, including search, aggregation, and visualization. We evaluated the visualization method on a large-scale application and found that it yields useful results, provided an appropriate vocabulary is available.",2013,http://dl.acm.org/citation.cfm?id=2494588&CFID=696538919&CFTOKEN=83912867,yes,undetermined,0
A Big Data Financial Information Management Architecture for Global Banking,"Global investment banks and financial institutions are facing growing data processing demands. These originate not only from increasing regulatory requirements and an expanding variety and disparity of data sources, but also from ongoing pressures in cost reduction without compromising system scalability and flexibility. In this context, the ability to apply promising state-of-the-art big data technologies to extract the maximum value from the vast amounts of the data generated is generating a lot of interest in the financial services industry. In this paper we present a Big Data architecture system design, based in open distributed computing paradigms like Hadoop map-reduce, offering horizontal scalability and no-SQL flexibility while at the same time meeting the stringent quality and resilience requirements of the banking software standards. The proposed architecture is able to consolidate, validate, enrich and process with different Big Data analytics techniques the data gathered from the different source systems as encountered in the banking practice, while at the same time supporting the different data integration, transmission and process orchestration requirements traditionally encountered in a global financial institution.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984224,no,undetermined,0
A Compliance Aware Software Defined Infrastructure,"With cloud eclipsing the $100B mark, it is clear that the main driver is no longer strictly cost savings. The focus now is to exploit the cloud for innovation, utilizing the agility to expand resources to quickly build out new designs, products, simulations and analysis. As the cloud lowers the unit cost of IT and improves agility, the time to market for applications will improve significantly. Companies will use this agility and speed as competitive advantage. An example of the agility is the adoption by enterprises of the software-defined datacenter (SDDC)[3] model, which allows for the rapid build of environments with composable infrastructures. With adoption of the SDDC model, intelligent and automated management of the SDDC is an immediate priority, required to support the changing workloads and dynamic patterns of the enterprise. Often, security and compliance become an 'after thought', bolted on later when problems arise. In this paper, we will discuss our experience in developing and deploying a centralized management system for public, as well as an Openstack [4] based cloud platform in SoftLayer, with an innovative, analytics-driven 'security compliance as a service' that constantly adjusts to varying compliance requirements based on workload, security and compliance requirements. In this paper we will also focus on techniques we have developed for capturing and replaying the previous state of a failing client virtual machine (VM) image, roll back, and then re-execute to analyze failures related to security or compliance. This technique contributes to agility, since failing VM's with security issues can quickly be analyzed and brought back online, this is often not the case with security problems, where analysis and forensics can take several days/weeks.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6930580,no,undetermined,0
Viral Marketing and Its Application in Enterprise Drive Operation,"This paper mainly develops the operation strategy of an enterprise drive based on viral marketing. Firstly, it introduces the concept, the features and successful cases of viral marketing. Next, it introduces what is Kingdee cloud drive and shows the comparative results among Kingdee cloud drive and other drives. With respect to the market positioning, this paper designs the operation strategy. Based on viral marketing, the detailed progress and reward mechanism are put forward in order to attract more users. Finally, the lottery model is developed based on the analytic network process in the reward mechanism.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6655730,no,undetermined,0
A Comprehensive Service-Oriented Innovation Support System for E-Commerce Innovation Process,"E-commerce innovations have to break through a four-phase process before adopted by industry. To improve the comprehensiveness and extensibility for innovation supporting, this paper proposes a service-oriented innovation support system which covers all phases of the innovation process, customizes for most kinds of E-commerce innovations and provides an AHP (Analytic Hierarchy Process) and entropy method combined approach for innovation evaluation. Such service-oriented system enables multi-granularity service deployment, composition and orchestration, and provides uniform interface for service providing, invoking and managing. Compared to other innovation support system, the proposed system enhances the capacity for the support of entire innovation process and extensibility for collaborative innovation. The implemented system is adopted by National Engineering Laboratory for E-commerce Technologies of China to serve as the official public service platform.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982092,no,undetermined,0
"A Domain-Driven, Generative Data Model for Big Pet Store","Generating large amounts of semantically-rich data for testing big data workflows is paramount for scalable performance benchmarking and quality assurance in modern machine-learning and analytics workloads. The most obvious use case for such a generative algorithm is in conjunction with a big data application blueprint, which can be used by developers (to test their emerging big data solutions) as well as end users (as a starting point for validating infrastructure installations, building novel applications, and learning analytics methods). We present a new domain-driven, generative data model for Big Pet Store, a big data application blueprint for the Hadoop ecosystem included in the Apache Big Top distribution. We describe the model and demonstrate its ability to generate semantically-rich data at variable scale ranging from a single machine to a large cluster. We validate the model by using the generated data to answer questions about customer locations and purchasing habits for a fictional targeted advertising campaign, a common business use case.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034765,no,undetermined,0
A Flexible Framework for Asynchronous in Situ and in Transit Analytics for Scientific Simulations,"High performance computing systems are today composed of tens of thousands of processors and deep memory hierarchies. The next generation of machines will further increase the unbalance between I/O capabilities and processing power. To reduce the pressure on I/Os, the in situ analytics paradigm proposes to process the data as closely as possible to where and when the data are produced. Processing can be embedded in the simulation code, executed asynchronously on helper cores on the same nodes, or performed in transit on staging nodes dedicated to analytics. Today, software environnements as well as usage scenarios still need to be investigated before in situ analytics become a standard practice. In this paper we introduce a framework for designing, deploying and executing in situ scenarios. Based on a component model, the scientist designs analytics workflows by first developing processing components that are next assembled in a dataflow graph through a Python script. At runtime the graph is instantiated according to the execution context, the framework taking care of deploying the application on the target architecture and coordinating the analytics workflows with the simulation execution. Component coordination, zero-copy intra-node communications or inter-nodes data transfers rely on per-node distributed daemons. We evaluate various scenarios performing in situ and in transit analytics on large molecular dynamics systems simulated with Gromacs using up to 2048 cores. We show in particular that analytics processing can be performed on the fraction of resources the simulation does not use well, resulting in a limited impact on the simulation performance (less than 9%). Our more advanced scenario combines in situ and in transit processing to compute a molecular surface based on the Quick surf algorithm.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6846463,no,undetermined,0
A framework for power saving in IoT networks,"An IoT / M2M system may support large number of battery operated devices in addition to some mains operated devices. It is important to conserve energy of these battery operated constrained devices. An IoT / M2M Gateway used in this system is an intermediate node between IoT / M2M devices and an IoT / M2M Service Platform. It enables distributed analytics and helps to reduce traffic load in the network. This gateway could be stationary or mobile. In an IoT / M2M system, it becomes important to conserve energy of this Gateway as well. This paper proposes a framework to reduce power consumption of M2M / IoT devices as well as Gateway nodes. We buffer data at IoT Application, IoT Gateways and Devices to keep devices and Gateway nodes in sleep mode as long as possible. We allow computation of the duration to buffer this data using factors such as QoS requirements, predicted pattern of future IoT / M2M messages and congestion indicators from different network nodes. This potentially also allows intelligent aggregation of IoT messages at the Gateway node. We also enhance signaling mechanisms and present software building blocks for this framework. Mesh as well as Cellular access technologies are considered here.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6968211,no,undetermined,0
A Framework for Supporting Creative Thinking in Concept Mapping,"In this paper, we present a framework associated with the development of analytic and synthetic thinking that is also related to improving learner creativity. In this framework, the whole concept map is treated as a structure, the characteristics of which the learner has to learn to improve, similarly every proposition is treated as an entity, the characteristics of which express the creativity of the learner.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901520,no,undetermined,0
A goal-based technique for requirements prioritization,"It is a well-known fact that the number of software requirements from customers usually exceeds the number of features that can be implemented within a given time and available resources. Therefore, requirements prioritization (RP) is essential during the requirements elicitation activity of the requirements engineering phase of software development. In addition, not all requirements can be implemented within one release, so RP is also essential for release planning. There are many RP techniques available in the literature, which are mostly attempting to solve a multi-criteria decision making problem. However, most of them work well on a small number of requirements, and many still suffer from different shortcomings such as scalability, uncertainty, a lot of stakeholders' time consumption, and complexity. According to many studies, none of the RP techniques can be considered the best; the best RP technique depends on the situation. In addition, most of the RP techniques don't take into account the effects of the required goals on the final alternatives' ranking. In this paper, we present a brief survey of the most popular decision making RP techniques, before presenting our new goal-based RP technique. Our goal-based RP technique is based on generating a relative weight for the requirements with respect to the identified goals by stakeholders. It is inspired from prioritization decision making techniques in an attempt to enhance reported RP problems of time consumption, scalability and complexity.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7036697,no,undetermined,0
A Hybrid Fuzzy Framework for Cloud Service Selection,"QoS-based service rating has made positive contributions to the area of service selection. Especially for Cloud service users, the right decision when choosing suitable Cloud services can help them improve user satisfaction and trading revenues. This work aims to address the issue of uncertainty in service requests, service descriptions, user and expert preferences, as well as evaluation criteria in a MCDM-based service selection procedure. A hybrid fuzzy framework for Cloud service selection is proposed, addressing the challenge using three approaches: a fuzzy-ontology-based approach for function matching and service filtering, a fuzzy AHP technique for informed criterion weighting, and, a fuzzy TOPSIS approach for service ranking.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928913,no,undetermined,0
"A Laboratory-Targeted, Data Management and Processing System for the Early Detection Research Network","The National Institutes of Health (NIH), National Cancer Institute's Early Detection Research Network (EDRN) is a cross-institutional collaborative initiative seeking to accelerate the clinical application of cancer biomarker research. Over the past decade, it has been our role, as EDRN's Informatics Center (IC), to develop a comprehensive information services infrastructure as well as a set of software tools and services to support this overall initiative. We have recently developed a novel application called the Laboratory Catalog and Archive Service (LabCAS) whose focus is to extend EDRN IC data management and processing capabilities to EDRN laboratories. By leveraging the same technologies used to manage and process NASA Earth and Planetary data sets, we offer EDRN researchers an effective way of managing their laboratory data. More specifically, LabCAS enables EDRN researchers to reliably archive their experimental data, to optionally share these data in a controlled manner with other researchers, and to gain insight into these data through highly configurable data analysis pipelines tailored to the broad range of biomarker related experiments. This particular collaboration leverages expertise from NASA's Jet Propulsion Laboratory, Vanderbilt University Medical Center, and Dartmouth Medical School, as well as builds upon existing cross-governmental collaboration between NASA and the NIH.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881914,no,undetermined,0
A Linear Performance-Breakdown Model for GPU Programming Optimization Guidance,"The use Graphic Processing Units (GPU) as computing accelerators has been. Nevertheless, writing efficient GPU programs is a difficult and time consuming task. In this paper we present the Linear Performance Breakdown Model (LBPM), an analytic model that is used to extract the breakdown of GPU kernel programs execution time into the three major components that affect its running time. The model can be used as a tool to provide guidelines to detect the performance bottlenecks. Our approach is the incorporation of three elements, the Global-to-Shared Memory Time slice, Shared-to-Private Time slice and Processing Units Time slice. These three factors are integrated into a performance model formula by applying the Normalized Least Squares Method (NLSM). The resulting coefficients are used to construct a performance breakdown graph that reveals the effects of each element in the total execution time of the kernel program. We demonstrate the results obtained with our proposed model with two common numeric routines: Single-Precision General Matrix Multiplication (SGMM) and Fast Fourier Transform (FFT), and apply the model to the results obtained from two GPU devices: A8-3870 AMD Accelerated Processing Unit (APU) and a GTX 660 Nvidia GPU.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6969440,no,undetermined,0
Vis4Heritage: Visual Analytics Approach on Grotto Wall Painting Degradations,"For preserving the grotto wall paintings and protecting these historic cultural icons from the damage and deterioration in nature environment, a visual analytics framework and a set of tools are proposed for the discovery of degradation patterns. In comparison with the traditional analysis methods that used restricted scales, our method provides users with multi-scale analytic support to study the problems on site, cave, wall and particular degradation area scales, through the application of multidimensional visualization techniques. Several case studies have been carried out using real-world wall painting data collected from a renowned World Heritage site, to verify the usability and effectiveness of the proposed method. User studies and expert reviews were also conducted through by domain experts ranging from scientists such as microenvironment researchers, archivists, geologists, chemists, to practitioners such as conservators, restorers and curators.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634172,no,undetermined,0
Video analytics for abandoned object detection and its evaluation on atom and ARM processor,"The goal of proposed method is to enhance the safety and security by identifying the abandoned objects in the environment under consideration. This paper mainly exploits some of the properties of image processing and embedded system to implement the Video Analytics based robust and simple Security System for the surveillance of environment for twenty four hours a day. Proposed method supports a human operator by automatically detecting abandoned objects and drawing operator's attention to such events. This method is based on the various methods of the image processing and pattern recognition such as Gaussian Mixture model, Absolute background subtraction, image segmentation, connected component analysis and Histogram of oriented gradient. The algorithm evaluation is done on the hardware platform viz. Friendly Arm and INTEL's IVI board. Here we find results which explain whether the accuracy and reliability can be achieved at the cost of processing power. This is done to reduce the cost of system. Abandoned object can be decided by time basis. Proposed algorithm fits for the PETS2006 dataset and works real time. We have used OpenCV as software platform.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724150,no,undetermined,0
Three dimensional finite element analysis of radial-flow impeller temperature field,"The technique of three dimensional solid element model and assembly was used to determine the temperature field of radial-flow impeller. The FEM software Cosmos was applied to analyze model system, and the precise analytic results were obtained. The results show that the analytic model can reflect the stead-state and transient temperature field characteristics of impeller directly, and thus can be worthy reference to analyze and calculate the temperature field of impeller in engineering design.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5535344,no,undetermined,0
Towards Cloud-Based Analytics-as-a-Service (CLAaaS) for Big Data Analytics in the Cloud,"Data Analytics has proven its importance in knowledge discovery and decision support in different data and application domains. Big data analytics poses a serious challenge in terms of the necessary hardware and software resources. The cloud technology today offers a promising solution to this challenge by enabling ubiquitous and scalable provisioning of the computing resources. However, there are further challenges that remain to be addressed such as the availability of the required analytic software for various application domains, estimation and subscription of necessary resources for the analytic job or workflow, management of data in the cloud, and design, verification and execution of analytic workflows. We present a taxonomy for analytic workflow systems to highlight the important features in existing systems. Based on the taxonomy and a study of the existing analytic software and systems, we propose the conceptual architecture of CLoud-based Analytics-as-a-Service (CLAaaS), a big data analytics service provisioning platform, in the cloud. We outline the features that are important for CLAaaS as a service provisioning system such as user and domain specific customization and assistance, collaboration, modular architecture for scalable deployment and Service Level Agreement.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597120,no,undetermined,0
The Handoff Performance of Mobile Wi-Fi Systems in Vehicular Networks,The Mobile Wi-Fi system using Mobile Access Points (M-APs) is a new and effective method for vehicles to access the Internet service. This paper presents an analytic model to evaluate several handoff algorithms for using M-APs in vehicular networks. The analytic results show that the best handoff strategy depends on factors such as M-AP availability or connection duration.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579342,no,undetermined,0
The Lowly API Is Ready to Step Front and Center,The API is taking on new roles and is becoming critical to important technologies such as cloud computing and to the use of both Web and mobile applications.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6583176,no,undetermined,0
Thermal analysis software design and implementation applied in line focusing solar collector,This paper introduces the implementation process of the software which can analyze the thermal performance of the line focusing solar collector. The paper focused on the mathematical model and the algorithm of the software. Heat-collecting analytic software uses least squares to solve the problem of the multiple high order equations. Finally the software is completed using the Matlab GUI.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6885459,no,undetermined,0
TimeSeer: Scagnostics for High-Dimensional Time Series,"We introduce a method (Scagnostic time series) and an application (TimeSeer) for organizing multivariate time series and for guiding interactive exploration through high-dimensional data. The method is based on nine characterizations of the 2D distributions of orthogonal pairwise projections on a set of points in multidimensional euclidean space. These characterizations include measures, such as, density, skewness, shape, outliers, and texture. Working directly with these Scagnostic measures, we can locate anomalous or interesting subseries for further analysis. Our application is designed to handle the types of doubly multivariate data series that are often found in security, financial, social, and other sectors.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200267,no,undetermined,0
Toward a Care Process Metamodel: For business intelligence healthcare monitoring solutions,"Improving care processes in healthcare institutions relies on effectively monitoring and making timely decisions for improving patient experience. Business Intelligence solutions have proven to be effective for monitoring processes in other industries. However, healthcare organizations face three challenges for implementing Business Intelligence solutions that effectively monitor care processes. First, the great variation of processes in healthcare domain makes it difficult to model them. Second, there is a gap between abstract administrative indicators and fine-grained operation-level measures of healthcare processes. Finally, it is difficult to reuse the underlying healthcare processes used for other successful solutions. In this paper, we present a Care Process Metamodel geared towards modeling healthcare processes. This metamodel (a) provides a platform for creating uniform care processes, (b) enables hierarchical care processes for modeling of composite processes as well as bridging the gap between abstract performance indicators and operation-level measures of healthcare processes, and (c) facilitates reusing the processes and the data structures required for monitoring them. This metamodel thus addresses some of the challenges for implementing successful Business Intelligence care process monitoring solutions for healthcare organizations. We also demonstrate how the Care Process Metamodel-based processes fit into an architecture, where data collected about encounters of patients can be used by stakeholders for improving the process and its execution. We use samples of cardiac-related processes to illustrate our approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602483,no,undetermined,0
Toward a scale-out data-management middleware for low-latency enterprise computing,"Emerging transactional workloads from Internet and mobile commerce require low-latency, massive-scale, and integrated data analytics to enhance user experience and to improve up-selling opportunities. These analytics require new application platforms that must be able to absorb large volumes of data, provide low-latency access to the data, and cache data objects to improve access times in distributed environments. This paper reports on recent technologies built at IBM Research to address challenges in data access latency, data ingestion, and caching in the exemplary context of an online product recommendation application. We describe three technologies related to the issues and optimizations of key-value data object store and access. First, we describe the architecture of a global secondary index to greatly improve data access latency of Hadoopä‹¢ Database (HBaseä‹¢), an open-source key-value distributed data store. Second, we present an in-memory write-ahead log feature on HBase that significantly improves write operations for high-volume data ingestion. Third, we detail an innovative distributed caching system that exploits low-latency interconnects to use hash maps of data keys on each server for local lookup, while data resides and are accessed across clustered systems. The distributed cache can achieve a 100- to 1,000-fold performance gain over many caching methods. These technologies together form some necessary building blocks for a next-generation data-centric middleware for integrated transaction and analytic workloads.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6517300,no,undetermined,0
Towards a Hybrid Approach of Primitive Cognitive Network Process and K-Means Clustering for Social Network Analysis,"Social network sites (SNSs) have been influencing the social activities of many people. Consequently, analysis of social network data may produce meaningful information for decision making. This paper represents the basic hybrid approach of Primitive Cognitive Network Process (PCNP) and classical K-Means Clustering for grouping users in social network sites (SNSs) into appropriate clusters by the similarities among users. This new method has combined the PCNP approach, which is a revised approach of the Analytic Hierarchy Process (AHP), and the K-means method for evaluating the weighted attributes influencing the similarity between users. The proposed approach can act as a friends referring function in various kinds of SNSs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682233,no,undetermined,0
Towards a knowledge base for a wind electric pumping system design,"A knowledge base tool for wind electric pumping system design has been presented. In this paper, the used approach is illustrated through the rotor component. The performance of the rotor is predicted using the axial momentum theory combined with the blade element theory. The design variables of the blade are its geometry and the airfoil aerodynamic characteristics which are introduced into the tool using the analytic model AERODAS. To validate the developed model, it is formulated as a Mixed Complementarity (Problem and is solved using the GAMS software (Generalized Algebraic Modeling System). The model validation refers to the experimental data of NREL (National Renewable Energy Laboratory) obtained using the NASA Ames wind tunnel. The validated model is then used to design a new blade for a small wind turbine which we attempt to manufacture locally.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6529708,no,undetermined,0
Towards a quality model for the evaluation of DSS based on KDD process,"A Decision Support System (DSS) based on Knowledge Discovery from Data (KDD) process is used to give confident knowledge to the final users in order to help them making right decisions. Such systems can be underused if the mined knowledge is unconfident, or if the system is hardly usable or unusable. Our target is to supply out a Quality Model (QM) ensuring a global evaluation of DSS based on KDD process (DSS/KDD). In our point of view, a QM should involve three dimensions: the evaluation of the DSS as a Software Product, as a User Interface and as a DSS. We should also take into account ISO recommendations. We intend to build a model which defines a set of criteria and allows measurement of a DSS/KDD quality evaluation using Goal-Question Method (GQM) and Analytic Hierarchy Process (AHP).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6568457,no,undetermined,0
Towards logistics service provider selection strategy using primitive cognitive network process,"Outsourcing of logistics activities is the common business practice in the enterprise. Selection of a proper logistics service provider (LSP) meeting the requirements of operation strategy of the outsourcing enterprise is a complex task, due to its significant influence on the business performance. This paper demonstrates an application of the Primitive Cognitive Network Process (P-CNP) to the LSP selection strategy considering multiple criteria and alternatives.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602547,no,undetermined,0
VAICo: Visual Analysis for Image Comparison,"Scientists, engineers, and analysts are confronted with ever larger and more complex sets of data, whose analysis poses special challenges. In many situations it is necessary to compare two or more datasets. Hence there is a need for comparative visualization tools to help analyze differences or similarities among datasets. In this paper an approach for comparative visualization for sets of images is presented. Well-established techniques for comparing images frequently place them side-by-side. A major drawback of such approaches is that they do not scale well. Other image comparison methods encode differences in images by abstract parameters like color. In this case information about the underlying image data gets lost. This paper introduces a new method for visualizing differences and similarities in large sets of images which preserves contextual information, but also allows the detailed analysis of subtle variations. Our approach identifies local changes and applies cluster analysis techniques to embed them in a hierarchy. The results of this process are then presented in an interactive web application which allows users to rapidly explore the space of differences and drill-down on particular features. We demonstrate the flexibility of our approach by applying it to multiple distinct domains.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634107,no,undetermined,0
Towards Portable Learning Analytics Dashboards,"This paper proposes a novel approach to build and deploy learning analytics dashboards in multiple learning environments. Existing learning dashboards are barely portable: once deployed on a learning platform, it requires considerable effort to deploy the dashboard elsewhere. We suggest constructing dashboards from lightweight web applications, namely widgets. Our approach allows to port dashboards with no additional cost between learning environments that implement open specifications (Open Social and Activity Streams) for data access and use widget APIs. We propose to facilitate reuse by sharing the dashboards and widgets via a centralized analytics repository.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601968,no,undetermined,0
Towards software defined ICN based edge-cloud services,"ICN deployment will be based on the grounds of saving CAPEX/OPEX and/or enabling new services. This paper makes a case for the latter leveraging, emerging technologies such as network function virtualization (NFV) and software defined networking (SDN). We propose a framework to enable ICN based service platform as virtualized network functions to enable several edge-cloud services such as enterprise applications, big data analytic, or M2M/IoT services. This platform is generic to support several ICN protocols and corresponding real-time and non-real time services leveraging ICN features such as name based routing, caching, multicasting, and flexible security techniques. As an implementation of this architecture, we discuss how a scalable network based conferencing solution can be realized over the proposed ICN platform and compare it with a peer-to-peer design through a performance analysis.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6710583,no,undetermined,0
Tracer: A Tool to Measure and Visualize Student Engagement in Writing Activities,"Learning analytic techniques are allowing the observation of complex learning activities that were hidden until now. Writing is a task in which behavioral patterns can be observed to measure the level of engagement. Previous studies relied mostly on data collected by observers. In this paper Tracer, a novel learning analytic system to visualize behavioral patterns of students while writing and measuring engagement is described. The tool combines and analyzes the information obtained from document revisions and Website logs while students work in a writing assignment and provides visualizations and measurements for the level of engagement. A user study was conducted in a software engineering course where students wrote and submitted a project proposal using Google Docs. Tracer generated a graphical view of the gauged engagement, and an engagement time for each student. The obtained results show that the engagement time gauged by Tracer was moderately correlated to those reported by the students.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601971,no,undetermined,0
Usage Control of Programs and Application Libraries in z/OS Environment in the Big Data Age,"The great volume of information is currently a reality. Processing these data, mainly in institutions with a great capacity of processing, is carried out by means of mainframes that are capable of performing operations at a great speed and on an extremely large volume of data in the Big Data age. Indirect methods of gauging the usage of software installed in the mainframe environment are used to measure the usage of products, making it difficult to obtain information to renew these products. This paper presents the z/Audit product, which offers information on the actual usage of the programs installed in the mainframe environment. The product features data extration for mining (data mining) and for presentation (analytics) of this information with open-source products.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755348,no,undetermined,0
Use of simple analytic performance models for streaming data applications deployed on diverse architectures,Modern hardware is often heterogeneous. With heterogeneity comes multiple abstraction layers that hide underlying complex systems. This complexity makes quantitative performance modeling a difficult task. Designers of high-performance streaming applications for heterogeneous systems must contend with unpredictable and often non-generalizable models to predict performance of a particular application and hardware mapping. This paper outlines a computationally simple approach that can be used to model the overall throughput and buffering needs of a streaming application on heterogeneous hardware. The model presented is based upon a hybrid maximum flow and decomposed discrete queueing model. The utility of the model is assessed using a set of real and synthetic benchmarks with model predictions compared to measured application performance.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557162,no,undetermined,0
Uses of differential-algebraic equations for trajectory planning and feedforward control of spatially two-dimensional heat transfer processes,"Tracking control design is a common task in many engineering applications, where efficient controllers commonly consist of two components. First, a feedback control law is implemented to guarantee asymptotic stability of the closed-loop system and to meet robustness requirements. Second, a feedforward control signal is usually integrated, which improves the tracking of predefined output trajectories in transient operating phases. The analytic computation of this feedforward signal often becomes complicated or even impossible, if the output variables of the system do not coincide with the system's flat outputs. The situation becomes even worse, if the system is not flat at all, or if nonlinearities are included in the system model, which makes analytic solutions for the state trajectories unavailable in the non-flat case. For these reasons, the differential-algebraic equation solver Daets was used in previous work to solve this task numerically. The current paper describes extensions of the use of Daets in the framework of feedforward control design for non-flat outputs of a spatially two-dimensional heat transfer process with significant parameter uncertainty and actuator constraints. Numerical and experimental results are presented for a suitable test rig at the Chair of Mechatronics at the University of Rostock, where the robust feedback control part is designed using linear matrix inequalities in combination with a polytopic uncertainty model.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6669898,no,undetermined,0
Using Interactive Visual Reasoning to Support Sense-Making: Implications for Design,"This research aims to develop design guidelines for systems that support investigators and analysts in the exploration and assembly of evidence and inferences. We focus here on the problem of identifying candidate 'influencers' within a community of practice. To better understand this problem and its related cognitive and interaction needs, we conducted a user study using a system called INVISQUE (INteractive Visual Search and QUery Environment) loaded with content from the ACM Digital Library. INVISQUE supports search and manipulation of results over a freeform infinite 'canvas'. The study focuses on the representations user create and their reasoning process. It also draws on some pre-established theories and frameworks related to sense-making and cognitive work in general, which we apply as a 'theoretical lenses' to consider findings and articulate solutions. Analysing the user-study data in the light of these provides some understanding of how the high-level problem of identifying key players within a domain can translate into lower-level questions and interactions. This, in turn, has informed our understanding of representation and functionality needs at a level of description which abstracts away from the specifics of the problem at hand to the class of problems of interest. We consider the study outcomes from the perspective of implications for design.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651935,no,undetermined,0
"Using the ADL Experience API for Mobile Learning, Sensing, Informing, Encouraging, Orchestrating","The new ADL Experience API is an attempt for better interoperability between different types of educational systems and devices. This new specification is designed to link sensor networks for collecting and analyzing learning experiences in different contexts. This paper analyses how the concepts of the Experiences API were integrated in a mobile learning application and how the app uses learning analytics functions based on the collected data for informing the learners about their learning performance, for encouraging them to actively use the app, and for orchestrating and sequencing the learning resources.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658136,no,undetermined,0
UTOPIAN: User-Driven Topic Modeling Based on Interactive Nonnegative Matrix Factorization,"Topic modeling has been widely used for analyzing text document collections. Recently, there have been significant advancements in various topic modeling techniques, particularly in the form of probabilistic graphical modeling. State-of-the-art techniques such as Latent Dirichlet Allocation (LDA) have been successfully applied in visual text analytics. However, most of the widely-used methods based on probabilistic modeling have drawbacks in terms of consistency from multiple runs and empirical convergence. Furthermore, due to the complicatedness in the formulation and the algorithm, LDA cannot easily incorporate various types of user feedback. To tackle this problem, we propose a reliable and flexible visual analytics system for topic modeling called UTOPIAN (User-driven Topic modeling based on Interactive Nonnegative Matrix Factorization). Centered around its semi-supervised formulation, UTOPIAN enables users to interact with the topic modeling method and steer the result in a user-driven manner. We demonstrate the capability of UTOPIAN via several usage scenarios with real-world document corpuses such as InfoVis/VAST paper data set and product review data sets.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634167,no,undetermined,0
A log data analytics based scheduling in open source cloud software,"The paper proposes a Log Data Analytics based Scheduling in the Private Cloud environment in order to boost up the probability of launching Virtual Machine successfully. Analytics is applied to the logs maintained by Eucalyptus, a Open Source Cloud Software. The Proposed Methodology focuses on scheduling in the private cloud built with Eucalyptus Cloud Software, so that the proposed Scheduling at Eucalyptus Cloud Software is Analytics based; From the logs certain use cases are determined, which shall be used for capacity planning. Analytics is a conjucture, grouping, investigation, tracking and reporting of data for the cause of perception and optimization of utilization.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030777,no,undetermined,0
A method for the selection of software development life cycle models using analytic hierarchy process,"The success rate of software system depends upon the type of software development life cycle (SDLC) models that we employ at the time of software development process. In literature, we have identified different types of SDLC models like, agile methods, rapid application development method, and traditional methods; and choosing one of them is not an easy task according to need/criteria of the software projects. Therefore, in order to address this issue, we present a method for the selection of SDLC models using analytic hierarchy process (AHP). Finally, the utilization of the proposed approach is demonstrated with the help of an example.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6781338,no,undetermined,0
A Method of Discriminative Information Preservation and In-Dimension Distance Minimization Method for Feature Selection,"Preserving sample's pair wise similarity is essential for feature selection. In supervised learning, labels can be used as a direct measure to check whether two samples are similar with each other. In unsupervised learning, however, such similarity information is usually unavailable. In this paper, we propose a new feature selection method through spectral clustering based on discriminative information as an underlying data structure. Laplacian matrix is used to obtain more partitioning information than other previously proposed structures such as the Eigen space of original data. The high dimension of sample data is projected into a low dimensional space. The in-dimension distance is also considered to get a better compact clustering result. The proposed method can be solved efficiently by updating the projection matrix and its inverse normalized diagonal matrix. A comprehensive experimental study has demonstrated that the proposed method outperforms many state-of-the-art feature selection algorithms with different criterion including the accuracy of clustering/classification and Jaccard score.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976996,no,undetermined,0
Automating Deployment of Customized Scientific Data Analytic Environments on Clouds,"Cloud computing has become a widely used solution for efficiently provisioning computational and storage resources. Meanwhile, it is essential to provide customizable scientific data analytic platforms for researchers to conduct their personalized data intensive analysis. The integration of scientific data analytics and Cloud computing has the potential to improve resource utilization and facilitate the development of scientific researches. This paper proposes an automatic deployment framework for deploying computing environments on Clouds for every customized scientific data analytics. To achieve customization and deployment functionalities, this framework has two major components: customization service and workspace deployment service. Users are allowed to customize their personalized scientific data analytics and required Cloud resources under the customization service. A workspace language is defined in the workspace deployment service to describe the requirements of computing resources and software tools of a scientific data analytics. Workspace deployment service then adopts Chef to deploy corresponding computing environments on Clouds based on the workspace descriptions. We also implement a system based on this automatic deployment framework and present a RNA-seq analysis use case to demonstrate how this framework and its system can be used in practice.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034764,no,undetermined,0
Analytic model for cross-layer dependencies in VDSL2 access networks,"Recent changes in user employment of Internet based services, new deployment technologies for mobile networks as well as an ongoing realisation of fixed and mobile converged networks e.g. the EU FP7 project COMBO, are significant examples of enablers for increasing demands on DSL links. Investigating cross-layer dependencies between all layers in the OSI reference model becomes increasingly important. In this paper we present an analytical model and experimental results for the relation between impulse noise on a VDSL2 link and the effect this have on the network layer packet loss. We show how the packet loss rate is dependent not only on the disturbance signal level and periodicity but also on the link utilisation.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7039075,no,undetermined,0
Analytic throughput model for TCP-NC,"Network coding improves TCP's performance in lossy wireless networks. However, the complex congestion window evolution of network coded TCP (TCP-NC) makes the analysis of end-to-end throughput challenging. This paper analyzes the evolutionary process of TCP-NC against lossy links. An analytic model is established by applying a two-dimensional Markov chain. With maximum window size, end-to-end erasure rate and redundancy parameter as input parameters, the analytic model can reflect window evolution and calculate end-to-end throughput of TCP-NC precisely. The key point of our model is the novel definition for the states of Markov chain. It substantially reduces related states and much lower complexity is obtained. Our work helps understand the factors that affect TCP-NC's performance and lay the foundation of optimization. Extensive simulations on NS2 show that the analytic model features fairly high accuracy.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6952828,no,undetermined,0
Analyzing the Learning Process in Online Educational Game Design: A Case Study,"Educational game design environments are used for teaching computational thinking and software engineering concepts to novices. In software engineering education, there has recently been calls for ""innovative methods for software teaching and training in online courses"" (http://2014.icse-conferences.org/seet). However, to date, learning these concepts is tied to a formal learning environment and the presence of a teacher. In line with the new educational opportunities provided by the Web such as massive open online courses (MOOCs) and e-learning 2.0 platforms, we have created an online educational game design environment with integrated learning resources including video tutorials, showcases, and communication tools. To understand the effect of online educational game design environments with integrated support for learning on novices' use of the system and their learning, we conducted a mixed-method study with nine participants. While the learning goals were achieved to a high degree, the analysis of participants' interaction with the system reveals interesting phenomena about user preferences, such as the fact that during the given computational thinking tasks, the participants preferred the synchronous communication channel to other forms of commonly provided learning resources such as forums.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824112,no,undetermined,0
Animated Geo-temporal Clusters for Exploratory Search in Event Data Document Collections,"This paper presents a novel visual analytics technique developed to support exploratory search tasks for event data document collections. The technique supports discovery and exploration by clustering results and overlaying cluster summaries onto coordinated timeline and map views. Users can also explore and interact with search results by selecting clusters to filter and re-cluster the data with animation used to smooth the transition between views. The technique demonstrates a number of advantages over alternative methods for displaying and exploring geo-referenced search results and spatio-temporal data. Firstly, cluster summaries can be presented in a manner that makes them easy to read and scan. Listing representative events from each cluster also helps the process of discovery by preserving the diversity of results. Also, clicking on visual representations of geo-temporal clusters provides a quick and intuitive way to navigate across space and time simultaneously. This removes the need to overload users with the display of too many event labels at any one time. The technique was evaluated with a group of nineteen users and compared with an equivalent text based exploratory search engine.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6902897,no,undetermined,0
Application Characterization Using Oxbow Toolkit and PADS Infrastructure,"Characterizing the behavior of a scientific application and its associated proxy application is essential for determining whether the proxy application actually does mimic the full application. To support our ongoing characterization activities, we have developed the Oxbow toolkit and an associated data store infrastructure for collecting, storing, and querying this characterization information. This paper presents recent updates to the Oxbow toolkit and introduces the Oxbow project's Performance Analytics Data Store (PADS). To demonstrate the possible insights when using the toolkit and data store, we compare the characterizations of several full and proxy applications, along with the High Performance Linpack (HPL) and High Performance Conjugate Gradient (HPCG) benchmarks. Using techniques such as cluster visualizations of PADS data across many experiments, we found that the results show unexpected similarities and differences between proxy applications, and a greater similarity of proxy applications to HPCG than to HPL along many dimensions.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017964,no,undetermined,0
Application of Hybrid Assessment Method for Priority Assessment of Functional and Non-Functional Requirements,"Requirements prioritization is recognized as a critical but often neglected activity during software development process. To achieve a high quality software system, both functional and non-functional requirements must be taken into consideration during the prioritization process. Although in recent past years a lot of research has been devoted to requirements prioritization problems, research on proposing approaches to consider both functional and non-functional requirements throughout the prioritization process is still limited. In this article, we propose an approach using Hybrid Assessment Method (HAM) to prioritize both functional and non-functional requirements simultaneously. The effectiveness of the proposed approach has been evaluated through an experiment with the aim of comparing the approach with the other state-of-the-art-based approach, Analytic Hierarchy Process (AHP).",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6847365,no,undetermined,0
Applying learning analytics to simplify serious games deployment in the classroom,"In this paper we present our approach to introduce educational videogames as class exercises in face-to-face education. The main objective is to simplify teachers' task when using games by providing real-time information of the actual students' use of the games while in the classroom. The approach is based on defining the educational goals for the exercise/game precisely, designing a game that captures these goals, establishing relations between game interactions and educational goals and finally, create data capturing and visualizations of the relevant information to support the teacher. We applied this approach to a real case study, creating an educational videogame about the XML markup language that substituted the usual exercises in a Web Technologies class. This was tested with 34 computer science students with positive and promising results.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6826199,no,undetermined,0
Architectural model for next generation content management system,"Content Management System (CMS) is one of the recent upcoming concepts due to its efficient and explicit use in the real world applications. The demand for growth in its diversity has been documented by many researchers. Due to the limitation of traditional database management system (DBMS), which enforces limits on the type of data items need to be stored, CMS becomes the cost-effective alternative in the scope of information management. In this ever increasing data scenario we need a framework for managing and publishing all types of data with appropriate data analytics and natural language processing. Nowadays the information, especially in the Internet, consists of structured, semi-structured and unstructured data. Therefore it is unwise to think only about the use of DBMS, rather its alternative framework must be used, keeping the objectives same. The main reason for proposing a new architectural model for a content management system is that there are several important characteristics that CMSs should have to provide opportunity to specific optimizations [1]. Our äóìNext Generation CMS modeläó includes functionalities like multi-language, copyright, multichannel publishing, natural language processing, garbage collection management which are not included in the traditional ones.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7026859,no,undetermined,0
Architecture and capabilities of a data warehouse for ATM research,"This paper describes the design, implementation, and use of a data warehouse that supports air traffic management (ATM) research at NASA's Ames Research Center. The data warehouse, dubbed Sherlock, has been in development since 2009 and is a crucial piece of the ATM research infrastructure used by Ames and its partners. Sherlock comprises several components, including a database, a Web-based user interface, and supplementary services for query and visualization. The information stored includes raw data collected from the National Airspace System (NAS), parsed and processed data, derived data, and reports derived from pre-defined queries. The raw data include a variety of flight information from live streams of FAA operational systems, weather observations and forecasts, and NAS advisories and statistics. The modified data comprise parsed and merged data sources and metadata, enabling parameterized searches for data of interest. The derived data represent the results of research analyses deemed to be of significant interest to a wide cross-section of users. Sherlock is implemented on an Oracle 11g database, with supplemental services built on open-source packages and custom software. It contains over 20 TB of data spanning several years, and more data are added daily. It has supported several research studies, such as finding similar days in the NAS and predicting imposition of traffic flow management restrictions. Planned enhancements include integrated search across data sources and the capability for large-scale analytics.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6979418,no,undetermined,0
Beginning with big data simplified,"Big Data is a collection of datasets containing massive amount of data in the range of zettabytes and yottabytes. Organizations are facing difficulties in manipulating and managing this massive data as existing traditional database and software techniques are unable to process and analyze voluminous data. Dealing with Big Data requires new tools and techniques that can extract valuable information using some analytic process. Volume, Variety, Velocity, Value, Veracity, Variability and Complexity are attributes associated with Big Data in various works in the literature. In this paper, we briefly describe these existing attributes and also propose to add Viability, Cost and Consistency as new attributes to this set. This paper also discusses existing tools and techniques associated with Big Data. Fleet management is an evolving application of GPS data. It is taken as a case study in this work to illustrate various attributes of Big Data. This paper also presents the implementation of a sorting problem by varying Hadoop cluster sizes for the GPS data.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6954229,no,undetermined,0
A Methodological Approach to Provide Effective Web-Based Training by Using Collaborative Learning and Social Networks,"This paper presents an approach to rebuild the benefits lost from moving from traditional Instructor-led Training to Web-based Training by using Computer-Supported Collaborative Learning and Social Networks. The innovative approach of using Learning Analytics and Educational Data Mining techniques to track and analyse the interactive behavior of on-line collaborative learning and social networks is explored to enhance and improve corporate distance training. The results of this multidisciplinary proposal addressing pedagogical, technological and business issues to support Web-based Training becomes decisive for enhancing and improving the overall distance training experience and for finding new opportunities for cost-effective ways to deliver training programs. We believe the outcomes of this research will be crucial for greatly enhance and improve corporate distance training.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915498,no,undetermined,0
Behavioral analytics for inferring large-scale orchestrated probing events,"The significant dependence on cyberspace has indeed brought new risks that often compromise, exploit and damage invaluable data and systems. Thus, the capability to proactively infer malicious activities is of paramount importance. In this context, inferring probing events, which are commonly the first stage of any cyber attack, render a promising tactic to achieve that task. We have been receiving for the past three years 12 GB of daily malicious real darknet data (i.e., Internet traffic destined to half a million routable yet unallocated IP addresses) from more than 12 countries. This paper exploits such data to propose a novel approach that aims at capturing the behavior of the probing sources in an attempt to infer their orchestration (i.e., coordination) pattern. The latter defines a recently discovered characteristic of a new phenomenon of probing events that could be ominously leveraged to cause drastic Internet-wide and enterprise impacts as precursors of various cyber attacks. To accomplish its goals, the proposed approach leverages various signal and statistical techniques, information theoretical metrics, fuzzy approaches with real malware traffic and data mining methods. The approach is validated through one use case that arguably proves that a previously analyzed orchestrated probing event from last year is indeed still active, yet operating in a stealthy, very low rate mode. We envision that the proposed approach that is tailored towards darknet data, which is frequently, abundantly and effectively used to generate cyber threat intelligence, could be used by network security analysts, emergency response teams and/or observers of cyber events to infer large-scale orchestrated probing events for early cyber attack warning and notification.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6849283,no,undetermined,0
Big data analytics for supply chain management,"A high number of business cases are characterized by an expanded complexity. This is based on increased collaboration between companies, customers and governmental organizations on one hand and more individual products and services on the other hand. Due to that, companies are planning to address these issues with Big Data solutions. This paper deals with Big Data solutions focusing on Supply Chains, which represents a key discipline for handling the increased collaboration next to vast amounts of exchanged data. Today, the main focus lays on optimizing Supply Chain Visibility to handle complexity and to support decision making for handling risks and interruptions along supply chains. Therefore, Big Data concepts and technologies will play a key role. This paper describes the current skituation, actual solutions and presents exemplary use-cases for illustration. A classification regarding the area of application and potential benefits arising from Big Data Analytics are also given. Furthermore, this paper outlines general technologies to show capabilities of Big Data analytics.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058772,no,undetermined,0
Big Data analytics frameworks,"Big Data concerns massive, heterogeneous, autonomous sources with distributed and decentralized control. These characteristics make it an extreme challenge for organizations using traditional data management mechanism to store and process these huge datasets. It is required to define a new paradigm and re-evaluate current system to manage and process Big Data. In this paper, the important characteristics, issues and challenges related to Big Data management has been explored. Various open source Big Data analytics frameworks that deal with Big Data analytics workloads have been discussed. Comparative study between the given frameworks and suitability of the same has been proposed.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839299,no,undetermined,0
Big Data analytics with case study on financial organization,"Use of data and Big Data technologies is becoming a common theme to solve the problems that were otherwise seemed to require huge amount of storage and computing power. In the financial industry too, the use of Big Data is enabling solutions that are being used to detect and fight fraud. In this talk, we will focus on a couple of case studies from financial world to demonstrate the real world problems and their solutions to fight fraud.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056919,no,undetermined,0
Big data and predictive analytics in ERP systems for automating decision making process,"ERP systems, at present, are found to be inflexible to adapt to changing organizational processes. They are required to quickly adjust to changing processes and value-added chains and streamline their internal organizational structure. Data in ERP systems is becoming increasingly voluminous in their transactional programs. In this scenario, ERP systems are increasingly exposed to big data wherein the combined analysis of larger amounts of structured and unstructured data from disparate systems takes place in a short amount of time. Big data analytics requires greater use of predictive analytics to uncover hidden patterns and their relationships to visualize and explore data. The evolution of big data and predictive analytics have given a new way for exploring new frontiers in analytics-driven automation and decision management in highvolume, front-line operational decisions. In this paper the authors have focused on predictive capabilities of ERP systems, to analyze current data and historical facts in order to identify potential risks and opportunities for any organization. Analytical Decision Management & Business Rules are used to deploy decision as a service.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933558,no,undetermined,0
Big Data Density Analytics Using Parallel Coordinate Visualization,"Parallel coordinate is a popular tool for visualizing high-dimensional data and analyzing multivariate data. With the rapid growth of data size and complexity, data clutter in parallel coordinates is a major issue for Big Data visualization. This has given rise to three problems, (1) how to rearrange the parallel axes without the loss of data patterns, (2) how to shrink data attributes on each axis without the loss of data trends, (3) how to visualize the structured and unstructured data patterns for Big Data analysis. In this paper, we introduce the 5Ws dimensions as the parallel axes and establish the 5Ws sending density and receiving density as additional axes for Big Data visualization. Our model not only demonstrates Big Data attributes and patterns, but also reduces data over-lapping by up to 80 percent without the loss of data patterns. Experiments show that this new model can be efficiently used for Big Data analysis and visualization.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023729,no,undetermined,0
"Big data fueled process management of supply risks: Sensing, prediction, evaluation and mitigation","Supplier risks jeopardize on-time or complete delivery of supply in a supply chain. Traditionally, a company can merely do an ex-post evaluation of a supplier's performance, and handles emergencies in a reactive rather than a proactive way. We propose an agile process management framework to monitor and manage supply risks. The innovation is two fold - Firstly, a business process is established to make sure that the right data, the right insights, and the right decision-makers are in place at the right time. Secondly, we install a big data analytics component, a simulation component and an optimization component into the business process. The big data analytics component senses and predicts supply disruptions with internally (operational) and external (environmental) data. The simulation component supports risk evaluation to convert predicted risk severity to key performance indices (KPIs) such as cost and stockout percentage. The optimization component assists the risk-hedging decision-making.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019960,no,undetermined,0
Big data implementation and visualization,"Government agencies and large corporations are launching research programs to address big data's challenges. Visualization in today's time is very effective for presenting essential information in vast amounts of data. Big-data discovery tools present new research opportunities to the graphics and visualization community. The size of the collected data about the Web and mobile device users is even greater. To provide the ability to make sense and maximize utilization of such vast amounts of data for knowledge discovery and decision making is crucial to scientific advancement; we need new tools beyond conventional data mining and statistical analysis. Visualization is a tool which is shown to be effective for gleaning insight in big data. Here we also discuss data cube that fits in a tablet or a smart phone memory, actually for billions of entrances; we call this information structure a nanocube. [13]. We present pseudo code to compute and query a nanocube [13], and show how it can be used to generate well-known visual encodings such as heat maps, histograms, and parallel coordinate plots. While Apache* Hadoop* and other technologies are emerging to support back-end concerns such as storage and processing, visualization-based data discovery tools focus on the front end of big data-on helping businesses explore the data more easily and understand it more fully.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012883,no,undetermined,0
Big data in daily manufacturing operations,"Big data analytics is at the brink of changing the landscape in NXP Semiconductors Back End manufacturing operations. Numerous IT tools, implemented over the last decade, collect gigabytes of data daily, though the potential value of this data still remains to be explored. In this paper, the software tool called Heads Up is presented. Heads Up intelligently scans, filters, and explores the data with use of simulation. The software provides real-time relevant information, which is of high value in daily, as well as long term, production management. The software tool has been introduced at the NXP high volume manufacturing plant GuangDong China, where it is about to shift the paradigm on manufacturing operations.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020080,no,undetermined,0
Analysis and application of solenoid inductor,"An accurate analytic method of the coreless solenoid inductor is presented in this paper. The coreless solenoid inductor is simulated with electromagnetic field simulation software, and its S-parameters are obtained and translated to ABCD matrix. The coreless solenoid inductor is equivalent to pi network. The pi network is expressed with ABCD matrix, and then the coreless solenoid inductor can be inverted to equivalent lumped inductor. According to this method, the coreless solenoid inductor is used in a band pass filter. The simulation results of the band pass filter are close to measurement results.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6992759,no,undetermined,0
An Online Performance Prediction Framework for Service-Oriented Systems,"The exponential growth of Web service makes building high-quality service-oriented systems an urgent and crucial research problem. Performance of the service-oriented systems highly depends on the remote Web services as well as the unpredictability of the Internet. Performance prediction of service-oriented systems is critical for automatically selecting the optimal Web service composition. Since the performance of Web services is highly related to the service status and network environments which are variable over time, it is an important task to predict the performance of service-oriented systems at run-time. To address this critical challenge, this paper proposes an online performance prediction framework, called OPred, to provide personalized service-oriented system performance prediction efficiently. Based on the past usage experience from different users, OPred builds feature models and employs time series analysis techniques on feature trends to make performance prediction. The results of large-scale real-world experiments show the effectiveness and efficiency of OPred.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6720144,no,undetermined,0
An Integrated Security Framework for GOSS Power Grid Analytics Platform,"In power grid operations, security is an essential component for any middleware platform. Security protects data against unwanted access as well as cyber attacks. GridOpticsTM Software System (GOSS) is an open source power grid analytics platform that facilitates ease of access between applications and data sources and promotes development of advanced analytical applications. GOSS contains an API that abstracts many of the difficulties in connecting to various heterogeneous data sources. A number of applications and data sources have already been implemented to demonstrate functionality and ease of use. A security framework has been implemented which leverages widely accepted, robust Java TM security tools in a way such that they can be interchanged as needed. This framework supports the complex fine-grained, access control rules identified for the diverse data sources already in GOSS. Performance and reliability are also important considerations in any power grid architecture. An evaluation is done to determine the overhead cost caused by security within GOSS and ensure minimal impact to performance.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903642,no,undetermined,0
An Instructional Cloud-Based Testbed for Image and Video Analytics,"This paper describes a cloud-based software infrastructure for teaching big data analytics. Using this infrastructure, students can retrieve and analyze real-time visual data retrieved from globally distributed network cameras.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7037774,no,undetermined,0
A model architecture for Big Data applications using relational databases,"Effective Big Data applications dynamically handle the retrieval of decisioned results based on stored large datasets efficiently. One effective method of requesting decisioned results, or querying, large datasets is the use of SQL and database management systems such as MySQL. But a problem with using relational databases to store huge datasets is the decisioned result retrieval time, which is often slow largely due to poorly written queries / decision requests. This work presents a model to re-architect Big Data applications in order to efficiently present decisioned results: lowering the volume of data being handled by the application itself, and significantly decreasing response wait times while allowing the flexibility and permanence of a standard relational SQL database, supplying optimal user satisfaction in today's Data Analytics world. In this paper we review a Big Data case study in the telecommunications field and use it to experimentally demonstrate the effectiveness of our approach.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004462,no,undetermined,0
A Multiple Features Distance Preserving (MFDP) Model for Saliency Detection,"Playing a vital role, saliency has been widely applied for various image analysis tasks, such as content-aware image retargeting, image retrieval and object detection. It is generally accepted that saliency detection can benefit from the integration of multiple visual features. However, most of the existing literatures fuse multiple features at saliency map level without considering cross-feature information, i.e. generate a saliency map based on several maps computed from an individual feature. In this paper, we propose a Multiple Feature Distance Preserving (MFDP) model to seamlessly integrate multiple visual features through an alternative optimization process. Our method outperforms the state-of-the-arts methods on saliency detection. Saliency detected by our method is further cooperated with seam carving algorithm and significantly improves the performance on image retargeting.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008087,no,undetermined,0
A network analytics system in the SDN,"The emergence of virtualization and security problems of the network services, their lack of scalability and flexibility force network operators to look for äóìsmarteräó tools for network design and management. With the continuous growth of the number of subscribers, the volume of traffic and competition at the telecommunication market, there is a stable interest in finding new ways to identify weak points of the existing architecture, preventing the collapse of the network as well as evaluating and predicting the risks of problems in the network. To solve the problems of increasing the fail-safety and the efficiency of the network infrastructure, we offer to use the analytical software in the SDN context.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6995603,no,undetermined,0
A new algorithm for numerical comparison of environmental sustainability of heating systems,"This paper describes a technique for the selection of an heating system, among a choice of technologies, depending only on the environmental sustainability of the system itself. The analytical bases of this technique are introduced and a suitable numerical procedure is developed. The software implementation, built by the Authors in a Matlab environment, is described and numerical results are shown.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6850569,no,undetermined,0
A practical approaches to decrease the consistency index in AHP,"The Consistency is the most important measurement of the results from pairwise comparison in the AHP. Pairwise comparison is a method to calculate the weights for each element in order to perform a comparison of two advantages. The consistency of the AHP will be determined by the value of C.I. from the pairwise comparison table. If its value is for the case exceeding 0.1 (Saaty's criteria 1980) [12] is not consistent, pairwise comparison has to perform again. A longstanding weakness of AHP is explored by considering combinations of research costs, which seeks to address a difficult problem. This article aims to develop an efficient method to decrease the value of C.I. using an evolutionary algorithm and random search approach. Finally, we propose a software solution and discuss its effectiveness.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044748,no,undetermined,0
A Quantitative Evaluation of ERP Systems Quality Model,"The ERP system is a complex and comprehensive software that integrates various enterprise's functions and resources. Although this system provides the firms many benefits, they still hesitate to adopt it due to high cost and risks. Thus, this study identifies and analysis critical quality characteristics that should be considered to ensure successful development and implementation for the ERP system. The study identifies the new features of ERP systems that differ from other information systems' features. Then in order to develop ERP system quality model, ISO/IEC 9126 standard has been adapted. Finally, analytic Hierarchy Process (AHP) has been applied to evaluate the quality of the proposed model's characteristics. The derived quality characteristics could be used to compare ERP systems which help the organizations to implement better system quality.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6822174,no,undetermined,0
A social analytics platform for smarter commerce solutions,"When providing customers with a personalized shopping experience, there is tremendous value in understanding and applying social data shared by those consumers. Understanding this data and how best to generate business value from it is the core challenge of many businesses today. Friends, family, and experts alike influence consumers in their shopping preferences and purchase decisions. Yet, the ability of a business to analyze data on such influence, and recommend products and services that best respond to its customers' needs or aspirations, is typically limited by fragmented capabilities; a business relies heavily on the use of spreadsheets, manual market analysis, isolated software, or reactive messaging. This paper offers a solution to this fragmentary approach by introducing a social analytics platform for smarter commerce. This platform provides a holistic understanding of the customer by making use of social and enterprise data to present recommendations and related opinions, and to isolate influencers so as to ultimately provide customers with a personalized shopping experience. The functionality described in this paper is in the context of the retail industry but can be applied to other industries. The paper describes the architecture of the social analytics platform and the various analytics components currently implemented as part of the platform.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964897,no,undetermined,0
A System of Systems Service Design for Social Media Analytics,"Most social media analyses such as sentiment analysis for microblogs are often built as standalone, endpoint to endpoint applications. This makes the collaboration among distributed software and data service providers to create composite social analytic solutions difficult. This paper first proposes a system of systems service architecture (SoS-SA) design for social media analytics that support and facilitate efficient collaboration among distributed service providers. Then we propose a novel Twitters sentiment analysis service implemented on top of this design to illustrate its potentials. Current sentiment classification applications based on supervised learning methods relies too heavily on the chosen large training datasets, approaches using automatically generated training datasets also often result in the huge imbalance between the subjective classes and the objective classes in the sentiment of tweets, making it difficult to obtain good recall performance for the subjective ones. To address this issue, our proposed solution is based on a semi-supervised learning method for tweet sentiment classification. Experiments show that the performance of our method is better than those of the previous work.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6930609,no,undetermined,0
A web-based visual analytics system for air quality monitoring data,"With the increasingly severe air pollution, how to effectively process and analyze air quality data has become a hot issue. This paper introduces visual analytics into the processing and analysis of air quality data, and presents a web-based visual analytics system, AirVIS, for collaborative and comprehensive analysis on the spatial-temporal and multi-dimensional features of air quality monitoring datasets. This system offers three visual views: a GIS view used for spatial analysis, a scatter plot view used for temporal analysis and a parallel coordinate's view used for multi-dimensional analysis, all of which are inner-tied by the newest air environment standard, unified color-mapping strategies of Air Quality Index (AQI) levels and various interactive means. In case studies, our paper takes the air quality data in Beijing as our examples and illustrates that how this system provides users with deepened insight to discern the spatial-temporal characteristics and analyze the internal links of different air pollutants.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6950834,no,undetermined,0
Abstract Machine Models and Proxy Architectures for Exascale Computing,"To achieve exascale computing, fundamental hardware architectures must change. This will significantly impact scientific applications that run on current high performance computing (HPC) systems, many of which codify years of scientific domain knowledge and refinements for contemporary computer systems. To adapt to exascale architectures, developers must be able to reason about new hardware and determine what programming models and algorithms will provide the best blend of performance and energy efficiency in the future. An abstract machine model is designed to expose to the application developers and system software only the aspects of the machine that are important or relevant to performance and code structure. These models are intended as communication aids between application developers and hardware architects during the co-design process. A proxy architecture is a parameterized version of an abstract machine model, with parameters added to elucidate potential speeds and capacities of key hardware components. These more detailed architectural models enable discussion among the developers of analytic models and simulators and computer hardware architects and they allow for application performance analysis, system software development, and hardware optimization opportunities. In this paper, we present a set of abstract machine models and show how they might be used to help software developers prepare for exascale. We then apply parameters to one of these models to demonstrate how a proxy architecture can enable a more concrete exploration of how well application codes map onto future architectures.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017960,no,undetermined,0
Accommodating the Variable Timing of Software AES Decryption on Mobile Receivers,"Broadcast and multicast services in CDMA2000 wireless networks restrict the provision of high-quality multimedia services to their intended recipients by encrypting the content using the advanced encryption standard (AES) block cipher in the security layer of the broadcast protocol suite. We profile the execution time and the energy of each transformation within the AES decryption process and propose a novel analytic model for predicting the time and energy that are required to decrypt the content at a mobile receiver. The model uses the cross-layer information, including the characteristics of error control in the MAC layer and the varying conditions of the fading channel in the physical layer. In particular, we find that the decryption time varies significantly with the condition of the physical channel. Rate control is, therefore, required to smooth out these variations in the decryption time. For this purpose, we propose the introduction of a jitter buffer into the security layer and estimate the size of this jitter buffer to provide seamless multimedia services.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6412708,no,undetermined,0
Advanced planning and control of manufacturing processes in steel industry through big data analytics: Case study and architecture proposal,"Enterprises in today's globalized world are compelled to react on threats and opportunities in a highly flexible manner. Hence, companies that are able to analyze the current state of their business processes, forecast their most optimal progresses and with this proactively control them will have a decisive competitive advantage. Technological progress in sensor technology has boosted real-time situation awareness, especially in manufacturing operations. The paper at hands examines, based on a case study stemming from the steel manufacturing industry, which production-related data is collectable using state of the art sensors forming a basis for a detailed situation awareness and for deriving accurate forecasts. However, analyses of this data point out that dedicated big data analytics approaches are required to utilize the full potential out of it. By proposing an architecture for predictive process planning and control systems, the paper intends to form a working and discussion basis for further research and implementation efforts in big data analytics.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004408,no,undetermined,0
Air damping model for laterally oscillating MOEMS vibration sensors,"This paper presents a comprehensive model for the damping coefficient of a laterally moving micro-opto-electro-mechanical system. While viscous forces acting onto large areas of laterally oscillating microstructures are well understood, there are contributions to the air damping that receive less attention. These include pressure forces, the effects of regularly placed holes perforating the large surfaces as well as forces exerted onto the typically much smaller side faces. The results of our analytic models are backed up with very good agreement by finite volume method simulations with the open source software OpenFOAMŒ¬ as well as measurements. Since the read-out of our MOEMS sensor is optical, it does not involve highly complicated comb-drive geometries that are hard to model. Thus, our semi-numerical model describes the damping behavior of the MOEMS very accurately.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6985067,no,undetermined,0
AllJoyn Lambda: An architecture for the management of smart environments in IoT,"The increasing number of everyday embedded devices that are interconnected over the Internet leads to the need of new software solutions for managing them in an efficient, scalable, and smart way. In addition, such devices produce a huge amount of information, causing the well known Big Data problem, that need to be stored and processed. This emerging scenario, known as Internet of Things (IoT), raises two main challenges: large-scale smart environments management and Big Data storage/analytics. In this paper, we address both challenges, proposing AllJoyn Lambda, a software solution integrating AllJoyn in the Lambda architecture used for Big Data storage and analytics. In order, to describe how the architecture works, a software prototype integrating the AllJoyn system with MongoDB and Storm is presented and tested, analyzing a äóìsmart homeäó case of study. Finally, we will focus on how can be possible to manage embedded devices in a smart fashion processing both batch and real-time data.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7046676,no,undetermined,0
"An advanced technique to recover from BOC(1,1) false locks during the acquisition stage","This paper investigates a novel algorithm based on the Bump and Jump technique, capable of detecting and recovering from possible false lock events during the acquisition stage of BOC(1,1) Signals. Indeed the traditional Bump and Jump technique might lead to a false lock, in particular in harsh scenarios affected by strong multipath. Concerning this undesired event, an analysis of False Lock probability is provided to assess the goodness of the new approach w.r.t. the traditional Bump and Jump algorithm. The new algorithm is compared to the classical one, providing statistics on the False Locks occurrence after the Transition to Tracking stage, given the probability density function of the coarse delays estimated in the Acquisition stage. This analysis has been extensively assessed by means of a semi-analytic MATLABä‹¢ simulator, representative of the Acquisition and Transition to Tracking stages of a GNSS Ground Receiver. Two different multipath models have been considered applicable to the Reference Stations context: the two rays ground multipath and diffuse multipath. A preliminary assessment with a GNSS signals simulator and a receiver prototype are also included so as to demonstrate the applicability of the software simulation results to both Bump and Jump methods.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045153,no,undetermined,0
An approach to discover the best-fit factors for the optimal performance of Hadoop map reduce in virtualized environment,"Map Reduce pioneered by Google is mainly employed in Big Data analytics. In Map Reduce environment, most of the algorithms are re-used for mining the data. Prediction of execution time and system overhead of MapReduce job is very vital, from which performance shall be ascertained. Cloud computing is widely used as a computing platform in business and academic communities. Performance plays a major role, when user runs an application in the cloud. User may want to estimate the application execution time (latency) before submitting a Task or a Job. Hadoop clusters are deployed on Cloud environment performing the experiment. System overhead is determined by running Map Reduce job over Hadoop Clusters. While performing the experiment, metrics such as network I/O, CPU, Swap utilization, Time to complete the job and RSS, VSZ were captured and evaluated in order to diagnose, how performance of Hadoop is influenced by reconstructing the block size and split size with respect to block size.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7238471,no,undetermined,0
An approach to provide security to unstructured Big Data,"Security of Big Data is a big concern. In broad sense Big Data contains two types of data such as structured and unstructured. To provide security to unstructured data is more difficult than that of structured. In this paper we have developed an approach to give adequate security to the unstructured data by considering the types of the data and their sensitivity levels. We have reviewed the different analytics methods of Big Data, which gives us the facility to build a data node of databases of different types of data. Each type of data has been further classified to provide adequate security and enhance the overhead of the security system. To provide security to data node a security suite has been designed by incorporating different security standards and algorithms. The proper security standards or algorithms can be activated using an algorithm, which has been interfaced with the data node. We have shown that data classification with respect to sensitivity levels enhance the performance of the system.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7083392,no,undetermined,0
An experimental design to compare software requirements prioritization techniques,"Software products are getting increasingly complex described by a large number of requirements that characterize the user needs. In most cases, not all requirements can usually be met with available time and resource constraints; therefore, taking correct decisions about which requirements to implement in a release is crucial as the wrong decision from requirements engineers results in implementation of the false requirements, which lead to failure of project or product in the market. Although much research is done on requirements prioritization techniques, but studies in the area pointed out that when investigating the research made within the area, there seems to be little evidence regarding which approach is better than others and in what situations and environments. This paper presents an initial design of a controlled experiment to compare three prioritization methods. The goal of this experiment is to investigate which technique is better in terms of time consumption, accuracy, and ease of use. The experiment will be conducted in an academic context in Malaysian university.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045010,no,undetermined,0
An Improved Network Terminal Security Evaluation Index System,"For Liao Hui and others proposed network terminal security assessment index system exists index weights unreasonable distribution problem, using Delphi method and AHP calculate each index weight and the weights of total ranking of lowest level indexes relative to the highest lever indexes, identified the indicators which have greatest impact to the assessment objectives and apply it to a host of information security evaluation system. Combined with examples to prove the index system after improving its weight distribution can be more scientifically reflect the importance of the indexes in the evaluation system and the result of the host information security evaluation is reasonable and comprehensive.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7046892,no,undetermined,0
The extension of GDSS architecture by the subsystem of group decision method synthesis,"In the article the architecture of group decision support system is described. The developed system differs from the existing ones by the presence of the subsystem for synthetic procedure of group decision, which allows to synthesize the required version of group decision method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662674,no,undetermined,0
The enterprises' core competencies evaluation based on knowledge capitals: An empirical study in hotel industry,"Facing increasingly drastic competition situation, the enterprises imminently need to build core competencies in order to gain and sustain competitive advantage. The impact of knowledge capitals on the enterprises' core competencies is increasing. Based on differentiating these related concepts of resources, capabilities, competencies, core competencies, traditional capitals and knowledge capitals, from a perspective of knowledge capitals, this paper divided the enterprises' core competencies into five dimensions: human capitals and staff capabilities, information capitals and technology capabilities, organizational capitals and management capabilities, customer capitals and marketing capabilities, social capitals and cooperation capabilities, constructs the evaluation indicators system of the enterprises' core competencies, calculates the evaluation indicators weights in hotel industry by yaahp V6.0 software, and comprehensively evaluates and compares four hotels through AHP (Analytical Hierarchy Process) method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6703203,no,undetermined,0
The effectiveness evaluation of nano-satellites used in military operations,"The feasibility and effectiveness of nano-satellites in military operations need further study. An overview of nano-satellite programs with explicit military objectives was presented. In order to evaluate the effectiveness of nano-satellite, an optimization method for reconnaissance nano-satellite constellation was proposed based on a simulation environment using STK and Visual C++ software tools. Then, a more detailed effectiveness evaluation process for the optimized constellation was proposed using fuzzy Analytical Hierarchy Process (AHP) method. The result shows that a nano-satellite constellation is more effective than a large satellite for tactical missions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6885544,no,undetermined,0
Massively scalable near duplicate detection in streams of documents using MDSH,"In a world where large-scale text collections are not only becoming ubiquitous but also are growing at increasing rates, near duplicate documents are becoming a growing concern that has the potential to hinder many different information filtering tasks. While others have tried to address this problem, prior techniques have only been used on limited collection sizes and static cases. We will briefly describe the problem in the context of Open Source analysis along with our additional constraints for performance. In this work we propose two variations on Multi-dimensional Spectral Hash (MDSH) tailored for working on extremely large, growing sets of text documents. We analyze the memory and runtime characteristics of our techniques and provide an informal analysis of the quality of the near-duplicate clusters produced by our techniques.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691610,no,undetermined,0
Introducing a Data Sliding Mechanism for Cooperative Caching in Manycore Architectures,"In future micro-architectures, the increase of the number of cores and wire network complexity is leading to several performance degradation. These platforms are intended to process large amount of data. One of the biggest challenges for systems scalability is actually the memory wall: the memory latency is hardly increasing compared to technology expectations. Recent works explore potential software and hardware solutions mainly based on different caching schemes for addressing off-chip access issues. In this paper, we propose a new cooperative caching method improving the cache miss rate for many core micro-architectures. The work is motivated by some limitations of recent adaptive cooperative caching proposals. Elastic Cooperative caching (ECC), is a dynamic memory partitioning mechanism that allows sharing cache across cooperative nodes according to the application behavior. However, it is mainly limited with cache eviction rate in case of highly stressed neighborhood. Another system, the adaptive Set-Granular Cooperative Caching (ASCC), is based on finer set-based mechanisms for a better adaptability. However, heavy localized cache loads are not efficiently managed. In such a context, we propose a cooperative caching strategy that consists in sliding data through closer neighbors. When a cache receives a storing request of a neighbor's private block, it spills the least recently used private data to a close neighbor. Thus, solicited saturated nodes slide local blocks to their respective neighbors to always provide free cache space. We also propose a new Priority-based Data Replacement policy to decide efficiently which blocks should be spilled, and a new mechanism to choose host destination called Best Neighbor selector. The first analytic performance evaluation shows that the proposed cache management policies reduce by half the average global communication rate. As frequent accesses are focused in the neighboring zones, it efficiently improves on-Chip traff- c. Finally, our evaluation shows that cache miss rate is enhanced: each tile keeps the most frequently accessed data 1-Hop close to it, instead of ejecting them Off-Chip. Proposed techniques notably reduce the cache miss rate in case of high solicitation of the cooperative zone, as it is shown in the performed experiments.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650905,no,undetermined,0
IOT-StatisticDB: A General Statistical Database Cluster Mechanism for Big Data Analysis in the Internet of Things,"In large scale Internet of Things (IoT) systems, statistical analysis is a crucial technique for transforming data into knowledge and for obtaining overall information about the physical world. However, most existing statistical analysis methods for sensor sampling data are implemented outside the database kernel and focus on specialized analytics, making them unsuited for the IoT environment where both the data types and the statistical queries are diverse. To solve this problem, we propose a General Statistical Database Cluster Mechanism for Big Data Analysis in the Internet of Things (IOT-StatisticDB) in this paper. In IOT-StatisticDB, statistical functions are performed through statistical operators inside the DBMS kernel, so that complicated statistical queries can be expressed in the standard SQL format. Besides, statistical analysis is executed in a distributed and parallel manner over multiple servers so that the performance can be greatly improved, which is confirmed by the experiments.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682118,no,undetermined,0
Keeping requirements on track via visual analytics,"For many software projects, keeping requirements on track needs an effective and efficient path from data to decision. Visual analytics creates such a path that enables the human to extract insights by interacting with the relevant information. While various requirements visualization techniques exist, few have produced end-to-end values to practitioners. In this paper, we advance the literature on visual requirements analytics by characterizing its key components and relationships. This allows us to not only assess existing approaches, but also create tool enhancements in a principled manner. We evaluate our enhanced tool supports through a case study where massive, heterogeneous, and dynamic requirements are processed, visualized, and analyzed. In particular, our study illuminates how increased interactivity of requirements visualization could lead to actionable decisions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636720,no,undetermined,0
Land suitability evaluation method based on GIS technology,"Land suitability evaluation plays an important role in the planning of land use. With the help of GIS technology, the efficiency and the accuracy of land suitability evaluation improve greatly. The paper focuses on: 1) the method of land suitability evaluation based on GIS technology, 2) Enriching the styles of the evaluation results and improving its application value. The study of GIS-based evaluation method, proposed in this paper, includes the following aspects: 1) Collect and collate basic data in research area, 2) Build a graphics and properties database through collecting layer information by using ArcGIS software. 3) Process graphics data to generate DEM in research area, and carry out elevation analysis and slope analysis, 4) Taking agriculture, forestry and animal husbandry as land use goals, establish a land suitability evaluation index, get the weight of each index with AHP and Delphi method and divide land suitability level system. 5) Identify the minimum evaluation unit. 6 )Modeling and calculate the score of minimum evaluation unit, and finally get the land suitability rank of each evaluation unit to the specified purposes. In order to improve application value of the evaluation results, it is necessary to mine spatial data based on the evaluation results and other materials. We get some thematic maps such as arable land potential area, returning farmland to forest area, and returning farmland to pastoral areas. GIS plays an important role in this process. These materials can be used directly to serve land use planning. Multivariate analysis method is the basic method and it is proved to be effect to improve the land suitability evaluation. To get a correct result, it is important to establish index systems and select suitable indicators to divide evaluation units according to spatial scales and regional types. Given the socio-economic property of land-use, it's necessary to select socio-economic indicators.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621869,no,undetermined,0
Large Payload Streaming Database Sort and Projection on FPGAs,"In recent years, real-time analytics has seen widespread adoption in the business world. While it provides useful business insights and improved market responsiveness, it also adds a computational burden to traditional online transaction processing (OLTP) systems. Analytics queries involve complex database operations such as sort, aggregation, and join that consume significant computational resources, and, when executed on the same system, may affect the performance of OLTP queries. In this paper, we try to address this issue by accelerating two such database operations, namely, projection and sort, using a field programmable gate array (FPGA). Our prototype is implemented on an Alter a Stratix V FPGA and achieves an order of magnitude speedup in the sort operation compared to baseline software. Furthermore, our prototype implements projection in parallel with other query operations on FPGA, thus completely eliminating the cost of projection without consuming any extra cycles on the FPGA. FPGA accelerated sort and projection have been integrated with our previous work on accelerating other query operations [1], making our analytics acceleration prototype on FPGA applicable to a wider variety of queries.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702576,no,undetermined,0
Leveraging applied materials TechEdge Prizmä‹¢ for advanced lithography process control,"Presently the CDSEM metrology tool continues to be the äóìruler of the fabäó providing metrology at over 100 steps along the way to building a modern semiconductor product. We explore the history of data volume generated with Fab CDSEM fleets. As we quantify the size of the fleet dataset, it becomes clear why historically only a subset of the rich dataset is ever utilized. We review what the treatment of the data using modern äóìbig dataäó analytics and enterprise level server hardware does to accelerate development learning, increase engineering turns per day and raise the velocity of accurate manufacturing feedback. This paper further explores ways to leverage TechEdge Prizm to extract more value out of metrology for Lithography process control. The paper will describe some of the challenges facing the changing Lithography metrology landscape and how Prizm delivers tailored analysis for these challenges. Results at a customer site demonstrate that various types of analysis of CDSEM images and data are 10 times more efficient and thorough than traditional methods.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6552814,no,undetermined,0
Library Automation in Cloud,"By combining the most effective practices of Vitalization, Grid computing, utility computing and network technologies cloud computing is the resultant cost effective computing service delivery mechanism. This survey paper discusses prospect of cloud based Library Management System for cost effective library automation. Another important aspect of this survey work is analytic discussion on various open source library automation software. CYBRARIANTM<sup>TM</sup> an internet based integrated Library Automation and Management solution developed by CR2 Technologies Ltd is also included for the benefit of implementer.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658039,no,undetermined,0
LLSuperCloud: Sharing HPC systems for diverse rapid prototyping,"The supercomputing and enterprise computing arenas come from very different lineages. However, the advent of commodity computing servers has brought the two arenas closer than they have ever been. Within enterprise computing, commodity computing servers have resulted in the development of a wide range of new cloud capabilities: elastic computing, virtualization, and data hosting. Similarly, the supercomputing community has developed new capabilities in heterogeneous, massively parallel hardware and software. Merging the benefits of enterprise clouds and supercomputing has been a challenging goal. Significant effort has been expended in trying to deploy supercomputing capabilities on cloud computing systems. These efforts have resulted in unreliable, low-performance solutions, which requires enormous expertise to maintain. LLSuperCloud provides a novel solution to the problem of merging enterprise cloud and supercomputing technology. More specifically LLSuperCloud reverses the traditional paradigm of attempting to deploy supercomputing capabilities on a cloud and instead deploys cloud capabilities on a supercomputer. The result is a system that can handle heterogeneous, massively parallel workloads while also providing high performance elastic computing, virtualization, and databases. The benefits of LLSuperCloud are highlighted using a mixed workload of C MPI, parallel MATLAB, Java, databases, and virtualized Web services.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6670329,no,undetermined,0
Malware detection by text and data mining,"Cyber frauds are a major security threat to the banking industry worldwide. Malware is one of the manifestations of cyber frauds. Malware authors use Application Programming Interface (API) calls to perpetrate these crimes. In this paper, we propose a static analysis method to detect Malware based on API call sequences using text and data mining in tandem. We analyzed the dataset available at CSMINING group. First, we employed text mining to extract features from the dataset consisting a series of API calls. Further, mutual information is invoked for feature selection. Then, we resorted to over-sampling to balance the data set. Finally, we employed various data mining techniques such as Decision Tree (DT), Multi Layer Perceptron (MLP), Support Vector Machine (SVM), Probabilistic Neural Network (PNN) and Group Method for Data Handling (GMDH). We also applied One Class SVM (OCSVM). Throughout the paper, we used 10-fold cross validation technique for testing the techniques. We observed that SVM and OCSVM achieved 100% sensitivity after balancing the dataset.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724229,no,undetermined,0
Master Data Management and Data Warehouse: An architectural approach for improved decision-making,"In corporate environments it is common to have multiple data sources that represent the same real world entity (such as customers, suppliers, products, etc.). This situation can also occur for external data (social networks, regulatory institutions). The extraction activity has the need to decide among the available sources, how best to obtain a given data. This paper presents an architectural approach to dealing with the problem of multiple data sources with the objective of improving the data quality of a Data Warehouse in conjunction with the presentation of a set of best practices to identify and implement the process of Master Data Management (MDM).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615708,no,undetermined,0
Optimal Clustering of Time Periods for Electricity Demand-Side Management,"Several pure binary integer optimization models are developed for clustering time periods by similarity for electricity utilities seeking assistance with pricing strategies. The models include alternative objectives for characterizing various notions of within-cluster distances, admit as feasible only clusters that are contiguous, and allow for circularity, where time periods at the beginning and end of the planning cycle may be in the same cluster. Restrictions upon cluster size may conveniently be included without the need of additional constraints. The models are populated with a real-world dataset of electricity usage for 93 buildings and solutions and run-times attained by conventional optimization software are compared with those by dynamic programming, or by a greedy algorithm applicable to one of the models, that run in polynomial time. The results provide time-of-use segments that an electricity utility may employ for selective pricing for peak and off-peak time periods to influence demand for the purpose of load leveling.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6522912,no,undetermined,0
Methods and Metrics for Evaluating Analytic Insider Threat Tools,"The insider threat is a prime security concern for government and industry organizations. As insider threat programs come into operational practice, there is a continuing need to assess the effectiveness of tools, methods, and data sources, which enables continual process improvement. This is particularly challenging in operational environments, where the actual number of malicious insiders in a study sample is not known. The present paper addresses the design of evaluation strategies and associated measures of effectiveness; several quantitative/statistical significance test approaches are described with examples, and a new measure, the Enrichment Ratio, is proposed and described as a means of assessing the impact of proposed tools on the organization's operations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6565235,no,undetermined,0
Mobile malware visual analytics and similarities of Attack Toolkits (Malware gene analysis),"We use Normalized Compression Distance (NCD) (owing to its capabilities to perform similarity measure of unstructured data) to enumerate code similarity between malicious Android apps and visualize their clusters. Our classification methods and visual analytics can help the antivirus community to ensure that a variant of a known malware can still be detected without the need of creating a signature. We also present when a new malware is released, our methods can be used to understand the similarity/behavior with known malware families.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6567221,no,undetermined,0
Models for Measuring Access Security of Web Application: Security Reference Model,Measurement units and knowledge of security properties are hardly known. This process causes the complex systems decomposition into simpler and smaller systems thus allowing the estimative of properties that will help the understanding and measurement of software systems security properties. This process provides the security model and the score of security attributes priority is calculated by AHP methodology. A security model example to illustrate this approach is presented.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693463,no,undetermined,0
MotionExplorer: Exploratory Search in Human Motion Capture Data Based on Hierarchical Aggregation,"We present MotionExplorer, an exploratory search and analysis system for sequences of human motion in large motion capture data collections. This special type of multivariate time series data is relevant in many research fields including medicine, sports and animation. Key tasks in working with motion data include analysis of motion states and transitions, and synthesis of motion vectors by interpolation and combination. In the practice of research and application of human motion data, challenges exist in providing visual summaries and drill-down functionality for handling large motion data collections. We find that this domain can benefit from appropriate visual retrieval and analysis support to handle these tasks in presence of large motion data. To address this need, we developed MotionExplorer together with domain experts as an exploratory search system based on interactive aggregation and visualization of motion states as a basis for data navigation, exploration, and search. Based on an overview-first type visualization, users are able to search for interesting sub-sequences of motion based on a query-by-example metaphor, and explore search results by details on demand. We developed MotionExplorer in close collaboration with the targeted users who are researchers working on human motion synthesis and analysis, including a summative field study. Additionally, we conducted a laboratory design study to substantially improve MotionExplorer towards an intuitive, usable and robust design. MotionExplorer enables the search in human motion capture data with only a few mouse clicks. The researchers unanimously confirm that the system can efficiently support their work.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634102,no,undetermined,0
Multi-perspective Process Variability: A Case for Smart Green Buildings (Short Paper),"The variability scale in large-scale Cyber-Physical Systems (CPSs) is high and complex due to the voluminousness, dynamicity and diversity of available computing resources (people, things and software services), domain-specific processes, domain-specific elements (stakeholders, assets and contracts), and their relationships. This requires us to go beyond current variability modeling and management techniques which neglect the complexity and the diversity of relevant stakeholders, data and assets, and thus cannot cope with intelligent business and analytics requirements in dynamic environments, such as smart city management. In this paper, we present a comprehensive analysis for understanding the multi-perspective variability in processes atop people, data and things in CPSs, particularly, for the sustainability governance of Smart Green Buildings (SGBs). We examine domain-specific processes and domain-specific elements and their relationships to derive a multiple-perspective variability management for SGBs. On the basis of this, we conceptualize a novel model for the multi-perspective process variability representation.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6717280,no,undetermined,0
MVSE: A Multi-core Video decoder System level analytics Engine,"Multi-core platform has become a trend in hand-held embedded systems, such as smartphone and tablet. To improve the video decoding performance by using the multiple cores, one of parallel algorithms should be adopted. However, different parallel algorithm should be selected for different video standard on different platform. Therefore, an engine to estimate performance on a target platform from existing single-thread video decoder is very helpful. This paper proposes a Multi-core Video decoder System level analytics Engine (MVSE) to estimate the performance on a target multi-core platform. In the MVSE, a general video decoder runs according to profiling data and macroblock information by three major parallel algorithms. The profiling data and macroblock information are obtained from existing single-thread video decoder so that the MVSE can support different video standard. The MVSE runs on target platform to consider the effect of memory access contention and cache intercommunication, which are traditionally difficult to estimate. Our experimental result shows the MVSE estimation is accuracy enough. The estimation results from MVSE shows the best speedup ratio is 1.7 times in a dual-core platform and 2.9 times in a quad-core platform for H.264 720p decoding. In addition, MVSE is also helpful for hardware and software co-design in heterogeneous computing. The experimental results show the best performance is improved by VLD hardware, and the speedup ratio is 2.3 times in a dual-core platform and 3.9 times in a quad-core platform.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6533859,no,undetermined,0
Need 4 Speed: Leverage New Metrics to Boost Your Velocity without Compromising on Quality,"As Agile becomes the de-facto SDLC practice, it has become evident that additional practices are required to allow enterprises to realize the premise of it. In this experience report, we will share the practices learnt and exercised over the past 3 years that helped us cope with the common challenges of large scale enterprise projects. The concept of ""Application Lifecycle Intelligence"" discussed in this paper covers the key principles of these best practices. It shares the additional analytics and metrics that allowed HP to enable cross time zones collaboration and alignment through transparency and provides insights into quality and risk. The key principal accomplished by extracting data from various practitioners tools and surface it in an actionable format for the various stakeholders.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612886,no,undetermined,0
On mixing high-speed updates and in-memory queries: A big-data architecture for real-time analytics,"Up-to-date business intelligence has become a critical differentiator for the modern data-driven highly engaged enterprise. It requires rapid integration of new information on a continuous basis for subsequent analyses. ETL-based and traditionally batch-processing oriented methods of absorbing changes into a relational database schema take time, and are therefore incompatible with very low-latency demands of realtime analytics. Instead, in-memory clustered stores that employ tunable consistency mechanisms are becoming attractive since they dispense with the need to transform and transit data between storage layouts and tiers. When data is updated infrequently, in-memory approaches such as RDD transformations in Spark can suffice, but as updates become frequent, such in-memory approaches need to be extended to support dynamic datasets. This paper describes a few key additional requirements that result from having to support in-memory processing of data while updates proceed concurrently. The paper describes Real-time Analytics Foundation (RAF), an architecture to meet the new requirements. Performance of an early implementation of RAF is also described: for an unaudited TPC-H derived workload, RAF shows a node-to-node scaling ratio of 88% at 8 nodes, and for a query equivalent to Q6 in the TPC-H set, RAF is able to show 9x improvement over that of Hive-Hadoop. The paper also describes two RAF based solutions that are being put together by two independent software vendors in China.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691704,no,undetermined,0
On the Hardware/Software Design and Implementation of a High Definition Multiview Video Surveillance System,"This paper proposes a distributed architecture for high definition multiview video surveillance system. It adopts a modular design where single view/stereo intelligent internet protocol (IP)-based video surveillance cameras are connected to a front-end field programmable gate array (FPGA) board(s) which are connected to a back-end local video server through the IP network. The data intensive video analytics (VA) algorithms such as background modeling, connected component labeling and single view object tracking are implemented in the FPGA using an efficient fix-point based architecture. Each back-end video server is equipped with a storage and graphics processing units for supporting high-level VA and other processing algorithms such as video decompression/display, mean depth estimation and consistent labeling. A real-time prototype system was constructed to illustrate the architecture and VA algorithms involved. Satisfactory results were obtained for both publicly available data set and real surveillance video data.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6515196,no,undetermined,0
Intelligent system for multivariables reconfiguration of distribution networks,"This paper proposes a new distribution network reconfiguration approach in normal operating conditions. The multivariables are considered such as the energy losses minimization of the primary network and two reliability indices. The selection strategy of network configuration is based on a heuristic method. The method considers only the automated equipment such as switches and remote controlled reclosers in the new configuration analysis. The AHP (Analytic Hierarchic Process) method is used to define the weights for the optimization criteria and to determine the best switching sequence of the network reconfiguration. The reconfiguration is performed by monitoring the real time network. Changes in demand feeders are considered and analyzed by load rates. Also, the best setting for each operation level is defined. The results of the implemented techniques indicate a satisfactory methodology. Tests were performed using a real power utility system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6554380,no,undetermined,0
Intelligent BVAC information capturing system for smart building information modelling,"Building Information Modelling (BIM) has implications for all processes and activities related to construction supply chain and can, thus, make significant contributions to lean construction process. The existing dimensions of BIM not only attend to most aspects of the construction work and processes, but the technology also has the potential to add further dimensions responding to other existing or future challenges. This paper will look into ways in which Artificial Neural Network (ANN)-COBie can help architects and engineers to perform HVAC analysis with the support of a BIM platform. Other applications e.g. HVAC load analysis and life-cycle cost analysis for any system or component associated with a building may be conducted using the ANN-COBie system that can be stored in the BIM authoring application and exported using IFC or gbXML to any analytic software. These applications' associated future challenges will be briefly discussed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6828247,no,undetermined,0
IntegrityMR: Integrity assurance framework for big data analytics and management applications,"Big data analytics and knowledge management is becoming a hot topic with the emerging techniques of cloud computing and big data computing model such as MapReduce. However, large-scale adoption of MapReduce applications on public clouds is hindered by the lack of trust on the participating virtual machines deployed on the public cloud. In this paper, we extend the existing hybrid cloud MapReduce architecture to multiple public clouds. Based on such architecture, we propose IntegrityMR, an integrity assurance framework for big data analytics and management applications. We explore the result integrity check techniques at two alternative software layers: the MapReduce task layer and the applications layer. We design and implement the system at both layers based on Apache Hadoop MapReduce and Pig Latin, and perform a series of experiments with popular big data analytics and management applications such as Apache Mahout and Pig on commercial public clouds (Amazon EC2 and Microsoft Azure) and local cluster environment. The experimental result of the task layer approach shows high integrity (98% with a credit threshold of 5) with non-negligible performance overhead (18% to 82% extra running time compared to original MapReduce). The experimental result of the application layer approach shows better performance compared with the task layer approach (less than 35% of extra running time compared with the original MapReduce).",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691780,no,undetermined,0
Integration of DEMATEL and ANP methods for calculate the weight of characteristics software quality based model ISO 9126,"One of the difficulties that occur in the model is to decide the weights of quality characteristics. This is due to the interrelations existence among the quality factors based model ISO 9126. Each of these characteristics can influence or even contradict each other. The interrelations existence among the factors affects the weight of characteristics software quality, and will affect the software quality calculation. Therefore, researchers will integrate DEMATEL and ANP methods for calculate the weight of characteristics software quality based model ISO 9126. DEMATEL method used to calculate sum of influences for each characteristics model ISO 9126, while the ANP method used to calculate local weights and global weight for each sub characteristics model ISO 9126. Results from this study is the value of local weights for each of the characteristics of ISO 9126, and global weights for each sub characteristics ISO 9126 which represent the level of importance of the characteristics and sub characteristics ISO 9126.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676228,no,undetermined,0
Evaluation of NEMP vulnerability on cots electronic equipments,"This study deals with a new evaluation method of NEMP (Nuclear ElectroMagnetic Pulse) vulnerability on cots electronic equipments. The method consists in comparing EMC (ElectroMagnetic Compatibility) test severities to NEMP conducted stresses. The comparison uses five characteristic criteria of the induced stresses, calculated with an analytic method. The process is based on a software named äóìSUSIEäó (in French: SUSceptibilite’ a’ó l'IEMN äŠÈ Impulsion e’lectroMagne’tique d'origine Nucle’aire Haute Altitude äŠ‚). Today, only conducted stresses are analyzed. An evolution is planned in 2013 to determine the equivalent vulnerability of radiated stresses and to validate the software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6632197,no,undetermined,0
Exploring energy and performance behaviors of data-intensive scientific workflows on systems with deep memory hierarchies,"The increasing gap between the rate at which large scale scientific simulations generate data and the corresponding storage speeds and capacities is leading to more complex system architectures with deep memory hierarchies. Advances in non-volatile memory (NVRAM) technology have made it an attractive candidate as intermediate storage in this memory hierarchy to address the latency and performance gap between main memory and disk storage. As a result, it is important to understand and model its energy/performance behavior from an application perspective as well as how it can be effectively used for staging data within an application workflow. In this paper, we target a NVRAM-based deep memory hierarchy and explore its potential for supporting in-situ/in-transit data analytics pipelines that are part of application workflows patterns. Specifically, we model the memory hierarchy and experimentally explore energy/performance behaviors of different data management strategies and data exchange patterns, as well as the tradeoffs associated with data placement, data movement and data processing.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799122,no,undetermined,0
FlexIO: I/O Middleware for Location-Flexible Scientific Data Analytics,"Increasingly severe I/O bottlenecks on High-End Computing machines are prompting scientists to process simulation output data online while simulations are running and before storing data on disk. There are several options to place data analytics along the I/O path: on compute nodes, on separate nodes dedicated to analytics, or after data is stored on persistent storage. Since different placements have different impact on performance and cost, there is a consequent need for flexibility in the location of data analytics. The FlexIO middleware described in this paper makes it easy for scientists to obtain such flexibility, by offering simple abstractions and diverse data movement methods to couple simulation with analytics. Various placement policies can be built on top of FlexIO to exploit the trade-offs in performing analytics at different levels of the I/O hierarchy. Experimental results demonstrate that FlexIO can support a variety of simulation and analytics workloads at large scale through flexible placement options, efficient data movement, and dynamic deployment of data manipulation functionalities.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569822,no,undetermined,0
"Game Analytics for Game User Research, Part 1: A Workshop Review and Case Study","The emerging field of game user research (GUR) investigates interaction between players and games and the surrounding context of play. Game user researchers have explored methods from, for example, human-computer interaction, psychology, interaction design, media studies, and the social sciences. They've extended and modified these methods for different types of digital games, such as social games, casual games, and serious games. This article focuses on quantitative analytics of in-game behavioral user data and its emergent use by the GUR community. The article outlines open problems emerging from several GUR workshops. In addition, a case study of a current collaboration between researchers and a game company demonstrates game analytics' use and benefits.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482531,no,undetermined,0
General chairs and keynote speakers,"These tutorials/keynote speeches: Z-numbers, a new direction in the analysis of uncertain and complex systems; human space computing and cyber-physical systems; digital ecosystems, the resources for future humanity and society; massive data analytics for smart planet; social media for sustained digital ecosystems; new era of civilization, technology understand human and human; SAP co-innovation, envision the future, crossroots innovation; prediction markets, virtual currencies and social scores applied and technology innovation for networked life.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6611306,no,undetermined,0
Generating Erroneous Human Behavior From Strategic Knowledge in Task Models and Evaluating Its Impact on System Safety With Model Checking,"Human-automation interaction, including erroneous human behavior, is a factor in the failure of complex, safety-critical systems. This paper presents a method for automatically generating formal task analytic models encompassing both erroneous and normative human behavior from normative task models, where the misapplication of strategic knowledge is used to generate erroneous behavior. Resulting models can be automatically incorporated into larger formal system models so that safety properties can be formally verified with a model checker. This allows analysts to prove that a human-automation interactive system (as represented by the formal model) will or will not satisfy safety properties with both normative and generated erroneous human behavior. Benchmarks are reported that illustrate how this method scales. The method is then illustrated with a case study: the programming of a patient-controlled analgesia pump. In this example, a problem resulting from a generated erroneous human behavior is discovered. The method is further employed to evaluate the effectiveness of different solutions to the discovered problem. The results and future research directions are discussed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519275,no,undetermined,0
HFMS: Managing the lifecycle and complexity of hybrid analytic data flows,"To remain competitive, enterprises are evolving their business intelligence systems to provide dynamic, near realtime views of business activities. To enable this, they deploy complex workflows of analytic data flows that access multiple storage repositories and execution engines and that span the enterprise and even outside the enterprise. We call these multi-engine flows hybrid flows. Designing and optimizing hybrid flows is a challenging task. Managing a workload of hybrid flows is even more challenging since their execution engines are likely under different administrative domains and there is no single point of control. To address these needs, we present a Hybrid Flow Management System (HFMS). It is an independent software layer over a number of independent execution engines and storage repositories. It simplifies the design of analytic data flows and includes optimization and executor modules to produce optimized executable flows that can run across multiple execution engines. HFMS dispatches flows for execution and monitors their progress. To meet service level objectives for a workload, it may dynamically change a flow's execution plan to avoid processing bottlenecks in the computing infrastructure. We present the architecture of HFMS and describe its components. To demonstrate its potential benefit, we describe performance results for running sample batch workloads with and without HFMS. The ability to monitor multiple execution engines and to dynamically adjust plans enables HFMS to provide better service guarantees and better system utilization.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6544907,no,undetermined,0
Hiding data in indexed images,"In this paper, a method for hiding data is proposed based on color table expansion technique for indexed images. At the same time the security and the hiding capacity are analyzed in great detail. The analytic results show that this method has very strong security and very high hiding capacity. Secret data can be hidden in a stego-image which has exactly the same visual effect as the original image. If the correct key is used, these confidential data can be recovered from the stego-image without distortion. In addition, due to the use of random numbers, the statistical properties of the confidential data are completely overshadowed, and the stego-image is independent of those confidential data.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615434,no,undetermined,0
High-performance imaging subsystems and their integration in mobile devices,"Within today's SoCs, functionality such as video, audio, graphics, and imaging is increasingly integrated through IP blocks, which are subsystems in their own right. Integration of IP blocks within SoCs always brought software integration aspects with it. However, since these subsystems increasingly consist of programmable processors, many more layers of firmware and software need to be integrated. In the imaging domain, this is particularly true. Imaging subsystems typically are highly heterogeneous, with high levels of parallelism. The construction of their firmware requires target-specific optimization, yet needs to take interoperability with sensor input systems and graphics/display subsystems into account. Hard real-time scheduling within the subsystem needs to cooperate with less stringent image analytics and SoC-level (OS) scheduling. In many of today's systems, the latter often only supports soft scheduling deadlines. At HW level, IP subsystems need to be integrated such that they can efficiently exchange both short-latency control signals and high-bandwidth data-plane blocks. Solutions exist, but need to be properly configured. However, at the SW level, currently no support exists that provides (i) efficient programmability, (ii) SW abstraction of all the different HW features of these blocks, and (iii) interoperability of these blocks. Starting points could be languages such as OpenCL and OpenCV, which do provide some abstractions, but are not yet sufficiently versatile.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513494,no,undetermined,0
High-Performance RDMA-based Design of Hadoop MapReduce over InfiniBand,"MapReduce is a very popular programming model used to handle large datasets in enterprise data centers and clouds. Although various implementations of MapReduce exist, Hadoop MapReduce is the most widely used in large data centers like Facebook, Yahoo! and Amazon due to its portability and fault tolerance. Network performance plays a key role in determining the performance of data intensive applications using Hadoop MapReduce as data required by the map and reduce processes can be distributed across the cluster. In this context, data center designers have been looking at high performance interconnects such as InfiniBand to enhance the performance of their Hadoop MapReduce based applications. However, achieving better performance through usage of high performance interconnects like InfiniBand is a significant task. It requires a careful redesign of communication framework inside MapReduce. Several assumptions made for current socket based communication in the current framework do not hold true for high performance interconnects. In this paper, we propose the design of an RDMA-based Hadoop MapReduce over InfiniBand and several design elements: data shuffle over InfiniBand, in-memory merge mechanism for the Reducer, and pre-fetch data for the Mapper. We perform our experiments on native InfiniBand using Remote Direct Memory Access (RDMA) and compare our results with that of Hadoop-A [1] and default Hadoop over different interconnects and protocols. For all these experiments, we perform network level parameter tuning and use optimum values for each Hadoop design. Our performance results show that, for a 100GB TeraSort running on an eight node cluster, we achieve a performance improvement of 32% over IP-over InfiniBand (IPoIB) and 21% over Hadoop-A. With multiple disks per node, this benefit rises up to 39% over IPoIB and 31% over Hadoop-A.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651094,no,undetermined,0
I/O Containers: Managing the Data Analytics and Visualization Pipelines of High End Codes,"Lack of I/O scalability is known to cause measurable slowdowns for large-scale scientific applications running on high end machines. This is prompting researchers to devise 'I/O staging' methods in which outputs are processed via online analysis and visualization methods to support desired science outcomes. Organized as online workflows and carried out in I/O pipelines, these analysis components run concurrently with science simulations, often using a smaller set of nodes on the high end machine termed 'staging areas'. This paper presents a new approach to dealing with several challenges arising for such online analytics, including: how to efficiently run multiple analytics components on staging area resources providing them with the levels of end-to-end performance they need and how to manage staging resources when analytics actions change due to user or data-dependent behavior. Our approach designs and implements middleware constructs that delineate and manage I/O pipeline resources called 'I/O Containers'. Experimental evaluations of containers with realistic scientific applications demonstrate the feasibility and utility of the approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651106,no,undetermined,0
IEC61850 Gateway SCL Configuration Document Research,"Smart grid's development is rapid, and the application of IEC61850 standard has become a necessary tendency. It can use nets to realize the transformation of the original statute to IEC61850 standard, because the configuration document description IED model cannot be recognized by MMS. It needed to adopt a specifical function which was analyzed for the document and produced IEC 61850 object corresponding MMS object. Because how the SCL configuration document parses the relationship to code is the effectiveness of the conversion, it has become the key to the whole system. By logging on SCLServer gateway Telnet terminal on the modified configuration information, we can get the magnitude and the bottom of the real-time communication data. The monitoring system of remote sensing and remote data transmission set up the view through the database design and the configuration document. The analytic design and original data such as the gateway's date can be got through locating design software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6455739,no,undetermined,0
Improved decision-support making for selecting future traffic signal controllers using expert-knowledge acquisition,"Transportation agencies are facing the decision-making problem while selecting traffic signal controller that corresponds to the needs of their future signal system. The complexity of this problem originates from the current level of controller standardization, market-driven competition, responsibility for long-term operation, and scale of investment. This paper presents an improvement of methodology and a decision-support system (DSS) for selecting traffic signal controllers. DSS bases upon Analytical Hierarchy Process, and is developed as an application in MS Excel. The main improvement is the component for expert knowledge acquisition for assignment of criteria weights. The graphical user interface and supporting analytical engine based on fuzzy logic are developed to enhance the expert knowledge acquisition. Paper presents application interface and analytical engine with an example. Possibilities for further research should provide potential for greater flexibility of this application to aid in decision-making for other equipment selection.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6728318,no,undetermined,0
Improved power control of photovoltaic generation system under unbalanced grid voltage conditions,"In order to ride through the unbalanced grid fault, the output power of photovoltaic inverter will fluctuate and its output current will rise and distort under unbalanced grid voltage. This paper proposes an improved formula of reference current and its optimal coefficients method for the photovoltaic inverter under unbalanced grid voltage sag, taking into account the requirements of active and reactive power control. Then the analytic formula of its current harmonic distortion, three-phase current peak, active and reactive power fluctuation, DC voltage fluctuation and the control factor of its output current reference were derived. Considering the constraints of inverter phase current and the dc voltage fluctuation of capacitance in dc side, the optimal model of inverter current reference was established with minimum integrated amplitude of the active and reactive power function as a goal. The correctness of the proposed method has been verified by PSCAD/EMTDC simulation software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6837218,no,undetermined,0
Improving Multi-job MapReduce Scheduling in an Opportunistic Environment,"As a state-of-the-art programming model for big data analytics, MapReduce is well suited for parallel processing of large data sets in opportunistic environments. Existing research on MapReduce in opportunistic environment has focused on improving single job performance, the issue of fairness that is critical in the more dominant scenario of multiple concurrent jobs remains unexplored. We address this problem by proposing an opportunistic fair scheduling algorithm, which extends the broadly adopted Fair Scheduler to an environment where nodes are intermittently available with possibly different availability patterns. The proposed scheduler maintains statistics specific to the opportunistic environment, e.g., node availability rates and pairwise availability correlations, and utilizes this information in scheduling decisions to improve fairness. Using a Hadoop-based implementation, we compare our scheduler with the current Hadoop Fair Scheduler on representative benchmarks. Our experiments verify that our scheduler can significantly reduce the variability in job completion times.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676672,no,undetermined,0
Incorporating Uncertainty into In-Cloud Application Deployment Decisions for Availability,"Cloud consumers have a variety of deployment related techniques, such as auto-scaling policies and recovery strategies, for dealing with the uncertainties in the cloud. Uncertainties can be characterized as stochastic (such as failures, disasters, and workload spikes) and subjective (such as choice among various deployment options). Cloud consumers must consider both stochastic and subjective uncertainties. Analytic support for consumers in selecting appropriate techniques and setting the required parameters in the face of different types of uncertainty is currently limited. In this paper, we propose a set of application availability analysis models that capture subjective uncertainties in addition to stochastic uncertainties. We built and validated the models by using industry best practices on deployment, and actual commercial products for disaster recovery and live migration. Our results show that the models permit more informed and quantitative availability analysis than industry best practices under a wide range of scenarios.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676727,no,undetermined,0
Influence of learning analytics in software engineering education,"Software is an important aspect in the modern world because it uses by everyone and everywhere. Therefore software engineering education gets more important in the computer science curricula but it has lapses to produce good software engineers to the industries requirements. Learning analytics helps the students to improve their learning activities. This was analyzed among the software engineering students how the learning style influences in gathering knowledge. Forty six questionnaires were distributed to the first year MCA students in the Department of Computer Applications, Bharathiar University, Coimbatore, India for this study.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6528597,no,undetermined,0
Innovative practices session 5C: Cloud atlas äóî Unreliability through massive connectivity,"The rapid pace of integration, emergence of low power, low cost computing elements, and ubiquitous and ever-increasing bandwidth of connectivity have given rise to data center and cloud infrastructures. These infrastructures are beginning to be used on a massive scale across vast geographic boundaries to provide commercial services to businesses such as banking, enterprise computing, online sales, and data mining and processing for targeted marketing to name a few. Such an infrastructure comprises of thousands of compute and storage nodes that are interconnected by massive network fabrics, each of them having their own hardware and firmware stacks, with layers of software stacks for operating systems, network protocols, schedulers and application programs. The scale of such an infrastructure has made possible service that has been unimaginable only a few years ago, but has the downside of severe losses in case of failure. A system of such scale and risk necessitates methods to (a) proactively anticipate and protect against impending failures, (b) efficiently, transparently and quickly detect, diagnose and correct failures in any software or hardware layer, and (c) be able to automatically adapt itself based on prior failures to prevent future occurrences. Addressing the above reliability challenges is inherently different from the traditional reliability techniques. First, there is a great amount of redundant resources available in the cloud from networking to computing and storage nodes, which opens up many reliability approaches by harvesting these available redundancies. Second, due to the large scale of the system, techniques with high overheads, especially in power, are not acceptable. Consequently, cross layer approaches to optimize the availability and power have gained traction recently. This session will address these challenges in maintaining reliable service with solutions across the hardware/software stacks. The currently available commercial data-cente- and cloud infrastructures will be reviewed and the relative occurrences of different causalities of failures, the level to which they are anticipated and diagnosed in practice, and their impact on the quality of service and infrastructure design will be discussed. A study on real-time analytics to proactively address failures in a private, secure cloud engaged in domain-specific computations, with streaming inputs received from embedded computing platforms (such as airborne image sources, data streams, or sensors) will be presented next. The session concludes with a discussion on the increased relevance of resiliency features built inside individual systems and components (private cloud) and how the macro public cloud absorbs innovations from this realm.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6548907,no,undetermined,0
Integrated wireless sensor network for large scale intelligent systems,"There is a critical need to improve the efficiency of the large systems and eco-system of this planet. It is important to successfully integrate a variety of technology blocks ranging from sensors to communication system to analytics software in order to realize our goal of smarter systems and eco-systems. In order to apply a proactive approach to solve this planet's problem, everything must be instrumented. We would like to deploy the sensors widely so that these sensors act as the eyes, ears, and noses for the analytics engine and provide real time data at a desired frequency. In this paper, we demonstrate the first step to realize a convergence of wireless sensor network with analytics software. The wireless sensors transmit data at regular intervals through the 2.4 GHz ISM band to a coordinator. The coordinator forwards the aggregated data to a messaging software. The time stamps, topics, and payloads of the messages can be visualized using a GUI explorer of the messaging software. These messages can be further forwarded through a message broker to database servers and web servers via internet. Accordingly, the analytics software could make use of the live data to provide decision support for engineers, automated control system, and actuators.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637463,no,undetermined,0
OpenCV based road sign recognition on Zynq,"Road sign recognition is a key component in autonomous vehicles and also has applications in driver assistance systems and road sign maintenance. Here an algorithm is presented using the Xilinx Zynq-7020 chip on a Zedboard to scan 1920í„1080 images taken by an ON Semiconductor VITA-2000 sensor attached via the FMC slot. The PL section of the Zynq is used to perform essential image pre-processing functions and color based filtering of the image. Software classifies the shapes in the filtered image, and OpenCV's template matching function is used to identify the signs from a database of UK road signs. The system was designed in six weeks, and can process one frame in approximately 5 seconds. This is a promising start for a real-time System on Chip based approach to the problem of road sign recognition and also for using the Zynq platform for rapid deployment of these types of applications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622951,no,undetermined,0
Optimizations and Analysis of BSP Graph Processing Models on Public Clouds,"Large-scale graph analytics is a central tool in many fields, and exemplifies the size and complexity of Big Data applications. Recent distributed graph processing frameworks utilize the venerable Bulk Synchronous Parallel (BSP) model and promise scalability for large graph analytics. This has been made popular by Google's Pregel, which provides an architecture design for BSP graph processing. Public clouds offer democratized access to medium-sized compute infrastructure with the promise of rapid provisioning with no capital investment. Evaluating BSP graph frameworks on cloud platforms with their unique constraints is less explored. Here, we present optimizations and analyses for computationally complex graph analysis algorithms such as betweenness-centrality and all-pairs shortest paths on a native BSP framework we have developed for the Microsoft Azure Cloud, modeled on the Pregel graph processing model. We propose novel heuristics for scheduling graph vertex processing in swaths to maximize resource utilization on cloud VMs that lead to a 3.5x performance improvement. We explore the effects of graph partitioning in the context of BSP, and show that even a well partitioned graph may not lead to performance improvements due to BSP's barrier synchronization. We end with a discussion on leveraging cloud elasticity for dynamically scaling the number of BSP workers to achieve a better performance than a static deployment, and at a significantly lower cost.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569812,no,undetermined,0
"The BTWorld use case for big data analytics: Description, MapReduce logical workflow, and empirical evaluation","The commoditization of big data analytics, that is, the deployment, tuning, and future development of big data processing platforms such as MapReduce, relies on a thorough understanding of relevant use cases and workloads. In this work we propose BTWorld, a use case for time-based big data analytics that is representative for processing data collected periodically from a global-scale distributed system. BTWorld enables a data-driven approach to understanding the evolution of BitTorrent, a global file-sharing network that has over 100 million users and accounts for a third of today's upstream traffic. We describe for this use case the analyst questions and the structure of a multi-terabyte data set. We design a MapReduce-based logical workflow, which includes three levels of data dependency - inter-query, inter-job, and intra-job - and a query diversity that make the BTWorld use case challenging for today's big data processing tools; the workflow can be instantiated in various ways in the MapReduce stack. Last, we instantiate this complex workflow using Pig-Hadoop-HDFS and evaluate the use case empirically. Our MapReduce use case has challenging features: small (kilobytes) to large (250 MB) data sizes per observed item, excellent (10<sup>-6</sup>) and very poor (10<sup>2</sup>) selectivity, and short (seconds) to long (hours) job duration.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691631,no,undetermined,0
Small Is Beautiful: Summarizing Scientific Workflows Using Semantic Annotations,"Scientific workflows have become the workhorse of Big Data analytics for scientists. As well as being repeatable and optimizable pipelines that bring together datasets and analysis tools, workflows make-up an important part of the provenance of data generated from their execution. By faithfully capturing all stages in the analysis, workflows play a critical part in building up the audit-trail (a.k.a. provenance) meta-data for derived datasets and contributes to the veracity of results. Provenance is essential for reporting results, reporting the method followed, and adapting to changes in the datasets or tools. These functions, however, are hampered by the complexity of workflows and consequently the complexity of data-trails generated from their instrumented execution. In this paper we propose the generation of workflow description summaries in order to tackle workflow complexity. We elaborate reduction primitives for summarizing workflows, and show how primitives, as building blocks, can be used in conjunction with semantic workflow annotations to encode different summarization strategies. We report on the effectiveness of the method through experimental evaluation using real-world workflows from the Tavern a system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597153,no,undetermined,0
ScatterBlogs2: Real-Time Monitoring of Microblog Messages through User-Guided Filtering,"The number of microblog posts published daily has reached a level that hampers the effective retrieval of relevant messages, and the amount of information conveyed through services such as Twitter is still increasing. Analysts require new methods for monitoring their topic of interest, dealing with the data volume and its dynamic nature. It is of particular importance to provide situational awareness for decision making in time-critical tasks. Current tools for monitoring microblogs typically filter messages based on user-defined keyword queries and metadata restrictions. Used on their own, such methods can have drawbacks with respect to filter accuracy and adaptability to changes in trends and topic structure. We suggest ScatterBlogs2, a new approach to let analysts build task-tailored message filters in an interactive and visual manner based on recorded messages of well-understood previous events. These message filters include supervised classification and query creation backed by the statistical distribution of terms and their co-occurrences. The created filter methods can be orchestrated and adapted afterwards for interactive, visual real-time monitoring and analysis of microblog feeds. We demonstrate the feasibility of our approach for analyzing the Twitter stream in emergency management scenarios.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634195,no,undetermined,0
Semi-formal and formal interface specification for system of systems architecture,"The independence of the constituent systems of a system of systems presents a key challenge to the discipline of system of systems (SoS) engineering. The fact that constituent systems can and do function independently of the SoS means that engineers of a constituent system cannot rely on the behaviour of other constituent systems. This paper advocates a model-based approach to SoS engineering that requires the interfaces to constituent systems to be specified. We propose an use of an interface design pattern for interface specification that uses the industry standard notation, SysML.We also indicate a translation of these specifications to a formal notation, CML, in order to extend the range of analytic techniques available to the SoS engineer.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549946,no,undetermined,0
Senseseer mobile-cloud-based Lifelogging framework,"Smart-phones are becoming our constant companions, they are with us all of the time, being used for calling, web surfing, apps, music listening, TV viewing, social networking, buying, gaming, and a myriad of other uses. Smart-phones are a technology that knows us much better than most of us could imagine. Based on our usage and the fact that we are never far away from our smart phones, they know where we go, who we interact with, what information we consume, and with a little clever software, they can know what we are doing and even why we are doing it. They are beginning to know us better than we know ourselves. In this work we present SenseSeer a generic mobile-cloud-based mobile Lifelogging framework. This framework supports customisable analytic services for sensing the person, understanding the semantics of life activities and the easy deployment of analytic tools and novel interfaces. At present, SenseSeer supports services in many domains, such as personal health monitoring, location tracking, lifestyle analysis and tourism focused applications. This work demonstrate the design principles of SenseSeer and three of its services: My Health, My Location and My Social Activity.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613113,no,undetermined,0
SentiView: Sentiment Analysis and Visualization for Internet Popular Topics,"There would be value to several domains in discovering and visualizing sentiments in online posts. This paper presents SentiView, an interactive visualization system that aims to analyze public sentiments for popular topics on the Internet. SentiView combines uncertainty modeling and model-driven adjustment. By searching and correlating frequent words in text data, it mines and models the changes of the sentiment on public topics. In addition, using a time-varying helix together with an attribute astrolabe to represent sentiments, it can visualize the changes of multiple attributes and relationships among demographics of interest and the sentiments of participants on popular topics. The relationships of interest among different participants are presented in a relationship map. Using a new evolution model that is based on cellular automata, it is able to compare the time-varying features for sentiment-driven forums on both simulated and real data. Adaptable for different social networking platforms, such as Twitter, blog and forum, the methods demonstrate the effectiveness of SentiView in analyzing and visualizing public sentiments on the Web.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650118,no,undetermined,0
Service-Generated Big Data and Big Data-as-a-Service: An Overview,"With the prevalence of service computing and cloud computing, more and more services are emerging on the Internet, generating huge volume of data, such as trace logs, QoS information, service relationship, etc. The overwhelming service-generated data become too large and complex to be effectively processed by traditional approaches. How to store, manage, and create values from the service-oriented big data become an important research problem. On the other hand, with the increasingly large amount of data, a single infrastructure which provides common functionality for managing and analyzing different types of service-generated big data is urgently required. To address this challenge, this paper provides an overview of service-generated big data and Big Data-as-a-Service. First, three types of service-generated big data are exploited to enhance system performance. Then, Big Data-as-a-Service, including Big Data Infrastructure-as-a-Service, Big Data Platform-as-a-Service, and Big Data Analytics Software-as-a-Service, is employed to provide common big data related services (e.g., accessing service-generated big data and data analytics results) to users to enhance efficiency and reduce cost.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597164,no,undetermined,0
Simfrastructure: A Flexible and Adaptable Middleware Platform for Modeling and Analysis of Socially Coupled Systems,"Socially coupled systems are comprised of interdependent social, organizational, economic, infrastructure and physical networks. Today's urban regions serve as an excellent example of such systems. People and institutions confront the implications of the increasing scale of information becoming available due to a combination of advances in pervasive computing, data acquisition systems as well as high performance computing. Integrated modeling and decision making environments are necessary to support planning, analysis and counter factual experiments to study these complex systems. Here, we describe SIMFRASTRUCTURE - a flexible coordination middleware that supports high performance computing oriented decision and analytics environments to study socially coupled systems. Simfrastructure provides a multiplexing mechanism by which simple and intuitive user-interfaces can be plugged in as front-end systems, and high-end computing resources can be plugged in as back-end systems for execution. This makes the computational complexity of the simulations completely transparent to the users. The decoupling of user interfaces and data repository from simulation execution allows users to access simulation results asynchronously and enables them to add new datasets and simulation models dynamically. Simfrastructure enables implementation of a simple yet powerful modeling environment with built-in analytics-as-a service platform, which provides seamless access to high end computational resources, through an intuitive interface for studying socially coupled systems. We illustrate the applicability of Simfrastructure in the context of an integrated modeling environment to study public health epidemiology.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6546132,no,undetermined,0
Simulation of 2.5-dimensional borehole acoustic waves with convolutional perfectly matched layer,"A 2.5-dimensional method using the PDE package of the commercial finite element software COMSOL Multiphysics is developed to simulate wave propagation in a borehole. The computation is conducted in the frequency wave-number domain. A convolutional perfectly matched layer is implemented to eliminate the reflections from artificial truncation boundaries. Waveforms obtained in time domain are in good agreement with analytic solutions in a special model, which proves the validity of the method. A numerical modeling example is presented to illustrate the capabilities of the method. It is shown that this method can be used to solve a variety of non-axisymmetric borehole acoustic wave propagation problems.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6841132,no,undetermined,0
Simulation of CSS communication system in VLF atmospheric noise,"Chirp spread spectrum (CSS) technique has been extensively studied in the application of Ultra-wide band (UWB). Considering the long distance propagation capacity and low intercepted probability of chirp signal, we propose to apply the CSS technique to the very low frequency (VLF) communication system. First, we discuss the CSS system based on fractional Fourier transform (FRFT), including the definition of FRFT and means of modulation and demodulation. Then we introduce an analytic model of VLF atmospheric noise presented by E C. Field [1] and generate atmospheric noise by MATLAB software. Finally, we perform simulation of the CSS communication system in atmospheric noise. The simulation results validate its feasibility.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6747838,no,undetermined,0
SketchPadN-D: WYDIWYG Sculpting and Editing in High-Dimensional Space,"High-dimensional data visualization has been attracting much attention. To fully test related software and algorithms, researchers require a diverse pool of data with known and desired features. Test data do not always provide this, or only partially. Here we propose the paradigm WYDIWYGS (What You Draw Is What You Get). Its embodiment, SketchPad<sup>ND</sup>, is a tool that allows users to generate high-dimensional data in the same interface they also use for visualization. This provides for an immersive and direct data generation activity, and furthermore it also enables users to interactively edit and clean existing high-dimensional data from possible artifacts. SketchPad<sup>ND</sup> offers two visualization paradigms, one based on parallel coordinates and the other based on a relatively new framework using an N-D polygon to navigate in high-dimensional space. The first interface allows users to draw arbitrary profiles of probability density functions along each dimension axis and sketch shapes for data density and connections between adjacent dimensions. The second interface embraces the idea of sculpting. Users can carve data at arbitrary orientations and refine them wherever necessary. This guarantees that the data generated is truly high-dimensional. We demonstrate our tool's usefulness in real data visualization scenarios.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634118,no,undetermined,0
Smart data centers for green Clouds,"The key idea behind this work is the development of smart data centers able to monitor, control, and manage themselves through advanced analytics and management policies, collecting and analyzing real time data on their behavior. They also make adjustments to interdependent components across the physical infrastructure, in order to address changing business and technology needs. To this aim, we have designed and implemented a new framework for supporting green computing in the management of Cloud data centers. Through heterogeneous sensing devices deployed in the system, the framework is able to know the working state of the data center and to activate specific energy saving policies. We have evaluated the proposed solution through experiments on a real testbed implemented at the University of Messina. Experiments show interesting results thus proving real benefits deriving from the adoption of the proposed framework.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754914,no,undetermined,0
Overcoming Limitations of Off-the-Shelf Priority Schedulers in Dynamic Environments,"It is common nowadays to architect and design scaled-out systems with off-the-shelf computing components operated and managed by off-the-shelf open-source tools. While web services represent the critical set of services offered at scale, big data analytics is emerging as a preferred service to be colocated with cloud web services at a lower priority raising the need for off-the-shelf priority scheduling. In this paper we report on the perils of Linux priority scheduling tools when used to differentiate between such complex services. We demonstrate that simple priority scheduling utilities such as nice and ionice can result in dramatically erratic behavior. We provide a remedy by proposing an autonomic priority scheduling algorithm that adjusts its execution parameters based on on-line measurements of the current resource usage of critical applications. Detailed experimentation with a user-space prototype of the algorithm on a Linux system using popular benchmarks such as SPEC and TPC-W illustrate the robustness and versatility of the proposed technique, as it provides consistency to the expected performance of a high-priority application when running simultaneously with multiple low priority jobs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6730807,no,undetermined,0
Software analytics for incident management of online services: An experience report,"As online services become more and more popular, incident management has become a critical task that aims to minimize the service downtime and to ensure high quality of the provided services. In practice, incident management is conducted through analyzing a huge amount of monitoring data collected at runtime of a service. Such data-driven incident management faces several significant challenges such as the large data scale, complex problem space, and incomplete knowledge. To address these challenges, we carried out two-year software-analytics research where we designed a set of novel data-driven techniques and developed an industrial system called the Service Analysis Studio (SAS) targeting real scenarios in a large-scale online service of Microsoft. SAS has been deployed to worldwide product datacenters and widely used by on-call engineers for incident management. This paper shares our experience about using software analytics to solve engineers' pain points in incident management, the developed data-analysis techniques, and the lessons learned from the process of research development and technology transfer.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693105,yes,undetermined,0
"Software defined networks (SDN) - Enabling virtualised, programmable infrastructure","Abstract form only given. In its formative stages, SDN was popular in research and venture capital communities. However as organisations tackle issues such as complexity, automation and time to market, the industry has become involved in this transition. SDN potentially addresses some of these challenges at the same time as exposing new options for cloud, predictive analytics and the Internet of Everything. This presentation provides an industry perspective of the drivers, status and potential of SDN and the promise of infrastructure virtualisation and programmability.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758469,no,undetermined,0
"Software defined networks (SDN) - Enabling virtualised, programmable infrastructure","In its formative stages, SDN was popular in research and venture capital communities. However as organisations tackle issues such as complexity, automation and time to market, the industry has become involved in this transition. SDN potentially addresses some of these challenges at the same time as exposing new options for cloud, predictive analytics and the Internet of Everything. This presentation provides an industry perspective of the drivers, status and potential of SDN and the promise of infrastructure virtualisation and programmability.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6761213,no,undetermined,0
Statistical model of power supply subsystem Satellite,"In this paper, based on the fact that in designing the energy providing subsystem for satellites, most approaches and relations are empirical and statistical, and also, considering the aerospace sciences and its relation with other engineering fields such as electrical engineering to be young, these are no analytic or one hundred percent proven empirical relations in many fields. Therefore, we consider the statistical design of this subsystem. The presented approach in this paper is entirely innovative and all parts of the energy providing subsystem for the satellite are specified. In codifying this approach, the data of 602 satellites and some software programs such as SPSS have been used. In this approach, after proposing the design procedure, the total needed power for the satellite, the mass of the energy providing subsystem, the material of solar array, and finally the placement of these arrays on the satellite are designed. All these parts are designed based on the mission of the satellite and its weight class.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6581293,no,undetermined,0
Surveillance of sentiment and affect in open source text,"New topics of discussion emerge continuously as political and cultural environments shift. These topics can convey a range of ideas, stories, values, and shared experiences. In open source data, discourse is often infused with emotionally-charged prose expressed by large and diverse pools of authors. Manually identifying and analyzing the emotional content of combined discussions from millions of individuals is an impossible task for any single analyst or decision maker. Technologies that can automatically identify emerging topics of discussion and align them with public opinions can provide early warning to acts of violence or other threats [1]. We present an approach to detecting basic emotions associated with topics by combining work in topic modeling with affect analysis. We apply the correspondence LDA topic model (CorrLDA2) [2] to correlate emotional states with topics. Using this technique, topics have associated emotions and emotion categories are correlated over a corpus of text.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6785831,no,undetermined,0
Symbolic derivation of nonlinear benchmark bicycle dynamics with holonomic and nonholonomic constraints,"We present a symbolic method for modeling nonlinear multibody underactuated systems with holonomic and nonholonomic constraints. Using MAPLE software, we are able to solve the quartic holonomic constraint analytically. We then use the constraints and extra Lagrange-Euler equations to systematically eliminate all the auxiliary coordinates and Lagrange multipliers, thereby obtaining a minimum set of unconstrained nonlinear analytic ordinary differential equations corresponding to the degrees of freedom of the system. The method is applied to a benchmark bicycle, in which all the six ground contact constraint equations are eliminated, leaving analytic coupled ordinary differential equations corresponding to the bicycle rear body roll, steer angle, and rear wheel rotation degrees of freedom without any approximation. This reduced analytic model offers insights in understanding complex nonlinear bicycle dynamic behaviors and enables the development of an efficient model suitable for real time control outside of the linear regime.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6728251,no,undetermined,0
"Synergizing people, process, and technology to motivate knowledge sharing and collaboration Industry case study","Intel is the world's largest Semiconductor chip manufacturing company with over 95,000 employees and 164 sites in 63 countries across the globe. Not only does Intel's IT group keep Intel's business operations running, it also contributes to Intel's business transformation via user experience research and architecture path finding for leading edge technologies. IT sees social computing as a strategic way to improve collaboration, foster innovation, and facilitate learning. Our research has identified the best opportunities for using social computing and other technologies to boost collaboration and productivity across Intel. To keep achieving maximum benefit from its collaboration efforts, Intel IT continues to invest in social capabilities and also partners with Intel HR to help address cultural and motivational barriers. Beyond improving personal productivity, we are looking to enable efficiency in Intel's business divisions for product design, manufacturing, and sales, through the use of cutting-edge social technologies, including social analytics, immersive video/sketching, federated identity and access management, and cross system activity stream aggregation. Our goal is to continue to transform collaboration across Intel into a seamless and unified experience that brings together relevant information, people, and business intelligence to fully support employee and business workflows.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6567200,no,undetermined,0
Synthesizable Integrated Circuit and System Design for Solar Chargers,"In this paper, an automatic design tool for a solar energy harvesting IC and system is developed with visual basic software, and the synthesis tool employed in this approach can be used to shorten the design time to market. In addition, a smart meter system is developed to measure the solar energy harvesting system's information with an online system. Users can thus get the proposed system's information at any time and from anywhere. Finally, good agreement has been found between the analytic and experimental results.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320660,no,undetermined,0
Teaching business analytics,"It is essential to prepare students with knowledge and skills in area of business analytics (BA) which will help business to process data, find patterns and relations, develop insights from past transactions, and make prediction. We develop hands-on labs to teach business analytics to students in Computer Science, Information Technology, and Software Engineering disciplines. Our hands-on labs can be adopted in courses such as database systems, data warehousing, data mining, etc. We use enterprise BA tools including MS SQL Server Business Intelligence and Cognos 10 platforms, which are essential to increase student interests, improve student learning, and enhance student confidence. Our hands-on labs contain three parts with one is built upon another: 1) Data integration; 2) Data Warehouse; and 3) Business analytics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6685090,no,undetermined,0
SASH: Enabling continuous incremental analytic workflows on Hadoop,"There is an emerging class of enterprise applications in areas such as log data analysis, information discovery, and social media marketing that involve analytics over large volumes of unstructured and semi-structured data. These applications are leveraging new analytics platforms based on the MapReduce framework and its open source Hadoop implementation. While this trend has engendered work on high-level data analysis languages, NoSQL data stores, workflow engines etc., there has been very little attention to the challenges of deploying analytic workflows into production for continuous operation. In this paper, we argue that an essential platform component for enabling continuous production analytic workflows is an analytics store. We highlight five key requirements that impact the design of such a store: (i) efficient incremental operations, (ii) flexible storage model for hierarchical data, (iii) snapshot support (iv) object-level incremental updates, and (v) support for handling change sets. We describe the design of SASH, a scalable analytics store that we have developed on top of HBase to address these requirements. Using the workload from a production workflow that powers search within IBM's intranet and extranet, we demonstrate orders of magnitude improvement in IO performance using SASH.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6544911,no,undetermined,0
SALMA: Standard Arabic Language Morphological Analysis,"Morphological analyzers are preprocessors for text analysis. Many Text Analytics applications need them to perform their tasks. This paper reviews the SALMA-Tools (Standard Arabic Language Morphological Analysis) [1]. The SALMA-Tools is a collection of open-source standards, tools and resources that widen the scope of Arabic word structure analysis - particularly morphological analysis, to process Arabic text corpora of different domains, formats and genres, of both vowelized and non-vowelized text. Tag-assignment is significantly more complex for Arabic than for many languages. The morphological analyzer should add the appropriate linguistic information to each part or morpheme of the word (proclitic, prefix, stem, suffix and enclitic); in effect, instead of a tag for a word, we need a subtag for each part. Very fine-grained distinctions may cause problems for automatic morphosyntactic analysis - particularly probabilistic taggers which require training data, if some words can change grammatical tag depending on function and context; on the other hand, fine-grained distinctions may actually help to disambiguate other words in the local context. The SALMA - Tagger is a fine grained morphological analyzer which is mainly depends on linguistic information extracted from traditional Arabic grammar books and prior-knowledge broad-coverage lexical resources; the SALMA - ABCLexicon. More fine-grained tag sets may be more appropriate for some tasks. The SALMA - Tag Set is a standard tag set for encoding, which captures long-established traditional fine-grained morphological features of Arabic, in a notation format intended to be compact yet transparent.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6487311,no,undetermined,0
Ripple: Improved Architecture and Programming Model for Bulk Synchronous Parallel Style of Analytics,"We present Ripple, an architecture and a programming model for a broad set of data analytics. Ripple builds on the ideas of iterated MapReduce and adds two innovations. First it has a richer programming model, including more ideas from the Bulk Synchronous Parallel (BSP) model of computation and others. By doing so, Ripple creates a flexible and higher-level platform that is easier for both application programmers and platform implementors. Second, Ripple is based on a limited interface for key/value storage making it portable among many different key/value store implementations. By building on these two ideas Ripple improves the scope, performance, and openness of the data analytics platform. We evaluate Ripple using three representative, and non-trivial, data analysis scenarios requiring iterative computation. Using these examples, we show how Ripple achieves clear performance advantages over iterated MapReduce.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681615,no,undetermined,0
Rethinking computer architecture for throughput computing,"Summary form only given. The rise of rich media in mobile devices and massive analytics in data centers has created new opportunities and challenges for computer architects. On one hand, commercial computer organizations have been undergoing fast transformation to drastically increase the throughput of processing large amounts of data while keeping the power consumption in check. On the other hand, computer architecture has evolved too slowly to facilitate hardware innovations, software productivity, algorithm advancement and user perceived improvements. In this talk, I will present some major challenges facing the computer architecture research community and some recent advancements in throughput computing. I will conclude by arguing that we must rethink the scope of computer architecture research as we seek to create growth paths for the computer systems industry.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621096,no,undetermined,0
Pacer: A Progress Management System for Live Virtual Machine Migration in Cloud Computing,"Live migration of virtual machines is a key management function in cloud computing. Unfortunately, no live migration progress management system exists in the state-of-theart, leading to (1) guesswork over how long a migration might take and the inability to schedule dependent tasks accordingly; (2) unacceptable application degradation when application components become split over distant cloud datacenters for an arbitrary period during migration; (3) inability to tradeoff application performance and migration time e.g. to finish migration later for less impact on application performance. Pacer is the first migration progress management system that solves these problems. Pacer's techniques are based on robust and lightweight run-time measurements of system and workload characteristics, efficient and accurate analytic models for progress predictions, and online adaptation to maintain user-defined migration objectives for coordinated and timely migrations. Our experiments on a local testbed and on Amazon EC2 show that Pacer is highly effective under a range of application workloads and network conditions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662353,no,undetermined,0
Parallel-Resonance-Type Fault Current Limiter,"This paper proposes a new parallel-<i>LC</i>-resonance-type fault current limiter (FCL) that uses a resistor in series with a capacitor. The proposed FCL is capable of limiting the fault current magnitude near to the prefault magnitude of distribution feeder current by placing the mentioned resistor in the structure of the FCL. In this way, the voltage of the point of common coupling does not experience considerable sag during the fault. In addition, the proposed FCL does not use a superconducting inductor which has high construction cost. Analytical analysis for this structure is presented in detail, and simulation results using power system computer-aided design/electromagnetic transients, including dc software are obtained to validate the effectiveness of this structure. Also, an experimental setup is provided to show the accuracy of the analytic analyses and simulation results.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6192321,no,undetermined,0
Pathways to technology transfer and adoption: Achievements and challenges (mini-tutorial),"Producing industrial impact has often been one of the important goals of academic or industrial researchers when conducting research. However, it is generally challenging to transfer research results into industrial practices. There are some common challenges faced when pursuing technology transfer and adoption while particular challenges for some particular research areas. At the same time, various opportunities also exist for technology transfer and adoption. This mini-tutorial presents achievements and challenges of technology transfer and adoption in various areas in software engineering, with examples drawn from research areas such as software analytics along with software testing and analysis. This mini-tutorial highlights success stories in industry, research achievements that are transferred to industrial practice, and challenges and lessons learned in technology transfer and adoption.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606644,no,undetermined,0
PatRICIA -- A Novel Programming Model for IoT Applications on Cloud Platforms,"Cloud computing technologies have recently been intensively exploited for the development and management of large-scale IoT systems, due to their capability to integrate diverse types of IoT devices and to support big IoT data analytics in an elastic manner. However, due to the diversity, complexity and scale of IoT systems, the need to handle large volumes of IoT data in a nontrivial manner, and the plethora of domain-dependent IoT controls, programming IoT applications on cloud platforms still remains a great challenge. To date, existing work neglects high-level programming models and focuses on low-level IoT data and device integration. In this paper, we outline PatRICIA, which aims at providing an end-to-end solution for high-level programming and provisioning of IoT applications on cloud platforms. We present a novel programming model, based on the concept of intent and intent scope. Further, we introduce its runtime for dealing with the complexity, diversity and scale of IoT systems in the cloud. Our programming model defines abstractions to enable easier, efficient and more intuitive development of cloud-scale IoT applications. To illustrate our programming model, we present a case study with real-world applications for controlling and managing electric vehicles.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6717285,no,undetermined,0
Performance analysis for priority based broadcast in vehicular networks,"Transportation safety is one of the most important applications for vehicular ad-hoc networks based on IEEE 802.11p. When a vehicle is in an emergency condition, a safety-related message is transmitted to the neighboring vehicles and infrastructures. Vehicles and infrastructures are exchanging periodic messages on the vehicle position, traffic information to provide various services. When the traffic load is very high, the emergency message cannot be delivered immediately. To overcome this situation, a priority based transmission scheme is considered to guarantee the transmission of the emergency message. We analyze the performance of vehicular communication networks in two perspectives. Firstly, an analytical Markov-chain model for vehicle-to-vehicle (V2V) ad-hoc communication networks is proposed for broadcasting messages with priority based on IEEE 802.11p. Secondly, an analytic Queuing model for vehicular communication networks are proposed to evaluate the network performance for dealing with safety and non-safety messages from the point of the infrastructure.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614776,no,undetermined,0
Performance analysis of a fault-tolerant exact motif mining algorithm on the cloud,"In this paper, we present the performance analysis and design challenges of implementing a fault-tolerant parallel exact motif mining algorithm leveraging the services provided by the underlying cloud storage platform (e.g., data replication, node failure detection). More specifically, first, we present the design of the intermediate data structures and data models that are needed for effective parallelization of the motif mining algorithm on the cloud. Second, we present the design and implementation of a fault-tolerant parallel motif mining algorithm that enables the data analytic system to recover from arbitrary node failures in the cloud environment by detecting node failures and redistributing remaining computational tasks in real-time. We also present a data caching scheme to improve the system performance even further. We evaluated the impact of various factors such as the replication factor and random node failures on the performance of our system using two different datasets, namely, an EOG dataset and an image dataset. In both cases, our algorithm exhibits superior performance over the existing algorithms, thus demonstrating the effectiveness of our presented system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6742786,no,undetermined,0
Performance Evaluation of an Audio Tuner System Based on AIR Audio Processing Library,"In the development of applications for mobile devices that can be easily portable and lightweight, the language of the byte code, such as Action script is preferred. However, if you need a high speed, it is necessary to cooperate with software library written in C/C++. In its cooperation, buffer management, such as efficiency of binary data conversion, is still an important research topic. In this paper, we focus on the buffer management of spectrum analysis on mobile audio signal processing. And, we report the performance evaluation in cooperation with the Fourier transform C++ language library and Action script running on Adobe AIR.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498265,no,undetermined,0
Preface: Massive-scale analytics,"Big Data refers to large datasets that are beyond the capability of traditional software tools to quickly manage,process, and analyze. The development of techniques for gaining insight from such information provides potential benefits in such arenas as business, science, and public policy. Big Data is generally characterized by its volume, variety, velocity, and veracity. This special issue of the IBM Journal of Research and Development emphasizes applications, analytics, software, and hardware technologies that form the foundational building blocks for massive-scale analytics (MSA) and the processing of Big Data.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6517308,no,undetermined,0
Prioritizing CRC cards as a simple design tool in extreme programming,"The analytic hierarchy process (AHP) has been applied in many fields and especially to complex engineering problems and applications. The AHP is capable of structuring decision problems and finding mathematically determined judgments built on knowledge and experience. This suggests that AHP should prove useful in agile software development where complex decisions occur routinely. In this paper, the AHP is used to prioritize Class Responsibility Collaboration (CRC) cards in Extreme Programming (XP) design activity. XP encourages the simplicity in design that takes less time to accomplish than more complex approaches. The CRC cards tool has been proved to effectively achieve this purpose. Moreover, prioritizing CRC cards helps the XP team to implement the most valuable class to design.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6567820,no,undetermined,0
Ranking of financial and electronic debts using analytic hierarchy process (AHP),"Financial and electronic debts are one of the important issues in Iran's financial scope that are considered by economists due to their role in creating financial instabilities. Therefore, a model was represented in this survey to evaluate and rank financial and electronic debts in Iran including foreign debts, governmental debts, non-governmental debts and banking debts during the period 1999-2012 using analytic hierarchy process (AHP). To this end, four indexes of volume of debt, ability to repay, willingness to repay and return rate of debts were identified and paired comparison of debts was conducted given to the research literature in this scope and interview with experts. Then total weight of each index was calculated, score of each index was exploited and finally ranking of all kinds of financial and electronic debts was determined by sum of the score of indexes. Expert Choice 11 software was used to calculate the weights.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6556751,no,undetermined,0
Rapid virtual prototyping of real-time systems using predictable platform characterizations,"Virtual prototypes (VPs) provide an early development platform to embedded software designers when the hardware is not ready yet and allows them to explore the design space of a system, both from the software and architecture perspective. However, automatic generation of VPs is not straightforward because several aspects such as the validity of the generated platforms and the timing of the components needs to be considered. To address this problem, based on a framework which characterizes predictable platform templates, we propose a method for automated generation of VPs which is integrated into a combined design flow consisting of analytic and simulation based design-space exploration. Using our approach the valid TLM 2.0-based simulated VP instances with timing annotation can be generated automatically and used for further development of the system in the design flow. We have demonstrated the potential of our method by designing a JPEG encoder system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6646652,no,undetermined,0
Real-Time Analytics for Legacy Data Streams in Health: Monitoring Health Data Quality,"Healthcare organizations are increasingly using information technology to ensure patient safety, increase effectiveness and improve efficiency of healthcare delivery. While the use of health information technology (HIT) has realized many improvements, it has also introduced new failure modes arising from data quality and IT system usability issues. This paper presents an approach towards addressing these failure modes by applying real-time analytics to existing streams of clinical messages exchanged by HIT systems. We use complex event processing provided by the Event Swarm software framework to monitor data quality in such systems through intercepting messages and applying rules reflecting the syndromic surveillance model proposed in [4]. We believe this is the first work reporting on the real-time application of syndromic surveillance rules to legacy clinical data streams. Our design and implementation demonstrates the feasibility of this approach and highlights benefits obtained through improved operational quality of HIT systems, notably better patient safety, reduced risks in healthcare delivery and potentially reduced costs.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658267,no,undetermined,0
Real-time data analysis in ClowdFlows,"ClowdFlows is an open cloud based platform for composition, execution, and sharing of interactive data mining workflows. In this paper we extend the ClowdFlows platform with the ability to mine real-time data streams. This functionality was implemented by creating a specialized type of workflow component and a stream mining daemon that delegates the execution of workflows in real-time. In this way, we have transformed a batch data processing platform into a real-time stream mining platform with an intuitive user interface. The real-time analytics aspect of the platform is demonstrated in a Twitter sentiment analysis use case where the sentiment of tweets about whistleblower Edward Snowden was monitored for approximately one month.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691682,no,undetermined,0
Reliability of erasure coded storage systems: A geometric approach,"We consider the probability of data loss in an erasure coded distributed storage system. Data loss in an erasure coded system depends on the repair duration and the failure probability of individual disks. This dependence on the repair duration complicates the data loss probability analysis. In previous work, the data loss probability of such systems has been studied under the assumption of exponentially distributed disk life and disk repair durations, using well-known analytic methods from the theory of Markov processes. Here, we assume that the repair duration is a constant and derive an upper bound on the probability of data loss by calculating the volumes of specific polytopes that are determined by the code. Closed form bounds are exhibited for some example codes.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691662,no,undetermined,0
Research and implementation of AHP scheduling algorithms for load balancing,"For great change of service time for request, big difference of hardware and software server and different network performance, this paper proposes a dynamic-feedback algorithm based on AHP in the course of studying the algorithm of load balancing in the cluster-based system. Combined with Weighted scheduling algorithm of the kernel, based on the parameters influencing the performance of cluster system from dynamic feedback, we can adjust the servers' weight, solve the load imbalance problem among the servers effectively and certainly improve the throughput of the whole system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784988,no,undetermined,0
Research of analytic hierarchy process applying for case based reasoning in automotive welding jig design,"On the basis of the structure of automotive welding jigs, the welding jig case database is constructed in this paper. The analytic hierarchy process (AHP) is applied for partitioning the influence factors of automotive welding jig case unit. The weight calculation and weighted coefficient distribution of case unit properties is presented adopting 1 - 9 scaling algorithm. Thus similarity can be calculated using neighbor similarity. The system case based on reasoning for welding jig case database is developed with VC++6.0 and NX8.0 CAD software, which satisfies the design requirement of rapidity, high efficiency, flexibility and portability.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6885574,no,undetermined,0
Research on Fuzzy Comprehensive Evaluation of Human-Machine Interface Layout of Driller Control Room,"In order to evaluate the human-machine interface layout of oil rig driller control room effectively, a kind of evaluation method that uses CATIA software simulation and Fuzzy AHP (Fuzzy-Analytic Hierarchy Process) was put forward. According to the drillers' visual area, human body size, comfortable joint range and so on physiological characteristics, CATIA was used to evaluate the accessibility, visibility, and comfort objectively. Combining with the expert opinions of drillers, such as psychological characteristics, operating habits and experience, the evaluation level and evaluation indexes were determined, Fuzzy-AHP evaluation method was used to realize the conversion of qualitative index to quantitative index, and overcome the subjectivity of the evaluation. This comprehensive method balanced the objective evaluation and subjective evaluation, and realized the scientific evaluation of human-machine interface layout of driller control room. Finally the further research work was pointed out.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6643213,no,undetermined,0
Resource optimization for speculative execution in a MapReduce Cluster,"The MapReduce paradigm is now the de facto standard for large-scale data analytics. In this paper we address the resource management issues in MapReduce Cluster. Speculative execution (task backup) plays an important role in resource management. We propose two different strategies and build two models to formulate the backup issue as an optimization problem when the cluster is lightly loaded. Moreover, we present an Enhanced Speculative Execution (ESE) algorithm when the cluster is heavily loaded and adopt the approximate analysis to get an optimal value for the parameter in the algorithm. The simulation results show that the algorithm can reduce the job completion time by 50% while consuming much less resource compared to the naive method without backup.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733646,no,undetermined,0
Resource Provisioning for Staging Components,"To deal with the inordinate output data volumes of current and future high end simulations, researchers are turning to online methods in which multiple software components that implement desired data analytics and visualization are run on 'staging resources' of the petascale machine, concurrently and coupled with the simulations producing these outputs. Efficient online execution of data analytics 'in the output stream', however, requires careful provisioning of staging resources, to obtain delays for analytics processing that prevent applications from blocking on stalled output, while also bounding total required staging resources. This paper addresses the 'staging provisioning' problem, assuming sets of components arranged as potentially multiple analytics/output pipelines that differ in runtime behavior and resource requirements. For such configurations, it then meets the throughput constraint of online analytics while also minimizing end-to-end pipeline latency, all based on runtime observations and predictions of component performance. Experimental evaluations demonstrate the algorithm's utility. Its complexity for minimizing latency without violating throughput constraints is O(M), where M is the number of components in the staging area.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651098,no,undetermined,0
Big Data issues in Computational Chemistry,"Digital data have become a torrent engulfing every area of business, science and engineering disciplines. In the age of Big Data, deriving values and insights from large amounts of data using rich analytics becomes an important differentiating capability for competitiveness, success and leadership in every field. Scientists and engineers of many different domains are increasingly clamouring for mechanisms to manage and analyse the massive quantities of information now available in order to obtain new answers and extract from it maximum value. Computational modelling and simulation is the central technology to numerous of these domains. Molecular Dynamics (MD) is a computational simulation technique that describes the physical forces and movements of interacting microscopic elements such atoms and molecules. MD has important applications in the fields of chemistry, biotechnology, pharmaceutical industry, energy, climate or materials science, among others. Advanced MD algorithms include not only Molecular Mechanics (MM), but also Quantum Mechanics (QM) approaches, raising important big data challenges still to be sorted out. MD simulations perform an iterative process generating large amounts of data in streaming. Current software technology is far from being able to manage, analyze and visualize the extremely large and complex data sets generated by important molecular processes. This paper analyzes the current big data limits in the Computational Chemistry field, especially in the MD processes. To overcome these challenging situations, this work provide guidance for future research including advances in scalable algorithms for data analysis, dynamic query technology, data models and storage strategies, parallel executions, I/O optimization, and interactive visual exploration and analysis of MD data.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984225,no,undetermined,0
Big data machine learning and graph analytics: Current state and future challenges,"Big data machine learning and graph analytics have been widely used in industry, academia and government. Continuous advance in this area is critical to business success, scientific discovery, as well as cybersecurity. In this paper, we present some current projects and propose that next-generation computing systems for big data machine learning and graph analytics need innovative designs in both hardware and software that provide a good match between big data algorithms and the underlying computing and storage resources.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004471,no,undetermined,0
Big Data Opportunities and Challenges: Discussions from Data Analytics Perspectives [Discussion Forum],"""Big Data"" as a term has been among the biggest trends of the last three years, leading to an upsurge of research, as well as industry and government applications. Data is deemed a powerful raw material that can impact multidisciplinary research endeavors as well as government and business performance. The goal of this discussion paper is to share the data analytics opinions and perspectives of the authors relating to the new opportunities and challenges brought forth by the big data movement. The authors bring together diverse perspectives, coming from different geographical locations with different core research expertise and different affiliations and work experiences. The aim of this paper is to evoke discussion rather than to provide a comprehensive survey of big data research.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920114,no,undetermined,0
Simulation based analytics for efficient planning and management in multimodal freight transportation industry,"The multimodal freight transportation planning is a complex problem with several factors affecting decisions, including network coverage, carriers and their schedules, existing contractual agreements with carriers and clients, carrier capacity constraints, and market conditions. Day-to-day operations like booking and bidding are mostly done manually and there is a lack of decision support tools to aid the operators. These operations are governed by a complex set of business rules involving service agreements with the clients, contractual agreements with the carriers and forwarder's own business objectives. The multimodal freight transportation industry lacks a comprehensive solution for end-to-end route optimization and planning. We developed analytics for trade lane managers to identify and exploit opportunities to improve procurement, carrier selection, capacity planning, and business rules management. Our simulation based analytics tool is useful for managing business rules and for doing what-if analysis which can lead to better resource planning, cost management, and rate negotiations.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020041,no,undetermined,0
Scaling Irregular Applications through Data Aggregation and Software Multithreading,"Emerging applications in areas such as bioinformatics, data analytics, semantic databases and knowledge discovery employ datasets from tens to hundreds of terabytes. Currently, only distributed memory clusters have enough aggregate space to enable in-memory processing of datasets of this size. However, in addition to large sizes, the data structures used by these new application classes are usually characterized by unpredictable and fine-grained accesses: i.e., they present an irregular behavior. Traditional commodity clusters, instead, exploit cache-based processor and high-bandwidth networks optimized for locality, regular computation and bulk communication. For these reasons, irregular applications are inefficient on these systems, and require custom, hand-coded optimizations to provide scaling in both performance and size. Lightweight software multithreading, which enables tolerating data access latencies by overlapping network communication with computation, and aggregation, which allows reducing overheads and increasing bandwidth utilization by coalescing fine-grained network messages, are key techniques that can speed up the performance of large scale irregular applications on commodity clusters. In this paper we describe GMT (Global Memory and Threading), a runtime system library that couples software multithreading and message aggregation together with a Partitioned Global Address Space (PGAS) data model to enable higher performance and scaling of irregular applications on multi-node systems. We present the architecture of the runtime, explaining how it is designed around these two critical techniques. We show that irregular applications written using our runtime can outperform, even by orders of magnitude, the corresponding applications written using other programming models that do not exploit these techniques.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877341,no,undetermined,0
SCOOP - The Social Collaboratory for Outcome Oriented Primary Care,"Many primary care clinics have transitioned from paper-based record keeping to computer-based Electronic Medical Record (EMR) systems. This transition provides opportunities for computer-based data analytics in support of practice improvement and more evidence-based clinical research. Unfortunately, the data in primary care EMRs is often not readily accessible to researchers, who often have to overcome significant political, organizational and technical hurdles before gaining access to such data. As a consequence, knowledge discovery and translation has been slow and burdensome in this area. Primary care research networks (PCRN) have been proposed as a way to addressing these limitations. This paper reports on the development of a PCRN in British Columbia, referred to as SCOOP (The Social Collaboratory for Outcome Oriented Primary Care). We describe its technical architecture and draw comparisons to related and previous initiatives.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052491,no,undetermined,0
SCOOP -- The Social Collaboratory for Outcome Oriented Primary Care,"Many primary care clinics have transitioned from paper-based record keeping to computer-based Electronic Medical Record (EMR) systems. This transition provides opportunities for computer-based data analytics in support of practice improvement and more evidence-based clinical research. Unfortunately, the data in primary care EMRs is often not readily accessible to researchers, who often have to overcome significant political, organizational and technical hurdles before gaining access to such data. As a consequence, knowledge discovery and translation has been slow and burdensome in this area. Primary care research networks (PCRN) have been proposed as a way to addressing these limitations. This paper reports on the development of a PCRN in British Columbia, referred to as SCOOP (The Social Collaboratory for Outcome Oriented Primary Care). We describe its technical architecture and draw comparisons to related and previous initiatives.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881955,no,undetermined,0
Security configuration analytics using video games,"Computing systems today have a large number of security configuration settings that enforce security properties. However, vulnerabilities and incorrect configuration increase the potential for attacks. Provable verification and simulation tools have been introduced to eliminate configuration conflicts and weaknesses, which can increase system robustness against attacks. Most of these tools require special knowledge in formal methods and precise specification for requirements in special languages, in addition to their excessive need for computing resources. Video games have been utilized by researchers to make educational software more attractive and engaging. Publishing these games for crowdsourcing can also stimulate competition between players and increase the game educational value. In this paper we introduce a game interface, called NetMaze, that represents the network configuration verification problem as a video game and allows for attack analysis. We aim to make the security analysis and hardening usable and accurately achievable, using the power of video games and the wisdom of crowdsourcing. Players can easily discover weaknesses in network configuration and investigate new attack scenarios. In addition, the gameplay scenarios can also be used to analyze and learn attack attribution considering human factors. In this paper, we present a provable mapping from the network configuration to 3D game objects.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6997493,no,undetermined,0
Security Threat Analytics and Countermeasure Synthesis for Power System State Estimation,"State estimation plays a critically important role in ensuring the secure and reliable operation of the power grid. However, recent works have shown that the widely used weighted least squares (WLS) estimator, which uses several system wide measurements, is vulnerable to cyber attacks wherein an adversary can alter certain measurements to corrupt the estimator's solution, but evade the estimator's existing bad data detection algorithms and thus remain invisible to the system operator. Realistically, such a stealthy attack in its most general form has several constraints, particularly in terms of an adversary's knowledge and resources for achieving a desired attack outcome. In this light, we present a formal framework to systematically investigate the feasibility of stealthy attacks considering constraints of the adversary. In addition, unlike prior works, our approach allows the modeling of attacks on topology mappings, where an adversary can drastically strengthen stealthy attacks by intentionally introducing topology errors. Moreover, we show that this framework allows an operator to synthesize cost-effective countermeasures based on given resource constraints and security requirements in order to resist stealthy attacks. The proposed approach is illustrated on standard IEEE test cases.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903576,no,undetermined,0
Security with Privacy -- Opportunities and Challenges: Panel Position Paper,This paper summarizes opportunities and challenges concerning how we can achieve security while still ensuring privacy. It identifies research directions and includes a number of questions that have been debated by the panel.,2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6899246,no,undetermined,0
Semiotics in visualisation,"Digital visualisation is a way of representing data and information with the aid of digital means. It ranges from a simple form such as a graph or chart to a complex form like animated visualisations that allows user to interact with the underlying data through direct manipulation (Chen et al., 2008). The notion of digital visualisation engages human interpretation on information in order to gain insights in a particular context (Robert, 2007, Ware, 2012, Czernicki, 2010). Hence, it is a complex process involving multiple disciplines, including the socio-technical element. The social element relates to human perception in interpreting information. The technical element on the other hand, refers to the technology used to enable visualisation, for example the SAS suite (SAS, 2014) that offers visual analytics to support interactive dashboard and reporting.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7077099,no,undetermined,0
SEPP: Semantics-Based Management of Fast Data Streams,"In the era of big data processing there is an emerging need for methodologies supporting the management of data-intensive application scenarios. Complex Event Processing is an integral part of many fast data application as an underlying technology for event correlation and pattern detection. Increased volume of event streams as well as the demand for more complex real-time analytics require for execution of processing pipelines among heterogeneous event processing engines. In this paper, we propose a semantic model for the management of fast data streams using the concept of Semantic Event Processing Pipelines (SEPP). We provide methodology, architecture and language for semantic discovery and binding of real-time processing services from arbitrary stream processing engines. Our approach aims to improve reusability of real-time processing services by providing high-level interfaces to stream processing implementations. By these means this work paves the way for an easier development and management of real-time big data applications.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6978598,no,undetermined,0
SiLK: A Tool Suite for Unsampled Network Flow Analysis at Scale,"A large organization can generate over ten billion network flow records per day, a high-velocity data source. Finding useful, security-related anomalies in this volume of data is challenging. Most large network flow tools sample the data to make the problem manageable, but sampling unacceptably reduces the fidelity of analytic conclusions. In this paper we discuss SiLK, a tool suite created to analyze this high-volume data source without sampling. SiLK implementation and architectural design are optimized to manage this Big Data problem. SiLK provides not just network flow capture and analysis, but also includes tools to analyze large sets and dictionaries that frequently relate to network flow data, incorporating higher-variety data sources. These tools integrate disparate data sources with SiLK analysis.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906777,no,undetermined,0
SoDA: Dynamic visual analytics of big social data,"In this work we apply dynamic visual analytics on big social data by the example of microblogs from Twitter. Thereby, we address current challenges like real-time analytics as well as analyses of unstructured data. To this end, we propose SoDA - a concept enabling the integrated analysis of the dimensions: message, location and time. Furthermore, we introduce a novel design for tag cloud visualizations, the weighted tag network, offering enhanced semantic insights. All concepts are fully implemented and evaluated by a comprehensive software prototype in different application scenarios.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6741433,no,undetermined,0
The Berkeley Data Analytics Stack (BDAS),"Summary form only given. The session on äóìThe Berkeley Data Analytics Stackäó shall elucidate its current components which include Spark, Shark and Mesos with emphasis on Spark and it's real-time extension called Spark-Streaming which adds stream processing capabilities to Spark. One-liners describing each of these technologies are as follows: 1) BDAS is an open source, next-generation data analytics stack under development at the UC Berkeley AMPLab. 2) Spark, a high-speed cluster computing system compatible with Hadoop that can outperform it by up to 100x thanks to its ability to perform computations in memory. 3) Shark, a port of Apache Hive onto Spark that is compatible with existing Hive warehouses and queries. Shark can answer HiveQL queries up to 100x faster than Hive without modification to the data and queries, and is also open source as part of BDAS. 4) Mesos is a cluster manager that provides efficient resource isolation and sharing across distributed applications or frameworks. It can run Hadoop, MPI, Hypertable, Spark, and other applications on a dynamically shared pool of nodes. 5) Apart, from an elaborate explanation of various facets of Spark, the session would also aim to walk through machine learning algorithm benchmarking and examples that would substantiate the concepts covered.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056925,no,undetermined,0
Software defined environments: An introduction,"During the past few years, enterprises have been increasingly aggressive in moving mission-critical and performance-sensitive applications to the cloud, while at the same time many new mobile, social, and analytics applications are directly developed and operated on cloud computing platforms. These two movements are encouraging the shift of the value proposition of cloud computing from cost reduction to simultaneous agility and optimization. These requirements (agility and optimization) are driving the recent disruptive trend of software defined computing, for which the entire computing infrastructureäóîcompute, storage and networkäóîis becoming software defined and dynamically programmable. The key elements within software defined environments include capability-based resource abstraction, goal-based and policy-based workload definition, and outcome-based continuous mapping of the workload to the available resources. Furthermore, software defined environments provide the tooling and capabilities to compose workloads from existing components that are then continuously and autonomously mapped onto the underlying programmable infrastructure. These elements enable software defined environments to achieve agility, efficiency, and continuous outcome-optimized provisioning and management, plus continuous assurance for resiliency and security. This paper provides an overview and introduction to the key elements and challenges of software defined environments.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6798712,no,undetermined,0
Star Ratings versus Sentiment Analysis -- A Comparison of Explicit and Implicit Measures of Opinions,"A typical trade-off in decision making is between the cost of acquiring information and the decline in decision quality caused by insufficient information. Consumers regularly face this trade-off in purchase decisions. Online product/service reviews serve as sources of product/service related information. Meanwhile, modern technology has led to an abundance of such content, which makes it prohibitively costly (if possible at all) to exhaust all available information. Consumers need to decide what subset of available information to use. Star ratings are excellent cues for this decision as they provide a quick indication of the tone of a review. However there are cases where such ratings are not available or detailed enough. Sentiment analysis -text analytic techniques that automatically detect the polarity of text- can help in these situations with more refined analysis. In this study, we compare sentiment analysis results with star ratings in three different domains to explore the promise of this technique.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758702,no,undetermined,0
Statistical learning and multiple linear regression model for network selection using MIH,"A key requirement to provide seamless mobility and guaranteeing Quality of Service in heterogeneous environment is to select the best destination network during handover. In this paper, we propose a new schema for network selection based on Multiple Linear Regression Model (MLRM). A thorough investigation, on a huge live data collected from GPRS/UMTS networks led to identify the Key Performance Indicators (KPIs) that play the most important role in the handover process. These KPIs are: Received Signal Code Power (RSCP), received energy per chip (Ec/No)and Available Bandwidth (ABW) of the destination network. To extract a handover model from collected data, we study the correlation among values of identified KPIs parameters, before, during and after handover, thanks to a statistical learning approach, using the predictive analytics software SPSS. For model assessment, Pearson Correlation Coefficient and determination coefficient R-squared (R<sup>2</sup>) are used. Media Independent Handover (MIH) IEEE 802.21 standard is used in this work to retrieve the lower layer information of available networks and announce the handover needs (handover initiation). The proposed model will help to select the most appropriate network between many existing ones in the vicinity of the mobile node.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991378,no,undetermined,0
"Stream computing for large-scale, multi-channel cyber threat analytics","The cyber threat landscape, controlled by organized crime and nation states, is evolving rapidly towards evasive, multi-channel attacks, as impressively shown by malicious operations such as GhostNet, Aurora, Stuxnet, Night Dragon, or APT1. As threats blend across diverse data channels, their detection requires scalable distributed monitoring and cross-correlation with a substantial amount of contextual information. With threats evolving more rapidly, the classical defense life cycle of post-mortem detection, analysis, and signature creation becomes less effective. In this paper, we present a highly-scalable, dynamic cybersecurity analytics platform extensible at runtime. It is specifically designed and implemented to deliver generic capabilities as a basis for future cybersecurity analytics that effectively detect threats across multiple data channels while recording relevant context information, and that support automated learning and mining for new and evolving malware behaviors. Our implementation is based on stream computing middleware that has proven high scalability, and that enables cross-correlation and analysis of millions of events per second with millisecond latency. We report the lessons we have learned from applying stream computing to monitoring malicious activity across multiple data channels (e.g., DNS, NetFlow, ARP, DHCP, HTTP) in a production network of about fifteen thousand nodes.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051865,no,undetermined,0
Supporting Decision-Making for Biometric System Deployment through Visual Analysis,"Deployment of biometric systems in the specific environment is not straightforward. Based on pre-deployment performance test results, a decision maker needs to consider the selection of sensors and matching algorithms in terms of the cost, expected false-match and false-non-match failure rates and the underlying quality factors. Which depend on operational scenarios, personnel training, demographics, etc. In this paper, we investigate information aggregation through visualization of fingerprint authentication experiments obtained from a large scale data collection with 494 participants. The data was collected using four biometric image capture devices. Each fingerprint image was analysed with two image quality algorithms, and the matching scores were generated using three different matchers. Additionally we collected and analyzed the impact of demographic characteristics, such as gender, age, ethnicity, height and weight, on system performance.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983865,no,undetermined,0
Supporting Regression Test Scoping with Visual Analytics,"Background: Test managers have to repeatedly select test cases for test activities during evolution of large software systems. Researchers have widely studied automated test scoping, but have not fully investigated decision support with human interaction. We previously proposed the introduction of visual analytics for this purpose. Aim: In this empirical study we investigate how to design such decision support. Method: We explored the use of visual analytics using heat maps of historical test data for test scoping support by letting test managers evaluate prototype visualizations in three focus groups with in total nine industrial test experts. Results: All test managers in the study found the visual analytics useful for supporting test planning. However, our results show that different tasks and contexts require different types of visualizations. Conclusion: Important properties for test planning support are: ability to overview testing from different perspectives, ability to filter and zoom to compare subsets of the testing with respect to various attributes and the ability to manipulate the subset under analysis by selecting and deselecting test cases. Our results may be used to support the introduction of visual test analytics in practice.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823890,no,undetermined,0
System requirements prioritization based on AHP,"When analyzing software requirements specification, the question of determining which tasks need to be implemented first arises often. In fact, in any software project, it's important to begin by identifying and defining priority tasks. This process is called software requirements prioritization. However in this process, the effect of NFRs on FRs to be prioritized has always been neglected, and all the requirements are prioritized independently. In this paper, we propose a new technique for software requirements prioritization taking account of the relationship of dependency between FR and NFR.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016612,no,undetermined,0
System simulation as decision data in heathcare it,"Information Technology in healthcare is an ever-growing enterprise, with medical providers becoming more and more reliant on data to make care decisions. With the increased reliance on these applications for care, questions arise around the availability and manageability of those systems. This paper examines a model which has been developed for the selection of computing infrastructure architectures in healthcare organizations. This model utilizes the Analytics Hierarchy Process (AHP) to weigh the various criteria that come into play for decisions of this nature. Further, to vet the recommendations of the AHP model, and to lend quantitative data to the decision making process, simulations of the various architectural options were built for various application scenarios. The results of these simulations thus serve as additional validation of the model's efficacy. This paper focuses on the use of discrete event simulation using ExtendSim<sup>Œ¬</sup> to assist in the architectural selection process for computing architectures.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019987,no,undetermined,0
Tablet-based interaction panels for immersive environments,"With the current widespread interest in head mounted displays, we perceived a need for devices that support expressive and adaptive interaction in a low-cost, eyes-free manner. Leveraging rapid prototyping techniques for fabrication, we have designed and manufactured a variety of panels that can be overlaid on multi-touch tablets and smartphones. The panels are coupled with an app running on the multi-touch device that exchanges commands and state information over a wireless network with the virtual reality application. Sculpted features of the panels provide tactile disambiguation of control widgets and an onscreen heads-up display provides interaction state information. A variety of interaction mappings can be provided through software to support several classes of interaction techniques in virtual environments. We foresee additional uses for applications where eyes-free use and adaptable interaction interfaces can be beneficial.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6802066,no,undetermined,0
Scalar: Systematic Scalability Analysis with the Universal Scalability Law,"Analyzing the scalability and quality of service of large scale distributed systems requires a highly scalable benchmarking framework with built-in communication and synchronisation functionality, which are features that are lacking in current load generation tools. This paper documents Scalar, our distributed, extensible scalability analysis tool that can generate high request volumes using multiple communicating, coordinated nodes. We show how Scalar offers analytics capabilities that support the Universal Scalability Law. We illustrate Scalar on an electronic payment case study, and find that the framework supports complex work flows and is able to characterize and give predictive insights into the quality of service and relative capacity of the system under test in function of the user load.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984244,no,undetermined,0
Rock Stars of Big Data Analytics [Advertisement],Advertisement: IEEE Computer Society.,2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898689,no,undetermined,0
Research on reliability evaluation and sensitivity analysis of domain software based on AHP method,"This paper aims at the key problem of software engineering - assessment and sensitivity analysis of the domain software reliability to study. Through the existing research, establish the evaluation attributes of comprehensive categorical, and analysis assessment evidence the assessment model computational logic by AHP, establish corresponding assessment method. Finally, aim at the power plant information management system analysis the domain software reliability by this method, and carry on the attribute sensitivity analysis based on relative data. Through practical experiments the paper prove this method can analysis and evaluate the domain software reliability clearly, and propose sensitivity analysis.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976306,no,undetermined,0
Research on dependability of cloud computing systems,"With the continuous growth of application requirements and a significant advance in the research of cloud computing systems, a large number of cloud computing systems nowadays based on different structures and virtualization technologies are still being developed. However, dependability of cloud computing system is always a critical issue for all the cloud service providers, brokers, carriers and consumers around the world. How to ensure the dependability of cloud computing systems is still not well solved by former researchers. This paper first introduces the infrastructure of a cloud computing system and its major actors. Based on the research of the running mode of a cloud computing system, main influencing factors of cloud computing dependability and its normal failure modes are given. It provides a generic definition of cloud computing system dependability, which including availability, performability, security, recoverability and so on. We also discuss some methods to establish cloud computing dependability models, such as analytic method, state space method and simulating method. In order to evaluate the validity of the cloud dependability measurement method, a novel simulating approach is proposed with well-designed framework and procedure. Through reasonable simulating and analysis, proper solutions can be promoted to find and solve the problem related to cloud computing systems dependability. Our future work will focus on an integrated software platform for cloud computing system dependability simulating and verification.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7107234,no,undetermined,0
OCBPSR: Orthogonal Complex BandPass Sampling Receiver,"Second-order/Complex BandPass Sampling Receivers (CBPSR) are attractive for acquiring multi-signals for SDR/CR applications. One of the issues caused by the implementation of such receivers is the signals IQ mismatch. This paper proposes a CBPSR implementation that eliminates this IQ mismatch by reformatting the received signals in an orthogonal analytic form (thus named OCBPSR). Our implementation will also reduce the required sampling frequency to below the Nyquist rate, and so reducing the processing time to recover the signals. This is achieved by folding the upper-side of the received signals to the same fold-frequency in the baseband domain without overlapping, by making the signals orthogonal. MATLAB simulation is used to evaluate the performance of our OCBPSR using various scenarios of harsh signal environment, including Doppler and multipath effects.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6916633,no,undetermined,0
OOPN-SRAM: A Novel Method for Software Risk Assessment,"This paper proposes a Software Risk Assessment Method based on Object-Oriented Petri Net (OOPN-SRAM), in which risk assessment procedure is divided into four steps, expressed as four corresponding objects, including asset recognition, weakness analysis, consequence property confirmation and risk calculation. Each object is modeled with Petri net. Specialists recognize software assets by the 1-9 scales method of Analytic Hierarchy Process (AHP). The weaknesses in a system are found by the vulnerability scanner. The damage degree and the exploitation likelihood of a weakness are evaluated by such authorities as Common Weakness Enumeration (CWE). The consequence properties are confirmed by specialists according to the software requirements. Finally, in the risk calculation, risk degree and overall risk value are calculated by using exponential method and weighted average method respectively. Furthermore, we illustrate the application of our OOPN-SRAM method with realistic examples including web-banking and forum, and make a comparison with traditional methods. The results show that OOPN-SRAM not only increases the efficiency of the evaluation process, but also makes the evaluation result more objective and accurate.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923130,no,undetermined,0
Opening the Black Box: Strategies for Increased User Involvement in Existing Algorithm Implementations,"An increasing number of interactive visualization tools stress the integration with computational software like MATLAB and R to access a variety of proven algorithms. In many cases, however, the algorithms are used as black boxes that run to completion in isolation which contradicts the needs of interactive data exploration. This paper structures, formalizes, and discusses possibilities to enable user involvement in ongoing computations. Based on a structured characterization of needs regarding intermediate feedback and control, the main contribution is a formalization and comparison of strategies for achieving user involvement for algorithms with different characteristics. In the context of integration, we describe considerations for implementing these strategies either as part of the visualization tool or as part of the algorithm, and we identify requirements and guidelines for the design of algorithmic APIs. To assess the practical applicability, we provide a survey of frequently used algorithm implementations within R regarding the fulfillment of these guidelines. While echoing previous calls for analysis modules which support data exploration more directly, we conclude that a range of pragmatic options for enabling user involvement in ongoing computations exists on both the visualization and algorithm side and should be used.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875995,no,undetermined,0
Ophidia: A full software stack for scientific data analytics,"The Ophidia project aims to provide a big data analytics platform solution that addresses scientific use cases related to large volumes of multidimensional data. In this work, the Ophidia software infrastructure is discussed in detail, presenting the entire software stack from level-0 (the Ophidia data store) to level-3 (the Ophidia web service front end). In particular, this paper presents the big data cube primitives provided by the Ophidia framework, discussing in detail the most relevant and available data cube manipulation operators. These primitives represent the proper foundations to build more complex data cube operators like the apex one presented in this paper. A massive data reduction experiment on a 1TB climate dataset is also presented to demonstrate the apex workflow in the context of the proposed framework.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903706,no,undetermined,0
Opportunities and challenges of the Internet of Things for healthcare: Systems engineering perspective,"In the incoming world of Internet of Things (IoT) for healthcare, different distributed devices will gather, analyze and communicate real time medical information to open, private or hybrid clouds, making it possible to collect, store and analyze big data streams in several new forms, and activate context dependent alarms. This innovative data acquisition paradigm allows continuous and ubiquitous medical data access from any connected device over the Internet, and a novel health application ecosystem emerges. In these complex ecosystems could be insufficient to discuss only classical requirements regarding hardware issues and software support of individual elements. In the involved multidisciplinary development area, with intricate vertical and horizontal markets, it is essential a close collaboration between the corresponding stakeholders: endusers, application domain experts, hardware designers, software developers, market specialists, road mapping strategists and even the collaboration of visionaries to implement successful healthcare ecosystems. In this paper we describe some crucial systems engineering viewpoints to analyse the corresponding complex decision space. We complement the general examination of the IoT space by commenting some particular examples and specific details, which correspond to remarkable options of the involved dimensions.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7015961,no,undetermined,0
Optimization of relational database usage involving Big Data a model architecture for Big Data applications,"Effective Big Data applications dynamically handle the retrieval of decisioned results based on stored large datasets efficiently. One effective method of requesting decisioned results, or querying, large datasets is the use of SQL and database management systems such as MySQL. But a problem with using relational databases to store huge datasets is the decisioned result retrieval time, which is often slow largely due to poorly written queries/decision requests. This work presents a model to re-architect Big Data applications in order to efficiently present decisioned results: lowering the volume of data being handled by the application itself, and significantly decreasing response wait times while allowing the flexibility and permanence of a standard relational SQL database, supplying optimal user satisfaction in today's Data Analytics world. We experimentally demonstrate the effectiveness of our approach.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008703,no,undetermined,0
Optimized incremental state replication for automation controllers,"This paper presents optimization techniques for implementing software-based redundancy in industrial control devices. Initially, a brief survey of software-based state replication techniques with a special focus on their applicability in industrial control devices is conducted. The scalability, predictability and low latency of the technique are of particular interest in this case. Based on this survey, an analytic evaluation of different implementation alternatives is performed. As part of this analysis, a novel state replication algorithm is introduced. The approach uses support from the compiler or runtime environment to detect changes in the application state with very low overhead. This information is used to replicate the state of an automation controller in a redundant setup. Lastly, experimental results using a prototype implementation of the presented technique demonstrate that the proposed novel approach is able to perform state replication with constant overhead.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005112,no,undetermined,0
Panel: The future of research in modeling & simulation,"Due to the increasing availability of data and wider use of analytics, the ingredients for increased reliance on modeling and simulation are now present. Tremendous progress has been made in the field of modeling and simulation over the last six decades. Software and methodologies have advanced greatly. In the area of weather, future-casts based on model predictions have become highly accurate and heavily relied upon. This is happening in other domains, as well. In a similar vein, drivers may come to rely upon future-casts of traffic that are based on predictions from models fed by sensor data. The need for and the capabilities of simulation have never been greater. This panel will examine the future of research in modeling and simulation by (1) examining prior progress, (2) pointing out current weaknesses and limitations, (3) highlighting directions for future research, and (4) discussing support for research including funding opportunities.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020122,no,undetermined,0
Performance analysis of virtualized environments using HPC Challenge benchmark suite and Analytic Hierarchy Process,"Steep improvement of cloud computing in recent years, persuaded experts admit it as a suitable and appropriate substitution for traditional computing methods. Nowadays, more and more organizations are getting used to create private clouds, on the other hand, public clouds must be robust enough to handle scientific-driven computing requests of users in an efficient and cost effective manner. Apart from all these necessities, it is intellectual to improve the cloud infrastructure performance by appropriate choices. In this paper we are going to evaluate the performance of some virtualized environments, including VMware ESXi, KVM, Xen, Oracle VirtualBox, and VMware Workstation using HPC Challenge (HPCC) benchmark suite and Open MPI in order to represent solutions for virtualization layer of cloud computing architecture and then designate the best approach in general using Analytic Hierarchy Process.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6802585,no,undetermined,0
"Performance Study of Spindle, A Web Analytics Query Engine Implemented in Spark","This paper shares our experiences building and benchmarking Spindle as an open source Spark-based web analytics platform. Spindle's design has been motivated by real-world queries and data requiring concurrent, low latency query execution. We identify a search space of Spark tuning options and study their impact on Spark's performance. Results from a self-hosted six node cluster with one week of analytics data (13.1GB) indicate tuning options such as proper partitioning can cause a 5x performance improvement.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7037709,no,undetermined,0
Portable indecisive options selected assistance software based on Fuzzy AHP,"This paper presents a kind of indecisiveness selecting solution software installed on android mobile device which can help user to choose the äóìbestäó one thing among the options in most cases. When used, the user can customize any related criterions about the things and program will dynamically generated a questionnaire based on it; User may answer the questionnaire and also can upload the problem to the cloud server invite friends or experts online to participate this survey; Program will analysis the survey results by Fuzzy Analytic Hierarchy Process (FAHP) method, ultimately selected the psychological optimal option from the candidate options.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029865,no,undetermined,0
Predicting player churn in the wild,"Free-to-Play or äóìfreemiumäó games represent a fundamental shift in the business models of the game industry, facilitated by the increasing use of online distribution platforms and the introduction of increasingly powerful mobile platforms. The ability of a game development company to analyze and derive insights from behavioral telemetry is crucial to the success of these games which rely on in-game purchases and in-game advertising to generate revenue, and for the company to remain competitive in a global marketplace. The ability to model, understand and predict future player behavior has a crucial value, allowing developers to obtain data-driven insights to inform design, development and marketing strategies. One of the key challenges is modeling and predicting player churn. This paper presents the first cross-game study of churn prediction in Free-to-Play games. Churn in games is discussed and thoroughly defined as a formal problem, aligning with industry standards. Furthermore, a range of features which are generic to games are defined and evaluated for their usefulness in predicting player churn, e.g. playtime, session length and session intervals. Using these behavioral features, combined with the individual retention model for each game in the dataset used, we develop a broadly applicable churn prediction model, which does not rely on game-design specific features. The presented classifiers are applied on a dataset covering five free-to-play games resulting in high accuracy churn prediction.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6932876,no,undetermined,0
Progressive Testbed Application for Performance Analysis in Real Time Ad Hoc Networks Using SAP HANA,"This paper proposes and subsequently delineates quantification of network security metrics using software defined networking approach in real time using a progressive testbed. This comprehensive testbed implements computation of trust values which lend sentient decision making qualities to the participant nodes in a network and fortify it against threats like blackhole and flooding attacks. AODV and OLSR protocols were tested in real time under ideal and malicious environment using the testbed as the controlling point. With emphasis on reliability, interpreting voluminous data, monitoring attacks immediately with negligible time lag, the paper concludes by justifying the use of SAP HANA and UI5 for the testbed.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906017,no,undetermined,0
Pythia: Faster Big Data in Motion through Predictive Software-Defined Network Optimization at Runtime,"The rise of Internet of Things sensors, social networking and mobile devices has led to an explosion of available data. Gaining insights into this data has led to the area of Big Data analytics. The MapReduce framework, as implemented in Hadoop, is one of the most popular frameworks for Big Data analysis. To handle the ever-increasing data size, Hadoop is a scalable framework that allows dedicated, seemingly unbound numbers of servers to participate in the analytics process. Response time of an analytics request is an important factor for time to value/insights. While the compute and disk I/O requirements can be scaled with the number of servers, scaling the system leads to increased network traffic. Arguably, the communication-heavy phase of MapReduce contributes significantly to the overall response time, the problem is further aggravated, if communication patterns are heavily skewed, as is not uncommon in many MapReduce workloads. In this paper we present a system that reduces the skew impact by transparently predicting data communication volume at runtime and mapping the many end-to-end flows among the various processes to the underlying network, using emerging software-defined networking technologies to avoid hotspots in the network. Dependent on the network oversubscription ratio, we demonstrate reduction in job completion time between 3% and 46% for popular MapReduce benchmarks like Sort and Nutch.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877244,no,undetermined,0
Quantitative analysis of graduate-level engineering management programs,"The purpose of this study is to analyze the curricula of Engineering Management programs awarding a graduate-level EM degree (Master's and Doctoral). The analysis is conducted using text analysis software, which brings the necessary rigor and objectivity required when dealing with qualitative data. Preliminary research using multiple sources (American Society for Engineering Management and American Society of Engineering Education) result in a list of over one-hundred universities that offer a graduate-level degree in EM. To conduct the analysis, a curriculum database will be formed that includes course names and descriptions. This data will provide the input to the conceptual and semantic analysis using a computer tool. The results of this analysis will provide an insight into different threads, commonalities and differences between EM programs.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918590,no,undetermined,0
R-Store: A scalable distributed system for supporting real-time analytics,"It is widely recognized that OLTP and OLAP queries have different data access patterns, processing needs and requirements. Hence, the OLTP queries and OLAP queries are typically handled by two different systems, and the data are periodically extracted from the OLTP system, transformed and loaded into the OLAP system for data analysis. With the awareness of the ability of big data in providing enterprises useful insights from vast amounts of data, effective and timely decisions derived from real-time analytics are important. It is therefore desirable to provide real-time OLAP querying support, where OLAP queries read the latest data while OLTP queries create the new versions. In this paper, we propose R-Store, a scalable distributed system for supporting real-time OLAP by extending the MapReduce framework. We extend an open source distributed key/value system, HBase, as the underlying storage system that stores data cube and real-time data. When real-time data are updated, they are streamed to a streaming MapReduce, namely Hstreaming, for updating the cube on incremental basis. Based on the metadata stored in the storage system, either the data cube or OLTP database or both are used by the MapReduce jobs for OLAP queries. We propose techniques to efficiently scan the real-time data in the storage system, and design an adaptive algorithm to process the real-time query based on our proposed cost model. The main objectives are to ensure the freshness of answers and low processing latency. The experiments conducted on the TPC-H data set demonstrate the effectiveness and efficiency of our approach.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6816638,no,undetermined,0
Recent advances in investigation of propagation of GNSS signals through the ionosphere with local random time-dependent inhomogeneities,Analytic theory is developed for solving the set of Markov's momenta parabolic equations for the case of the inhomogeneous ionospheric background layer with the time varying local random inhomogeneities of the electron density in order to describe the GNSS signal propagation in the regime of strong scintillation formed already at the heights of the inhomogenous ionosphere. It is then employed for constructing the software simulator capable of generating the field amplitude and phase random time series at any given height above the Earth's surface and arbitrary geometry of propagation.,2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6929721,no,undetermined,0
Recognizing Gaits Across Views Through Correlated Motion Co-Clustering,"Human gait is an important biometric feature, which can be used to identify a person remotely. However, view change can cause significant difficulties for gait recognition because it will alter available visual features for matching substantially. Moreover, it is observed that different parts of gait will be affected differently by view change. By exploring relations between two gaits from two different views, it is also observed that a part of gait in one view is more related to a typical part than any other parts of gait in another view. A new method proposed in this paper considers such variance of correlations between gaits across views that is not explicitly analyzed in the other existing methods. In our method, a novel motion co-clustering is carried out to partition the most related parts of gaits from different views into the same group. In this way, relationships between gaits from different views will be more precisely described based on multiple groups of the motion co-clustering instead of a single correlation descriptor. Inside each group, a linear correlation between gait information across views is further maximized through canonical correlation analysis (CCA). Consequently, gait information in one view can be projected onto another view through a linear approximation under the trained CCA subspaces. In the end, a similarity between gaits originally recorded from different views can be measured under the approximately same view. Comprehensive experiments based on widely adopted gait databases have shown that our method outperforms the state-of-the-art.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680737,no,undetermined,0
Recomputation.org: Experiences of Its First Year and Lessons Learned,"We founded recomputation.org about 18 months ago as we write. The site is intended to serve as a repository for computational experiments, embodied in virtual machines so that they can be recomputed at will by other researchers. We reflect in this paper on those aspects of recomputation.org that have worked well, those that have worked less well, and to what extent our views have changed on reproducibility in computational science.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027625,no,undetermined,0
Testability of object-oriented systems: An AHP-based approach for prioritization of metrics,This paper investigates testability from the perspective of metrics used in an object-oriented system. The idea is to give an overview of object oriented design metrics with the prioritization of same keeping testability as the overall goal. We have used Analytic Hierarchy Process (AHP) method to attain which metric is mostly used and is best for testability.,2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019595,no,undetermined,0
The CACTOS Vision of Context-Aware Cloud Topology Optimization and Simulation,"Recent advances in hardware development coupled with the rapid adoption and broad applicability of cloud computing have introduced widespread heterogeneity in data centers, significantly complicating the management of cloud applications and data center resources. This paper presents the CACTOS approach to cloud infrastructure automation and optimization, which addresses heterogeneity through a combination of in-depth analysis of application behavior with insights from commercial cloud providers. The aim of the approach is threefold: to model applications and data center resources, to simulate applications and resources for planning and operation, and to optimize application deployment and resource use in an autonomic manner. The approach is based on case studies from the areas of business analytics, enterprise applications, and scientific computing.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7037644,no,undetermined,0
NanoStreams: Advancing the hardware and software stack for real-time analytics on fast data streams,"NanoStreams is a consortium project funded by the European Commission under its FP7 programme and is a major effort to address the challenges of processing vast amounts of data in real-time, with a markedly lower carbon footprint than the state of the art. The project addresses both the energy challenge and the high-performance required by emerging applications in real-time streaming data analytics. NanoStreams achieves this goal by designing and building disruptive micro-server solutions incorporating real-silicon prototype micro-servers based on System-on-Chip and reconfigurable hardware technologies.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058143,no,undetermined,0
VAST 2014: Summary on Mini Challenge two,"VAST 2014 mini challenge two required us to analyze movements and tracking data in a fictional company employee missing event. This paper includes our design and software to tackle the challenge. In addition, the procedure to find the abnormal patterns in the given data is illustrated.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042567,no,undetermined,0
Traversing Trillions of Edges in Real Time: Graph Exploration on Large-Scale Parallel Machines,"The world of Big Data is changing dramatically right before our eyes-from the amount of data being produced to the way in which it is structured and used. The trend of ""big data growth"" presents enormous challenges, but it also presents incredible scientific and business opportunities. Together with the data explosion, we are also witnessing a dramatic increase in data processing capabilities, thanks to new powerful parallel computer architectures and more sophisticated algorithms. In this paper we describe the algorithmic design and the optimization techniques that led to the unprecedented processing rate of 15.3 trillion edges per second on 64 thousand Blue Gene/Q nodes, that allowed the in-memory exploration of a petabyte-scale graph in just a few seconds. This paper provides insight into our parallelization and optimization techniques. We believe that these techniques can be successfully applied to a broader class of graph algorithms.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877276,no,undetermined,0
User Environment Tracking and Problem Detection with XALT,"This work enhances our understanding of individual users' software needs, then leverages that understanding to help stakeholders conduct business in a more efficient, effective, and systematic way. The product, XALT, builds on work that is already improving the user experience and enhancing support programs for thousands of users on twelve supercomputers across the United States and Europe. XALT will instrument individual jobs on high-end computers to generate a picture of the compilers, libraries, and other software that users need to run their jobs successfully. It will highlight the products our researchers need and do not need, and alert users and support staff to the root causes of software configuration issues as soon as the problems occur. A key objective of this work is generating the information needed to improve efficiency and effectiveness for an extensive community of stakeholders including users, sponsoring institutions, support organizations, and development teams. Efficiency, effectiveness, and responsible stewardship each require a clear picture of users' needs. XALT is an important step in the quest to achieve that clarity.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081224,no,undetermined,0
Using commercial web services to build Automated Test Equipment cloud based applications,"The purpose of this paper is to present a framework from which Automated Test Equipment (ATE) manufacturers can use to help them develop, integrate, and deliver ATE cloud-based applications to the consumers of their products. In order to create these applications, the developer can utilize Commercial Web Services (CWS) as a means to help access compute power, storage devices, and other services that provide the flexibility to choose a development platform or programming model that makes the most sense in trying to resolve the problem at hand. CWS provides a flexible environment from which to choose various programming models, operating systems, databases, and architectures to serve the consumers needs. CWS is highly cost-effective in that the developers and consumers pay only for what they use. Using CWS makes it extremely easy to create scalable and elastic systems as the developers can quickly add and subtract resources to their applications in order to meet current or future consumer needs. Furthermore, security is always a concern so CWS builds services in accordance with security best practices by providing the appropriate security features in those services. Using a CWS provides a level of scale, security, reliability, and privacy that are often cost prohibitive for most organizations to meet. This paper will examine available CWS cloud service platforms that organizations can potentially use to help deploy applications and services in a cost effective manner. The CWS platform consists of the following six main services, which will be discussed in more detail in this paper - (1) Computational/Networking, (2) Storage/Content Delivery, (3) Databases, (4) Analytics, (5) Applications, and (6) Deployment and Management services. One or more of these services may be utilized to help develop, integrate, and deliver ATE cloud-based applications to the consumer. The goals of ATE are to (1) quickly and accurately detect and isolate each fault, (2) provide software tool- for analyzing historical data, (3) gather, manage, and distribute accurate and reliable maintenance information for the failed Unit Under Test (UUT). The CWS cloud platform will aid in the development of cloud based tools and applications that are cost effective, flexible, scalable, and secure that can be used by multiple end users to aid in the development of ATE system software tool sets. One example of a cloud-based application is a diagnostic reasoner that could be used to aid in diagnosing UUT repair actions. This paper will show how developers can use CWS to develop ATE cloud-based applications and tools that will help improve the overall ATE testing throughput, thus resulting in bottom line improvements to ATE life cycle costs.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935153,no,undetermined,0
Using data visualization to facilitate secure engineering hardening your code through improved next generation softSecVis,"Cyber security threats and vulnerabilities are as prevalent today as ever. Security should be designed and built into programs from day one. There are many code analysis and scanners available, but few offer a data visualization view, and little has been written about leveraging data analytics and visualization for the improvement of security posture across a code base. Analyzing large enterprise projects results in both massive data consumed and reported. The need to visualize this data for security, softSecVis, is very real. We examine prior findings, prototypes, and current tools used in software security visualization at the code level. We go on to propose several new methods of visualizing security vulnerabilities.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6950668,no,undetermined,0
Using infographies as a tool for introductory data analytics education in 9äóñ12,"Data Analytics is an all pervasive concept and involves procuring and analyzing data, visualizing the results of the analysis, and drawing conclusions and insights from the results. This paper describes the results of using infographies to teach data analytics to 262 high school students in grades 9 through 12 across four schools in Philadelphia. For the purpose of this paper, an infographie is defined as a graphical representation of information gleaned from data-driven analyses. A collaboration between university professors, Ph.D. students, and high-school teachers enabled covering core data analytics topics in multiple science classes. Results from the pre-class and post-class survey results indicate that students were able to gain skills to create infographies using basic descriptive statistics. Students were able to articulate data-driven insights and explain their findings using the infographies in the classroom.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044488,no,undetermined,0
Using open source modeling tools to enhance engineering analysis,"There are several very capable open source engineering analysis tools that a utility can use to assist in solving engineering analysis problems. Applying these open source applications, alongside a commercial engineering analysis application, can be a very cost effective way for utility engineers to solve problems presented by newer technologies, such as Solar and Wind DG and PEV charging, using the data they already have in their commercial applications. In this paper we will use GridLAB-D, OpenDSS, and Milsoft's WindMil, to show examples of today's engineering analysis problems that can be solved using open source and commercial applications in concert with each other. The purpose of the paper is to show what is possible, and what is needed to use multiple analysis tools to solve more complex analysis. The purpose of the paper is not, for example, to complete a complex time series study on the affect of PV generation on a distribution network, which could be a paper on its own merit.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6842209,no,undetermined,0
Using software architectures to retrieve interaction information in eLearning environments,"This research paper presents a software architecture based on services and deployed in a cloud environment that retrieves, analyzes and presents information collected from a closed eLearning environment like the Virtual Worlds. This software architecture is able to gather the user interaction in the digital platform, organize the interaction data and to perform measurements, estimates and basic analysis of the data, in order to give information to the managers and teachers about usage indicators and the resolution degree of the goals that students need to achieve in these scenarios and learning systems. To test this idea, the paper describe the application of the analytics layer of this software architecture on a real case, so it is possible to understand how can help this kind of architectures in the detection of the achievement of the learning goals or in the knowledge discover about users usage within the learning environment.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017715,no,undetermined,0
Value Evaluation of Enterprise Management Informatization Based on Comprehensive Method,"It is very difficult for Chinese enterprises to estimate whether the management informatization has been implemented successfully because of lacking a proper evaluation method. This paper involves 3 first-level indicators, 13 second-level indicators and 100 third-level indicators which are classified respectively according to informatization basis, application and effect. The third-level index can be chosen flexibly based on given restrictions and actual situation. We propose a comprehensive evaluation method which quantifies the qualitative problems through the questionnaires filled in by the measured object of target enterprises rather than onsite judgment by the experts. To make sure the evaluation method is valid, we evaluated five enterprises and used analytic hierarchy process to calculate the weight of the indicator and the expert. The results of evaluation indicate that the proposed evaluation system is reasonable, simple and practical to reflect the performance of enterprise management informatization.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982059,no,undetermined,0
VAST 2014: Summary on grant challenge work,"VAST 2014 grant challenge required a big picture on a fictional company employee missing event. This paper introduces the software we developed to tackle the challenge. In addition, how we integrate the visual analysis results from related information identified is included. And, the integrated analysis results (motivations, suspects, and related locations) are illustrated to help further investigations.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042539,no,undetermined,0
Venture Capital: Fueling the Innovation Economy,"Venture capital experts explore current opportunities to fuel and benefit from the innovation economy. Venture capital remains a vibrant force in the US, funding a broad array of disruptive technologies and playing a major role in fueling a startup-based economy. A survey of Austin Technology Incubator's recent startup portfolio suggests promising trends for venture investment in the innovation economy. Next-generation data analytics, offering strategic business applications that integrate data and technology into innovative and affordable tools, provide significant opportunities for venture investment. Corporations worldwide increasingly look to Silicon Valley's culture of entrepreneurship and venture investment to create new models for successful disruptive innovation.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6879742,no,undetermined,0
The Complexity of University Curricula According to Course Cruciality,"Many universities have recently focused significant efforts on enhancing their graduation rates. Numerous factors may impact a student's ability to succeed and ultimately graduate, including pre-university preparation, as well as the student support services provided by a university. However, even the best efforts to improve in these areas may fail if other institutional factors overwhelm their ability to facilitate student progress. Specifically, in this paper we consider degree to which the underlying curriculum that a student must traverse in order to earn a degree impacts progress. Using complex network analysis and graph theory, this paper proposes a framework for analyzing university course networks at the university, college and departmental levels. The analyses we provide are based on quantifying the importance of a course based on its delay and blocking factors, as well as the number of curricula that incorporate the course, leading to a metric we refer to as the course cruciality. Experimental results, using data from the University of New Mexico, show that the distribution of course cruciality follows a power law distribution. Applications of the proposed framework are extended to study the complexity of curricula within colleges as well as the tendency of a university's disciplines to associate with others that are unlike them. This work may be useful to both students and decision makers at universities as it presents a robust framework for analyzing the ease of flow of students through curricula, which may lead to improvements that facilitate improved student success.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915523,no,undetermined,0
Video security with human identification and tracking,"With the pervasiveness of monitoring cameras installed in public places, schools, hospitals and homes, video analytics technologies for interpreting the generated video content are becoming more and more relevant to people's lives. Along this context, we develop a human-centric video surveillance system that identifies and tracks people in a given scene. In this paper, a parallel processing pipeline is proposed that integrates image processing modules in the system, such as face detection, person recognition and tracking, efficiently and smoothly, so that multiple people can be simultaneously tracked in real time. Furthermore, significant innovations are involved in this work in making each of the major image analysis modules both fast and robust to variations in pose, illumination, occlusions and so on. A demonstration software has been implemented that supports finding, tagging, identifying and tracking people in live or recorded videos with uncontrolled capturing conditions.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890591,no,undetermined,0
"Visrad, 3-D target design and radiation simulation code","Summary form only given. The 3-D view factor code VISRAD is widely used in designing HEDP experiments at major laser and pulsed-power facilities, including NIF, OMEGA, OMEGA-EP, ORION, LMJ, Z, and PLX. It simulates target designs by generating a 3-D grid of surface elements, utilizing a variety of 3-D primitives and surface removal algorithms, and can be used to compute the radiation flux throughout the surface element grid by computing element-to-element view factors and solving power balance equations. Target set-up and beam pointing are facilitated by allowing users to specify positions and angular orientations using a variety of coordinates systems (e.g., that of any laser beam, target component, or diagnostic port). Analytic modeling for laser beam spatial profiles for OMEGA DPPs and NIF CPPs is used to compute laser intensity profiles throughout the grid of surface elements. We will discuss recent improvements to the software package and plans for future developments.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012249,no,undetermined,0
Visual Analytics of Malignant Blood Flow for Medical Professionals,"Arterial diseases of sclerosis and aneurysm are known to be related with a local blood flow. There are indeed decades of related studies, recently being with an aid of a computational approach on a patient-specific basis, but those have yet to be translated into clinical medicine. ""Simply why?"", and ""how to achieve it?"" Computational blood flow originates from theoretical and computational fluid dynamics. Understanding the underlying knowledge may be beyond the scope of medical professionals. Engineers have to be fully aware of a barrier on an interface with clinical medicine, and manage to translate the evolving technology with a special focus on balancing the efficacy and safety. Visual analytics is to facilitate to overcome an academic gap between engineering and clinical medicine. This paper deals with the recent development of visual analytics of malignant blood flows in cerebral aneurysms for medical professionals.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6787195,no,undetermined,0
Visual learning analytics techniques applied in software engineering subjects,"The technology applied to educational contexts, and specially the learning platforms, provides students and teachers with a set of tools and spaces to carry out the learning processes. Information related to the participation and interaction of these stakeholders with their peers and with the platform is recorded. It would be useful to exploit this information in order to make decisions. However this is a complex activity mainly because of the huge quantity of information stored. This work presents a visual learning analytics system that makes possible the exploitation of that information. The system includes several tools that help to analyze users' interaction attending to different dimensions, such as: when interaction is carried out, which are more important contents for users, how they interact with others, etc. This system has been tested with the subject information recorded during five academic years. From this analysis it is possible to show that visual learning analytics may help to improve educational practices.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044486,no,undetermined,0
"WAN Optimization Tools, Techniques and Research Issues for Cloud-Based Big Data Analytics","Increasing data volumes, data replication at offsite, and the greater than ever use of content-rich and Big Data, applications are mandating IT organizations to optimize their network resources. Trends such as Virtualization and Cloud computing further emphasize this requirement of this current era of Big data. To help with this process, companies are increasingly relying on a new generation of WAN optimization Techniques, Appliances, Controllers, Platforms and Products that are displacing standalone physical appliances by offering more scalability, flexibility, and manageability by additional inclusion of software to handle this Big data and bring valuable insights through big data analytics. An optimized WAN environment can increase network reliability, accessibility and availability and improve cost profiles. It also improves the performance and consistency of data backup, replication, and recovery processes. This paper covers the introduction to WAN optimization, prominent WAN optimization techniques, WAN optimization products used for Big data analytics and finally future trends and research Issues of WAN optimization in the ensuing era of Big data.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755159,no,undetermined,0
Watching the Aliens,"Big Data analytics has become a powerful tool in monitoring invasive species, with ecologists saying that with the problem of invasive species being on such a large scale, it is essential to analyse lots of data through studying several ecosystems. Supercomputers can help conduct analysis of the terabytes of data available, allowing data to be processed much more quickly. The previouslymentioned GARP software programme, developed at the San Diego Supercomputer Centre, can perform modelling analysis using a genetic algorithm to predict the potential distribution of an invasive species, and these models can be visualised as distribution maps using GIS. The model has been deployed successfully in several studies, for example in 2007 when it predicted that large estuaries including Chesapeake Bay in the US were at risk of invasion of Chinese mitten crab (Eriocheir sinensis). Just one month later crabs were discovered in the estuary's waters.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6870302,no,undetermined,0
Web resource management and evaluation system based on multi-attribute fusion,"In order to evaluate the services occupying server resources, a comprehensive evaluation of Web system performance model frame has been built and researched in the general. The GAHP and MADM method are separately proposed to evaluate to services health, which have their advantages as same as the dis advantages on multi-attribute resource evaluation. To improve the evaluation quality, we fuse the result of GAHP and MADM by fuzzy method. Resource evaluation of services can be finally sorted. Finally, A simulation result shows feasible of the paper's idea.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6896061,no,undetermined,0
Web Service Recommendation via Exploiting Location and QoS Information,"Web services are integrated software components for the support of interoperable machine-to-machine interaction over a network. Web services have been widely employed for building service-oriented applications in both industry and academia in recent years. The number of publicly available Web services is steadily increasing on the Internet. However, this proliferation makes it hard for a user to select a proper Web service among a large amount of service candidates. An inappropriate service selection may cause many problems (e.g., ill-suited performance) to the resulting applications. In this paper, we propose a novel collaborative filtering-based Web service recommender system to help users select services with optimal Quality-of-Service (QoS) performance. Our recommender system employs the location information and QoS values to cluster users and services, and makes personalized service recommendation for users based on the clustering results. Compared with existing service recommendation methods, our approach achieves considerable improvement on the recommendation accuracy. Comprehensive experiments are conducted involving more than 1.5 million QoS records of real-world Web services to demonstrate the effectiveness of our approach.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684151,no,undetermined,0
Web-based learning object selection software using analytical hierarchy process,"The concepts of sustainability and reusability have great importance in engineering education. In this context, metadata provides reusability and the effective use of Learning Objects (LOs). In addition, searching the huge LO Repository with metadata requires too much time. If the selection criteria do not exactly match the metadata values, it is not possible to find the most appropriate LO. When this situation arises, the multi-criteria decision making (MCDM) method can meet the requirements. In this study, the SDUNESA software was developed and this software allows for the selection of a suitable LO from the repository by using an analytical hierarchy process MCDM method. This web-based SDUNESA software is also used to store, share and select a suitable LO in the repository. To meet these features, the SDUNESA software contains Web 2.0 technologies such as AJAX, XML and SOA Web Services. The SDUNESA software was especially developed for computer engineering education. Instructors can use this software to select LOs with defined criteria. The parameters of the web-based SDUNESA learning object selection software that use the AHP method are defined under the computer education priorities. The obtained results show that the AHP method selects the most reliable learning object that meets the criteria.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6871464,no,undetermined,0
Transforming Scagnostics to Reveal Hidden Features,"Scagnostics (Scatterplot Diagnostics) were developed by Wilkinson et al. based on an idea of Paul and John Tukey, in order to discern meaningful patterns in large collections of scatterplots. The Tukeys' original idea was intended to overcome the impediments involved in examining large scatterplot matrices (multiplicity of plots and lack of detail). Wilkinson's implementation enabled for the first time scagnostics computations on many points as well as many plots. Unfortunately, scagnostics are sensitive to scale transformations. We illustrate the extent of this sensitivity and show how it is possible to pair statistical transformations with scagnostics to enable discovery of hidden structures in data that are not discernible in untransformed visualizations.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875999,no,undetermined,0
Towards the optimization of a parallel streaming engine for telco applications,"Parallel and distributed computing is becoming essential to process in real time the increasingly massive volume of data collected by telecommunications companies. Existing computational paradigms such as MapReduce (and its popular open-source implementation Hadoop) provide a scalable, fault tolerant mechanism for large scale batch computations. However, many applications in the telco ecosystem require a real time, incremental streaming approach to process data in real time and enable proactive care. Storm is a scalable, fault tolerant framework for the analysis of real time streaming data. In this paper we provide a motivation for the use of real time streaming analytics in the telco ecosystem. We perform an experimental investigation into the performance of Storm, focusing in particular on the impact of parameter configuration. This investigation reveals that optimal parameter choice is highly non-trivial and we use this as motivation to create a parameter configuration engine. As first steps towards the creation of this engine we provide a deep analysis of the inner workings of Storm and provide a set of models describing data flow cost, central processing unit (CPU) cost, and system management cost.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6770354,no,undetermined,0
Towards robustness and self-organization of ESB-based solutions using service life-cycle management,"Enterprise Service Bus (ESB) is a middleware infrastructure that provides a way to integrate loosely-coupled heterogeneous software applications based on the services principles. The life-cycle management of services in such environments is a critical issue for the component's reuse, maintenance and operation. This paper introduces a service life-cycle management module that extends the traditional functionalities with advanced monitoring and data analytics to contribute for the robustness, reliability and self-organization of networks of clusters based on ESB platforms. The realization of this module was embedded in the JBoss ESB, considering a sniffer mechanism to collect the service messages crossing the bus and a Liferay portal to display relevant information related to the services' health.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7049246,no,undetermined,0
Towards OpenFlow based software defined networks,"Software defined networks (SDN) are an emerging technology that is being increasingly adopted by various network operators. These technologies provide new services and powerful analytics that help to transform the network and unfasten its intelligence to serve today's business demands. This paper briefs about the need for change in the current networking technology and explores the role of Open Flow protocol that is used by researchers to experiment with more realistic settings to provide for a new network architecture. Further, this paper discusses the advantages offered by SDN and the huge potential of OpenFlow based SDN. As SDN can simplify management of virtualized networks, enable cloud computing and reduce costs, the vendors would be encouraged to adopt SDN and OpenFlow. The objective of this paper is to provide an insight into the latest technology to the vendors to assist them in future enhancement of their switch products in the network.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7238395,no,undetermined,0
The Digital Transformation: Staying Competitive,"Today's information technologies accelerate the speed at which enterprises make decisions, process information, and collaborate to solve problems, but do they provide a competitive advantage? Only if the organization transforms how it does business--using the same old approaches with new software aren't sufficient. The pace of collaboration, problem solving, innovation, and value creation has been increasing as new enabling tools emerge, resulting in a cycle in which innovation leads to new inventions. For example, the development of knowledge bases presented an innovative way to collaborate, which in turn lead to inventions in many fields. Some might assume that this process creates an advantage, but in reality, almost everyone else is developing these tools at the same time. New capabilities are evolving in the context of an ecosystem of competitors, all of whom are trying to do the same thing. The key is to leverage new tools and approaches faster than others in your industry, creating differentiated value for the customer.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6798637,no,undetermined,0
The EMBERS architecture for streaming predictive analytics,"Developed under the IARPA Open Source Initiative program, EMBERS (Early Model Based Event Recognition using Surrogates) is a large-scale Big-Data analytics system for forecasting significant societal events, such as civil unrest incidents and disease outbreaks on the basis of continuous, automated analysis of large volumes of publicly available data. It has been operational since November of 2012, delivering approximately 50 predictions each day. EMBERS is built on a streaming, scalable, share-nothing architecture and is deployed on Amazon Web Services (AWS).",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004477,no,undetermined,0
The grading scheme based on fuzzy comprehensive evaluation and analytic hierarchy process for classified protection of information system,"This paper starts with the grading standard of classified protection of information system, comes up with the grading scheme based on fuzzy comprehensive evaluation and analytic hierarchy process for classified protection of information system. It makes a quantitative and qualitative analysis, argument for the two classificatory factors: the object and the extent of damage to the object when the target of classified security is damaged. The influence which results from judge's subjective differences is decreased. At last, an example is given to prove it that this method can be used efficiently in the grading of classified protection.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933575,no,undetermined,0
The impact analysis of distribution grid based on Battery energy storage system,"The voltage variation caused by increasing renewable energy generation can be attenuated effectively by using Battery energy storage system (BESS). Firstly, the widely used PNGV method is adopted to build the battery cells model in this paper. Then, the identification methods of electrochemistry battery's electrical equivalent circuit are introduced according to `continuous function analytic expression' and `difference equation' respectively. The experimental results show that the identified parameters are agreed with simulation data very well. Meanwhile, the low-frequency equipment model of converter is proposed by neglecting high frequency components since the main performance is determined by the fundamental frequency. The classical distribution grid is established using EMTP_RV software, including the battery storage system, converter, various power load, etc. The study results reported in this paper indicate that the battery storage system can effectively attenuate over-voltage and support the power grid. Furthermore, the proposed modelling method can be used to further analyse the optimization of BESS location and capacity.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991662,no,undetermined,0
The Overtime Waiting Model for Web Server Performance Evaluation,"Though the queuing model with impatient customer has been studied in the past decades, little is definitively known about the overtime waiting characteristics. In this paper, we present an overtime-waiting model for Web servers. This model is taken as a special case of the impatient customer, which quit when the waiting time is longer than a tolerable value. The pdf of the client response time is deduced for analysis of the model. By imposing the timeout threshold on the Web server, the system performance is derived. Finally, numerical results corroborate this analytic model.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984311,no,undetermined,0
The role of IC technology in development and application of experimental methods and multivariate analysis,"Information and communications (IC) technology influences all human activities but also plays an important role in development of various scientific disciplines and other types of technologies thus enabling their large scale implementation. This paper presents the relations among IC technology, experimental methods and multivariate analysis. As the development of experimental techniques and multivariate analysis paved way to revolutionary discoveries referring to IC technology, this, as a feed back, enabled further development and implementation of the mentioned methods in the most diversified fields of human activities. The necessity of the integral approach, based on implementation of multivariate analysis and adequate experimental methods and respective IC technologies is illustrated in the example of solving the problem of preservation, protection and evaluation of cultural heritage objects. In the process, various experimental analytic techniques based on principles of physics and/or chemistry are used along with IC technologies. In order to obtain the best possible analysis and interpretation of series consisting of enormous number of data, aiming to filter only the most important information relevant for particular object, statistical approach is necessary where a prominent position belongs to multivariate analysis - a task that can only be accomplished by using adequate statistics software.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859690,no,undetermined,0
"The state-of-the-art of Social, Mobility, Analytics and Cloud Computing an empirical analysis","Recent years have seen explosive emergence of the SMAC era, which is a combination of Social, Mobility, Analytics and Cloud Computing. The uses of Social networking are growing rapidly to collaborate at all levels of the extended enterprise. Employees are using Mobility to enhance the productivity. Apart from this people started to use Analytics based on big data and Cloud Computing which provides different services like Infrastructure-as-a-Service (IaaS), Software-as-a-Service (SaaS) and Platform-as-a-Service (PaaS). This abundance of services help businesses to develop approximate solutions that ultimately leads to leverage public IT infrastructure, minimizing cost of ownership and minimizing time. The deployment of SMAC in IT sector not only enhances the decision making capability but also allows them to rollout new unchecked business models and increase their reach to customers. Enterprises need to fully understand the capability and utilization of this emerging trend in order to make sense of their intended dominance in the next few years ahead.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045356,no,undetermined,0
Tools for design of knowledge management systems based on business intelligence,The present article is concerned about the knowledge of the different tools of Business Intelligence used to generate bases for Knowledge Management Systems that allow doing a better decisions making with less risk at any level of the organization. Tests will be made using a relational database management system and performing results in a common business application.,2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000410,no,undetermined,0
Tools for Gamification Analytics: A Survey,"Application and gamification data contains valuable information about users and their behavior. This data can be used to measure the success of gamification projects, to analyze user behavior, and to continuously improve gamification designs. Existing software solutions promise support for analyzing game- and gamification-related data. However, research shows that most gamification projects do not monitor and analyze this data in an automated way, even though gamification experts are aware of its potential. In this survey paper we identify relevant software solutions and assesses them with regards to their fulfillment of user requirements in the gamification analytics domain. The survey results can be used by practitioners to make tool decisions. Furthermore, we identify gaps of current solutions that should be addressed by future work in the field of gamification analytics.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027560,no,undetermined,0
Toward Implementation of a Software Defined Cloud on a Supercomputer,"Conventional cloud computing architectures may seriously constrain computational throughput for high performance computing (HPC) and high-performance data (HPD) applications. The traditional approach to circumvent such problems has been to map these applications and problems onto other specialized hardware and coprocessor architectures. This is both time and resource expensive, and poses a challenge for rapidly rising demands for computation and data analytics. In this paper we report on progress to develop an alternative experimental software defined cloud implementation that virtualizes the topology of a standard HPC computational architecture. This software defined system re-arranges access to the nodes and dynamically customizes the features of the HPC hardware architecture so that they map to the specifics of the computation and data analysis application. This allows a cloud computing implementation to utilize the specialized infrastructure capabilities of an HPC system. We have created this type of user reconfigurable architecture on an IBM Blue Gene/P supercomputing environment at the Department of Energy's Argonne Leadership Computing Facility (ALCF). This pilot configuration was implemented using both an open source cloud technology called VCL (Virtual Computing Laboratory) in combination with a provisioning module called Kittyhawk. Cloud security is addressed by configuring and running a root-less version of the VCL cloud system on the ALCF's Blue Gene/P login node.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903532,no,undetermined,0
Toward Scalable Systems for Big Data Analytics: A Technology Tutorial,"Recent technological advancements have led to a deluge of data from distinctive domains (e.g., health care and scientific sensors, user-generated data, Internet and financial companies, and supply chain systems) over the past two decades. The term big data was coined to capture the meaning of this emerging trend. In addition to its sheer volume, big data also exhibits other unique characteristics as compared with traditional data. For instance, big data is commonly unstructured and require more real-time analysis. This development calls for new system architectures for data acquisition, transmission, storage, and large-scale data processing mechanisms. In this paper, we present a literature survey and system tutorial for big data analytics platforms, aiming to provide an overall picture for nonexpert readers and instill a do-it-yourself spirit for advanced audiences to customize their own big-data solutions. First, we present the definition of big data and discuss big data challenges. Next, we present a systematic framework to decompose big data systems into four sequential modules, namely data generation, data acquisition, data storage, and data analytics. These four modules form a big data value chain. Following that, we present a detailed survey of numerous approaches and mechanisms from research and industry communities. In addition, we present the prevalent Hadoop framework for addressing big data challenges. Finally, we outline several evaluation benchmarks and potential research directions for big data systems.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6842585,no,undetermined,0
Towards a Framework for Enterprise Architecture Analytics,"Current approaches for enterprise architecture lack analytical instruments for cyclic evaluations of business and system architectures in real business enterprise system environments. This impedes the broad use of enterprise architecture methodologies. Furthermore, the permanent evolution of systems desynchronizes quickly model representation and reality. Therefore we are introducing an approach for complementing the existing top-down approach for the creation of enterprise architecture with a bottom approach. Enterprise Architecture Analytics uses the architectural information contained in many infrastructures to provide architectural information. By applying Big Data technologies it is possible to exploit this information and to create architectural information. That means, Enterprise Architectures may be discovered, analyzed and optimized using analytics. The increased availability of architectural data also improves the possibilities to verify the compliance of Enterprise Architectures. Architectural decisions are linked to clustered architecture artifacts and categories according to a holistic EAM Reference Architecture with specific architecture metamodels. A special suited EAM Maturity Framework provides the base for systematic and analytics supported assessments of architecture capabilities.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975371,no,undetermined,0
Towards a High Speed Video Cloud Based on Batch Processing Integrated with Fast Processing,"With the rise of video surveillance applications for analyzing real-time and batching video data in large scales, traditional video processing systems are being challenged due to real-time, intelligence and fault-tolerance demand for networked high resolution and large-scale video processing. These challenges can be further exacerbated by the existing predicaments of multi-platform, multi-format, multi-codec on video itself. The emergence of cloud computing and big data techniques makes it possible to manage complicated intelligent processing for large-scale video data. This paper proposes a general cloud-based video platform that can provide a robust solution to intelligent analytic and storage for video data, which is called ViCiBaF architecture (Video Cloud integrated with Batch processing and Fast processing) architecture. This ViCiBaF architecture is elaborated through an implementation that can effectively handle massive surveillance video data, where real-time analysis, batch processing, distributed storage and cloud services are seamlessly integrated to meet the requirements of intelligent analysis, real time, fault tolerance and massive storage for massive video data. The evaluations show that the proposed approach is efficient.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7063992,no,undetermined,0
Towards a hybrid approach of primitive cognitive network process and Bayesian network for evaluation,"Evaluation is an essential step for decision making. This paper proposes a hybrid approach, named PCNP-BN, which combines the Primitive Cognitive Network Process (PCNP) and the Bayesian Network (BN) for evaluation activities. PCNP, which is an ideal alternative of the Analytic Hierarchy Process (AHP), helps to quantify the influence of factors, whilst Bayesian network is utilized to combine all the useful feedbacks. The proposed approach can support evaluation activities through quantifying complex factors into measurable values.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972307,no,undetermined,0
Towards a hybrid approach of primitive cognitive network process and fuzzy cognitive map for box office analysis,"Box office analysis is critical to make profitable movies. Various factors have different influences on box office sales. This paper combines Primitive Cognitive Network Process (PCNP) and Fuzzy Cognitive Map (FCM) to measure and analyze the factors of box office. PCNP is a revised approach of Analytic Hierarchy Process (AHP) to quantify the weights of factors to construct a concept in FCM. FCM is used to simulate the influences of the concepts in the network. The proposed hybrid approach can enhance the evaluation and. To show the applicability of PCNP-FCM, an example of box office analysis is illustrated.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891629,no,undetermined,0
Towards a hybrid approach of primitive cognitive network process and weighted iterative dichotomiser 3 for customer e-payment adoption analysis,"Attracting customers to use e-payment is the critical success factor for the e-business transaction. Various factors influence the customers to adopt e-Payment. A hybrid approach of Primitive Cognitive Network Process (PCNP) and Weighted Iterative Dichotomiser Three (WID3) is proposed to classify the factors which influence the customer e-payment adoptions. Whilst PCNP is a revised approach of Analytic Hierarchy Process (AHP) to quantify the weights of factors, WED3 is the classification approach combining the weighted factors to the classical ID3 to classify data into distinct groups. An application shows the proposed approach could identify the patterns of customer e-Payment adoption, and predict the potential customer adoption behaviour.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7065038,no,undetermined,0
Towards a Set Theoretical Approach to Big Data Analytics,"Formal methods, models and tools for social big data analytics are largely limited to graph theoretical approaches such as social network analysis (SNA) informed by relational sociology. There are no other unified modeling approaches to social big data that integrate the conceptual, formal and software realms. In this paper, we first present and discuss a theory and conceptual model of social data. Second, we outline a formal model based on set theory and discuss the semantics of the formal model with a real-world social data example from Facebook. Third, we briefly present and discuss the Social Data Analytics Tool (SODATO) that realizes the conceptual model in software and provisions social data analysis based on the conceptual and formal models. Fourth and last, based on the formal model and sentiment analysis of text, we present a method for profiling of artifacts and actors and apply this technique to the data analysis of big social data collected from Facebook page of the fast fashion company, H&M.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906838,no,undetermined,0
Towards a Visual Analytics Framework for Handling Complex Business Processes,"Organizing data that can come from anywhere in the complex business process in a variety of types is a challenging task. To tackle the challenge, we introduce the concepts of virtual sensors and process events. In addition, a visual interface is presented in this paper to aid deploying the virtual sensors and analyzing process events information. The virtual sensors permit collection from the streams of data at any point in the process and transmission of the data in a form ready to be analyzed by the central analytics engine. Process events provide a uniform expression of data of different types in a form that can be automatically prioritized and that is readily meaningful to the users. Through the visual interface, the user can place the virtual sensors, interact with and group the process events, and delve into the details of the process at any point. The visual interface provides a multiview investigative environment for sense making and decisive action by the user.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758774,no,undetermined,0
Towards Model-Driven Engineering for Big Data Analytics -- An Exploratory Analysis of Domain-Specific Languages for Machine Learning,"Graphical models and general purpose inference algorithms are powerful tools for moving from imperative towards declarative specification of machine learning problems. Although graphical models define the principle information necessary to adapt inference algorithms to specific probabilistic models, entirely model-driven development is not yet possible. However, generating executable code from graphical models could have several advantages. It could reduce the skills necessary to implement probabilistic models and may speed up development processes. Both advantages address pressing industry needs. They come along with increased supply of data scientist labor, the demand of which cannot be fulfilled at the moment. To explore the opportunities of model-driven big data analytics, I review the main modeling languages used in machine learning as well as inference algorithms and corresponding software implementations. Gaps hampering direct code generation from graphical models are identified and closed by proposing an initial conceptualization of a domain-specific modeling language.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758697,no,undetermined,0
Needle in a haystack: Cost-Effective data analytics for real-time cloud sharing,"Real-time file sharing is important to improve the quality of cloud services. Current cloud systems however fail to efficiently offer cost-effective data analytics due to slow response and energy inefficiency. In order to support real-time processing and improve energy efficiency in the cloud, this paper proposes a novel cost-effective data analytics scheme, called Needle. The idea behind Needle is to use flat-addressing and content-aware naming approaches to deliver high performance of network transmission. Needle leverages a suitable division-of-labor model between core and edge network nodes to efficiently support content-aware queries in the cloud data center networks. Experimental results demonstrate that Needle is able to support cloud sharing service in a real-time and efficient manner.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6914316,no,undetermined,0
Nakshatra: Towards Running Batch Analytics on an Archive,"Long term retention of data has become a norm for reasons like compliance and data preservation for future needs. With storage media continuing to become cheaper, this trend has further strengthened and is testified with introduction of archival solutions like Amazon Glacier and Spectra Logic Black Pearl. On the other hand, analytics and big data have become key enablers for business and research. However, analytics and archiving happens on separate storage silos. This generates additional costs and inefficiencies when part of archived data needs to be analyzed using batch analytics platforms like Hadoop because a) We need additional storage for data transferred from archive to analytics tier and b) Transfer time costs are incurred due to data migration to analytics tier. Moreover, accessing archived data has high times to first byte, as much of the data is stored in offline media like tapes or spun down disks. We introduce Nakshatra, a data processing framework to run analytics directly on an archive based on offline media. To the best of our knowledge, this is the first work of its kind available in literature. We leverage batched pre-fetching and scheduling techniques for improved retrieval of data and scalable analytics on archives. Our preliminary evaluation shows Nakshatra to be upto 81% faster than the traditional ingest-then-compute workflow for archived data.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033688,no,undetermined,0
Big data technologies in support of real time capturing and understanding of electric vehicle customers dynamics,"Energy overconsumption and greenhouse gas emission have been contributing to air pollutions and the global warming for years. The unceasingly increasing number of fossil fuels based vehicles around the world is considered as one of main factors making to the situation worse year by year. Electric vehicles (EV) are promoted as a viable and promising alternative transportation means for customers. However, there is an array of issues hindering EVs from the fast adoption in the global auto market. As these issues bear different priorities that surely vary with marketplaces, it becomes essential for EV makers and governments to capture and understand the dynamics of EV consumers in real time. This paper explores how the emerging big data technologies can be applied to facilitate the process of deciphering the acceptance and behavior of EV customers from marketplace to marketplace. A data-collecting web system is discussed. IBM BigInsights platform technologies, including Hadoop, Streams, SPSS modeler and text analytics, are utilized for looking into the insights of collected data. Examples are provided to show the promising future of big data technologies in the field of customer analytics in today's globalized economy.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933559,no,undetermined,0
Emerging trends in Cloud Computing and Big Data,Summary form only given. This talk looks at the processing requirements of Big Data and challenges in using Map-Reduce paradigm. It looks at some of the bench mark big data analytics problems using Map-Reduce on HDFS. We motivate the use of new paradigm called Generate Map Reduce (GMR) for doing big data analytics in the cloud. GMR supports shared data structure abstraction across map jobs and supports iterative and recursive map-reduce jobs. GMR performs quite well for Big Data Analytics on the cloud and the talk presents the results of using GMR for Big Data Analytics on the cloud.,2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056923,no,undetermined,0
Development of Tools for Data Analysis of Earthquakes,"In this paper, we focus on proposing tools for enabling scientists analyze and interpret large-scale datasets about earthquakes in a way which will complement current analytical tools and thinking thus, complement current effort on understanding the event itself. Apart from briefing current trends on analytical techniques we focus on illustrating the architectures of our proposed tools in order to familiarize readers of how things are structured and the way they work. We also describe the implementation of the produced tools as well as we test and thus, demonstrate their functionality and operational order through the development of YouTube help videos and use of relevant use case scenarios.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7057123,no,undetermined,0
Discovering cross-organizational business rules from the cloud,"Cloud computing is rapidly emerging as a new information technology that aims at providing improved efficiency in the private and public sectors, as well as promoting growth, competition, and business dynamism. Cloud computing represents, today, an opportunity also from the perspective of business process analytics since data recorded by process-centered cloud systems can be used to extract information about the underlying processes. Cloud computing architectures can be used in cross-organizational environments in which different organizations execute the same process in different variants and share information about how each variant is executed. If the process is characterized by low predictability and high variability, business rules become the best way to represent the process variants. The contribution of this paper consists in providing: (i) a cloud computing multi-tenancy architecture to support cross-organizational process executions; (ii) an approach for the systematic extraction/composition of distributed data into coherent event logs carrying process-related information of each variant; (iii) the integration of online process mining techniques for the runtime extraction of business rules from event logs representing the process variants running on the infrastructure. The proposed architecture has been implemented and applied for the execution of a real-life process for acknowledging an unborn child performed in four different Dutch municipalities.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008694,no,undetermined,0
DocBot: A novel clinical decision support algorithm,"DocBot is a web-based clinical decision support system (CDSS) that uses patient interaction and electronic health record analytics to assist medical practitioners with decision making. It consists of two distinct HTML interfaces: a preclinical form wherein a patient inputs symptomatic and demographic information, and an interface wherein a medical practitioner views patient information and analysis. DocBot comprises an improved software architecture that uses patient information, electronic health records, and etiologically relevant binary decision questions (stored in a knowledgebase) to provide medical practitioners with information including, but not limited to medical assessments, treatment plans, and specialist referrals.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945067,no,undetermined,0
Does waveform analysis hold the key to a predictive grid?,"Waveforms can reveal what is a normal operation within your distribution and what is not. When fault current is detected, waveforms of the event and background information (e.g. temperature, GPS location, time) can reveal a lot not only about the real-time conditions of your electric grid, but these signatures could also hold the keys to predicting how to prevent future problems before they arise. But, capturing fault current is not enough to give utilities 100 percent visibility into the potential trouble spots on the distribution network. By capturing, classifying and analyzing fault currents, momentaries and line disturbances that do not cause immediate outages with a software analytics tool, we can build intelligence about what might cause a power failure in the future. For example, incidents like momentaries or line disturbances that spike seasonally across a circuit could prioritize needs around vegetation management. Or, by counting waveform signatures that are normal like recloser operations, it is possible to know when maintenance or replacement is needed - valuable information for a conditioned based asset management strategy.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6808104,no,undetermined,0
Drawing Large Weighted Graphs Using Clustered Force-Directed Algorithm,"Clustered graph drawing is widely considered as a good method to overcome the scalability problem when visualizing large (or huge) graphs. Force-directed algorithm is a popular approach for laying graphs yet small to medium size datasets due to its slow convergence time. This paper proposes a new method which combines clustering and a force-directed algorithm, to reduce the computational complexity and time. It works by dividing a Long Convergence: LC into two Short Convergences: SC1, SC2, where SC1+SC2 <; LC. We also apply our work on weighted graphs. Our experiments show that the new method improves the aesthetics in graph visualization by providing clearer views for connectivity and edge weights.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6902873,no,undetermined,0
e-shop user preferences via user behavior,We deal with the problem of using user behavior for business relevant analytic task processing. We describe our acquaintance with preference learning from behavior data from an e-shop. Based on our experience and problems we propose a model for collecting (java script tracking) and processing user behavior data. We present several results of offline experiments on real production data. We show that mere data on users (implicit) behavior are sufficient for improvement of prediction of user preference. As a future work we present richer data on time dependent user behavior.,2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7509048,no,undetermined,0
Early Experience with Model-Driven Development of MapReduce Based Big Data Application,"With internet becoming increasingly pervasive, data analytics is playing increasingly critical role in the business. Data to be analyzed exists in large quantity and in multiple formats. Many technologies exist to support Big Data analytics. However, they remain somewhat of a challenge for average developer to use. It's been seen that model-driven development (MDD) approach can eliminate accidental complexity to a large extent. We discuss MDD approach for development of MapReduce based Big Data applications, its efficacy and lessons learnt.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091296,no,undetermined,0
Efficacy of an online writing program on URM students in engineering,"Recently, universities have explored using online technologies to increase instructor efficiency and improve student performance. One such technology is the Educational Testing Service (ETS) web-based service, Criterion. Criterion is an online writing service whose purpose is to help students improve their grasp of the English language through offering critical, detailed feedback in the areas of: grammar, spelling, mechanics, usage and organization, and development. With Criterion, students get the benefit of additional writing practice without adding to the instructor's workload, allowing instructors to focus on the content and style of students' work. To assess the effectiveness of the Criterion writing program, our researchers used the program's analytics to gauge student proficiency, use, and the effectiveness of the program. Criterion provides the instructor with each student's progress from the moment of assignment submission. This allowed researchers to track the number of submissions each student needs to submit an acceptable writing assignment. In this study we compiled all of the data from Criterion and tracked the number of submissions and types of errors and sorted them by major and ethnicity. A high percentage of San Jose’ State University (SJSU)'s incoming freshmen are remedial in English or mathematics. Furthermore, at SJSU underrepresented minority (URM) students have notably higher remediation rates; 70% of African Americans, 60% of Hispanic students, and 50% of Asian Americans require English remediation during their first year, compared with 31% of Caucasian students. Many of these students are U.S. educated English learners with lower English skills than Native English students. They often struggle to complete their English and writing requirements, which negatively impacts their retention and graduation rates. As an online program that offers none of the bias present in traditional university classes, URM students using Criterion d- not feel intimidated or overwhelmed. Additionally, the program is set in such a way that a student may use it as many times as needed to learn at their own pace. It effectively assesses student work and provides opportunities for them to learn from their mistakes, while its structure allows for specific, constructive feedback from faculty.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044294,no,undetermined,0
Electrical modeling of split ring resonators operating in the UHF band,"The article in hand deals present a new approach focuses on the electric modelling of split ring resonators based on an equivalent circuit. The elements of circuit are determined from geometric parameters forming the three dimensional model of this type of resonators. Firstly an analytic study is proposed about the determination of the components of the circuit in function the physical parameters with a review about comparison between obtained results when the simulation of two different models is presented. Secondly, a demonstrative study on the effect of varying the value of the relative permittivity and determination the error rate at level the resonant frequency in the two models. The objective is to put the conditions and constraints necessary to make our equivalent circuit most responded and most efficient with zero error. The results of simulations are confirmed with using the two commercials software HFSS and ADS.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6970101,no,undetermined,0
Enhancing higher education experience: The eMadrid initiative at UNED university,"In this paper we focus on the achievements of eMadrid initiative in some fields of technology-enhanced learning, mainly involving the improvement of the mechanisms for open educational content retrieval from Internet, considering Internet resources as potential learning objects. Also we facilitate the integration of remote laboratories and external tools in virtual campuses architectures supporting enriched capabilities and describe a way to cluster and identify learner weaknesses using a learning analytics approach in combination with the item response theory.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044051,no,undetermined,0
Flow identification and characteristics mining from internet traffic with hadoop,"Characteristics of flow describe the pattern and trend of network traffic, it helps network operator understanding network usage and user behavior, especially useful for those who concerns more about network capacity planning, traffic engineering and fault handling. Due to the large scale of datacenter network and explosive growth of traffic volume, it's hard to collect, store and analyze Internet traffic on a single machine. Hadoop has become a popular infrastructure for massive data analytics because it facilitates scalable data processing and storage services on a distributed computing system consisting of commodity hardware. In this paper, we present a Hadoop-based traffic analysis system, which accepts input from multiple data traces, performs flow identification, characteristics mining and flow clustering, output of the system provides guidance in resource allocation, flow scheduling and some other tasks. Experiment on a dataset about 8G size from university datacenter network shows that the system is able to finish flow characteristics mining on a four node cluster within 23 minutes.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6878955,no,undetermined,0
Evacuation and Emergency Management Using a Federated Cloud,"Contemporary disaster relief techniques fall significantly short in terms of efficiency and timeliness. In a postdisaster scenario, the generation and transmission of voluminous data at a high velocity impacts the communication framework. This work exploits the benefits of opportunistic communication and efficient big data management policies, focusing on the collaboration of multiple private and/or public clouds of diverse nature to perform damage assessment and determine the spatial distribution of the live victims and their physical and mental status. Based on the analytics, real-time decision making of the rescue operation is achieved.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7057595,no,undetermined,0
Evaluating retrofit strategies for greening existing buildings by energy modelling & data analytics,"Design strategies that account for local micro-climatic conditions can reduce the overall energy consumption of buildings over its life. This paper evaluates non-structural retrofit strategies for reducing energy consumption for existing buildings. Based on real-life data on usage statistics of buildings and accounting for local climatic conditions, we validate energy consumption predictions by Sefaira<sup>Œ¬</sup> building simulation software. A comparative analysis is then performed for various energy conservation scenarios by estimating energy reduction and payback periods. Our analysis shows that various retrofit strategies can lead to reduction in energy consumption upto 60% with a pay back period of about 9 years.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6835192,no,undetermined,0
Evaluation of a state-based real-time scheduling analysis technique,"The analysis of real-time properties is crucial in safety critical areas. Systems have to work in a timely manner to offer correct services. The analysis of timing properties is particularly difficult for distributed systems when complex interferences between individual tasks can occur. Considering only critical instances, as analytic approaches do, may deliver pessimistic results leading to higher production costs. In previous works we introduced a state-based approach to validate task-and end-to-end deadlines for distributed systems. To improve scalability and reduce the analysis time, the approach computes the state spaces of the individual resources in a compositional fashion. For this, abstraction and composition operations were defined to remove those parts of the inputs of resources which have no influence on the response times of the allocated tasks. In this work, a new abstraction technique is introduced for scenarios where event bursts occur. Further, we extend our approach for systems with cyclic dependencies among the resources. We evaluate our approach on a set of example scenarios and compare the results with the state-of-the-art tool Uppaal.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945501,no,undetermined,0
Event-based text visual analytics,"We present an event-based approach for solving a directed sensemaking task in which we combine powerful information foraging tools with intuitive synthesis spaces to solve the VAST Challenge 2014 Mini-Challenge 1. A combination of student-created and commericially available software are used to solve various aspects of the scenario. In addition to applying entitiy extraction and topic modelling, we enable the user to explore a large dataset using multi-model semantic interaction, which infers analytical reasoning from user actions to augment the data spatialization and determine what information should be presented and suggested to the user. Additionally, we visualize extracted topics using Tableau to construct a timeline of events surrounding the questions posed by the challenge.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042552,no,undetermined,0
Evolutionary data reorganization for efficient workload processing,"Digital data universe size is exponentially growing up from year to year and currently is estimated to be more than 4.4 Zb. It compels scientific community to found out more efficient approaches in collecting, organizing and processing of information. A lot of enterprise solutions offer extended software tools based on MapReduce principles for big data analytics. One of the required parts of MapReduce solutions is data replication organization which permanently helps to increase safety and to provide increased performance. In this paper we investigate the possibility of applying queries workload optimization using metaheuristic algorithm for data dynamic reorganization according to executed tasks influence in MapReduce-based storages.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7035952,no,undetermined,0
"Facilitating Twitter data analytics: Platform, language and functionality","Conducting analytics over data generated by Social Web portals such as Twitter is challenging, due to the volume, variety and velocity of the data. Commonly, adhoc pipelines are used that solve a particular use case. In this paper, we generalize across a range of typical Twitter-data use cases and determine a set of common characteristics. Based on this investigation, we present our Twitter Analytical Platform (TAP), a generic platform for conducting analytical tasks with Twitter data. The platform provides a domain-specific Twitter Analysis Language (TAL) as the interface to its functionality stack. TAL includes a set of analysis tools ranging from data collection and semantic enrichment, to machine learning. With these tools, it becomes possible to create and customize analytical workflows in TAL and build applications that make use of the analytics results. We showcase the applicability of our platform by building Twinder-a search engine for Twitter streams.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004259,no,undetermined,0
Feature-based comparison and selection of Software Defined Networking (SDN) controllers,"Software Defined Networking (SDN) is seen as one way to solve some problems of the Internet including security, managing complexity, multi-casting, load balancing, and energy efficiency. SDN is an architectural paradigm that separates the control plane of a networking device (e.g., a switch / router) from its data plane, making it feasible to control, monitor, and manage a network from a centralized node (the SDN controller). However, today there exists many SDN controllers including POX, FloodLight, and OpenDaylight. The question is, which of the controllers is to be selected and used? To find out the answer to this question, a decision making template is proposed in this paper to help researchers choose the SDN controller that best fits their needs. The method works as follows; first, several existing open-source controllers are analyzed to collect their properties. For selecting the suitable controller based on the derived requirements (for example, a äóìJavaäó interface must be provided by the controller), a matching mechanism is used to compare the properties of the controllers with the requirements. Additionally, for selecting the best controller based on optional requirements (for example, GUI will be extremely preferred over the age of the controller), a Multi-Criteria Decision Making (MCDM) method named Analytic Hierarchy Process (AHP) has been adapted by a monotonic interpolation / extrapolation mechanism which maps the values of the properties to a value in a pre-defined scale. By using the adapted AHP, the topmost five controllers have been compared and äóìRyuäó is selected to be the best controller based on our requirements.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6916572,no,undetermined,0
Fighting Botnets with Cyber-Security Analytics: Dealing with Heterogeneous Cyber-Security Information in New Generation SIEMs,"One of the cyber-threats with the highest impact nowadays, in terms of number of compromised systems and the impact they can have on the Internet at large, is commonly known as the botnet. In the ACDC (Advanced Cyber Defence Centre) project, partners from 14 European countries, including public administrations, private sector organizations and academia, are trying to achieve a sustainable victory over botnets. This paper presents how a new generation SIEM is being used in the ACDC project to leverage its scalability and enhanced analytic capabilities and produce advance cyber-intelligence from the heterogeneous and massive streams of data continuously produced in the cyber-security context, in combination with traditional security events and system logs. The paper describes a case study where this approach is being tested. In the case study, the SIEM has been adapted to cope, not only with traditional security events and system logs, but also with pre-analyzed information about cyber-threats and incidents reported by the tools of some of the ACDC partner organizations. The case study also tests the adoption of the standard XML-based format called STIX, developed by the Mitre Corporation in the USA, and its suitability as a common specification for exchanging cybersecurity information between a subset of ACDC tools, the Atos SL SIEM and the ACDC's centralized data clearing house (CCH).",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980282,no,undetermined,0
Finding the capacity of next-generation networks by linear programming,"Proving or disproving an information inequality is a crucial step in establishing the converse results in the coding theorems of communication networks. However, next-generation networks are very large-scale, typically involving multiple users and many transceivers and relays. This means that an information inequality involving many random variables can be difficult to be proved or disproved manually. In [1], Yeung developed a framework that uses linear programming for verifying linear information inequalities, and it was recently shown in [2] that this framework can be used to explicitly construct an analytic proof of an information inequality or an analytic counterexample to disprove it if the inequality is not true in general. In this paper, we consider the construction of the smallest counterexample, and also give sufficient conditions for that the inequality can be manipulated to become true. We also describe the software development of automating this analytical framework enabled by cloud computing to analytically verify information inequalities in large-scale problem setting.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7024792,no,undetermined,0
Detecting Industrial Control Malware Using Automated PLC Code Analytics,"The authors discuss their research on programmable logic controller (PLC) code analytics, which leverages safety engineering to detect and characterize PLC infections that target physical destruction of power plants. Their approach also draws on control theory, namely the field of engineering and mathematics that deals with the behavior of dynamical systems, to reverse-engineer safety-critical code to identify complex and highly dynamic safety properties for use in the hybrid code analytics approach.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7006408,no,undetermined,0
Designing in-wheel switched reluctance motor for electric vehicles,"Estimation of dimension parameters for an electrical machine has great importance before manufacturing. For this reason, analytical design should be performed in an optimum form. While motor analysis is accomplished by package programs, initial size parameters are intutivily provided and then various trials are examined to get optimum results. In this study, we are trying to find dimensional and electrical parameters generating mathematical equations in analytic approaches for In-Wheel Switched Reluctance Motor (IW-SRM), which will be employed by Electric Vehicle (EV). Therefore, optimum motor parameters for required speed and torque have been estimated by solving generated equations for in-wheel SRM with 18/12 poles via MATLAB. Using the parameters, analysis of in-wheel SRM has been carried out 3D Finite Element Method (FEM) by Ansoft Maxwell 15.0 Package Software. Consequently, the accuracy of the estimated parameters has been validated by the results of Maxwell 3D FEM.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980594,no,undetermined,0
"Designing a new infectious healthcare-waste management system in sfax governorate, tunisia","Infectious Healthcare-Waste (IHW) management is of great importance due to its potential environmental hazards and public health risks. The collection and the disposal of IHW are highly visible and important services that involve large expenditures in the world and particularly in Tunisia. This paper discusses two main problems in the current solid IHW management practices in Sfax governorate (Tunisia). The first problem is about how choose the best waste treatment and disposal scenario, using a multiple criteria decision making analysis. The Analytic hierarchy process (AHP) implemented with Expert choice software, confirms that the best scenario is to apply central steam sterilization equipment, which will be used by all hospitals. Accordingly, the second problem concerns the off-site transport of IHW from the twelve hospitals (public and private) in the governorate of Sfax to this overall center. This problem of transportation is modeled as a capacitated vehicle routing problem (CVRP). Experimental results are reported for the design of the routing system for IHW transportation, using the solver CPLEX 9.0 software. Future legislation concerning IHW management is supposed to take into account the various results of this research.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6866337,no,undetermined,0
Design and Implementation of Learning Analytics System for Teachers and Learners Based on the Specified LMS,"Learning analytics can give powers to teachers and learners to optimize their teaching and learning. This paper presents the design and implementation of learning analytics system for teachers and students based on a special LMS -- THEOL LMS, which is commonly used in blended learning in universities and colleges. The technical architecture of learning analytics system for teachers and students is different from for managers and researchers because of the difference of the requirements. For helping teachers and learners improve their teaching and learning, the primary aim of the THEOL learning analytics system includes: can collect data from THEOL LMS and show analysis results in THEOL LMS, supports all the teachers and learners in the THEOL LMS system, has the flexibility to do individual data analysis for expert users, can give the analysis result online and on time.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982567,no,undetermined,0
Big R: Large-Scale Analytics on Hadoop Using R,"As the volume of available data continues to rapidly grow from a variety of sources, scalable and performant analytics solutions have become an essential tool to enhance business productivity and revenue. Existing data analysis environments, such as R, are constrained by the size of the main memory and cannot scale in many applications. This paper introduces Big R, a new platform which enables accessing, manipulating, analyzing, and visualizing data residing on a Hadoop cluster from the R user interface. Big R is inspired by R semantics and overloads a number of R primitives to support big data. Hence, users will be able to quickly prototype big data analytics routines without the need of learning a new programming paradigm. The current Big R implementation works on two main fronts: (1) data exploration, which enables R as a query language for Hadoop and (2) partitioned execution, allowing the execution of any R function on smaller pieces of a large dataset across the nodes in the cluster.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906830,no,undetermined,0
BigData visualization: Parallel coordinates using density approach,"Information visualization is a very important tool in BigData analytics. BigData, structured and unstructured data which contains images, videos, texts, audio and other forms of data, collected from multiple datasets, is too big, too complex and moves too fast to analyse using traditional methods. This has given rise to two issues; 1) how to reduce multidimensional data without the loss of any data patterns for multiple datasets, 2) how to visualize BigData patterns for analysis. In this paper, we have classified the BigData attributes into `5Ws' data dimensions, and then established a `5Ws' density approach that represents the characteristics of data flow patterns. We use parallel coordinates to display the `5Ws' sending and receiving densities which provide more analytic features for BigData analysis. The experiment shows that this new model with parallel coordinate visualization can be efficiently used for BigData analysis and visualization.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009441,no,undetermined,0
Blueprint for Business Middleware as a Managed Cloud Service,"Cloud offers numerous technical middleware services such as databases, caches, messaging systems, and storage but very few business middleware services as first tier managed services. Business middleware such as business process management, business rules, operational decision management, content management and business analytics, if deployed in a cloud environment, is typically only available in a hosted (black-box) model. This is partly due to where cloud is in its evolution, and mostly due to the relatively higher complexity of business middleware vs. technical middleware in the deployment, provisioning, usage, etc. Business middleware consists of multiple functions for business processes design and modeling, execution, optimization, monitoring, and analysis. These functions and their associated complexity have inhibited the wholesale migration of existing business middleware to the cloud. To better understand the complexity in bringing business middleware to the cloud and to develop a systematic cloud enablement approach, we studied the deployment of IBM's Operational Decision Manager (ODM) business middleware product as a managed service (Cloud Decision Service) in IBM's BlueMix cloud platform. Our study indicates that complex middleware must be componentized along functional boundaries, and provide these functions for different business users and developers with cloud experience. In addition, middleware services must leverage other cloud services and they should provide interfaces so that they can be consumed by Java applications as well as by polyglot applications (JavaScript, Ruby, Python, etc). Applications can bind to and use our Cloud Decision Service in a matter of seconds. In contrast, it takes hours to days to setup such a service in the traditional packaged software model. Based on the lessons learned from this experiment we develop a blueprint for enabling high value business middleware as managed cloud services.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903481,no,undetermined,0
Building a Massive Stream Computing Platform for Flexible Applications,"Driven by the rapid growth of large scale real-time data mining applications for personalized ads and content recommendations, distributed stream processing systems are widely applied in modern big-data architectures. Designs of existing stream computing systems are mostly focusing on the scalability and availability issues. Other important issues which are essential to the actual cost and productivity, such as the fluctuating work load handling, the stream topology alternation efficiency and the computing topology overlapping, are not well studied. To address these issues in a live, production environment, a new stream processing architecture that is based on a scalability enhanced subscription model is proposed in this paper. We also present a system, called Vortex, that has been implemented using this new architecture. Vortex is a distributed stream computing system engineered to support flexible applications at Baidu. The new architecture enables Vortex to scale well for highly fluctuating workloads and perform on-demand stream topology alternations with minimal overheads. Furthermore, the dynamic message routing mechanism of Vortex allows one processing node to serve different stream topologies. This maximizes the computing resource utilization in the scenarios of topology overlapping. With all these features, Vortex is a powerful platform for both realtime data processing and Map-Reduce job acceleration. Finally, in this paper, we also discuss some applications at Baidu to demonstrate how Vortex can be deployed for various stream computing applications ranging from real-time analytics to the efficient large-scale data mining.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906810,no,undetermined,0
Building a National E-Service using Sentire experience report on the use of Sentire: A volere-based requirements framework driven by calibrated personas and simulated user feedback,"User experience (UX) is difficult to quantify and thus more challenging to require and guarantee. It is also difficult to gauge the potential impact on users' lived experience, especially at the earlier stages of the development life cycle, particularly before hi fidelity prototypes are developed. We believe that the enrolment process is a major hurdle for e-government service adoption and badly designed processes might result in negative repercussions for both the policy maker and the different user groups involved; non-adoption and resentment are two risks that may result in low return on investment (ROI), lost political goodwill and ultimately a negative lived experience for citizens. Identity assurance requirements need to balance out the real value of the assets being secured (risk) with the user groups' acceptance thresholds (based on a continuous cost-benefit exercise factoring in cognitive and physical workload). Sentire is a persona-centric requirements framework built on and extending the Volere requirements process with UX-analytics, reusable user behavioural models and simulated user feedback through calibrated personas. In this paper we present a story on how Sentire was adopted in the development of a national public-facing e-service. Daily journaling was used throughout the project and a custom built cloud-based CASE tool was used to manage the whole process. This paper outlines our experiences and lessons learnt.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912288,no,undetermined,0
Characterization of semi-synthetic dataset for big-data semantic analysis,"Over the past decade, the use of semantic databases has served as the basis for storing and analyzing complex, heterogeneous, and irregular data. While there are similarities with traditional relational database systems, semantic data stores provide a rich platform for conducting non-traditional analyses of data. In support of new graph analytic algorithms and specialized graph analytic hardware, we have developed a large semi-synthetic, semantically rich dataset. The construction of this dataset mimics the real-world scenario of using relational databases as the basis for semantic data construction. In order to achieve real-world variable distributions and variable dependencies, data.gov data was used as the basis for developing an approach to build arbitrarily large semi-synthetic datasets. The intent of the semi-synthetic dataset is to serve as a testbed for new semantic graph analyses and computational software/hardware platforms. The construction process and basic data characterization is described. All code related to the data collection, consolidation, and augmentation are available for distribution.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7040994,no,undetermined,0
Cloud Mobile Media: Reflections and Outlook,"This paper surveys the emerging paradigm of cloud mobile media. We start with two alternative perspectives for cloud mobile media networks: an end-to-end view and a layered view. Summaries of existing research in this area are organized according to the layered service framework: i) cloud resource management and control in infrastructure-as-a-service (IaaS), ii) cloud-based media services in platform-as-a-service (PaaS), and iii) novel cloud-based systems and applications in software-as-a-service (SaaS). We further substantiate our proposed design principles for cloud-based mobile media using a concrete case study: a cloud-centric media platform (CCMP) developed at Nanyang Technological University. Finally, this paper concludes with an outlook of open research problems for realizing the vision of cloud-based mobile media.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6782722,no,undetermined,0
Cloud service recommendation based on trust measurement using ternary interval numbers,"Owing to the deficiency of usage experiences and the information overload of QoE (quality of experience) evaluations from consumers, how to discover the trustworthy cloud services is a challenge for potential users. This paper proposed a cloud service recommendation approach based on trust measurement using ternary interval numbers for potential user. The concept of ternary interval number is introduced. The user feature maybe affecting the QoE evaluations are analyzed and the client-side feature similarity between consumers and potential user is calculated. The transform mechanism from trust evaluations to ternary interval number is presented by employing the K-means clustering algorithm. On the basis of multi-attributes trust aggregation based On FAHP (fuzzy analytic hierarchy process) method, a new possibility degree formula is designed for ranking ternary interval numbers and selecting trustworthy service. Finally, the experiments and results show that this approach is effective to improve the accuracy of the trustworthy service recommendation.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7043834,no,undetermined,0
Cloud-Based Data Analytics Framework for Autonomic Smart Grid Management,"Global energy problems necessitate an urgent transformation of the existing electrical generation grid into a smart grid, rather than a gradual evolution. A smart grid is a real-time bi-directional communication network between end users and their utility companies which monitors power demand and manages the provisioning and transport of electricity from all generation sources. As a crucial part of this transformation, increasing numbers of smart meters generate correspondingly increasing amounts of data every day. Analyzing this data to extract insight into, and to maintain control over energy usage has become a big data problem - one which cannot be handled manually, and which requires autonomic computing solutions. In this paper, we examine electric vehicles (EVs) as a use case to investigate how to use social media, sensing data, and big data analytics to optimize smart grid management. We discuss the requirements to realize such an approach and describe an autonomic system architecture and a possible design. We believe the proposed architecture and strategy will help optimize how provisioning is performed in a smart grid, even when smart meters are not available.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7024050,no,undetermined,0
CloudWave: Where adaptive cloud management meets DevOps,"The transition to cloud computing offers a large number of benefits, such as lower capital costs and a highly agile environment. Yet, the development of software engineering practices has not kept pace with this change. Moreover, the design and runtime behavior of cloud based services and the underlying cloud infrastructure are largely decoupled from one another.This paper describes the innovative concepts being developed by CloudWave to utilize the principles of DevOps to create an execution analytics cloud infrastructure where, through the use of programmable monitoring and online data abstraction, much more relevant information for the optimization of the ecosystem is obtained. Required optimizations are subsequently negotiated between the applications and the cloud infrastructure to obtain coordinated adaption of the ecosystem. Additionally, the project is developing the technology for a Feedback Driven Development Standard Development Kit which will utilize the data gathered through execution analytics to supply developers with a powerful mechanism to shorten application development cycles.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912638,no,undetermined,0
Comparative Evaluation of Registration Algorithms in Different Brain Databases With Varying Difficulty: Results and Insights,"Evaluating various algorithms for the inter-subject registration of brain magnetic resonance images (MRI) is a necessary topic receiving growing attention. Existing studies evaluated image registration algorithms in specific tasks or using specific databases (e.g., only for skull-stripped images, only for single-site images, etc.). Consequently, the choice of registration algorithms seems task- and usage/parameter-dependent. Nevertheless, recent large-scale, often multi-institutional imaging-related studies create the need and raise the question whether some registration algorithms can 1) generally apply to various tasks/databases posing various challenges; 2) perform consistently well, and while doing so, 3) require minimal or ideally no parameter tuning. In seeking answers to this question, we evaluated 12 general-purpose registration algorithms, for their generality, accuracy and robustness. We fixed their parameters at values suggested by algorithm developers as reported in the literature. We tested them in 7 databases/tasks, which present one or more of 4 commonly-encountered challenges: 1) inter-subject anatomical variability in skull-stripped images; 2) intensity homogeneity, noise and large structural differences in raw images; 3) imaging protocol and field-of-view (FOV) differences in multi-site data; and 4) missing correspondences in pathology-bearing images. Totally 7,562 registrations were performed. Registration accuracies were measured by (multi-)expert-annotated landmarks or regions of interest (ROIs). To ensure reproducibility, we used public software tools, public databases (whenever possible), and we fully disclose the parameter settings. We show evaluation results, and discuss the performances in light of algorithms' similarity metrics, transformation models and optimization strategies. We also discuss future directions for the algorithm development and evaluations.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6834815,no,undetermined,0
Compiling text analytics queries to FPGAs,"Extracting information from unstructured text data is a compute-intensive task. The performance of general-purpose processors cannot keep up with the rapid growth of textual data. Therefore we discuss the use of FPGAs to perform large scale text analytics. We present a framework consisting of a compiler and an operator library capable of generating a Verilog processing pipeline from a text analytics query specified in the annotation query language AQL. The operator library comprises a set of configurable modules capable of performing relational and extraction tasks which can be assembled by the compiler to represent a full annotation operator graph. Leveraging the nature of text processing we show that most tasks can be performed in an efficient streaming fashion. We evaluate the performance, power consumption and hardware utilization of our approach for a set of different queries compiled to a Stratix IV FPGA. Measurements show an up to 79 times improvement of document-throughput over a 64 threaded software implementation on a POWER7 server. Moreover the accelerated system's energy efficiency is up to 85 times better.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927500,no,undetermined,0
COnCEPT developing intelligent information systems to support colloborative working across design teams,"Rapid developments in hardware and software are creating opportunities to enhance the user experience. For example, advances in social analytics can provide near instant feedback. State of the art information extraction tools, filtering, categorization and presentation mechanisms all greatly facilitate knowledge exploitation activities. However, these technologies are not yet fully integrated into modern business systems. This paper describes research being undertaken in order to develop a new collaborative creative design platform (COnCEPT) aimed at investigating of new data-mining and collaboration technologies in order to enhance the information systems of future businesses. This paper describes the software architecture and the components, together with the design principles which underpin the design of the new COnCEPT platform, which is being developed to address the needs of professional design teams working collaboratively in a professional context.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293855,no,undetermined,0
Customizing Scientific Data Analytic Platforms via SaaS Approach,"At the era of data driven science discovery, it is essential to provide customizable scientific data analytic platforms for researchers to conduct their personalized data intensive analysis. Science Gateway has been a viable solution to enabling scientists to run scientific simulations, data analysis, and visualization through their web browsers. But most science gateway frameworks are designed for integrating commonly used software tools and datasets in a specific science domain, thus requiring significant effort to implement the essential variability in lab-specific data processing workflows. In this paper we introduce a multitenancy architecture (MTA) based customization framework that can greatly accelerate the customization cycle of science gateway systems. Each tenant has his own workspace that assembles the software stack and tools to meet the software requirements of his specific data analytics tasks. Through this framework, developers can import their domain-specific analysis pipeline scripts and mashup relevant templates including GUI templates, tool recipes and workspace templates to generate both workspace and web interface for running these application workflows and visualizing the output from workflow executions without writing extra wrapping code.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6830930,no,undetermined,0
Data Fusion as an Enterprise Service,"We present our work-in-progress on making an analytic data fusion service, originally developed and deployed to support high-performance computing and cloud-based applications, available in a enterprise service-oriented architecture (SOA) environment. We posit that not only can SOA-based integration of research software be useful in enterprise business use cases, but also that providing ways to integrate domain-specific and enterprise data is beneficial. We describe use cases driving our work, our data fusion service and its analytic capabilities, and our integration efforts using an open-source enterprise service bus framework.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6930616,no,undetermined,0
Data Interlocking: Coupling Analytics to the Data,"'Big data' analytics can be defined by the requirement for flexible, high throughput computational analysis methods applied to large, heterogeneous datasets. We propose an architectural approach to 'big data' challenges in which the movement of data is minimized, and analysis methods are implemented on the data as portable services. We term this approach 'data interlocking'. We demonstrate the feasibility of this approach through a domain specific implementation of a data interlocking architecture, in which an on-demand computational workbench provides portable high-throughput analysis methods to large genomic datasets on cloud infrastructure.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027580,no,undetermined,0
"Data, Data Everywhere...",Editor-in-chief Forrest Shull talks about the practical application of software analytics.,2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898721,no,undetermined,0
Decision-Making about Software Release Time Using Analytic Hierarchy Process,"This paper shows a systematic method to decide software release time using AHP (analytic hierarchy process). The method helps a project team to make a correct decision about the software release time by combining multiple evaluations obtained from different viewpoints. In general, it is difficult to obtain quantitative evaluations of all the aspects to be considered in a software development project, due to the lack of methodologies, manpower, and information. Therefore, qualitative evaluation based on the intuition and experience of specialists in software development, software sales activities, management, etc. is incorporated into the decision-making process.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913396,no,undetermined,0
Decisively: Application of Quantitative Analysis and Decision Science in Agile Requirements Engineering,"While many mature Requirements Engineering (RE) tools for Agile exist, RE professionals at large have not been able to benefit from Quantitative Analysis and Decision Science (QUADS) techniques in this context. In this paper we present an Agile RE tool, Decisively, which brings a new perspective to automation in the RE process through application of QUADS to address Requirement Discovery, Analysis, Estimation and Prioritization. Techniques explored in Decisively include Analytical Hierarchical Process (AHP) for prioritization and estimation, Lorenz function to shortlist user stories by analyzing the distribution of votes, Box Plot Analysis to predict velocity, and Text Mining to discover implied requirements from documents.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912278,no,undetermined,0
Flexible traffic management in broadband access networks using Software Defined Networking,"Over the years, the demand for high bandwidth services, such as live and on-demand video streaming, steadily increased. The adequate provisioning of such services is challenging and requires complex network management mechanisms to be implemented by Internet service providers (ISPs). In current broadband network architectures, the traffic of subscribers is tunneled through a single aggregation point, independent of the different service types it belongs to. While having a single aggregation point eases the management of subscribers for the ISP, it implies huge bandwidth requirements for the aggregation point and potentially high end-to-end latency for subscribers. An alternative would be a distributed subscriber management, adding more complexity to the management itself. In this paper, a new traffic management architecture is proposed that uses the concept of Software Defined Networking (SDN) to extend the existing Ethernet-based broadband network architecture, enabling a more efficient traffic management for an ISP. By using SDN-enabled home gateways, the ISP can configure traffic flows more dynamically, optimizing throughput in the network, especially for bandwidth-intensive services. Furthermore, a proof-of-concept implementation of the approach is presented to show the general feasibility and study configuration tradeoffs. Analytic considerations and testbed measurements show that the approach scales well with an increasing number of subscriber sessions.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6838322,no,undetermined,0
Framework for evaluating Capture the Flag (CTF) security competitions,"A large number of ethical hacking competitions are organized worldwide as Capture The Flag (CTF) events. But there does not exist a framework to evaluate and rank CTFs that will guide participants as to which CTF's to participate. In a CTF event, the participants are required to either solve a set of challenges to gain points or they are required to defend their system by eliminating the vulnerabilities while attacking other's system vulnerabilities. We are proposing a framework that would evaluate and rank CTFs according to factors like similarity of the tasks to the common critical vulnerabilities, solvability of tasks, periodicity, training given prior to CTF, geographical reach, problem solving skills etc. In the next step these factors are systematically assigned weights using Analytic Hierarchy Process. As part of frame work creation and validation, ten CTFs have been analysed. Our analysis indicates that: All CTFs fall in to one of the three categories (jeopardy, attack-defence and mixed); CTFs often adopt popular software vulnerabilities and threats as tasks to be solved; Only few CTFs give formal training prior to the event; Complexity of the tasks to be solved varies from CTF to CTF. Five CTFs were ranked using the newly developed framework.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092098,no,undetermined,0
Multiple Kernel Learning Based Multi-view Spectral Clustering,"For a given data set, exploring their multi-view instances under a clustering framework is a practical way to boost the clustering performance. This is because that each view might reflect partial information for the existing data. Furthermore, due to the noise and other impact factors, exploring these instances from different views will enhance the mining of the real structure and feature information within the data set. In this paper, we propose a multiple kernel spectral clustering algorithm through the multi-view instances on the given data set. By combining the kernel matrix learning and the spectral clustering optimization into one process framework, the algorithm can determine the kernel weights and cluster the multi-view data simultaneously. We compare the proposed algorithm with some recent published methods on real-world datasets to show the efficiency of the proposed algorithm.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977360,no,undetermined,0
M2C: Energy efficient mobile cloud system for deep learning,"With the number increasing of applications and services that are available on mobile devices, mobile cloud computing has drawn a substantial amount of attention by academia and industry in the past several years. When facing the most exciting machine learning applications such as deep learning, the computing requirement is intensive. For the purpose of improving energy efficiency of mobile device and enhancing the performance of applications through reducing execution time, M2C offloads computation of its machine learning application to the cloud side. We propose the prototype of M2C with the mobile side on Android, iPad and with the cloud side on the open source cloud: Spark, a part of the Berkeley Data Analytics Stack with NVIDA GPU. M2C's distinct set of varying computational tools and mobile nodes allows for thorough implementing distributed machine learning algorithm and innovative wireless protocols with energy efficiency, verifying the theoretical research and bringing the user extremely fast experience.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6849208,no,undetermined,0
In unity there is strength: Showcasing a unified big data platform with MapReduce Over both object and file storage,"Big Data platforms often need to support emerging data sources and applications while accommodating existing ones. Since different data and applications have varying requirements, multiple types of data stores (e.g. file-based and object-based) frequently co-exist in the same solution today without proper integration. Hence cross-store data access, key to effective data analytics, can not be achieved without laborious application re-programming, prohibitively expensive data migration, and/or costly maintenance of multiple data copies. We address this vital issue by introducing a first unified big data platform over heterogeneous storage. In particular, we present a prototype joining Apache Hadoop MapReduce with OpenStack's open-source object store Swift and IBM's cluster file system GPFS<sup>TM</sup>. A sentiment analysis application using 3 months of real Twitter data is employed to test and showcase our prototype. We have found that our prototype achieves 50% data capacity savings, eliminates data migration overhead, offers stronger reliability and enterprise support. Through our case study, we have learned important theoretical lessons concerning performance and reliability, as well as practical ones related to platform configuration. We have also identified several potentially high-impact research directions.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004328,no,undetermined,0
Individual decision model for urban regional land planning,"One example of the problems that affect many people's lives is urban regional land planning. Required a collaboration mechanism which based on the ability of decision analysis in determining the spatial policy of the use of the land. This paper contains an individual decision model by using Analytical Hierarchical Process-AHP technique. The model is applied to the case of the selection of the best location Green Open Spaces Samarinda City. Among the eight criteria, namely C1 (land area) is the criterion with the highest weight (0.208), followed by C3 (population=0.152) and then C2 (land prices=143) and so on. Location of Citra Niaga is the most suitable to serve as a green open space, followed by the Ex Kaltim, Mahakam edge, the port area and the around warehouses.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7065753,no,undetermined,0
Integration and Virtualization of Relational SQL and NoSQL Systems Including MySQL and MongoDB,"NoSQL databases are growing in popularity for Big Data applications in web analytics and supporting large web sites due to their high availability and scalability. Since each NoSQL system has its own API and does not typically support standards such as SQL and JDBC, integrating these systems with other enterprise and reporting software requires extra effort. In this work, we present a generic standards-based architecture that allows NoSQL systems, with specific focus on MongoDB, to be queried using SQL and seamlessly interact with any software supporting JDBC. A virtualization system is built on top of the NoSQL sources that translates SQL queries into the source-specific APIs. The virtualization architecture allows users to query and join data from both NoSQL and relational SQL systems in a single SQL query. Experimental results demonstrate that the virtualization layer adds minimal overhead in translating SQL to NoSQL APIs, and the virtualization system can efficiently perform joins across sources.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6822123,no,undetermined,0
Intersection of the Cloud and Big Data,"The two biggest trends in the data center today are cloud computing and big data. This column will examine the intersection of the two. Industry hype has resulted in nebulous definitions for each, so I'll start by defining terms.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6848687,no,undetermined,0
Introduction to Social Media and E-Business Transformation Minitrack,"Social media are online platforms that facilitate global collaboration and sharing amongst users. New social media applications in e-business and e-commerce appear on a daily basis and result in enormous shocks to the ecosystem of individuals and businesses. This minitrack provides a forum for the exchange of research ideas and best practices related to social media in e-business environments. It also aims to raise awareness in terms of the latest developments in social media, and address the challenges of using social media. This year, eight papers were selected for inclusion in the proceedings. The first paper, ""Social Media at Socio Systems Inc.: A Socio-technical Systems Analysis of Strategic Action"" by Don Heath, Rahul Singh and Jai Ganesh proposes an analytic framework to explain organizational strategies for directed action in social media. The next paper by Eric T.K. Lim, Dianne Cyr, and Chee-Wee Tan, ""Understanding Members' Attachment to Social Networking Sites: An Empirical Investigation of Three Theories"", constructs a theoretical model of members' communal attachments within SNSs. The model is then empirically validated via an online survey of 787 active members of SNSs. Drawing from the push-pull-mooring model and uses and gratification theory, Fei Liu and Bo Xiao proposed and empirically tested a theoretical model explaining SNS users' switching behavior in their paper, ""Do I Switch? Understanding Users' Intention to Switch between Social Network Sites"". The fourth paper by Alexander Richte, David Wagner and Andrea Back, ""Leadership 2.0: Engaging and Supporting Leaders in the Transition Towards a Networked Organization"", illustrates the concept of Leadership 2.0 through a series of interviews with the persons who are responsible for the implementation of social software at publicly listed, multinational organizations in Germany. The next paper, ""Understanding Information Adopt- on in Online Review Communities: The Role of Herd Factors"" by Xiao-Liang Shen, Kem Z.K. Zhang, and Sesia J. Zhao, extends prior research on information adoption by incorporating the perspective of herd behavior to explain the influence of massive online reviews in online communities. The research model was empirically tested with 376 users of a Chinese online review community. ""Impact of Online Firm Generated Content (FGC) on Supply Chain Performance: An Exploratory Empirical Analysis"", by Ajaya Swain and Qing Cao uses an advanced sentiment analysis approach to examine the impact of FGC effect on supply chain performance. Information sharing and collaboration are identified as two key FGC elements affecting supply chain performance. Based on an experimental investigation of the judgment ability of 478 subjects, Christian Wagner and Ayoung Suh found that collective size and expertise transfer effects are moderated by task difficulty and are strongest for tasks in a medium difficulty range in their paper, ""The Wisdom of Crowds: Impact of Collective Size and Expertise Transfer on Collective Performance"". The final paper, ""Assessing the Effects of Navigation Support and Group Structure on Collaborative Online Consumers' Consensus and Mutual Understanding"" by Yanzhen Yue and Zhenhui (Jack) Jiang, explores an emerging phenomenon of collaborative online shopping by investigating the effects of navigation support and group structure on collaborative online consumers' consensus and mutual understanding. We thank the authors for submitting their work to make this another engaging minitrack. We hope you enjoy the papers and their presentation at the conference.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758671,no,undetermined,0
Janus -- Analytics-Driven Transition Planner,"In this paper, we address the problem of transition of IT operations from one service provider to another. We present analytics-driven solutions to generate a transition plan while addressing various aspects such as coverage, risk, time, and cost. We model the IT operations through graphs and use the well defined problems in graph theory to build solutions for transition planner. We demonstrate the proof-of-concept of proposed ideas using a real-world case-study.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023390,no,undetermined,0
Learning Methods for Rating the Difficulty of Reading Comprehension Questions,"This work deals with an Intelligent Tutoring System (ITS) for reading comprehension. Such a system could promote reading comprehension skills. An important step towards building a full ITS for reading comprehension is to build an automated ranking system that will assign a hardness level to questions used by the ITS. This is the main concern of this work. For this purpose we, first, had to define the set of criteria that determines the rate of difficulty of a question. Second, we prepared a bank of questions that were rated by a panel of experts using the set of criteria defined above. Third, we developed an automated rating software based on the criteria defined above. In particular, we considered and compared different machine learning techniques for the ranking system of the third part of the process: Artificial Neural Network (ANN), Support Vector Machine (SVM), decision tree and nai’šve Bayesian network. The definition of the criteria set for rating a question's difficulty, and the development of an automated software for rating a questions' difficulty, contribute to a tremendous advancement in the ITS domain for reading comprehension by providing a uniform, objective and automated system for determining a question's difficulty.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6887542,no,undetermined,0
Linking Social Media with Open Innovation: An Intelligent Model,"A hybrid intelligent model for linking social media with open innovation strategies, processes and diffusion is proposed and discussed in this paper. In order to deal with the various facets or properties of the open innovation problem, we recommend and present a new paradigm and framework for integrating the strengths or advantages of intelligent software agents, fuzzy logic, expert systems, complex adaptive system theory, the analytic hierarchy process, simulation technique, and hybrid intelligent system method. The theoretical underpinning and rationale for the hybrid framework are provided. In addition, the effectiveness and efficiency of using social media, artificial intelligence, groupware and group decision support systems, and other decision support techniques and technologies in open innovation management are explored. Furthermore, research hypotheses on this topic are formulated.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7003550,no,undetermined,0
Load balancing solution based on AHP for Hadoop,"Apache Hadoop is an open-source software framework for cloud computing, the server load balancing algorithm named as Balancer which Hadoop provided can load balance for each DataNode, but Balancer algorithm only consider the storage space factor, in this case it will easily lead to load imbalance while operation of the system for a long time. To solve this problem, we consider the file number of visited, frequency of concurrent access, bandwidth, CPU power, memory utilization and other factors, we propose a load balancing solution combined APH, our solution can more accurately calculate the load capacity for each DataNode for Hadoop, and it can automatically adjust the load capacity of Hadoop distributed system while user requests a file again.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6845699,no,undetermined,0
Making sense of daily life data: From commonalities to anomalies: VAST 2014 Mini Challenge #2,"We report the approach and results on the VAST 2014 Mini-Challenge 2: Analysis Movement and Tracking data of GAStech Employees' daily lives. Based on the commercial interactive visualization software Tableau[l], we follow the sense-making loop for analysis of the massive multi-dimensional, multi-source and time-varying data sets. The findings show that we can effectively identify the patterns and discovery the anomaly from these complex data sets.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042568,no,undetermined,0
Frequency Veering Analysis of Cable-Girder System for Cable-Stayed Bridges,Analytic characteristic values of the simplified model were solved for cable-girder system. The characteristic vectors were derived. The system frequencies and modes were obtained which were simplified with non-dimensional method. The frequency veering of cable-girder system was analyzed with Maple numerical analysis software. The results show that there exist frequency veering and mode conversion among cable-girder modes under special parameter combination. Some slight changes of mechanical parameters cause fast conversion among cable-girder modes. Hybrid modes appear in the center of frequency veering area. Mode localization factor can be used to predict whether hybrid modes occur in cable-girder system effectively.,2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7003692,no,undetermined,0
Management of complex data objects in ship designing process,"Management of ship designing is difficult because of the complexity in modeling of the process and large amounts of data throughout the process. Though existing technologies of BPMN can solve the first problem, BPMN does not provide sufficient supports on dealing with complex dependencies in the process, e.g., storage and search problems of correlated data with äóìmany to manyäó relations. In this paper, we introduce a series of annotations of data objects to solve this problem. First, we extend the annotations of data objects based on BPMN and utilize foreign key in relational database to manage the relations between data objects. Second, we use SQL queries to execute the common operations to data objects in the process. Our approach is the extension of mature BPMN and database technologies, so it is standard and reusable. We implemented our approach on the Produce Management System to verify the availability and efficiency.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058123,no,undetermined,0
MatchVis: A generalized visual multi-scale analysis framework for competitive sports,"Sports are highly competitive, fast-paced, and teamwork-based. In this article, we introduce a novel approach in analyzing competitive sports based on music metaphor. Our proposed framework MatchVis extracts match information from raw webcast dataset about NBA and incorporates historical models into the investigation, providing a more compact and understandable visual representation of the details and patterns of match, which can consequently aid analysts in performing specific tasks and decision-making.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042533,no,undetermined,0
Methods selection and overall process of distribution network planning comprehensive evaluation,"Distribution network planning is an important constituent of overall urban planning. It is advantageous to the improvement of decision-making level and benefit of investment through performing comprehensive assessment on planning schemes. Comprehensive evaluation of distribution network planning is a multiple attribute decision making problem. The typical methods to solve this problem include analytic hierarchy process (AHP), data envelopment analysis (DEA), and technology for order preference by similarity to ideal solution (TOPSIS), etc. It is of significance to select appropriate methods according to specific distribution network planning. For the first time, this paper proposes that evaluation methods could be selected according to three attributes of the evaluation object, including preference information, scheme number and the comparison between scheme number and index number. In the distribution network planning assessment, indices are not equally important. So it is a problem how to determine the preference information. The object' s scheme number could be either single or multiple, and the scheme number is normally less than the index number. Based on these characteristics, the suitable methods for distribution network planning are analytic hierarchy process (AHP), fuzzy comprehensive evaluation (FCE) and the delphi method (DM). The overall process of distribution network planning assessment using the selected methods is also proposed, including construction of index system, setting of index weight, setting of index score standard, analysis of the evaluation results and so on. The effectiveness of the proposed method is verified by analysis on actual distribution network planning schemes for a certain region in 2015. The results, which can be used for similar evaluation work, will be of great value in real applications.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991789,no,undetermined,0
Metrics for effectiveness of e-learning objects in software engineering education,"In this paper we present the rationale and beginning of work on improving e-learning objects through the use of analytics modeled after Google Analytics. Prior work on the use of metrics in e-learning has focused on user satisfaction, and the ranking and selection of learning objects from a set of available choices. This work differs in its focus on the kinds of metrics needed to improve an existing object, or even more specifically to make improvements to specific pages within an object. This work is based on the now well established track record of using Google Analytics for Web site optimization in e-commerce. We discuss adaptations needed to apply similar metrics in the context of e-learning and more specifically e-learning objects.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6950671,no,undetermined,0
Minimally-invasive body-fluids analyzer for continuous glucose monitoring,"Summary form only given. Efficient management of blood glucose (BG) levels in diabetic patients relies heavily on accurate and frequent analysis of BG levels. Therefore, detection techniques (direct or indirect) that can provide reliable measurements of BG levels are of utmost interest. In a typical integrated analytic device, sampling (most utilized are the lancing devices) is a painful experience for patients that require frequent finger-prick to draw a minimal volume of blood for analysis. Besides the unpleasant experience, drawing blood is a source of contamination and/or transfer of blood-borne infections. Microneedles is one of the great candidates for minimally invasive BG level monitoring. One type of microneedles is the hollow one which is used to extract small samples of blood or body interstitial fluids to measure the BG level. A second type of microneedles is used to probe the impedance of the blood to indirectly measure the BG level. We designed a new miniature attenuated total reflectance-Fourier transform infrared spectrophotomer for body-fluids analysis. The key component of the system is the microneedle used to probe and detect the BG level either directly or indirectly (interstitial fluid sampling). Our approach will exploit established infrared spectrophotometry technology in a new design of the probe element for spectral analysis. We designed and simulated the microneedle shown in the figure that analyzes blood or interstitial fluid at the site of stinging and directly on the surface of the microneedle. Full 2-D optical analysis using Comsol software is performed. The microneedle is excited from its top tapered side by a normal incident of a Gaussian beam. The beam will propagate inside the microneedle, which will encounter multiple reflections from the microneedle surface in contact with the interstitial fluids and will return back to the same tip of the microneedle. The other side is plated with metallic reflective layer to fully reflect the i- cident Gaussian beam back to exit from the same tip of the microneedle. The reflected Gaussian beam with a reference beam will constitute an interferogram from which the signature of the glucose is mathematically extracted. The fabrication process of the microneedle is a simple three masks process composed of wet and deep reactive ion etching of the device layer of an SOI wafer.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6955630,no,undetermined,0
Mobile Traffic Analysis Exploiting a Cloud Infrastructure and Hardware Accelerators,"Recently, traffic analysis and measurements have been used to characterize, from a security point of view, applications' and network behavior to avoid intrusion attempts, malware injections and data theft. Since most of the generated data traffic is from the embedded mobile devices, the analysis techniques have to cope on the one hand with the scarce computing capabilities and battery limitation of the devices, and on the other hand with tight performance constraints due to the huge generated traffic. In recent years, several machine learning approaches have been proposed in the literature, providing different levels of accuracy and requiring high computation resources to extract the analytic model from available training set. In this paper, we discuss a traffic analysis architecture that exploits FPGA technology to efficiently implement a hardware traffic analyzer on mobile devices, and a cloud infrastructure for the dynamic generation and updating of the data model based on ongoing mis-classification events. Finally, we provide a case study based on the implementation of the proposed traffic analyzer on a Xilinx Zynq 7000 architecture and Android OS, and show an overview of the proposed cloud infrastructure.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7024620,no,undetermined,0
Model of capacity estimation of roundabout in Beijing,"Weaving sections have been shown to have a great effect on capacity of a roundabout, because at weaving sections the traffic conflict and interference are predominant, resulting in a reduction of speed as well as increase of gaps. It is known that weaving section of roundabout is the bottleneck of the traffic movement. Therefore, it should be given the highest priority in the capacity analysis. The current methods of estimating the approach capacity of a roundabout mainly utilize information of vehicles entering and exiting the roundabout. From literature review it has been found that few studies are conducted based on the traffic characteristics of weaving section in capacity estimation. The purpose of this research is to estimate the capacity of roundabout by modeling radius of roundabout, width of circulating roadway, and gap acceptance at the weaving sections. For this reason, a total of 21 roundabouts that are located at the metropolitan Beijing are selected to conduct data collection. Significance analysis of speed at weaving sections has been conducted by means of the variance analytic method. By taking geometric conditions and traffic conditions into account, new concepts of äóìQuasi-saturation state of roundaboutäó and äóìThe Capacity of Roundaboutäó are introduced in this study. The regression models of the headway and the radius on both the outer lanes and the inner lanes are established respectively using Statistical Program for Social Sciences (SPSS 18.0) for windows. Moreover, based on the theory of saturation flow rate, a new method is proposed to estimate the capacity of single-lane and multi-lane roundabouts using MATLAB software. And then the real data are taken to validate this model. The findings from this research demonstrate that capacity estimates with weaving section vehicles result in improved prediction of the actual capacity of a roundabout.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6957955,no,undetermined,0
Multicriteria decision making with fuzziness and criteria interdependence in cloud service selection,"With the advent of Cloud computing and subsequent big data, online decision makers usually find it difficult to make informed decisions because of the great amount of irrelevant, uncertain, or inaccurate information. In this paper, we explore the application of multicriteria decision-making (MCDM) techniques in the area of Cloud computing and big data, to find an efficient way of dealing with criteria relations and fuzzy knowledge based on a great deal of information. We propose a MCDM framework, which combines the ISM-based and ANP-based techniques, to model the interactive relations between evaluation criteria, and to handle data uncertainties. We present an application of Cloud service selection to prove the efficiency of the proposed framework, in which a user-oriented sigmoid utility function is designed to evaluate the performance of each criterion.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891892,no,undetermined,0
Multifunctional Gateway Sensor Node for agriculture and forest actvities,"The Multifunctional Gateway Sensor Node is an extension of the Gateway sensor node of Wireless Sensor Network having multiple functionalities. Addendum of functionalities comprise different types of technologies which are LCD Color display, keypad to operate Gateway Node, AVR camera and solar panel for additional power support. The node itself also acts as a weather station based on Fuzzy AHP for precise decision support. So the main idea behind this research is to operate WSN without computer. It will be most usable in agriculture, forest monitoring as well as marine securities when pc is unavailable. The location of particular place for weather station feature will be selected by the help of Koppen Classification, instead of GPS. This sensor node is also operable by the specially designed java based software having great intelligence as per the locality. Hence the paper presents the idea about the proposed research area.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915416,no,undetermined,0
Improving Performance of Forensics Investigation with Parallel Coordinates Visual Analytics,"Computer forensics investigators aim to analyse and present facts through the examination of digital evidences in short times. As the volume of suspicious data is becoming large, the difficulties of catching the digital evidence in a legally acceptable time are high. This paper proposes an effective method for reducing investigation time redundancy to achieve the normalization of data on hard disk drives (HDD) for computer forensics. We use visualization techniques, parallel coordinates, to analyse data instead of using data analysis algorithms only, and also choose a Red-Black tree structure to de-duplicate data. It reduces the time complexity, including the time spent of searching data, adding data as well as deleting data. We show the advantages of our approach, moreover, we demonstrate how this method can enhance the efficiency and quality of computer forensics task.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023848,no,undetermined,0
Improved Priority Based Job Scheduling Algorithm in Cloud Computing Using Iterative Method,"Cloud Computing is a platform for computing resources (Hardware and Software) that are delivered as a service over an internet network to the customers. Its intention is to share large scale equipments and resources for computation, storage, information and knowledge for scientific researches. There are many jobs that are required to be executed by the available resources to achieve best performance, minimal total time for completion, shortest response time, utilization of resource usage and etc. Because of these different objectives and high performance of computing environment, we need to design, develop, and propose a scheduling algorithm that outperforms appropriate allocation map of jobs due to different factors. Job scheduling is one of the major issue in cloud computing environment. In job scheduling priority is the biggest issue because some jobs need to be scheduled first then all other remaining jobs which can wait for a long time. In this paper, we have proposed an improvement in priority based job scheduling algorithm in cloud computing which is based on multiple criteria and multiple attribute decision making model.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906024,no,undetermined,0
Improved low cost induction motor control for stand alone solar pumping,"Control strategy for standalone solar pumping system based on induction motor and without DC/DC converter has been widely studied and discussed in the literature. This topology is of great concern due its economic issues, especially when a standard frequency converter (SFCs) with scalar control is used instead of a dedicated PV inverter. This paper proposes an external control module to generate SFCs frequency reference in order to ensure both maximum power point tracking (MPPT) and optimize IM power. It is a low cost solution since it requires no additional power equipment. Modeling and design of each system parts are performed to determine the analytic expression of frequency reference. The effectiveness of the proposed approach is illustrated by simulations carried out under PSIM Software, and validated through experimental investigations on a 1.5kW laboratory set-up.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7076955,no,undetermined,0
Implications of Learning Analytics for Serious Game Design,"This paper addresses the implications of combining learning analytics and serious games for improving game quality, monitoring and assessment of player behavior, gaming performance, game progression, learning goals achievement, and user's appreciation. We introduce two modes of serious games analytics: in-game (real time) analytics, and post-game (off-line) analytics. We also explain the GLEANER framework for in-game analytics and describe a practical example for off-line analytics. We conclude with a brief outlook on future work, highlighting opportunities and challenges towards a solid uptake of SGs in authentic educational and training settings.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901445,no,undetermined,0
From Multiple Linked Views to Multiple Linked Analyses: The Meme Media Digital Dashboard,"We describe a system called the Digital Dashboard that uses multiple linked views of data. All views allow interaction with the visualization results and interaction is done through direct manipulation. The system has been extended to allow new complex data to be generated in analysis components at runtime, e.g. By statistical analysis or data mining of parts of the data. The resulting data can be used in other linked views or analysis components, so when e.g. A data mining parameter is changed, all linked views (or analysis components) are automatically updated as soon as the new calculations are finished, and when something changes in linked components (e.g. A different subset of the data is selected), the calculations are automatically redone (if necessary).",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6902899,no,undetermined,0
Fuzzy AHP based design decision for Product Line architecture,"Product line architecture (PLA)is one of the important assets in Software Product Line (SPL). In PLA there are numerous variation points and variants in which software engineers can decide and select as the best or suitable enough design decision for the PLA. Therefore it's crucial for SPL to have an explicit architecture design decision representation to enable the important knowledge to be reused in several similar applications in the product line.Though there are many proposals towards incorporating design decision in SPL, however it is still lacking in dealing with multiple criteria and fuzziness of the PLA design decision. In this paper, we propose to overcome this problem by incorporating Fuzzy Analytical Hierarchical Process (FAHP) in the design decision. Our approach considers the use of architecturally significant requirements, which is the quality attributes in the design decision. We demonstrate our approach using the case study of Autonomous Mobile Robot Product Line (AMRPL). The findingstend to show a potential use of FAHP in the design decision.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986000,no,undetermined,0
Fuzzy Cognitive Network Process: Comparisons With Fuzzy Analytic Hierarchy Process in New Product Development Strategy,"Fuzzy analytic hierarchy process (F-AHP) has increasingly been applied in many areas. However, as the perception and cognition toward the semantic representation for the linguistic rating scale used by the fuzzy pairwise comparison in F-AHP are still open to discuss, F-AHP very likely produces misapplications. This research proposes the fuzzy cognitive network process (F-CNP) as an ideal alternative to F-AHP. A new product development application using F-AHP is revised using F-CNP. This study shows that the proposed F-CNP yields better results due to the appropriate mathematical definition of the fuzzy paired interval scale for the human perception of paired difference.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6542700,no,undetermined,0
Fuzzy qualitative evaluation of reliability of object oriented software system,"Reliability is the key characteristics of any efficient software system. Most of the software system fails due to no-reliable in nature. Evaluating reliability is one of the main functions of the software system. Since last decades, the popularity of object oriented software system is increased in exponential ways. Many methods or models have been developed for reliability evaluation but few of them took considerations of object oriented features. In this paper, fuzzy approach is used to evaluate reliability. Due to reliable in nature, CK metrics are used in reliability measurement. Based on the case studies performed on object oriented software, it is concluded that we can choose reliable software very easily among various available software system.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012813,no,undetermined,0
Fuzzy-Set Based Sentiment Analysis of Big Social Data,"Computational approaches to social media analytics are largely limited to graph theoretical approaches such as social network analysis (SNA) informed by the social philosophical approach of relational sociology. There are no other unified modelling approaches to social data that integrate the conceptual, formal, software, analytical and empirical realms. In this paper, we first present and discuss a theory and conceptual model of social data. Second, we outline a formal model based on fuzzy set theory and describe the operational semantics of the formal model with a real-world social data example from Facebook. Third, we briefly present and discuss the Social Data Analytics Tool (SODATO) that realizes the conceptual model in software and provisions social data analysis based on the conceptual and formal models. Fourth, we use SODATO to fetch social data from the Facebook wall of a global brand, H&M and conduct a sentiment classification of the posts and comments. Fifth, we analyse the sentiment classifications by constructing crisp as well as the fuzzy sets of the artefacts (posts, comments, likes, and shares). We document and discuss the longitudinal sentiment profiles of artefacts and actors on the facebook page. Sixth and last, we discuss the analytical method and conclude with a discussion of the benefits of set theoretical approaches based on the social philosophical approach of associational sociology.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972052,no,undetermined,0
Gateways to high-perfomance and distributed computing resources for global health challenges,"Computational simulations for disease modeling and efficient analysis tools using large data collections have become invaluable tools for Global Health programs fighting infectious diseases. Simulations are used in many ways e.g., from predicting the effectiveness of interventions for certain diseases through Bayesian-based data-model assimilation to genomic analysis of diverse vector species in their different growth states. Even though the approaches and technologies vary, they have several common requirements on the underlying infrastructure. Simulations for infectious diseases, for example, rely on environmental data like weather, geospatial data, biodiversity and transmission complexity. Data-intensive applications need efficient distributed data management capabilities facilitating replication services or Software-as-a-Service solutions. Such solutions might follow the paradigm to transfer applications to the data instead of transferring data to where the applications are deployed. In this paper we present our work towards providing a common extensible platform to build the computational investigation environment. This platform will provide an API for developers of science gateways, which can be adapted for specific simulations, various distributed data management technologies and diverse data structures. Furthermore, it will include metadata to increase the quality and the information about the data, its provenance and its context. Such an API will ease the development of new science gateways and the core technologies for both modeling and running the models. Developers can focus on the targeted domain and are relieved from re-developing core features for the underlying infrastructure. These gateways also enable the stakeholders (scientists, policy makers, etc.) in using the sophisticated tools and/or offer a single point of entry to large data collections and data analytics tools.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7147530,no,undetermined,0
GE Brilliant wind farms,"Since the Brilliant wind platform was launched in early 2013, GE has developed and commercialized a host of hardware and software features to increase wind farm Annual Energy Production, improve services productivity, and open up new customer revenue streams. By utilizing advanced control algorithms and analytics coupled with deep domain expertise in power electronics and grid integration, GE is creating more with less - getting more power and efficiency out of existing hardware, taking an inherently variable wind resource and integrating it more smoothly onto the grid, managing the complexity of multiple turbines within a farm and multiple farms within a grid system, and making wind predictable and reliable even in areas with weak infrastructure. By harnessing the power of the industrial internet GE will continue to develop innovative features within the Brilliant wind platform, transforming the wind industry as we tackle the next generation of challenges and opportunities.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912227,no,undetermined,0
Giving Text Analytics a Boost,"The amount of textual data has reached a new scale and continues to grow at an unprecedented rate. IBM's SystemT software is a powerful text-analytics system that offers a query-based interface to reveal the valuable information that lies within these mounds of data. However, traditional server architectures are not capable of analyzing so-called big data efficiently, despite the high memory bandwidth that is available. The authors show that by using a streaming hardware accelerator implemented in reconfigurable logic, the throughput rates of the SystemT's information extraction queries can be improved by an order of magnitude. They also show how such a system can be deployed by extending SystemT's existing compilation flow and by using a multithreaded communication interface that can efficiently use the accelerator's bandwidth.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6871704,no,undetermined,0
GPFS-based implementation of a hyperconverged system for software defined infrastructure,"The need for an increasingly dynamic and more cost-efficient data-center infrastructure has led to the adoption of a software defined model that is characterized by: the creation of a federated control plane to judiciously allocate and control appropriate heterogeneous infrastructure resources in an automated fashion, the ability for applications to specify criteria, such as performance, capacity, and service levels, without detailed knowledge of the underlying infrastructure; and the migration of data-plane capabilities previously embodied as purpose-built devices or firmware into software running on a standard operating systems in commercial off-the-shelf servers. This last trend of hardware-based capabilities migrating to software is enabling yet another shift to hyperconvergence, which refers to merger of traditionally separate networking, compute, and storage capabilities in integrated system software. This paper examines the convergence of the software defined infrastructure stack, and introduces a hyperconverged compute and storage architecture, in which the IBM General Parallel File System (GPFSŒ¬) implements the software defined data plane that dynamically supports workloads ranging from high-I/O virtual desktop infrastructure applications to more compute-oriented analytics applications. The performance and scalability characteristics of this architecture are evaluated with a prototype implementation.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6798737,no,undetermined,0
Handling Big Data in medical imaging: Iterative reconstruction with large-scale automated parallel computation,"The primary goal of this project is to implement the iterative statistical image reconstruction algorithm, in this case maximum likelihood expectation maximum (MLEM) used for dynamic cardiac single photon emission computed tomography, on Spark/GraphX. This involves porting the algorithm to run on large-scale parallel computing systems. Spark is an easy-toprogram software platform that can handle large amounts of data in parallel. GraphX is a graph analytic system running on top of Spark to handle graph and sparse linear algebra operations in parallel. The main advantage of implementing MLEM algorithm in Spark/GraphX is that it allows users to parallelize such computation without any expertise in parallel computing or prior knowledge in computer science. In this paper we demonstrate a successful implementation of MLEM in Spark/GraphX and present the performance gains with the goal to eventually make it useable in clinical setting.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7430758,no,undetermined,0
Hardware Partitioning for Big Data Analytics,"Targeted deployment of hardware accelerators can improve the throughput and energy efficiency of large-scale data processing. Data partitioning is a critical operation for manipulating large datasets and is often the limiting factor in database performance. A hardware-software streaming framework offers a seamless execution environment for streaming accelerators such as the Hardware-Accelerated Range Partitioner (HARP). Together, the streaming framework and HARP provide an order of magnitude improvement in partitioning and energy performance.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6762799,no,undetermined,0
Hardware-accelerated text analytics,Presents a collection of slides covering the following topics: SystemT text analytics software; hardware-accelerated SystemT; Big text Data; and field programmable gate array.,2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478822,no,undetermined,0
Hierarchical management of large-scale malware data,"As the pace of generation of new malware accelerates, clustering and classifying newly discovered malware requires new approaches to data management. We describe our Big Data approach to managing malware to support effective and efficient malware analysis on large and rapidly evolving sets of malware. The key element of our approach is a hierarchical organization of the malware, which organizes malware into families, maintains a rich description of the relationships between malware, and facilitates efficient online analysis of new malware as they are discovered. Using clustering evaluation metrics, we show that our system discovers malware families comparable to those produced by traditional hierarchical clustering algorithms, while scaling much better with the size of the data set. We also show the flexibility of our system as it relates to substituting various data representations, methods of comparing malware binaries, clustering algorithms, and other factors. Our approach will enable malware analysts and investigators to quickly understand and quantify changes in the global malware ecosystem.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004290,no,undetermined,0
High Performance and Fault Tolerant Distributed File System for Big Data Storage and Processing Using Hadoop,Hadoop is a quickly budding ecosystem of components based on Google's MapReduce algorithm and file system work for implementing MapReduce algorithms in a scalable fashion and distributed on commodity hardware. Hadoop enables users to store and process large volumes of data and analyse it in ways not previously possible with SQL-based approaches or less scalable solutions. Remarkable improvements in conventional compute and storage resources help make Hadoop clusters feasible for most organizations. This paper begins with the discussion of Big Data evolution and the future of Big Data based on Gartner's Hype Cycle. We have explained how Hadoop Distributed File System (HDFS) works and its architecture with suitable illustration. Hadoop's MapReduce paradigm for distributing a task across multiple nodes in Hadoop is discussed with sample data sets. The working of MapReduce and HDFS when they are put all together is discussed. Finally the paper ends with a discussion on Big Data Hadoop sample use cases which shows how enterprises can gain a competitive benefit by being early adopters of big data analytics.,2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6965006,no,undetermined,0
High Precision Screening for Android Malware with Dimensionality Reduction,"We present a new method of classifying previously unseen Android applications as malware or benign. The algorithm starts with a large set of features: the frequencies of all possible n-byte sequences in the application's byte code. Principal components analysis is applied to that frequency matrix in order to reduce it to a low-dimensional representation, which is then fed into any of several classification algorithms. We utilize the implicitly restarted Lanczos bidiagonalization algorithm and exploit the sparsity of the n-gram frequency matrix in order to efficiently compute the low-dimensional representation. When trained upon that low-dimensional representation, several classification algorithms achieve higher accuracy than previous work.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033086,no,undetermined,0
High-frequency financial statistics with parallel R and Intel Xeon Phi coprocessor,"Financial statistics covers a wide array of applications in the financial world, such as (high frequency) trading, risk management, pricing and valuation of securities and derivatives, and various business and economic analytics. Portfolio allocation is one of the most important problems in financial risk management. One most challenging part in portfolio allocation is the tremendous amount of data and the optimization procedures that require computing power beyond the currently available desktop systems. In this article, we focus on the portfolio allocation problem using high-frequency financial data, and propose a hybrid parallelization solution to carry out efficient asset allocations in a large portfolio via intra-day high-frequency data. We exploit a variety of HPC techniques, including parallel R, Intel Math Kernel Library, and automatic offloading to Intel Xeon Phi coprocessor in particular to speed up the simulation and optimization procedures in our statistical investigations. Our numerical studies are based on high-frequency price data on stocks traded in New York Stock Exchange in 2011. The analysis results show that portfolios constructed using high-frequency approach generally perform well by pooling together the strengths of regularization and estimation from a risk management perspective. We also investigate the computation aspects of large-scale multiple hypothesis testing for time series data. Using a combination of software and hardware parallelism, we demonstrate a high level of performance on high-frequency financial statistics.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004414,no,undetermined,0
iCARE: A framework for big data-based banking customer analytics,"The amount of data stored by banks is rapidly increasing and provides the opportunity for banks to conduct predictive analytics and enhance its businesses. However, data scientists are facing large challenges, handling the massive amount of data efficiently and generating insights with real business value. In this paper, the Intelligent Customer Analytics for Recognition and Exploration (iCARE) framework is presented to analyze banking customer behaviors from banking big data, through analytical modeling methodologies and techniques designed for a key business scenario. Combining IBM software platforms and big data processing power with customized data analytical models, the iCARE solution provides deeper customer insights to satisfy a bank's specific business need and data environment. The advantages of the iCARE framework have been confirmed in a real case study of a bank in southeast China. In this case, iCARE helps generate insights for active customers based on their transaction behavior, using close to 20 terabytes of data.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964895,no,undetermined,0
Implementation of a cognitive radar perception/action cycle,"Herein we demonstrate that a cognitive capability can be readily added to modern digital radars. Such radars already consist of adaptive electronically scanned array front ends (often with built-in on-array processing) driven by back ends composed of modern flexible signal/data processor clusters plus receiver-exciters with arbitrary waveform generation capability. Using these as building blocks, a cognitive cycle can be readily constructed. The cognitive paradigm uses these existing hardware capabilities common to modern radars but implemented with new drivers; specifically a cognitive software overlay cycle linking advanced near-real-time analytic techniques. The authors believe that this investigation, which focused on practical existing implementations of the component algorithms to produce a straightforward cognitive cycle, will help convince the radar community that the Cognitive Radar paradigm merits further investigation.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875651,no,undetermined,0
Implementation of continuous integration and automated testing in software development of smart grid scheduling support system,"When smart grid scheduling support system (D5000 system) was developed, the development team ran across tough issue that the system is difficult for integration and becomes unstable after integration due to the complexity. To resolve the problem, the author made research and introduced continuous integration and automated testing approach on D5000 system development. This paper provides the concept and advantages of continuous integration, and analyzes the necessity for continuous integration. It also describes automated testing for quality improvement with code static analytics, automated unit testing, and automated function testing; This paper gives a case study to deploy continuous integration and automated testing on D5000 system development which resolves quality and integration issues effectively and efficiently.",2014,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6993503,no,undetermined,0
Evaluating the Trust of Android Applications through an Adaptive and Distributed Multi-criteria Approach,"New generation mobile devices, and their app stores, lack of a methodology to associate a level of trust to applications to faithfully represent their potential security risks. This problem is even more critical with newly published applications, for which either user reviews are missing or the number of downloads is still low. In this scenario, users may not fully estimate the risk associated with downloading apps found on on-line stores. Hence, here we propose a methodology for evaluating the trust level of an application through an adaptive, flexible, and dynamic framework. The evaluation of an application trust is performed using both static and dynamic parameters, which consider the application meta-data, its run-time behavior and the reports of users with respect to the software critical operations. We have validated the proposed approach by testing it on more than 180 real applications found both on official and unofficial markets by showing that it correctly categorizes applications as trusted or untrusted in 94% of the cases and it is resilient to poisoning attacks.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681013,no,undetermined,0
Evaluating the credibility of the C3I information network simulation based on fuzzy AHP,"The evaluation of a simulation model's credibility is a hot issue in present research of simulation. Furthermore, the credibility evaluation on the information network simulation of the command_control equipment is an important part to build and evaluate the information network's simulation model through simulation experiment. also a critical challenge for us to evaluate the credibility of command-control equipment information network simulation. On the basis of the analysis of the command-control equipment information network's system structure, this paper establishes a half-real simulation model of a command-control equipment network, employs the Analytical Hierarchy Process to build up the indicator set that affect the evaluation of the simulation credibility, and further figures out the weight of each indicator, then upbuilds a grey model of evaluation on simulation's credibility on the ground of putting forward positive, negative ideal comparatively standard, to make a synthetic evaluation and analysis of its simulation credibility. Finally, the method's feasibility and validity are verified by citing and analyzing some practical examples.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615485,no,undetermined,0
Evaluating Security of Software Components Using Analytic Network Process,"Increasing use of Component Based Software Engineering (CBSE) has raised the issues related with the security of software components. Several methodologies are being used to evaluate security of software components and that of the base system with which it is integrated. Security characteristics of a component must be specified effectively and unambiguously. To make possible software development progression, it will be effective to have a method which evaluates the security of software components. The study presented here attempts to propose analytic network process (ANP) for component security evaluation. The method is applied using ISO/IEC 27002 (ISO 27002) standard.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6717250,no,undetermined,0
Safety-driven software reliability allocation in medical device application,"Software reliability allocation deals with setting reliability objectives and allocating testing resources for software components or sub-systems. The whole allocation process is complicated as many factors, such as cost, complexity, maintainability, and etc, are included for consideration and there are many uncertainty resources. In this paper, a simplified approach is presented. The approach starts from safety features which are the top concerns in medical device applications; allocates reliability to components associated with safety features via utilizing analytic hierarchy process and fault tree analysis; and finalizes the allocation by combining the reliability contribution from all safety features. The approach makes some reasonable approximations to simplify the allocation process and resolve the problem fast and more efficiently. The allocation result is conservative and practicable in medical device applications.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976578,no,undetermined,0
Research on Goat Health Management System,"A goat health management system is presented in this paper, which can manage goat disease from each stage including goat file creation, routine monitoring, disease prevention and control. Integrate electronic medical records was set up, which based on medical records include goat basic information and routine monitoring results and disease prevention information. It can implement statistical analytic function of disease rate and guide goat immunization. This system includes four subsystems, goat basic information management subsystem, goat individual health monitoring and evaluation subsystem, goat electronic medical records subsystem and goat disease prevention and control subsystem. With the help of system analysis and software design techniques, it can manage goat disease on goat farm effectually.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5873323,no,undetermined,0
Research on performance management of service firm based on BSC and F-ANP,"The feature of the service firm are highly opening, substantial interactions among customers and companies. Hence, there will be flaws if only evaluate the performance from classical financial indicators. As an instrument for performance evaluation, Balanced Scorecard (BSC) generally evaluate the performance from four dimensions, which are financial condition, customer service, internal business processes and learning and growth. Fuzzy Analytic Network Process (F-ANP) is a quantitative method which can deal with complicated decision-making problem. By introducing BSC together with F-ANP to the performance evaluation of service firm, this performance evaluation model will have functions of determining the weight of each index reasonably and improving the effectiveness of BSC in the application of the performance evaluation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014619,no,undetermined,0
Research on safety monitoring and evaluation system of dangerous goods transportation,"The micro controller with the LPC2294 and ARM7TDMI-S kernel is designed to realize the realtime monitoring of the vehicles, meanwhile, the system software of safety monitoring and evaluation of dangerous goods transportation is developed based on Microsoft Visual web developer, ASP.NET and Microsoft SQL Server. The three levels index system about evaluation model of dangerous goods transportation was established based on experts' survey and research. The analytic hierarchy process (AHP) was use to structure jugging matrix which can ascertain the weights of each level evaluation index. Dynamic and static indexes were quantitated respectively in different ways, and then to realize the technology integration between them. Finally, the Fuzzy Comprehensive Evaluation method is used to evaluate the dangerous goods transportation corporations and then obtain the security level of those corporations. Facts have shown that this monitoring and Evaluation system is applicable and effective, and it can be used in the safety monitoring and evaluation of existing dangerous goods transportation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199585,no,undetermined,0
Research on selection of third-party logistics enterprise based on the extenics,"Facing the increasing development of third-party logistics, a cargo owner should set a systematic approach to choose the right logistics enterprises. In this paper, an evaluate index system of third-party logistics enterprise was built up, which originates from balanced scorecard's four dimensions. After introducing the concept of extension theory, this paper constructs an evaluation model which combined extension theory with fuzzy analytic hierarchy process. At last, a case study was given to validate this method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014432,no,undetermined,0
Research on the Performance Indicators of civil aviation E-government basic on IT governance,"With the deeper construction of civil aviation informatization, the development and construction of the new system of civil aviation E-government in modern information society and civil aviation, will meet the development needs of civil aviation and economic society in post-industrial era. This paper studies based on the status of civil aviation government and the design principle of performance indicator, use analytic hierarchy process, and puts forward a performance indicator include 35 index, such as degree of government affairs information digitization etc. Then we calculate the weight and the normalized processing by using yaahp5.0 software, and give a comprehensive evaluation plan at last. The evaluation result of civil aviation E-government has positive and effective simulative effect to improve civil aviation E-government affairs system efficiency.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6010478,no,undetermined,0
Research on the roller feed track during spinning of hollow part with triangle cross-section,"The roller feed track during spinning of hollow part with straight edge triangle cross-section was researched. The variation characteristic of the contact point between the roller and workpiece was obtained: the contact point moves not only along the axial direction caused by the roller axial feed, but also along the radial, circumferential and axial directions caused by the non-circular cross-section profile of workpiece. Based on the variation characteristic of the contact point, a graphical analytic method for the calculation of roller radial feed movement was put forward. The software MSC.ADAMS was adopted to verify the calculation results, the tolerance range is [-0.09mm, +0.09mm], it indicates that the calculation method is reasonable and can be used to calculate the roller feed track during spinning of hollow parts with various non-circular cross-section.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5987008,no,undetermined,0
S-preconditioner for Multi-fold Data Reduction with Guaranteed User-Controlled Accuracy,"The growing gap between the massive amounts of data generated by petascale scientific simulation codes and the capability of system hardware and software to effectively analyze this data necessitates data reduction. Yet, the increasing data complexity challenges most, if not all, of the existing data compression methods. In fact, lossless compression techniques offer no more than 10% reduction on scientific data that we have experience with, which is widely regarded as effectively incompressible. To bridge this gap, in this paper, we advocate a transformative strategy that enables fast, accurate, and multi-fold reduction of double-precision floating-point scientific data. The intuition behind our method is inspired by an effective use of preconditioners for linear algebra solvers optimized for a particular class of computational ""dwarfs"" (e.g., dense or sparse matrices). Focusing on a commonly used multi-resolution wavelet compression technique as the underlying ""solver"" for data reduction we propose the S-preconditioner, which transforms scientific data into a form with high global regularity to ensure a significant decrease in the number of wavelet coefficients stored for a segment of data. Combined with the subsequent EQ-calibrator, our resultant method (called S-Preconditioned EQ-Calibrated Wavelets (SPEQC-Wavelets)), robustly achieved a 4- to 5-fold data reduction-while guaranteeing user-defined accuracy of reconstructed data to be within 1% point-by-point relative error, lower than 0.01 Normalized RMSE, and higher than 0.99 Pearson Correlation. In this paper, we show the results we obtained by testing our method on six petascale simulation codes including fusion, combustion, climate, astrophysics, and subsurface groundwater in addition to 13 publicly available scientific datasets. We also demonstrate that application-driven data mining tasks performed on decompressed variables or their derived quantities produce results of comparable quality with the ones for- the original data.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137233,no,undetermined,0
SaaS Vendor Selection Basing on Analytic Hierarchy Process,"Analytic hierarchy process is used to select the best SaaS vendor for enterprises. By constructing the hierarchy model, analyzing the attributes and calculating the attribute values, the decision problem with multi-objectives in selecting the best SaaS vendor for enterprises is effectively solved. An example is utilized to show the calculating process. Calculative result shows that the SaaS vendor with the highest score by the method is the best choice and the analytic hierarchy process is an effective approach for enterprises to choose the best SaaS vendor.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5957714,no,undetermined,0
Safety evaluation study on groundwater for drinking in Yinchuan Plain,"Due to exposed to serious pollution and influence of natural conditions, drinking water safety has become a hotspot issue drawing high attention from the state and government. Yinchuang Plain is in serious quality-induced water shortage. Through investigation and analysis, this paper established a drinking water safety evaluation system from the perspective of headwaters and human health, studying quality and quantity of groundwater, as well as geological protective performance; safety evaluation on groundwater exploration for drinking in Yinchuan Plain was conducted by adopting analytic hierarchy process and overlay and index methods with the help of GIS software platform. The evaluation result has practical significance for groundwater resource protection and groundwater safety supply.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5893018,no,undetermined,0
Scattering Simulation and Reconstruction of a 3-D Complex Target Using Downward-Looking Step-Frequency Radar,"Numerical simulations of polarized scattering, 3-D imaging, and profile reconstruction of a complex-shaped electric-large target above a rough surface are developed. The bidirectional analytic ray tracing method is first applied to calculation of polarized scattering from volumetric target and underlying surface. By using the step-frequency radar working in downward-looking spotlight mode and moving within a 2-D circular arc aperture, a 3-D matrix of backscattering fields in both the amplitude and phase is obtained. The 3-D fast Fourier transform algorithm is adopted for uniformly resampling data, which are acquired by interpolating the collected uniformly sampling backscattering fields to quickly form a focused image. Automatic reconstruction of the target is then well demonstrated. As validation and comparison, the scattering fields are also computed and compared using widely accepted software FEKO based on physical optics. The technique of imaging and reconstruction for a 3-D complex-shaped perfect electric conductor electric-large target, such as a tank-like object, is presented.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5764516,no,undetermined,0
System construction of comprehensive occupational abilities of engineering students in junior college,"Education of junior college is employment-oriented and good occupational ability training is very important for students' future work and career development after graduation. Using group analytic hierarchy process (Group-AHP), a model for evaluating the comprehensive occupational abilities of engineering students in junior college has been built in this study and this model is composed of objective level, criteria level and index level. Criteria level includes 3 factors of basic capacity, major skill and key competency. Index level includes 14 indices: work ethics, judgment and discrimination, teamwork, humanity, language performance, computer operation, occupational position knowledge, technical proficiency, practical operating ability, application ability of new materials, processes and techniques as well as new facilities, interpersonal communication, psychological endurance, organizational managing ability, development and innovation, etc. The results have shown that, among the comprehensive occupational abilities of students, interpersonal communication, practical operation ability, teamwork, etc. have the largest weights, which is consistent with the teaching objective of junior college towards engineering students. Based on this model, grades of comprehensive occupational abilities of students could be calculated scientifically to provide the basis for improving the teaching quality, model and methods of college.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013783,no,undetermined,0
Security considerations in data center configuration management,"Data centers need to manage a large amount of configuration information for a variety of computational, storage and networking assets at multiple levels (e.g., individual devices to entire data center). The increasingly sophisticated configuration management required to support virtualization significantly enhances chances of misconfigurations and exploitation by hackers that could impact data center operations. In this paper, we expose a number of attack/misconfiguration scenarios for data center resources. We also propose a mechanism, called Sentry, for securing this data by exploiting the hierarchical setup and redundancy in the server and network configurations. We show that the scheme can secure configuration data with only a small overhead.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111676,no,undetermined,0
Security infrastructure for commercial and military ports,"Battelle has been developing robust detection and scanning technologies that integrate with existing and future port security systems as part of overall integrated security networks that can be tailored to meet individual requirements for commercial ports or Navy facilities. Major challenges include the water side issues such as geography, bathymetry, channel dimensions, changing bottom topology, environmental conditions, operation under all conditions and sea states suitable for vessel and small craft operation, harbor arrangement, and port-specific unique threats. However, many port security systems develop and implement new sensor technologies rather than assess the overall security need-that is, they follow a technology push rather than a market pull approach. While stated security approaches may integrate sensor data, stored data, intelligence databases, and human observations in various forms and formats, they generally do not provide the port-specific vulnerability and threat information required to alert security personnel with an integrated, composite threat analysis. Most approaches depend upon real-time person-in-the-loop detection strategies that can falter when faced with real world constraints: limited personnel resources, human fatigue and error, distractions, and misinterpretation of anomalies. Statistical and probabilistic analyses for anomaly identification are necessary to provide viable potential threat solution sets. Starlightä‹¢, a Battelle developed software tool, combines information from multiple sources and multiple formats into a single information management and analysis composite to identify anomalies and threats. The system rapidly integrates text, network, geospatial, and temporal data, and then processes the disparate information to provide one cohesive threat solution to alert security staff. This synergistic approach enables multiple data base and information formats to be addressed holistically instead of through stovepipes,- giving the ability to uncover a threat pattern within the otherwise overwhelming amount of information. Starlightä‹¢ differs from traditional systems by incorporating a range of processing algorithms to analyze individual information streams and form a composite threat solution. Current applications include intelligence gathering, network intrusion, entity extraction, law enforcement, visual intelligence (VISINT) processing, and threat anticipation. The overall strategy is the identification of appropriate sensors for integration into multiple real time data sets that combine sensor data with stored data, intelligence information, and human observations in various forms and formats. Individual data bases and sensor streams are processed with algorithms appropriate for their content and format. Data fusion, processing, statistical and probabilistic criterion provide synergistic solution sets that are ranked to identify threats and to select appropriate responses ranging from nonlethal deterrents to disabling force. Implementation strategies assimilate data and information necessary to process and identify risks compatible with and supportive of security personnel functions. Critical implementation requirements for adding or expanding port security systems include minimal impact to vessel traffic, minimal real-time human monitoring; minimizing false positives and false negatives; calibration; and flexibility to be adapted, expanded, and integrated for port-specific needs. In-water installation issues include secure anchoring and safety, bio-fouling protection, corrosion protection, reliability, availability, ease of maintenance, and life cycle costs. Implementation is accommodated by a spiral building block approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107174,no,undetermined,0
SEGrapher: Visualization-based SELinux policy analysis,"Performing SELinux policy analyses can be difficult due to the complexity of the policy language and the sheer number of policy rules and attributes involved. For example, the default policy on most SELinux-enabled systems has over 1; 500; 000 flat rules, involving over 1; 780 types. Simple analyses between types can result in a large amount of data, which is poorly presented to administrators in existing analysis tools. We propose and implement a policy analysis tool äóìSEGrapheräó that addresses the above challenges. SEGrapher visually presents analysis results as a simplified directed graph, where nodes are types, and edges are corresponding policy rules between types. Graphs are generated via a proposed clustering algorithm that clusters types based on their accesses. Clusters provide an abstraction layer that removes undesired data, and focuses on analysis attributes specified by the administrator.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111675,no,undetermined,0
Simulation of motion artifacts in offset flat-panel cone-beam CT,"Cone-beam computed tomography (CBCT) with a tangentially offset flat-panel detector is used for target imaging in image-guided radiotherapy and for localization and attenuation correction in SPECT/CT imaging. The offset-detector geometry offers a large field-of-view for a given detector size. Data are acquired on a circular trajectory over 360 degrees, with gantry rotation times ranging from 12 s to 60 s and longer on commercially available systems. These acquisition times, much longer than in conventional CT, make flat-panel CBCT more susceptible to motion artifacts from voluntary or involuntary patient motion. Here, we present phantom studies on motion artifacts in flat-panel CBCT, with the goal to facilitate better artifact assessment on clinical cases and to provide a basis for future investigations into artifact mitigation methods. Software phantoms for five typically observed motion types were simulated; phantom reconstructions are presented and compared to clinical cases. In addition, three analytic and iterative reconstruction methods were evaluated with respect to their performance on motion-affected data.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152670,no,undetermined,0
Simulation standard for business process management,"Simulation is considered a key component for business process management suites. Within business process management, simulation can be readily used for both process design, and ongoing improvement. Despite the predictive capabilities of simulation, the lack of wide scale adoption within business process management compared with what might be expected suggests that more can be done to better integrate, and use, simulation with business process management suites. While mature standards exist for the definitions of the business processes, there is a lack of standards for defining business process simulation parameters. This paper describes the current challenges when using simulation for business process management, and shows how a standard for defining business process simulation scenarios would help organizations implementing business process management suites leverage the prescriptive power of simulation. The components of such a standard and how this standard might be extended into a complete process analytics framework will be presented.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147801,no,undetermined,0
Study on ARM-based fire alarm networking unit,"An ARM-based fire alarm unit was presented in the paper. The unit hardware was composed of temperature, smog sensors, LPC2478 and GPRS transmission. The unit software was composed of ‘_C/OS - II embedded operation system, data acquisition, the fire identification algorithms and transmission programs. The unit was able to monitor and analyze the room's temperature and smog, and then the analytic result was transmitted to the server by the GPRS wireless module. The comprehensive fuzzy similar algorithm was used to identify the fire and the reliability of alarm was improved effectively.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038014,no,undetermined,0
Study on performance of non-isolated double time slots algorithm of conflict resolution,"It is analyzed and studied non-isolated double time slots algorithm of conflict resolution. The average resolution times analytic expression, resolution efficiency analytic expression and the average resolution delay analytic expression of conflict packet are exlicitly obtained. Furthermore, some exiting problems of doubel time slots algorithm are discussed and some measures of improve performance of algorithm are given.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037069,no,undetermined,0
Study on pid parameters tuning method based on Matlab/Simulink,"Enormous calculation of proportional-integral-derivative (PID) controller parameters tuning with analytic methods is an important problem demanding prompt solution. Parameters tuning based on Matlab/Simulink is simplicity, visual manipulation method which leaves out above program. According to the Ziegler-Nichols (Z-N) method, this paper introduces how to reduce and validate the PID controller parameter with the help of MATLAB tool taking a certain control model as an example. The simulation results show the effectiveness of this method and can be fit for application in the engineering.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014596,no,undetermined,0
Study on the wind energy resources assessment in wind power generation,"The wind energy resources assessment refers to a way to confirm the possibility of establishing a wind electric field based on the observation data on wind speed and direction through wind test station, as well as the data analysis and process for related data got from the local meteorological office long-range observation. At present, there has no universal wind energy analytic software for wind electric field in the domestic market, either few correlated software exist in the global market. And the RisoWA<sup>S</sup>P (Wind Atlas Analysis and Application Program) developed by Denmark Laboratory is a common use for wind energy resources assessment. This article analyzed both the advantages and disadvantages of the RisoWA<sup>S</sup>P software and some powerful software in china, and together with the comprehensive reference to the advantages of other assessment software, it concluded with a relative completed assessment for wind energy resources. In addition, with a view to such characteristics as a large data process and graphic making work in the wind energy resources assessment, this article put forward a concept to develop a new wind energy resources assessment software based on both the VB and ORACLE, which results from the wind energy resources analysis according to such integrated advantages as friendly interface of VB development software, strong diagram service and data.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009905,no,undetermined,0
Research on fuzzy evaluation of teaching practice based on AHP: äóî An example of evaluating curriculum design performance in computer major,"Evaluating performance of practice course is a difficult problem in the process of teaching. On the basis of analyzing the existing fuzzy evaluation models, this paper establishes a fuzzy comprehensive evaluation model based on analytic hierarchy process (AHP). Three-level index evaluation system is set up combining the characteristics of curriculum design in computer major. Relative and comprehensive weights of indexes have been determined using AHP method. And membership degree function is designed in order to transform index score into a fuzzy vector, thus the fuzzy evaluation model can be established. This model reduces the subjectivity which giving a mark accurately brings and has been already applied in teaching.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014193,no,undetermined,0
Research on Evaluating and Selecting of Suppliers Based on AHP in Supply Chain Design,"Supplier quality determines the quality of products offered, and how to quickly and effectively select high-quality suppliers is one of the core issues facing enterprise. From the perspective of the supply chain, this article proposes the theoretical model of the supplier evaluation and selection, establishes a index system of comprehensive evaluation, and applies Analytic Hierarchical Process (AHP) to built the decision model of supplier evaluation and selection. In the final analysis, the feasibility and scientific validity of this system are verified through the simulation analysis based on Simulink module in Matlab software, and the practical example.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6006423,no,undetermined,0
Research on Chinese Services Outsourcing Vendors' Capabilities Based on Fuzzy Comprehensive Evaluation,"This paper establishes index system of Chinese services outsourcing vendors' capabilities, uses analytic hierarchy process to determine the weight of each level index, establishes the fuzzy comprehensive evaluation model of Chinese services outsourcing vendors' capabilities, finally the case study shows that it is reasonable and credible to evaluate Chinese services outsourcing vendors' capabilities with analytic hierarchy process and fuzzy comprehensive evaluation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5998719,no,undetermined,0
Research on auto-filling hole based on mesh information of car body panel,"In terms of the feature of the Self-Piercing Riveting process program, the evaluating index system for the process is created. The weighting aggregate is created with the analytic hierarchy process.The correctness of the weighting aggregate is tested by test results, Selecting the right test program basede on the value of weighting aggregate and synthetical importance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6025548,no,undetermined,0
Multi-elements Decision-making method based on grey decision model and analytic hierarchy process,"Multi-elements Decision-making is one of the important methods which simulate reality problems in both social and economic fields. By using grey decision model and analytic hierarchy process, it probes a newly method of multi-elements decision-making. And by using the software of Matlab and Excel, it also develops a process of optimization sorting algorithm which could describe and calculate the actual problem accurately. The multi-element decision-making method proposed is feasibility and available with high practical value. Base on selecting and sorting multi-elements in decision-making, the method could be used in the design project selection, policy analysis, decision forecast.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5965015,no,undetermined,0
Multiple non-functional criteria service selection based on user preferences and decision strategies,"In order to choose from a list of functionally similar services, users often need to make their decisions based on multiple non-functional criteria they require on the target service. It is a natural fit to apply the Multi-Criteria Decision Making (MCDM) theory to this selection problem. However, the high demand of MCDM approaches on user expertise and user involvement could become an obstacle of using them for service selection. In this paper, we address this issue by taking a user-centric standpoint to design the non-functional criteria based service selection system. On one hand, we try to reduce the workload and the skill level requirement on users. On the other hand, we still give them the flexibility to define the necessary information, which include their preferences on multiple criteria, as well as the decision strategies they would follow to select the desired services from a list of alternatives. The former is crucial for optimal decision making. The latter is often ignored by most of the service selection systems and a common default selection strategy is to find a service which has the best overall score calculated by a certain formula. In reality, users may not necessarily follow this strategy and there are many other possible strategies they may follow. We should take this into consideration when designing selection systems. We use a case study to show that our system could produce a more accurate and customized result for individual users.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6166200,no,undetermined,0
Never Die Network Extended with Cognitive Wireless Network for Disaster Information System,"In the actual disaster case, there is a certain possibility that electric power line is damaged and power energy cannot be supplied to those communication network devices, and eventually those wireless LANs cannot be functioned. Although mobile wireless network is easy to reconstruct than wired network, there may be the case that network disconnection is not affordable after disaster. That is, Disaster Information System needs a robust Never Die Network (NDN) which will be unaffected by any changes in environment after severe disaster. Satellite Network System is one of possible solution for such a severe disaster, but it has some problems like low throughput, large latency, high cost, and so on. On the other hands, single wireless communication like IEEE802.11a/b/g also have some problems like a possible transmission distance, throughput limitation for maintaining QoS for urgent user's situations. Therefore, NDN needs to consider about additional functions for these problems of Satellite and Wireless Network System. In this paper, we introduce Satellite System for optimal transmission control method in Cognitive Wireless Network in order to consider with severe disaster. First, as our previous study, proper wireless link and route selection is held by Extend AHP and Extend AODV with Min-Max AHP value methods for optimal transmission control in Cognitive Wireless Network. Then, check-alive function, alternate data transmission function, possible alternative route suggestion, and network reconfiguration are introduced to our proposed Disaster Information Network by using Satellite System. In the simulation, ns2 are used for the computational results to the effectiveness of the suggested transmission methods in the hybrid system of cognitive wireless and satellite network system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989014,no,undetermined,0
New analytic results for the incomplete Toronto function and incomplete Lipschitz-Hankel Integrals,"This paper provides novel analytic expressions for the incomplete Toronto function, T<sub>B</sub>(m, n, r), and the incomplete Lipschitz-Hankel Integrals of the modified Bessel function of the first kind, Ie<sub>‘_, n</sub>(a, z). These expressions are expressed in closed-form and are valid for the case that m äŠ« n and n being an odd multiple of 1/2, i.e. n Œ± 0.5 äšš ä‹´ Capitalizing on these, tight upper and lower bounds are subsequently proposed for both T<sub>B</sub>(m, n, r) function and Ie<sub>‘_, n</sub>(a, z) integrals. Importantly, all new representations are expressed in closed-form whilst the proposed bounds are shown to be rather tight. To this effect, they can be effectively exploited in various analytical studies related to wireless communication theory. Indicative applications include, among others, the performance evaluation of digital communications over fading channels and the information-theoretic analysis of multiple-input multiple-output systems.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6169356,no,undetermined,0
New approach for application architecture adequacy in hardware/software embedded system design,"This paper deals with a new approach to minimize the gap between application development and the architecture synthesis, in the hardware/software embedded system design flow. Indeed, two intermediate models are proposed, the analytic H<sub>model</sub>, and the MAC<sub>Builder</sub> environment. The first one allows embedded system modelling with a state space approach in order to determine the recursive equations, whereas, the second one builds a set of task graph that models the system functionalities. Further, this paper investigates how a similar approach can be applied to improve the controller architecture design through a case study.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5948913,no,undetermined,0
Non-linear compensation algorithm of LOS locating in aerial remote sensor,"A non-linear light-of-sight(LOS) locating compensation algorithm for certain aerial remote sensor is presented in this paper. The LOS locating system is composed of two main components, which are an inertial reference frame to provide coarse LOS locating and a 2-axis Fast Steering Mirror to remove the frame's residual error. The azimuth and elevation of LOS are both affected by the FSM two-degree-of-freedom rotation, which should be dealt with to raise the accuracy of LOS location. This paper extrapolates LOS locating algorithm of FSM by coordinate transformation, and gives corresponding deviation analysis for different LOS poiting angles. It is found that the angle deviation grows rapidly with range of FSM's travel enlarging, so the deviation must be eliminated. Through the data analysis for deviation, a non-linear compensation algorithm is proved effective to reduce the error of LOS significantly. Taking LOS changing in [-3 deg, 3 deg] as an example, two analytic expressions are given, and the LOS locating accuracy rises by 34 times compared with non-linear compensation unused, and the cross coupling of LOS loating is 0.08%.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6023539,no,undetermined,0
Notice of Retraction<BR>Assessment of the logistics service supply Chain's coordination performance based on ANP,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>The index system of logistics service supply chain's coordination performance is constructed from four aspects: service flow coordination, information flow coordination, capital flow coordination and work flow coordination, considering the importance of the coordination in the logistics service supply chain. The ANP is used to build the assessment model. The model is solved by adopting the Super Decision software, with the example, to get the final results of the coordination performance assessment.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5881285,no,undetermined,0
Notice of Retraction<BR>Study and application of optimization design scheme of roadway bolt support parameter,"Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>According to the incomplete clear mechanism of bolt support action presently, the bolt support parameter of 12910 face haulage roadway is given optimized design in HANDAN TAOYI mine by the analytic system software: ANSYS, using theory analysis, numerical simulation and engineering actual measurement. The surrounding rock failure region of the optimized roadway is 12.54m<sup>2</sup>, decreased by 31.0% compared to present bolt support roadway, which has theoretical significance and practical value in popularizing bolt support technique and ensuring the support roadway safe and reliable.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777514,no,undetermined,0
Optimal Model-Based Policies for Component Migration of Mobile Cloud Services,"Two recent trends are major motivators for service component migration: the upcoming use of cloud-based services and the increasing number of mobile users accessing Internet-based services via wireless networks. While cloud-based services target the vision of Software as a Service, where services are ubiquitously available, mobile use leads to varying connectivity properties. In spite of temporary weak connections and even disconnections, services should remain operational. This paper investigates service component migration between the mobile client and the infrastructure-based cloud as a means to avoid service failures and improve service performance. Hereby, migration decisions are controlled by policies. To investigate component migration performance, an analytical Markov model is introduced. The proposed model uses a two-phased approach to compute the probability to finish within a deadline for a given reconfiguration policy. The model itself can be used to determine the optimal policy and to quantify the gain that is obtained via reconfiguration. Numerical results from the analytic model show the benefit of reconfigurations and the impact of different reconfigurations applied to three service types, as immediate reconfigurations are in many cases not optimal, a threshold on time before reconfiguration can take place is introduced to control reconfiguration.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038602,no,undetermined,0
Performance Analysis of Key Management Schemes in Wireless Sensor Network Using Analytic Hierarchy Process,"To achieve security in wireless sensor networks (WSNs), key management is one of the most challenging issues in design of WSN due to resource-constrained sensor nodes. Various key management schemes (KMs) have been proposed to enable encryption and authentication in WSN for different application scenarios. According to different requirements, it is important to select the trustworthy KMs in a WSN for setting up a fully appropriate WSN mechanism. An Analytic Hierarchy Process (AHP)-aided method helping with the complex decision has been presented in our previous work. Our purpose in this paper is to do performance analysis of KMs in WSN using our previous AHP-aided method. We analyze the characters of abundance KMs intuitively. The following five performance criteria are considered: scalability, key connectivity, resilience, storage overhead and communication overhead. As all permutations of five performance criteria include 120 types' situations, experimental analyses on 43 KMs for the optimum selection are presented.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6121040,no,undetermined,0
Performance evaluation of the Integrated Avionics Simulation system based on grey theory,Performance Evaluation of the Integrated Avionics simulation is more and more important in the current military. The grey theory based on Analytic Hierarchy Process (AHP) is an effective method for the Performance Evaluation of the Integrated Avionics Simulation System. In this paper AHP and grey evaluation model are expounded firstly and then it tells the combination of the two. The method which the article expounds is smart. It gives a better result for the Performance Evaluation of the Integrated Avionics Simulation System.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013544,no,undetermined,0
Plenary talk: Big data and real time analytics,"Moore's law predicted the processor and other IT technologies providing us faster computing storage and networking power for cheaper cost, year after year. This helped us to automate most of the industries and processes, leading to creation of terabytes data every day in organizations. Social computing is also fueling the growth of the data. If one can able to process this data in real time, it will provide market trends and user behavior, which can be used in various businesses. To manage and efficiently process this growing terabytes or petabytes of data, per day, the current data management technologies does not meet the requirements. New concepts and models are emerging to meet these challenges.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972495,no,undetermined,0
Polynomial modeling of acoustic guided wave propagation in homogeneous cylinder of infinite length,"In this paper, we present a polynomial approach for determining the acoustic guided waves in homogeneous infinitely long cylinders using elastic materials of cylindrical anisotropy. The formulation is based on linear three-dimensional elasticity using an analytic form for the displacement field. The approach incorporates the stress-free boundary conditions directly into the equations of motion which are solved numerically by expanding each displacement component using Legendre polynomials and trigonometric functions. The problem is then reduced for anisotropic homogeneous structures to a tractable eigenvalue problem allowing the dispersion curves and associated profiles to be easily calculated. Numerical results of the guided waves for axisymmetric and flexural modes are presented and compared with those published earlier in order to check up the accuracy and range of applicability of the approach. The developed software proves to be very efficient to retrieve the guided waves of any nature and the modes of all orders. The computational advantages of the approach are described.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945602,no,undetermined,0
Predicting in-memory database performance for automating cluster management tasks,"In Software-as-a-Service, multiple tenants are typically consolidated into the same database instance to reduce costs. For analytics-as-a-service, in-memory column databases are especially suitable because they offer very short response times. This paper studies the automation of operational tasks in multi-tenant in-memory column database clusters. As a prerequisite, we develop a model for predicting whether the assignment of a particular tenant to a server in the cluster will lead to violations of response time goals. This model is then extended to capture drops in capacity incurred by migrating tenants between servers. We present an algorithm for moving tenants around the cluster to ensure that response time goals are met. In so doing, the number of servers in the cluster may be dynamically increased or decreased. The model is also extended to manage multiple copies of a tenant's data for scalability and availability. We validated the model with an implementation of a multi-tenant clustering framework for SAP's in-memory column database TREX.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5767936,no,undetermined,0
Prioritizing Architectural Concerns,"Efficient architecture work involves balancing the degree of architectural documentation with attention to needs, costs, agility and other factors. This paper presents a method for prioritizing architectural concerns in the presence of heterogeneous stakeholder groups in large organizations that need to evolve existing architecture. The method involves enquiry, analysis, and deliberation using collaborative and analytical techniques. Method outcomes are action principles directed to managers and improvement advice directed to architects along with evidence for recommendations made. The method results from 3 years of action research at Ericsson AB with the purpose of adding missing views to architectural documentation and removing superfluous ones. It is illustrated on a case where 29 senior engineers and managers within Ericsson prioritized 37 architectural concerns areas to arrive at 8 action principles, 5 prioritized improvement areas, and 24 improvement suggestions. Feedback from the organization is that the method has been effective in prioritizing architectural concerns, that data collection and analysis is more extensive compared to traditional prioritization practices, but that extensive analysis seems inevitable in architecture improvement work.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959695,no,undetermined,0
Query Performance Tuning in Supply Chain Analytics,"Data explosion with knowledge shortage is becoming increasingly prominent. By utilizing business intelligence technology, supply chain analytics turns data into business insights and optimizes supply chain management decisions. Firstly, this paper describes the levels of Business Intelligence analytics, and formulates the architecture of supply chain analytics topics, then explains the analytics details of each topic. Furthermore, as OLAP is the most important decision support analysis tools of which query performance directly impacts the quality of analytics system end user experience, this paper proposes a variety of tuning technologies to accelerate query performance, including optimizing design of dimension, table aggregations, partitions, column store and tuning server resources technologies etc. A use scenario shows performance can be dramatically improved by dropping the processing time from previous 6-8 seconds to less than 0.1 seconds when aggregating 20+ million business transaction records.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5957672,no,undetermined,0
Radiation characteristics of the electric dipole excited by surge impulse current,"Surge is a kind of transient over-voltage or over-current with short duration and high energy in power system, which mainly conducts through wires, also produces electromagnetic radiation in space. It was obtained that the amplitude spectrum of surge impulse current principally concentrates within 1 MHz based on the spectrum analysis. The electromagnetic radiation model of the electric dipole excited by surge impulse current was deduced by using retarded vector potential and electromagnetic field theory, and the analytic expressions in time domain of the radiated electromagnetic fields were given. Moreover, results were acquired by software MATLAB. The results show that the amplitude of the radiated electromagnetic field of an electric dipole is proportional to the surge peak current; the radiated E-field and H-field attenuate with distance; and the radiated electromagnetic field amplitude changes with time in pulse shape at a field point.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016481,no,undetermined,0
Ranking of technology transfer barriers in developing countries; case study of Iran's biotechnology industry,"In this paper, first we shall define a list of technology transfer barriers in the biotechnology field through interviews and literature reviews. Next, we shall categorize them into four major categories of technology, and at last introduce all of the hypotheses on the existence of relationship between each part of technology and failure in the technology transfer process. Questionnaires consisting of two parts were created and handed out to all of the specialists and biotechnology idea-holders in the Iranian National Research center of Biotechnology, Lidco Co, Iran Pastor Institute, Biotechnology Department of University of Tehran, etc. Going on, we've evaluated the factors' validity through statistical tests and the recognized barriers' priorities in each category, which were generally, identified by Analytical Hierarchy Process (AHP) via the software Expert Choice. These priorities describe the high importance of organization-ware (34.1%), information-ware (27%), technique-ware (20.7%) and eventually human-ware (18.1%) respectively.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118187,no,undetermined,0
Research of evaluation information processing of inventory optimization based on grey clustering and fuzzy evaluation method,"Market competition is no longer among individual enterprises, but among the Supply Chains of enterprises. How to set up and maintain a reasonable inventory level to balance the risk and loss brought by the shortage of inventory shortage and the increased inventory cost and capital cost caused by too much inventory has become an imperative problem to logistics enterprises. In this paper, the AHP method is used to determine the various classification indexes weights of the inventory, and based on the gray clustering analysis, inventory optimization evaluation model and algorithm is presented with fuzzy evaluation method, which provides a broad prospect for logistics enterprise inventory optimization and evaluation technology.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6021156,no,undetermined,0
Synthesis Algorithm of Homologous Detection Based on Analytic Hierarchy Process,"Nowadays, the research of software homologous detection is especially important in the flourishing software market while the existing detection techniques based on text, token, abstract syntax tree always get the real similarity inaccurately. In this paper, a synthesis algorithm based on Analytic Hierarchy Process(AHP) is proposed, which analyzes text, token, syntax tree three comparison algorithms synthetically and reflects software homologous similarity more accurately by artificial simulation plagiarism and experts calculation in the homologous field. This algorithm compares superiorities of homologous detection algorithms based on text, token and abstract syntax tree in performances factors such as miss rate, error rate, location accuracy, It also combines three comparison algorithms linearly according to their contributions to performances factors of synthesis similarity. The result of this synthesis algorithm meets the demand of homologous detection, reflects the reasonable real similarity which is calculated by experts.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128148,no,undetermined,0
Tangshan construction modern industry system's foundation ability appraisal and countermeasure,"Construction modern industry system's foundation ability has reflected a local development modern industry system's comprehensive power and the development potential. Sharpens this local construction modern industry system's foundation ability, develops the modern industry system's powerful guarantee. This article utilized the factor analytic method comparative external analysis Tangshan construction modern industry system's foundation ability, and put forward the promotion construction modern industry system ability countermeasure proposal.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6010186,no,undetermined,0
Metrics for service granularity in Service Oriented Architecture,"Service Oriented Architecture (SOA) is becoming an increasingly popular architectural style for many organizations due to the promised agility, flexibility benefits. One of the prominent principles of designing services is the matter of how abstract services should be i.e. granularity. Since service-oriented analysis and design methods lack on providing a quantitative metric for service granularity level evaluation, identification of optimally granular services is the key challenge in service-oriented solution development. This paper is divided into three types of services, and listed on the properties of service granularity. Various properties of the three service types the weight is not the same, in order to get the appropriate service granularity, Using the analytic hierarchy process analysis of the attributes in each service type.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6182003,no,undetermined,0
A Work-Centered Visual Analytics Model to Support Engineering Design with Interactive Visualization and Data-Mining,"To support the knowledge discovery and decision making from large-scale, multi-dimensional, continuous data sets, novel systems of visual analytics need the capability to identify hidden patterns in data that are critical for in-depth analysis. In this paper, we present a work-centered approach to support visual analytics of complex data sets by combining user-centered interactive visualization and data-oriented computational algorithms. We design and implement a specific system prototype, Learning-based Interactive Visualization for Engineering design (LIVE), for engineering designers to handle overwhelming information such as numerous design alternatives generated from automatic simulating software. During the exploration within a ""trade space"" consisting of possible designs and potential solutions, engineering designers want to analyze the data, discover hidden patterns, and identify preferable solutions. The proposed system allows designers to interactively examine large design data sets through visualization and interactively construct data models from automatic data mining algorithms. We expect that our approach can help designers efficiently and effectively make sense of large-scale design data sets and generate decisions. We also report a preliminary evaluation on our system by analyzing a real engineering design problem related to aircraft wing sizing.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149110,no,undetermined,0
A multiple criteria decision making approach for E-Learning platform selection: the Primitive Cognitive Network Process,"E-learning is a learning form supported by information communication technology to facilitate the teachers' teaching and students' learning activities. As there are a number of E-Learning platform providers available in the market, selection of the most suitable E-Learning platform is an essential decision for the learning organization due to its high investments and lasting influence. This paper proposes the Primitive Cognitive Network Process considering multiple criteria and alternatives to address this issue. An illustrated example of E-learning platform selection demonstrates the usability and validity of the proposed approach, with comparing to the Analytic Hierarchy Process (AHP) approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154860,no,undetermined,0
A multiple-objective workflow scheduling framework for cloud data analytics,"One of the most important characteristics of a cloud system is elasticity in resources provisioning. Cloud fabric often composes of massive and heterogeneous types of resources allowing the sciences and engineering applications in many domains to collaboratively utilize the infrastructure. As the cloud systems are designed for a large number of users, a large volume of data, and various types of applications, efficient task management is needed for cloud data analytics. One of the popular methods used in task management is to represent a set of tasks with a workflow diagram, which can capture task decomposition, communication between subtasks, and cost of computation and communication. In this paper, we proposed a workflow scheduling framework that can efficiently schedule series workflows with multiple objectives onto a cloud system. Our designed framework uses a meta-heuristics method, called Artificial Bee Colony (ABC), to create an optimized scheduling plan. The framework allows multiple constraints and objectives to be set. Conflicts among objectives can also be resolved using Pareto-based technique. A series of experiments are then conducted to investigate the performance in comparison to the algorithms often used in cloud scheduling. Results show that our proposed method is able to reduce 57% cost and 50% scheduling time within a similar makespan of HEFT/LOSS for a typical scientific workflow like Chimera-2.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261985,no,undetermined,0
A New Axes Re-ordering Method in Parallel Coordinates Visualization,"Visualization and interaction of multidimensional data always requires optimized solutions to integrate the display, exploration and analytical reasoning of data into one visual pipeline for human-centered data analysis and interpretation. Parallel coordinate, as one of the popular multidimensional data visualization techniques, is suffered from the visual clutter problem. Though changing the ordering of axis is a straightforward way to address it, optimizing the order of axis is a NP-complete problem. In this paper, we propose a new axes re-ordering method in parallel coordinates visualization: a similarity-based method, which is based on the combination of Nonlinear Correlation Coefficient (NCC) and Singular Value Decomposition (SVD) algorithms. By using this approach, the first remarkable axe can be selected based on mathematics theory and all axis are re-ordered in line with the degree of similarities among them. Meanwhile, we would also propose a measurement of contribution rate of each dimension to reveal the property hidden in the dataset. At last, case studies demonstrate the rationale and effectiveness of our approaches: NCC reordering method can enlarge the mean crossing angles and reduce the amount of polylines between the neighboring axes. It can reduce the computational complexity greatly in comparison with other re-ordering methods.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406759,no,undetermined,0
A novel approach to identify problematic call center conversations,"Voice based call centers enable customers to query for information by speaking to agents in the call center. Most often these call conversations are recorded for analysis with the intent of trying to identify things that can help improve the performance of the call center to serve the customer better. Today the recorded conversations are analyzed by humans by listening to call conversations, which is both time consuming, fatigue prone and not accurate. Additionally, humans are able to analyze only a small percentage of the total calls because of economics. In this paper, we propose a visual method to identify problem calls quickly. The idea is to sieve through all the calls and identify problem calls, these calls can then be further analyzed by human. We first model call conversations as a directed graph and then identify a structure associated with a normal call. All call conversations that do not have the structure of a normal call are then classified as being abnormal. In this paper, we use the speaking rate feature to model call conversation because it makes it easy to spot potential problem calls. We have experimented on real call center conversations acquired from a call center and the results are encouraging.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261805,no,undetermined,0
A Pessimistic Approach for Solving a Multi-criteria Decision Making,"An extension of the DS/AHP method in the paper. The extension assumes that expert judgments concerning the criteria are often imprecise and incomplete. The proposed extension also uses groups of experts or decision makers for comparing decision alternatives and criteria. However, it does not require assigning favorable values for different groups of decision alternatives and criteria. The computation procedure for processing and aggregating the incomplete information about criteria and decision alternatives is reduced to solving a finite set of linear programming problems. Main results are explained and illustrated by numerical examples.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6299408,no,undetermined,0
A Ray Tracing Simulation of Sound Diffraction Based on the Analytic Secondary Source Model,"This paper describes a novel ray tracing method for solving sound diffraction problems. This method is a Monte Carlo solution to the multiple integration in the analytic secondary source model of edge diffraction; it uses ray tracing to calculate sample values of the integrand. The similarity between our method and general ray tracing makes it possible to utilize the various approaches developed for ray tracing. Our implementation employs the OptiX ray tracing engine, which exhibits good acceleration performance on a graphics processor. Two importance sampling methods are derived from different aspects, and they provide an efficient and accurate way to solve the numerically challenging integration. The accuracy of our method was demonstrated by comparing its estimates with the ones calculated by reference software. An analysis of signal-to-noise ratios using an auditory filter bank was performed objectively and subjectively in order to evaluate the error characteristics and perceptual quality. The applicability of our method was evaluated with a prototype system of interactive ray tracing.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214586,no,undetermined,0
A study on channel potential for lightly-doped gate-all-around 6H-SiC nanowire FETs,"A continuous and analytic channel potential model for lightly doped gate-all-around (GAA) 6H-SiC nanowire (NW) FETs is developed incorporating the influence of incomplete dopant ionization. By solving the 1D Poisson's equation, and using Lambert-W function, the channel potential and the inversion charge are adequately described from the sub-treshold to the strong inversion.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482888,no,undetermined,0
A Sustainable Building Application Design Based on the mOSAIC API and Platform,"Building sustanability is an emerging area of research related to the use of renewable energy sources and energy efficiency throughout the buildings lifecycle. For this to happen, it is necessary to collect and analyse an increasing amounts of data from various sources including sensor data. As a result, energy production and use could be optimised based on a number of sustainability criteria. In the present study, the mOSAIC Cloud computing platform and API is used to design an application that would make it possible to store and analyse data about the building stock in Slovenia for various research and commercial purposes. Key parts of the design are: (1) an OWL/RDF knowledge base of high-level concepts describing sustainability aspects of a building including sensor data, CO2 consumption, energy use, and other sustainability parameters, and (2) the mOSAIC platform that takes care about the lower level details related to the application bottlenecks, such as elasticity needed when dealing with the sensor data. In order to connect these two distinct parts an URI referencing scheme is used. At runtime, when running analytic services, various decisions are possible to optimise the data and computationally intensive part of the application.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6391844,no,undetermined,0
A users guide to arc resistant low voltage switchgear & motor control äóî Analytical comparison vs arc flash test results,"This paper describes enhanced safety and maintenance features required within LV Motor Control Centers and Switchgear. Enhanced capabilities will include discussion of using NFPA 70E [1], ANSI C37.20.7 [2] and IEEE 1584 [3] in the development of low voltage arc resistance equipment. Highlights will include discussion of arc flash danger to personnel and the solutions used in the development of arc resistant low voltage switchgear and motor control centers. Arc flash software modeling shall be compared with actual testing of equipment per ANSI C37.20.7.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6215701,no,undetermined,0
An approach to aging assessment of power transformer based on multi-parameters,"As the population of transformers in service increases and becomes older, more attentions are focused on their availability and reliability. Through decades' progress, it is undeniable that considerable advances have been achieved on the techniques of fault diagnosis and condition assessment for oil-immersed transformer, for its great importance in the whole transmission and distribution (T&D) system. However, still insufficient research work focused on how to evaluate transformer's aging condition. This paper represents Aging Index (AI) which is a practical tool that combines the results of routine inspections, and site and laboratory testing to estimate the aging condition. This Aging Index calculation considers not only typical test results such as dissolved gas analysis (DGA), oil quality, furan, and dissipation factor, but also other parameters such as top oil temperature and hot spot temperature estimating. And further, Frequency Domain Spectroscopy (FDS) is also introduced as a novel non-destructive testing technique to estimate remained life expectancy. Moreover, a software system based on transformer's electrical and thermal parameters is developed correspondingly, by using a multi-parameters analytic approach. This system is expected to help to decision-making and replacement planning and to have good application prospects.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416453,no,undetermined,0
Target Value Sequencing Based on Weighted Evidence Theory,"We proposed a new approach for target value sequencing problem using weighted evidence theory. Target value evaluation index system is constructed located at artillery battlefield, the uncertain target information are fused based on the evidence theory, the index weight is decided based on analytic network process. The case proves the reliability of the results and the validity of this method. This method provides the artillery commander with a reference to the theoretic basis of decision-making of the target value sequencing.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038208,no,undetermined,0
"An empirical research on relationship between demand, people and awareness towards training needs: A case study in Malaysia Halal logistics industry","Recently, the Halal industry has a huge potential for market demand. The rising of Halal has made the consumers to think twice before using any product because it is proven that Halal product can offer good quality, cleanliness, hygienic, safety, authentic and nutritious. Halal has created awareness to the consumers and supplier to use or supply the good according to Halalan Thoyyibban principle. Many logistics companies in Malaysia have adapted Halal in their operation. However, the lack of employees, skills and experience become the major problem to operate Halal in their operations. There are not enough skills and experience workers who can handle the consignment according to the Halal procedure. Hence, training in the Halal logistics industry professional is crucial as the lack of professionalism in the transport and logistics process in the supply chain may cast doubts on the Halal status of a product. The purpose of this research is to present the relationship between the training needs in Halal logistics industry in Malaysia and investigates its relationship with demand, people and awareness. The data collection instruments used was a questionnaire which was administrated to a total sample of 162 respondents from the middle management level in the logistics companies who have the Halal certification by Jabatan Kemajuan Islam Malaysia (JAKIM) or Halal Industry Corporation (HDC). Sample selection was based on purposive sampling. The analysis involved statistical methods using Predictive Analytics Software (PASW) 18.0 such as reliability and validity test and multiple regressions. The result indicated that training needs in Halal logistics industry are related to demand, people and awareness which to explain the significant influence to the Halal logistics industry.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226062,no,undetermined,0
An improved analytic expression for write amplification in NAND flash,"Agarwal et al. gave a closed-form expression for write amplification in NAND flash memory by finding the probability of a page being valid over the whole flash memory. This paper gives an improved analytic expression for write amplification in NAND flash memory by finding the probability of a page being invalid in the block selected for garbage collection. The improved expression uses the Lambert W function. Through asymptotic analysis, write amplification is shown to depend on the overprovisioning factor only, consistent with the previous work. Comparison with numerical simulations shows that the improved expression achieves a more accurate prediction of write amplification. For example, when the overprovisioning factor is 0.3, the improved expression gives a write amplification of 2.36 whereas that of the previous work gives 2.17, when the actual value is 2.35.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6167472,no,undetermined,0
An optical CDMA method for in-vehicle information service,"This study proposes a new inter-vehicle communication (IVC) system based on optical spectral-amplitude-coding optical code division multiple access (SACOCDMA) systems architecture, then, analysis and calculate the system's performance by using analytic and simulation methods. The important feature of the SAC-OCDMA systems is that multiple access interference (MAI) can be eliminated by code sequences of a fixed in-phase cross-correlation value with balance detection schemes. The advantage of optical CDMA multiplexed method compared with electrical multiplexed method is no electromagnetic interference and security enhanced. The performance of modify prime codes (MPCs) and Walsh-Hadamard codes as signature codes for the SAC-OCDMA systems is analyzed. The simulation results of MPCs in SAC-OCDMA system structures are first presented by commercial simulation obtained using OptiSystem software. The simulation results show that the bit error rate (BER) through use of the MPCs is superior to the conventional Walsh-Hadamard codes, especially when the received effect power is large. The eye diagram also shows that the SAC-OCDMA system with MPCs exhibits a wider opening than the Walsh-Hadamard codes.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6425195,no,undetermined,0
Analysis and prototyping of a low-speed small scale permanent magnet generator for wind power applications,"Many companies engaged in wind generator development are investigating permanent magnet generators (PMG) mainly because of higher efficiency over a broad range of wind speed and reduced costs of maintenance. One important parameter of PM generators is the primary magnetic flux that has certain influence to no load voltage of the machine. Thus, needs for efficient calculation methods for primary magnetic field is required. Finite element method is one of the most common and efficient way to analyze the magnetic flux of the machine. But on the other hand it is relatively complicated calculation method and it might be incomprehensible. Analytic calculation or calculations based on reluctance network are comprehensive and suitable for engineers-designers. This paper deals with the primary magnetic field analysis and basic winding calculations by an example of 54 slots and 20 poles PM synchronous generator. Modeling of the primary magnetic field distribution by the finite element method in software FEMM is described. Results from the analytic calculations and the finite element method are compared with experimental data of PM prototype generator study.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256193,no,undetermined,0
Analytic determination of cogging torque harmonics of brushless permanent magnet machines,"In this paper a new analytic method for the determination of possibly arising cogging torque harmonics of brushless permanent magnet machines is presented. Cogging torque is caused by a change of the system's total magnetic energy concerning a rotor revolution for no stator coil(s) excitation. Usually, this change in magnetic energy is very low compared to the torque values for nominal load. The cogging torque calculation using finite element software is very sensitive regarding the quality of the used mesh. Therefore, an analytic consideration of possibly arising cogging torque harmonics is advantageous. Cogging torque harmonics only arising from calculation errors can be determined and the comparison of the calculated and the analytically determined cogging torque spectrum can be used for a qualitative evaluation of the calculation accuracy. With the presented generalized method, it is further even possible to calculate cogging torque harmonics resulting from unbalances, manufacturing errors, etc. Finally, application examples are presented to effectively reduce cogging torque without a decrease of the machine's efficiency by analyzing the cogging torque harmonics depending on the magnetic field harmonics of the permanent magnet excitation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264408,no,undetermined,0
Analytic modeling of software coincidence detection in PET,"PET imaging is based on the detection in coincidence of two back-to-back annihilation photons. Implementation of coincidence detection in PET data acquisition systems usually involve energy discrimination and time discrimination. Depending on which is performed at first, the coincidence detection can be divided into two ways: energy discrimination before time discrimination (ETD) and time discrimination before energy discrimination (TED). The major difference of them is that, certain double coincidences from multiples which will be discarded by TED may be recovered by ETD. Since the extra double coincidences retrieved by ETD contain not only true coincidences but also scatter coincidences and random coincidences, it is hard to tell whether they can improve the final image quality. In this work, an primary analytic model of coincidence detection in PET is proposed, and the noise equivalent count (NEC) of TED and ETD are deduced from the model to estimate the image quality. The validity of the model is evaluated by GATE simulation. The model and simulation results suggested that the ETD has a better sensitivity while suffers from the random coincidences sorted from multiple coincidences. The NEC of TED is superior when the dose is relatively low and/or a narrow coincidence time window is used, while this turns to inverse while dose getting higher and/or a broader coincidence time window is used. This implies that compromises should be made while choosing the coincidence detection method and the choice is rather application dependent.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6551705,no,undetermined,0
Application DANP with MCDM model to explore smartphone software,"To understand the behavior of smartphone online application software will be helpful to predict whether the software application would be adopted by the users and to guide the providers to enhance the functions of the software. A wide range of criteria are used to assess smartphone software quality, but most of these criteria have interdependent or interactive characteristics, which can make it difficult to effectively analyze and improve smartphone use intention. The purpose of this study is to address this issue using a hybrid MCDM (multiple criteria decision-making) approach that includes the DEMATEL (decision-making trial and evaluation laboratory), DANP (the DEMATEL-based analytic network process) methods to achieve an optimal solution. By exploring the influential interrelationships between criteria related to mobile communication industry's and related value-added service content providers' reference in the respect of operation. This approach can be used to solve interdependence and feedback problems, allowing for greater satisfaction of the actual needs of mobile communication industries.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505023,no,undetermined,0
Applying Cluster Computing to Enable a Large-scale Smart Grid Stability Monitoring Application,"The real-time execution of grid stability monitoring algorithms are critical to enabling a truly smart grid. However, the combination of a high sampling rate for grid monitoring devices, combined with a large number of devices scattered across a grid, result in very high throughput requirements for the execution of these algorithms. Here we define a centralized hardware and software infrastructure to enable the real-time execution of a small signal oscillation detection algorithm using a cluster of commodity nodes. Our research has demonstrated that readings from up to 500 phasor measurement units (PMUs) sampling at 60Hz can be analyzed in real-time by a single 8-core, 2.53GHz machine with 8GB of RAM, and that a cluster of four of these machines can be used to monitor up to 2,000 PMUs in parallel.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332191,no,undetermined,0
Applying Learning Analytics in an Open Personal Learning Environment: A Quantitative Approach,"Every activity in the web leaves a digital trace. Analytics are used to measure all gathered traces along with the activities and the traffic that a user provokes, aiming to exploit the application-under-analysis with an optimal way in terms of time and cost. As a result, the question about how are they applied into learning environment, raises. This paper discusses analytics dedicated to the learning process named learning analytics. After presenting their context, and a literature review regarding this relatively new term, this paper presents a methodology that is applied the academic year 2011-12 on HOU2LEARN Environment, an open educational environment set by Hellenic Open University. This methodology consists of social network analysis along with a set of metrics that is also presented. The procedure and the outcomes of this ongoing research so far are discussed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6377407,no,undetermined,0
A framework to improve evaluation in educational games,"The evaluation process is key for educator's acceptance of any educational action. The evaluation is challenging in most cases but especially when educational games are used. In educational games if in-game evaluation exist it is usually based on a series of simple goals and whether these goals are achieved (i.e. assessment). But we consider that evaluation can be improved by taking advantage of in-game interaction, such as the user behavior during the game and the type and number of interactions performed by the user while playing. In this paper, we propose an evaluation framework for educational games based on in-game interaction data. We discuss how user interaction data is collected in the most automatic and seamless way possible, how to analyze the data to extract relevant information, and how to present this information in a usable way to educators so they achieve the maximum benefit from the experience. The evaluation framework is implemented as part of the eAdventure educational platform, where it can be used both to improve upon traditional basic assessment methods (i.e. goals, scores & reports) and to provide information to help improve interaction with games (e.g. discovery strategies).",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6201154,no,undetermined,0
A Demo Paper: An Analytic Workflow Framework for Green Campus,"This paper proposes a multi-tenant workflow framework that allows users to create data analytic workflows whose tasks are efficiently scheduled and distributed in cloud computing environment. We provide a demo of an event room assignment (ERA) as a test application of the framework. The ERA dynamically and automatically assigns registered events (e.g., meetings, classes, conferences, etc.) to available rooms meeting the user requirements such as the event size, purpose, reservation period, etc. The assignment will lead to the energy efficiency with respect to the power usage (e.g., lighting, ventilation, devices, etc.), and the energy savings can be achieved without affecting people's comfort. We run the ERA with power consumption data (whose size is approximately 50GB) collected from each of over 200 rooms in a building at Dept. of Engineering, Tokyo University. Through the demonstration, we will show that the proposed framework accelerates the speed of data analysis by providing user-friendly workflow composition and parallel processing features utilizing cloud computing technologies.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413593,no,undetermined,0
XQConverter: A System for XML Query Analysis,"As XQuery programs became one of the standard instruments of XML manipulation, for the optimization purposes it is important to identify commonly used constructs and patterns. In this paper we describe the design and implementation of a system for the analysis of XQuery programs, XQ Converter, as the extension of our previously described analytic framework Analyzer. The XQuery program is converted to an XML representation which allows to formulate analytical queries in XPath. The analysis is based on the frequency of occurrence of various language constructs.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6059805,no,undetermined,0
"What Time Is It, Eccles?","Requirements analysts need a new toolbox with both the right tools and the instructions to use them including agile development and user-centered design for techniques such as analysis of Web analytics, wire-framing, and user stories. We can also look to the creativity literature and take techniques such as constraint removal, storytelling, and other worlds.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929526,no,undetermined,0
Technology transition of network defense visual analytics: Lessons learned from case studies,"Despite more than a decade of significant government investment in network defense research and technology development, there have been relatively few successful transitions across the chasm between research and operational use. Prior work describes approaches to crossing the äóìvalley of deathäó from the perspective of the government sponsor or independent tester. The researcher and developer's perspective offered in this paper adds to our understanding of the challenges faced and solutions applied to deployment of advanced technologies into operational environments. The paper describes lessons learned from recent transitions of two information assurance technologies - the VIAssistŒ¬ netflow visualization tool and the MeerCATŒ¬ wireless vulnerability analysis tool - into operational use by the Department of Homeland Security (DHS) and the Department of Defense (DoD).",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107916,no,undetermined,0
The application of FANP in selecting an optimal construction project,"An index system for selecting an optimal construction project is established. Due to the uncertainty and the inaccuracy of information during the evaluation process, an evaluation selection model based on fuzzy analytic network process method is proposed. The weight of each index, including the weight of the criterion level indicators, the weight of independent sub-indices and the weight of dependent sub-indices are determined by fuzzy preference programming method with the help of Matlab software. Meanwhile, the unweighted supermatrix is built for those interactional indicators, and the limit supermatrix is calculated after randomizing the unweighted supermatrix. Accordingly, a comprehensive weight of each index and the final score of each alternative can be calculated. Finally, an example is given by proposed method, and the result is shown that it can deal well with this kind of problems.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975892,no,undetermined,0
The application of fuzzy analytic network process in risk evaluation of dynamic alliance,"Due to its adaptation to environmental change and market competition, the organizational form of dynamic alliance has been developing rapidly in recent years, but it also brings significant risk to enterprises. Taking into account the interaction and feedback relationships between criteria/indices, an index system for evaluating the risk of dynamic alliance is presented. Considering the vagueness and inaccuracy of information during the evaluation process, a risk evaluation model for dynamic alliance based on fuzzy analytic network process method is proposed. The local weights of indices are determined by fuzzy preference programming method. Meanwhile, an unweighted supermatrix based on its network structure is built for those interactional indicators, and the limit supermatrix is calculated. Matlab software is used to solve the problems. Finally, a numerical example of the risk evaluation of a dynamic alliance is illustrated by proposed method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5914449,no,undetermined,0
The design and flow simulation of the volute casing of a mixed flow pump,"Based on the three dimensional geometry of the prototype design of a mixed flow pump, a numerical flow simulation and analytic platform based on commercial CFD software NUMECA was established to simulate the three-dimensional flows in the pumps. The pump volute with scroll-type and different cross-sections were studied. The key sources of the flow losses were analyzed and detailed structural modifications were proposed in this paper. Final results indicate that the efficiency can be increased by 4%.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6023220,no,undetermined,0
The information security risk assessment based on AHP and fuzzy comprehensive evaluation,"Information security risk assessment is the most fundamental basis of the risk management about information security. Moreover, the objectivity and accuracy of information security risk assessment play an important role to safeguard the information system security. Through the study on the information security risk of the network system in a university, we firstly build an index system of information security risk assessment and analyze the functions of its essential elements in detail. Then, based on AHP and fuzzy comprehensive evaluation, an information security risk assessment method is proposed to effectively resolve the quantitative assessment problems of qualitative indicators in the risk assessment. An actual instance shows that the method is of strong rationality and effectiveness, and it can be better applied to information security risk assessment.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014018,no,undetermined,0
The prioritization factors of Enterprise Application Integration (EAI) adoption in Malaysian e-Government,"This paper examines the factors that influence Enterprise Application Integration (EAI) adoption in Malaysian government. This research focuses on one of the Malaysian e-Government project which has been selected as the case study for this research. The case study analysis is to investigate the prioritization of EAI factors by using Miles & Huberman [1] scale and Analytic Hierarchy Process (AHP) technique. The study of these factors is based on EAI adoption model in LGA that proposed by Kamal et al. [2]. The results are comprised through a series of tables which illustrated the prioritization for each factors. Therefore, this research outcome provides a guideline for Malaysian public sector for EAI adoption in e-Government project.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6125710,no,undetermined,0
The study of supplier selection of vegetable supply chain in Hebei Province,"Purpose of this paper is that the choice of vegetable suppliers provides a strong guarantee for the vegetable supply chain to effectively avoid risks and enhance core competencies in Hebei Province. From four types of the vegetable suppliers and five indicators, the article uses the Analytic Hierarchy Process (AHP) to compare their advantages and disadvantages. The results show that the Agricultural Association is the best suppliers on the vegetable supply chain. This finding can strengthen the construction of vegetable supply chain and improve the vegetables competition in the market.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5707616,no,undetermined,0
Towards Delivering Analytical Solutions in Cloud: Business Models and Technical Challenges,"Analytical Solutions increasingly play a key role in the modern enterprise business. Currently, such solutions are usually very costly for customer to consume, as the deployment cost is high due to high performance hardware requirement and complex software configuration. Moreover, such on-premises solutions are not suitable for the occasional analytics consumers. On the other hand, Analytical solutions are also hard for solution providers to deliver cost efficient service, since the cost is high in initial customer engagement as well as conducting incremental service. To deliver analytical solutions in a cost-effective way, we propose the idea of ""analytical cloud"", which is designed to provide on-demand decision support, analytical capabilities, and computational resources in a manageable cloud environment. In this paper, we summarize the potential business models supported by this new delivery model and identify the technical issues required to be addressed in such business models.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104640,no,undetermined,0
Towards efficient resource management for data-analytic platforms,"We present architectural and experimental work exploring the role of intermediate data handling in the performance of MapReduce workloads. Our findings show that: (a) certain jobs are more sensitive to disk cache size than others and (b) this sensitivity is mostly due to the local file I/O for the intermediate data. We also show that a small amount of memory is sufficient for the normal needs of map workers to hold their intermediate data until it is read. We introduce Hannibal, which exploits the modesty of that need in a simple and direct way - holding the intermediate data in application-level memory for precisely the needed time - to improve performance when the disk cache is stressed. We have implemented Hannibal and show through experimental evaluation that Hannibal can make MapReduce jobs run faster than Hadoop when little memory is available to the disk cache. This provides better performance insulation between concurrent jobs.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5990676,no,undetermined,0
Tracking commander's intent in dynamic networks,"A goal of the Tactical Human Integration of Networked Knowledge (THINK) Army Technology Objective (ATO) is to develop and test a proposed socio-cognitive network analytic method that aims to improve the effective understanding and execution of commander's intent (CI), for a given echelon, based on the interpretation of the operational order (OPORD) and the monitoring of the warfighters' activities, via their communications. Network Text Analysis is used to extract a meta-network model from OPORDs and operational discourse, such as email and chat logs. The contents of this meta-network model are coded using the Battle Management Language (BML), and Dynamic Network Analysis (DNA) techniques are then applied to the extracted data in order to determine the extent to which a Commander's Intent is being accurately followed and to identify points and bases for departure. This will lead to the establishment of new requirements for instrumentation of the net-centric environment for detection of human behavior.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6127522,no,undetermined,0
Understanding Boundary Scan test with Trainer 1149,"In this paper, we describe a training environment based on multi-functional software system called äóìTrainer 1149äó. It provides simulation and demonstration functionality for learning, research, and development related to IEEE 1149.1 Boundary Scan (BS) standard. The software supports both the analytic and synthetic learning process. Trainer 1149 is the main component of a recent goJTAG initiative that aims at creating of an open-source platform for learning and working with BS. Trainer 1149 provides a cozy graphical design and simulation environment of BS-enabled chips and non-BS clusters. It is also possible to simulate the behaviour of various interconnect faults and inspect them using interactive tools. Using a convenient low-cost USB-JTAG cable, one can test real defects in real hardware. The system is implemented using multi-platform Java environment and distributed as an open-source freeware. Such combination of features is unique for public domain BS software.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165727,no,undetermined,0
Video Analytics: Opportunity or Spoof Story? The State of the Art of Intelligent Video Surveillance,"In 2010 FORMIT developed the VIEWER project with the support of the Prevention, Preparedness and Consequence Management of Terrorism and other Security Related Risks Programme of the European Commission, DG Home Affairs. The main goals of the project were to realize a recognition of Video Analytics (hereafter VA) software currently on the market; to collect qualitative information and quantitative data on the VA software performances in order to be able to define the state of the art; to address policy recommendations to the European Commission; to address recommendation to the research community on the future desirable development in the sector.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061220,no,undetermined,0
Visual analytical approaches to evaluating uncertainty and bias in crowd sourced crisis information,"Concerns about verification mean the humanitarian community are reluctant to use information collected during crisis events, even though such information could potentially enhance the response effort. Consequently, a program of research is presented that aims to evaluate the degree to which uncertainty and bias are found in public collections of incident reports gathered during crisis events. These datasets exemplify a class whose members have spatial and temporal attributes, are gathered from heterogeneous sources, and do not have readily available attribution information. An interactive software prototype, and existing software, are applied to a dataset related to the current armed conflict in Libya to identify `intrinsic' characteristics against which uncertainty and bias can be evaluated. Requirements on the prototype are identified, which in time will be expanded into full research objectives.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102470,no,undetermined,0
Visual analytics of terrorist activities related to epidemics,"The task of the VAST 2011 Grand Challenge was to investigate potential terrorist activities and their relation to the spread of an epidemic. Three different data sets were provided as part of three Mini Challenges (MCs). MC 1 was about analyzing geo-tagged microblogging (Twitter) messages to characterize the spread of an epidemic. MC 2 required analyzing threats to a computer network using a situational awareness approach. In MC 3 possible criminal and terrorist activities were to be analyzed based on a collection of news articles. To solve the Grand Challenge, insight from each of the individual MCs had to be integrated appropriately.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102498,no,undetermined,0
Visual analytics tools and theirs application in social networks analysis,"The first association on the word ""data"" for common man is a visualization of the alpha-numeric characters and various pictures in the form of a table or chart. Distinction between data, information and knowledge, is one of the primary tasks of information science. The quantity of electronic data in today's information society is steadily increased by using Internet technologies (among other things with the e-government and all it's parts-from commerce, education to public health, also with the emergence of social networks and media in all their forms). It requires visualization of the data for easier understanding and analysis which are obviously necessary. In this paper some of the commercial and non-commercial visual tools and their application in analysis of social networks are considered, with especially interest for theirs application in the field of criminology.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143801,no,undetermined,0
Visualization of Parameter Space for Image Analysis,"Image analysis algorithms are often highly parameterized and much human input is needed to optimize parameter settings. This incurs a time cost of up to several days. We analyze and characterize the conventional parameter optimization process for image analysis and formulate user requirements. With this as input, we propose a change in paradigm by optimizing parameters based on parameter sampling and interactive visual exploration. To save time and reduce memory load, users are only involved in the first step - initialization of sampling - and the last step - visual analysis of output. This helps users to more thoroughly explore the parameter space and produce higher quality results. We describe a custom sampling plug-in we developed for CellProfiler - a popular biomedical image analysis framework. Our main focus is the development of an interactive visualization technique that enables users to analyze the relationships between sampled input parameters and corresponding output. We implemented this in a prototype called Paramorama. It provides users with a visual overview of parameters and their sampled values. User-defined areas of interest are presented in a structured way that includes image-based output and a novel layout algorithm. To find optimal parameter settings, users can tag high- and low-quality results to refine their search. We include two case studies to illustrate the utility of this approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065007,no,undetermined,0
Visualizing an information assurance risk taxonomy,"The researchers explore the intersections between Information Assurance and Risk using visual analysis of text mining operations. The methodological approach involves searching for and extracting for analysis those abstracts and keywords groupings that relate to risk within a defined subset of scientific research journals. This analysis is conducted through a triangulated study incorporating visualizations produced using both Starlight and In-Spire visual analysis software. The results are definitional, showing current attitudes within the Information Assurance research community towards risk management strategies, while simultaneously demonstrating the value of visual analysis processes when engaging in sense making of a large body of knowledge.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102477,no,undetermined,0
Vulnerability hierarchies in access control configurations,"This paper applies methods for analyzing fault hierarchies to the analysis of relationships among vulnerabilities in misconfigured access control rule structures. Hierarchies have been discovered previously for faults in arbitrary logic formulae [11,10,9,21], such that a test for one class of fault is guaranteed to detect other fault classes subsumed by the one tested, but access control policies reveal more interesting hierarchies. These policies are normally composed of a set of rules of the form äóìif [conditions] then [decision]äó, where [conditions] may include one or more terms or relational expressions connected by logic operators, and [decision] is often 2-valued (äóìgrantäó or äóìdenyäó), but may be n-valued. Rule sets configured for access control policies, while complex, often have regular structures or patterns that make it possible to identify generic vulnerability hierarchies for various rule structures such that an exploit for one class of configuration error is guaranteed to succeed for others downstream in the hierarchy. A taxonomy of rule structures is introduced and detection conditions computed for nine classes of vulnerability: added term, deleted term, replaced term, stuck-at-true condition, stuck-at-false condition, negated condition, deleted rule, replaced decision, negated decision. For each configuration rule structure, detection conditions were analyzed for the existence of logical implication relations between detection conditions. It is shown that hierarchies of detection conditions exist, and that hierarchies vary among rule structures in the taxonomy. Using these results, tests may be designed to detect configuration errors, and resulting vulnerabilities, using fewer tests than would be required without knowledge of the hierarchical relationship among common errors. In addition to practical applications, these results may help to improve the understanding of access control policy configurations.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111679,no,undetermined,0
WAMäóîThe Weighted Average Method for Predicting the Performance of Systems with Bursts of Customer Sessions,"Predictive performance models are important tools that support system sizing, capacity planning, and systems management exercises. We introduce the Weighted Average Method (WAM) to improve the accuracy of analytic predictive performance models for systems with bursts of concurrent customers. WAM considers the customer population distribution at a system to reflect the impact of bursts. The WAM approach is robust with respect to distribution functions, including heavy-tail-like distributions, for workload parameters. We demonstrate the effectiveness of WAM using a case study involving a multitier TPC-W benchmark system. To demonstrate the utility of WAM with multiple performance modeling approaches, we developed both Queuing Network Models and Layered Queuing Models for the system. Results indicate that WAM improves prediction accuracy for bursty workloads for QNMs and LQMs by 10 and 12 percent, respectively, with respect to a Markov Chain approach reported in the literature.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953602,no,undetermined,0
Modeling and Analyzing Server System with Rejuvenation through SysML and Stochastic Reward Nets,"High-availability assurance of server systems is becoming an important issue, since many mission-critical applications are implemented on server systems. To achieve high-availability, software rejuvenation is a practical technique to reduce unexpected downtime caused by software aging in software applications running on server systems. Although analytic models of software rejuvenation are well-studied, such analysis is not used in server system administration due to the complexity of modeling. In this paper, we present an availability modeling method for server system with software rejuvenation based on SysML that is used to describe system configurations and maintenance operations semi-formally. The proposed approach allows system administrators, who do not have expertise in availability modeling, to design and study the effects of different rejuvenation policies deployed in server systems. To show the applicability of the proposed modeling and evaluation process, a case study of a web application server is presented. We show the correctness of our modeling method by comparing the conventional models for condition-based and time-based software rejuvenation.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045928,no,undetermined,0
Measuring firewall security,"In the recent years, more attention is given to firewalls as they are considered the corner stone in Cyber defense perimeters. The ability to measure the quality of protection of a firewall policy is a key step to assess the defense level for any network. To accomplish this task, it is important to define objective metrics that are formally provable and practically useful. In this work, we propose a set of metrics that can objectively evaluate and compare the hardness and similarities of access policies of single firewalls based on rules tightness, the distribution of the allowed traffic, and security requirements. In order to analyze firewall polices based on the policy semantic, we used a canonical representation of firewall rules using Binary Decision Diagrams (BDDs) regardless of the rules format and representation. The contribution of this work comes in measuring and comparing firewall security deterministically in term of security compliance and weakness in order to optimize security policy and engineering.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111669,no,undetermined,0
Automatic Bibliometric Analysis of Research Literature in Adult Education,"Due to the vast volume of publications in nowadays research environment, methods for analyzing, organizing, and accessing information from large databases are in great need. Through appropriate analytic tools, competitive intelligence can be revealed to enhance the organization or individual performance. This article will show how a free software package called CATAR carry on the analysis about the literature of the Adult Education science. Adult Education Quarterly (AEQ) is a scholarly forum of research and theory in adult education. The field of adult education is broad-based and inter disciplinary, during 1990 to 2010 having 672 papers from the ISI Web of Knowledge database was collected. The data analyzed include the distribution of authors' countries, topical clusters, the most cited references, the productivity rankings, and others to provide information for further study. Our analysis scenario also suggests a feasible way of analyzing other similar data for readers interesting in literature mining applications.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6296987,no,undetermined,0
An Evidence Based Analytical Approach for Process Automation: Case Study in Finance and Administration Process Delivery Services,"Process Automation is a critical activity to reduce the need for human work in the production of goods and services to make the processes for uniform and efficient. It is always challenging to make informed decisions on areas for automation which addresses time-to-value to the business. In this paper, we present an evidence based analytics approach to identify top opportunities for process automation, and provide objective assessment of benefit to enable process leaders to take informed decisions. This approach is composed of three major steps. The first step is to identify the top hitters of human intensity in the delivery processes through analyzing evidence gathered from activity time-motion monitoring and onsite process deep dive. The second step is to prioritize the opportunities for automation by analyzing technology choices and estimated business impact. The final step is to assess the benefit through deep analysis on the additional evidence gathered on time-motion data on operational procedures through sampling and extrapolation, and degree of automation can be achieved through technology components. A case study in Finance and Administration Process Delivery Services is used to illustrate the core idea of our analytical approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104630,no,undetermined,0
A rank-reducing and division-free algorithm for inverse of square matrices,"The paper puts forward a new direct algorithm for computing the inverse of a square matrix. The algorithm adopts a skill to compute the inverse of a regular matrix via computing the inverse of another lower-ranked matrix and contains neither iterations nor divisions in its computations-it is division-free. Compared with other direct algorithms, the new algorithm is easier to implement with either a recursive procedure or a recurrent procedure and has a preferable time complexity for denser matrices. Mathematical deductions of the algorithm are presented in detail and analytic formulas are exhibited for time complexity and spatial complexity. Also, the recursive procedure and the recurrent procedure are demonstrated for the implementation, and applications are introduced with comparative studies to apply the algorithm to tridiagonal matrices and bordered tridiagonal matrices.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6184687,no,undetermined,0
A software toolkit for visualizing enterprise routing design,"Routing design is widely considered as one of the most challenging parts of enterprise network design. The challenges come from the typical large scale of such networks, the diverse objectives to meet through design, and a wide variety of protocols and mechanisms to choose from. As a result network operators often find it difficult to understand and trouble-shoot the routing design of their networks. Furthermore, today's common practice of focusing on one router or one protocol at a time makes it a onerous task to reason about the network-wide routing behavior.We believe that to mitigate the problem, there is a need for software tools to produce effective visualization of enterprise routing designs. In this paper we report on our experience building such a toolkit. We begin by abstracting various routing mechanisms into a small number of design primitives. The abstraction allows for a more concise representation of routing design without losing important design information, which is critical for making the tool scalable. Guided by the abstraction, we then develop a set of algorithms and heuristics, which take router configuration files as input and output a graphical representation of the routing design. The layout and components of the graph are highly customized to optimize its readability and power to infer the network-wide design pattern. We also present a case study using the toolkit to analyze the routing design of two large campus networks, and report on our findings. Our experience confirms the effectiveness of our toolkit in revealing key design characteristics of the networks, and in illustrating the network-wide routing behavior.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111664,no,undetermined,0
A study of hierarchical network security situation evaluation system for electric power enterprise based on Grey Clustering Analysis,"Focused on the network security situation evaluation, a novel hierarchical evaluation system, which is based on Grey Clustering Analysis, is proposed. In this system, network attacks are classified into ""Strong"", ""Medium"", and ""Weak"" three harmful levels by Grey Clustering Analysis to construct a hierarchical index system. Then, the Analytic Hierarchy Process is used to calculate the impact factor of each network attack, and form the evaluation system to calculate the network security situation value. With Grey Clustering Analysis, harmful level ownership of each network attacks is determined, and each network attacks' influence can be truly reflected. Moreover, network security situation value's calculation speed can be improved by Analytic Hierarchy Process. Finally, large amounts of electric power on-site experiments indicated that the evaluation system is well performed by showing network security situation in both coarse-grained and fine-grained analysis.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974463,no,undetermined,0
A study on evaluation system of bridge technical condition based on fuzzy synthetic judgment,"Bused on the fundamental principle of analytic hierarchy process, taking the simply supported reinforced concrete beam bridge as an example, the methods to select assess indexes and to establish evaluation model of bridge technical condition were expounded. The ways to determine the weights of indexes with credit degree method of group decision making in analytic hierarchy process, and to evaluate bridge technical condition with fuzzy synthetic judgment, were illustrated. In addition, the main functions and basic formations of the evaluation system for bridge technical condition, and the basic ideas to develop the related software were introduced. This system can provide decision basis for the maintenance of bridges and has extensive practicality. It can not only satisfy the technical condition evaluation for general bridges, but also add new evaluation models for other special bridges if necessary.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5776052,no,undetermined,0
A universal analytic potential function applied to diatomic molecules,"In this paper, a new method on constructing analytical potential energy functions is presented, and from this a analytical potential energy function applied to both neutral diatomic molecules and charged diatomic molecular ions is obtained. This potential energy function includes three dimensionless undetermined parameters which can be determined uniquely by solving linear equations with the experimental spectroscopic parameters of molecules. The solutions of the dimensionless undetermined parameters are real numbers rather than complex numbers, this ensures that the analytical potential energy function has extensive universality. Finally, the potential energy function is examined with four kinds of diatomic molecules or ions homonuclear neutral diatomic molecule H<sub>2</sub>(X<sup>1</sup> ‘£<sub>g</sub><sup>+</sup>), homonuclear charged diatomic molecular ion, He<sub>2</sub><sup>+</sup>(X<sup>2</sup> ‘£<sub>u</sub><sup>+</sup>) heternuclear neutral diatomic Molecule AlBr(A<sup>1</sup>‘æ) and heternuclear charged diatomic Molecular ion BC<sup>-</sup>(X<sup>3</sup>‘æ), as a consequence, good results are obtained.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014229,no,undetermined,0
AHP study on energy indicators system for sustainable development of Henan province,"Analytic Hierarchy Process is an approach to decision making that involves structuring multiple choice criteria into a hierarchy, assessing the relative importance of these criteria, comparing alternatives for each criterion, and determining an overall ranking of the alternatives. This paper applies AHP for evaluating the energy indicators system for sustainable development. A case applied for Henan province is studied. All weight coefficients for energy indicators system are gotten from an investigation. Moreover, the score methodology by MATLAB software is also used for getting the energy indicators' weight scores. The results show that there are four indicators, which are the key indicators influencing sustainable development of energy for Henan province.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014245,no,undetermined,0
AHP-based assessment of emergency response agencies,"Advanced emergency management agency or not, directly related to the handling of public emergency is in place in time. At present, the management level of domestic emergency response agencies between provinces exists irregularity, in order to solve this problem, this paper proposed an emergency response agencies assessment method based on Analytic Hierarchy Process, used AHP to quantity evaluating indicator, assessed emergency response agencies between Henan Hunan and Shanxi provinces, finally we can get a more accurate evaluation result. Which will improve management level of emergency response agencies at the same time evaluate emergency response agencies scientifically and objectively.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014680,no,undetermined,0
An analysis of mechanism kinematics accuracy based on linear elastic,"The simulation-based prediction of the mechanism kinematics accuracy is of great importance in engineering application. For widely used and low-cost open-loop mechanism having no feedback signals, it is important to take a balance between economic consideration and high accuracy requirement, thus containing high research value. Based on current widely used methods, this paper proposes a more precise method to revise the kinematics accuracy of movement mechanism. Considering the manufacture error and joint clearance in mechanism, we reckon in elastic deformation displacement caused by gravity load and inertia force, that the analytic expression of displacement correction is presented, and that it is independent of the Business Software. Theoretical analysis and case studies show that this amendment for the precision request is necessary.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979302,no,undetermined,0
An Analytic Network Process Model of Trust in B2C E-Commerce,"Online consumer trust in Business to Consumer (B2C) e-commerce trust has been viewed as a key differentiator that determines the success or failure of many companies conducting their business over the Internet. In order to explore the influence factor of consumer trust and their priority, and to look for the way to promote consumer trust, an Analytic Network Process (ANP) model is constructed in this paper. Based on three dimension of consumer trust, an index system is suggested. By analyzing the interaction of indicators, the ANP model can rationally calculate the weight of each indicator with Super Decisions software. From data analysis, the research comes to a conclusion that brand of the company, law and technique, scale of the company and reputation of the website have a great impact on consumer trust. And at the present stage, consumers take more attention to competence and integrity dimension than benevolence dimension. Besides, the model also shows that 360buy wins the most consumer trust compared with the other two selected B2C companies. The consistence with the reality proves that the ANP model can effectively find the priority of influence factors.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103565,no,undetermined,0
An experimental testbed to predict the performance of XACML Policy Decision Points,"The performance and scalability of access control systems is a growing concern as organisations deploy ever more complex communications and content management systems. This paper describes how an (offline) experimental testbed may be used to address performance concerns. To begin, timing measurements are collected from a server component incorporating the Policy Decision Point (PDP) under test, using representative policies and corresponding requests. Our experiments with two XACML PDP implementations show that measured request service times are typically clustered by request type; thus an algorithm for request cluster identification is presented. Cluster characterisations are used as inputs to a PDP performance model for a given policy/request mix and an analytic (queueing) model is used to estimate the equilibrium server load for different mixes of request clusters. The analytic performance prediction model is validated and extended by discrete event simulation of a PDP subject to additional load. These predictive models enable network administrators to explore the capacity of the PDP for different overall loadings (requests per unit time) and profiles (relative frequencies) of requests.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5990711,no,undetermined,0
Analytics-driven asset management,"Asset-intensive businesses across industries rely on physical assets to deliver services to their customers, and effective asset management is critical to the businesses. Today, businesses may make use of enterprise asset-management (EAM) solutions for many asset-related processes, ranging from the core asset-management functions to maintenance, inventory, contracts, warranties, procurement, and customer-service management. While EAM solutions have transformed the operational aspects of asset management through data capture and process automation, the decision-making process with respect to assets still heavily relies on institutional knowledge and anecdotal insights. Analytics-driven asset management is an approach that makes use of advanced analytics and optimization technologies to transform the vast amounts of data from asset management, metering, and sensor systems into actionable insight, foresight, and prescriptions that can guide decisions involving strategic and tactical assets, as well as customer and business models.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5697272,no,undetermined,0
An IT Project selection method based on fuzzy analytic network process,"IT Project selection is a multi-criteria decision-making (MCDM) problem. The key process of IT project selection is the assessment criteria framework establishment and criteria weight evaluation. Several criteria should be included into the judging system, and the complexity of multi-criteria judgment could even increase if relationships among selection criteria are taken into consideration. A method named fuzzy analytic network process (FANP) is proposed to tackle with the relationships of criteria and the uncertainty of them. A network structure of criteria is developed according to ANP method first, and then fuzzy method is used to evaluate the interdependent importance between criteria. After the weight determination of relative criteria, project selection result could be clear. At the end, an example is given to explain the usage and availability of this method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6081297,no,undetermined,0
Analysis of data clustering support for service,"With the current direction of service and cloud, unique characteristics of online software services impose new algorithmic requirements and cause differential applicability/ suitability of different clustering approaches in service analytics. In this paper, we investigate the efficiency and effectiveness of current important data clustering techniques, partitioning and hierarchical, for service analytics. It is our goal that results from this paper will serve as requirement guidelines for developing and deploying future intelligence services.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982246,no,undetermined,0
Analysis software of general dynamic circuit based on MATLAB,"The analysis software of General dynamic circuit is developed based on GUI programming in MATLAB. For the equation of circuit formed by node-listing method of the circuit theory the popularity and applicability of the circuit analysis is improved. The visualization of the input and output signal, power function transformation and initial value problem can be solved by GUI activex and programming. In the paper, we accomplish automatic computing, wave out and analytic solution of the general dynamic circuit. The computation examples verify the accuracy and validity of the program.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6057226,no,undetermined,0
Analysis to Industry Agglomeration Degree for Changzhou Hi-Tech Zone Based on an AHP Model,"This paper proposes a multi-criteria evaluation model for assessing industry agglomeration degree referring to Changzhou Hi-Tech Zone. By using Analytic Hierarchy Process (AHP) method, the model combines qualitative analysis and quantitative measurement, which is derived from Changzhou Industry Information System for Changzhou Hi-Tech Zone, resulting in a reasonable evaluation scientifically and authoritatively.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999069,no,undetermined,0
Analytic calculation of magnetic field and force in Halbach permanent magnet linear motor,"The calculating formula of magnetic field produced by single permanent magnet (PM) is deduced based on equivalent current model and magnetic field of Halbach PM array is also given. Secondly, A model of linear synchronous motor whose stator is air-cored coil and mover is Halbach PM array is designed. Then, air-gap magnetic field and force are calculated by both the above formula and commercial electromagnetic simulation software MaxWell. The error between the two methods is less than 2 percent. Compared with the software, the formula presented in the paper is accurate and timesaving which can be used as a fast calculating method for designing and validating linear synchronous motor with Halbach PM array.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037951,no,undetermined,0
Analytic hierarchy process integrated hybrid agent system for intelligent legal assistance,"Legal systems developed around the world tend to be very complex in the eyes of a common man with no idea as to how to approach a judiciary seeking justice or exhibiting his right to information about a particular law. Mostly a man with a need for judicial assistance approaches with an advocate to deal with his case, but this may not be useful when a person is interested in knowing and applying the exact law approach to petty issues in his life and also to avoid being a scapegoat at sometimes by few advocates who elongates the case system for his benefits. The proposed approach has an intelligent hybrid agent system composed of two subsystems, law gatherer and law decider to address the two key issues, finding out right laws and making a right choice. The law gatherer subsystem is based on the ontology web service architecture to define the ontology which is used for processing the semantic content of gathered law information. The law gatherer subsystem is composed of a knowledge system that combines Case-Based Reasoning (CBR) and Rule-Based Reasoning (RBR). Simple Object Access protocol (SOAP) is used for establishing the communication interface and gathering XML-based contents between the agent system and the database system created, through remote procedure calls with condition parameters. In the second subsystem Law Decider, the Analytic Hierarchy Process which is a structured technique for dealing with complex decisions is used to make an optimal decision for satisfying the case structure of the individual. The proposed Hybrid agent system will assist the people with no judicial knowledge to feed their case details into the system to get the appropriate law suitable for that particular instance.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6049001,no,undetermined,0
Analytic model of FlexRay synchronization mechanism,"FlexRay is an incoming standard for automotive distributed systems. For safety critical applications like x-by-wire systems it is necessary to develop techniques for their verification and validation. This paper presents a model of FlexRay Synchronization Mechanism, validation of this model and offers its usage for validation of parameters of synchronization mechanism in real FlexRay networks.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6072918,no,undetermined,0
Analytic Solution for Variations of Magnetic Fields in Closed Circuits: Examination of Deviations From the äóìStandardäó Ampere's Law Equation,"The calculation of magnetic fields in devices with a nonuniform distribution of magnetomotive force (MMF) and with nonlinear magnetic components has proven problematic within electromagnetic systems. This is caused by insufficiently precise determination of the dependence of magnetic induction B, on magnetic field H. This paper develops a method for analytically calculating the variation of magnetic fields H, in magnetic circuits by introducing a specific expression into to the Ampere's circuital law formula to take into account nonuniform MMF. The new formula uses a conformal mapping procedure which allows calculation of the magnetic field at different displacements from the field generating coil. The analytic approximation proposed is developed for the specific problem of a closed circuit magnetic core. This analytic model gives accurate results faster than can be achieved in FEM software.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5686938,no,undetermined,0
Analytical Hierarchy Process and PROMETHEE application in measuring object oriented software quality,This paper describes the combination of Analytic Hierarchy Process (AHP) and PROMETHEE to measure the quality of object oriented software design based on predetermined criteria. The measurement is performed by calculating the metric values that indicate the quality factors. The metric values will be evaluated by AHP method and that result will be used by PROMETHEE method for selecting the most optimal software design that fit the predetermined criteria.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140791,no,undetermined,0
A QoS-Aware Service Evaluation Method for Co-selecting a Shared Service,"In service selection, an end user often has his or her personal preferences imposing on a candidate service's non-functional properties. For a service selection process promoted by a group of users, candidate services are often evaluated by a group of end users who may have different preferences or priorities. In this situation, it is often a challenging effort to make a tradeoff among various preferences or priorities of the users. In view of this challenge, a multi-criteria decision-making method, named AHP (Analytic Hierarchy Process), is introduced to transform both qualitative personal preferences and users' priorities into numeric weights. Furthermore, a QoS-aware service evaluation method is presented for a shared service's co-selection taking advantage of AHP theory. At last, a case study is presented to demonstrate the feasibility of the method.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009383,no,undetermined,0
A method for system auditing based on baseline assessment,"Common Criteria (CC) provides only the standard for evaluating information security product or system. CC based evaluation on system auditing is considered crucial for the overall evaluation and in trouble without an effective method; however, the information system is a large-scale complex system. It includes many uncertain factors, as software, hardware, people and so on. As a result, information systems security risk is related to many ambiguous factors, what are difficult to measure, with ambiguity. In this paper, a method for system auditing based on baseline assessment was presented, In our method, analytic hierarchy process is introduced, which could be used to evaluate the security situation of information system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014334,no,undetermined,0
A Logistics model in natural disaster,"An emergency logistics model based on Location-routing Problem (LRP) is proposed in this paper, and a two-phase heuristic algorithm is used to solve the model. It uses the minimization envelopment analysis method to solve the LRP in the emergency supplies center, and then calculates K shorter paths by the application of the nearest neighbor method. Considering the indexes of time, security, economic cost and environment cost of emergency transportation routing, the paper establishes a shortest-path with the method of Analytic Network Process (ANP). Based on the analysis and calculation of an example, the results indicate that the model and method are effective and feasible.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5707652,no,undetermined,0
A guarantee Model based on ANP for Implementing äóìA plan for educating and training outstanding engineersäó,"Educating and training outstanding engineers involves the government, the institution of higher learning and enterprises. The chief problems of implementing äóìA Plan for Educating and Training Outstanding Engineers (PETOE)äó are how to build an guarantee evaluation index system and determine the guarantee index weight and interaction among indexes. In this paper, the method of Analytic Network Process (ANP) were adopted, eleven indexes, such as the policy, capital and teachers involved in implementing guarantee were analyzed, the guarantee Model and supermatrix of implementing PETOE Based on ANP were built, the SD software was used to find out the convergence state of the associated supermatrix, the stable limit supermatrix, the index weights were calculated and their total sequences of affecting indexes was sorted. The results show that the guarantee model of implementing PETOE based on ANP is reasonable.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181980,no,undetermined,0
Ticker text extraction from Bangla news videos,"In this paper, a framework for recognition of Bangla ticker text<sup>1</sup> from the Bangla news videos is presented. Tesseract OCR [1] has been used for Bangla script recognition. Tesseract OCR gives good results for text recognition in documents. But in case of images and videos, some processing is required beforehand. Approach here is to provide processed images to the Tesseract OCR to get better results than directly providing the raw video frames to the Tesseract OCR. The ticker text recognized can further be used for indexing of news videos on the basis of recognized keywords. Indexing of news videos is important for news monitoring agencies. Till now this is done manually. Automation of the monitoring process and indexing the news videos can save a lot of time as well as the efficiency of the news monitoring system.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5712595,no,undetermined,0
Time efficient method for automated antenna design for wireless energy harvesting,"The rectifier circuit in a rectenna (rectifying antenna) is analyzed employing a fast, efficient time-marching algorithm. The thus found complex input impedance dictates the antenna design. To maximize RF-to-DC conversion efficiency we do not want to employ an impedance matching and filtering network. Instead, we require the input impedance of the antenna to be equal to the complex conjugate value of the input impedance of the rectifier circuit. One antenna type feasible of supplying the required complex input impedance is the wire folded dipole array antenna. For this antenna type, an efficient, analytic model has been developed. The fast calculation times when implemented in software allow for an automated antenna design employing a Genetic Algorithm optimization. Thus, antenna designs can be generated within minutes employing standard office computing equipment. The design of a complete rectenna can be accomplished within hours. The direct complex conjugate matching ensures that the design is power-efficient and physically compact.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5666213,no,undetermined,0
Tourism Planning Assessment Based on Analytic Hierarchy Process,"The assessment of tourism planning is significant to tourism development. As expert evaluation is short of scientificity, analytic hierarchy process is applied to assessment for tourism planning in the paper. Assessment indexes of tourism planning are analyzed, assessment indexes of tourism planning are composed of infrastructure and investment cost, social and natural environment, and passenger origin condition and expected income. Then, assessment model of tourism planning is established. Weiyuan reservoir basin in Jinggu county, Yunnan province is used as our application object. The experimental results indicate the effectiveness of analytic hierarchy process.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432592,no,undetermined,0
Towards a Foundation for Quantitative Service Analysis An Approach from Business Process Models,"Recent advancements in process centric systems, the concept of Services have been adopted widely over business processes. Services which are discrete reusable functional blocks have become the choice to achieve highest order of reuse. However, key to having such services is to identify them with right objectives. Current methods and techniques for service analysis are heuristic and rely heavily on the experience and intuition of the designer. We present the prerequisite formalism for service analysis from business process models and basic quantitative metrics.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557286,no,undetermined,0
Trust Model of Software Behaviors Based on Check Point Risk Evaluation,"In order to evaluate the credibility of software behaviors more reasonably and accurately, a trust model of software behaviors based on check point risk evaluation is presented. Firstly, adopt fuzzy analytic hierarchy process (FAHP) to figure out the weight of every risk factor of check point, meanwhile, assess the modules value according to Markov Chain Usage Model, thus calculating out the risk value of check point and accumulating every suspected risk check point, and then adopt rewarding or punishment mechanism to evaluate a software behaviors trustworthy, which can judge whether software behaviors are credible or not. The simulated experiment shows that this model can distinguish the potential risk effectively in software behaviors, evaluate the risk value trustworthy and provide objective and reliable information to judge whether software behaviors are credible or not.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945050,no,undetermined,0
"Twitter analytics: Architecture, tools and analysis","We study the temporal behavior of messages arriving in a social network. We specifically study the tweets and re-tweets sent to president Barack Obama on Twitter. We characterize the inter-arrival times between the tweets, the number of re-tweets and the spatial coordinates (latitude, longitude) of the users who sent the tweets. The modeling of the arrival process of tweets in Twitter can be applied to predict co-ordinated user behavior in social networks. While there is sufficient literature on social networks that present large volumes of collected data, the modeling and characterization of the data have been rarely discussed. The available data are usually very expensive and not comprehensive. Here, we develop a software architecture that uses a Twitter application program interface (API) to collect the tweets sent to specific users. We then extract the user ids and the exact time-stamps of the tweets. We use the collected data to characterize the inter-arrival times between tweets and the number of re-tweets. Our studies indicate that the arrival process of new tweets to a user can be modeled as a Poisson Process while the number of re-tweets follow a geometric distribution. Our data collection architecture is operating system (OS) independent. The results obtained in this research can be applied to study correlations between patterns of user behavior and their locations.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680493,no,undetermined,0
Understanding text corpora with multiple facets,"Text visualization becomes an increasingly more important research topic as the need to understand massive-scale textual information is proven to be imperative for many people and businesses. However, it is still very challenging to design effective visual metaphors to represent large corpora of text due to the unstructured and high-dimensional nature of text. In this paper, we propose a data model that can be used to represent most of the text corpora. Such a data model contains four basic types of facets: time, category, content (unstructured), and structured facet. To understand the corpus with such a data model, we develop a hybrid visualization by combining the trend graph with tag-clouds. We encode the four types of data facets with four separate visual dimensions. To help people discover evolutionary and correlation patterns, we also develop several visual interaction methods that allow people to interactively analyze text by one or more facets. Finally, we present two case studies to demonstrate the effectiveness of our solution in support of multi-faceted visual analysis of text corpora.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652931,no,undetermined,0
Using Cloud Technologies to Optimize Data-Intensive Service Applications,"The role of data analytics increases in several application domains to cope with the large amount of captured data. Generally, data analytics are data-intensive processes, whose efficient execution is a challenging task. Each process consists of a collection of related structured activities, where huge data sets have to be exchanged between several loosely coupled services. The implementation of such processes in a service-oriented environment offers some advantages, but the efficient realization of data flows is difficult. Therefore, we use this paper to propose a novel SOA-aware approach with a special focus on the data flow. The tight interaction of new cloud technologies with SOA technologies enables us to optimize the execution of data-intensive service applications by reducing the data exchange tasks to a minimum. Fundamentally, our core concept to optimize the data flows is found in data clouds. Moreover, we can exploit our approach to derive efficient process execution strategies regarding different optimization objectives for the data flows.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558140,no,undetermined,0
Utilization of DLT Theory in 2D Image Analysis System,"This paper analyzed several mistakes in the traditional 2D analytic theories and then introduced in detail a new method basing on DLT theory from two aspects: the foundation stone of new 2D analytic theories, program design, and also utilized a direct inspection method to analyze the errors between the new method and the old one. The results showed that new method could obtain more accurate than the traditional one.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718313,no,undetermined,0
VACCINATED äóî Visual analytics for characterizing a pandemic spread VAST 2010 Mini Challenge 2 award: Support for future detection,"Given a set of hospital admittance and death records, the challenge was to characterize the spread of a pandemic in terms of the attack and mortality rates, spatiotemporal patterns of onset and the recovery time. We began the analysis by preprocessing the hospital admittance records using the University of Pittsburgh's CoCo classifier. CoCo is a text classification software that takes hospital admittance fields and classifies them into chief complaint categories (Botulinic, Constitutional, Gastrointestinal, Hemorrhagic, Neurological, Rash, Respiratory, and Other). The choice of the CoCo classifier was based on its online availability as well as its well documented classification performance metrics, see. Once the data was classified, we utilized and extended work done by the Purdue University Visual Analytics Center on healthcare analysis. Our system consists of a combination of linked views, showing time series views of syndromes and death rates through line graph views (Figure 1 - Top), stacked graph views showing deaths (Figure 1 - Bottom), geographical map views showing the impact by country (not illustrated in this paper), and summary windows providing statistical breakdowns of the data (not illustrated in this paper). All views are linked through an interactive time slider that allows users to explore the data over time. Extensions to our previous work include the stacked graph view, summary windows, new control chart methods, and an interactive `tape measure' tool.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652661,no,undetermined,0
Vision-Based Autonomous Vehicle Guidance for Indoor Security Patrolling by a SIFT-Based Vehicle-Localization Technique,"A novel method for guidance of vision-based autonomous vehicles for indoor security patrolling using scale-invariant feature transformation (SIFT) and vehicle localization techniques is proposed. Along-path objects to be monitored are used as landmarks for vehicle localization. The localization work is accomplished by three steps: SIFT-based object image feature matching, 2-D affine transformation using the Hough transform, and analytic 3-D space transformation. Object monitoring can be simultaneously achieved during the vehicle-localization process, and most planar-surfaced objects can be utilized in the process, greatly enhancing the applicability of the proposed method. Vehicle trajectory deviations from the planned path due to mechanic error accumulation are also estimated by setting up a calibration line on the monitored object image and applying the 3-D space transformation. Moreover, a path-correction technique is proposed to conduct a path adjustment and guide the vehicle to navigate to the next path node. Analysis of the accuracy of the vehicle-localization and path-correction results is finally included. The experimental results show that the proposed method, utilizing only a single view of each object, can guide the vehicle to navigate accurately and monitor objects successfully.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482146,no,undetermined,0
Visual analytics of a pandemic spread: VAST 2010 Mini Challenge 2 award: Thorough description of analytic process,"In this paper, automated medical data analysis and visualisation are discussed. The task of the VAST 2010 Mini Challenge 2 was to characterize the spread of an epidemic outbreak. The analysis should take into consideration symptoms, mortality rates and temporal patterns of the disease. Finally, the outbreak should be compared across different locations searching for anomalies. For the preprocessing and the automated analysis of the data we used the Konstanz Information Miner (KINME), which is a modular data exploration platform that enables the user to visually create dataflows, the R software for statistical computing and some selfwritten Java programs for data preprocessing. For conducting the visual investigations we applied Many Eyes and Protovis, which compose scalable and customized views for datasets of interest.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5653071,no,undetermined,0
Visualising Computational Intelligence Through Converting Data into Formal Concepts,"Formal Concept Analysis (FCA) is an emerging data technology that complements collective intelligence such as that identified in the Semantic Web by visualising the hidden meaning in disparate and distributed data. The paper demonstrates the discovery of these novel semantics through a set of FCA open source software tools FcaBedrock and In-Close that were developed by the authors. These tools add computational intelligence by converting data into a Boolean form called a Formal Context, prepare this data for analysis by creating focused and noise-free sub-Contexts and then analyse the prepared data using a visualisation called a Concept Lattice. The Formal Concepts thus visualised highlight how data itself contains meaning, and how FCA tools thereby extract data's inherent semantics. The paper describes how this will be further developed in a project called CUBIST, to provide in-data-warehouse visual analytics for RDF-based triple stores.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5662774,no,undetermined,0
VisWorks text and network visual analytics: VAST 2010 Mini Challenge 1 award: äóìEffective interactive visualization of document contentsäó,"VisWorks is a software package for text and network visual analytics. This paper introduces its visualization, analytic process and lesson learned in solving Mini-Challenge 1 of VAST 2010 contest.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5651204,no,undetermined,0
White's Three Disciplines and Relative Valuation Order: Countering the Social Ignorance of Automated Data Collection and Analysis,"This paper asks which of White's (2009) three disciplines and relative valuation orders does the Singapore blogosphere adhere to. Analysing not just the hyperlink connections but the textual discourse; and in doing so attempts to highlight certain limitations of using automated data mining and analysis software. Using the Singapore blogosphere, described by Lin, Sundaram, Chi, Tatemura, and Tseng, (2006) and Hurst (2006), as an isolated and distinct network with no theme or focus, I have targeted blogs using social network analysis uncovering the key players, with higher levels of `betweenness centrality' (de Nooy & Mrvar et al., 2005) and the themes and discipline of the Singapore blogosphere. This case study will help highlight the analytic framework, benefits and limitations of using social network analysis and an ethnographical approach to networks. This paper also highlights the use of various software technology; blogs, IssueCrawler, HTTrack, NetDraw, and Leximancer while using an ethnographic approach to counter the social ignorance of automated electronic software.",2010,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562788,no,undetermined,0
A Framework for Discovering Internal Financial Fraud Using Analytics,"In today's knowledge based society, financial fraud has become a common phenomenon. Moreover, the growth in knowledge discovery in databases and fraud audit has made the detection of internal financial fraud a major area of research. On the other hand, auditors find it difficult to apply a majority of techniques in the fraud auditing process and to integrate their domain knowledge in this process. In this Paper a framework called ""Knowledge-driven Internal Fraud Detection (KDIFD)"" is proposed for detecting internal financial frauds. The framework suggests a process-based approach that considers both forensic auditor's tacit knowledge base and computer-based data analysis and mining techniques. The proposed framework can help auditor in discovering internal financial fraud more efficiently.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5966462,no,undetermined,0
A Framework for Efficient Data Analytics through Automatic Configuration and Customization of Scientific Workflows,"Data analytics involves choosing between many different algorithms and experimenting with possible combinations of those algorithms. Existing approaches however do not support scientists with the laborious tasks of exploring the design space of computational experiments. We have developed a framework to assist scientists with data analysis tasks in particular machine learning and data mining. It takes advantage of the unique capabilities of the Wings workflow system to reason about semantic constraints. We show how the framework can rule out invalid workflows and help scientists to explore the design space. We demonstrate our system in the domain of text analytics, and outline the benefits of our approach.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123302,no,undetermined,0
A framework for evidence-based health care incentives simulation,"We present a general simulation framework designed for modeling incentives in a health care delivery system. This first version of the framework focuses on representing provider incentives. Key framework components are described in detail, and we provide an overview of how data-driven analytic methods can be integrated with this framework to enable evidence-based simulation. The software implementation of a simple simulation model based on this framework is also presented.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147833,no,undetermined,0
A fuzzy approach to bilingual education assessment and analysis,"This paper is concerned with the problem of applying fuzzy theory and analytic hierarchy process in bilingual education assessment and analysis. By the triangular fuzzy number theory, a bilingual education assessment model is developed. Based on this model, we can get the bilingual education assessment rating, and by analysis the estimation results, we can develop a scheme to guide the decision-makers to improve the bilingual education rating. Furthermore, the numerical example is given to show the effectiveness of the proposed theorems.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013797,no,undetermined,0
Analytics for similarity matching of IT cases with collaboratively-defined activity flows,"Handling IT support cases efficiently is very important for operational excellence of IT organizations. Many IT service centers receive thousands of cases per day, some of which are similar to previously reported cases. To improve efficiency it is important to build upon lessons learned from past cases in the resolution of new cases. Therefore, a desired functionality of case management tools is finding similar previous cases to an open one, in order to leverage information about previous cases to effectively find resolution. A new generation of tools for IT case management, e.g., IT Support Conversation Manager, enables collaborative and adaptive process definition for IT case resolution. Leveraging collaborative and social networking technology makes the case information model increasingly richer and more structured compared to flat textual format case reports in traditional IT case management systems. We have developed an automated method for matching IT support cases that takes into account multiple information attributes including the collaborative flow of activities during case handling. We evaluated the system and the early evaluation results show that this method achieves a higher accuracy and comparable efficiency to text-based similarity approaches.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5767639,no,undetermined,0
ANP-based fuzzy matter-element model for Gas Enterprise Risk Assessment,"The establishment of Gas Enterprise Risk Assessment Model is an important basis for Enterprises to implement risk management decisions. We make use of some research methods, such as systems analysis and empirical analysis for Chinese gas enterprises own characteristics and adopt a combination of ANP and fuzzy matter-element assessment method for the establishment of the urban gas enterprise risk assessment index system in the paper. Based on the risk assessment index system, combining with the gas corporate social welfare, market competition and other characteristics, we build the gas enterprise risk assessment model that took into account the complex network relationship among the various risk factors and their fuzziness, and it provides bases for the gas enterprise managers to do quantitative analysis of their own risk.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035375,no,undetermined,0
Mapping an epidemic outbreak: Effective analysis and presentation,"The microblog challenge presented an opportunity to use commercial software for visual analysis. An epidemic outbreak occurred in the city of Vastopolis, requiring visualizations of symptoms and their spread over time. Using these tools, analysts could successfully identify the outbreak's origin and pattern of dispersion. The maps used to analyze the data and present the results provided clear, easily understood representations, and presented a logical explanation of a complex progression of events.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102486,no,undetermined,0
Human centricity and perception-based perspective of architectures of Granular Computing,"In spite of their striking diversity, numerous tasks and architectures of intelligent systems such as those permeating multivariable data analysis (e.g., time series, spatio-temporal, and spatial dependencies), decision-making processes along with their models, recommender systems and others exhibit two evident commonalities. They promote human centricity and vigorously engage perceptions (rather than plain numeric entities) in the realization of the systems and their usage. Information granules play a pivotal role in such settings. In the sequel, Granular Computing delivers a cohesive framework supporting a formation of information granules and facilitating their processing. We exploit two essential concepts of Granular Computing. The first one, formed with the aid of a principle of justifiable granularity, deals with the construction of information granules. The second one, based on an idea of an optimal allocation of information granularity, helps endow constructs of intelligent systems with a very much required conceptual and modeling flexibility. The talk covers in detail two representative studies. The first one is concerned with a granular interpretation of temporal data where the role of information granularity is profoundly visible when effectively supporting human centric description of relationships existing in data. In the second study being focused on the Analytic Hierarchy Process (AHP) used in decision-making, we show how an optimal allocation of granularity helps facilitate collaborative activities (e.g., consensus building) in group decision-making.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016114,no,undetermined,0
"Evaluating human-automation interaction using task analytic behavior models, strategic knowledge-based erroneous human behavior generation, and model checking","Human-automation interaction, including erroneous human behavior, is a factor in the failure of complex, safety-critical systems. This paper presents a method for automatically generating task analytic models encompassing both erroneous and normative human behavior from normative task models by manipulating modeled strategic knowledge. Resulting models can be automatically translated into larger formal system models so that safety properties can be formally verified with a model checker. This allows analysts to prove that a human automation-interactive system (as represented by the formal model) will or will not satisfy safety properties with both normative and generated erroneous human behavior. This method is illustrated with a case study: the programming of a patient-controlled analgesia pump. In this example, a problem resulting from a generated erroneous human behavior is discovered and a potential solutions is explored. Future research directions are discussed.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083931,no,undetermined,0
Evaluation of a PET prototype using LYSO:Ce monolithic detector blocks,We have analyzed the performance of a PET demonstrator formed by two sectors of four monolithic detector blocks placed face-to-face. Both front-end and read-out electronics have been evaluated by means of coincidence measurements using a rotating <sup>22</sup>Na source placed at the center of the sectors in order to emulate the behavior of a complete full ring. A continuous training method based on neural network (NN) algorithms has been carried out to determine the entrance points over the surface of the detectors. Reconstructed images from 1 MBq <sup>22</sup>Na point source and <sup>22</sup>Na Derenzo phantom have been obtained using both filtered back projection (FBP) analytic methods and the OSEM 3D iterative algorithm available in the STIR software package [1]. Preliminary data on image reconstruction from a <sup>22</sup>Na point source with íÖ = 0.25 mm show spatial resolutions from 1.7 to 2.1 mm FWHM in the transverse plane. The results confirm the viability of this design for the development of a full-ring brain PET scanner compatible with magnetic resonance imaging for human studies.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152605,no,undetermined,0
Exact localization of acoustic reflectors from quadratic constraints,In this paper we discuss a method for localizing acoustic reflectors in space based on acoustic measurements on source-to-microphone reflective paths. The method converts Time of Arrival (TOA) and Time Difference of Arrival (TDOA) into quadratic constraints on the line corresponding to the reflector. In order to be robust against measurement errors we derive an exact solution for the minimization of a cost function that combines an arbitrary number of quadratic constraints. Moreover we propose a new method for the analytic prediction of reflector localization accuracy. This method is sufficiently general to be applicable to a wide range of estimation problems.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6082277,no,undetermined,0
Extracting Named Entities at Web Scale for Competitive Intelligence,"Summary form only given.Businesses of all sizes have now realized that the Web is an invaluable resource for competitive intelligence, and consequently business decision making. But many have trouble collecting targeted & useful information, and are often further overwhelmed by the time required for analysis & monitoring. On another hand, text mining techniques have become widely used for information analysis in the scientific community in general, and are now ubiquitous in most Web Intelligence fields. With the availability of services such as Google Prediction API, or mature open source software such as GATE, RapidMiner or NLTK, one can expect a much wider adoption of text mining and associated machine learning techniques by expert developers. But how can these techniques benefit to the daily life of a wider business audience? As competitive intelligence is often focused on products, people, customers and competitors, there is an added value for systems providing analytics on these entities, whose recognition is fundamental to text mining and semantic analysis, and consequently is still under active scientific investigation. In this talk we will tour some of the specific requirements and options for building an efficient Web based competitive intelligence system with named entity analytics. We will see how some savvy simplifications can help to overcome common issues such as Web scale and Web content noise, and finally deliver acceptable usability and value for non-specialists, business users.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040721,no,undetermined,0
Filter models of CDM measurement channels and TLP device transients,Charged Device Model (CDM) waveforms are fast enough to be altered considerably by the oscilloscope used. Step response of CDM measurement channels allow time-domain finite impulse response (FIR) filters to be formulated in software. Similar analytic methods allow filter-like representation of devices as measured by transmission-line pulsing (TLP).,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045616,no,undetermined,0
Finite Element Analysis on Vibration of Lifting Pipeline,"According to the theory of wave and current, the load of deep-sea mining lifting pipeline suffered in ocean environment is analyzed. Mechanical model of lifting pipeline is established by adopting Finite Element software of ANSYS, the stress of the first section of pipeline below sea level under combined effect of hydrostatic pressure, wave and current and plane stress of lifting pipeline at different water depth of 2000 m, 3000 m, 4000 m, 5000 m are studied in details, mode analysis of lifting pipeline of 5000 m length is also researched, and the first ten order natural frequency and modal shape are attained. Analytic results have an important significance on the design, check and choose of lifting pipeline, and it also provides some references for the further study on vibration mechanism and reduction.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721162,no,undetermined,0
Fluid controlled models of computer networks under denial of service attacks,In this paper we consider fluid controlled models of networks. We proposed an new approach to analyzing network work using fluid models and conflict control theory domain. We obtain analytic solution for certain class of networks and proved time optimality. In the network game we found Nash equilibrium. We consider special case of network activity - denial of service attack and found dependency on download time depending of attack direction and traffic volume. The theoretical results were tested in an simulation environment OMNET++.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6110970,no,undetermined,0
GPU-based beamformer: Fast realization of plane wave compounding and synthetic aperture imaging,"Although they show potential to improve ultrasound image quality, plane wave (PW) compounding and synthetic aperture (SA) imaging are computationally demanding and are known to be challenging to implement in real-time. In this work, we have developed a novel beamformer architecture with the real-time parallel processing capacity needed to enable fast realization of PW compounding and SA imaging. The beamformer hardware comprises an array of graphics processing units (GPUs) that are hosted within the same computer workstation. Their parallel computational resources are controlled by a pixel-based software processor that includes the operations of analytic signal conversion, delay-and-sum beamforming, and recursive compounding as required to generate images from the channel-domain data samples acquired using PW compounding and SA imaging principles. When using two GTX-480 GPUs for beamforming and one GTX-470 GPU for recursive compounding, the beamformer can compute compounded 512 í„ 255 pixel PW and SA images at throughputs of over 4700 fps and 3000 fps, respectively, for imaging depths of 5 cm and 15 cm (32 receive channels, 40 MHz sampling rate). Its processing capacity can be further increased if additional GPUs or more advanced models of GPU are used.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5995229,no,undetermined,0
High availability in Controller Area Networks,"The Controller Area Network (CAN) fieldbus is a popular technology for distributed embedded system networking. Its usage spans several domains, ranging from home automation to factory control. There are, however, restrictions to its application in a specific domain: highly dependable applications. A crucial step towards CAN-based dependable systems was taken by the CAN Enhanced Layer (CANELy) architecture. This architecture provided the analytic models of CAN operation. Based on these models, it defines both the hardware and software mechanisms for dependable and timely CAN-based network service provision. This paper provides the materialization of CANELy network availability mechanisms. The mapping of the error monitoring and fault-confinement mechanisms defined by CANELy into hardware has proven to be resource-effective, allowing their integration in an inexpensive Field Programmable Gate Array (FPGA) device. This opens room for cost-effective high dependability CAN-based solutions in several domains, e.g. the aerospace industry.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929399,no,undetermined,0
Innovation capacity appraisal of junior college engineering students based on AHP-TOPSIS,"Cultivating the practical capacity and technical operations of students should be attached the greatest importance by junior college, at the premise of ensuring theoretical knowledge during teaching process. Only equipped with innovation capacity, can engineering students combine theory and technology, engineering and science, school and social reality, so as to be able to meet the needs of society. In this study, Analytic Hierarchy Process (AHP) and Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS)) are employed. By sampling 30 students from the College of Henan Mechanical and Electrical majoring in Mechanical, the innovation evaluation model is established according to evaluation indicators as integrated analysis capacity, project planning capacity, program development capacity, project implementation capacity, innovation and development capacity, and organizational coordination capacity. At first, the attribute weight vectors of indicators shall be obtained by using AHP, and then the corresponding indicators shall be calculated by TOPSIS, so that the integrated evaluation result is given. The result shows that this method has a very good discrimination in innovation capacity evaluation of junior college engineering students.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6013782,no,undetermined,0
Application of technology combined GIS and transient electromagnetic technique in coal mine water burst prevention äóî A case of Sihe Coal mine,"Transient Electromagnetic Method (TEM) has the characters with sensitivity to water body, high resolution and less influence from topography. Geographic Information System (GIS) has the characters with powerful geospatial analytics, modeling and visualization. Measured data surveyed with TEM are loaded into ArcGIS, then multi-feature overlaid with the panel water-rich distribution map. Three-dimensional distribution of rich water was obtained. And then the water yield, water rich zone in front of roadway can be analyzed. The application of this technique in west area of Sihe Colliery was proved that this method was one effective methods in detecting the rich water distribution and range around mining and tunnelling face. And more the analytical results was visual. Reliable basis of prediction and prevention water burst was obtained.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980837,no,undetermined,0
"Interaction, Mediation, and Ties: An Analytic Hierarchy for Socio-Technical Systems","To understand how technological designs encourage synergistic encounters between people and ideas within socio-technical systems, techniques are needed to bridge between levels of description from process traces such as log data, through individual trajectories of activity that interact with each other, to dynamic networks of associations that are both created by and further shape these interactions. Towards this end, we have developed an analytic hierarchy and associated representations. Process traces are abstracted to contingency and uptake graphs: directed graphs that record observed relationships (contingencies) between events that offer evidence for interaction and other influences between actors (uptake). Contingency graphs are further abstracted to associograms: two-mode directed graphs that record how associations between actors are mediated by digital artifacts. Patterns in associograms summarize sequential patterns of interaction. Transitive closure of associograms yields sociograms, to which existing network analytic techniques may be applied. We discuss how the hierarchy bridges between theoretical levels of analysis.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718896,no,undetermined,0
Intraoperative device targeting using real-time MRI,"Real-time MRI has the potential to significantly improve intraoperative surgical processes. However, development of these technologies is complicated by the need for software tools that support task-oriented visualization of 2D and 3D datasets, enable flexible real-time image processing pathways, and interface with proprietary scanner hardware. Developing such tools requires substantial software engineering effort for these low-level tasks that distracts from a focus on developing and implementing algorithms that relate to the surgical technique in question. RTHawk and Vurtigo are extensible software platforms that simplify the development of tools for performing real-time MRI-guided procedures. We present here initial work on a system for performing intracerebral drug infusion under real-time MRI guidance using the Navigus pivot point-based MRI-compatible external trajectory guide. Initial testing performed using a GE scanner targeting a rigid object in a water-filled phantom allowed realtime aiming and infusion monitoring with 1 mm targeting accuracy. In vivo testing is forthcoming.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872335,no,undetermined,0
Keynote Abstracts,The following topics are dealt with: e-business engineering; data management; service-oriented knowledge management; emergency communications; multi-agent supply chain collaboration operation model; cloud computing; business analytics; business optimization; Web service selection; user centric security mode; mobile commerce; pervasive commerce; service engineering; software engineering; warehousing approach; RFID; process automation; nonintrusive load monitoring system; e-marketplace integration; e-marketplace interoperability; cloud services; and business intelligence.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104586,no,undetermined,0
"Knowledge Extraction and Reuse within ""Smart"" Service Centers","In this paper, we describe the initial version of a text analytics system under development and use at Cisco, where the objective is to ""optimize"" the productivity and effectiveness of the service center. More broadly, we discuss the practical needs in industry for developing powerful ""Smart"" Service Centers and the gaps in research to meet these needs. Ideally, service engineers in service centers should be utilized to handle issues which have not been solved previously and machines should be used to solve problems already solved, or at least help the service engineers obtain pertinent information from related and solved service cases when responding to a new request. Such a role for a machine would be a core element of the ""Smart Services"" offering. Hence, design of a highly efficient human-machine combination to derive insights from text and respond to a user request, is critical and fundamental, this enables service agents to capture relevant information quickly and accurately, and to develop the foundation for upper layer applications. Despite extensive earlier literature, the optimization for service process that involves very long, unstructured documents referencing a number of technology and product related terms with implicit inter-relationships has not been fully investigated. Our approach enables firms such as Cisco to achieve efficient service delivery by automating knowledge extraction to support ""Self Service"" by end users. The Cisco text analytics system termed Service Request Analyzer and Recommender (SRAR) addresses gaps in the Support Services function, by optimizing the use of human resources and software analytics in the service delivery process. The Analyzer is able to handle complex service requests (SRs) and to present categorized and pertinent information to service agents, based on which the Recommender, an upper layer application, is built to retrieve similar solved SRs, when presented with a new request. Our contributions in the context of- - text analysis and system design are three-fold. First, we identify the elements of the diagnostic process underlying the creation of SRs, and design a hierarchical classifier to decompose the complex SRs into those elements. Such decomposition provides specific information from the functional perspectives about ""What was the problem?"" ""Why did it occur?"" and ""How was it solved?"" which assists service agents in acquiring the knowledge they need more effectively and rapidly. Second, we build an SR Recommender on top of SR Analyzer to extend the system functionality for improved knowledge reuse, to measure SR similarity for more accurate recommendation of SRs. Third, we validate our SRAR in an initial pilot study in the service center for Cisco network diagnostics and support, and demonstrate the effectiveness and extensibility of our system. Our system appears applicable to the service centers across multiple domains, including networks, aerospace, semiconductors, automotive, health care, and financial services, and potentially adapted and expanded to all the other business functions of an enterprise. We conclude by indicating open research problems and new research directions, to expand the set of problems that need to be addressed in developing a Smart Support Services capability, and the solutions required to achieve them. These include the capture, retrieval, and reuse of more refined, structured and granulated knowledge, as well as the use of forum threads and semi-automated, dynamic categorization, together with considerations of the optimal use of humans and machine learning based software. Other aspects we discuss include recommendation systems based on temporal pattern clustering and incentives for experts to permit their expertise to be captured for machine (re-)use.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958085,no,undetermined,0
"Landslide vulnerability evaluation: A case study from Sichuan, China","Selecting Sichuan Province as the study area, setting rainfall-induced shallow landslide hazard for the research objectives, choosing slope, fault zone, rivers, lithology, soil type and vegetation as the main evaluation factors of the environmental vulnerability assessment. Based on the landslides disaster database since 5.12 Earthquake in Sichuan Province, the weights of the evaluation factors are determined. Producing 2008-2010 environmental vulnerability assessment maps in Sichuan Province Using Analytic Hierarchy Process with the GIS software, and the Spatial resolution of which is 250 meters. The risk of the environmental vulnerability assessment maps are divided into five levels: Slight(I), mild(II), moderate(III), high(IV) and the highest (V) risk areas. From this study, we conclude that vulnerability is with temporal and spatial distribution, and it can be quantified effectively using AHP model.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981061,no,undetermined,0
LAR-CC: Large atomic regions with conditional commits,"HW/SW Co-designed systems rely on dynamic binary translation and optimizations for efficient execution of binary code. Due to memory ordering properties and other architectural constraints, most binary optimizations are applied to regions of code that are atomically executed. To ensure that the underlying hardware has enough speculative resources to execute the whole atomic region, these systems typically form short atomic regions, with only 20 to 30 instructions. However, the shorter is the atomic region the smaller is the scope for optimizations. We present LAR-CC, a novel technique that enables HW/SW co-designed systems to optimize large atomic regions and dynamically fit them into the available speculative hardware resources by means of conditional commits. The LAR-CC technique consists of two major components: 1) conditional branch instructions to conditionally skip commit operations; 2) code transformations that replace commit operations by conditional commits and enable optimizations to be applied on the large atomic regions. Our experiments show that LAR-CC can effectively achieve dynamic atomic region sizes larger than 1000 instructions, providing sufficiently large scope to apply many advanced optimizations on HW/SW co-designed systems.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5764674,no,undetermined,0
Large in-memory cyber-physical security-related analytics via scalable coherent shared memory architectures,"Cyber-physical security-related queries and analytics run on traditional relational databases can take many hours to return. Furthermore, programming analytics on distributed databases requires great skill, and there is a shortage of such talent worldwide. In this talk on computational intelligence within cyber security, we will review developments of processing large datasets in-memory using a coherent shared memory approach. The coherent shared memory approach allows programmers to view a cluster of servers as a system with a single large RAM. By hiding the actual system architecture under a software layer, we proffer a more intuitive programming model. Furthermore, the design of applications is äóìtimelessäó since hardware upgrades require no changes to the software. The advantages of shared memory are countered by some disadvantages in that race conditions can occur; however, in many of these cases, we can provide models that protect us against such problems. Exemplars include sensemaking of Twitter feeds, the processing of Smart Meter datasets, and the large scale simulation of the caching of files at disparate points around the globe.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949414,no,undetermined,0
Magnetic field analysis of bearingless permanent magnet motor,"As bearingless motors have all advantages of magnetic bearings; they play an important role in solving difficult problems of special electric drives. The simplest and most versatile form of bearingless motor is the bearingless slice rotor motor. This kind of motor with centrifugal pump has a wide range of applications in life science, chemical processing industry, semiconductor manufacturing industry, foodstuff processing industry, and so on. This paper presents the analysis of the operational principle, basic equations is given for a bearingless permanent magnet slice electromotor first. Then a modeling of a bearingless permanent magnet slice electromotor is built using Maxwell 3D of ANSOFT finite element software. Finally, the magnetic field and the torque force are analyzed for the two different structures of P<sub>B</sub>=P<sub>M</sub>Œ±1. According to the analytic results, the structure of P<sub>B</sub>=P<sub>M</sub>+1 is more steady. In addition; its control is easier to get in the actual implementation and application.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5768466,no,undetermined,0
Managing Procurement Spend Using Advanced Compliance Analytics,"Often the processes for purchasing commodities and services within a business enterprise are centralized into a procurement organization. These purchases are often sourced from one or more suppliers, or vendors, based on contract terms and conditions (such as price, payment terms etc.), availability, and quality or legacy habit of purchasing service with known vendors. We have found that many organizations lack appropriate processes and disciplines to drive demand to preferred suppliers. Thus these enterprises are unable to leverage the value of the pre-negotiated contracts due to lack of process education, approval process steps or appropriate purchasing tools that could result in significant amounts of spending that would be considered not compliant (not being sourced through preferred suppliers). Depending upon the size of the organization, such transactions range from several million dollars to billions of dollars. Manually sifting or employing typical query tools to review large amounts of spend transaction data with multiple attributes to identify the level of non compliant spend and identify areas to take action is a daunting task. In this paper, we discuss a software solution for spend compliance analytics that includes measurements of cost savings due to increased compliance and identification of areas where spend tends to be non compliant. We have developed a web enabled advanced analytical solution called Compliance Analytics Tool (CAT) that embeds a two phase methodology for compliance management. In the first phase, we use advanced data mining techniques to segment a large amount of historical spend transactions to quickly identify promising areas of improvement, exploiting a multitude of purchasing attributes such as business unit, procurement category, suppliers, etc. The second phase employs portfolio optimization techniques to further focus on specific segments that provide maximum benefit based on desired compliance targets or available budget. We- also discuss the solution architecture that integrates business analytics along with business intelligence tools, dashboards, and data warehousing.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104610,no,undetermined,0
Ensuring tight computational security against higher-order DPA attacks,"While DES has been proven to be breakable within a day given sufficient computational power, AES is still in use because it is extremely resistant to cryptanalytic attacks. Power Analytic Attacks use power consumption traces of the hardware or software implementation of these algorithms to reduce search space exponentially in the size of the key, thereby making computational complexity several orders of magnitude lower. This paper analyzes the increase in the computational advantage of an adversary who uses DPA and higher order power analysis attacks as opposed to algorithmic cryptanalysis. We highlight why there can be no perfect masking against DPA, and then define a standard for the security of masking countermeasures to such attacks. The main contribution is a security metric for systems and a cut-off for the number of encryptions allowable for a given order of masking to make the system immune to higher order DPA attacks.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5971970,no,undetermined,0
Enabling Analysis and Measurement of Conventional Software Development Documents Using Project-Specific Formalism,"We describe a new approach to modeling and analyzing software development documents that are typically written using conventional office applications. Our approach brings automation to content extraction, quality checking and measurement of massive document artifacts that tend to be handled by labor-intensive manual work in industry today. Rather than seeking an approach based on creation or rewriting of contents using more rigid, machine-friendly representations such as standardized formal models and restricted languages, we provide a method to deal with the diversity of document artifacts by making use of project-specific formalism that exists in target documents. We demonstrate that such project-specific formalism often tends to ""naturally"" exist at syntactic levels, and it is possible to define a ""document model"", a logical data representation gained by transformation rule from the physical, syntactic structure to the logical, semantic structure. With this transformation, various quality checking rules for completeness, consistency, traceability, etc., are realized by evaluating constraints for data items in the logical structure, and measurement of these quality aspects is automated. We developed a tool to allow a user to easily define document models and checking rules, and provide the insights on transformations when defining document models for various industry specification documents written in word processor files, spreadsheets and presentations. We also demonstrate the use of natural language processing can improve document modeling and quality checking by compensating for a weakness of formalism and applying analysis to specific parts of the target documents.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113043,no,undetermined,0
Efficient Fault Detection and Diagnosis in Complex Software Systems with Information-Theoretic Monitoring,"Management metrics of complex software systems exhibit stable correlations which can enable fault detection and diagnosis. Current approaches use specific analytic forms, typically linear, for modeling correlations. In practice, more complex nonlinear relationships exist between metrics. Moreover, most intermetric correlations form clusters rather than simple pairwise correlations. These clusters provide additional information and offer the possibility for optimization. In this paper, we address these issues by using Normalized Mutual Information (NMI) as a similarity measure to identify clusters of correlated metrics, without assuming any specific form for the metric relationships. We show how to apply the Wilcoxon Rank-Sum test on the entropy measures to detect errors in the system. We also present three diagnosis algorithms to locate faulty components: RatioScore, based on the Jaccard coefficient, SigScore, which incorporates knowledge of component dependencies, and BayesianScore, which uses Bayesian inference to assign a fault probability to each component. We evaluate our approach in the context of a complex enterprise application, and show that 1) stable, nonlinear correlations exist and can be captured with our approach; 2) we can detect a large fraction of faults with a low false positive rate (we detect up to 18 of the 22 faults we injected); and 3) we improve the diagnosis with our new diagnosis algorithms.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714701,no,undetermined,0
"Developing a common analytical framework for models, simulations, and data","DHS has recently established the Analytic Capability Development Working Group (ACDWG) whose aim is to reduce the lifecycle and capital investment costs of analytic efforts while continually enhancing the ability to inform decision-makers within mandated timeframes. Specifically, the working group is helping to develop a common analytic frameworks in order to address the challenges that users and developers of analytic tools face, including increasing awareness around existing analytical capabilities, spurring collaborations that reduce stove-piping and duplication of analytic effort, creating consistent, transparent, defensible, and shared knowledge, and streamlining development efforts at all levels. In this talk we discuss some of the challenges of developing a common analytic framework including models, simulations, and data.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928660,no,undetermined,0
Assessment of the Groundwater Renewability in Beijing Plain Area,"Groundwater is the main water source of Beijing plain area. It plays a vital role in the safety of water supply. Assessment of the groundwater renewability in study area is important to its water resources sustainable utilization. In this study, 5 thematic maps including precipitation, precipitation infiltration coefficient, aquifer structure, slope and, drainage density were selected to integrate using ArcGIS software. Standardization of index values is expressed by membership function. The weights were assigned by analytic hierarchy process (AHP). The result of groundwater renewability zone shows groundwater renewability of Beijing plain area decreases from alluvial fans top to fringe and it is very low in the centre of study area for the small precipitation infiltration coefficient.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5780854,no,undetermined,0
"Automated computer network defence technology demonstration project (ARMOUR TDP): Concept of operations, architecture, and integration framework","Modern militaries rely heavily on computer networks and they have become a part of the critical infrastructure that must be protected. Computer networks, both military and non-military, are constantly being attacked and data about new vulnerabilities and attacks must be analyzed and processed at a speed that enables timely mitigation. The ARMOUR Technology Demonstration Project (TDP) is a five-year activity that will demonstrate automated Computer Network Defence (CND) capabilities based on the Observe, Orient, Decide and Act (OODA) decision process. We present the ARMOUR TDP Concept of Operations and Architecture, and promote an open source integration framework for collaborative development.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107865,no,undetermined,0
Automatic Dent-landmark detection in 3-D CBCT dental volumes,"Orthodontic craniometric landmarks provide critical information in oral and maxillofacial imaging diagnosis and treatment planning. The Dent-landmark, defined as the odontoid process of the epistropheus, is one of the key landmarks to construct the midsagittal reference plane. In this paper, we propose a learning-based approach to automatically detect the Dent-landmark in the 3D cone-beam computed tomography (CBCT) dental data. Specifically, a detector is learned using the random forest with sampled context features. Furthermore, we use spacial prior to build a constrained search space other than use the full three dimensional space. The proposed method has been evaluated on a dataset containing 73 CBCT dental volumes and yields promising results.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6091532,no,undetermined,0
Automation for creating and configuring security manifests for hardware containers,Hardware containers provide fine-grained memory access control to isolate memory regions and sandbox memory references between components of an application. A hardware reference monitor enforces a security manifest of memory access permissions for the currently executing component. In this paper we discuss how automation tools can help software developers to create the security manifest that configures hardware containers. The goal of this work is to foster discussion about our proposals for automation tools: to date we know of no solutions for extracting the metadata (permissions) required for fine-grained memory access control.,2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111677,no,undetermined,0
Autonomic management of client concurrency in a distributed storage service,"A distributed autonomic system adapts its constituent components to a changing environment. This paper reports on the application of autonomic management to a distributed storage service. We developed a simple analytic model which suggested potential benefit from tuning the degree of concurrency used in data retrieval operations, to suit dynamic conditions. We then validated this experimentally by developing an autonomic manager to control the degree of concurrency. We compared the resulting data retrieval performance with non-autonomic versions, using various combinations of network capacity, membership churn and workload patterns. Overall, autonomic management yielded improved retrieval performance. It also produced a distinct but not significant increase in network usage relative to one non-autonomic configuration, and a significant reduction relative to another.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5990521,no,undetermined,0
Boosting text extraction from biomedical images using text region detection,"In this paper, we show that domain-optimized text detection in biomedical images is important for boosting text extraction recall via off-the-shelf OCR engines. Methodologically, we contrast OCR performance when processing raw biomedical images, compared to preprocessing those images, and performing OCR on detected image text regions only. To quantify OCR extraction results, we rely on a gold standard image text corpus with manually identified image text strings. To demonstrate the positive effect on biomedical image retrieval, we apply image text detection and extraction to a large corpus of biomedical images in the Yale Image Finder system. We show that improved text extraction results in the retrieval of a larger number of relevant images for a set of domain-relevant keyword searches.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872319,no,undetermined,0
Boundary element analysis of rolling tire noise,"The vibration and radiated noise characteristics of rolling tire was analyzed by virtual prediction method. The acceleration response spectrum of rolling tire was acquired through the method of analyzing the dynamic response under the road impact excitations by the modal superposition approach. The acceleration date of rolling tire surface were imported to the acoustics analytic software as the input load of the boundary element model, and the noise radiated from rolling tire by vibration were calculated. The disrupted and characters of the noise in different the tread pattern and road surface was discussed.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199601,no,undetermined,0
Business Intelligence in the Cloud,"Business Intelligence (BI) deals with integrated approaches to management support. Currently, there are constraints to BI adoption and a new era of analytic data management for business intelligence these constraints are the integrated infrastructures that are subject to BI have become complex, costly, and inflexible, the effort required consolidating and cleansing enterprise data and Performance impact on existing infrastructure / inadequate IT infrastructure. So, in this paper Cloud computing will be used as a possible remedy for these issues. We will represent a new environment atmosphere for the business intelligence to make the ability to shorten BI implementation windows, reduced cost for BI programs compared with traditional on-premise BI software, Ability to add environments for testing, proof-of-concepts and upgrades, offer users the potential for faster deployments and increased flexibility. Also, Cloud computing enables organizations to analyze terabytes of data faster and more economically than ever before. Business intelligence (BI) in the cloud can be like a big puzzle. Users can jump in and put together small pieces of the puzzle but until the whole thing is complete the user will lack an overall view of the big picture. In this paper reading each section will fill in a piece of the puzzle.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014351,no,undetermined,0
Cataloga: A Software for Semantic-Based Terminological Data Mining,"This paper is focused on Catalog a, a software package based on Lexicon-Grammar theoretical and practical analytical framework and embedding a ling ware module built on compressed terminological electronic dictionaries. We will here show how Catalog a can be used to achieve efficient data mining and information retrieval by means of lexical ontology associated to terminology-based automatic textual analysis. Also, we will show how accurate data compression is necessary to build efficient textual analysis software. Therefore, we will here discuss the creation and functioning of a software for semantic-based terminological data mining, in which a crucial role is played by Italian simple and compound-word electronic dictionaries. Lexicon-Grammar is one of the most profitable and consistent methods for natural language formalization and automatic textual analysis it was set up by French linguist Maurice Gross during the '60s, and subsequently developed for and applied to Italian by Annibale Elia, Emilio D'Agostino and Maurizio Martin Elli. Basically, Lexicon-Grammar establishes morph syntactic and statistical sets of analytic rules to read and parse large textual corpora. The analytical procedure here described will prove itself appropriate for any type of digitalized text, and will represent a relevant support for the building and implementing of Semantic Web (SW) interactive platforms.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061017,no,undetermined,0
City sentinel - VAST 2011 mini challenge 1 award: äóìOutstanding integration of computational and visual methodsäó,"We present City Sentinel, an in-house built visual analytic software capable of handling a large collection of textual documents by combining diverse text mining and visualization tools. We applied this tool for the Vast Challenge 2011, Mini Challenge 1 over millions of tweet messages. We demonstrate how City Sentinel aided the analyst in retrieving the hidden information from the tweet messages to analyze and locate a hypothetical epidemic outbreak.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102485,no,undetermined,0
CloudChecker: An imperative framework for cloud configuration management,"Summary form only given. Cloud computing became one of the major research areas recently. The interest in cloud computing increases day by day because of the features provided by cloud providers. Pay-as-you-go is one of these features that attract customers to adopt this idea. Another feature is providing different levels of services to the customers; Software, Platform, and Infrastructure as a Service are the major services provided by clouds.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111666,no,undetermined,0
Colored Petri nets model based conformance test generation,"A novel Colored Petri Nets (CP-nets) model based test case generation approach is proposed to makes the best of advantages of the ioco testing theory and the CP-nets modeling, where the Conformance Testing orientated CP-nets (CT-CPN) is proposed for modeling certain software systems, and PN-ioco relation is defined as a new conformance relation, and finally test cases are generated through simulating the system CT-CPN models. CP-nets model simulation based test generation approach reflects the data-dependent control flow of the system behaviors, so all test cases are completely feasible for the actual test executions. Besides, better formal modeling and analytic capabilities in CP-nets modeling quite facilitate validating the accuracy of the system CT-CPN model. For effectively extending the applicability of the Petri nets based testing technologies, our novel CT-CPN model based test generation approach may well become a competent choice.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5983967,no,undetermined,0
Computational simulation of rotating noise of fan,"A rotating fan could produce noise of both broadband and discrete frequency. The discrete part typically consists of shaft frequency, blade passage frequency (BPF) and their higher harmonics. The shaft-order noise, which is mainly induced by unbalanced rotating, is often hard to be eliminated. In this paper, the rotating noise caused by geometric asymmetry is discussed and predicted. Analytic point force model is firstly used to predict the noise at points in different directions. Then a hybrid method based on computational fluid dynamic (CFD) simulation and FW-H equation is used to predict the noise of a simplified fan model. Compared with the case of symmetric rotating, a case of asymmetric rotating is studied, and the results show that geometric asymmetry could cause the increase of fan noise considerably at certain frequencies.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014371,no,undetermined,0
CONFLuEnCE: Implementation and application design,"Data streams have become pervasive and data production rates are increasing exponentially, driven by advances in technology, for example the proliferation of sensors, smart phones, and their applications. This fact effectuates an unprecedented opportunity to build real-time monitoring and analytics applications, which when used collaboratively and interactively, will provide insights to every aspect of our environment, both in the business and scientific domains. In our previous work, we have identified the need for workflow management systems which are capable of orchestrating the processing of multiple heterogeneous data streams, while enabling their users to interact collaboratively with the workflows in real time. In this paper, we describe CONFLuEnCE (CONtinuous workFLow ExeCution Engine), which is an implementation of our continuous workflow model. CONFLuEnCE is built on top of Kepler, an existing workflow management system, by fusing stream semantics and stream processing methods as another computational domain. Furthermore, we explicate our experiences in designing and implementing real-life business and scientific continuous workflow monitoring applications, which attest to the ease of use and applicability of our system.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6144803,no,undetermined,0
Continuous remote vital sign/environment monitoring for returning soldier adjustment assessment,"A three-stage study to develop and test an unobtrusive room sensor unit and subject data management system to discover correlation between sensor-based time-series measurements of sleep quality and clinical assessments of combat veterans suffering from Post-traumatic Stress Disorder (PTSD) and mild Traumatic Brain Injury (TBI), is described. Experiments and results for testing sensitivity and robustness of the sensor unit and data management protocol are provided. The current sensitivity of remote vital sign monitoring system is below 20% and 10% for respiration and heart rates, respectively.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6090419,no,undetermined,0
Contractor selection using fuzzy comparison judgement,"A model is proposed for selecting the optimal result of contractor selection under multi criteria environment. Fuzzy comparing judgment is used to tackling the vagueness and uncertainty in choosing significant preferences by decision maker regarding to the subjective opinion. Finally, the model was tested in tender evaluation processes for awarding the most beneficial contractor to perform the construction project.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140703,no,undetermined,0
Data intensive architecture for scalable cyber analytics,"Cyber analysts are tasked with the identification and mitigation of network exploits and threats. These compromises are difficult to identify due to the characteristics of cyber communication, the volume of traffic, and the duration of possible attack. In this paper, we describe a prototype implementation designed to provide cyber analysts an environment where they can interactively explore a month's worth of cyber security data. This prototype utilized On-Line Analytical Processing (OLAP) techniques to present a data cube to the analysts. The cube provides a summary of the data, allowing trends to be easily identified as well as the ability to easily pull up the original records comprising an event of interest. The cube was built using SQL Server Analysis Services (SSAS), with the interface to the cube provided by Tableau. This software infrastructure was supported by a novel hardware architecture comprising a Netezza TwinFin for the underlying data warehouse and a cube server with a FusionIO drive hosting the data cube. We evaluated this environment on a month's worth of artificial, but realistic, data using multiple queries provided by our cyber analysts. As our results indicate, OLAP technology has progressed to the point where it is in a unique position to provide novel insights to cyber analysts, as long as it is supported by an appropriate data intensive architecture.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107901,no,undetermined,0
Defining malware families based on analyst insights,"Determining whether arbitrary files are related to known malicious files is often useful in network and host-based defense. Doing so can give network defenders sufficient exemplars of a particular threat to develop comprehensive signatures and heuristics for identifying the threat, leading to decreased response time and improved prevention of a cyber attack. Identifying these malicious families is a complex process involving the categorization of potentially malicious code into sets that share similar features, while being distinguishable from unrelated threats or non-malicious code. Current methods for automatically or manually describing malware families are typically unable to distinguish between indicators derived from the structure of the malware and indicators derived from the behavior of the malware. Further, attempts to cluster potentially related files by mapping them into alternate domains, including histograms, fuzzy hashes, Bloom filters, and so on often produces clusters of files solely derived from structural information. These similarity measurements are often very effective on crudely similar files, yet they fail to identify files that have similar or identical behavior and semantics. We propose an analytic method, driven largely by human experience and based on objective criteria, for assigning arbitrary files membership in a malicious code family. We describe a process for iteratively refining the criteria used to select a malicious code family, until such criteria described are both necessary and sufficient to distinguish a particular malicious code family. We contrast this process with similar processes, such as antivirus signature generation and automatic and blind classification methods. We formalize this process to describe a roadmap for practitioners of malicious code analysis and to highlight opportunities for improvement and automation of both the process and the observation of relevant criteria. Finally, we provide experimental results of- applying this methodology to real-world malware.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6107902,no,undetermined,0
Design and implementation of a data analytics infrastructure in support of crisis informatics research: NIER track,"Crisis informatics is an emerging research area that studies how information and communication technology (ICT) is used in emergency response. An important branch of this area includes investigations of how members of the public make use of ICT to aid them during mass emergencies. Data collection and analytics during crisis events is a critical pre-requisite for performing such research, as the data generated during these events on social media networks are ephemeral and easily lost. We report on the current state of a crisis informatics data analytics infrastructure that we are developing in support of a broader, interdisciplinary research program. We also comment on the role that software engineering research plays in these increasingly common, highly-interdisciplinary research efforts.",2011,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032533,no,undetermined,0
Authorship and Documentary Boundary Objects,"Earlier research on documentary boundary objects has underlined the contextual nature of the process of their emergence. The aim of this paper is to discuss how the process of making and the attribution or non-attribution of authorship affects documentary boundary objects. A better understanding of the making of boundary objects is helpful in understanding why and how particular boundary objects work, and what are their implications. The article proposes an analytic model of four modes of authorship of documentary boundary objects (1. solitary, and 2. emergent authorship, 3. light-weight, and 4. heavy-weight peer-production) based on a review and synthesis of the spectrum of solitary and collaborative practices of creating documentary boundary objects discussed in the literature.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149084,no,undetermined,0
Axial-Flux-Machine Modeling With the Combination of FEM-2-D and Analytical Tools,"This paper deals with the development of analysis tools for axial-flux permanent-magnet machines. Normally, the study of this kind of machine involves three-dimensional (3-D) finite element method (FEM) (FEM-3-D) due to the 3-D nature of the magnetic problem. As it is widely known, the FEM-3-D software could take too much time, and both definition and solving processes of the problem may be very arduous. In this paper, a novel analysis procedure for axial-flux synchronous machines is proposed. This method consists in the combination of 2-D FEM simulations with analytical models based on the Fourier-series theory. The obtained results prove that the proposed method could be a very interesting option in terms of time and accuracy.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200327,no,undetermined,0
Equivalent radius analytic formulas of Substrate Integrated Cylindrical Cavity,"Equivalent radius of a Substrate Integrated Cylindrical Cavity (SICC) is an important parameter for calculating the resonant frequency and designing a SICC device. In this paper, based on two sets of equivalent width analytic formulas of Substrate Integrated Waveguide (SIW), the equivalent radius analytic formulas of SICC were derived by the conformal transformation method for the first time. In order to verify the validity of these formulas, using the formulas given in this paper calculated the equivalent radii and the corresponding resonant frequencies of TM<sub>010</sub> mode SICCs, and the resonant frequencies of TM<sub>010</sub> mode SICCs were also calculated by the method introduced in [12] and simulated by electromagnetic simulation software. The results show that the formulas given in this paper have higher precision, and are convenient, suitable to more application fields, so they will play important role in the analysis and design of SICC devices.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6717586,no,undetermined,0
A Workflow Framework for Big Data Analytics: Event Recognition in a Building,"This paper studies event recognition in a building based on the patterns of power consumption. It is a big challenge to identify what kinds of events happened in a building without additional devices such as camera and motion sensors, etc. Instead, we learn when and how the events happened from the historical record of power consumption and apply the lesson into the design of an event recognition system (ERS). The ERS will find out abnormal power usage to avoid wasting power, which leads to the energy savings in a building. The ERS involves big data analytics with a large size of dataset collected in a real time. Such a data intensive system is usually viewed as a workflow. A workflow management is a significant task of the system requiring data analysis in terms of the system scalability to maintain high throughput or fast speed analysis. We propose a workflow framework that allows users to perform remote and parallel workflow execution, whose tasks are efficiently scheduled and distributed in cloud computing environment. We run the ERS as a target system for the proposed framework with power consumption data (whose size is approximately 20GB or more) collected from each of over 240 rooms in a building at Dept. of Engineering, Tokyo University in 2011. We show that the proposed framework accelerates the speed of data analysis by providing scaling infrastructure and parallel processing feature utilizing cloud computing technologies. We also share our experience and results on the big data analytics and discuss how the studies contribute to achieve Green Campus.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6655671,no,undetermined,0
"A Precision Information Environment (PIE) for emergency responders: Providing collaborative manipulation, role-tailored visualization, and integrated access to heterogeneous data","During a crisis, emergency responders must rapidly integrate information from many separate sources to satisfy their role-specific needs and to make time-sensitive decisions. Responders currently receive this information through numerous software applications and face the challenge of integrating this heterogeneous data into an all-encompassing picture. Individual responders with distinct roles, such as police, fire, and EMS, often have very different information needs, but existing tools do not provide individual tailoring of workspaces to support this need. As responders communicate using text, voice, or other multimodal collaboration systems, important details can also be lost or become stale over time. This paper describes our approach to developing a Precision Information Environment (PIE) that: (1) streamlines access to multiple information resources by fusing heterogeneous information for presentation through a single access point; (2) supports role-tailorable understanding of the unified data sources through a flexible workspace; and (3) supports collaboration between teams of local and distributed responders by providing a work environment that allows teams to share and manipulate dynamic data sources in real time. We also describe our initial results from a usability evaluation of the system with subject matter experts.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6699100,no,undetermined,0
A research of critical factors in the the enterprise adoption of cloud service,"Cloud is essentially a flexible and scalable model for the way IT services are delivered and consumed. There are lots of advantages to using cloud computing for international companies. One of the major ones is the flexibility that it offers. Cloud computing means that users can access the files and data that they need even when they're working remotely. Since the popularity of cloud service, businesses are looking for cloud solutions to solve some of their biggest business and technology challenges: reducing costs, creating new levels of efficiency, and facilitating innovative business models. To understand what that means to the business, the benefits and potential risks of migrating to cloud services need to be carefully considered. The suitable business model is therefore becoming the focus as adopting the cloud service. The study is based on the Critical Factor Index with AHP method which had here a different function than it usually does. Basically all the other results given by the Critical Factor Index are based on a questionnaire inside the companies. In this case, the study was carried out by using external experts. The research outcome will conclude the critical success factors and new business model of cloud service adoption. We also discuss the factors that make cloud computing an attractive option for enterprises.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6765485,no,undetermined,0
A Scalable Distributed Framework for Efficient Analytics on Ordered Datasets,"One of the most common datasets used by many corporations to gain business intelligence is event log files. Oftentimes, the records in event log files are temporally ordered, and need to be grouped by user ID with the temporal ordering preserved to facilitate mining user behaviors. This kind of analytical workload, here referred to as Relative Order-preserving based Grouping (RE-ORG), is quite common in big data analytics. Using MapReduce/Hadoop for executing RE-ORG tasks on ordered datasets is not efficient due to its internal sort-merge mechanism. In this paper, we propose a distributed framework that adopts an efficient group-order-merge mechanism to provide faster execution of RE-ORG tasks. We demonstrate the advantage of our framework by comparing its performance with Hadoop through extensive experiments on real-world datasets. The evaluation results show that our framework can achieve up to 6.3í„ speedup over Hadoop in executing RE-ORG tasks.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6809349,no,undetermined,0
A Software Suite for Efficient Use of the European Qualifications Framework in Online and Blended Courses,"Since introduction of the European qualifications framework (EQF) as one instrument to bridge from learning institutions to competence driven lifelong learning, it remains a challenge for instructors and teachers in higher education to make efficient use of this framework for designing, monitoring, and managing their lessons. This paper presents a software suite for enabling teachers to make better use of EQF in their teaching. The software suite extends course design based on well-defined learning outcomes, monitoring performance and competence acquisition according to the EQF levels, assessment using scoring rubrics of EQF levels and competences in a 360-degree feedback, as well as visualizations of learning analytics and open student models in dashboards for different social perspectives in social planes. This paper includes a case study with 20 teachers who used the software suite in all phases of the course lifecycle for three programming courses. The results show that integrated applications for adopting the EQF in teaching practice are strongly needed. These results also show that the suite can assist teachers in creating contextual awareness, kindling reflection, understanding students and course progress, and inferring patterns of success and failure in competences development.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6518105,no,undetermined,0
A triple-mode ring dielectric resonator band-pass filter using substrate integrated waveguide (SIW),"This paper presents a design of triple mode ring dielectric resonator band-pass filter. The first mode TM<sub>010</sub> and the second degenerated modes TM<sub>110</sub> were excited. Instead of using conventional metal enclosures, substrate integrated waveguide (SIW) was used for easily integrating with other microwave circuits. Analytic equations were derived to determine the resonant frequencies and the quality factor of the ring dielectric resonator filter. It has been found that the calculated results agreed with simulated ones by using EM software. To develop a triple mode ring dielectric resonator filter, novel input and output structures were realized. The measured insertion of the filter is about 0.6dB with 3-dB fractional bandwidth of 9.8% obtained at operating frequency 2.1GHz. This proposed filter configuration effectively reduced the cost of assembly and the difficulty of integration associated with other circuits. The concept is very attractive for using in low cost, low insertion loss and high Q production of wireless filter application.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686616,no,undetermined,0
A Trust Evaluation Method for Supplier Selection,"Trust and reputation systems play important roles in supplier selection, a topic that has been widely investigated from a business and operation point of view. However, we still lack of effective studies on supplier selection with security as the optimal target, which is a very important factor for Information and Communication Technology (ICT) systems. In response, this paper outlines a method that enables people to evaluate the relative and objective trustworthiness of the alternative suppliers. Our analysis is based solely on the original data of the vulnerabilities publicly available from OSVDB and NVD, which are impossible to tamper with. They are then interpreted by an approach that combines the Analytic Hierarchy Process (AHP) and objective analysis that enables the inference of relative trust valuations based on different evaluation indexes. A case study of five well known vendors is demonstrated with qualitative comparison based on visualization of data.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681006,no,undetermined,0
A Universal Storage Architecture for Big Data in Cloud Environment,"With the rapid development of the Internet of Things and Electronic Commerce, we have entered the era of big data. The characteristics, such as great amount and heterogeneousity, of big data bring the challenge to the storage and analytics. The paper presented a universal storage architecture for big data in cloud environment. We use clustering analysis to divide the cloud nodes into multiple clusters according to the communication cost between different nodes. The cluster with the strongest computing power is selected to provide the universal storage and query interface for users. Each of other clusters is responsible for storing the data of a particular model, such as relational data, key-value data, and document data and so on. Experiments show that our architecture can store all kinds of heterogeneous big data and provide users with unified storage and query interface for big data easily and quickly.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682110,no,undetermined,0
A Web service based application serving vegetation condition indices for Flood Crop Loss Assessment,"Vegetation condition assessment is very useful and helpful for researchers and decision makers to evaluate crop loss and value, and identify and manage risks in the flood hazard areas. Crop responses to flooding vary with crop types, crop growing stages, soil characteristics, weather condition, flood duration and depth, etc. How to measure and understand crop response is a challenging and important research topic in agriculture. The availability and integration of high spatial and temporal resolution remote sensing data facilitates crop type identification, crop condition monitoring, soil moisture measurement, crop yield estimation, and crop damage evaluation. Remote sensing based vegetation condition indices are widely used by researchers in these fields. In this paper, an integrated Web geospatial application named Remote-sensing-based Flood Crop Loss Assessment Service System (RF-CLASS) is developed to automate the äóìdata-information-knowledge-decisionäó process of downloading near real time 250m resolution MODIS land surface reflectance data from NASA website, re-projecting, reformatting, and mosaicking these data using geospatial software packages, calculating various daily, weekly, and bi-weekly vegetation condition indices through efficient computing method, visualizing and analyzing these indices in an interactive way, assessing crop progress and condition, and evaluating crop damages in the flood areas. Currently, the vegetation indices for the whole continental United States from the year of 2000 have been generated routinely. These indices and other geospatial data like the boundary layers, the road layers, and the latest cropland data layer (CDL) have been served in the prototype system of RF-CLASS. RF-CLASS not only provides basic map operations, area of interest definition, geospatial data customization and downloading, and geospatial analytics functions for decision making through its interactive and intuitive user interface, but also o- fers standard Web services to query, visualize, disseminate, and analyze various types of vegetation condition indices for integration in other applications or invocation in scientific workflows.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621910,no,undetermined,0
A Web-Based Hybrid System for Evaluating Marketing and e-Commerce Web Site Performance,"In the globalised markets with changing customer demands, linking manufacturing or production with sound marketing strategies and effective marketing performance management becomes increasingly important. In this paper, a Web-based hybrid system, WebMarP (created by the authors), for evaluating marketing and e-commerce Web site performance is introduced. The proposed novel approach integrates the strengths of the analytic hierarchy process, Web-based expert system, online fuzzy rules and graphical displays. The software system architecture is outlined with input and output screen samples illustrated. Initial evaluation work is also reported with evaluation findings briefly presented.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597941,no,undetermined,0
Accelerating Batch Analytics with Residual Resources from Interactive Clouds,"The popularity of cloud-based interactive computing services (e.g., virtual desktops) brings new management challenges. Each interactive user leaves abundant but fluctuating residual resources while being intolerant to latency, precluding the use of aggressive VM consolidation. In this paper, we present the Resource Harvester for Interactive Clouds (RHIC), an autonomous management framework that harnesses dynamic residual resources aggressively without slowing the harvested interactive services. RHIC builds ad-hoc clusters for running throughput-oriented ""background"" workloads using a hybrid of residual and dedicated resources. These hybrid clusters offer significant gains over normal dedicated clusters: 20-40% cost and 20-29% energy savings in our test bed. For a given background job, RHIC intelligently discovers and maintains the ideal cluster size and composition, to meet user-specified goals such as cost/energy minimization or deadlines. RHIC employs black-box workload performance modeling, requiring only system-level metrics and incorporating techniques to improve modeling accuracy with bursty and heterogeneous residual resources. We demonstrate the effectiveness and adaptivity of our RHIC prototype with two parallel data analytics frameworks, Hadoop and HBase. Our results show that RHIC finds near-ideal cluster sizes and compositions across a wide range of workload/goal combinations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6730798,no,undetermined,0
Analytical modelling of the current (I)-voltage (V) characteristics of sub-micron gate-length ion-implanted GaAs MESFETs under dark and illuminated conditions,"An analytical model for current (I)-voltage (V) characteristics of a short-channel ion-implanted GaAs MESFET has been presented for dark and illuminated conditions. For the sake of simplicity, the non-analytic (i.e. non-integral) Gaussian doping function commonly considered for the channel doping of an ion-implanted GaAs metal semiconductor field effect transistor (MESFET) has been replaced by an analytic Gaussian-like doping profile in the vertical direction. The device uses an indium-tin oxide-based Schottky gate through which an optical radiation of 0.87 ‘_m wavelength is coupled from an external source into the device to modulate the I-V characteristics of the short-gate length GaAs MESFET. The coupled light generates electron-hole pairs in the active channel region below the gate and develops a photovoltage across the Schottky gate-channel junction and modulates the device characteristics. This study also includes the modelling of this photovoltaic effect by taking the short-gate length effects into consideration. The developed model includes the effects of doping profile and device parameters on the drain current of the short-channel ion-implanted GaAs MESFETs under dark and illuminated conditions of operations. The accuracy of the proposed model is extensively verified by comparing the theoretically predicted results with numerical simulation data obtained by using the commercially available ATLASTM device simulation software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531070,no,undetermined,0
Accelerating Join Operation for Relational Databases with FPGAs,"In this paper, we investigate the use of field programmable gate arrays (FPGAs) to accelerate relational joins. Relational join is one of the most CPU-intensive, yet commonly used, database operations. Hashing can be used to reduce the time complexity from quadratic (nai’šve) to linear time. However, doing so can introduce false positives to the results which must be resolved. We present a hash-join engine on FPGA that performs hashing, conflict resolution, and joining on a PCIe-attached system, achieving greater than 11x speedup over software.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6545988,no,undetermined,0
An analysis of contribution rates in relation with funding structurization in formulating the Indonesian National Standards (SNIS),"Standard formulation/development processes/stages are important to be analyzed as they determine whether a standard is acceptable in the market and/or able to meet the stakeholders' expectation. This research aims to identify the contribution rates of the SNI (Indonesian National Standard) formulation stages. The order of SNI formulation stages based on contribution rate is compared with that based on funding structurization to analyze the relations between them. Analytical Hierarchy Process (AHP) method is considered effective and efficient to measure contribution rate to evaluate the SNI formulation stages while activity-based costing (ABC) technique is used to identify the funding structurization. The research concludes that the existing SNI formulation stages are still relevant to be used. The rate contribution shows differences between stages but not significant. The stage of publication has the largest contribution rate (0.197703) while the stage of concept drafting has the largest funding structurization (USD 624.). There are differences in the order of importance of SNI formulation stages between the contribution rates and funding structurization. Moreover, this research also concludes that priority setting can be used to monitor the allocation of limited resources in order to be more effective. Thus, contribution rate analysis can become a tool to implement it. Also, the funding structurization can be used as a basis to create an equation to determine standard formulation costs that include fixed and variable costs. Further research can be conducted to analyze the relationship of contribution rate with other resources such as time requirement and human resources in the SNI formulation stages.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6774581,no,undetermined,0
An Analytic Framework for Frame-Level Dependent Bit Allocation in Hybrid Video Coding,"In this paper, we address the frame-level dependent bit allocation (DBA) problem in hybrid video coding. In most existing methods, the DBA solution is achieved at the expense of high, sometimes even unbearable, computational complexity because of the multipass coding involved. Motivated by this, we propose a model-based approach as an attempt to solve this problem analytically. Leveraging the predictive nature in hybrid video coding, we develop a novel interframe dependency model (IFDM), which enables a quantitative measure of the coding dependency between the current frame and its reference frame. Based on the IFDM, the buffer-constrained frame-level DBA problem is carefully formulated. Finally, the model-based DBA method called IFDM-DBA is derived, in which successive convex approximation techniques are employed to convert the original optimization problem into a series of convex optimization problem s of which the optimal solutions can be obtained efficiently. Experimental results suggest that the proposed IFDM-DBA method can achieve up to a 23% bitrate reduction over the JM reference software of H.264.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6428636,no,undetermined,0
An effect evaluation method for IMS SIP flooding attacks based on fuzzy comprehensive evaluation,"To quantitatively understand the effects of SIP flooding attacks in IMS, the paper introduces an evaluation method for attack effect based on fuzzy comprehensive evaluation. The paper gets the evaluation indexes by analyzing the possible attack flow, and introduces analytic hierarchy process (AHP) to compute the indexes' weight, and makes fuzzy comprehensive evaluation for attacks. To reduce the bias brought by a single fuzzy operator's properties, the paper selects several operators to complement with each other. Finally, it is validated that the evaluation method is able to distinguish the attack effects resulted by different SIP flooding attacks effectively.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615468,no,undetermined,0
An End-to-End QoS Mapping Approach for Cloud Service Selection,"In order to select and rank the best services in a cloud computing environment, the end-to-end quality of service (QoS) values of cloud services have to be computed. For a new SaaS provider, the deployment of its software application in the cloud is a challenging job. It has to find a hosting service (IaaS) that hosts its service. The primary goal of the SaaS provider is to make its service at the top of the ranked list of cloud services returned to end users through satisfying their QoS requirements. In this paper, we propose a mechanism to map the users' QoS requirements of cloud services to the right QoS specifications of SaaS then map them to best IaaS service that offers the optimal QoS guarantees. Then together SaaS and IaaS services can provide the best service offer to end users. As a result of the mapping, the end-to-end QoS values can be calculated. We propose a set of rules to perform the mapping process. We hierarchically model the QoS specifications of cloud services using the Analytic Hierarchy Process (AHP) method. The AHP based model helps to facilitate the mapping process across the cloud layers, and to rank the candidate cloud services for end users. We use a case study to illustrate and validate our solution approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6655719,no,undetermined,0
An Open Source Simulator for IEEE1588 Time Synchronization over 802.11 Networks,"As a key technique in distributed systems, time synchronization plays an important role in time sensitive modern industrial wireless local area networks (WLANs). Based on time packet exchange technology, the IEEE 1588 precise time protocol (PTP) achieves high precise and low power consumption time synchronization in wired Ethernet. In WLANs, however, it is difficult for the wireless sensor nodes to acquire accurate timestamps due to their limited resources. Moreover, compared to wired Ethernet, WLANs suffer more from wireless channel sharing, fading and packet collisions, which leads to overwhelming transmission delay jitters. Therefore, analytic solution to PTP's performance in WLANs is very difficult and it is of importance to evaluate PTP's performance by realistic simulation. In this paper, based on the open source OMNeT++ simulation engine, we present the simulator we developed for PTP time synchronization in 802.11 WLANs. The behavior of the PTP time synchronization, the simulation results and the factors that affect the PTP performance are presented and evaluated.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779905,no,undetermined,0
Analysis of electromechanical interface model for liquid floated micro-gyroscope,"This paper presents an electromechancial interface model of liquid floated micro-gyroscope, which can analyze sensing capacitors and resistance of gyroscope. The impacts of them are analyzed and simulated with the simulation software of circuit. The analytic results indicate that the reasonable interface circuit can substantially remove the impacts of these parasitic capacitances, increase the signal noise ratio. To ensure the high resolution, the model of liquid floated micro-gyroscope with ANSOFT conducted. The results show the transfer characteristics of the sensor and nonlinear error of transfer characteristics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6559846,no,undetermined,0
Analysis on the Scheduling Problem in Transparent Computing,"Transparent computing has received increasing attention recently. As a variation and implementation of cloud computing, scheduling is destined to be one of the most hot topics for performance optimization. In this paper, we focus on a typical transparent computing platform, where a number of clients are connected with a server cluster in a gigabit LAN. Due to the NP-completeness of the scheduling problem on parallel processors, the optimal scheduling solution of TC can not be achieved in polynomial time. To find an efficient scheduling algorithm, We present two approximation algorithms and analysed their upper bounds theoretically. Though the analytic results are less than satisfactory, the practical performances of the proposed algorithms are demonstrated to be acceptable by extensive simulations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832145,no,undetermined,0
Analytic Hierarchy process selection for batteries storage technologies,"The objective of this study is to select the most appropriate battery technology for photovoltaic application. However, the selection criteria and the diversity of technologies make choice difficult. So, we focus on the Analytic Hierarchy Process which is among the most widely used and has been applied in several multicriteria decision making domains.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578374,no,undetermined,0
A MapReduce Based Approach of Scalable Multidimensional Anonymization for Big Data Privacy Preservation on Cloud,"The massive increase in computing power and data storage capacity provisioned by cloud computing as well as advances in big data mining and analytics have expanded the scope of information available to businesses, government, and individuals by orders of magnitude. Meanwhile, privacy protection is one of most concerned issues in big data and cloud applications, thereby requiring strong preservation of customer privacy and attracting considerable attention from both IT industry and academia. Data anonymization provides an effective way for data privacy preservation, and multidimensional anonymization scheme is a widely-adopted one among existing anonymization schemes. However, existing multidimensional anonymization approaches suffer from severe scalability or IT cost issues when handling big data due to their incapability of fully leveraging cloud resources or being cost-effectively adapted to cloud environments. As such, we propose a scalable multidimensional anonymization approach for big data privacy preservation using Map Reduce on cloud. In the approach, a highly scalable median-finding algorithm combining the idea of the median of medians and histogram technique is proposed and the recursion granularity is controlled to achieve cost-effectiveness. Corresponding MapReduce jobs are dedicatedly designed, and the experiment evaluations demonstrate that with our approach, the scalability and cost-effectiveness of multidimensional scheme can be improved significantly over existing approaches.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686016,no,undetermined,0
A hybrid method for word segmentation with English-Vietnamese bilingual text,"This paper proposes a hybrid approach for Vietnamese word segmentation. The approach combines a dictionary-based method and a machine learning method to detect word boundaries in Vietnamese text by comparing English-Vietnamese pairs. We also point out several characteristics of Vietnamese which affect the Vietnamese word segmentation task and word alignment of English-Vietnamese text. Moreover, we built an English-Vietnamese bilingual corpus with nearly 10 million words, namely EVBCorpus, while a part of EVBNews has been manually segmented at the word level. We evaluate the performance of our approach by comparing its word segmentation results on this corpus. Our hybrid approach achieves 97% accuracy on the EVBNews corpus.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6720528,no,undetermined,0
A formal model for verifying stealthy attacks on state estimation in power grids,"The power system state estimation is very important for maintaining the power system securely, reliably, and efficiently. An attacker can compromise meters or communication systems and introduce false measurements, which can evade existing bad data detection algorithms and lead to incorrect state estimation. This kind of stealthy attack is well-known as Undetected False Data Injection (UFDI) attack. However, attackers usually have different constraints with respect to knowledge, capabilities, resources, and attack targets. These attack attributes are important to consider in order to know the potential attack vectors. In this paper, we propose a formal model for UFDI attack verification in order to provide security analytics for power grid state estimation. Our model formalizes the grid information and different constraints, particularly with respect to attackers' point of view. The solution to the model provides an attack vector, when it exists, by satisfying the given constraints. We demonstrate our UFDI attack verification model with the help of an example. We evaluated our proposed model by running experiments on different IEEE test systems and we found that our model is very efficient in solving problems with hundreds of buses.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6687993,no,undetermined,0
A fast and hardware mimicking analytic CT simulator,"Different algorithms have been utilized for x-ray computed tomography (CT) simulation based on Monte Carlo technique, analytic calculation, or combination of them. Software packages based on Monte Carlo algorithm provide sophisticated calculations but the time consuming nature of them limits its applicability. Analytic calculation for CT simulation has been also evaluated in recent years. Due to ignoring basic physical processes, analytic methods have limited applications. In this study, a hardware mimicking algorithm has been developed to accurately model the CT imaging chain using analytic calculation. The model includes x-ray spectrum generation according to the pre-defined scanning protocol. The detector is designed to acquire the data either in integral or spectral modes. CT geometry can be used as parallel or fan beam with different sizes. Poisson noise model was applied to the acquired projection data. Varieties of projection-based computerized phantoms have been designed and implemented in the simulator. CT number and background noise of the simulated images have been compared with experimental data. On average, the relative difference between simulated and experimental HUs are 8.3%, 7.5%, and 8.0% for bone; 12.1%, 10.3%, and 7.8% for contrast agent; and 16.6%, 3.6%, and 5.2% for the background at 80 kVp/500 mAs, 120 kVp/250 mAs, and 140 kVp/125 mAs, respectively. The relative difference between simulated and experimental noise values vary between 2% to slightly less than 26%. For scanning and image generation with a computer equipped with Intel Core2 Quad CPU and 2.0 GB of RAM, the simulator takes about 32 seconds for generating a 512í„512 single slice image when it is adjusted to acquire 900 projection angles with 20 mm slice thickness and 140kVp/200 mAs scanning protocol. The simulation time is independent of photon intensity.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6829157,no,undetermined,0
User-driven cloud transportation system for smart driving,"Intelligent transportation systems (ITS) have emerged as an efficient and effective way of alleviating the traffic congestion and improving the performance of transportation systems. Key challenges of ITS in recent years include the pervasive data collection, data security, privacy preserving, large volume data processing, and intelligent analytics. These challenges lead to a revolution in ITS development by leveraging the crowdsourcing scheme and cloud computing architecture. In this paper, we propose a user-driven Cloud Transportation system (CTS) which employs a scheme of user-driven crowdsourcing to collect user data for traffic model construction and congestion prediction including data collection, filtering, modeling, intelligent computation and publish. We describe in details the application scenario, system architecture, and core CTS services model. To verify the feasibility of our approach, we have developed a prototype system which elaborated the cloud architecture and other implementation details. This paper aims to inspire further research of user-driven CTS on intelligent data processing model for smarter utilization of transportation infrastructure.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427600,no,undetermined,0
Using technical debt data in decision making: Potential decision approaches,"The management of technical debt ultimately requires decision making - about incurring, paying off, or deferring technical debt instances. This position paper discusses several existing approaches to complex decision making, and suggests that exploring their applicability to technical debt decision making would be a worthwhile subject for further research.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225999,no,undetermined,0
Using the fuzzy analytic hierarchy process to the balanced scorecard: A case study for the elementary and secondary schools' information department of south Taiwan,"The purpose of this study is to establish balanced scorecard (BSC) in performance measurement of elementary and secondary schools' MIS Department. We take a broader definition of elementary and secondary schools' MIS Department as äóìan assembly which brings forth some specific functional activities to fulfill the task of MIS.äó BSC used as a measurement tool to assess study subjects, according to its strategy and goal formed by its assignment property, can be divided into four dimensions: finance, customer, inter process, learning and growth, which can provide us with a timely, efficient, flexible, simple, accurate, and highly overall reliable measurement tool. In order to extract the knowledge and experience from related experts to pick out important evaluation criteria and opinion, this study combines fuzzy theory and the analytical hierarchy process (AHP) to calculate the weights. After completing weighted calculation of every dimension and indicator, the BSC model is thus established. The findings of this study show that the indicator weightings between and among all the levels are not the same, rather there exists certain amount of differences. The degrees of attention drawing in order of importance among all dimensions are customer, financial, internal process and learning and growth dimension. After comprehensively analyzing indictors of performance measurement included in every level, the highly valued top five indictors are, when conducting dimension performance measurement in elementary and secondary schools' MIS Department, äóìRationalize software and hardware and maintenance expenses,äó äóìBudget satisfy and control,äó äóìQuick response and handling,äó äóìImprove service quality,äó and äóìHigh effective information systemäó.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6359574,no,undetermined,0
Validation and optimization of modular railgun model,"Considering the electromagnetic railgun structure features, an applied model has been established by modularization modeling method. Because model validation plays a key role in system simulation for analysis and optimization of railgun, a novel validation method is proposed in this paper. By utilizing the railgun dynamic test and predicted data, the simulation model is validated based on theil's inequality coefficient (TIC) method. To execute sufficient validation of the model and improve reliability of the validation results, similarity theory is adopted to analyze the credibility of simulation model by comparing the characteristics of the railgun. Inspired by analytic hierarchy process (AHP) for multiple attribute decision making, we define the muzzle velocity, current peak and discharging time as evaluation index for assessing the credibility of the simulation model and then evaluation structure is established. The final synthesis results demonstrate the feasibility of the adopted validation method and the validity of the modular model. The whole system model is run under MATLAB software platform, which is divided into several main components encapsulated into classes by object-oriented technology. On the basis of the validated model, an optimization pursuing highest velocity with the constraint of the current peak and the residual current in the rail is carried out to find a set of optimal parameters for the railgun using genetic algorithm.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6325055,no,undetermined,0
VAST Challenge 2012: Visual analytics for big data,"The 2012 Visual Analytics Science and Technology (VAST) Challenge posed two challenge problems for participants to solve using a combination of visual analytics software and their own analytic reasoning abilities. Challenge 1 (C1) involved visualizing the network health of the fictitious Bank of Money to provide situation awareness and identify emerging trends that could signify network issues. Challenge 2 (C2) involved identifying the issues of concern within a region of the Bank of Money network experiencing operational difficulties utilizing the provided network logs. Participants were asked to analyze the data and provide solutions and explanations for both challenges. The data sets were downloaded by nearly 1100 people by the close of submissions. The VAST Challenge received 40 submissions with participants from 12 different countries, and 14 awards were given.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400529,no,undetermined,0
VDQAM: A toolkit for database quality evaluation based on visual morphology,"Data quality evaluation is one of the most critical steps during the data mining processes. Data with poor quality often leads to poor performance in data mining, low efficiency in data analysis, wrong decision which bring great economic loss to users and organizations further. Although many researches have been carried out from various aspects of the extracting, transforming, and loading processes in data mining, most researches pay more attention to analysis automation than to data quality evaluation. To address the data quality evaluation issues, we propose an approach to combine human beings' powerful cognitive abilities in data quality evaluation with the high efficiency ability of computer, and develop a visual analysis method for data quality evaluation based on visual morphology.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400531,no,undetermined,0
Virtual floortime using games to engage children with Autism Spectrum Disorder,"1 in 88 children is diagnosed with Autism Spectrum Disorder (ASD). Specific software, virtual worlds, and games may be used to improve social and language skills. These tools may be combined with the DIR/Floortime therapeutic model that encourages parents, teachers, and therapists to engage these children through their own (sometimes limited) interests. Applying this framework, this paper suggests the creation of a workshop modeled after the Computer Clubhouse, as developed at the Massachusetts Institute of Technology, which would take advantage of the children's sometimes above-average analytic and visual abilities. Participants would produce their own video games with Scratch or other age-appropriate tool. In this way, children on the autism spectrum could pursue their interests in computers and games while strengthening their creativity and problem-solving skills: areas that are sometimes difficult for those with the disorder.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329852,no,undetermined,0
Virtual Time Integration of Emulation and Parallel Simulation,"A high fidelity testbed for large-scale system analysis requires emulation to represent the execution of critical software, and simulation to model an extensive ensemble of background computation and communication. We leverage prior work showing that large numbers of virtual environments may be emulated on a single host, and that the time stamped interactions between them can be mapped to virtual time, and we leverage existing work on simulation of large-scale communication networks. The present paper brings these concepts together, marrying the scale emulation framework OpenVZ (modified earlier to operate in virtual time) with a scalable network simulator S3F. Our algorithmic contributions lay in the design and management of virtual time as it transitions from emulation, to simulation, and back. In particular, inescapable uncertainties in emulation behavior force us to explicitly set and reset timestamps so as to avoid either emulator or simulator having to deal with a packet arriving in its logical past. We provide analytic bounds and empirical evidence that the error introduced in resetting timestamps is small. Finally, we present a case-study using this capability, of a cyber-attack with the smart power grid communication infrastructure.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6305913,no,undetermined,0
Visual analytics for the big data era äóî A comparative review of state-of-the-art commercial systems,"Visual analytics (VA) system development started in academic research institutions where novel visualization techniques and open source toolkits were developed. Simultaneously, small software companies, sometimes spin-offs from academic research institutions, built solutions for specific application domains. In recent years we observed the following trend: some small VA companies grew exponentially; at the same time some big software vendors such as IBM and SAP started to acquire successful VA companies and integrated the acquired VA components into their existing frameworks. Generally the application domains of VA systems have broadened substantially. This phenomenon is driven by the generation of more and more data of high volume and complexity, which leads to an increasing demand for VA solutions from many application domains. In this paper we survey a selection of state-of-the-art commercial VA frameworks, complementary to an existing survey on open source VA tools. From the survey results we identify several improvement opportunities as future research directions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400554,no,undetermined,0
Visual Data Analysis as an Integral Part of Environmental Management,"The U.S. Department of Energy's (DOE) Office of Environmental Management (DOE/EM) currently supports an effort to understand and predict the fate of nuclear contaminants and their transport in natural and engineered systems. Geologists, hydrologists, physicists and computer scientists are working together to create models of existing nuclear waste sites, to simulate their behavior and to extrapolate it into the future. We use visualization as an integral part in each step of this process. In the first step, visualization is used to verify model setup and to estimate critical parameters. High-performance computing simulations of contaminant transport produces massive amounts of data, which is then analyzed using visualization software specifically designed for parallel processing of large amounts of structured and unstructured data. Finally, simulation results are validated by comparing simulation results to measured current and historical field data. We describe in this article how visual analysis is used as an integral part of the decision-making process in the planning of ongoing and future treatment options for the contaminated nuclear waste sites. Lessons learned from visually analyzing our large-scale simulation runs will also have an impact on deciding on treatment measures for other contaminated sites.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327213,no,undetermined,0
Visualizing Arrays in the Eclipse Java IDE,"The Eclipse Java debugger uses an indented list to view arrays at runtime. This visualization provides limited insight into the array. Also, it is cumbersome and time-consuming to search for certain values at an unknown index. We present a new Eclipse plug in for visualizing large arrays and collections while debugging Java programs. The plug in provides three views to visualize the data. These views are designed to support different tasks more efficiently. A tabular view gives detailed information about the elements in the array, such as the value of their field variables. A line chart aims to depict the values of a numerical field over the array. Lastly, bar charts and histograms show how the values of a field are distributed. We show how these views can be used to explore linear data structures and hashes from the Collections Framework. The plug in features tight integration with the Eclipse IDE, and is freely available as an open-source project. Developers' feedback confirmed the utility of the plug in to explore large arrays in real-world scenarios.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178939,no,undetermined,0
Weather influence on alarm occurrence in home telemonitoring of heart failure patients,"The study investigated weather influences on occurrence of alarm conditions among heart failure patients subjected to home telemonitoring. The telemonitored patients were located in the vicinities of five Austrian cities: Vienna, Graz, Innsbruck, Klagenfurt and Linz. The associated daily weather conditions were obtained from the Austrian Central Institute for Meteorology and Geodynamics. The investigations included correlations between patient's vital signs: systolic and diastolic blood pressure, heart rate, and weight, and weather conditions: air temperature, humidity and atmospheric pressure. GNU-R statistical software was used for the analysis. The results show statistically significant differences in measured blood pressure between the days with high thermal stress, particularly with falling temperatures, cold stress days. At the same time, blood pressure was associated with the highest number of patient alarm conditions requiring medical response. Including weather data within the home telemonitoring alarm generation systems offers potential for enhancing decision support towards prevention or minimization of the occurrence of adverse events.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6420446,no,undetermined,0
Wireless sensor network security visualization,"Security is becoming a major concern for many mission-critical applications wireless sensor networks (WSNs) are envisaged to support. This is because WSNs are susceptible to various types of attacks or to node compromises that exploit known and unknown vulnerabilities of protocols, software and hardware, and threaten the security, integrity, authenticity, and availability of data that resides in these networked systems. While various security mechanisms have been proposed for these networks dealing with either MAC layer or network layer security issues, or key management problems, the security benefits that can be obtained from an upper visualization layer have not been adequately considered in their design. In this paper, we explore the issues and concerns surrounding the application of visual analysis for wireless sensor network security purposes. This paper focuses on several distinct advantages information visualization and visual analytics can offer in the security domain. In addition, this paper reviews security visualization tools that are available to network security analysts. Finally, it concludes by identifying challenges for this new area of research.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6459781,no,undetermined,0
Workflow framework to support data analytics in cloud computing,"This paper reports on the development of the Cloud Oriented Data Analytics (CODA) framework which has functions for composing, managing, and processing workflows for data analytics in cloud computing. The framework provides a number of reusable software components for data analytics to users which can be composed as workflows through well-known workflow composers, e.g., RapidMiner, Taverna, and JOpera. In particular, workflow scheduling, workflow recommendation, resource provisioning, resource monitoring, data locality, and security for the workflow computation are addressed by the framework. By using the framework, we demonstrate that workflows can be easily composed and processed in cloud computing. By coordinating the submitted workflows, we can obtain a significant improvement in performance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427489,no,undetermined,0
3D documentation for the Conservation and Restoration of contemporary works of art: The sculptures of Maurizio Savini,"As part of its five-year university course ""Conservation and Restoration of Cultural Heritage"", ""Carlo Bo"" Urbino University has introduced a study programme involving the sculptures of the contemporary artists, which are particularly significant in terms of three-dimensional experimentation for the documentation and conservation of the works. In the case, the work of Maurin’o Savini belongs to a contemporary artistic context - where matter becomes increasingly ephemeral and streamlined - making it more important than ever to have a scientific analytic tool that ensures good acquisition. The fundamental problem is once again, in the work of art itself, and during acquisition, that of making a distinction between what is relevant or irrelevant, identifying information that is or will become fundamental in time due to its specificity and the choice of documentation techniques that best meet these needs. The importance of correct data acquisition thus becomes a fundamental stage for understanding how to manage the project, and is in fact a pre-diagnosis of the work of art as well as a trace for future conservation works. The documentation of works of art like those in question must necessarily include acquisition systems that are able to record objects with complex and varied volumes with extreme precision. The development of hardware and software for three-dimensional acquisition makes it possible to diversify results, according to the morphological characteristics of the objects analysed. .ne of the most important aspects of this work is the methodology used to catalogue the individual items and their positioning in space, and to evaluate the state of conservation on 3D models.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6743845,no,undetermined,0
3D visual analytics for quality control in engineering,"This article will focus on 3D visual analytics for quality control in engineering. The world today is a busy place with a rapidly increasing amount of data to be dealt with every day. Problems are encountered when most of the data are stored without filtering and refinement for later use. Industry has raised the demands for the high technological performance of final products, such as short production time, low manufacturing costs and overall product quality. Digitizing in the real-world has various application domains, and is of vital importance when it comes to methods involving industrial quality assurance. The process of the rapid development of products depends on new technologies, such as 3D scanning, 3D printing and prototyping. Detailed digitizing can provide more product information, making it easier to locate the causes of inaccuracies and optimize production. These possibilities help us check and improve tools and gadgets, control the form of prototypes and test series in production optimization, quality assurance of serial production etc. Quality and rapid digitizing using these systems makes copying easy and thus accelerates serial production, which is why these procedures are used by numerous companies. This paper explores and analyzes existing technology and devices used in industrial quality assurance, and provides a brief review of current options and application opportunities.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6596263,no,undetermined,0
A comparative study of enterprise and open source big data analytical tools,"In this paper, we bring forward a comparative study between the revolutionary enterprise big data analytical tools and the open source tools for the same. The Transaction Processing Council (TPC) has established a few benchmarks for measuring the potential of software and its use. We use similar benchmarks to study the tools under discussion. We try to cover as many different platforms for big data analytics and compare them based on computing environment, amount of data that can be processed, decision making capabilities, ease of use, energy and time consumed, and the pricing.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6558123,no,undetermined,0
A Data Intensive Statistical Aggregation Engine: A Case Study for Gridded Climate Records,"Satellite derived climate instrument records are often highly structured and conform to the ""Data-Cube"" topology. However, data scales on the order of tens to hundreds of Terabytes make it more difficult to perform the rigorous statistical aggregation and analytics necessary to investigate how our climate is changing over time and space. It is especially cumbersome to supply the full derivation (provenance) of this analysis, as is increasingly required by scientific conferences and journals. In this paper, we address our approach toward the creation of a 55 Terabyte decadal record of Outgoing Long wave Spectrum (OLS) from the NASA Atmospheric Infrared Sounder (AIRS), and describe our open source data-intensive statistical aggregation engine ""Gridderama"" intended primarily for climate trend analysis, and may be applicable to other aggregation problems involving large structured datasets.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651122,no,undetermined,0
A Dynamic Data Placement Scheme for Hadoop Using Real-time Access Patterns,"Hadoop has become a popular platform for largescale data analytics. In this paper, we identify a major performance bottleneck of Hadoop-its lack of ability to place data near to its required users. This new Data Placement scheme using access patterns will improve the performance of Hadoop. With this strategy the data will be placed nearer to the required users thereby achieving optimization in access time and bandwidth. Finally, the simulation experiments indicate that our strategy behaves much better than the HDFS blocks placement.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637175,no,undetermined,0
Analytical analysis of magnetic field of eddy-current driver based on equivalent current method,"According to the structure parameters of the eddy-current driver, this paper established the magnetic field analytical model of eddy-current driver by utilizing the method of equivalent surface current. We calculated and analyzed the magnetic field of the eddy-current driver with the help of the MATLAB simulation software. The purpose was to explore the regularity of analytical model of magnetic field. Then the result was compared with that of finite element method and measured values. The compared result showed that the space distribution of the axial magnetic field on the surface of copper conductor plate of eddy current drive was periodically distributed along the circumferential Angle. The error of analytic method is less than that of the numerical solution method.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6885590,no,undetermined,0
Analytical performance analysis of mesh network-on-chip based on network calculus,"The design of on chip interconnection architecture (NoC) should carefully take on consideration both hardware and communication constraints in order to built up a system that meets quality of service requirements. In NoC architecture, the on chip switch available hardware and software resources drive up the global performances of communication processes. Therefore it is crucial, before the physical design process, to carry out the required capacities such as buffer depth and management-tasks of a flit. In fact, one of the most critical parameters that can affect communication characteristics are the available memory space in addition to flit-time processing according to a given scheduling approach. This paper deals with these concepts. It presents a study of NoC switch using Network Calculs (NC) theory. It provides an analytic model of the internal on chip switch architecture to study the performance with a mathematical approach. This helps to specify the best physical and logical characteristics that can achieve enhanced performances.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6689565,no,undetermined,0
Two Visualization Tools for Analyzing Agent-Based Simulations in Political Science,"Agent-based modeling has become a key technique for modeling and simulating dynamic, complicated behaviors in the social and political sciences. Although many robust toolkits for developing and running these simulations exist, systems that support analysis of their results are few and tend to be overly general. So, social scientists have had difficulty interpreting the results of their increasingly complex simulations. To help bridge this gap between data generation and interpretation, researchers collaborated with political science analysts to design two tools for interactive data exploration and domain-specific data analysis. Testing by the analysts validated that these tools provided an efficient framework to explore individual trajectories and the relationships between variables. The tools also supported hypothesis generation by enabling analysts to group simulations according to multidimensional similarity and drill down to investigate further.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051409,no,undetermined,0
Development and validation of analytic equations of the electromagnetic fields radiated by the elementary dipoles in time domain,"In this paper, we will present the development of the analytic equations and the corresponding calculating code to models and evaluate the electromagnetic field radiated by the elementary dipoles in time domain. Results obtained with analytic equations are compared to those obtained after simulations made with software based on numerical method in the time domain. Then, they are compared to results obtained with analytic and numerical methods working in frequency domain. In this paper, the development of analytic resolution is carried out in order to implement the electromagnetic inverse method in time domain.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564022,no,undetermined,0
Composing hierarchical stochastic model from SysML for system availability analysis,"Comprehensive analytic model for system availability analysis often confronts the largeness issue where a system designer cannot easily handle the model and the solution is not given in a feasible solution time. Hierarchical decomposition of a large state-space model gives a promising solution to the largeness issue when the model is decomposable. However, the decomposability of analytic model is not always manually tractable especially when the model is generated in an automated manner. In this paper, we propose an automated model composition technique from a system design to a hierarchical stochastic model which is the judicious combination of combinatorial and state-space models. In particular, from SysML-based system specifications, a top-level fault tree and associated stochastic reward nets are automatically generated in hierarchical manner. The obtained hierarchical stochastic model can be solved analytically considerably faster than monolithic state-space models. Through an illustrative example of three-tier web application system on a virtualized infrastructure, the accuracy and efficiency of the solution are evaluated in comparison to a monolithic state space model and a static fault tree.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698904,no,undetermined,0
Comprehensive Credit Evaluation Model of Electricity Customer Based on the Changing Trend of Credit,"In order to solve the credit quantification and scoring problem, this paper did some research on credit evaluation model of electricity customers. The credit was divided into 5 first-grade indicators and 15 second-grade indicators. The paper quantified these indicators and evaluated their weights by utilizing Analytic Hierarchy Process model. The indicators were scored by utilizing exponential scoring. According to the weight and scores of each indicator, we calculated the month credit values of the last 12 months. Through these month credit values and their changing trend, the comprehensive credit was calculated. We used a customer sample data to verify and analyze the model. The experimental result shows that the model can evaluate credit indicators reasonably and effectively. Thus, the credit quantification and scoring problem is solved.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973647,no,undetermined,0
Criteria for ERP selection using an AHP approach,"Information systems are a foundation key element of modern organizations. Quite often, chief executive officers and managers have to decide about the acquisition of new software solution based in an appropriated set of criteria. Analytic Hierarchy Process (AHP) is one technique used to support that kind of decisions. This paper proposes the application of AHP method to the selection of ERP (Enterprise Resource Planning) systems, identifying the set of criteria to be used. A set of criteria was retrieved from the scientific literature and validated through a survey-based approach.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615818,no,undetermined,0
Cultural heritage omni-stereo panoramas for immersive cultural analytics äóî From the Nile to the Hijaz,"The digital imaging acquisition and visualization techniques described here provides a hyper-realistic stereoscopic spherical capture of cultural heritage sites. An automated dualcamera system is used to capture sufficient stereo digital images to cover a sphere or cylinder. The resulting stereo images are projected undistorted in VR systems providing an immersive virtual environment in which researchers can collaboratively study the important textural details of an excavation or historical site. This imaging technique complements existing technologies such as LiDAR or SfM providing more detailed textural information that can be used in conjunction for analysis and visualization. The advantages of this digital imaging technique for cultural heritage can be seen in its non-invasive and rapid capture of heritage sites for documentation, analysis, and immersive visualization. The technique is applied to several significant heritage sites in Luxor, Egypt and Saudi Arabia.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6703802,no,undetermined,0
Cura: A Cost-Optimized Model for MapReduce in a Cloud,"We propose a new MapReduce cloud service model, Cura, for data analytics in the cloud. We argue that performing MapReduce analytics in existing cloud service models - either using a generic compute cloud or a dedicated MapReduce cloud - is inadequate and inefficient for production workloads. Existing services require users to select a number of complex cluster and job parameters while simultaneously forcing the cloud provider to use those potentially sub-optimal configurations resulting in poor resource utilization and higher cost. In contrast Cura leverages MapReduce profiling to automatically create the best cluster configuration for the jobs so as to obtain a global resource optimization from the provider perspective. Secondly, to better serve modern MapReduce workloads which constitute a large proportion of interactive real-time jobs, Cura uses a unique instant VM allocation technique that reduces response times by up to 65%. Thirdly, our system introduces deadline-awareness which, by delaying execution of certain jobs, allows the cloud provider to optimize its global resource allocation and reduce costs further. Cura also benefits from a number of additional performance enhancements including cost-aware resource provisioning, VMaware scheduling and online virtual machine reconfiguration. Our experimental results using Facebook-like workload traces show that along with response time improvements, our techniques lead to more than 80% reduction in the compute infrastructure cost of the cloud data center.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569903,no,undetermined,0
Data stream mining to address big data problems,"Today, the IT world is trying to cope with äóìbig dataäó problems (data volume, velocity, variety, veracity) on the path to obtaining useful information. In this paper, we present implementation details and performance results of realizing äóìonlineäó Association Rule Mining (ARM) over big data streams for the first time in the literature. Specifically, we added Apriori and FP-Growth algorithms for stream mining inside an event processing engine, called Esper. Using the system, these two algorithms were compared over LastFM social music site data and by using tumbling windows. The better-performing FP-Growth was selected and used in creation of a real-time rule-based recommendation engine. Our most important findings show that online association rule mining can generate (1) more rules, (2) much faster and more efficiently, and (3) much sooner than offline rule mining. In addition, we have found many interesting and realistic musical preference rules such as äóìGeorge HarrisonäˆÍBeatlesäó. We hope that our findings can shed light on the design and implementation of other big data analytics systems in the future.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531483,no,undetermined,0
Decision support system of software architect,"Represented system is for supplying of architect with data, knowledge and methods needed to make decisions in the process of software systems (SS) design. It includes two subsystems: first one is subsystem of requirements specification and communication to SS, and second is architecture design subsystem. For requirements specification and communication formalism of basic protocols method and technic of expert pairwise comparisons are used. The selection of architecture decisions is represented as model of multi criteria hierarchic optimization, where modified Analytic Hierarchic Process (AHP) was applied. Main units of the system is implemented as formalized models or software units.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662686,no,undetermined,0
Delivering optimal real-time manufacturing intelligence,"IT Services can now deliver optimal Real-Time Manufacturing Intelligence (äóìMIäó). Enterprise Resource Planning (äóìERPäó) has been implemented for years, providing supply chain and business improvement intelligence. Real-Time MI has the opportunity for rapid ROI with enhanced yields, less downtime, and less waste. Much research has shown that people are poor at determining correlations subjectively. Most industry experts only trust computer calculated correlations using statistical models. MI is a concept in the world of process manufacturing - unique for analytically alarming process trends to prevent out of control product problems. Chemical, packaging, pharmaceutical and energy companies have long known that automating data collection and data analysis can lead to improving processes and yields. These companies have developed massive databases that collect detailed measurements from factory automation tools which are used later for off-line analysis. The key to today's MI improvement is to use new IT capabilities to leverage these existing, disparate database silos; provide real-time analysis and intelligence; and identify and correct problems in-line. This can be done without creating, duplicating or installing yet another database. This paper looks at MI and case studies in chemical, pharmaceutical, and packaging process manufacturing and compares the major companies providing software options.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6641576,no,undetermined,0
Designing Hybrid Architectures for Massive-Scale Graph Analysis,"Turning large volumes of data into actionable knowledge is a top challenge in high performance computing. Our previous work in this area demonstrated algorithmic techniques for massively parallel graph analysis on multithreaded systems. This work led to the development of GraphCT, the first end-to-end graph analytics platform for the Cray XMT and x86-class systems with OpenMP, and STINGER, a high performance, multithreaded, dynamic graph data structure and algorithms. Both of these packages are freely available as open source software. This dissertation research culminates in experimental and analytical techniques to study the marriage of disk-based systems, such as Hadoop, with shared memory-based systems, such as the Cray XMT, for data-intensive applications. David Ediger is a fifth year PhD candidate in Electrical and Computer Engineering.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651145,no,undetermined,0
Does SEO Matter? Increasing Classroom Blog Visibility through Search Engine Optimization,"Educators today motivate learning and foster engagement through the use of Web 2.0 software such as classroom blogs. In this study, we discussed the reasons and benefits of moving classroom blogs to the public, and how Search Engine Optimization (SEO) strategies from industry can help increase classroom blog visibility. We proposed an SEO approach and demonstrated how it can be applied to the design, implementation, and analysis classroom blogs in higher education through an empirical study.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6480034,no,undetermined,0
"Analytics over Big Data: Exploring the Convergence of DataWarehousing, OLAP and Data-Intensive Cloud Infrastructures","This paper explores the convergence of Data Warehousing, OLAP and data-intensive Cloud Infrastructures in the context of so-called analytics over Big Data. The paper briefly reviews some state-of-the-art proposals, highlights open research issues and, finally, it draws possible research directions in this scientific field.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649871,no,undetermined,0
"Droid Analytics: A Signature Based Analytic System to Collect, Extract, Analyze and Associate Android Malware","Smartphones and mobile devices are rapidly becoming indispensable devices for many users. Unfortunately, they also become fertile grounds for hackers to deploy malware. There is an urgent need to have a ""security analytic & forensic system"" which can facilitate analysts to examine, dissect, associate and correlate large number of mobile applications. An effective analytic system needs to address the following questions: How to automatically collect and manage a high volume of mobile malware? How to analyze a zero-day suspicious application, and compare or associate it with existing malware families in the database? How to reveal similar malicious logic in various malware, and to quickly identify the new malicious code segment? In this paper, we present the design and implementation of DroidAnalytics, a signature based analytic system to automatically collect, manage, analyze and extract android malware. The system facilitates analysts to retrieve, associate and reveal malicious logics at the ""opcode level"". We demonstrate the efficacy of DroidAnalytics using 150, 368 Android applications, and successfully determine 2, 475 Android malware from 102 different families, with 327 of them being zero-day malware samples from six different families. To the best of our knowledge, this is the first reported case in showing such a large Android malware analysis/detection. The evaluation shows the DroidAnalytics is a valuable tool and is effective in analyzing malware repackaging and mutations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680837,no,undetermined,0
Dynamic information-theoretic measures for security informatics,"Many important security informatics problems require consideration of dynamical phenomena for their solution; examples include predicting the behavior of individuals in social networks and distinguishing malicious and innocent computer network activities based on activity traces. While information theory offers powerful tools for analyzing dynamical processes, to date the application of information-theoretic methods in security domains has focused on static analyses (e.g., cryptography, natural language processing). This paper leverages information-theoretic concepts and measures to quantify the similarity of pairs of stochastic dynamical systems, and shows that this capability can be used to solve important problems which arise in security applications. We begin by presenting a concise review of the information theory required for our development, and then address two challenging tasks: 1.) characterizing the way influence propagates through social networks, and 2.) distinguishing malware from legitimate software based on the instruction sequences of the disassembled programs. In each application, case studies involving real-world datasets demonstrate that the proposed techniques outperform standard methods.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578784,no,undetermined,0
E-Learning standards and learning analytics. Can data collection be improved by using standard data models?,"The Learning Analytics (LA) discipline analyzes educational data obtained from student interaction with online resources. Most of the data is collected from Learning Management Systems deployed at established educational institutions. In addition, other learning platforms, most notably Massive Open Online Courses such as Udacity and Coursera or other educational initiatives such as Khan Academy, generate large amounts of data. However, there is no generally agreedupon data model for student interactions. Thus, analysis tools must be tailored to each system's particular data structure, reducing their interoperability and increasing development costs. Some e-Learning standards designed for content interoperability include data models for gathering student performance information. In this paper, we describe how well-known LA tools collect data, which we link to how two e-Learning standards - IEEE Standard for Learning Technology and Experience API - define their data models. From this analysis, we identify the advantages of using these e-Learning standards from the point of view of Learning Analytics.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6530268,no,undetermined,0
Efficient techniques for relative motion analysis between eccentric orbits under J2 effect,"Accurate orbital propagation is required in order to correctly estimate (in design phase) and carry out (during operations) the control actions needed to maintain relative geometry among formation's spacecraft. Such an accurate propagation could be easily obtained by numerical methods, but their relevant computation cost would neither allow for general trade-offs during design nor match the onboard capabilities once spacecraft are in space. The interest for analytic, closed form solution is clear, as it would save on computation resources and allows for both speed and portability advantages. This paper proposes a special writing of the equations of motion which does provide a closed form solution for orbits including the oblateness effect. Such a representation is far more realistic than the approximate Keplerian one for LEO and medium altitude formations environment, and has indeed a remarkable appeal. The approach, originated from previous literature, is to express the variables of interest (radius, node, inclination, anomaly) as a series, which can be limited to the desired accuracy level in terms of eccentricity. The authors worked on this approach for several years, including currently available symbolic mathematics to allow for exact computation of the parameters of interest at every desired time. The more important contribution is a correct writing of the formulation in terms of relative dynamics, i.e. in terms of differences in the orbital parameters of the platforms, which is actually what is required in the spacecraft formation case. The paper details this special writing of the equation of motion and provides the analytical solution for eccentricities up to 0.2; i.e., remarkably extending the range of orbits previously considered in literature. These solutions are validated with respect to standard numerical propagators that end up with taking orders of magnitude longer to provide the same accuracy. Their quite high efficiency in terms of computational - esources needed make them a suitable solution for inclusion in the onboard software, or a performing option for trade-off analysis during design phase.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497186,no,undetermined,0
Embedded Analytics and Statistics for Big Data,"Embedded analytics and statistics for big data have emerged as an important topic across industries. As the volumes of data have increased, software engineers are called to support data analysis and applying some kind of statistics to them. This article provides an overview of tools and libraries for embedded data analytics and statistics, both stand-alone software packages and programming languages with statistical capabilities.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648585,no,undetermined,0
Enabling Bring-Your-Own-Device using mobile application instrumentation,"Many enterprises are investigating Bring-Your-Own-Device (BYOD) policies, which allow employees to use their personal devices in the workplace. This has led to mixed-use scenarios, where consumer and enterprise software are installed on the same device. In this paper, we describe the Secured Application Framework for Enterprise (SAFE), a comprehensive system for enabling BYOD that allows enterprise and consumer applications to coexist side-by-side on the device. Rather than partition the device by profiles, SAFE embeds enterprise functions in each enterprise application; this allows for a seamless user experience and minimal intrusiveness on the part of the enterprise. We describe the SAFE toolset that implements the embedding of the SAFE instrumentation layer, and then provide an overview of several enterprise features that can be configured using SAFE. Specifically, we describe modeling for analytics, testing and replay, anomaly detection, and cloud data services, all enterprise features that can transparently be added to mobile applications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6665094,no,undetermined,0
Enabling Distributed Key-Value Stores with Low Latency-Impact Snapshot Support,"Current distributed key-value stores generally provide greater scalability at the expense of weaker consistency and isolation. However, additional isolation support is becoming increasingly important in the environments in which these stores are deployed, where different kinds of applications with different needs are executed, from transactional workloads to data analytics. While fully-fledged ACID support may not be feasible, it is still possible to take advantage of the design of these data stores, which often include the notion of multiversion concurrency control, to enable them with additional features at a much lower performance cost and maintaining its scalability and availability. In this paper we explore the effects that additional consistency guarantees and isolation capabilities may have on a state of the art key-value store: Apache Cassandra. We propose and implement a new multiversioned isolation level that provides stronger guarantees without compromising Cassandra's scalability and availability. As shown in our experiments, our version of Cassandra allows Snapshot Isolation-like transactions, preserving the overall performance and scalability of the system.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623643,no,undetermined,0
Engineering High-Performance Community Detection Heuristics for Massive Graphs,"The amount of graph-structured data has recently experienced an enormous growth in many applications. To transform such data into useful information, high-performance analytics algorithms and software tools are necessary. One common graph analytics kernel is community detection (or graph clustering). Despite extensive research on heuristic solvers for this task, only few parallel codes exist, although parallelism is often necessary to scale to the data volume of real-world applications. We address the deficit in computing capability by a flexible and extensible clustering algorithm framework with shared-memory parallelism. Within this framework we implement our parallel variations of known sequential algorithms and combine them by an ensemble approach. In extensive experiments driven by the algorithm engineering paradigm, we identify the most successful parameters and combinations of these algorithms. The processing rate of our fastest algorithm exceeds 10M edges/second for many large graphs, making it suitable for massive data streams. Moreover, the strongest algorithm we developed yields a very good tradeoff between quality and speed.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6687351,no,undetermined,0
Entropic Inequalities and Marginal Problems,"A marginal problem asks whether a given family of marginal distributions for some set of random variables arises from some joint distribution of these variables. Here, we point out that the existence of such a joint distribution imposes nontrivial conditions already on the level of Shannon entropies of the given marginals. These entropic inequalities are necessary (but not sufficient) criteria for the existence of a joint distribution. For every marginal problem, a list of such Shannon-type entropic inequalities can be calculated by Fourier-Motzkin elimination, and we offer a software interface to a Fourier-Motzkin solver for doing so. For the case that the hypergraph of given marginals is a cycle graph, we provide a complete analytic solution to the problem of classifying all relevant entropic inequalities, and use this result to bound the decay of correlations in stochastic processes. Furthermore, we show that Shannon-type inequalities for differential entropies are not relevant for continuous-variable marginal problems; non-Shannon-type inequalities are both in the discrete and in the continuous case. In contrast to other approaches, our general framework easily adapts to situations where one has additional (conditional) independence requirements on the joint distribution, as in the case of graphical models. We end with a list of open problems. A complementary article discusses applications to quantum nonlocality and contextuality.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6336823,no,undetermined,0
Comparing optimum operation of Pulse Width-Pulse Frequency and Pseudo-Rate modulators in spacecraft attitude control subsystem employing thruster,"The Aim of this paper is to verify which modulator will extend the life time of a spacecraft attitude control system utilizing thrusters. For this purpose, the optimal region of operation for Pulse Width-Pulse Frequency modulator and Pseudo-Rate modulator is defined where they are used as a compensator in the control subsystem. The optimal region is where the modulator behavior is close to linear and simultaneously, fuel consumption and thruster activity is at the lowest. Because of nonlinear nature of the modulator, an analytic approach is difficult so as an alternative, system simulations are carried out. Optimal region of parameters is determined through Static and dynamic analysis. Modulators are compared by employing them in a spacecraft model for minimum fuel use once for same parameters and once for their optimal parameters. Results give valuable information about control system fuel usage and reliability and effects of modulators in control effort. Trustworthiness and accuracy of results are increased by employing nonlinear attitude dynamics in system simulations. Results can be utilized for practical designs in order to select proper modulator for different missions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6581286,no,undetermined,0
Compact Wideband Corrugated Feedhorns With Ultra-Low Sidelobes for Very High Performance Antennas and Quasi-Optical Systems,"The corrugated or scalar feedhorn has found many applications in millimeter wave and sub-millimeter wave systems due to its high beam symmetry, relatively low sidelobe levels and strong coupling to the fundamental mode Gaussian beam. However, for applications such as millimeter wave cosmology, space-based experiments, or even high performance imaging, there is a generic requirement to reduce the size of horns whilst maintaining very high levels of performance. In this paper we describe a general analytic methodology for the design of compact dual-profiled corrugated horns with extremely low sidelobe levels. We demonstrate that it is possible to achieve -50 dB sidelobe levels, over wide bandwidths with short horns, which we believe represents state-of-the-art performance. We also demonstrate experimentally a simple scalar design that operates over wide bandwidths and can achieve sidelobes of better than -40 dB, whilst maintaining a frequency independent phase center. This design methodology has been validated experimentally by the successful manufacture and characterization of feedhorns at 94 GHz and 340 GHz for both radar and quasi-optical instrumentation applications.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6420894,no,undetermined,0
Collaborative Analytics with Genetic Programming for Workflow Recommendation,"Formulation of appropriate data analytics workflows requires intricate knowledge and rich experiences of data analytics experts. This problem is further compounded by continuous advancement and improvement in analytical algorithms. In this paper, a generic non-domain specific solution for the creation of appropriate workflows targeted at supervised learning problems is proposed. Our adaptive workflow recommendation engine based on collaborative analytics matches analytics needs with relevant workflows in repository. It is capable of picking workflows with better performance as compared to randomly selected workflows. The recommendation engine is now augmented by a workflow optimizer that applies genetic programming to further improve the recommended workflows through iterative evolution, leading to better alternative workflows. This unique Collaborative Analytics Recommender System is tested on seven UCI benchmark datasets. It is shown that the final workflows produced by the system could closely approximate, in terms of accuracy, the best workflows that analytics experts could possibly design.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6721870,no,undetermined,0
Cloud-Based Software Platform for Big Data Analytics in Smart Grids,"This article focuses on a scalable software platform for the Smart Grid cyber-physical system using cloud technologies. Dynamic Demand Response (D<sup>2</sup>R) is a challenge-application to perform intelligent demand-side management and relieve peak load in Smart Power Grids. The platform offers an adaptive information integration pipeline for ingesting dynamic data; a secure repository for researchers to share knowledge; scalable machine-learning models trained over massive datasets for agile demand forecasting; and a portal for visualizing consumption patterns, and validated at the University of Southern California's campus microgrid. The article examines the role of clouds and their tradeoffs for use in the Smart Grid Cyber-Physical Sagileystem.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475927,no,undetermined,0
Animating signing avatar using descriptive sign language,"This paper proposes an interactive manipulation of signing avatar (complex human's figures) by means of a descriptive sign language. The objective is to improve the human-computer communication, to generate precise signs for Deaf community, to control and animate the virtual character. This approach has been developed to manipulate robots through several analytic and numerical resolution methods. These methods have been applied to the virtual character for better monitoring and to ensure a realistic animation in real time.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578385,no,undetermined,0
Application management services analytics,"Enterprises often maintain many IT applications to support their business. Application Management Services (AMS) aim to maintain high levels of service quality and availability by restoring normal application service operations and minimizing negative business impact. In this paper, we present the AMS Analytics System for improving the productivity and quality of delivery for AMS practices. Issues regarding IT applications are formally referred as IT incidents or tickets, which are an important vehicle for measuring quality of AMS. IT incident ticket analytics, an important component of of the analytics system, measures workload variability, resource productivity and delivery performance using algorithms from statistics, queuing theory, data clustering and signal processing. The AMS Analytics System provides a standardized, integrated analytics platform supporting AMS delivery. It is built on a Web platform using a set of standard open stack software, enhanced with advanced analytics. Since its initial release, we have applied the AMS Analytics System to several dozens of real-world enterprise users, receiving very positive feedback.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6611442,no,undetermined,0
Approximate computing: Energy-efficient computing with good-enough results,"Summary form only given. With the explosion in digital data, computing platforms are increasingly being used to execute applications (such as web search, data analytics, sensor data processing, recognition, mining, and synthesis) for which äóìcorrectnessäó is defined as producing results that are good enough, or of sufficient quality. Such applications invariably demonstrate a high degree of inherent resilience to their underlying computations being executed in an approximate manner. This inherent resilience is due to several factors including redundancy in the input data, the statistical nature of the computations themselves, and the acceptability (often, inevitability) of less-than-perfect results. Approximate computing is an approach to designing systems that are more efficient, by leveraging the inherent resilience of applications. We will outline a range of approximate computing techniques that we have developed from software to architecture to circuits, which have shown promising results. We conclude with a discussion of some of the challenges that need to be addressed to facilitate a broader adoption of approximate computing.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604092,no,undetermined,0
Archetypical motion: Supervised game behavior learning with Archetypal Analysis,"The problem of creating believable game AI poses numerous challenges for computational intelligence research. A particular challenge consists in creating human-like behaving game bots by means of applying machine learning to game-play data recorded by human players. In this paper, we propose a novel, biologically inspired approach to behavior learning for video games. Our model is based on the idea of movement primitives and we use Archetypal Analysis to determine elementary movements from data in order to represent any player action in terms of convex combinations of archetypal motions. Given these representations, we use supervised learning in order to create a system that is able to synthesize appropriate motion behavior during a game. We apply our model to teach a first person shooter game bot how to navigate in a game environment. Our results indicate that the model is able to simulate human-like behavior at lower computational costs than previous approaches.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6633609,no,undetermined,0
Architecture Security Evaluation Method Based on Security of the Components,"This paper presents a quantitative architecture security evaluation method to identify potential risks of an architecture. The method is based on security of the architecture components. In this method, components of the architecture are classified and their security measures are identified according to component function and architecture level. Then, an integration process applies analytic hierarchy process (AHP) and fuzzy evaluation analysis to determine quantitative and qualitative factors in evaluating the security of components. These factors are used to obtain security conclusions of the architecture. The experiment shows that the method not only improves efficiency of the evaluation, but also makes security evaluation process more objective and accurate.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805446,no,undetermined,0
Assessment of student's learning style and engagement in traditional based software engineering education,"Software Engineering courses are nucleus elements of the Computer Science syllabus. While the main aspire of such courses is to give students practical industry-relevant äóìsoftware engineering in the largeäó familiarity, frequently such courses plummet short of this significant intention owed to be short of industrial experience and support infrastructure, degenerating the course into äóìone big coding assignmentäó. Therefore, it is obligatory to design and develop improved infrastructure support for teaching or taking such courses. This would benefit instructors and students communities in computer science atmosphere. Learning analytics lend a hand to the learners to enhance their learning tricks. This was scrutinized among the software engineering students how the learning style and learning engagement sways in gathering knowledge. This paper discusses the importance of learning analytics in software engineering education especially the learning style, learning engagement and its influences in this domain.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606434,no,undetermined,0
Attitude Control of Five Degrees of Freedom Air-Bearing Platform Based on Fractional Order Sliding Mode,"Spacecraft attitude determination and control hard verification and software development can be simultaneously simulated with five degrees of freedom air-bearing spacecraft simulator. In this paper, a new robust fractional order sliding mode controller is proposed for simulator attitude control with control input limited amplitude under considering gravity unbalance torque disturbance and actuator assembling error. The attitude system can convergence to fractional order sliding mode surface within the finite time, which is proven by using fractional order Lyapunov stability theory. Furthermore, numerical simulations are also included to reinforce the analytic results and to validate the excellent effect of the new robust fractional sliding mode controller.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6840731,no,undetermined,0
Backtrack-Based Failure Recovery in Distributed Stream Processing,"Since stream analytics is treated as a kind of cloud service, there exists a pressing need for its reliability and fault-tolerance. In a streaming process, the parallel and distributed tasks are chained in a graph-structure with each task transforming a stream to a new stream, the transaction property guarantees the streaming data, called tuples, to be processed in the order of their generation in every dataflow path, with each tuple processed once and only once. The failure recovery of a task allows the previously produced results to be corrected for eventual consistency, which is different from the instant consistency of global state enforced by the failure recovery of general distributed systems, and therefore presents new technical challenges. Transactional stream processing typically requires every task to checkpoint its execution state, and when it is restored from a failure, to have the last state recovered from the checkpoint and missing tuple re-acquired and processed. Currently there exist two kind approaches: one treats the whole process as a single transaction, and therefore suffers from the loss of intermediate results during failures, the other relies on the receipt of acknowledgement (ACK) to decide whether moving forward to emit the next resulting tuple or resending the current one after timeout, on the per-tuple basis, thus incurs extremely high latency penalty. In contradistinction to the above, we propose the backtrack mechanism for failure recovery, which allows a task to process tuples continuously without waiting for ACKs and without resending tuples in the failure-free case, but to request (ASK) the source tasks to resend the missing tuples only when it is restored from a failure which is a rare case thus has limited impact on the overall performance. We have implemented the proposed mechanisms on Fontainebleau, the distributed stream analytics infrastructure we developed on top of Storm. As a principle, we ensure all the transactional proper- ies to be system supported and transparent to users. Our experience shows that the ASK-based recovery mechanism significantly outperforms the ACK-based one.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598475,no,undetermined,0
Big Data -- Opportunities and Challenges Panel Position Paper,This paper summarizes opportunities and challenges of big data. It identifies important research directions and includes a number of questions that have been debated by the panel.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649870,no,undetermined,0
"BioExtract Server, a Web-based workflow enabling system, leveraging iPlant collaborative resources","In order to handle the vast quantities of biological data generated by high-throughput experimental technologies, the BioExtract Server (bioextract.org) has leveraged iPlant Collaborative (www.iplantcollaborative.org) functionality to help address big data storage and analysis issues in the bioinformatics field. The BioExtract Server is a Web-based, workflow-enabling system that offers researchers a flexible environment for analyzing genomic data. It provides researchers with the ability to save a series of BioExtract Server tasks (e.g. query a data source, save a data extract, and execute an analytic tool) as a workflow and the opportunity for researchers to share their data extracts, analytic tools and workflows with collaborators. The iPlant Collaborative is a community of researchers, educators, and students working to enrich science through the development of cyberinfrastructure - the physical computing resources, collaborative environment, virtual machine resources, and interoperable analysis software and data services - that are essential components of modern biology. The iPlant Agave API (Agave), developed through the iPlant Collaborative, is a hosted, Software-as-a-Service resource providing access to a collection of High Performance Computing (HPC) and Cloud resources [6]. Leveraging Agave, the BioExtract Server gives researchers easy access to multiple high performance computers and delivers computation and storage as dynamically allocated resources via the Internet.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702692,no,undetermined,0
Biological tissues dispersivity and power loss density in transcranial magnetic stimulation,"Power loss density and energy converted into heat considering dispersive biological tissue was investigated in the framework of transcranial magnetic stimulation (TMS). The solutions were obtained by applying an analytic method of Eaton [1] and a finite-element method (FEM). A commercial available figure-of-8 coil with a biphasic current pulse operates as the source of excitation. The calculations were performed in the frequency domain by using 1000 complex harmonics at different frequencies in order to reconstruct the transient signal in the excitation coil. The displacement current density was taken into account to provide an accurate estimate of the total energy. The human head was modeled as a homogeneous isotropic dispersive volume conductor consisting of grey matter (GM). The dielectric properties of GM were calculated in dependence on the frequency by means of the Cole-Cole model from Gabriel et al. [2]. The induced electric field as well as the current density are compared against those obtained in the non-dispersive case. The results revealed an increased magnitude of the peak value of the current density by 22.3% compared to the non-dispersive case. However, the induced electric field was not influenced by tissues dispersivity. Finally it was shown that the frequency dependent biological tissue affects the time development of the power loss density but has only minor effects on the total energy converted into heat.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671857,no,undetermined,0
Blending SQL and NewSQL Approaches: Reference Architectures for Enterprise Big Data Challenges,"As it becomes ever more pervasively engaged in data driven commerce, a modern enterprise becomes increasingly dependent upon reliable and high speed transaction services. At the same time it aspires to capitalize upon large inflows of information to draw timely business insights and improve business results. These two imperatives are frequently in conflict because of the widely divergent strategies that must be pursued: the need to bolster on-line transactional processing generally drives a business towards a small cluster of high-end servers running a mature, ACID compliant, SQL relational database, while high throughput analytics on massive and growing volumes of data favor the selection of very large clusters running non-traditional (NoSQL/NewSQL) databases that employ softer consistency protocols for performance and availability. This paper describes an approach in which the two imperatives are addressed by blending the two types (scale-up and scale-out) of data processing. It breaks down data growth that enterprises experience into three classes-Chronological, Horizontal, and Vertical, and picks out different approaches for blending SQL and NewSQL platforms for each class. To simplify application logic that must comprehend both types of data platforms, the paper describes two new capabilities: (a) a data integrator to quickly sift out updates that happen in an RDBMS and funnel them into a NewSQL database, and (b) extensions to the Hibernate-OGM framework that reduce the programming sophistication required for integrating HBase and Hive back ends with application logic designed for relational front ends. Finally the paper details several instances in which these approaches have been applied in real-world, at a number of software vendors with whom the authors have collaborated on design, implementation and deployment of blended solutions.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6685675,no,undetermined,0
Branching strategies based on Social Networks,"Effective code branching strategy must be adapted to the unique needs of each organization. Teams and workflows organization as well as software architecture should be reflected in the branching strategies to maximize productivity and to minimize development risks. When conceptualized carefully, proper branching structure produces superior results. This paper proposes an analytic approach for adapting structure of branches based on Social Network Analysis to find out Branch-dependencies. The article provides context-based scenarios of successful application of such branching strategies in different situations.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607693,no,undetermined,0
Bringing big analytics to the masses,Big data analytics is valuable to many companies but has been too complex and expensive for smaller businesses. This is beginning to change.,2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6419709,no,undetermined,0
Building value chain through actionbale benchmarking for sustainability and excellence,"Knowledge scarcity has been, and will continue to be the de-facto pain point to almost every organization globally, more specifically to Information Technology (IT) services organizations. Several initiatives, however, has been underwent in many organizations including knowledge transformation, knowledge re-engineering etc., to support the new business models for sustainability. Unfortunately, the knowledge scarcity journey is neither the beginning nor the end. Therefore creating knowledge based value chains are absolutely critical for organizational sustainability and excellence. One big question remains: how do we create value for existence? Our research reveals that autonomous value creation organizational strategy will fail to deliver the promise if is designed and implemented without data integration and analytics. We, the authors, attempt to leverage our experiences, from in-the-field and research work, and propose Generic Benchmarking Integrated Innovation Framework (GBMIIF), to transform insights-to actions-to results, in making the strategy work. What differentiates this paper from other is, however, to provide process intelligence to make informed decisions using the framework to align organizational objectives. Central to the factors is analytics with research findings.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915068,no,undetermined,0
Business intelligence solutions in healthcare a case study: Transforming OLTP system to BI solution,"Healthcare environment is growing to include not only the traditional information systems, but also a business intelligence platform. For executive leaders, consultants, and analysts, there is no longer a need to spend hours in design and develop of typical reports or charts, the entire solution can be completed through using Business Intelligence äóìBIäó software. This paper discusses current state-of-the-art B.I components (tools) and outlines hospitals advances in their businesses by using B.I solutions through focusing on inter-relationship of business needs and the IT technologies. We also present a case study that illustrates of transforming a traditional online transactional processing (OLTP) system towards building an online analytical processing (OLAP) solution.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579551,no,undetermined,0
Catching the wave: Big data in the classroom,"Many diverse domains-in the sciences, engineering, healthcare, and homeland security-have been grappling with the analysis of äóìBig Data,äó which has become shorthand to represent extremely large amounts of diverse types of data. A recent Gartner report predicts that around 4.4 million IT jobs globally will be created by 2015 to support Big Data, with 1.9 million of those jobs in the United States. Therefore, understanding approaches and techniques for handling and analyzing Big Data from diverse domains has become crucial for not only in computing but also engineering students. The mini-workshop will make use of active and collaborative learning exercises to introduce faculty in computer science, software engineering, and other disciplines to concepts and techniques involved in managing and analyzing Big Data. Approaches for incorporating Big Data into the engineering and computing curricula will also be presented.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684855,no,undetermined,0
Cloud Bank Model Based on AHP Resource Scheduling Strategy Research,"Under the circumstances of the cloud bank model, this paper studies resource scheduling problem on cloud computing. Considering the the user's needs for optimal allocation of resources and the shortest time to complete the total task, cloud computing resource scheduling of analytic hierarchy process model (AHP) is established. Under this model, conduct simulation experiment through the CloudSim platform. Results show that the AHP algorithm under the model, while protecting users' (quality of service) QoS, is even shorter than the general algorithm in completing the total task time. It is an effective resource scheduling algorithm.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6821038,no,undetermined,0
Cloud Based Big Data Analytics for Smart Future Cities,"ICT is becoming increasingly pervasive to urban environments and providing the necessary basis for sustainability and resilience of the smart future cities. Often ICT tools for a smart city deal with different application domains e.g. land use, transport, energy, and rarely provide an integrated information perspective to deal with sustainability and socioeconomic growth of the city. Smart cities can benefit from such information using Big, and often real-time cross-thematic, data collection, processing, integration and sharing through inter-operable services deployed in a Cloud environment. However, such information utilisation requires appropriate software tools, services and technologies to collect, store, analyse and visualise large amounts of data from the city environment, citizens and various departments and agencies at city scale. This paper presents a theoretical perspective on the smart cities focused Big data processing and analysis by proposing a Cloud-based analysis service that can be further developed to generate information intelligence and support decision-making in smart future cities context.",2013,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6809436,no,undetermined,0
Undercovering research trends: Network analysis of keywords in scholarly articles,"In this study we present a network approach to uncovering trends in an area of research by analyzing keywords appearing in scholarly articles. We represent keywords as nodes. We link a pair of keywords if they appear in the same article. Each link is assigned a weight, representing the number of co-occurrences of the pair in different articles. We perform a statistical and visual analysis of the network's structural and temporal characteristics. These characteristics or patterns provide a broad understanding of how keywords are organized and research areas evolved over time. Our findings show that keywords organize themselves into three categories: topical keywords, complimentary keywords and, diverse keywords. Through comparative analysis of the networks built from articles published in two successive time windows, we are able to detect some interesting keyword patterns and emerging areas. Results from this analysis can be used for identifying emerging research areas or updating academic programs and course curricula. To demonstrate the approach we use keywords from the European Journal of Operational Research.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261963,no,undetermined,0
TSVHOA-traffic sensitive Vertical Handoff algorithm for best quality network technology,"Growth in the field of wireless communication technology has captivated the user to make use of it in recent years. The user expects more supporting services to access the best available network which satisfies user preferences and application requirements. To fulfill the requirements of the user, the next generation mobile terminals have multiple interface technologies, which allow data to be received over multiple system bearers with different characteristics. During user mobility, there is a need to have vertical handoff solution to have seamless communication. The main issue is how to select the best available network to meet requirements of user, network and application during handoff. This paper proposes an algorithm TSVHOA for Vertical Handoff decision to select the best quality network technology based on the traffic type using Analytical Hierarchical Process (AHP). Also, the algorithm is simulated with respect to different network technologies like WiFi, WiMAX and WCDMA to select the best one.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549327,no,undetermined,0
Big Data analytics,"In this paper, we explain the concept, characteristics & need of Big Data & different offerings available in the market to explore unstructured large data. This paper covers Big Data adoption trends, entry & exit criteria for the vendor and product selection, best practices, customer success story, benefits of Big Data analytics, summary and conclusion. Our analysis illustrates that the Big Data analytics is a fast-growing, influential practice and a key enabler for the social business. The insights gained from the user generated online contents and collaboration with customers is critical for success in the age of social media.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398180,no,undetermined,0
Innovation-Related Enterprise Semantic Search: The INSEARCH Experience,"Innovation is a crucial process for enterprizes and pushes for strict requirements towards semantic technologies. Large scale and timely search processes on the Web are here often involved in different business analytics tasks. In the European project INSEARCH, an advanced information retrieval system has been developed integrating robust semantic technologies and industry-standard software architectures for Web monitoring and alerting, proactive search as well as personalized domain-specific classification and ranking functionalities.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337104,no,undetermined,0
From Business Application Execution to Design Through Model-Based Reporting,"Cross-disciplinary models constitute essential instruments to master complexity. Often it is easier to relate to high-level concepts than to deal with low-level technical details. In model-driven engineering (MDE) models are designated a pivotal role from which systems are generated. As such, MDE enables different stakeholders of business applications to participate in the engineering process. Until now however, MDE does not penetrate phases beyond generation and deployment such as monitoring, analysis, and reporting. To display information from runtime and analytics it would be interesting if reporting could utilize models from design time. Therefore, this paper presents model-based reporting (MbR). Bridging the gap between reporting and design, it enables stakeholders to intuitively specify the reporting through a domain-specific language (DSL) while accelerating development cycles. In non-model-driven settings, MbR can help to introduce models as a first step towards MDE.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337245,no,undetermined,0
GIS-Based Evaluating and Analysis for Farmland Fertility of Men-hai County,"Through using the ArcGIS software as the platform, and taking DEM analysis, raster calculation and spatial interpolation of spatial analysis technology as the core method, this paper selects Menghai County as the study area, combines with the Delphi method, analytic hierarchy process, fuzzy membership function, farmland Integrated Fertility Index ( IFI ) and other methods, to establish major factors of influence and their weights for the farmland land productivity, as well as built a comprehensive evaluation system and it's spatial computing model. Meanwhile, the analysis such as spatial distribution, area statistics and farmland nutrient analysis for the evaluating results is also discussed in detail. The experimental results and analysis show that, 51575.73 hectares of cultivated land in Menghai County is divided into six grades, which percentages on the total farmland from grade one to grade six respectively are 14.12%, 27.4%, 20.17%, 17.23%, 12.70% and 8.38%, and the level over three grade of cultivated land occupies only less than 60%. As a result, it could be declared that the overall situation of cultivated land fertility is not good, which needs the government to adjust measures to local conditions to maintain the existing quality of cultivated land resources, as well as to enhance the farming ability of cultivated land from level of grade 3, especially grade 4 and 5 to higher level.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449557,no,undetermined,0
HPC-VMs: Virtual machines in high performance computing systems,"The concept of virtual machines dates back to the 1960s. Both IBM and MIT developed operating system features that enabled user and peripheral time sharing, the underpinnings of which were early virtual machines. Modern virtual machines present a translation layer of system devices between a guest operating system and the host operating system executing on a computer system, while isolating each of the guest operating systems from each other. <sup>1</sup> In the past several years, enterprise computing has embraced virtual machines to deploy a wide variety of capabilities from business management systems to email server farms. Those who have adopted virtual deployment environments have capitalized on a variety of advantages including server consolidation, service migration, and higher service reliability. But they have also ended up with some challenges including a sacrifice in performance and more complex system management. Some of these advantages and challenges also apply to HPC in virtualized environments. In this paper, we analyze the effectiveness of using virtual machines in a high performance computing (HPC) environment. We propose adding some virtual machine capability to already robust HPC environments for specific scenarios where the productivity gained outweighs the performance lost for using virtual machines. Finally, we discuss an implementation of augmenting virtual machines into the software stack of a HPC cluster, and we analyze the affect on job launch time of this implementation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6408668,no,undetermined,0
Hybrid ANP: Quality attributes decision modeling of a product line architecture design,"Architecture design of a software product line includes a lot of decision of how the variability component will be implemented. The decision corresponds to the requirements of the architecture specifications. Furthermore, most of the decision may have dependency between components. However, only some research devoted to address the quality attributes. In this paper, we propose a new approach that hybrid the Analytical Network Process to model the decision, which addressed the quality attributes. In our method, first, the software component derives from the feature model by the domain expert. Then, groups the components to specific quality attributes. Subsequently, models the dependency of quality attributes in the ANP model. We use a eLearning Product Line Architecture to demonstrate the feasibility of our approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319572,no,undetermined,0
ICT Solution for Managing Electronic Health Record in India,"Government of India is planning to spend 2.5 percent of GDP in Public Healthcare during Twelfth Five Year Plan (2012-2017). It has been well recognized by the Government that IT would play a major role in achieving the intended health goals and improving the overall health status and services in India. Individuals' health data is the key source for Government to assess the healthcare status of the country. This paper proposes an ICT (Information Communication and Technology) solution to capture, manage and track individual's health data in a phased manner. This health data will be exposed to various health management information systems for further usage and analysis. Analytics on data will help in tracking health goals against measurable targets, real time disease surveillance, in pattern analysis and in identifying community needs of healthcare services. Proposed scalable, reliable and highly economical solution is primarily based on open source software and commodity hardware.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468181,no,undetermined,0
Improved Estimation of Weibull Parameters Considering Unreliability Uncertainties,"We propose a linear regression method for estimating Weibull parameters from life tests. The method uses stochastic models of the unreliability at each failure instant. As a result, a heteroscedastic regression problem arises that is solved by weighted least squares minimization. The main feature of our method is an innovative s-normalization of the failure data models, to obtain analytic expressions of centers and weights for the regression. The method has been Monte Carlo contrasted with Benard's approximation, and Maximum Likelihood Estimation; and it has the highest global scores for its robustness, and performance.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6036203,no,undetermined,0
Incorporating hardware trust mechanisms in Apache Hadoop: To improve the integrity and confidentiality of data in a distributed Apache Hadoop file system: An information technology infrastructure and software approach,"Pairing Apache Hadoop distributed file storage with hardware based trusted computing mechanisms has the potential to reduce the risk of data compromise. With the growing use of Hadoop to tackle big data analytics involving sensitive data, a Hadoop cluster could be a target for data exfiltration, corruption, or modification. By implementing open standards based Trusted Computing technology at the infrastructure and application levels; a novel and robust security posture and protection is presented. An overview of the technologies involved, description of the proposed infrastructure, and potential software integrations are discussed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6477672,no,undetermined,0
Infographics at the Congressional Budget Office,"The Congressional Budget Office (CBO) is an agency of the federal government with about 240 employees that provides the U.S. Congress with timely, nonpartisan analysis of important budgetary and economic issues. Recently, CBO began producing static infographics to present its headline stories and to provide information to the Congress in different ways.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400533,no,undetermined,0
InfoMaps : A Session Based Document Visualization and Analysis Tool,"InfoMaps is an information visualization tool designed for personal information management and for supporting data analysis. In this paper we briefly discuss the design of InfoMaps and explain its role in finding relevant information. InfoMaps is tightly coupled with Weave, an open source framework, providing a set of data analysis and visualization tools. Weave's framework is built with session states as its core and this provides InfoMaps the ability to store the entire user's interactions as well as visualization layouts. We discuss the implications of using the Weave framework with InfoMaps and its relevance to the field of information retrieval and visual analytics.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6295826,no,undetermined,0
Intelligent automation and controls of power industry Microgrid solutions,"Power is one of the fundamentals in developing and developed nations world-wide. Over 80% of the world's population does not have reliable power. Without guaranteed power, nations quickly lose capabilities, efficiencies, and other critical infrastructure to their people and livelihood. The barrier for reliable power, in the past was the large investment requirement for centralized bulk generation. Additionally, the belief was that without large centralized power, the nation would not get efficiencies needed to provide reliable power. Across the world today, still many countries have inconsistent power and see rolling brown or black outs hourly or daily. At best developed countries have, reliable and secure power that is centrally managed through a bulk power grid and sometime governed by central government or large corporations. However with the advancement of distributed generation, substation automation, Microgrid (äóìMGäó) control software, the industry is going through a fundamental change. The change is bringing on smaller Microgrid pockets, smaller generation due with less capital investment, and higher efficiency solutions that can compete neck and neck with large bulk generation. All of these items, while critical for the new äóìWorld Power Infrastructureäó still lack some basic building blocks for the transformation to take its final shape. This paper will describe some of the concepts, barriers, lessons learned, and next steps to move from either unreliable Power structures or Central Bulk Power to Microgrid solutions with Distributed Automated Intelligent Power solutions.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320995,no,undetermined,0
Malware Analysis and attribution using Genetic Information,"As organizations become ever more dependent on networked operations, they are increasingly vulnerable to attack by a variety of attackers, including criminals, terrorists and nation states using cyber attacks. New malware attacks, including viruses, Trojans, and worms, are constantly and rapidly emerging threats. However, attackers often reuse code and techniques from previous attacks. Both by recognizing the reused elements from previous attacks and by detecting patterns in the types of modification and reuse observed, we can more rapidly develop defenses, make hypotheses about the source of the malware, and predict and prepare to defend against future attacks. We achieve these objectives in Malware Analysis and Attribution using Genetic Information (MAAGI) by adapting and extending concepts from biology and linguistics. First, analyzing the äóìgeneticsäó of malware (i.e., reverse engineered representations of the original program) provides critical information about the program that is not available by looking only at the executable program. Second, the evolutionary process of malware (i.e., the transformation from one species of malware to another) can provide insights into the ancestry of malware, characteristics of the attacker, and where future attacks might come from and what they might look like. Third, functional linguistics is the study of the intent behind communicative acts; its application to malware characterization can support the study of the intent behind malware behaviors. To this point in the program, we developed a system that uses a range of reverse engineering techniques, including static, dynamic, behavioral, and functional analysis that clusters malware into families. We are also able to determine the malware lineage in some situations. Using behavioral and functional analysis, we are also able to identify a number of functions and purposes of malware.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6461006,no,undetermined,0
Interactive visual exploration of simulator accuracy: A case study for stochastic simulation algorithms,"Visual Analytics offers various interesting methods to explore high dimensional data interactively. In this paper we investigate how it can be applied to support experimenters and developers of simulation software conducting simulation studies. In particular the usage and development of approximate simulation algorithms poses several practical problems, e.g., estimating the impact of algorithm parameters on accuracy or detecting faulty implementations. To address some of those problems, we present an approach that allows to relate configurations and accuracy visually and exploratory. The approach is evaluated by a brief case study, focusing on the accuracy of Stochastic Simulation Algorithms.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6465190,no,undetermined,0
Interfstud electromagnetic interference software: An accurate evaluation of current distribution in soil and in underground pipelines,"A user friendly software application was developed to study the electromagnetic interference problem between high voltage power lines and underground metallic pipelines. The dedicated developed InterfStud software was based also on the development of the magnetic vector potential and current distribution within the soil formulas. The best way to investigate the soil behavior as a conducting media is to determine the current distribution within the soil. New analytical formulas for the induced magnetic vector potential and current distribution in soil are derived. The determined formulas contain semi-infinite integral terms which will be evaluated. We might seek approximations of the semi-infinite integrals by replacing an exponential or algebraic function, the objective being to permit analytic integration. Since there is no good systematic method for making these replacements, their success depends directly on the intuition and ingenuity, taking into account that in practice the integrand has limited accuracy.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6396894,no,undetermined,0
Introduction to Geometric Processing through Optimization,"As an introduction to the field, this article shows how to formulate several geometry-processing operations to solve systems of equations in the&amp;amp;#x201C;least-squares&amp;amp;#x201D; sense. The equations are derived from local geometric relations using elementary concepts from analytic geometry, such as points, lines, planes, vectors, and polygons. Simple and useful tools for interactive polygon mesh editing result from the most basic descent strategies to solve these optimization problems. Throughout the article, the author develops the mathematical formulations incrementally, keeping in mind that the objective is to implement simple software for interactive editing applications that works well in practice. Readers can implement higher-performance versions of these algorithms by replacing the simple solvers proposed here with more advanced ones.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6265062,no,undetermined,0
Investigating network traffic through compressed graph visualization: VAST 2012 Mini Challenge 2 award: äóìGood adaptation of graph analysis techniquesäó,"Compressed Graph Visualization is a visual analytics method to scale the traditional node-link representation to huge graphs. This paper introduces its visualization, data processing and visual analytics process in solving Mini-Challenge 2 of VAST 2012 contest.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400517,no,undetermined,0
iPresage: An innovative patent landscaping tool,"iPresage is a web-based interactive tool for rapid and scalable patent landscaping. iPresage relies on sophisticated text mining, statistical machine learning and computational intelligence algorithms to allow analysts to mine large patent databases and evaluate whitespaces and temporal trends. This paper describes iPresage algorithms in detail and showcases iPresage functionality, using an illustrative example.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256503,no,undetermined,0
Keynote abstracts,The paper provides a list of keynote speeches that involves the following: IoT/CPS with Sensors and Robots: Actuation Challenges; Data Analytics as Challenging Parallel and Distributed Computing Applications; Parameterized Testability; New Information Technology Utilizing Human-Cyber-Physical Resources; Dealing with Worm Propagation: Modelling and Defence Strategies.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6589231,no,undetermined,0
Link datarate based admission control in wireless networks,"This paper presents a link data-rate based admission control mechanism for a wireless network using a transmit power allocation algorithm. The admission control mechanism uses the nodes' link datarate requirements as an input parameter. Transmit power control enables the nodes to set their power levels such that the required link datarate requirements are met while minimizing the interference caused to other transmissions. The link datarate requirement of a node is converted to its signal strength requirement that is subsequently used for admission control. We establish the relationship between key parameters in admission control such as signal strength requirement, link datarate requirement, number of nodes in the system etc. The proposed algorithm is studied using analytic and simulation techniques to show that it enables the nodes to meet their respective link datarate requirements.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6524225,no,undetermined,0
Log Analytics for Dependable Enterprise Telephony,"Enterprise telephony servers for large enterprises in business sectors like finance and healthcare, are complex software systems that require at least five nines of availability. Such systems need guaranteed service with minimal service disruption during failures and are built with availability designed into various aspects of operation. One of the approaches used to improve availability is detecting and predicting failures through analysis of system traces. System trace/debug logs are very textual in nature and as such are not amenable to a fully automated analysis. In this work we develop a log analysis technique for analyzing trace logs of large enterprise telephony systems. We analyze the logs to define normal and failure operational states of a running system and then categorize the current state of the system based on previously learned states. Our analysis shows that the technique is successful in correctly identifying the failure states of the system.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214764,no,undetermined,0
Low Power Surge: Advanced Base-Station Demand Impact on RF Components [MicroBusiness],"The cellular base-station market is entering a time of considerable change. Despite the worldwide macroeconomic outlook, conditions are favorable for continuing growth, particularly in emerging markets, with the market for base-station radio components set to top US$5.4 billion worldwide by 2015. Mobile service providers and equipment vendors are moving toward smaller cells and improving spectral efficiency with new air interfaces such as LTE. Advances in technology are opening up the market to a host of new players, posing a substantial threat to the more established semiconductor firms and subsystem equipment vendors that have dominated this space for so long. New semiconductor materials and techniques for improving linearity and efficiency could potentially serve as the Trojan horses that allow new players to enter the base-station component market and undermine the current market leaders.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6167601,no,undetermined,0
Forecasting approach of subway tunnels deposition based on ANP-SOM model,"The main influencing factors on its nature of complexity and indetermination are analysed deeply in this paper. Super Decisions software of ANP (Network Analytic Hierarchy Process) is used to solve the weights of all the influence factors. Use them as the initial weights to train SOM neural network for the fine-tuning; a new ANP-SOM model which consider to various factors, levels and impact of each other feedback synthetically is pretended. This model is well-adjusted, convergence and precision. It was used in forecasting of Xi'an Tunnel deposition and has achieved a good predicted result.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6492770,no,undetermined,0
Explicit solution for transcendental equation in power electronics applications,This work presents a method to formulate an explicit solution for roots of analytic transcendental equation applied in power electronics circuits. Transcendental equations are extensively used in power electronics modeling and the solution normally adopts numerical approaches. Sometimes the solution is presented by means of graphics or abacus. In this paper a Cauchy's integral theorem based method is presented using only basic concepts of complex integration. This method can be easily applied in mathematical software's or programmable calculators given an exact solution for the problem.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6452903,no,undetermined,0
Experimental analysis of different optimization techniques on leakage localization using series alignment,"Leakage detection and localization system plays crucial role in pipeline technology. Traditionally, negative pressure wave (NPW) based methods have been widely applied for its relatively low construction and maintenance cost. Among these NPW based methods, the slope based method is a major way to estimate leakage position. This paper tries to explore another way in leakage position estimation by aligning two or more NPW signals. In order to provide comprehensive comparison, diverse kinds of optimization methods are used as the optimization engine in searching for the alignment point. Their performance is experimentally compared in multiple NPW alignment tasks to show the strengths and the weaknesses of different kinds optimization algorithms. The experimental results also demonstrate the superiorities of the proposed approach in terms of localization accuracy and algorithm reliability, compared with slope based methods.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6273520,no,undetermined,0
Experiences in Delivering Power System Decision Support Tools over the Web Using Software-as-a-Service (SaaS) Model,"Power System Analytics Applications have always been available only as in-premise licensed software -- the use of a SaaS model for delivering the Analytics to the user is a first in the history of the power sector. In this paper, we describe how SaaS based webDNA architecture [1] can be extended to facilitate common power system data repository and associated analytics. We present case studies in delivering two power system decision support applications using this platform. First case study details the experience gained from providing Short Term Load Forecast (webSTLF) service to a leading private electricity distribution company in India. The second case study shares the experiences from delivering a Transmission System Usage Cost and Loss Allocation service (webNetUse) to the electricity regulators and system operators in India. Experience with managing various aspects critical to success of SaaS model like data security, scalability, usability, high availability and disaster recovery is described.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311073,no,undetermined,0
Byte-precision level of detail processing for variable precision analytics,"I/O bottlenecks in HPC applications are becoming a more pressing problem as compute capabilities continue to outpace I/O capabilities. While double-precision simulation data often must be stored losslessly, the loss of some of the fractional component may introduce acceptably small errors to many types of scientific analyses. Given this observation, we develop a precision level of detail (APLOD) library, which partitions double-precision datasets along user-defined byte boundaries. APLOD parameterizes the analysis accuracy-I/O performance tradeoff, bounds maximum relative error, maintains I/O access patterns compared to full precision, and operates with low overhead. Using ADIOS as an I/O use-case, we show proportional reduction in disk access time to the degree of precision. Finally, we show the effects of partial precision analysis on accuracy for operations such as k-means and Fourier analysis, finding a strong applicability for the use of varying degrees of precision to reduce the cost of analyzing extreme-scale data.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468527,no,undetermined,0
Calculation of the electromagnetic characteristics of an electrically excited synchronous motor for an EV,"This paper depicts an analytical way for calculating the electromagnetic characteristics and the no-load iron losses of an electrically excited synchronous traction motor with salient poles for an electric vehicle (EV). The calculations are applied on a machine design, of the Department of Electronic and Electrical Engineering of the Pohang University of Science and Technology (POSTECH), Korea. The calculation of the winding parameters such as the coil length, the resistance per phase including the current displacement effect at high frequencies in the stator winding and the stray reactance as well as the rotor pole stray flux are discussed. The magnetic characteristic is calculated, leading to the no-load and the short-circuit characteristic of the machine at a certain frequency. The analytical results are compared with numerical results, obtained by the Finite Element Software JMAG. The last part is about the analytical calculation of the no-load iron losses using the methods of Bertotti and Steinmetz. Here also a numerical result is used as a validation of the analytic approach.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6422786,no,undetermined,0
Call for papers: Special Issue of Tsinghua Science and Technology on Visualization and Computer Graphics,"This special issue invites original articles to address the challenges in the areas of Visualization, Visual Analytics, and Computer Graphics. Topics for suitable articles may cover theoretical and applications of visualization techniques, systems, software, hardware, and user interface issues. All the submissions will go through the blind peer-reviewed process. Authors are invited to submit high quality, original, unpublished research and application papers in all related areas.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6314536,no,undetermined,0
Change-oriented aircraft fuel burn and emissions assessment methodologies,"Advancing the National Airspace System (NAS) generally requires research to project the costs and benefits of proposed operational changes that improve system efficiency and environmental performance. Aviation regulatory agencies increasingly rely on aircraft fuel consumption and emissions assessments to support investment decisions needed to adopt new technologies that ensure the best value to the public. The varying scopes of such assessments and often limited availability of data to support evaluations of future change scenarios pose analytic challenges. This paper describes a methodological framework for standardizing benefit estimations. The framework is centered on the concept of operational change, identifies key benefit mechanisms, defines applicable metrics, and presents quantitative fuel burn and emission benefit estimation examples. It aims to support assessments that are based on on-board flight data, surveillance data, and model evaluations in a manner independent of specific implementations of simulation capabilities and software tools. The examples characterize and compare typical operational change scenarios on a parametric basis and rank associated benefit pools of actual changes observed in recent operational improvements including implementations of Performance Based Navigation (PBN) procedures at major airports in the NAS.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6218432,no,undetermined,0
CLOUD 2012: 2012 IEEE 5th International Conference on Cloud Computing [Cover art],The following topics are dealt with: cloud computing; admission control; analytics cloud platform; economic models; collaborative cloud computing; cloud performance management; cloud resource management; infrastructure-as-a-service clouds; cloud resource provisioning; virtual server image management; cloud privacy; cloud application deployment; cloud exploitation analysis; cloud SLA management; mobile cloud; cloud application security; cloud data storage; cloud service selection; cloud resource optimization; cloud cost models; cloud architecture; and cloud energy.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6253480,no,undetermined,0
Condition monitoring & fault diagnosis system for Offshore Wind Turbines,"Due to the technological development, the electronic power progress and economic stake, through the use of Wound Rotor Induction Motor (WRIM) has taken more and more places in different domains (transport, energy production, electric drive..,) thanks to their robustness, efficiency and lower costs. Despite the performed work researches and the improvement that has been brought, these machines still remain the potential seats of failures both in stator and rotor levels. Consequently, WRIM faults detection is currently one of the centers of interest of several researches of both academic and industrial laboratories. In fact, this article addresses this problem by the use of Principal Components Analysis (PCA) for faults detection in Offshore Wind Turbine Generator (OWTG). An accurate analytic modeling of healthy and faulted OWTG is suggested to perform the data matrix needed for PCA method. Tests were achieved using a numeric simulator on Matlab/Simulink software. Analysis of OWTG simulation proves the efficiency of PCA method. Several simulation results will be presented and discussed.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221389,no,undetermined,0
Configurable and Extensible Multi-flows for Providing Analytics as a Service on the Cloud,"Compared to traditional analytics deployment models, cloud-based solutions for business analytics provide numerous advantages such as reduction of a large upfront infrastructural cost and the efforts to setup an in-house analytics team. Such advantages of cloud-based service delivery make it particularly attractive for small and medium businesses. In spite of these advantages, analytics penetration has been low particularly in developing regions such as India and China due to many other factors. In this paper, we propose pre-packaged configurable workflows for analytics as a means of endearing cloud-based analytics to customers, with a special focus on small and medium businesses in developing regions. We introduce the concept of configurable multi-flows that make it easy for non-technical personnel to use and customize without being aware of the technical details of the various operators involved in the workflow. Multi-flows comprise of an overlap of multiple possible workflows and are easily extensible to include more variations to support the evolving needs of customers non-disruptively and incrementally. We detail a case-study of the Retail sector where an extensive survey of retail businesses in India revealed that configurable pre-packaged workflows may indeed help improve market penetration. We then identify common analytics needs of retail customers, and detail how such tasks can be expressed as configurable multi-flows. Further, we describe a fully functional implementation of our system that supports configurable multi-flows for analytics. Finally, we illustrate the ease-of-use of configurable multi-flows with the use of multiple screenshots.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6310975,no,undetermined,0
Connecting the dots in IT service delivery: From operations content to high-level business insights,"IT service delivery relies on intelligent data-driven insights to make strategic decisions. It is a highly complex business with many sub-organizations that focus on different aspects of delivery operations. High-level business insights that emerge from understanding the collective value of all these viewpoints are invaluable to achieving excellent service quality and solid profit margin. However, this is hindered by the inability to integrate data models and taxonomies across business components such as asset management, configuration management, and incident management. Innovative solutions are necessary to effectively ""connect-the-dots"", bridging the gaps between available content and higher-level business insights. In this paper, we describe several real-world business decisions in service delivery and logistics that suffer from this content-model gap. We propose a unified approach to bridge this gap, with an information system component called Business-Knowledge Discovery Component. We discuss key challenges, architectural framework and the text analytic techniques that are involved.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6273572,no,undetermined,0
Cost-aware replication for dataflows,"In this work we are concerned with the cost associated with replicating intermediate data for dataflows in Cloud environments. This cost is attributed to the extra resources required to create and maintain the additional replicas for a given data set. Existing data-analytic platforms such as Hadoop provide for fault-tolerance guarantee by relying on aggressive replication of intermediate data. We argue that the decision to replicate along with the number of replicas should be a function of the resource usage and utility of the data in order to minimize the cost of reliability. Furthermore, the utility of the data is determined by the structure of the dataflow and the reliability of the system. We propose a replication technique, which takes into account resource usage, system reliability and the characteristic of the dataflow to decide what data to replicate and when to replicate. The replication decision is obtained by solving a constrained integer programming problem given information about the dataflow up to a decision point. In addition, we built a working prototype, CARDIO of our technique which shows through experimental evaluation using a real testbed that finds an optimal solution.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211896,no,undetermined,0
Data analytics in the cloud with flexible MapReduce workflows,"Data analytic applications are characterized by large data sets that are subject to a series of processing phases. Some of these phases are executed sequentially but others can be executed concurrently or in parallel on clusters, grids or clouds. The MapReduce programming model has been applied to process large data sets in cluster and cloud environments. For developing an application using MapReduce there is a need to install/configure/access specific frameworks such as Apache Hadoop or Elastic MapReduce in Amazon Cloud. It would be desirable to provide more flexibility in adjusting such configurations according to the application characteristics. Furthermore the composition of the multiple phases of a data analytic application requires the specification of all the phases and their orchestration. The original MapReduce model and environment lacks flexible support for such configuration and composition. Recognizing that scientific workflows have been successfully applied to modeling complex applications, this paper describes our experiments on implementing MapReduce as sub-workflows in the AWARD framework (Autonomic Workflow Activities Reconfigurable and Dynamic). A text mining data analytic application is modeled as a complex workflow with multiple phases, where individual workflow nodes support MapReduce computations. As in typical MapReduce environments, the end user only needs to define the application algorithms for input data processing and for the map and reduce functions. In the paper we present experimental results when using the AWARD framework to execute MapReduce workflows deployed over multiple Amazon EC2 (Elastic Compute Cloud) instances.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427527,no,undetermined,0
Description of the U.S. Geological Survey Geo Data Portal Data Integration Framework,"The U.S. Geological Survey has developed an open-standard data integration framework for working efficiently and effectively with large collections of climate and other geoscience data. A web interface accesses catalog datasets to find data services. Data resources can then be rendered for mapping and dataset metadata are derived directly from these web services. Algorithm configuration and information needed to retrieve data for processing are passed to a server where all large-volume data access and manipulation takes place. The data integration strategy described here was implemented by leveraging existing free and open source software. Details of the software used are omitted; rather, emphasis is placed on how open-standard web services and data encodings can be used in an architecture that integrates common geographic and atmospheric data.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212478,no,undetermined,0
Design and Development of an Adaptive Workflow-Enabled Spatial-Temporal Analytics Framework,"Cloud computing is a suitable platform for execution of complex computational tasks and scientific simulations that are described in the form of workflows. Such applications are managed by Workflow Management System (WfMS). Because existing WfMSs are not able to autonomically provision resources to real-time applications and schedule them while supporting fault tolerance and data privacy, we present a highly-scalable workflow-enabled analytics system that manages inter-dependable analytics tasks adaptively with varying operational requirements on a common platform and enables visualization of multidimensional datasets of real world phenomena. In this paper, we present the architecture of such a WfMS and evaluate it in terms of performance for execution of workflows in Clouds. A real world application of climate-associated dengue fever prediction was evaluated on public, private, and hybrid Clouds and experienced effective speedup in all the environments.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413591,no,undetermined,0
Designing a Programmable Wire-Speed Regular-Expression Matching Accelerator,"A growing number of applications rely on fast pattern matching to scan data in real-time for security and analytics purposes. The RegX accelerator in the IBM Power Edge of Network<sup>TM</sup> (PowerEN) processor supports these applications using a combination of fast programmable state machines and simple processing units to scan data streams against thousands of regular-expression patterns at state-of-the-art Ethernet link speeds. RegX employs a special rule cache and includes several new micro-architectural features that enable various instruction dispatch and execution options for the processing units. The architecture applies RISC philosophy to special-purpose computing: hardware provides fast, simple primitives, typically performed in a single cycle, which are exploited by an intelligent compiler and system software for high performance. This approach provides the flexibility required to achieve good performance across a wide range of workloads. As implemented in the PowerEN<sup>TM</sup> processor, the accelerator achieves a theoretical peak scan rate of 73.6 Gbit/s, and a measured scan rate of about 15 to 40 Gbit/s for typical intrusion detection workloads.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6493642,no,undetermined,0
Development of virtual lab system through application of fuzzy analytic hierarchy process,"Virtual lab has gained popularity among cloud computing practitioners due to its promise of on-demand provisioning of computer resources. However, lack of standardized guidelines hinders the adoption among educational institutions. Achievements of non-functional requirements possess greater challenge due to their qualitative nature. These requirements can be achieved by applying guidelines during software development. However, priority assessment of non-functional requirements is needed before the selection of suitable guidelines. This paper tries to apply fuzzy analytic hierarchy process in order to aggregate group decisions, as well as dealing with ambiguities of decision makers' judgement. The application of fuzzy analytic approach in this paper had successful create a solid foundation for software developers to identify the essential non-functional requirement in virtual lab environment. Importantly, the result can provide a promising reference model for better understanding of cloud computing in educational sector.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269258,no,undetermined,0
Does the Cost Function Matter in Bayes Decision Rule?,"In many tasks in pattern recognition, such as automatic speech recognition (ASR), optical character recognition (OCR), part-of-speech (POS) tagging, and other string recognition tasks, we are faced with a well-known inconsistency: The Bayes decision rule is usually used to minimize string (symbol sequence) error, whereas, in practice, we want to minimize symbol (word, character, tag, etc.) error. When comparing different recognition systems, we do indeed use symbol error rate as an evaluation measure. The topic of this work is to analyze the relation between string (i.e., 0-1) and symbol error (i.e., metric, integer valued) cost functions in the Bayes decision rule, for which fundamental analytic results are derived. Simple conditions are derived for which the Bayes decision rule with integer-valued metric cost function and with 0-1 cost gives the same decisions or leads to classes with limited cost. The corresponding conditions can be tested with complexity linear in the number of classes. The results obtained do not make any assumption w.r.t. the structure of the underlying distributions or the classification problem. Nevertheless, the general analytic results are analyzed via simulations of string recognition problems with Levenshtein (edit) distance cost function. The results support earlier findings that considerable improvements are to be expected when initial error rates are high.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989822,no,undetermined,0
Energetic-environmental requalification for tourist accommodation facilities by analytic hierarchy process,"Decision problems are often characterized by a great number of alternative actions and by the complexity of relationships between the various factors involved in the process. This leads to the determination of alternative choices in uncertainty conditions. So, the multicriteria decision methods (MCDM) represent a support to the decision maker to rationalize the process and identify the best solution. This paper presents an application of a multicriteria method, the analytic hierarchy process, by using a software package, Expert Choice, for analyzing the problem of an energetic and environmental requalification.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417854,no,undetermined,0
Epistemic signals and emoticons affect kudos,"Our focus is on the interaction between emoticon use and epistemic hedges in the perception of individual contributions to discourse (and posters of those contributions) as deserving of kudos for their input. The communities with English as a lingua franca that we explore consist of self-motivated contributors to user-fora supported by a major multinational with a software technology company. User categories are determined by a few orthogonal classifications: employees, novice users, and experts; recipients of kudos vs. non-recipients of kudos; etc. We explore the interaction between social signals and signals of certainty in content. Among the effects reported are the negative influence of epistemic hedges used in posting on propensity for others in the community to accord kudos to such postings, but a positive influence of the same in interaction with the use of emoticons.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6422036,no,undetermined,0
Establishing an lifelong learning environment using IOT and learning analytics,"With the trend of the development of global knowledge economy and lifelong learning society, the competition of lifelong learning market becomes increasingly keen. Lifelong learning units need to understand whether the courses they provide are appropriate and their students' learning effectiveness just as advertisers need to understand the market and customers. In this context, learning analytics is extremely important. Traditionally, it used the data of questionnaire after the courses to perform analysis and evaluation, however, the questionnaire survey usually was conducted at the end of the semester, and therefore the immediateness is lower and teachers couldn't adjust the content of the course and learning strategies based on their students' needs immediately. Some researchers used the data collected by LMS to conduct real-time learning analytics, nevertheless, students perform learning not just in classes, they do it when they go to libraries to borrow books, use classrooms to discuss the courses, and utilize Mobile Devices to download data and join in seminar...etc. Therefore, this study tried to combine internet of things (IOT) and the techniques of learning analytics to record and conduct the analysis of students' learning process and further enable them and schools to obtain feedbacks that they need and establish an effective lifelong learning environment.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6174821,no,undetermined,0
Evaluation of Java-based open source web frameworks with Ajax support,"Web frameworks are widely used in web applications to lower development effort and ease maintenance. The large number of Java<sup>1</sup> web frameworks makes it hard to decide for one. A systematic approach is necessary to come to an optimal decision under given requirements and conditions. For this work, 110 Java web frameworks were gathered; 13 of them were analyzed by the Analytic Hierarchy Process (AHP). The result is a systematic approach towards a selection guidance for a Java web framework within a particular project. The list of Java web frameworks and the goal hierarchy can be reused, adopted and extended as required. The process described in this paper enables project leaders and developers to do their own comparisons - it is not limited to evaluation of Java Web frameworks. The AHP, as described here, allows a decision maker to adjust the priorities of criteria and intensities according to his needs in a comprehensible manner.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320531,no,undetermined,0
MAAP: Mission Assurance Analytics Platform,"This paper describes the Mission Assurance Analytics Platform (MAAP), an open, experimental software framework that provides analysts with an environment for systematically studying the link between cyber attack and the resulting impact on operational missions that are supported by a cyber system. MAAP directly informs both risk decisions and mitigation prioritization.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6459908,no,undetermined,0
Measuring influence in social networks using a network amplification score - an analysis using cloud computing,"Information shared on social networks can be attributed to various factors such as common interests, innovation in areas of job role, etc. The propagation of information or its reach in a social network is directly related to the influence of the author. Traditional influence scoring algorithms rely only on the number of direct connections of a user or on a ratio of their connections. As proved by recent research, a focus solely on the connection between users skews an understanding of how influence operates and follows. In this paper we differentiate between passive and active information. Information is classified as active if it promotes engagement through conversation and is thus exposed to more members, increasing the reach of information. Using a large amount of social network data we analyze the different components of information generated by a user, its propagation within the network and derive a metric for calculating the user's influence score. This influence score considers both the passive content and the active conversation aspect of the shared information. The score relies on the network metric degree of centrality. The measure of inbound and outbound information for 2 levels in the network, provide insight to the amplification of the reach of the shared information.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6421367,no,undetermined,0
Tracking aggregate vs. individual gaze behaviors during a robot-led tour simplifies overall engagement estimates,"As an early behavioral study of what non-verbal features a robot tourguide could use to analyze a crowd, personalize an interaction and maintain high levels of engagement, we analyze participant gaze statistics in response to a robot tour guide's deictic gestures. There were thirty-seven participants overall, split into nine groups of three to five people each. In groups with the lowest engagement levels aggregate gaze response to the robot's pointing gesture involved the fewest total glance shifts, least time spent looking at indicated object and no intra-participant gaze. Our diverse participants had overlapping engagement ratings within their group, and we found that a robot that tracks group rather than individual analytics could capture less noisy and often stronger trends relating gaze features to self-reported engagement scores. Thus we have found indications that aggregate group analysis captures more salient and accurate assessments of overall humans-robot interactions, even with lower resolution features.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249512,no,undetermined,0
Smart Traffic Cloud: An Infrastructure for Traffic Applications,"With rapid development of sensor technologies and wireless network infrastructure, research and development of traffic related applications, such as real time traffic map and on-demand travel route recommendation have attracted much more attentions than ever before. Both archived and real-time data involved in these applications could potentially be very big, depending on the number of deployed sensors. Emerging Cloud infrastructure can elastically handle such big data and conveniently providing nearly unlimited computing and storage resources to hosted applications, to carry out analysis not only for long-term planning and decision making, but also analytics for near real-time decision support. In this paper, we propose Smart Traffic Cloud, a software infrastructure to enable traffic data acquisition, and manage, analyze and present the results in a flexible, scalable and secure manner using a Cloud platform. The proposed infrastructure handles distributed and parallel data management and analysis using ontology database and the popular Map-Reduce framework. We have prototyped the infrastructure in a commercial Cloud platform and we developed a real-time traffic condition map using data collected from commuters' mobile phones.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413598,no,undetermined,0
Scalable performance of ScaleGraph for large scale graph analysis,"Scalable analysis of massive graphs has become a challenging issue in high performance computing environments. ScaleGraph is an X10 library aimed for large scale graph analysis scenarios. This paper evaluates scalability of ScaleGraph library for degree distribution calculation, betweeness centrality, and spectral clustering algorithms. We make scalability evaluation by analyzing a synthetic Kronecker graph with 40.3 million edges (for all the three algorithms), and a real social network with 69 million edges (for degree distribution calculation) on Tsubame 2.0 distributed memory environment.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6507498,no,undetermined,0
Semi-infinite integral implementation in the development steps of Interfstud electromagnetic interference software,"A user friendly software application was developed to study the electromagnetic interference problem between high voltage power lines and underground metallic pipelines. A hybrid method presented in previous papers is implemented in the software to calculate the induced A.C. voltage in pipeline. The dedicated developed InterfStud software was based on induced magnetic vector potential and ground correction terms evaluation. New analytical formulas for the induced magnetic vector potential and ground correction terms for self and mutual impedances are derived. The determined formulas contain semi-infinite integral terms which will be evaluated. We might seek approximations of the semi-infinite integrals by replacing an exponential or algebraic function, the objective being to permit analytic integration. Since there is no good systematic method for making these replacements, their success depends directly on the intuition and ingenuity, taking into account that in practice the integrand has limited accuracy.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398640,no,undetermined,0
"Setting up, managing and using a complex Business Intelligence platform","CSI-Piemonte (Consortium for Information Systems in Piedmont) builds and manages Information Systems for public authorities in Italy in the Piedmont region. The consortium has adopted a Business Intelligence (BI) platform made up of SASŒ¬ and SAP BusinessObjectŒ¬ solutions. The company has been working with SASŒ¬ for 30 years and nowadays it has a BI Competency Centre (BICC), set up in order to use and manage the platform as its optimal level; the BICC's roots date back to the 80s. In this platform both hardware and software of all customers converge on a single high reliable infrastructure based on SASŒ¬ 9.2 and SAP BusinessObjectŒ¬ XI and the SASŒ¬ component is designed as a Grid Platform. On this architecture CSI-Piemonte design a wide range of BI applications to support the Piedmont local institutions. Applications cover all BI use cases, taking into account also Data Quality and Data Mining issues. The platform includes Data Integration, Data Quality, Data Mining, other Analytics, Query and Reporting and Master Data Management tools. The paper focuses on the overall experience of setting up and managing a BI platform of such complexity and on a success case study on e-government.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240911,no,undetermined,0
Shared disk big data analytics with Apache Hadoop,"Big Data is a term applied to data sets whose size is beyond the ability of traditional software technologies to capture, store, manage and process within a tolerable elapsed time. The popular assumption around Big Data analytics is that it requires internet scale scalability: over hundreds of compute nodes with attached storage. In this paper., we debate on the need of a massively scalable distributed computing platform for Big Data analytics in traditional businesses. For organizations which don't need a horizontal., internet order scalability in their analytics platform., Big Data analytics can be built on top of a traditional POSIX Cluster File Systems employing a shared storage model. In this study., we compared a widely used clustered file system: VERITAS Cluster File System (SF-CFS) with Hadoop Distributed File System (HDFS) using popular Map-reduce benchmarks like Terasort., DFS-IO and Gridmix on top of Apache Hadoop. In our experiments VxCFS could not only match the performance of HDFS., but also outperformed in many cases. This way., enterprises can fulfill their Big Data analytics need with a traditional and existing shared storage model without migrating to a different storage model in their data centers. This also includes other benefits like stability & robustness., a rich set of features and compatibility with traditional analytics applications.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6507520,no,undetermined,0
Simulations of a dynamical system model for electronic circuits,"Models and numerical simulations are used to understand concepts of electronic circuits as well as to evaluate their performance without the actual need to build them. Today, they are mostly implemented by softwares that rely on numerical solutions of analytic models, which usually require a great amount of computational resources. In this paper, we propose an alternative particle dynamical system to model electronic circuits. Its main features are a very simple evolution rule, which resembles the principles of classical electrodynamics, and purely deterministic scenarios. A simulation method is also proposed to simulate such model in order to allow the easy comprehension of electronic properties and concepts. Also, as we show in this paper, simulations of the model succeeds in displaying electron tunneling events, even though no particle is defined in terms of quantum mechanics.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6220090,no,undetermined,0
Simulative Analyses of the Rotor for Magnetically Suspended Flywheel with Vernier Gimballing Capacity,"Magnetically suspended flywheels provide a number of attractive advantages over ball bearing wheels. As an important executive component of satellite attitude control system, the improvement of flywheel system is spurred by high performance satellite attitude control system. The general situation of the flywheel with vernier gimballing capacity is introduced. To reduce power consumption and augment the outputting torque of the existing magnetically suspended flywheel when the rotor is tilting, a innovative 5DOFs magnetically suspended flywheel with vernier gimballing capacity is proposed also its composition and working principle are introduced briefly. As the most important part of the flywheel, the rotor is analyzed by finite element analysis software ANSYS-Workbench, including mode, harmonic vibration and random vibration simulative analyses. The analytic results indicate that structural design of the flywheel is reasonable. The analytical method takes an important part in designing the flywheel which is not only factualistic but also effective..",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298631,no,undetermined,0
Single-Antenna Coherent Detection of Collided FM0 RFID Signals,"This work derives and evaluates single-antenna detection schemes for collided radio frequency identification (RFID) signals, i.e. simultaneous transmission of two RFID tags, following FM0 (biphase-space) encoding. In sharp contrast to prior art, the proposed detection algorithms take explicitly into account the FM0 encoding characteristics, including its inherent memory. The detection algorithms are derived when error at either or only one out of two tags is considered. It is shown that careful design of one-bit-memory two-tag detection can improve bit-error-rate (BER) performance by 3dB, compared to its memoryless counterpart, on par with existing art for single-tag detection. Furthermore, this work calculates the total tag population inventory delay, i.e. how much time is saved when two-tag detection is utilized, as opposed to conventional, single-tag methods. It is found that two-tag detection could lead to significant inventory time reduction (in some cases on the order of 40%) for basic framed-Aloha access schemes. Analytic calculation of inventory time is confirmed by simulation. This work could augment detection software of existing commercial RFID readers, including single-antenna portable versions, without major modification of their RF front ends.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6150982,no,undetermined,0
Sleep and activity monitoring for Returning Soldier Adjustment Assessment,"This paper describes the development of unobtrusive room sensors to discover relationships between sleep quality and the clinical assessments of combat soldiers suffering from post-traumatic stress disorder (PTSD) and mild traumatic brain injury (TBI). We consider the use of a remote room sensor unit composed of a Doppler radar, light, sound and other room environment sensors. We also employ an actigraphy watch. We discuss sensor implementation, radar data analytics and preliminary results using real data from a Warrior Transition Battalion located in Fort Gordon, GA. Two radar analytical approaches are developed and compared against the actigraphy watch estimates - one, emphasizing system knowledge; and the other, clustering on several radar signal features. The radar analytic algorithms are able to estimate sleep periods, signal absence and restlessness in the bed. In our test cases, the radar estimates are shown to agree with the actigraphy watch. PTSD and mild-TBI soldiers do often show signs of sporadic and restless sleep. Ongoing research results are expected to provide further insight.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6346385,no,undetermined,0
Smart city surveillance: Leveraging benefits of cloud data stores,"The smart cities of future need to have a robust and scalable video surveillance infrastructure. In addition it may also make use of citizen contributed video feeds, images and sound clips for surveillance purposes. Multimedia data from various sources need to be stored in large scalable data stores for compulsory retention period, on-line, off-line analytics and archival. Multimedia feeds related to surveillance are voluminous and varied in nature. Apart from large multimedia files, events detected using video analytics and associated metadata needs to be stored. The underlying data storage infrastructure therefore needs to be designed for mainly continuous streaming writes from video cameras and some variety in terms of I/O sizes, read-write mix, random vs. sequential access. As of now, the video surveillance storage domain is mostly dominated by iSCSI based storage systems. Cloud based storage is also provided by some vendors. Taking in account the need for scalability, reliability and data center cost minimization, it is worth investigating if large scale video surveillance backend can be integrated to the open source cloud based data stores available in the äóìbig dataäó trend. We developed a multimedia surveillance backend system architecture based on the Sensor Web Enablement framework and cloud based äóìkey-valueäó stores. Our framework gets data from camera/ edge device simulators, splits media files and metadata and stores those in a segregated way in cloud based data stores hosted on Amazons EC2. We have benchmarked performances of a few cloud based key-value stores under large scale video surveillance workload and demonstrated that those perform satisfactorily, bringing in inherent scalability and reliability of a cloud based storage system to a video surveillance system for a smart safe city. With a case study of the storage of video surveillance system, we show in this paper that with the availability of several cloud based d- stributed data stores and benchmarking tools, an application's data management needs can be served using hybrid cloud based data stores and selection of such stores can be facilitated using benchmark tools if the application workload characteristics are known.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424076,no,undetermined,0
SnapShot: Visualization to Propel Ice Hockey Analytics,"Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today's sports analyst's routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327288,no,undetermined,0
Memory Access Control in Multiprocessor for Real-Time Systems with Mixed Criticality,"Shared resource access interference, particularly memory and system bus, is a big challenge in designing predictable real-time systems because its worst case behavior can significantly differ. In this paper, we propose a software based memory throttling mechanism to explicitly control the memory interference. We developed analytic solutions to compute proper throttling parameters that satisfy schedulability of critical tasks while minimize performance impact caused by throttling. We implemented the mechanism in Linux kernel and evaluated isolation guarantee and overall performance impact using a set of synthetic and real applications.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6257581,no,undetermined,0
Software analytics in practice and its implications for education and training,"Summary form only given. Software analytics is to enable software practitioners to perform data exploration and analysis in order to obtain insightful and actionable information for data-driven tasks around software and services. In this talk, based on the success of technology transfer on software analytics at Microsoft Research Asia, I will share our experiences in carrying out successful technology transfers mainly including (1) incorporation of a broad spectrum of domain knowledge and expertise, e.g., management, machine learning, large-scale data processing and computing, and information visualization; and (2) investigation into how practitioners take actions on the produced information, and providing effective support for such information-based action taking. I will talk about the implications of our experiences for software engineering education and training, such as skill-set requirements, curriculum development, and academia-Industry collaboration.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6244998,no,undetermined,0
Software analytics in practice: Mini tutorial,"Summary form only given. A huge wealth of various data exists in the software development process, and hidden in the data is information about the quality of software and services as well as the dynamics of software development. With various analytic and computing technologies, software analytics is to enable software practitioners to performance data exploration and analysis in order to obtain insightful and actionable information for data-driven tasks around software and services [1].",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227121,no,undetermined,0
Software requirement prioritization using fuzzy multi-attribute decision making,"Although many approaches have been proposed to prioritize requirements in software projects, almost none has been widely adopted. This is mostly due to their complexity, time commitment, lack of consistency, or implementation difficulties. This paper proposes a novel approach to do so that is practical, easy to implement and can show a reasonable level of consistency. In addition, it takes in consideration the imprecise nature of requirements and quality attributes by modeling the latter as fuzzy variables. The problem of prioritizing requirements is formulated as a fuzzy multi-attribute decision problem in which the expected value operator is used to rank the alternatives listed in the problem formulation. This approach can be easily extended to include other quality attributes as well as customized to fit the needs of most software projects.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417646,no,undetermined,0
Software Solution of Delay Differential Equations,"Methods used for ordinary differential equations cannot generally be used to solve delay differential equation, which is reflected in the choice of suitable software. The paper shows possibilities of software solution of delay differential equations. The aim of the paper is to present the possibilities of current software packages and programme systems (e.g. Matlab, Maple, R). The paper further shows how the aforementioned equations can be used in solutions of dynamical models. The conclusion of the papers demonstrates a solution of a specific dynamical model - the Phillips curve applied to the Czech Republic. Setting up the model required the use of analytic and synthetic methods, dynamical modelling and solving the system of two delay differential equations. In the conclusion the authors claim that although the quality of the available software, suitable for solving delay differential equations, is not as good as that of software used for solving ordinary differential equations, such software that meets basic requirements can be found. Therefore such software can be used as a supportive tool for exact modelling methods in practice.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394952,no,undetermined,0
STINGER: High performance data structure for streaming graphs,"The current research focus on äóìbig dataäó problems highlights the scale and complexity of analytics required and the high rate at which data may be changing. In this paper, we present our high performance, scalable and portable software, Spatio-Temporal Interaction Networks and Graphs Extensible Representation (STINGER), that includes a graph data structure that enables these applications. Key attributes of STINGER are fast insertions, deletions, and updates on semantic graphs with skewed degree distributions. We demonstrate a process of algorithmic and architectural optimizations that enable high performance on the Cray XMT family and Intel multicore servers. Our implementation of STINGER on the Cray XMT processes over 3 million updates per second on a scale-free graph with 537 million edges.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6408680,no,undetermined,0
Streamlining Service Levels for IT Infrastructure Support,"For IT Infrastructure Support (ITIS), it is crucial to identify opportunities for reducing service costs and improving service quality. We focus on streamlining service levels i.e., finding right resolution level for each ticket, to reduce time, efforts and cost for ticket handling, without affecting workloads and user satisfaction. We formalize this problem and present two statistics-based search algorithms for identifying problems suitable for left-shift (from expensive, expertise intensive L2 level to cheaper, simpler L1 level) and right-shift (from L1 to L2). The approach is domain-driven: it produces directly usable and often novel results, without any trial-and error experimentation, along with detailed justifications and predicted impacts. This helps in acceptance among end-users and more active use of the results. We discuss one real-life case-study of results produced by the algorithms.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406456,no,undetermined,0
Study on Innovative Capability Evaluation of Knowledge-innovation Talents Based on AHP: A Case Study on Henan Province,"Knowledge-innovation talents, as the core element of competition and technological development, are the first strategic resources of the national and regional development. Then, the innovation capacity of technological innovators is the fundamental factor which influences the effect of the technology and knowledge innovators and is also the research focus of this essay. According to the AHP, the paper built the evaluation model for the innovation capacity of technological innovators of Henan Province.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382594,no,undetermined,0
Synchronous Parallel Processing of Big-Data Analytics Services to Optimize Performance in Federated Clouds,"Parallelization of big-data analytics services over a federation of heterogeneous clouds has been considered to improve performance. However, contrary to common intuition, there is an inherent tradeoff between the level of parallelism and the performance for big-data analytics principally because of a significant delay for big-data to get transferred over the network. The data transfer delay can be comparable or even higher than the time required to compute data. To address the aforementioned tradeoff, this paper determines: (a) how many and which computing nodes in federated clouds should be used for parallel execution of big-data analytics; (b) opportunistic apportioning of big-data to these computing nodes in a way to enable synchronized completion at best-effort performance; and (c) sequence of apportioned, different sizes of big-data chunks to be computed in each node so that transfer of a chunk is overlapped as much as possible with the computation of the previous chunk in the node. In this regard, Maximally Overlapped Bin-packing driven Bursting (MOBB) algorithm is proposed, which improve the performance by up to 60% against existing approaches.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6253583,no,undetermined,0
The modeling and optimization of software engineering processes,"The using of the method of insertion programming and Saaty Analytic Hierarchy Process (AHP) is described for formalization and optimization of software engineering processes. The procedure of formalized representation of requirements specification for software system (SWS) is shown, and the algorithm of optimal selection of SWS architecture in case of large quantity of quality parameters is presented too.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6192592,no,undetermined,0
Scalable complex graph analysis with the knowledge discovery toolbox,"The Knowledge Discovery Toolbox (KDT) enables domain experts to perform complex analyses of huge datasets on supercomputers using a high-level language without grappling with the difficulties of writing parallel code, calling parallel libraries, or becoming a graph expert. KDT delivers competitive performance from a general-purpose, reusable library for graphs on the order of 10 billion edges and greater. We describe our approach for supporting arbirary vertex and edge attributes, in-place graph filtering, and graph traversal using pre-defined access patterns.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289128,no,undetermined,0
RPig: A scalable framework for machine learning and advanced statistical functionalities,In many domains such as Telecom various scenarios necessitate the processing of large amounts of data using statistical and machine learning algorithms. A noticeable effort has been made to move the data management systems into MapReduce parallel processing environments such as Hadoop and Pig. Nevertheless these systems lack the features of advanced machine learning and statistical analysis. Frame-works such as Mahout on top of Hadoop support machine learning but their implementations are at the preliminary stage. For example Mahout does not provide Support Vector Machine (SVM) algorithms and it is difficult to use. On the other hand traditional statistical software tools such as R containing comprehensive statistical algorithms for advanced analysis are widely used. But such software can only run on a single computer and therefore it is not scalable. In this paper we propose an integrated solution RPig which takes the advantages of R (for machine learning and statistical analysis capabilities) and parallel data processing capabilities of Pig. The RPig framework offers a scalable advanced data analysis solution for machine learning and statistical analysis. Analysis jobs can be easily developed with RPig script in high level languages. We describe the design implementation and an eclipse-based RPigEditor for the RPig framework. Using application scenarios from the Telecom domain we show the usage of RPig and how the framework can significantly reduce the development effort. The results demonstrate the scalability of our framework and the simplicity of deployment for analysis jobs.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427480,no,undetermined,0
Research on VAF of IFPUG Method Based on Fuzzy Analytic Hierarchy Process,"As the weight of the general system characteristics (GSC) in IFPUG method is the same in the different types of software systems. To solve the problem, this article uses the Fuzzy AHP (Analytic Hierarchy Process) approach to assign weight to the fourteen general system characteristics, and gives the improved formula of VAF. It makes IFPUG method more suitable to these other types of software systems besides the information management system. An example of functional size measurement is given to illustrate the feasibility of this method. And it also illustrates that the fuzzy AHP approach can get a better accuracy of the function point counting, and reduces the deviation.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211158,no,undetermined,0
Research on the comprehensive evaluation of patient satisfaction based on the attribute reduction and AHP,"Focusing on the systematicness and validity of the evaluation system of the patient satisfaction, large number of factors impacting the patient satisfaction were considered comprehensively, the analytical method combining the Attribute Reduction in Rough Set with AHP was adopted. First, the evaluation factors were analyzed to establish the evaluation hierarchical model of the patient satisfaction, the indexes were reduced by Attribute Reduction based on the information quantity; then, the significance of attribute was used to determine the index weight; lastly, the total sequence weight of sub-criteria was calculated by AHP method and the comprehensive evaluation value was obtained. This model was used to evaluate and analyze the five hospitals in one area and the validity of this method was tested. This method reduces the amount of calculation, improves the computational efficiency and provides a method of quantitative analysis.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6252261,no,undetermined,0
Modeling and simulation based study for on-line detection of partial discharge of solid dielectric,"Nowadays electric utilities are facing major problems due to the ageing and deterioration of high voltage (HV) power equipments in their operating service period. There are several solid materials are used in high voltage power system equipments for insulation purpose. The insulators used in HV power equipment always have a small amount of impurity inside it. The impurity is mainly in the form of solid, gas or liquid. In most cases the impurity is in the form of air bubbles (void) which creates a weak zone inside the insulator. Therefore, this void is the reason for the occurrence of partial discharge in high voltage power equipments while sustaining the high voltage. Ageing and deterioration is mainly occurs due to the presence of partial discharge in such insulator used in the high voltage power equipments. The presence of partial discharge for a long period of time is also causes the insulation failure of high voltage equipments used in power system. Therefore, the partial discharge detection and measurement is necessary for prediction and reliable operation of insulation in high voltage power equipments. In this work, to study the on-line detection of partial discharge an epoxy resin is taken as a solid dielectric for simulating and modeling purpose. This epoxy resin with small impurity (air bubble) under high voltage stress creates a source of partial discharge inside the dielectric. The generated partial discharge is continuously detected and monitored by using LabVIEW software. Simulation of real time detection, de-noising and different analytic techniques of partial discharge signal by using LabVIEW software is proposed which gives the real time visualization of partial discharge signal produced inside the high voltage power equipment.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6318904,no,undetermined,0
Modeling of radiowave propagation in tunnels,"We investigated propagation of radio frequency (RF) waves in tunnels. An understanding of RF propagation in tunnels is needed to plan for optimal radio communication system performance. We considered tunnels with a square cross section and right-angle junctions. We developed an analytic model of tunnel propagation by considering multiple reflections from tunnel walls and diffraction around junction corners. A comparison of model results with measurements shows a strong correlation between model predictions and measured data. Finally, we briefly describe a software tool that helps users visualize field strength of the propagating field in tunnels.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6415864,no,undetermined,0
MSR 2012 keynote: Software analytics in practice äóî Approaches and experiences,Summary form only given. A list of the plenary sessions speakers and tracks is given. Following that are abstracts for all full papers published on the original conference proceedings CD.,2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224292,no,undetermined,0
Multi-level Layout Optimization for Efficient Spatio-temporal Queries on ISABELA-compressed Data,"The size and scope of cutting-edge scientific simulations are growing much faster than the I/O subsystems of their runtime environments, not only making I/O the primary bottleneck, but also consuming space that pushes the storage capacities of many computing facilities. These problems are exacerbated by the need to perform data-intensive analytics applications, such as querying the dataset by variable and spatio-temporal constraints, for what current database technologies commonly build query indices of size greater than that of the raw data. To help solve these problems, we present a parallel query-processing engine that can handle both range queries and queries with spatio-temporal constraints, on B-spline compressed data with user-controlled accuracy. Our method adapts to widening gaps between computation and I/O performance by querying on compressed metadata separated into bins by variable values, utilizing Hilbert space-filling curves to optimize for spatial constraints and aggregating data access to improve locality of per-bin stored data, reducing the false positive rate and latency bound I/O operations (such as seek) substantially. We show our method to be efficient with respect to storage, computation, and I/O compared to existing database technologies optimized for query processing on scientific data.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6267895,no,undetermined,0
Multi-object tracking coprocessor for multi-channel embedded DVR systems,"In this paper, the architecture of a video analytics coprocessor is proposed for multi-channel embedded digital video recorder (DVR) systems. A reference video analytics algorithm is proposed for multi-object tracking and is divided into independent processing steps based on data flow. Each step is designed in hardware or software considering its computational complexity and required system resources. Pixelwise processing requiring a large amount of computational resources, such as frame difference and background modeling, are designed as hardware with embedded direct memory access (DMA) controllers. A single-pass connected component labeling (CCL) is designed as a hardware targeting real-time processing of stream input. High-level tasks such as object filtering, frame-based control of hardware modules, and communication with an external host are designed with software on an embedded processor. Object tracking and event detection are designed with software on a host processor. Considering both the bandwidth required for frame processing and the bandwidth available by memory buses, the architecture of a 4-channel video analytics coprocessor is explored. It is finally implemented on a field-programmable gate arrays (FPGA) device, integrated into a conventional DVR system, and verified as to its functions and performance. It can provide video analysis functions to conventional DVR system-on-chip (SoC), and can lessen the cost of real-time video monitoring at remote monitoring centers.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6415008,no,undetermined,0
Neuro Analytical hierarchy process (NAHP) approach for CAD/CAM/CIM tool selection in the context of small manufacturing industries,"The purpose of this paper is to provide a methodology to assist small automobile manufacturing companies of India in selecting CAD/CAM system. The use of these tools in design and manufacturing of automobile applications makes it possible to remove much of the tedium and manual labor work involved. They allow complex designs to be made quicker and accurately. Many companies are using obsolete packages to design and manufacture automobiles. Since technology is changing rapidly and most companies cannot update with the technology advancements. They need to know what CAD/CAM packages are available and how to select one will help the companies. The selection of CAD/CAM/CIM tool/package is multi criteria, multi attribute, decision making process which involve many factors that can be solved using NAHP (Neuro Analytical Hierarchy Process). In NAHP, we are combining the power of AHP and Neural Network. To accomplish this purpose, data have been collected about current CAD/CAM systems. Important criteria for system selection and parameters for selection have also been identified and prioritized. NAHP is used as decision making technique to identify and prioritize important factors for selection of CAD/CAM/CIM system for small automobile manufacturing companies of India.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398139,no,undetermined,0
Next generation emergency management common operating picture software/systems (COPSS),"In the state-of-the-art Command Center at the Morrelly Homeland Security Center in Bethpage NY, VCORE Solutions has integrated, demonstrated, tested, deployed, and is operating powerful Emergency Management Common Operating Picture Software/Systems (COPSS). This Regional COPSS demonstrates in an operational environment the next generation of emergency management situational awareness, command & control, and information sharing in a natural, easy to use and understand four-dimensional (4D) common operating picture. This Regional COPSS is based on patented fourDscapeŒ¬ software technology, developed over the past decade by Long Island, NY-based Balfour Technologies. This powerful fourDscape augmented virtual reality technology has been effectively applied to local and regional emergency management operations and can be deployed nationally to deliver comprehensive situational awareness in support of safety, security and emergency preparedness, prevention, mitigation, response and recovery operations at all levels. A fourDscape browser/server-based COPSS is designed as an open, multi-layered service/resource oriented networked architecture (SOA/ROA, i.e. äóìcloudäó) capable of (1) integrating and managing a multitude of disparate data sources of all types (including live and static data feeds); (2) interoperability with numerous other vendors information systems, COPs, information sharing frameworks, notification and alerting systems, analytics, etc.; and (3) sharing and passing information and comprehensive, timely situational awareness between first responders at the incident site, incident commanders, and emergency managers and decision makers at local, regional and national emergency operations centers across the country. And consistent with the recent Presidential Policy Directive on National Preparedness (PPD-8), this fourDscape COPSS capabilities and framework represents currently operational technology that can achieve an integrated, la- ered, and all-of-Nation [capabilities-based] preparedness approach that optimizes the use of available resources. Next Generation COPSS such as fourDscape need to be open and scalable to facilitate global information sharing; deliver information in an easy-to-understand augmented virtual reality common operating picture; be easy-to-use walk-up technology with a full complement of embedded training/simulation capability; provide for effective information assurance; and be compliant and effective in executing national preparedness goals. All this can and will be achieved by next generation COPSS.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6223101,no,undetermined,0
On analytic model of multiple vias for high-speed printed circuit board and electric band-gap structures,"We developed a full-wave formulation to model massive number of vias in high-speed printed circuit board (PCB), through silicon via (TSV) and electric band-gap (EBG) structures. This analytic method employs the equivalent magnetic frill array, Galerkin's procedure, image theory and Fourier transform to simplify the problem from a 3D configuration into a 2D frame. Based on Bessel's functions and addition theorem, the final matrix equation is formulated analytically without using any numerical techniques. The new method is purely from the boundary conditions. Consequently, it is simple, versatile, efficient and accurate. Numerical examples demonstrate good agreement between our analytical solution and commercial software (HFSS) for through silicon and PCB vias. The model is also used to study the EBG wall and cavity, for leakage fields.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6457904,no,undetermined,0
Parallel and pipelined processing of large-scale mobile comminucation data using hadoop open-source framework,"The fast increase in mobile device and bandwidth usage is generating big workloads on the IT infrastructures of mobile service providers and increasing management costs. These providers collect log files continuously and use these logs for billing, operational and marketing purposes. In this paper, we describe the design, implementation and efficient parallel processing of large-scale mobile logs using the open-source Hadoop-based low-cost private cloud system for near real-time analytics. We find that batching of small files, parallel loading and pipelining of different workloads by overlapping their disk-and-CPU intensive phases can have significant performance benefits. Optimizations were performed in the light of these findings. Our web-based interface helps users explore progress and performance of their workloads.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6204832,no,undetermined,0
Partial cuts in attack graphs for cost effective network defence,"Because of increasing vulnerabilities, maturing attack tools, and increasing dependence on computer network infrastructure, tools to support network defenders are essential. Course-of-action recommendation research has often assumed a goal of perfect network security. In reality, network administrators balance security with usability and so tolerate vulnerabilities and imperfect security. We provide realistic course-of-action decision support for network administrators by minimizing connectivity in attack graphs, by optimizing network configuration changes to separate defence goals from attackers as much as possible, even when complete security is impractical. We introduce vertex closures and closure-relation graphs in AND/OR digraphs as the underlying framework. Computing an optimal course-of-action is NP-hard but we design a polynomial-time greedy algorithm that almost always produces an optimal solution.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6459864,no,undetermined,0
Performance evaluation of node in cloud storage,"There is a method to evaluate the performance of the node storage the replica in the cloud storage, under this method we can get better node when we place the replica according to user requirement for service. First, establish the model of the performance of the node storage the replica using the analytic hierarchy process. Then, obtain the weighting of each factor in model using the combination weight. At last, the valuations of nodes are calculated and the rank of nodes is given by grey comprehensive evaluation. Experiment results show that the proposed method could provide more reasonable replica strategies effectively.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6201473,no,undetermined,0
Performance Evaluation of Yahoo! S4: A First Look,"Processing large data sets has been dominated recently by the map/reduce programming model [1], originally proposed by Google and widely adopted through the Apache Hadoop1 implementation. Over the years, developers have identified weaknesses of processing data sets in batches as in MapReduce and have proposed alternatives. One such alternative is continuous processing of data streams. This is particularly suitable for applications in online analytics, monitoring, financial data processing and fraud detection that require timely processing of data, making the delay introduced by batch processing highly undesirable. This processing paradigm has led to the development of systems such as Yahoo! S4 [2] and Twitter Storm.2 Yahoo! S4 is a general-purpose, distributed and scalable platform that allows programmers to easily develop applications for processing continuous unbounded streams of data. As these frameworks are quite young and new, there is a need to understand their performance for real time applications and find out the existing issues in terms of scalability, execution time and fault tolerance. We did an empirical evaluation of one application on Yahoo! S4 and focused on the performance in terms of scalability, lost events and fault tolerance. Findings of our analyses can be helpful towards understanding the challenges in developing stream-based data intensive computing tools and thus providing a guideline for the future development.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6362950,no,undetermined,0
"Platform 2012, a many-core computing accelerator for embedded SoCs: Performance evaluation of visual analytics applications","P2012 is an area- and power-efficient many-core computing accelerator based on multiple globally asynchronous, locally synchronous processor clusters. Each cluster features up to 16 processors with independent instruction streams sharing a multi-banked one-cycle access L1 data memory, a multi-channel DMA engine and specialized hardware for synchronization and aggressive power management. P2012 is 3D stacking ready and can be customized to achieve extreme area and energy efficiency by adding domain-specific HW IPs to the cluster. The first P2012 SoC prototype in 28nm CMOS will sample in Q3, featuring four 16-processor clusters, a 1MB L2 memory and delivering 80GOPS (with 32 bit single precision floating point support) in 18mm<sup>2</sup> with 2W power consumption (worst-case). P2012 can run standard OpenCLä‹¢ and proprietary Native Programming Model SW components to achieve the highest level of control on application-to-resource mapping. A dedicated version of the OpenCV vision library is provided in the P2012 SW Development Kit to enable visual analytics acceleration. This paper will discuss preliminary performance measurements of common feature extraction and tracking algorithms, parallelized on P2012, versus sequential execution on ARM CPUs.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6241648,no,undetermined,0
Privacy-Preserving Layer over MapReduce on Cloud,"Cloud computing provides powerful and economical infrastructural resources for cloud users to handle ever-increasing Big Data with data-processing frameworks such as MapReduce. Based on cloud computing, the MapReduce framework has been widely adopted to process huge-volume data sets by various companies and organizations due to its salient features. Nevertheless, privacy concerns in MapReduce are aggravated because the privacy-sensitive information scattered among various data sets can be recovered with more ease when data and computational power are considerably abundant. Existing approaches employ techniques like access control or encryption to protect privacy in data processed by MapReduce. However, such techniques fail to preserve data privacy cost-effectively in some common scenarios where data are processed for data analytics, mining and sharing on cloud. As such, we propose a flexible, scalable, dynamical and costeffective privacy-preserving layer over the MapReduce framework in this paper. The layer ensures data privacy preservation and data utility under the given privacy requirements before data are further processed by subsequent MapReduce tasks. A corresponding prototype system is developed for the privacy-preserving layer as well.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382833,no,undetermined,0
Private cloud delivery model for supplying centralized analytics services,"Business analytics has become vital for businesses to achieve their near-term and strategic goals. Historically, large enterprises have used a decentralized information technology delivery model to meet reporting and analytics needs within each business unit. Rising demand for analytics, combined with the cost of decentralized delivery, has driven businesses to look at alternative delivery patterns. Chief information officers (CIOs) have a renewed interest in considering centralized, virtualized servicesäóîgiven their desire to reduce expenses without reduced business flexibility. The Chief Information Office of IBM has pioneered a private cloud delivery model for delivering analytics tooling to internal users (IBM employees), providing centralization and standardization of service. Through evaluation of analytics usage patterns, IBM developed a common analytics services strategy to reduce expenses and solution deployment time while improving business agility and insights. IBM deployed a share-all private cloud model (in which users deploy reports on the same instance of the hardware and software) on an extensible LinuxŒ¬ on the IBM System zŒ¬ infrastructure to deliver ubiquitous analytics service to every business process area. The IBM software-as-a-service analytics delivery model allowed its more than 190,000 business users to maintain user or user-group solution autonomy while providing access to cost-effective ($25 million savings over five years), timely commodity services.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6353964,no,undetermined,0
Quantifying the performance impact of overbooking virtualized resources,"Cloud services are compelling to customers from many perspectives, but the immense benefits come at a price. Along with reliability and security, performance is the Achilles heel of Cloud services. In order to make Cloud services profitable, the Cloud service provider obviously must overbook physical resources. But at what level of resource overbooking (OB) does performance begin to suffer? The answer, of course, depends on many factors. In this paper, we present a simple analytic model to quantify the performance impact of overbooking virtualized resources as a function of the relevant environment and usage parameters. We then validate those analytic modeling results against simulation, lab tests, and field data. Finally, we propose a simple means for measuring the model parameters in the field, in order to use the analytic model to determine allowable OB factors (engineering rules) while still meeting performance constraints and service-level agreements.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6364669,no,undetermined,0
Research and application of a karst ground collapse risk assessment information system in Shenzhen Universiade Center (China),"To ensure the project safety of Shenzhen Universiade Center and normal use of the facilities, it's necessary to study the distribution rules of karst and predict the risks of Karst ground Collapse to provide a reference for prevention and control measures. With the support of MAPGIS software, a risk assessment information system of karst ground collapse in Shenzhen Universiade Center was researched and developed, which realized the automated process from basic data input to the result assessment of karst ground collapse. System functions include data input and output, data query, database generation assessment, model parameter calculations assessment, model features assessment, and result mapping assessment. The paper analyzed all possible factors that may affect karst ground collapse and obtained six important factors - distribution of dissoluble rock, cover thickness, groundwater level, karst development degree, building importance and structure. The weight coefficients of three grades of indexes were obtained by using AHP to analyze the six factors. The site was divided into 6843 grids by the size of 10m * 10m. The assessment results showed that 1.461% of the site is high-risk area, 8.666% is relatively higher risk area, 23.805% is less dangerous area, and 66.068% is low-risk area. Such assessment results can be used as reference for follow-up processing of the site.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6219244,no,undetermined,0
Research of School-Enterprise Research Evaluation Information Mining Technology Based on Analytical Hierarchy Process (AHP) and Genetic Algorithm (GA),"This paper describes the research development levels of home and abroad, and makes a mining research about Chinese school-enterprise research evaluation information. In consideration of the evaluation index system of school-enterprise research, data mining model is established in the comprehensive use of the improved genetic algorithm based on the AHP analytic hierarchy process (AHP). The paper also makes scientific quantitative analysis on the teach-research indexes to each other, the quantitative index system provides a broad prospect for teaching and research work. In order to solve the implementation of school-enterprise research mechanism the problem that is badly in need of solving in current higher vocational education in our country.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405799,no,undetermined,0
Research on Comprehensive Evaluation of the Students' Vocational Ability Based on AHP-FUZZY,"In the new situation of implementing innovative talent training mode and joint education between colleges and enterprises, colleges are focusing on comprehensively improving students' vocational skills and quality and realizing students' vocational management evaluation of students' vocational ability is a comprehensive evaluation which includes students' learning ability and vocational quality. Colleges can combine fuzzy comprehensive evaluation and AHP through building students' comprehensive evaluation system in the vocational training mode. AHP is used to identify evaluation target and index weights and fussy comprehensive evaluation is used for the overall evaluation of students' vocational ability. According to the evaluation result, colleges can improve their students' inspiration and training models.",2012,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413992,no,undetermined,0
2D-CHI: a tunable two-dimensional class hierarchy index for object-oriented databases,"We present a tunable two-dimensional class hierarchy indexing technique (2D-CHI) for object-oriented databases. We use a two-dimensional file organization as the index structure. 2D-CHI deals with the problem of clustering objects in a two-dimensional domain space consisting of the key attribute domain and the class identifier domain. In conventional class indexing techniques using one-dimensional index structures such as the B<sup>+</sup>-tree, the clustering property is exclusively owned by one attribute. These indexing techniques do not efficiently handle the queries that address both the attribute keys and the class identifiers. 2D-CHI enhances query performance by adjusting the degree of clustering between the key value domain and the class identifier domain based on the precollected usage pattern. For performance evaluation, we first compare 2D-CHI with the conventional class indexing techniques using an analytic cost model based on the assumption of uniform object distribution, and then verify the cost model through experiments using the multilevel grid file as the two-dimensional index. We further perform experiments with nonuniform object distributions. Our experiments show that our proposed method does indeed build optimal class index structures regardless of query types and object distributions. We strongly believe that our paper significantly contributes to building a self-tunable database system by supporting automatically tunable index structure",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884786,no,undetermined,0
