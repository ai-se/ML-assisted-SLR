Document Title,Abstract,Year,PDF Link,label
A discourse on complexity of process models,"Complexity has undesirable effects on, among others, the correctness, maintainability, and understandability of business process models. Yet, measuring complexity of business process models is a rather new area of research with only a small number of contributions. In this paper, we survey findings from neighboring disciplines on how complexity can be measured. In particular, we gather insight from software engineering, cognitive science, and graph theory, and discuss in how far analogous metrics can be defined on business process models.",2006,,yes
Understanding the occurrence of errors in process models based on metrics,"Business process models play an important role for the management, design, and improvement of process organizations and process-aware information systems. Despite the extensive application of process modeling in practice, there are hardly empirical results available on quality aspects of process models. This paper aims to advance the understanding of this matter by analyzing the connection between formal errors (such as deadlocks) and a set of metrics that capture various structural and behavioral aspects of a process model. In particular, we discuss the theoretical connection between errors and metrics, and provide a comprehensive validation based on an extensive sample of EPC process models from practice. Furthermore, we investigate the capability of the metrics to predict errors in a second independent sample of models. The high explanatory power of the metrics has considerable consequences for the design of future modeling guidelines and modeling tools.",2007,,yes
What makes process models understandable?,"Despite that formal and informal quality aspects are of significant importance to business process modeling, there is only little empirical work reported on process model quality and its impact factors. In this paper we investigate understandability as a proxy for quality of process models and focus on its relations with personal and model characteristics. We used a questionnaire in classes at three European universities and generated several novel hypotheses from an exploratory data analysis. Furthermore, we interviewed practitioners to validate our findings. The results reveal that participants tend to exaggerate the differences in model understandability, that self-assessment of modeling competence appears to be invalid, and that the number of arcs in models has an important influence on understandability.",2007,,yes
Recommendation Based Process Modeling Support: Method and User Experience,"Although most workflow management systems nowadays offer graphical editors for process modeling, the learning curve is still too steep for users who are unexperienced in process modeling. 'File efficiency of users may decrease when starting process modeling with minimal expertise and no obvious modeling support. This paper describes the first contribution towards a theoretically sound and empirically validated analysis of a recommender-based modeling support who is geared towards both novices and expert users. The idea is to interpret process descriptions as tags which describe the intention of the process. This leads us to the notion of virtual documents or signatures. Based oil these signatures we provide a. search interface to process models stored in a repository. Additionally the user call invoke a, recommendation function during modeling time and the system automatically identifies and suggests relevant, process fragments. BY adding two additional criteria. the frequency of process reuse and structural correctness. we arrive at a full-fledged modeling support system, which provides an easy to use interface to the user while retaining a high fidelity to the user's modeling intentions. We validated our support, system with a user experiment, based on real-life process models and our prototype implementation.",2008,,yes
On a quest for good process models: The cross-connectivity metric,"Business process modeling is an important corporate activity, but the understanding of what constitutes good process models is rather limited. In this paper, we turn to the cognitive dimensions framework and identify the understanding of the structural relationship between any pair of model elements as a hard mental operation. Based on the weakest-link metaphor, we introduce the cross-connectivity metric that measures the strength of the links between process model elements. The definition of this new metric builds on the hypothesis that process models are easier understood and contain less errors if they have a high cross-connectivity. We undertake a thorough empirical evaluation to test this hypothesis and present our findings. The good performance of this novel metric underlines the importance of cognitive research for advancing the field of process model measurement.",2008,,yes
The impact of structuredness on error probability of process models,"Recent research has shown that business process models from practice suffer from several quality problems. In particular, the correctness of control flow has been analyzed for industry-scale collections of process models revealing high error rates. In the past, structuredness has been discussed as a guideline to avoid errors, first in research on programming, and later also in business process modeling. In this paper we investigate the importance of structuredness for process model correctness from an empirical perspective. We introduce definitions of two metrics that capture the degree of (un)structuredness of a process model. Then, we use the Event-driven Process Chain models of the SAP Reference Model for validating the capability of these metrics to predict error probability. Our findings support the importance of structuredness as a design principle for achieving correctness in process models.",2008,,yes
Influence factors of understanding business process models,"The increasing utilization of business process models both in business analysis and information systems development raises several issues regarding quality measures. In this context, this paper discusses understandability as a particular quality aspect and its connection with personal, model, and content related factors. We use an online survey to explore the ability of the model reader to draw correct conclusions from a set of process models. For the first group of the participants we used models with abstract activity labels (e.g. A, B, C) while the second group received the same models with illustrative labels such as ""check credit limit"". The results suggest that all three categories indeed have an impact on the understandability.",2008,,yes
Reducing the Cognitive Complexity of Business Process Models,"Current research on the quality of business process models mostly discusses the quality of a model mainly in terms of correctness of its behaviour. In this paper, we discuss another perspective: Because the main purpose of business process models is to be read and understood by humans, understandability is an important requirement. We present some patterns for improving the understandability by reducing the cognitive load. BY applying these patterns to a repository of almost 1000 models, we have been able to improve the comprehensibility of several models.",2009,10.1109/COGINF.2009.5250717,yes
Analysis and Validation of Control-Flow Complexity Measures with BPMN Process Models,"Evaluating the complexity of business processes during the early stages of their development, primarily during the process modelling phase, provides organizations and stakeholders with process models which are easier to understand and easier to maintain. This presents advantages when carrying out evolution tasks in process models - key activities, given the current competitive market. In this work, we present the use and validation of the CFC metric to evaluate the complexity of business processes modelled with BPMN. The complexity of processes is evaluated from a control-flow perspective. An empirical evaluation has been carried out in order to demonstrate that the CFC metric can be useful when applied to BPMN models, providing information about their ease of maintenance.",2009,,yes
Error Density Metrics for Business Process Model,"In this paper, metrics for business process model (BPM), are proposed, which are capable to measure the usability and effectiveness of BPMs. The proposed model is adapting error density metrics to BPMs by considering the similarities between the conceptual characteristics of BPMs and software products. We applied seven software metrics for evaluating quality of business processes/process models. Results show that our metrics help the organization to improve their process, as weighted measurements are indicators for unexpected situations/behaviour for business processes.",2009,,yes
Quality metrics for business process modeling,"Modeling business processes is vital when improving or automating existing business processes, documenting processes properly or comparing business processes. In addition, it is necessary to be able to evaluate the quality of a business process model, which in term requires a set of quality metrics. Most of the works proposed to evaluate business process models deal with quality by adapting software metrics. This is possible, because software products and business processes software are quite similar. Our contribution in this paper consists in adapting object oriented software metrics to business process models. This adaptation is based on correspondences which we establish between BPMN (Business Process Modeling Notation) concepts and object oriented concepts. By adapting object oriented metrics, we aim to obtain new metrics which give us more information about the complexity of business processes, cohesion between process tasks and coupling between process themselves.",2009,,yes
The Impact of Secondary Notation on Process Model Understanding,"Models of business processes are usually created and presented using some visual notation. In this way, one can express important activities, milestones, and actors of a process using interconnected graphical symbols. While it has been established for other types of models that their graphical layout is a factor in making sense of these, this aspect has not been investigated in the business process modeling area. This paper proposes a set of propositions about the effects of the secondary notation, which entails layout, on process model comprehension. While individual graphical readership and pattern recognition skills are known mediators in interpreting visual cues, these propositions take expertise into account. The goal of this paper is to lay the foundation of follow-up, empirical investigations to challenge these propositions.",2009,,yes
Granularity as a Cognitive Factor in the Effectiveness of Business Process Model Reuse,"Reusing design models is an attractive approach in business process modeling as modeling efficiency and quality of design outcomes may be significantly improved. However, reusing conceptual models is not a cost-free effort, but has to be carefully designed. While factors such as psychological anchoring and task-adequacy in reuse-based modeling tasks have been investigated, information granularity as a cognitive concept has not been at the center of empirical research yet. We hypothesize that business process granularity as a factor in design tasks under reuse has a significant impact on the effectiveness of resulting business process models. We test our hypothesis in a comparative study employing high and low granularities. The reusable processes provided were taken from widely accessible reference models for the telecommunication industry (enhanced Telecom Operations Map). First experimental results show that Recall in tasks involving coarser granularity is lower than in cases of finer granularity. These findings suggest that decision makers in business process management should be considerate with regard to the implementation of reuse mechanisms of different garanularities. We realize that due to our small sample size results are not statistically significant, but this preliminary run shows that it is ready for running on a larger scale.",2009,,yes
Advanced Social Features in a Recommendation System for Process Modeling,"Social software is known to stimulate the exchange and sharing of information among peers. This paper describes how all existing system that supports process builders in completing a business process call be enhanced with various social Features. In that way, it is easier for process modeler to become aware of new related content. They call use that content to create, update or extend process models that, they are building themselves. The proposed way of achieving this is to allow users to generate and modify personalized views oil the social networks they are part, of. Furthermore, this paper describes mechanisms for propagating relevant changes between peers in such social networks. The presented work is particularly relevant in the context of enterprises that have already built large repositories of process models.",2009,,yes
On a Study of Layout Aesthetics for Business Process Models Using BPMN,"As BPMN spreads among a constantly growing user group, it is indispensable to analyze the expectations of users towards the appearance of a BPMN model. The user groups are mostly inhomogeneous since users stem from different backgrounds, e.g. IT, managerial sciences or economics. It is conceivable that BPMN novices may have different issues compared to higher skilled modeling experts. When this large set of users starts modeling, the expectations considering the graphical outcome of the modeling process may differ significantly. In this work, we analyze layout preferences of user groups when modeling with BPMN. We present a set of layout criteria that are formalized and then confirmed by a user study. The conduction of the study reveals preferences of single user groups with respect to secondary notation and layout aesthetics. From our results, proposals for adaptions of software tools towards different BPMN users can be derived.",2010,,yes
Seven process modeling guidelines (7PMG),"Business process modeling is heavily applied in practice, but important quality issues have not been addressed thoroughly by research. A notorious problem is the low level of modeling competence that many casual modelers in process documentation projects have. Existing approaches towards model quality might be of benefit, but they suffer from at least one of the following problems. On the one hand, frameworks like SEQUAL and the Guidelines of Modeling are too abstract to be applicable for novices and non-experts in practice. On the other hand, there are collections of pragmatic hints that lack a sound research foundation. In this paper, we analyze existing research on relationships between model structure on the one hand and error probability and understanding on the other hand. As a synthesis we propose a set of seven process modeling guidelines (7PMG). Each of these guidelines builds on strong empirical insights, yet they are formulated to be intuitive to practitioners. Furthermore, we analyze how the guidelines are prioritized by industry experts. In this regard, the seven guidelines have the potential to serve as an important tool of knowledge transfer from academia into modeling practice. (C) 2009 Elsevier B.V. All rights reserved.",2010,10.1016/j.infsof.2009.08.004,yes
Activity labeling in process modeling: Empirical insights and recommendations,"Few studies have investigated the factors contributing to the successful practice of process modeling. In particular, studies that contribute to the act of developing process models that facilitate communication and understanding are scarce. Although the value of process models is not only dependent on the choice of graphical constructs but also on their annotation with textual labels, there has been hardly any work on the quality of these labels. Accordingly, the research presented in this paper examines activity labeling practices in process modeling. Based on empirical data from process modeling practice, we identify and discuss different labeling styles and their use in process modeling praxis. We perform a grammatical analysis of these styles and use data from an experiment with process modelers to examine a range of hypotheses about the usability of the different styles. Based on our findings, we suggest specific programs of research towards better tool support for labeling practices. Our work contributes to the emerging stream of research investigating the practice of process modeling and thereby contributes to the overall body of knowledge about conceptual modeling quality. (C) 2009 Elsevier B.V. All rights reserved.",2010,10.1016/j.is.2009.03.009,yes
Quality Assessment of Business Process Models Based on Thresholds,"Process improvement is recognized as the main benefit of process modelling initiatives. Quality considerations are important when conducting a process modelling project. While the early stage of business process design might not be the most expensive ones, they tend to have the highest impact on the benefits and costs of the implemented business processes. In this context, quality assurance of the models has become a significant objective. In particular, understandability and modifiability are quality attributes of special interest in order to facilitate the evolution of business models in a highly dynamic environment. These attributes can only be assessed a posteriori, so it is of central importance for quality management to identify significant predictors for them. A variety of structural metrics have recently been proposed, which are tailored to approximate these usage characteristics. The aim of this paper is to verify how understandable and modifiable BPMN models relate to these metrics by means of correlation and regression analyses. Based on the results we determine threshold values to distinguish different levels of process model quality. As such threshold values are missing in prior research, we expect to see strong implications of our approach on the design of modelling guidelines.",2010,,yes
Structuredness and its significance for correctness of process models,"Recent research has shown that business process models from practice suffer from several quality problems. In particular, the correctness of control flow has been analyzed for industry-scale collections of process models revealing that error ratios are surprisingly high. In the past the structuredness property has been discussed as a guideline to avoid errors, first in research on programming, and later also in business process modeling. In this paper we investigate the importance of structuredness for process model correctness from an empirical perspective. We introduce definitions of two metrics that quantify the (un)structuredness of a process model, the degree of structuredness and the unmatched connector count. Then, we use the event-driven process chain models of the SAP reference model for validating the capability of these metrics to predict error probability. Our findings clearly support the importance of structuredness as a design principle for achieving correctness in process models.",2010,10.1007/s10257-009-0120-x,yes
Reducing Exception Handling Complexity in Business Process Modeling and Implementation: The WED-Flow Approach,"Today's enterprises reevaluate and adjust their business processes at a very high frequency, which presents a non-trivial challenge to classic BPM methodology. In particular, the dynamic nature of exception handling may generate highly significant costs when business processes are modeled and implemented statically based on formal frameworks (e.g., process algebra and Petri nets). In this work we introduce the WED-flow (Workflow, Event processing, and Data-flow) approach, a novel concept for modeling and implementation of business processes that significantly reduces the complexity of exception handling quantitatively, as compared to current approaches. WED-flows explicitly integrate events, data, conditions, and transitions by capturing data instances (future, current, and historical) as data states, which enables incremental business process development. More generally, this paper provides a conceptual basis and guidelines for capturing, processing, and storing event-handling environments. Consequently, information systems that implement business processes as WED-flows are truly dynamic and no longer time-invariant by design.",2010,,yes
Managing Process Model Complexity Via Abstract Syntax Modifications,"As a result of the growing adoption of Business Process Management (BPM) technology, different stakeholders need to understand and agree upon the process models that are used to configure BPM systems. However, BPM users have problems dealing with the complexity of such models. Therefore, the challenge is to improve the comprehension of process models. While a substantial amount of literature is devoted to this topic, there is no overview of the various mechanisms that exist to deal with managing complexity in (large) process models. As a result, it is hard to obtain an insight into the degree of support offered for complexity reducing mechanisms by state-of-the-art languages and tools. This paper focuses on complexity reduction mechanisms that affect the abstract syntax of a process model, i.e., the formal structure of process model elements and their interrelationships. These mechanisms are captured as patterns so that they can be described in their most general form, in a language-and tool-independent manner. The paper concludes with a comparative overview of the degree of support for these patterns offered by state-of-the-art languages and tools, and with an evaluation of the patterns from a usability perspective, as perceived by BPM practitioners.",2011,10.1109/TII.2011.2166795,yes
A Study Into the Factors That Influence the Understandability of Business Process Models,"Business process models are key artifacts in the development of information systems. While one of their main purposes is to facilitate communication among stakeholders, little is known about the factors that influence their comprehension by human agents. On the basis of a sound theoretical foundation, this paper presents a study into these factors. Specifically, the effects of both personal and model factors are investigated. Using a questionnaire, students from three different universities evaluated a set of realistic process models. Our findings are that both types of investigated factors affect model understanding, while personal factors seem to be the more important of the two. The results have been validated in a replication that involves professional modelers.",2011,10.1109/TSMCA.2010.2087017,yes
Human and automatic modularizations of process models to enhance their comprehension,"Modularization is a widely advocated mechanism to manage a business process model's size and complexity. However, the widespread use of subprocesses in models does not rest on solid evidence for its benefits to enhance their comprehension, nor are the criteria clear how to identify subprocesses. In this paper, we describe an empirical investigation to test the effectiveness of using subprocesses in real-life process models. Our results suggest that subprocesses may foster the understanding of a complex business process model by their ""information hiding"" quality. Furthermore, we explored different categories of criteria that can be used to automatically derive process fragments that seem suitable to capture as subprocesses. From this exploration, approaches that consider the connectedness of subprocesses seem most attractive to pursue. This insight can be used to develop tool support for the modularization of business process models. (C) 2011 Elsevier B.V. All rights reserved.",2011,10.1016/j.is.2011.03.003,yes
An Empirical Analysis of Human Performance and Error in Process Model Development,"Process models capture important corporate know-how for an effective Business Process Management. Inconsistencies between process models and corporate reality are a common phenomenon in corporate practice. Human performance in process model development is a major source for these inconsistencies. In this contribution, a human performance analysis of process model development paying special attention to the concept of human error was conducted. It was found that the frequencies of the omissions and erroneous executions of notation elements are significantly higher for novices than for experienced modelers. Moreover, experienced modelers inherently adhere to a verb-object activity labeling style. The overall empirical results indicate that experienced modelers achieve higher process model quality with less expenditure of time than novices.",2011,,yes
Recommendation-based editor for business process modeling,"To ensure proper and efficient modeling of business processes, it is important to support users of process editors adequately. With only minimal modeling support, the productivity of novice business process modelers may be low when starting process modeling. In this article, we present a theoretically sound and empirically validated recommendation-based modeling support system, which covers different aspects of business process modeling. We consider basic functionality, such as an intuitive search interface, as well as advanced concepts like patterns observed in other users' preferences. Additionally, we propose a multitude of interaction possibilities with the recommendation system, e.g., different metrics that can be used in isolation or an overall recommender component that combines several sub metrics into one comprehensive score. We validate a prototype implementation of the recommendation system with exhaustive user experiments based on real-life process models. To our knowledge, this is the only comprehensive recommendation system for business process modeling that is available. (C) 2011 Elsevier B.V. All rights reserved.",2011,10.1016/j.datak.2011.02.002,yes
Managing Process Model Complexity via Concrete Syntax Modifications,"While Business Process Management (BPM) is an established discipline, the increased adoption of BPM technology in recent years has introduced new challenges. One challenge concerns dealing with the ever-growing complexity of business process models. Mechanisms for dealing with this complexity can be classified into two categories: 1) those that are solely concerned with the visual representation of the model and 2) those that change its inner structure. While significant attention is paid to the latter category in the BPM literature, this paper focuses on the former category. It presents a collection of patterns that generalize and conceptualize various existing mechanisms to change the visual representation of a process model. Next, it provides a detailed analysis of the degree of support for these patterns in a number of state-of-the-art languages and tools. This paper concludes with the results of a usability evaluation of the patterns conducted with BPM practitioners.",2011,10.1109/TII.2011.2124467,yes
Quality indicators for business process models from a gateway complexity perspective,"Context: Quality assurance of business process models has been recognized as an important factor for modeling success at an enterprise level. Since quality of models might be subject to different interpretations, it should be addressed in the most objective way, by the application of measures. That said, however, assessment of measurement results is not a straightforward task: it requires the identification of relevant threshold values, which are able to distinguish different levels of process model quality. Objective: Since there is no consensual technique for obtaining these values, this paper proposes the definition of thresholds for gateway complexity measures based on the application of statistical techniques on empirical data. Method: To this end, we conducted a controlled experiment that evaluates quality characteristics of understandability and modifiability of process models in two different runs. The thresholds obtained were validated in a replication of the experiment. Results: The thresholds for gateway complexity measures are instrumental as guidelines for novice modelers. A tool for supporting business process model measurement and improvement is described, based on the automatic application of measurement, and assessment as well as derivation of advice about how to improve the quality of the model. Conclusion: It is concluded that thresholds classified business process models in the specific level of understandability and modifiability, so these thresholds were good and useful for decision-making. (c) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.infsof.2012.05.001,yes
Towards Understanding the Process of Process Modeling: Theoretical and Empirical Considerations,"Empirical studies of business process modeling typically aim at understanding factors that can improve model quality. We identify two limitations of such studies. First, the quality dimensions usually addressed are mainly syntactic and pragmatic, not addressing semantic quality sufficiently. Second, while findings related to model understanding have been anchored in cognitive theories, findings related to model construction have remained mostly unexplained. This paper proposes to study the process of process modeling, based on problem solving theories. Specifically, the work takes the approach that problems are first conceptualized as mental models, to which solution methods are applied. The paper suggests that investigating these two phases can help understand and hence improve semantic and syntactic quality of process models. The paper reports on an empirical study addressing the mental model created during process model development, demonstrating the feasibility of such studies. It then suggests designs for other studies that follow this direction.",2012,,yes
"Tying Process Model Quality to the Modeling Process: The Impact of Structuring, Movement, and Speed","In an investigation into the process of process modeling, we examined how modeling behavior relates to the quality of the process model that emerges from that. Specifically, we considered whether (i) a modeler's structured modeling style, (ii) the frequency of moving existing objects over the modeling canvas, and (iii) the overall modeling speed is in any way connected to the ease with which the resulting process model can be understood. In this paper, we describe the exploratory study to build these three conjectures, clarify the experimental set-up and infrastructure that was used to collect data, and explain the used metrics for the various concepts to test the conjectures empirically. We discuss various implications for research and practice from the conjectures, all of which were confirmed by the experiment.",2012,,yes
Factors of process model comprehension-Findings from a series of experiments,"In order to make good decisions about the design of information systems, an essential skill is to understand process models of the business domain the system is intended to support. Yet, little knowledge to date has been established about the factors that affect how model users comprehend the content of process models. In this study, we use theories of semiotics and cognitive load to theorize how model and personal factors influence how model viewers comprehend the syntactical information of process models. We then report on a four-part series of experiments, in which we examined these factors. Our results show that additional semantical information impedes syntax comprehension, and that theoretical knowledge eases syntax comprehension. Modeling experience further contributes positively to comprehension efficiency, measured as the ratio of correct answers to the time taken to provide answers. We discuss implications for practice and research. (C) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.dss.2011.12.013,yes
Thresholds for error probability measures of business process models,"The quality of conceptual business process models is highly relevant for the design of corresponding information systems. In particular, a precise measurement of model characteristics can be beneficial from a business perspective, helping to save costs thanks to early error detection. This is just as true from a software engineering point of view. In this latter case, models facilitate stakeholder communication and software system design. Research has investigated several proposals as regards measures for business process models, from a rather correlational perspective. This is helpful for understanding, for example size and complexity as general driving forces of error probability. Yet, design decisions usually have to build on thresholds, which can reliably indicate that a certain counter-action has to be taken. This cannot be achieved only by providing measures; it requires a systematic identification of effective and meaningful thresholds. In this paper, we derive thresholds for a set of structural measures for predicting errors in conceptual process models. To this end, we use a collection of 2000 business process models from practice as a means of determining thresholds, applying an adaptation of the ROC curve method. Furthermore, an extensive validation of the derived thresholds was conducted by using 429 EPC models from an Australian financial institution. Finally, significant thresholds were adapted to refine existing modeling guidelines in a quantitative way. (C) 2012 Elsevier Inc. All rights reserved.",2012,10.1016/j.jss.2012.01.017,yes
"A comprehensive model for cyclic voltammetric study of intercalation/de-intercalation process incorporating charge transfer, ion transport and thin layer phenomena","A comprehensive model involving intercalation, competitive background reaction and de-intercalation is described. The interaction step is modelled as simple irreversible charge transfer and charge transfer coupled with diffusion. The de-intercalation step is modeled as a thin-film process. Overall cyclic voltammetric (CV) responses under all these limiting conditions are also presented and discussed. The method is also used for comparing typical experimental CV responses reported in the literature. The method is found to be useful in comparing the intercalation processes under different experimental conditions and evaluating model parameters. (C) 2000 Elsevier Science S.A. All rights reserved.",2000,10.1016/S0378-7753(00)00369-4,no
Microstructural evolution of Inconel* 718 during ingot breakdown: process modelling and validation,"A computer model is presented that describes microstructural evolution during the ingot breakdown of nickel base superalloy Inconel 718 via the open die cogging operation, To support the development of the model, a compression testing programme has been carried out which covers the ranges of temperatures, strains, and strain rates experienced during thermomechanical processing. Analysis of the flow curves has allowed the identification of the regimes in which the various deformation mechanisms take place. Logic based rules have been incorporated into the model, and this has allowed predictions of the microstructural e-volution to be made. Where possible, the results have been compared with the available experimental data and it is shown that theory and experiment are in reasonable agreement, A number of computational experiments have been carried out, to study the effects of changing the forging procedure. MST/4426.",2000,10.1179/026708300773002627,no
Microstructural evolution of nickel-base superalloy forgings during ingot to-billet conversion: Process modelling and validation,"A computer-based process modelling capability has been developed to simulate the evolution of microstructure during ingot-to-billet conversion of nickel-base superalloys. A unique feature of the model is the incorporation of rules describing the dynamic and static recrystallisation phenomena that govern microstructural development. Computational investigations consider the influence of process control parameters such as die velocity and displacement to obtain an optimal microstructure. Furthermore, a comparison is made of the effective strain profiles obtained by squeezing flat versus rounded sections of the workpiece.",2000,,no
Modeling and simulation of hydrodemetallation and hydrodesulfurization processes with transient catalytic efficiency,"A model is presented for the description of the concentration behavior of organometallic and sulfurated compounds in hydrodemetallation and hydrodesulfurization catalytic processes, where catalyst effectiveness decreases with time. Due to the complexity of the mixture, an approach based on pseudocomponents was adopted. The system is modeled as an isothermal tubular reactor with axial dispersion, where the gas phase (hydrogen in excess) flows upward concurrently with the liquid phase (heavy oil) while the solid phase (catalyst) stays inside the reactor in an expanded (confined) bed regime. The catalyst particles are very small and are assumed to be uniformly distributed in the reactor. The heavy oil fractions contain organometallics and sulfurated compounds, from which the metals and sulfur are to be removed, the metals as deposits in the catalyst pores and the sulfur as gas products. Simulations were carried out where the concentration profile inside the reactor was calculated for several residence times.",2000,,no
A simulation approach based on the petri net model to evaluate the global performance of a weaving process,A monitoring system provides the value of machine efficiency from a previous weaning process. This paper presents an approach to a simulation to predict the efficiency in a weaving group by using stochastic information collected from similar weaving processes carried out in the past. The simulation study of the discrete weaving process is based on the stochastic Petri net model. The simulation results for a case study are also presented in this paper.,2000,,no
Gradient gas sensor microarrays for on-line process control - a new dynamic classification model for fast and reliable air quality assessment,"A new dynamic gas classification model was developed to achieve a reliable on-line discrimination at very fast response times. The aim was to be able to follow rapid changes in gas compositions using an electronic nose in consumer applications. The electronic nose is based on a microarray especially designed for production at very low costs. This is essential for application in mass products. Common classification methods used fur signal evaluation of electronic noses such as Linear Discriminant Analysis (LDA), Neural Networks (NN) or Soft Independent Modelling of Class Analogy (SIMCA) fail to detect non-stationary gas mixtures. The new model, however, combines classification of steady states with transient evaluation via time series analysis. Rapid signal transients are detected by appropriate digital filters, steady state signals are classified by the above mentioned standard methods. The simplicity of the algorithm model allows implementation in low-cost electronic units, containing micro controllers with very limited memory capacity. To give an example, the automatic control of thr ventilation flap of automobiles was investigated. Intermediate streams of bad air could be detected within 1-2 s. The error of pollutants detection was reduced from 25%, applying static classification only, to 10% for the new dynamic model. (C) 2000 Elsevier Science S.A. All rights reserved.",2000,10.1016/S0925-4005(00)00470-6,no
Evaluation of component behavior in process system on material-utilization diagram,"A new method is proposed to disclose the role of each component in a chemical process system. There are many methods for analyzing a process system from energy viewpoints. But these viewpoints cannot be applied to the behavior of each chemical component especially related to environmental problems, because the energy of such a dilute component is negligibly small as compared with that of the main component, Hence, a new diagram with ordinate of ln p(j) and abscissa of n(f)RT(0) for component j is proposed and it is called a material-utilization diagram. The relationship between this diagram and the energy-utilization diagram is demonstrated from the entropy production.",2000,10.1252/jcej.33.184,no
Effects of correlation among parameters on prediction quality of a process-based forest growth model,"A nonparametric method was introduced as a technique for evaluating the effects of parameter correlation on prediction quality of process-based forest growth models. The method was based on a rank correlation and Cholesky decomposition. For a given data matrix, by reordering the observations, the method would produce a rearranged matrix with the desired correlation structure. The method was computationally simple and efficient. Using a process-based model calibrated for red pine (Pinus resinosa Ait.) as an example, small-scale Monte Carlo simulations revealed that parameter correlation had different effects on the model's prediction means and variances, depending on the importance of the parameters involved. In general, given the same level of correlation, correlation between two important parameters would have greater influences on prediction quality than correlation between two less important parameters. Parameter correlation slightly affected the prediction means but could significantly change prediction variances. The relationship between prediction quality and the degrees of correlation, however, was not necessarily a linear one. The same parameter correlation might also have different effects on different state variables.",2000,,no
A relational database model for improving quality assurance and process control in a composite manufacturing environment,"A relational database is a powerful tool for collecting and analyzing the vast amounts of inner-related data associated with the manufacture of composite materials. A relational database contains many individual database tables that store data that are related in some fashion. Manufacturing process variables as well as quality assurance measurements can be collected and stored in database tables indexed according to lot numbers, part type or individual serial numbers. Relationships between manufacturing process and product quality can then be correlated over a wide range of product types and process variations. This paper presents details on how relational databases are used to collect, store, and analyze process variables and quality assurance data associated with the manufacture of advanced composite materials. Important considerations are covered including how the various types of data are organized and how relationships between the data are defined. Employing relational database techniques to establish correlative relationships between process variables and quality assurance measurements is then explored. Finally, the benefits of database techniques such as data warehousing, data mining and web based client/server architectures are discussed in the context of composite material manufacturing.",2000,10.1117/12.385497,no
Evaluation of aggregating brain cell cultures as a model for studying ischaemia-related neurodegenerative processes,"Aggregating brain cell cultures prepared from fetal rat telencephalon and grown in a chemically defined medium reach a high degree of cellular differentiation and organisation, resembling, in many aspects, brain tissue in vivo. In addition, aggregate cultures containing either mixed brain cells or predominantly neurons can be obtained in relatively large amounts. Because of the unique features of this culture system, we examined its use for ischaemia-related investigations. Using biochemical and immunocytochemical criteria, we showed that, in these cultures, ischaemia could be simulated by a transient switch from rotatory to stationary culture conditions, which increased the medium-to-tissue glucose and oxygen gradients and thereby caused selective neuronal cell death. The extent of the ischaemia-induced damage correlated with the observed degree of glucose depletion, and a similar pattern of adverse effects was obtained by transient glucose restriction. Further investigations showed that these deleterious effects were attenuated by antagonists of either N-methyl-D-aspartic acid receptors or L-type voltage-gated calcium channels, as well as by chelation of extracellular Ca2+, in accordance with the view that the influx of extracellular Ca2+ is a critical event in ischaemia-induced neuronal cell death. In addition, the presence of astrocytes enhanced the detrimental effects of ischaemia or glucose restriction on neurons. Taken together, the results obtained so far indicate that aggregating brain cell cultures offer a useful in vitro model for studying brain ischaemia-related pathogenic mechanisms.",2000,,no
"Process modeling, simulation, and paint thickness measurement for robotic spray painting","An algorithm and a computer program are developed for modeling of the spray painting process, simulation of robotic spray painting, and off-line programming of industrial robots for painting of curved surfaces. The computer program enables the user to determine the painting strategies, parameters, and paths which will give the desired paint thickness. Surface models of the parts that are to be painted are obtained by using a computer-assisted design (CAS) software. Models of relatively simple surfaces are formed by using the surface generation tools of the CAD software. For parts with more complex surfaces, point data related to the part is collected by using a laser scanner, and this data is used to form the CAD model of the part surface. The surface is then divided into small. triangular elements and centroid coordinates, and unit normals of the elements are determined. Surface data together with the spray distance, painting velocity, and paint flow rate flux are used for simulation of the process and paint thickness analysis. Paint flow rate flux is determined experimentally by using different spray gun settings and painting parameters. During the experiments flat surfaces are painted by using a single painting stroke of the gun. Then, paint thickness measurements are made on the surfaces. It is observed that besides the technical specifications of the spray gun, air and paint nozzles, and paint needle, basic settings like paint tank pressure, spray air pressure, and gun needle-valve position affect paint cone angle and paint flow rate, which finally characterize the spray painting process. For that reason, settings and parameters should be changed and the painting process should be simulated until an acceptable paint thickness distribution is obtained for the part that is going to be painted. The robot program is then generated in the robot's programming language. Paint thickness distribution on the painted surface is determined by measuring the thicknesses using the robot and the CAD model of the part surface. The thicknesses are at the centroids of the surface elements. A measurement probe of the coating thickness measurement gauge is attached to the wrist of the robot by using a feedback/safety adapter designed and manufactured for this purpose, (C) 2000 John Wiley & Sons, Inc.",2000,10.1002/1097-4563(200009)17:9<479::AID-ROB3>3.0.CO;2-L,no
Impact characteristics and mechanical alloying processes by ball milling: Experimental evaluation and modelling outcomes,"An experimental methodology has been developed and tested for evaluating the impact frequency and the relative velocity of the impacting ball in the course of deformation-mixing processes carried out in vibro- and shaker-mills undergoing periodic motion. We measure the times at which the reciprocating milling vial reaches extreme displacements and the times at which the ball collides with either flat end of the vial. These two periodic signals define a 'time-lag' parameter from which it is possible to obtain the impact velocity, provided the vial motion is known. The method allows one to verify the operating conditions under which the periodicity of the ball motion is achieved and plastic collisions are approached. Under these circumstances, the energy released at impact can be safely approximated as the total kinetic energy of the grinding ball. In addition, the collision frequency - another important parameter of the process - is concurrently obtained. Together, their knowledge permits tracking of a milling run (impact by impact) and, therefore, reconstruction of the dynamics of the impact events. Experimental results agree well with the outcomes of the kinematic and dynamic models developed for both vibro- and shaker-milling devices. In the later part of the paper, these procedures are employed to study the mechanical alloying of nickel-titanium powder mixtures milled under widely differing impact energy regimes. The amorphous fractions are determined by X-ray methods, and the transformation rates depend on the energy intensity defined as the product of the impact energy with the impact frequency. However, irrespective of the single impact energy, the same amount of the parent powders was observed to transform as a function of the energy dose (given by the product of the energy intensity and the milling time). A linear kinetic law characterises the dependence of the reaction semi-transformation period - representing the milling time required to amorphize half of the load - on the energy intensity parameter. These results extend the validity of relationships previously observed in the copper-titanium system, and suggest a general transformation path that is common to amorphization processes induced by mechanical treatment.",2000,,no
A gustatory information-processing model for the evaluation of taste preference,"Applications of sensory information processing in neural networks have made significant and varied advances with regard to senses such as vision and hearing. Although similar research has been conducted on gustatory sensation, progress in these area lags behind due to the complexity of the mechanisms involved. If the complex gustatory mechanisms could be elucidated from an engineering standpoint, however, it could help in the medical geld, for example, in the diagnosis of gustatory disturbances and treatment planning, while in the food industry it could hold promise for the production and processing of better tasting, healthier foods. Our group has therefore attempted an engineering analysis of the mechanisms of gustatory sensation. The present study examines a model for the evaluation of human taste; that is, how does our sense of taste determine ""good"" and ""bad."" In our experiments we used tofu as a model, since there are relatively strong taste preferences toward the food. The sensory evaluations were enforced by professional tasters, who have a developed sense of taste. Using a neural network with these evaluation results as the master data, the possibility of evaluating human taste evaluation G-om sensory information was evaluated. The results indicated that, using the present method and evaluation is indeed possible [1][2][3][4][5].",2000,,no
Modelling and measurements of below cloud scavenging processes in the highly industrialised region of Cubatao-Brazil,"Below cloud scavenging has been investigated from rainwater chemistry measurements using a one-dimensional scavenging numerical model and surface data from the local atmospheric conditions. The local emissions of pollutants from a large industrial complex in the south-eastern Brazil were also evaluated in the modelling. The scavenging model was coupled to the measurements of SO42-, NO3- and NH4+ found in the rainwater samples. The concentration of gases and particles samplings and meteorological parameters measured during the study of rain episodes were used as input data in the numerical model, which simulates the raindrop interactions associated with the removal process. The results were compared with those actually measured. The variability of the rainwater concentrations estimated through the scavenging model was also compared to the chemical analysis of the rainwater collected by fractionated rain samplers. The results show a good agreement between both modelled and observed data as seen from the role of the raindrop size distribution in the below cloud scavenging of pollutants (C) 2000 Elsevier Science Ltd. All rights reserved.",2000,10.1016/S1352-2310(99)00503-8,no
"Evaluation and modeling of natural attenuation processes in a landfill leachate plume (Banisveld, The Netherlands)","Biogeochemical processes were modeled along a central flow path in a landfill leachate plume (Banisverd, the Netherlands) using the forward reactive transport model PHREEQC 2. The following processes needed to be included in the model to explain the observed development in groundwater composition: microbially mediated oxidation of DOC coupled to Fe(III)reduction, kinetic precipitation of calcite and siderite, cation-exchange and proton buffering.",2000,,no
Business process modeling and simulation for quality management,"Business processes represent the most essential part of quality management. Precisely defined, a process is the only way the product can be obtained repeatably with a required quality. Specification of business processes is usually based on textual form and/or in a form of flow charts. The problem is that a lack of formalization in the process definition enables to employ only the ""best-practice"" approach that requires feed-back from real system to improve process. Modeling and simulation of business processes can help to shorten the cycle radically due to possibility of computer-based experimentation. This contribution shows how the formalized building of business process models can be unified with the principles of quality management to provide one integral and flexible system, easy to use and easy to develop.",2000,,no
Basic principles of data acquisition and data processing in the construction of high-quality virtual models,"Creating models for virtual reality subdivides into several steps. The aim of the data acquisition is the extraction of nearly isotropic (same solution in all three axes) data sets with low noise content. An approximate isotropy can be achieved by suitable choice of scan parameters. For raw data reconstruction, the application of high-resolution reconstruction algorithms is prohibited due to increased noise. A missing isotropy can computationally be approximated by interpolation. Further noise suppression is achieved by applying filters. Additionally, the contrast of the object for segmentation can be increased by image processing operators. The correct choice of the segmentation method and the editing tools is essential for a precise segmentation with minimal user interaction. Prior to visualization,smoothing the shape of the segmented model (shape-based or morphological interpolation, polygon reduction of wire frame model) further improves the visual appearance of the 3D model.",2000,10.1007/s001170050673,no
Modeling impairment: Using the disablement process as a framework to evaluate determinants of hip and knee flexion,"Elders often present to health care providers with multiple inter-related conditions that determine an individual's ability to function. The disablement process provides a generalized sociomedical framework for investigating the complex pathways from chronic disease to disability. At each stage of the main pathway, associations may exist among primary physical factors and modifying variables that ultimately have downstream effects on the progression toward disability. The purpose of the present analysis is to examine the inter-relationships between a cohesive set of variables primarily at the level of impairment that may affect hip and knee flexion range of motion (ROM). The San Antonio Longitudinal Study of Aging enrolled 833 community dwelling Mexican (MA) and European American (EA) elders aged 64-78 years between 1992 and 1996. Of these, 647 had complete data from both a home-based and performance-based battery of assessments for these analyses. Concerning impairments, hip ROM was measured using an inclinometer, and knee ROM using a goniometer. Pain location and intensity were assessed using the McGill Pain Questionnaire. Peripheral vascular disease was assessed using doppler brachial and ankle systolic blood pressures. Ankle and knee reflexes, and vibratory sensation were assessed by a standardized neurological examination. As to diseases, diabetes was assessed using a combination of blood glucose levels and self-report, and arthritis by self-report. Concerning modifying variables, height and weight were directly measured and used to calculate BMI. Activity level was assessed with the Minnesota Leisure Time Questionnaire. Analgesic use was assessed by direct observation of medications taken within the past two weeks. We used structural equation modeling to test associations between the variables that were specified a priori. These analyses demonstrate the central role of BMI as a determinant of hip and knee flexion ROM. For an increase in level of BMI, the coefficients [SEM] for changes in levels of hip and knee ROM were -0.38 [0.05] and -0.26 [0.05], respectively. A higher BMI resulted in lower hip and knee ROM. BMI was also directly associated with prevalent diabetes (0.10[0.05]) and arthritis (0.17 [0.05]). However, after adjustment for BMI, diabetes and arthritis did not have direct independent associations with either hip or knee ROM BMI was also indirectly associated with knee, but not hip, ROM through paths including lower-leg pain, pain intensity, and neurosensory impairments. Diabetes had an indirect association with hip, but not knee ROM, through a path including peripheral vascular disease. In conclusion, BMI is ct primary direct determinant of hip and knee ROM. The paths by which diabetes and arthritis lead to physical disability may be mediated, in part, at the level of impair ment by BMI's association with joint range of motion. Interventions designed to decrease the impact of diabetes and arthritis on disability should track changes in BMI and joint ROM to measure the paths that account for the intervention's success. The observed associations suggest that interventions targeted to decrease BMI itself may lead to improved function in part through improved joint ROM. (C) 2000, Editrice Kurtis. (C) 2000, Editrice Kurtis.",2000,,no
Efficient approximation for building error budgets for process models,"Error budgets of process models allow us to partition the uncertainty (estimation error) in model projections caused by propagation of uncertainty in model inputs. Orthogonal polynomials, which are often employed to empirically represent unknown and possibly very complex relationships, have been applied to the development of error budgets by fitting the variance of the projection as a function of the standard errors of the parameter estimates of the process model inputs. Data generation approaches to fit those polynomials have involved some type of factorial design. However, that strategy may become unworkable for process models with many model inputs. In this paper, we propose an efficient method for building error budgets to not only overcome the limitations of factorial arrangements, but also to approximate the results obtainable through those designs. The proposed data generation scheme consists of: (a) repeatedly sampling random, equally-spaced, and equally-probable levels of standard error of the model input parameter estimates to characterize the level of error of their probability distributions; and (b) for each set of standard errors, conducting a number of Monte Carlo runs by sampling random values of input parameter estimates from each of those distributions to obtain projections of the process model and thus estimate the variance of the projection. The variance of the projection is then regressed on the orthogonally-transformed values of the input standard errors. The terms of the orthogonal polynomial are then ranked in decreasing order with respect to their explanatory importance in the function. The characteristics of this sampling scheme are analyzed through intensive supercomputer simulations under different combinations of number of model inputs and sample sizes. An example is conducted for a process forest growth model based on the pipe model theory and the self-thinning rule applied to a stand of red pine (Pinus resinosa Ait.) growing in the Great Lakes region of North America. The error budget resulting from this example closely resembles the error budget obtained by another study conducted for the same process model and forest stand using a fractional factorial design. It is concluded that the method proposed here provides an efficient strategy for building error budgets for process models with many model inputs. (C) 2000 Elsevier Science B.V. All rights reserved.",2000,10.1016/S0304-3800(00)00347-1,no
Validation via FEM and plasticine modeling of upper bound criteria of a process-induced side-surface defect in forgings,"Finite element analysis and plasticine modeling has been performed for an axisymmetric forging with a double ram action to validate the results of upper bound analysis which provided the geometric and processing conditions that can be used to avoid the development of unacceptable side-surface cracks. The finite element analysis, by using NIKE2D, an implicit finite deformation FEM program, has provided detailed information about the stress- and strain-states that the workpiece experiences and shows that large axial strains develop in the mid-thickness region at small workpiece thicknesses, which correlates to the conditions of surface-crack formation as predicted by the upper bound analysis. Surface cracks obtained in an FEM analysis of two half-billet workpieces have shown excellent agreement with the trends developed through the upper bound analysis. The physical modeling experiments with plasticine have provided further validation of both the upper bound model and the criteria curves for the prevention of side-surface defects. The process of side-surface cracking in a double action axisymmetric forging process was analyzed by the upper bound method. The results of this analysis are presented elsewhere (Y.H. Moon, C.J. Van Tyne, W.A. Gordon, J. Mater. Process. Technol., WTRA, pp. 412, 413). The prime objective of the present paper is to provide validation of the cracking criteria that was developed, this validation being done by both the numerical finite element method and by the use of plasticine modeling material with plexiglass tooling in laboratory experiments. (C) 2000 Elsevier Science S.A. All rights reserved.",2000,10.1016/S0924-0136(99)00417-3,no
From simulation with a business process optimisation model to eco-efficiency,"For environmentally oriented business decision-making, the linear mixed integer optimisation model of the business process with included possibilities for an integrated approach to environmental management is presented in this paper. Using this model as a scenario in decision-making simulations, managers can recognize and evaluate the consequences of their decisions on business performance and oil the environment. Further the measures of resource efficiency, as important measures of eco-efficiency in practice, are developed using the presented optimisation model. Finally, some application results are presented with a practical case, and development possibilities are also introduced.",2000,,no
Study of a Markov model for a high-quality dependent process,"For high-quality processes, non-conforming items are seldom observed and the traditional p (or npl charts are not suitable for monitoring the state of the process. A type of chart based an the count of cumulative conforming items has recently been introduced and it is especially useful for automatically collected one-at-a-time data. However, in such a case, it is common that the process characteristics become dependent as items produced one after another are inspected. In this paper, we study the problem of process monitoring when the process is of high quality and measurement values possess a certain serial dependence. The problem of assuming independence is examined and a Markov model for this type of process is studied, upon which suitable control procedures can be developed.",2000,,no
Influence of disinfection processes on the microbial quality of potable groundwater in a laboratory-scale system model,"Groundwater was used to evaluate the influence of the disinfection processes on the microbial quality of potable water distribution systems using laboratory-scale units. Coliform bacteria, heterotrophic plate count and total bacteria were used for the evaluation of the bactericidal effectiveness of each disinfectant. The microbial disinfection efficacy of chlorine, chloramine, ozone and UV irradiation was found to be equally effective in the elimination of coliform bacteria during the first hours (20 min-2 h) after disinfection. Complete elimination of coliforms in hydrogen peroxide treated water occurred after 48 h. More than 4 log cfu. ml(-1) (average killing rate) heterotrophic bacteria were killed by all the disinfectants with the exception of hydrogen peroxide (average killing rates: 3-2 log cfu. ml(-1)). However, ozone was highly effective within the first 2 h as shown by the average killing rate of 4 log cfu. ml(-1) heterotrophic bacteria in both source waters. The phenomenon of bacterial regrowth was linked to the absence of concentrations of disinfectant residuals. Bacterial regrowth, however, could be detected earlier with chlorine (after 20 min-average regrowth rate 0.064 h(-1), average generation time 10.95 h), ozone (after 2 h-average regrowth rate 0.202 h(-1), average-generation time 5.04 h) and UV treated water (after 2 h-average regrowth rate 0.263 h(-1), average generation time 2.70 h) than chloramine (between 24 h and 48 h-average regrowth rate 0.057 h(-1), average generation time 13.87 h) and hydrogen peroxide treated water (after 48 h-average regrowth rate 0.063 h(-1), average generation time 12.74 h). The greater persistence of monochloramine (7 days) and hydrogen peroxide (14 days) residuals were found to inhibit bacterial regrowth in potable water.",2000,,no
Histologic evaluation of the repair process of chondral and osteochondral lesions in a rabbit model,"Healing of partial and osteochondral surgical cartilaginous lesions to the rabbit femur was studied histologically to determine healing time and quality of the repaired/regenerated tissue, so as to construct an experimental in vivo model for the evaluation of medical and/or surgical therapies, which so far have not ensured complete resolution of the cartilaginous defects. In eighteen New Zealand rabbits, an osteochondral lesion (2.5 x 2 mm) was drilled in the right femoral medial condyle by lateral cutaneous incision and arthrotomy of each knee was performed, while, a chondral lesion (1.25+/-0.2 x 0.8+/-0.2 mm) was created in the left femoral medial condyle of each rabbit. Results showed no healing of the chondral defect and abnormal bony growth beyond the articular profile with a fibrous coating, even at 12 weeks in osteochondral lesion.",2000,,no
An evaluation of three material models used in the finite element simulation of a sheet stretching process,"Hemispherical punch stretch tests were simulated using an in-house dynamic explicit finite element code which incorporated three material models: J2 flow, Gurson and Taylor crystalline, and results are compared with those obtained from experiments carried out on a Nakajima type test rig. Only the Gurson and crystalline models predicted material instabilities, though the former had a much more sudden characteristic. Although there was an enormous disparity between the computational times, the accuracy with which the crystalline model predicts the strain distribution throughout the loading history, the necking geometry and the load-displacement relationships, justifies, in this case, the additional expense. (C) 2000 Elsevier Science S.A. All rights reserved.",2000,10.1016/S0924-0136(00)00419-2,no
Geotechnical centrifuge modelling of gelifluction processes: validation of a new approach to periglacial slope studies,"Here we compare scaled centrifuge modelling of gelifluction processes with earlier full-scale physical modelling experiments. The objective is to assess the validity of the centrifuge technique for cryogenic slope-process investigations. Centrifuge modelling allows correct self-weight stresses to be generated within a small-scale physical model by placing it in an elevated gravitational field. This paper describes an experiment in which a scaled frozen-slope model was thawed in a gravitational field equivalent to ten gravities. After four cycles of thawing, during which soil temperatures, pore pressures, thaw settlement and downslope soil displacements were continuously monitored, a series of marker columns were excavated to reveal profiles of soil movement. Comparison of these data with those from an earlier full-scale laboratory simulation experiment indicates that thaw-related gelifluction was successfully reproduced during centrifuge modelling. It is concluded that rates of soil shear strain during gelifluction were not time-dependent, since soil displacements in the centrifuge tests were of a similar magnitude to or greater than those observed in the much longer-duration full-scale simulation. This suggests that no transition occurred in soil behaviour from a frictional plastic to a true viscous fluid during the period of high moisture contents immediately following thaw.",2000,10.3189/172756400781819842,no
"Antarctic regional modelling of atmospheric, sea-ice and oceanic processes and validation with observations","High-latitude interactions of local-scale processes in the atmosphere-ice-ocean system have effects on the local, Antarctic and global climate. Phenomena including polynyas and leads are examples of such interactions which, when combined, have a significant impact on larger scales. These small-scale features, which are typically parameterized in global models, can be explicitly simulated using high-resolution regional climate system models. As such, the study of these interactions is well suited to a regional model approach and is considered here using the Arctic Regional Climate System Model (ARCSyM). This model has been used for many simulations in the Arctic, and is now implemented for the Antarctic. Observations of such processes in the Antarctic are limited, which makes model validation difficult. However, using the best available observations for an annual cycle, we have determined a suite of model parameterizations which allows us to reasonably simulate the Antarctic climate. This work considers a fine-resolution (20 km) simulation in the Cosmonaut Sea region, with the eventual goal of elucidating the mechanisms in the formation and maintenance of the sensible-heat polynya which is a regular occurrence in this area. It was found in an atmosphere-sea-ice simulation that the ocean plays an important role in regulating the sea-ice cover in this region in compensating for the cold atmospheric conditions.",2000,10.3189/172756400781820138,no
Modelling health care processes for eliciting user requirements: A way to link a quality paradigm and clinical information system design,"Hospital information systems have to support quality improvement objectives. The design issues of health care information system can be classified into three categories: 1) time-oriented and event-labelled storage of patient data; 2) contextual support of decision-making; 3) capabilities for modular upgrading. The elicitation of the requirements has to meet users' needs in relation to both the quality (efficacy, safety) and the monitoring of all health care activities (traceability). Information analysts need methods to conceptualise clinical information systems that provide actors with individual benefits and guide behavioural changes. A methodology is proposed to elicit and structure users' requirements using a process-oriented analysis, and it is applied to the field of blood transfusion. An object-oriented data model of a process has been defined in order to identify its main components : activity, sub-process, resources, constrains, guidelines, parameters and indicators. Although some aspects of activity, such as ""where"", ""what else"", and ""why"" are poorly represented by the data model alone, this method of requirement elicitation fits the dynamic of data input for the process to be traced. A hierarchical representation of hospital activities has to be found for this approach to be generalised within the organisation, for the processes to be interrelated, and for their characteristics to be shared.",2000,,no
Quality loss function based manufacturing process setting models for unbalanced tolerance design,"In many manufacturing processes, unbalanced tolerance design is a common occurrence where the deviation of a quality characteristic in one direction is more harmful than in the opposite direction. Traditionally, the manufacturer of a component would set the process mean at a target value, but this method fails to minimise the expected quality loss. In order to minimise the expected loss of quality, the process mean should be shifted a little from the target value. Two specific models of quality loss functions are discussed; when the standard deviation of the quality characteristic remains constant, and when the standard deviation of the qualify characteristic is proportional to the process mean.",2000,10.1007/PL00013130,no
Analysis and numerics of strongly degenerate convection-diffusion problems modelling sedimentation-consolidation processes,"In one space dimension, the phenomenological sedimentation-consolidation model reduces to an initial-boundary value problem (IBVP) for a nonlinear strongly degenerate convection-diffusion equation. Due to the mixed hyperbolic-parabolic nature of the model, its solutions are discontinuous and entropy solutions must be sought. In this contribution, we review recent existence and uniqueness results for this and a related IBVP! and present numerical methods that can be used to correctly simulate this model, i.e. conservative methods satisfying an entropy principle. Included in our discussion are finite difference methods and methods based on operator splitting, which are employed to simulate the settling of flocculated suspensions.",2000,10.1016/B978-008043568-8/50013-4,no
"A process for improving multi-technology system high level design: Modeling, verification and validation of complex optronic systems","In order to support the design of complex systems such as optronic ones, and especially the function-processing-oriented part of this work, we propose a method set on a connection of stages. This assistance aims at making formal and thus checkable the representation of the system itself on the one hand and of its properties on the other hand. Then, it points to use those formalizations to manage the real verification of this system and to determine the agreement between ifs model and the expected properties. The principle is to follow the first steps (corresponding to the design phase) of the traditional V-cycle of system development, and to enrich each step with formal features: the first step is the elaboration, based on initial requirements, of a function processing oriented specification. The first result is then a formalized (functional and non-functional) requirements set. It allows the system designer to construct, using the generic model and properties inventory, on one hand the precise model of the system (i.e. an instantiated model, called MOTI), and on the other hand the list of properties to be verified. This MOTI is then (automatically) translated into a so-called MOTIF expressed into a formalism dedicated to formal proof (required for formal verification and validation processes). Concurrently, the expected properties (which have been extracted from the generic properties inventory enriched by specific characteristics such as user-defined values or :application-specific parameters), are translated into the accurate formalism for automated proof. By developing a solution dedicated to optronic systems, this project bn,aches the concept of multi-technologies system design. In order to support reuse, the approach is voluntarily generic and complete, thus allowing modeling and verification of (possibly any kind of) optronic systems.",2000,,no
How do we understand a system with (so) many diagrams? Cognitive integration processes in diagrammatic reasoning,"In order to understand diagrammatic reasoning with multiple diagrams, this study proposes a theoretical framework that focuses on the cognitive processes of perceptual and conceptual integration. The perceptual integration process involves establishing interdependence between relevant system elements that have been dispersed across multiple diagrams, while the conceptual integration process involves generating and refining hypotheses about a system by combining higher-level information inferred from the diagrams. This study applies a diagrammatic reasoning framework of a single diagram to assess the usability of multiple diagrams as an integral part of a system development methodology. Our experiment evaluated the effectiveness and usability of design guidelines to aid problem solving with multiple diagrams. The results of our experiment revealed that understanding a system represented by multiple diagrams involves a process of searching for related information and of developing hypotheses about the target system. The results also showed that these perceptual and conceptual integration processes were facilitated by incorporating visual cues and contextual information in the multiple diagrams as representation aids. Visual cues indicate which elements in a diagram are related to elements in other diagrams; the contextual information indicates how the individual datum in one diagram is related to the overall hypothesis about the entire system.",2000,10.1287/isre.11.3.284.12206,no
Infill asymptotics for a stochastic process model with measurement error,"In spatial modeling the presence of measurement error, or ""nugget"", can have a big impact on the sample behavior of the parameter estimates. This article investigates the nugget effect on maximum likelihood estimators for a one-dimensional spatial model: Omstein-Uhlenbeck plus additive white noise. Consistency and asymptotic distributions are obtained under infill asymptotics, in which a compact interval is sampled over a finer and finer mesh as the sample size increases. Spatial infill asymptotics have a very different character than the increasing domain asymptotics familiar from time series analysis. A striking effect of measurement error is that MLE for the Omstein-Uhlenbeck component of the parameter vector is only fourth-root-n consistent, whereas the MLE for the measurement error variance has the usual root-n rate.",2000,,no
Non-isothermal three-dimensional developments and process modeling of composites: Flow/thermal/cure formulations and experimental validations,"In the process modeling via Resin Transfer Molding (RTM) for thick composite sections, multi-layer preforms with varying thermophysical characteristics across the different layers, or for geometrically complex mold geometries with varying thicknesses, the assumption of a thin shell-like geometry is no longer valid. The flow in the through thickness direction is no longer negligible and current practices of treating the continuously moving flow front as two-dimensional and the temperature and cure as three-dimensional are not representative of the underlying physics. In view of these considerations, in the present study, the focus is on the nonisothermal process modeling of composites employing full three-dimensional modeling/analysis developments via effective computational techniques. The specific applications are for thick composite,geometries where the thickness is comparable to the ether dimensions of the part. For the first time, an implicit pure finite element front tracking technique is employed for the transient flow/thermal/cure coupled behavior of the full three-dimensional modeling of the moving boundary value problem, and, due to the highly advective nature of the non-isothermal conditions involving thermal and polymerization reactions, special considerations and stabilization techniques are proposed. Validations and comparisons with available experimental results are particularly emphasized and demonstrated.",2000,,no
Mathematical problems in the application of multilinear models to facial emotion processing experiments,"In this paper we describe the enormous potential that multilinear models hold for the analysis of data from neuroimaging experiments that rely on functional magnetic resonance imaging (fMRI) or other imaging modalities. A case is made for why one might fully expect that the successful introduction of these models to the neuroscience community could define the next generation of structure-seeking paradigms in the area. In spite of the potential for immediate application, there is much to do from the perspective of statistical science. That is, although multilinear models have already been particularly successful in chemistry and psychology, relatively little is known about their statistical properties. To that end, our research group at the University of Kentucky has made significant progress. In particular, we are in the process of developing formal influence measures for multilinear methods as well as associated classification models and effective implementations. We believe that these problems will be among the most important and useful to the scientific community. Details are presented herein and an application is given in the context of facial emotion processing experiments.",2000,10.1117/12.402429,no
Advancing polymer process understanding in package and board applications through molecular modeling,"In this paper we will discuss two molecular modeling methods which have been developed and applied at Honeywell to help predict material behavior from the process engineer's standpoint. Both stress cycling and process analyses have been used to trend the probable behavior of material types in order to provide advanced intelligence on possible failure mechanisms. For instance, we have found that there are similarities in the response trends to strain and the number of cycles which suggests a link exists between the molecular-scale mechanism and engineering theories of reliability. That is, the cycling response on a molecular scale appears to be Coffin-Manson-like. On a similar scale, the molecular response of polymer chains to process stress induced either thermally or mechanically has shown to give insight to the response from actual processed parts.",2000,10.1109/ECTC.2000.853385,no
Model-based quality monitoring of batch and semi-batch processes,"In this paper, a model-based inferential quality monitoring approach for a class of batch systems is investigated. Given the appropriate model form, the batch quality monitoring problem can be reduced to the problem of state estimation for batch and semi-batch processes. Because feed upsets are often a major source of disturbance in this type of system, it is shown that estimating the initial conditions can lead to improved state estimates throughout the batch as well as improved monitoring and control of end-use quality in many cases. The approach taken in this paper is to reduce the effects of the initial uncertainty resulting from feed disturbances by using algorithms designed to perform on-line smoothing of the initial conditions. First, an Extended Kalman Filter-based fixed-point smoothing algorithm is presented and compared to a popular approach to estimating the initial conditions. Subsequently, a nonlinear optimization-based approach is introduced and analyzed. A sub-optimal on-line approximation to the optimization problem is developed and shown to be directly related to the Extended Kalman Filter-based results. Finally, some practical implementation aspects are discussed, along with simulation results from an industrially relevant example application. (C) 2000 Published by Elsevier Science Ltd. All rights reserved.",2000,10.1016/S0959-1524(99)00047-5,no
An analytical model of multiple ILD thickness variation induced by interaction of layout pattern and CMP process,"In this paper, an analytical model for chemical mechanical polishing (CMP) is described. This model relates the physical parameters of the CMP process to the in-die variation of interlayer dielectric (ILD) in multilevel metal processes. The physical parameters considered in this model include the deposited no profile, deformation of the polishing pad and the hydrodynamic pressure of slurry flow. Model parameters are adjusted based on the first LLD layer and then applied to the upper no layers. Comparison of simulated results with sample data is performed at the die level of a state-of-the-art microprocessor.",2000,10.1109/66.857937,no
Problem representation in experts and novices: Part 2. Underlying processing mechanisms,"It has been well established that experts and novices focus on different aspects of problems, with novices focusing more on surface features rather than on deep principled features of a problem. What is less clear are the mechanisms that underlie these differences in construal of problem representation. The current study, which uses an 'old/new' recognition procedure, examines expert and novice representation of arithmetic equations in which the deep relational properties (i.e., principles of commutativity and associativity) were well known to both groups. Results indicate that both novices and experts encode both surface and principled features in the same serial manner, with surface features preceding principled features for both. At the same time, only for novices and not for experts, surface features compete with deep features, thus requiring additional resources to inhibit this attentional competition.",2000,,no
A conceptual framework for understanding business processes and business process modelling,"It is increasingly common to describe organizations as sets of business processes that can be analysed and improved by approaches such as business process modelling. Successful business process modelling relies on an adequate view of the nature of business processes, but there is a surprising divergence of opinion about the nature of these processes. This paper proposes a conceptual framework to organize different views of business processes under four headings. It also aims at providing an integrated discussion of the different streams of thought, their strengths and limitations, within business process modelling. It argues that the multifaceted nature of business processes calls for pluralistic and multidisciplinary modelling approaches.",2000,10.1046/j.1365-2575.2000.00075.x,no
"Understanding how creative thinking skills, attitudes and behaviors work together: A causal process model","Managers (N = 112) from a large international consumer goods manufacturer participated in a field experiment in which they learned and applied the Simplex process of creative thinking to solve real management problems. The interrelationships among six attitudinal and behavioral skill variables learned during the training were measured to improve understanding of how these variables contribute to the process. Predicted relationships were tested and a best-fit causal model was developed. Behavioral skill in generating quantity of options was the most important variable overall: it was directly associated with behavioral skill in both generating quality options and evaluating options. The key attitudinal skill and the second most important variable overall was the preference for avoiding premature evaluation of options (deferral of judgment). The other attitude measured, the preference for active divergence, played only an indirect role in the process.",2000,,no
Logic-based extensions of stochastic process algebras for high-level performance modelling and evaluation,"Markov reward models are commonly used for the performance, dependability, and performability analysis of computer and communication systems. In most cases, the experienced modellers are supposed to handle these mathematical models in connection with the individual states of Markov chains, i.e. as a low-level specification. In previous work based on the concept of Markov reward models we integrated rewards into SPA-descriptions directly and this resulted in a homogeneous framework for the specification and analysis of modern communication systems. - Here we propose another approach for the specification of the required performance measure: we use the calculus of (temporal) logics for describing a specific behaviour of system, which is relevant to a required measure of interest. After checking the system model against these requirements the corresponding reward model is built.",2000,,no
Modeling heat transfer for optimal control problems in food processing,Mathematical modeling plays an increasingly important role in the sterilization of food products. Issues like required sterility or improved sensory quality are part of industrial research efforts involving mathematical models. In this paper we give a view into the history and future of models for heat transfer in food products. Numerical results are used to evaluate the various models which are currently used and proposed.,2000,10.1109/CCA.2000.897479,no
"Nickel-base superalloy forging for gas turbine applications: Process model, microstructural model and validation",Microstructural control is vital to ensure adequate in-service properties of aero-engine components. In this paper we present finite element (FE) based models for the forging of the nickel-base superalloys Incone(R)718 and Waspaloy. The process chosen to demonstrate the techniques developed is the cogging operation used to break down the as-cast microstructure during ingot to billet conversion prior to disc forging. Constitutive behaviour laws and microstructure evolution models have been developed from an extensive matrix of compression testing and both have been coded into the FOWGE3(R) finite element software package. The results presented show both a conventional FE analysis approach to the effect of processing parameters on microstructural evolution and the use of the microstructural model as a tool for investigation of alternative forging conditions.,2000,,no
Problems of modelling of yield stress in the on-line control of hot rolling processes,"Model of the yield stress, applicable to the on-line control of hot steel plate rolling, is described in the paper. Developed model has a physical meaning. Three methods, based on an analysis of large number of experimental data, are applied for evaluation of coefficients in the model. These methods are approximation, optimisation and artificial neural network. Yield stress was not measured directly. It was determined from the roll force measurements, using inverse calculations of Sims equation. The results for three groups of steels (two carbon-manganese steels and one niobium steel), are presented in the publication. The adaptation technique is described in the paper, as well. Application of the adaptation technique allows for immediate reaction of the system on the changes of yield stress. Experimental validation of the model confirmed its good accuracy and usefulness for the on-line control of the hot plate rolling process.",2000,,no
Model process for implementing maintainability,"Most companies lack a formal method to address maintainability during the project delivery process, yet maintenance can seriously affect project costs. To address the opportunities available to companies through the effective inclusion of maintainability concepts, the Construction Industry Institute formed the Maintainability Research Team. Using a combination of recent literature, a questionnaire survey, personal interviews, and case studies with industry professionals, the research team organized maintainability best practices into a model implementation process. Developed from the owner's perspective, the model process has two levels: corporate and project. Milestones, steps, and activities further define each level. The benefit of the model process is that it provides owners with a starting point for implementing maintainability. This paper outlines the proposed model process and describes the potential roles and benefits of maintainability on various types of projects.",2000,10.1061/(ASCE)0733-9364(2000)126:6(440),no
A new type of multicomponent phase diagrams for the evaluation of process parameters and operating conditions in non-ferrous pyrometallurgy,"New operating conditions and process parameters are often necessary in today's industrial practice of non-ferrous smelting and converting in order to accommodate various new chemical compositions of mineral charges and environmental requirements or to conceive new and more efficient industrial technologies. An important parameter in non-ferrous smelting and converting is the liquidus surface of multicomponent slags. This surface is affected by multiple factors such as temperature, gaseous potentials, the content of each component in the slag, etc., and cannot be quantified by the existing ternary liquidus diagrams. In this work a new type of representation of the liquidus surface of multicomponent systems has been presented by means of a thermodynamic model. This representation waives the need of the multidimensional representation of multicomponent systems and yields practical two-dimensional diagrams, which can be easily used in the industrial practice.",2000,,no
A process evaluation model for patient education programs for pregnant smokers,"Objective-To describe and apply a process evaluation model (PEM) for patient education programs for pregnant smokers. Methods-The preparation of a process evaluation plan required each program to define its essential ""new"" patient assessment and intervention procedures for each episode (visit) of patient-staff contact. Following specification of these core implementation procedures (p) by each patient education program, the PEM, developed by the Smoke-Free Families (SFF) National Program Office, was applied. The PEM consists of five steps: (1) definition of the eligible patient sample (a); (2) documentation of patient exposure to each procedure (b); (3) computation of procedure exposure rate (b/a = c); (4) specification of a practice performance standard for each procedure (d); (5) computation of an implementation index (c/d = e) for each procedure. The aggregate of all indexes (e) divided by the number of procedures (P-n) produced a program implementation index (PII = Sigma e/P-n). Participants and settings-Data from four SFF studies that represent different settings were used to illustrate the application of the PEM. Results-All four projects encountered moderate to significant difficulty in program implementation. As the number and complexity of procedures increased, the implementation index decreased. From initial procedures that included patient recruitment, delivery of the intervention components, and conducting patient follow ups, a variety of problems were encountered and lessons learned. Conclusion-This process evaluation provided specific insight about the difficulty of routine delivery of any new methods into diverse maternity care setting. The importance of pilot testing all procedures is emphasised. The application of the PEM to monitor program progress is recommended and revisions to improve program delivery are suggested.",2000,,no
Digestive kinematics of suspension-feeding bivalves: modeling and measuring particle-processing in the gut of Potamocorbula amurensis,"Particle digestion in lamellibranch bivalves is partitioned between 2 paths, an 'intestinal' path through the stomach and intestine and a 'glandular' path through the stomach, digestive gland and intestine. In the Asian clam Potamocorbula amurensis (Schrenck, 1867), the relative importance of the intestinal path increases compared to the glandular path as food availability and ingestion rate increases. The effects of changes in food availability and ingestion rate on digestive partitioning are at least as important as the effect of changes in diet observed by other investigators. Analyses of residence-time distributions of inert 9 and 44 mu m particle tracers show that the gut of P. amurensis can be modeled as an ideal mixing reactor (stomach and digestive gland) and an ideal plug-flow reactor (intestine) in series. This model appears to be valid for the processing of particles less than or equal to 9 pm in size. For particles of greater than or equal to 15 pm, the ideal mixing component of the model must he modified to account for channeling of particles through the stomach to the intestine. Larger particles can enter the digestive gland, but are probably not phagocytized for intracellular digestion. Instead they may clog the ducts and tubules, limiting phagocytosis of smaller particles and potentially reducing the extent of digestion and absorption. Mixing, and the resultant intragut particle-sorting thus appear to be necessary components of a digestive strategy that incorporates intracellular digestion.",2000,10.3354/meps197181,no
A generalized algorithm for modelling phase change problems in materials processing,"Phase change problems form an integral part of several manufacturing and materials processing applications. Such problems are computationally challenging because of the presence of multiple length scales and morphologically complex interfaces. In this work, we address some of those challenges through a generalized algorithm for computational modelling of solidification processes of binary alloys along with the associated transport phenomena. Different approaches for modelling phase change problems are discussed, acid a fixed grid enthalpy-based model is presented. The phase change model is integrated with a pressure-based finite volume algorithm to obtain a complete solution of alloy solidification along with the associated flow, heat and mass transfer, Special algorithms related to non-equilibrium solidification issues are also discussed.",2000,,no
Structural measures for games and process control in the branch learning model,"Process control problems can be modeled as closed recursive games. Learning strategies for such games is equivalent to the concept of learning infinite recursive branches for recursive trees. We use this branch learning model to measure the difficulty of learning and synthesizing process controllers. We also measure the difference between several process learning criteria, and their difference to controller synthesis. As measure we use the information content (i.e., the Turing degree) of the oracle which a machine needs to get the desired power. The investigated learning criteria are finite, EX-, BC-, weak BC- and on-line learning. Finite, EX- and BC-style learning are well known from inductive inference, while weak BC- and on-line learning came up with the new notion of branch (i.e., process) learning. For all considered criteria - including synthesis - we also solve the questions of their trivial degrees, their omniscient degrees and with some restrictions their inference degrees. While most of the results about finite, EX- and BC-style branch learning can be derived from inductive inference, new techniques had to be developed for on-line learning, weak BC-style learning and synthesis, and for the comparisons of all process learning criteria with the power of controller synthesis. (C) 2000 Elsevier Science B.V. All rights reserved.",2000,10.1016/S0304-3975(98)00339-9,no
Guidelines of business process modeling,"Process modeling becomes more and more an important task not only for the purpose of software engineering, but also for many other purposes besides the development of software. Therefore it is necessary to evaluate the quality of process models from different viewpoints. This is even more important as the increasing number of different end users, different purposes and the availability of different modeling techniques and modeling tools leads to a higher complexity of information models. In this paper the Guidelines of Modeling (GoM)(1), a framework to structure factors for the evaluation of process models, is presented. Exemplary, Guidelines of Modeling for workflow management and simulation are presented. Moreover, six general techniques for adjusting models to the perspectives of different types of user and purposes will be explained.",2000,,no
Integrated process modeling: An ontological evaluation,"Process modeling has gained prominence in the information systems modeling area due to its focus on business processes and its usefulness in such business improvement methodologies as Total Quality Management, Business Process Reengineering, and Workflow Management. However, process modeling techniques are not without their criticisms [13]. This paper proposes and uses the Bunge-Wand-Weber (BWW) representation model to analyze the five views - process, data, function, organization and output - provided in the Architecture of Integrated Information Systems (ARIS) popularized by Scheer [39, 40, 41]. The BWW representation model attempts to provide a theoretical base on which to evaluate and thus contribute to the improvement of information systems modeling techniques. The analysis conducted in this paper prompts some propositions. It confirms that the process view alone is not sufficient to model all the real-world constructs required. Some other symbols or views are needed to overcome these deficiencies. However, even when considering all five views in combination, problems may arise in representing all potentially required business rules, specifying the scope and boundaries of the system under consideration, and employing a ""top-down"" approach to analysis and design. Further work from this study will involve the operationalization of these propositions and their empirical testing in the field. (C) 2000 Elsevier Science Ltd. All rights reserved.",2000,10.1016/S0306-4379(00)00010-7,no
Descriptive process modeling in an industrial environment: Experience and guidelines,"Process modeling is a key activity in process improvement to understand the software process, to detect weaknesses in the process and to allow estimation. A major problem when process modeling is done in industrial environment is obtaining access to the information needed. This paper describes experience from descriptive process modeling in an industrial environment and reports problems and difficulties encountered in acquiring and formalizing that knowledge. From the experience, guidelines for descriptive process modeling activities, especially for process knowledge acquisition, are derived.",2000,,no
A process based model for analyzing deal protection measures,"Recent case law has led to debate regarding the appropriate standard of review for so-called deal protection measures (i.e., ""no shop"" clauses, stock options, termination fees and related provisions). In the authors' view, the debate is neither necessary nor productive. Since courts consistently have focused their analysis upon the process designed and executed by the board and its advisors in connection with negotiating and eventually agreeing to deal protection measures, this should also be the principal focus of lawyers called upon to advise directors concerning the discharge of their fiduciary duties in this regard. This Article contends that a process-centric model for reviewing deal protection measures not only explains the existing case law, but should also be of great utility to corporate practitioners in advising directors in merger and acquisition transactions.",2000,,no
An efficiency model for general purpose instruction level parallel architectures in image processing,"RISC instruction level parallel systems are today the most commonly used highperformance computing platform. On such systems, Image Processing and Pattern Recognition (IPPR) tasks, if not thoroughly optimized to fit each architecture, exhibit a performance level up to one order of magnitude lower than expected. In this paper we identify the sources of such behavior and we model them defining a set of indices to measure their influence. Our model allows planning program optimizations, assessing the results of such optimizations as well as evaluating the efficiency of the CPUs architectural solutions in IPPR tasks. Besides it lends itself to automatic evaluation and visualization. A case study using a combination of a specific computing intensive IPPR task and a RISC workstation is used to demonstrate these capabilities. We analyze the sources of inefficiency of the task, we plan some source level program optimizations, namely data type optimization and loop unrolling, and we assess the impact of these transformations on the task performance. We observe an eight times performance improvement and we analyze the sources of such speed-up. Finally our study allows us to conclude that, in low-intermediate level IPPR tasks, it is more difficult to efficiently exploit superscalarity than pipelining. (C) 2000 Elsevier Science Ltd. All rights reserved.",2000,10.1016/S0045-7906(99)00045-2,no
Toward generic models for comparative evaluation and process selection in rapid prototyping and manufacturing,"Selection of the most suitable rapid prototyping and manufacturing (RP&M) process for a specific part creation is a difficult task due to the proliferation of RP&M processes and materials. Multiple considerations such as good dimensional accuracy, fine surface finish, short building time, and low building cost are all desired objectives, and no one process is superior in all aspects. For a software tool to assist in making such a selection, a systematic study is first required to compare pertinent attributes among the different RP&M processes. It is also necessary to design appropriate models that can be used to characterize these attributes and that can be modified for existing and future RP&M machines. This paper studies the four dominant RP&M processes currently in the market-stereolithography (SL), selective laser sintering (SLS), fused deposition modeling (FDM), and laminated object manufacturing (LOM)-through a benchmark part. Generic models for surface roughness, building time, and building cost are also presented.",2000,,no
Safety evaluation of sons vide-processed products with respect to nonproteolytic Clostridium botulinum by use of challenge studies and predictive microbiological models,"Sixteen different types of sous vide-processed products were evaluated for safety with respect to nonproteolytic group IT Clostridium botulinum by using challenge tests with low (2.0-log-CFU/kg) and high (5.3-log-CFU/kg) inocula and two currently available predictive microbiological models, Food MicroModel (FMM) and Pathogen Modeling Program (PMP). After thermal processing, the products were stored at 4 and 8 degrees C and examined for the presence of botulinal spores and neurotoxin on the sell-by date and 7 days after the sell-by date. Most of the thermal processes were found to be inadequate for eliminating spores, even in low-inoculum samples. Only 2 of the 16 products were found to be negative for botulinal spores and neurotoxin at both sampling times, Two products at the high inoculum level showed toxigenesis during storage at 8 degrees C, one of them at the sell-by date. The predictions generated by both the FMM thermal death model and the FMM and PMP growth models were found to be inconsistent with the observed results in a majority of the challenges. The inaccurate predictions were caused by the limited number and range of the controlling factors in the models. Based on this study, it was concluded that the safety of sous vide products needs to be carefully evaluated product by product, Time-temperature combinations used in thermal treatments should be reevaluated to increase the efficiency of processing, and the use of additional antibotulinal hurdles, such as biopreservatives, should be assessed.",2000,,no
A structural model of the influence of family problems and child abuse factors on serious delinquency among youths processed at a juvenile assessment center,"The authors tested a model of the influence of arrested youths' family problem factors, including their sexual victimization and physical abuse experiences, on their drug use and frequency of involvement in index offenses. The sample involved 277 youths processed at the Hillsborough County Juvenile Assessment Center, in Tampa, Florida, who entered a NIDA funded service delivery project, called the Youth Support Project. The self-reported data we analyzed were collected during confidential in-depth, baseline interviews lasting an average of 2 hours. The hypothesized model was supported by the data overall and for the male youths, with the female youth data suggesting they use alcohol and marijuana for different reasons. The authors' findings underscore the importance of prevention and early intervention efforts involving at-risk youth and their families to reduce the prevalence and adverse consequences of these traumatic experiences.",2000,10.1300/J029v10n01_02,no
Development and validation of a process oriented catchment model based on dominating runoff generation processes,"The conceptual rainfall runoff model TAC (tracer aided catchment model) has been developed based on the experimental results of a tracer hydrological investigation for the mountainous Brugga basin (39.9 km2). The model contains an extensive physically realistic description of the runoff generation, which includes seven unit types each with characteristic dominating runoff generation processes. These processes are conceptualised by different linear and non-linear reservoir concepts. The model is applied to a period of 3.2 years with good success. In addition, the model is validated with additional information, e.g, trace; concentrations, beside to the simulated discharge. (C) 2000 Elsevier Science Ltd. All rights reserved.",2000,10.1016/S1464-1909(00)00080-0,no
Validation of process models by construction of process nets,"The major aim of this chapter is to describe an approach towards the development of techniques and tools to support the construction, validation and the verification of Petri net models of information systems and business processes. To this end, the behavior of the models is defined by partially ordered causal runs, represented by process nets. We discuss how these runs are constructed and visualized for validation purposes, and how they are analyzed. Moreover, we demonstrate how different dynamic properties can be formulated and checked by searching respective patterns in process nets.",2000,,no
Modelling of soil decontamination by sorption - evaluation of a significance of each transport process,"The method employing statistical moments has been applied in to analyse and compare two models elaborated previously for a soil decontamination by sorption of pollutants on agglomerates. On the basis of the results obtained, the evaluation of significance of each transfer process has been proposed. From the comparison of the relationships between statistical moments (the first absolute moment and the second central one) and appropriate parameters of both models direct relationships between the parameters of these models have been found. The results obtained have been discussed in terms of a wide range of the operating parameters.",2000,,no
A permutation flow-shop scheduling problem with convex models of operation processing times,"The paper is an extension of the classical permutation flow-shop scheduling problem to the case where some of the job operation processing times are convex decreasing functions of the amounts of resources (e.g., financial outlay, energy, raw material) allocated to the operations (or machines on which they are performed). Some precedence constraints among the jobs are given. For this extended permutation flow-shop problem, the objective is to find a processing order of the jobs (which will be the same on each machine) and an allocation of a constrained resource so as to minimize the duration required to complete all jobs (i.e., the makespan), A computational complexity analysis of the problem shows that the problem is NP-hard. An analysis of the structure of the optimal solutions provides some elimination properties, which are exploited in a branch-and-bound solution scheme. Three approximate algorithms, together with the results of some computational experiments conducted to test the effectiveness of the algorithms, are also presented.",2000,10.1023/A:1018943300630,no
Speculative parallel processing applied to modelling of initial problems in electrical circuits,"The paper presents an analysis of the dynamics of an asynchronous motor performed by speculative parallel processing done with the use of 41 processors. A simulation of the motor dynamics showed an important speed-up of computation, which depends on the number of parallel processes organised in the analysed time sub-intervals.",2000,10.1109/PCEE.2000.873627,no
Factors affecting the efficiency of a photocatalyzed process in aqueous metal-oxide dispersions - Prospect of distinguishing between two kinetic models,"The photooxidative degradation of phenol in aqueous TiO2 dispersions has been revisited to determine the dependencies of the rate on the concentration of phenol and on the photon flow (rho) of the actinic light at 365 nm. The principal objective was to assess the factors that influence the efficiency of the photocatalytic process, the rate of which is described by the function dC/dt(rho, C) = (const) C(n)rho(m) where n and m are orders of the reaction with respect to concentration and photon flow (light intensity), respectively. The reaction order m varies with reagent concentration C, whereas the order n depends on photon flow rho. The description indicates that m -- 1 if n --> 0, whereas n --> 1 if m --> 0. Therefore, the reaction orders m and n of phenol photodegradation are interdependent. A detailed kinetic description of the process is given based on two well-known mechanistic/kinetic models, namely (i) the Langmuir-Hinshelwood (ZH) model whereby the organic reagent is pre-adsorbed on the photocatalyst surface prior to UV illumination, and (ii) the Eley-Rideal (ER) model for which the organic reagent diffuses from the solution bulk onto the photocatalyst surface to interact with the activated state of the photocatalyst. The kinetic treatment infers that it is possible (under certain conditions) to delineate between the LH and ER mechanistic models on the basis of the magnitude of the Langmuir constant K-L at very high photon how i.e. when rho-->infinity (for the LH pathway, K-L-->K; for the ER model K-L-->0), and on the dependence of k(obs), of the process on rho. For the ER model, kobs scales linearly with p at high photon flow, whereas for the LH pathway k(obs), displays a sub-linear dependence on rho and tends toward saturation at high photon flow. (C) 2000 EIsevier Science S.A. All rights reserved.",2000,10.1016/S1010-6030(00)00225-2,no
A process-oriented model of N2O and NO emissions from forest soils 2. Sensitivity analysis and validation,"The process-oriented model PnET-N-DNDC describing biogeochemical cycling of C- and N and N-trace gas fluxes (N2O and NO) in forest ecosystems was tested for its sensitivity to changes in environmental factors (e.g., temperature, precipitation, solar radiation, atmospheric N-deposition, soil characteristics). Sensitivity analyses revealed that predicted N-cycling and N-trace gas emissions varied within measured ranges. For model validation, data sets of N-trace gas emissions from seven different temperate forest ecosystems in the United States, Denmark, Austria, and Germany were used. Simulations of N2O emissions revealed that field observations and model predictions agreed well for both flux magnitude and its seasonal pattern. Differences between predicted and measured mean N2O fluxes were <27%. An exception to this was the N-limited pine stand at Harvard Forest, where predictions of fluxes deviated by 380% from field measurements. This difference is most likely due to a missing mechanism in PnET-N-DNDC describing uptake of atmospheric N2O by soils. PnET-N-DNDC was also validated for its capability to predict NO emission from soils. Predicted and measured mean NO fluxes at three different field sites agreed within a range of +/-13%. The correlation between modeled and predicted NO emissions from the spruce and beech stand at the Hoglwald Forest was r(2) = 0.24 (spruce) and r(2) = 0.35 (beech), respectively. The results obtained from both sensitivity analyses and validations with field data sets from temperate forest soils indicate that PnET-N-DNDC can be successfully used to predict N2O and NO emissions from a broad range of temperate forest sites.",2000,10.1029/1999JD900948,no
Processing quality of rough rice during drying - modelling of head rice yield versus moisture gradients and kernel temperature,"The published literature reviews various factors responsible for rice breakage during milling. Artificial drying immediately after harvesting is one of the main causes of kernel fissures in temperate countries like France. These virtually invisible fissures lead to high breakage ratios during milling. The present work attempts to construct a simulation tool capable of predicting head rice yield during drying, so that the design of industrial rice dryers can be improved. Relationships were established between head rice yield and both moisture content gradient and kernel temperature. The influence of the temperature and evaporating capacity of the drying air on head rice yield after processing was studied. Experimental quality data were fitted to a kinetic model. Evaporating capacity was identified as the principal factor responsible for head rice yield reduction: head rice yield is not affected by high drying temperatures if evaporating capacity remains low. The kinetic model developed by the authors describes head rice yield accurately, with an error of 6%. (C) 2000 Elsevier Science Ltd. All rights reserved.",2000,10.1016/S0260-8774(00)00057-1,no
The Berkeley Project Management Process Maturity Model: Measuring the value of project management,"The purpose of the Berkeley Project Management Process Maturity Model and an associated Assessment Methodology is to help organizations and people accomplish higher and more sophisticated PM maturity by a systematic and incremental approach. It measures, locates, and compares an organization's current PM maturity level. The primary advantage of using this model and methodology is that it is generalized across industries, whereas other maturity models have specific audiences like software development or new product development. The Maturity Model and Assessment technique has already been used to benchmark PM practices and processes in 43 companies. With it, we have also identified relationships between levels of organizational effectiveness and actual project performance data. The model is continuously being refined to reflect advances in our PM knowledge. Some of the most recent improvements include evaluating Replicabiltiy of Project Success, which will be the focus of this paper and presentation.",2000,,no
The formalized models of an evaluation of a verification process of critical digital systems software,The purpose of this paper is development of the formalized models of an evaluation of a verification process of critical digital systems software and recommendations for their use in the practical applications.,2000,,no
Measurements and models in heterogeneous physicochemical processes in the atmosphere,"The Reversed Flow - Gas Chromatographic method (RF - GC), is an experimental arrangement simulating a simple model for the action of air pollutants on solids, in the laboratory scale. By abandoning the main role of the carrier gas in gas chromatography and replacing it by gaseous diffusion currents, one can measure simultaneously various rate and distribution constants, constituting the coefficients of the rate processes and the equilibrium states of the overall mechanism. The method is extended to measure directly from experimental data adsorption energies, local monolayer capacities, local adsorption isotherms, and the probability density function for the adsorption energies as distributed over experimental time. The method is applied to the adsorption of (CH3)(2)S, NO2 and O-3 on CaCO3 and SiO2. The kinetic physicochemical parameters for the absorption / desorption phenomena are also calculated for the above systems.",2000,,no
Towards modeling the query processing relevant shape complexity of 2D polygonal spatial objects,"The shape complexity of two-dimensional (2D) polygonal spatial objects has implications for how the object can be best represented in a spatial database, and for the query-processing performance of that object. Nevertheless few useful definitions of query-processing relevant spatial complexity are available, A query-processing oriented shape complexity measure is likely to be different from a fractal measure of shape complexity that focused on compression/decompression or a shape complexity measure that would be used fur pattern recognition, and should give better performance for the analysis of query processing. It could be used to classify spatial objects, cluster spatial objects in multiprocessor database systems. In a recent paper Brinkhoff et al. (T. Brinkhoff, H-P. Kriegel, R. Shneider, A. Braun, Measuring the complexity of spatial objects, Proceedings of the 3rd ACM International Workshop on Advances in Geographic Information Systems, Baltimore, MD, 1995, pp. 109-117) demonstrated the usefulness of a spatial complexity measure. They did not however, offer much theoretical justification for their choice of parameters nor for the functional form that they used. In this paper we present a conceptual framework for discussing the query processing oriented shape complexity measures for spatial objects. It is hoped that this will lead to the development of improved measures of spatial complexity. (C) 2000 Elsevier Science B.V. All rights reserved.",2000,,no
Understanding runoff processes using a watershed model - a case study in the Western Ghats in South India,"The wet tropical Western Ghat Mountain ranges in South India present an interesting combination of meteorological and physical characteristics. The results of a watershed model analysis carried out to understand the catchment response and the relative importance of different runoff processes in the region are reported in this paper. A lumped parameter model simulating saturated source area runoff, lateral flow through pipes and the saturated zone groundwater flow, has been developed assuming that source area runoff is the only quickflow component. The model has been calibrated on seven catchments using sufficiently long records of daily data. A wide range of tests has been used to show that the model performs reliably. The influence of catchment characteristics on the relative importance of the flow components and the catchment response has been studied. The model simulations have been interpreted to infer that the pipeflow contributions augment the contributions of source area runoff to stream quickflow. Suggestions for further research in the area are given, based on the inferences drawn. (C) 2000 Elsevier Science B.V. All rights reserved.",2000,10.1016/S0022-1694(00)00141-4,no
The curing of powder coatings using gaseous infrared heaters: An analytical model to assess the process thermal efficiency,"This experimental investigation is structured to determine the efficiency of energy transfer between a gaseous infrared burner and a powder coated surface being cured. The results from the experimental burner tests are interpreted using a Monte Carte based numerical simulation. This simulation incorporates both an Edwards' exponential wide band model for the spectrally selective energy absorption and emission in the flue gas, and test measurements for the powders' spectral absorption characteristics. The results from the simulation show that efficiency of the energy transfer from the natural gas stream to the powdered surface ranges between 25 % and 30 %, when the burner is operating at stoichiometric conditions. The highest efficiency was obtained for the darkest coloured powder. The energy efficiency was also found to increase with burner temperature, However, the quality of the cured surface tended to decrease if the temperature rise was significant. (C) 2000 Editions scientifiques et medicales Elsevier SAS.",2000,10.1016/S1290-0729(00)00272-6,no
Hidden-Markov modeling for mean-shift detection of fraction defective in production process control,"This paper considers a statistical method of estimating mean-shift for a fraction defective of population. One traditional method for this estimation problem has been known as a CUSUM (cumulative sum) chart method. We consider this estimation problem of shift-occurrence in a production process. It is assumed that the process has two states, i.e., one is good (fraction defective is low) and the other bad (high), and starts at the good state with probability one. We are interested in judging when the state has moved to the bad state by only analyzing the observed data. In this paper, we model such a phenomenon as a hidden-Markov model and propose the estimation method by using the forward-backward procedure and the Baum-Welch re-estimation formulas. We compare the performance of this hidden-Markov model with the CUSUM chart method based on two types of simulation data sets.",2000,,no
Experimental evaluation of bilinear model predictive control for pH neutralization processes,"This paper deals with the improvement experimental evaluation of bilinear model predictive control applied to the pH neutralization process. A bilinear model for the pH neutralization process is developed, and a one-step-ahead predictive control law based on the bilinear model is designed without inclusion of an offset compensator. The performance of the proposed control system was studied both numerically and experimentally. The good control performance obtained verifies the effectiveness of the proposed adaptive bilinear model predictive control method in the control of nonlinear chemical processes.",2000,10.1252/jcej.33.285,no
An evaluation model for determining the business process benefits of thixoforming,"This paper describes the development of a financial model for evaluating the benefits of introducing a new technology. Thixoforming is a semi-solid metal processing technique that can offer potential users a number of technical and economic advantages. This process has been commercialized for a number of products, particularly in the United States and Italy, but there is significant scope for its further exploitation, especially within UK manufacturing. One of the major obstacles to the further adoption of this technique is the ability of potential users to evaluate and quantify the benefits offered. The model developed adopts a business process perspective to this and demonstrates how 'intangible' benefits can be quantified.",2000,,no
Emission inventory development and processing for the Seasonal Model for Regional Air Quality (SMRAQ) project,"This paper describes the experiences and insights gained from inventory preparation and emissions processing for the Seasonal Model for Regional Air Quality (SMRAQ) project. The emission inventory was derived from the 1990 and 1995 Ozone Transport Assessment Group (OTAG) inventories. Here we outline the emissions processing strategy used for the May-to-September simulation, summarize the inventory characteristics and corrections made on the OTAG inventories, and describe the quality assurance steps taken as part of the processing. We then provide spatial maps and daily total time series charts of the hourly, gridded emissions of nitrogen oxides (NOx), reactive organic gases (ROG), and carbon monoxide (CO). Large peaks from electric utility point sources and urban mobile sources characterize the NOx emissions, and the NOx emissions in nonpeak regions are primarily mobile-source emissions. ROG emissions are dominated by biogenic isoprene production in the southern United States, and they have a strong seasonal variability. CO emissions are characterized by less variability, with area and mobile sources dominating the inventory. We compare ratios of season-average nonmethane organic gases to NOx between the emission inventory and the Photochemical Assessment Monitoring Stations (PAMS) data, and these comparisons show poor correlation between the inventory and ambient ratios.",2000,10.1029/1999JD900975,no
"Impact of investing in quality improvement on (Q, r, L) model involving the imperfect production process","This paper investigates the impact of quality improvement on the modified lot size reorder point models involving variable lead time and partial backorders. The formulated models include the imperfect production process and an investing option of improving the process quality. The objective is simultaneously optimizing the lot size, reorder point, process quality level and lead time. We first assume that the lead time demand follows a normal distribution, then relax this assumption to consider the distribution-free case where only the mean and standard deviation of lead time demand are known. An algorithm procedure of finding the optimal solution is developed, and two numerical examples are given to illustrate the results.",2000,10.1080/095372800414160,no
Quality improvement of chemical-mechanical wafer planarization process in semiconductor manufacturing using a combined generalized linear modelling-non-linear programming approach,"This paper is aimed at how to develop and utilize a specialized response surface method, combined with state-of-the-art mathematical programming techniques, for quality improvements of the chemical-mechanical planarization (CMP) process in semiconductor manufacturing. CMP is one of the fastest growing technologies that enables to polish the topography of interlayer dielectrics (ILDs) and to obtain a high degree of global planarity due to increasingly stringent requirements of photolithography between process steps. A wafer held on a carrier is rotated against a polishing pad in the presence of a silica-based alkaline slurry while applying a down-force onto it. Two major challenging works posed by CMP involve maintaining stable removal rate with polishing time and achieving acceptable within-wafer non-uniformity (WIWNU) over an entire die. In this research, to robustly characterize and therefore optimize such a still unclear and fully complex process, the response surface methodology (RSM) as an external modelling technique and non-linear programming (NLP) approaches as an optimum-seeking procedure are proposed to the bicriteria situation. An example with real CMP data is rigorously investigated, revealing that not only does the proposed method flexibly and appropriately portray CMP, but also helps locate the optimal parameter settings that attain better polishing quality.",2000,10.1080/00207540050117413,no
An initial construction of a mathematical model to describe the behaviour of a wavesolder process prior to the application of an online quality assurance sysyem,"This paper is written as part of a joint research project into an on-line computer integrated quality assurance system. The objective of this project is to provide real-time preventative quality control by automatically quantifying attributes and variables within a Statistical Process Control (SPC) environment, involving the integration of quality control and process / machine monitoring (McLachlan, 1998).",2000,,no
A stochastic process-based model for the positional error of line segments in GIS,This paper presents a model for describing the positional error of line segments in geographical information systems (GIS). The model is based on stochastic process theory with the assumptions that the errors of the endpoints of a line segment follow two-dimensional normal distributions. The distribution and density functions of the line segments are derived statistically. The uncertainty information matrix of line segments is derived to indicate the error of an arbitrary point on the line segment. This model covers the cases where two-end points are correlated to each other and points on the line segment are stochastically continuous to each other. The model is a more generic error band model than those previously developed and is called the G-Band model.,2000,10.1080/136588100240958,no
A practical application of recent results in model and controller validation to a ferrosilicon production process,"This paper presents the application of our recently developed theory on model validation for control and controller validation in a Prediction Error framework to a realistic industrial case study. The industrial application, taken from [1], concerns the control of the silicon concentration in a ferrosilicon production process. Our case study produces findings about the design of the validation experiment (validation in open or closed loop). It also illustrates the respective merits of the tools developed, respectively, for control-oriented model validation and for the validation of a particular controller.",2000,,no
Family Assessment Measure (FAM) and Process Model of Family Functioning,"This paper provides an overview of twenty years' work in the development of the Family Assessment Measure (FAM), based on the Process Model of Family Functioning. The Process Model describes a conceptual framework for conducting family assessments according to seven key dimensions: task accomplishment, role performance, communication, affective expression, involvement, control, values and norms. The FAM provides measures of these dimensions at three levels: whole family system (general scale, fifty items), various dyadic relationships (dyadic scale, forty-two items) and individual functioning (self-rating scale, forty-two items). In addition, the general scale includes social desirability and defensiveness response style measures. Brief FAMs (fourteen items) are available for each scale as well. The measurement properties of FAM have been evaluated in a variety of clinical and non-clinical settings. Reliability estimates are very good in most contexts. FAM's validity has been supported using a number of techniques. Overall, the weight of the evidence is that FAM's effectively and efficiently assess family functioning and provide strong explanatory and predictive utility. This empirical evidence reinforces experiences of clinicians, indicating that FAM provides a rich source of information on family functioning.",2000,10.1111/1467-6427.00146,no
Modelling and validation of contributions to stress in the Shallow Trench Isolation process sequence,"This work is based upon a careful rendering of mechanics and mathematics to describe the phenomena that influence the stress engendered by the Shallow Trench Isolation process. The diffusion-reaction problem is posed in terms of fundamental mass balance laws. Finite strain kinematics is invoked to model the large expansion of SiO2, dielectrics are modelled as viscoelastic solids and annealing-induced density relaxation of SiO2 is incorporated as a history-dependent process. A levelset framework is used to describe the moving Si/SiO2 interface. Sophisticated finite element methods are employed to solve the mathematical equations posed for each phenomenon. These include the incorporation of discontinuity-resolving shape functions to describe jumps in concentration of O-2, methods to prevent oscillations of numerical solutions and techniques that allow highly inhomogeneous deformation of a single element. The use of experimental data to rigorously obtain material properties is emphasized. Mechanical properties of viscoelastic solids are extracted directly from stress-strain data, following which, parameters for the diffusion-reaction problem are obtained. Qualitative and quantitative validation of the medals is presented; the latter by comparison with micro-Raman spectroscopy measurements.",2000,,no
A unified analytical model and experimental validations of injection-locking processes,"Unified analytical expressions predicting the locking range for fundamental, subharmonic (n = 2), superharmonic (n = 2), and parametric injection(m = n = p = 1) locking are presented and compared in this paper. Power series are employed to model the device nonlinearity, The alpha-parameters, relating nonlinear I-V behavior, are extracted using a harmonic-balance approach. These expressions are verified using an UHF oscillator; and good agreement is obtained between the experimental and analytical results.",2000,10.1109/22.842019,no
Application of volume growth and survival graphs in the evaluation of four process-based forest growth models,"Volume growth and survival (VGS) graphs, which show volume growth rate and risk of mortality for individual trees (or tree size classes), have been proposed as a tool for assessing the validity of models that describe the development over time of tree size distributions within forest stands. We examined the utility of the VGS method in evaluating four process-based models. The performance of the models FORSKA, 4C, MORG, and PipeQual is analyzed against long-term data from a Scots pine stand in Eve, Finland, and the models FORSKA and 4C are also assessed with respect to data from a beech stand in Fabrikschleichach, Germany. Comparison of the measurement-based VGS graphs with those produced from the model-based data shows that although the models yield similar stand-level predictions, they can differ widely in their projections of individual tree growth and size distributions. Examination of the discrepancies between models and data in the context of the VGS graphs reveals several areas in which the models could be improved. We conclude that the method is useful in model evaluation, especially if used in combination with indicators of stand structure, such as the height/diameter ratio.",2000,,no
Image processing through multiscale analysis and measurement noise modeling,"We describe a range of powerful multiscale analysis methods. We also focus on the pivotal issue of measurement noise in the physical sciences. From multiscale analysis and noise modeling, we develop a comprehensive methodology for data analysis of 2D images, 1D signals (or spectra), and point pattern data. Noise modeling is based on the following: (i) multiscale transforms, including wavelet transforms; (ii) a data structure termed the multiresolution support; and (iii) multiple scale significance testing. The latter two aspects serve to characterize signal with respect to noise. The data analysis objectives we deal with include noise filtering and scale decomposition for visualization or feature detection.",2000,10.1023/A:1008938224840,no
The effects of development process modeling and task uncertainty on development quality performance,"We examine the impact of development process modeling on outcomes in software development projects, limiting our attention to process and product quality. Modeling the software development process requires a careful determination of tasks and their logical relationships. Essentially, the modeling is undertaken to establish a management framework of the project. We define and interrelate development process modeling, task uncertainty, and development outcomes, as assessed by product and process quality. A survey-based research design was used to collect data to prove our model. The results suggest that development process modeling is positively related to both product and process quality, while task uncertainty is negatively related to them. Development process modeling reduces the negative impact of task uncertainty on quality-oriented development outcomes. Development projects operating with high levels of task uncertainty should consider defining development process models that provide a framework for management of the project by establishing tasks and their logical interrelationships. Such a model should promote shared understanding of the work process among development constituents and enhance resource utilization efficiency. (C) 2000 Elsevier Science B.V. All rights reserved.",2000,10.1016/S0378-7206(00)00047-1,no
On strongly degenerate convection-diffusion problems modeling sedimentation-consolidation processes,"We investigate initial-boundary value problems for a quasilinear strongly degenerate convection-diffusion equation with a discontinuous diffusion coefficient. These problems come from the mathematical modeling of certain sedimentation-consolidation processes. The existence of entropy solutions belonging to BY is shown by the vanishing viscosity method. The existence proof for one of the models includes a new regularity result for the integrated diffusion coefficient. New uniqueness proofs for entropy solutions are also presented. These proofs rely on a recent extension to second-order equations of Kruzkov's method of ""doubling the variables."" The application to a sedimentation-consolidation model is illustrated by two numerical examples. (C) 2000 Academic Press.",2000,10.1006/jmaa.2000.6872,no
Volume growth and survival graphs: a method for evaluating process-based forest growth models,"We investigated the relationships within forest stands between tree size and (a) stem volume growth rate and (b) risk of mortality for individual trees. Values of both x and y variables were plotted relative to the the largest value in the stand. We refer to the resultant presentations as relative volume growth and relative survival graphs (VGSs). A pair of VGSs can be produced readily from an individual-tree growth model. It can also be constructed from consecutive sets of field measurements. Comparison of VGSs derived from model and measurement data provides a test of the validity of the components of the growth model. We have analyzed VGSs based on measurement data for Scots pine (Pinus sylvestris L.) in central Finland and for beech (Fagus sylvatica L.) in southern Germany. The graphs based on measurement data varied as a consequence of differences in competition, stand management, and tree species. We analyzed the relationship between VGSs and stand dynamics using a simple growth model. We found that different features of the VGSs imply characteristic tree size distributions in subsequent years. Thus, we conclude that if the VGSs generated by a model do not correspond to those based on field measurements, the model cannot be relied on to reproduce the development of tree-size distribution correctly. Relative growth and survival graphs thus provide a tool for evaluating complicated growth models.",2000,,no
Enhancing requirements and change management through process modelling and measurement,"We present a methodology that aims at improving the effectiveness of requirements management in software developmental and maintenance. In particular, we address quantitative assessment of the impact of requirements changes, and quantitative estimation of costs of the development activities that must be carried our to accomplish those changes. Our approach is based on enhanced traceability and process measurement. Traceability relations are derived from an integrated view of the process and product models adopted within the software development organization. Hence, traceability: spans over all the software life cycle and its products. Moreover, through proper measurement, the elements in the process and product models are quantitatively characterised, thus achieving a sound basis for different kinds of sophisticated analysis concerning the impact of requirements changes, their cost over the development process, and the sensitivity of the product to changes.",2000,10.1109/ICRE.2000.855597,no
Guidelines for developing assistive components for information appliances - Developing a framework for a process model,"A framework for a process model that leads the developer of assistive components for information appliances is being presented. This process model includes methods and tools for user, task and context analysis and usability testing and guidelines and tools for working with these guidelines. With these components the process model will offer specific support for the developer for all phases of analysis and design of assistive systems.",2001,,no
"Snow interception evaporation. Review of measurement techniques, processes, and models","A global warming, primarily affecting wintertime conditions at high latitudes will influence the functioning of the boreal forest. The least known term of the winter water-balance equation is evaporation of snow intercepted in forest canopies. Several investigations stress the importance of snow-interception evaporation in coniferous forests and evaporation fractions of gross precipitation as large as 0.2-0.5 have been observed by investigators in Scotland, Canada, and Japan. Evaporation rates as high as 0.56 mm h(-1) are reported. The largest differences between the rain and snow interception evaporation processes are the differences in Storage. Snow storage (both mass and duration) is often an order of magnitude larger than that for rain. Snow interception changes the canopy albedo although some studies indicate the opposite. Process knowledge is limited because of measurement difficulties but it is known that canopy closure, aerodynamic resistance (r(a)), and vapour-pressure deficit are important factors. Existing formulations of r(a) as function of storage location and age cannot fully explain observed differences in evaporation rates. Operational hydrology and weather models, and GCMs describe snow interception in a very simplified way and might benefit from incorporation of more realistic schemes.",2001,10.1007/s007040170010,no
Construction of a structural-parametric diagnostic model from the results of processing measurement information,"A justification is given of an approach in which it is proposed to use diagnostic information concerning the structural characteristics of an object, in the form of a structural-parametric mathematical model, together with data concerning the results of measurements of its important parameters in order to determine the technical state of the diagnosed object.",2001,10.1023/A:1014053032058,no
The antioxidant activity of tomato. I. Evaluation of fresh and processed products by chemical-physical indexes and biochemical model systems through principal component analysis,"A methodology was developed to study the antioxidant content and activity of tomato products differing in variety and processing. Eleven tomato samples (in duplicate) were assessed for dry matter, color, antioxidant content [e.g., ascorbic acid, total phenolics, and carotenoids (lycopene, beta-carotene, lutein)], and antioxidant activity of the hydrophilic and lipophilic fractions. The antioxidant activity was measured by the following modeling systems: (a) xanthine oxidase (XOD)/xanthine system, (b) myeloperoxidase (MPO)/NaCl/H2O2 system, and (c) linoleic acid/CuSO4 system. These modeling systems simulate the oxidative reactions occurring during the initiation and progression of human disease, thus allowing us to evaluate the potential inhibitory role of tomato products, The Principal Components Analysis (PCA) was used to assess differences between samples. Results showed that all the variables measured significantly explained the differences between samples, except for the antioxidant activity as measured by the MPO/NaCl/H2O2 system, which was thus removed from the statistical model. The model was then found to be appropriate as the first two components explained a fraction of total variance of the samples equal to 78%. The score plot showed that samples covered a wide area within the first two components, which reflected the variability of the products available on the market, In particular, it was observed that fresh samples were clearly distinguished from processed samples. In the first component (60% explained variance) fresh tomatoes were separated from tomato pulps and tomato pastes, whereas in the second component (18% explained variance) contributed to separation of tomato pulps from the other tomato products. Tomato purees were located closely to either the tomato pastes or the tomato pulps. By overlapping the score plot and the loading plot it was observed that fresh samples were mainly described by high ascorbic acid content, lutein content, and antioxidant activity as measured by the XOD/xanthine system. Tomato pastes were described by high values for lycopene, total phenolics, color, and antioxidant activity as measured by the linoleic acid/CuSO4 System. In conclusion, PCA showed that the variables chosen could discriminate samples according to variety and processing.",2001,,no
Modelling the stable effluent qualities of the A2O process with activated sludge model 2d under different return supernatant,"A modified mathematical model based on the Activated Sludge Model No. 2d (ASM2d) was established to describe the effluent qualities of the enhanced biological phosphate removal (EBPR) system A20. There were three modifications to the model in this study: (1) the biosorption effect of the soluble GOD, (2) the hydrolysis of the organic nitrogen in influent wastewater, and (3) the growth of heterotrophic organisms in the anaerobic tank. The A2O process is composed of an anaerobic/anoxic/oxide process which removes biological phosphorus with simultaneous nitrification-denitrification. The influent wastewater quality and quantity were fixed, the ratios of return sludge and sludge retention time were 0.25 and 12 days, and the model plant was operated at three different mixed liquid recycling ratios (MLRR, 0.5, 1.25, 2). When a steady state was reached, comparisons between the measured values and predicted values were made for each test. A good consistency between the test values and simulation values was shown. According to our results, the biosorption effect of the soluble COD and the hydrolysis of the organic nitrogen in influent wastewater are the important qualities in activated sludge systems. Additionally, heterotrophic organisms might grow in the anaerobic tank. Furthermore, it was indirectly proved that the denitrifying PAOs existed in the EBPR system.",2001,10.1080/02533839.2001.9670608,no
Evaluating a split processing model of visual word recognition: effects of word length,"A new theory of visual word recognition is based on the fact that the fovea is split in humans. When a reader fixates the center of a written word, the initial letters of the word that are to the left of fixation are projected first to the right cerebral hemisphere (RH) while the final letters are projected to the left cerebral hemisphere (LH). This paper explores the possibility that this has consequences for the early processing of the beginning and ends of centrally fixated words: specifically that lexical decision RTs are affected by the number of letters to the left of fixation but not by the number of letters to the right of fixation. For centrally presented five- and eight-letter words, we manipulated number of letters presented to the right or to the left of a fixation point (Experiment 1). We found that longer latencies to longer letter strings characterised the processing of the initial letters of words while LH word recognition features characterised the ends of words. Experiment 2 was a lateralized version of Experiment 1, and revealed the well established visual field and word length interaction. The results supported the split fovea theory. (C) 2001 Elsevier Science BM All rights reserved.",2001,10.1016/S0926-6410(01)00056-8,no
Modeling superconducting components based on the fabrication process and layout dimensions,"A procedure is described where the mask dimensions of superconducting components are used in SPICE simulations to predict the performance of a device. Thickness tolerances of the fabrication process, as well as mask bias offsets and mask tolerances are included in the component models. This makes it possible to predict circuit yield by Monte Carlo analyses, which are based on the fabrication process design rules. Model descriptions of all the components used in a standard superconductor multilayer process are given, and the accuracy of the models are verified. The superconducting components that are modeled include microstrip transmission lines, coupled transmission lines, resistors and Josephson junctions. The usefulness of this procedure is demonstrated by simulation results of a stack amplifier, where the yield of the circuit is calculated for the lumped element model and the proposed model, which includes parasitic elements.",2001,10.1109/77.919353,no
A thermodynamic model of nickel smelting and direct high-grade nickel matte smelting processes: Part I. Model development and validation,"A thermodynamic model has been developed to predict the distribution behavior of Ni, Cu, Co, Fe, S, As, Sb, and Bi in the Outokumpu flash-smelting process, the Outokumpu direct high-grade matte smelting process, and the INCO flash-smelting process. In this model, as many as 16 elements (Ni, Cu, Co, Fe, As, Sb, Bi, S, O, Al, Ca, Mg, Si, N, C, and H) are considered, and two nickel sulfide species are used to allow for modeling of sulfur-deficient mattes. The compositions of the matte, slag, and gaseous phases in equilibrium are calculated using Gibbs free energies of formation and the activity coefficients of the components derived from the experimental data. The model predictions are compared with the known industrial data from the Kalgoorlie Nickel Smelter (Kalgoorlie, Australia), the Outokumpu Harjavalta Nickel Smelter (Harjavalta, Finland), the INCO Metals Company (Sudbury, Canada), and from a number of experimental data. An excellent agreement is obtained. It was found that the distribution behaviors of Ni, Co, Cu, Fe, S, As, Sb, and Bi in the nickel smelting furnace depend on process parameters such as the smelting temperature, matte grade, and partial pressure of oxygen in the process.",2001,10.1007/s11663-001-0057-z,no
On validation of the modeling of overcharge process in VRLA cells,"Advanced battery modeling and simulation provide a unique and powerful tool for understanding battery performance and cycle characteristics. Our recent development in advanced battery simulation capability using computational fluid dynamics (CFD) techniques proved that this approach could have significant impacts on almost every aspect of battery design, characterization, operation and development. Good predictions from the simulation however strongly rely on careful validation of the numerical model and the parameters used. This paper will discuss how experimental validation work was conducted to support the simulation effort in valve-regulated lead-acid (VRLA) batteries. Particularly interested is the overcharging regime, where the experimental work is the most difficult and challenging, and the validation is the most needed for careful evaluation of the modeling capability and limitation.",2001,10.1109/BCAA.2001.905122,no
What? Another form? The process of measuring and comparing service utilization in a community mental health program model,"Although Assertive Community Treatment (ACT) is one of the most widely studied of all community mental health treatment models, the process through which it produces outcomes is often treated as a 'black box'. There is limited understanding of the model's essential elements and studies seldom describe program implementation, As a result, though localities may be interested in developing an ACT team, information necessary for implementation of the model (e.g. costing, cost-effectiveness, ACT service utilization) is inadequate to help plan services. Part of this gap in the literature can be attributed to the fact that the vital pieces of information needed to produce the much needed estimates are generally the most arduous to gather; they require daily recording of activities and hence, the participation of already overburdened program staff. The purpose of this paper is to describe our experiences in developing a multi-program economic evaluation and costing study of ACT. In discussing the process that we followed, we hope to pass on useful information that will help produce effective and efficient mental health evaluations in the future. Our project offers an example of how the worlds of research and service delivery can collaborate to come to symbiotic resolutions. Together, we have been able to collect data that is not only valuable for program evaluation but beneficial for administrative purposes to define priority areas, staffing and service planning. (C) 2001 Elsevier Science Ltd. All rights reserved.",2001,10.1016/S0149-7189(01)00023-4,no
Model evaluation for an industrial process of direct chlorination of ethylene in a bubble-column reactor with external recirculation loop,"An industrial process for the direct chlorination of ethylene at low temperatures to yield 1,2-dichloroethane has been considered. The reactor, whose configuration corresponds to a bubble column with external recirculation loop, has a working capacity of 19 m(3) and a production capacity of 54 000 tons of 1,2-dichloroethane per year. In order to describe its operation, the predictions from a model developed in a previous work were evaluated using a special analysis of experimental data. This enables to obtain both a model and a numerical simulation program which not only predict various performance variables with satisfactory relative errors but also have the capacity of representing the influence of changes in operating conditions adequately within the entire technically feasible operating range. (C) 2001 Elsevier Science Ltd. All rights reserved.",2001,10.1016/S0009-2509(00)00255-4,no
Prediction of quality changes during osmo-convective drying of blueberries using neural network models for process optimization,"Artificial neural network (ANN) models were used for predicting quality changes during osmo-convective drying of blueberries for process optimization. Osmotic drying usually involves treatment of fruits in an osmotic solution of predetermined concentration, temperature and time, and generally affects several associated quality factors such as color, texture, rehydration ratio as well as the finish drying time in a subsequent drier (usually air drying). multi-layer neural network models with 3 inputs (concentration, osmotic temperature and contact time) were developed to predict 5 outputs: air drying time, color, texture, and rehydration ratio as well as a defined comprehensive index. The optimal configuration of neural network model was obtained by varying the main parameters of ANN: transfer function, learning rule, number of neurons and layers, and learning runs. The predictability of ANN models was compared with that of multiple regression models, confirming that ANN models had much better performance than conventional mathematical models. The prediction matrices and corresponding response curves for main processing properties under various osmotic dehydration conditions were used for searching the optimal processing conditions. The results indicated that it is feasible to use ANN for prediction and optimization of osmo-convective drying for blueberries.",2001,10.1081/DRT-100103931,no
Intelligent modeling and optimization of process operations using neural networks and genetic algorithms: Recent advances and industrial validation,"Artificial Neural Networks (ANN) have been used as black-box models for many systems during the past years. Specifically, neural networks have been used advantageously in the Chemical Processing Industries (CPI) in a number of ways. Successful applications reported range from enhanced productivity by kinetic modeling, to improved product quality, and the development of models for market forecasting. Typically, a main objective in ANN modeling is to accurately predict steady-state or dynamic process behavior to monitor and improve process performance. Furthermore, they also can help in process fault diagnosis. The black-box character of neural net models can be enriched by available mathematical knowledge. This approach has been extended to consider nonlinear time-variant processes. The potential of neural network technology faces rewarding challenges in two key areas: evolutionary modeling and process optimization including qualitative analysis and reasoning. Recent work indicates that evolutionary optimization of non-linear time-dependent processes can be satisfactorily achieved by combining neural network models with genetic algorithms. Industrial validation studies indicate that present solutions point to the right direction, but additional effort is required to consolidate and generalize the results obtained.",2001,10.1142/9781848161467_0015,no
Modelling the laser fusion cutting process: III. Effects of various process parameters on cut kerf quality,"Based on the models for determining the geometry of the cutting front and the gas flow distribution inside the cut kerf of the laser fusion cutting process, the effects of the inlet stagnation pressure of the nozzle, the exit diameter of the supersonic nozzle, laser power, cutting speed and focal position upon the geometry of the cutting front and the flow field distribution which subsequently affect the cut edge quality are analysed in this paper. The theoretical predictions are verified by practical laser fusion cutting experiments using a high-power CO2 laser, a supersonic nozzle and a high-pressure cut-assisted gas in the range of 5 bar and above. The theoretical predictions are used to explain the roughness, the ripple directions and the amount of dross on the experimental cut edges.",2001,10.1088/0022-3727/34/14/310,no
Principal surface model based quality control approach for batch/semi-batch processes,"Batch/semi-batch quality control has attracted much attention from chemical engineering researchers recently. In this work, a novel empirical model based quality control approach for batch processes is developed. The model implemented is based on independent nonlinear principal components obtained by means of principal surface analysis- a very hot topic in the area of statistical modeling. The partial initial conditions and intermediate measurements obtained from each batch are analyzed using this approach to extract the independent nonlinear principal components. An empirical model with the nonlinear principal components as inputs is built to predict product quality. Correcting actions based on the initial conditions and intermediate measurements are taken based on the predictions obtained using the empirical model. In this work, the problem formulation and solution method are presented. The illustrative examples include a simulation example and an experimental study on the batch reactive distillation process. The simulation and experimental results show that the proposed approach is highly promising.",2001,,no
The benefits of a notification process in addressing the worsening computer virus problem: Results of a survey and a simulation model,"Computer viruses present an increasing risk to the integrity of information systems and the functions of a modern business enterprise. Systematic study of this problem can yield better indicators of the impact of computer viruses as well as a better understanding of strategies to reduce that impact. We conducted a Computer Virus Epidemiology Survey (CV-ES) on the World Wide Web to examine indicators of the impact of computer viruses. A major finding from the CVES is that multiple indicators of the impact of computer viruses reveal a problem growing more severe that affects large, as well as small, organizations. Another important finding is that apparently undetectable viruses caused only about 15% to 21% of problems reported in workgroups using antiviral soft-ware, leaving a substantial amount of damage due to viruses that were probably detectable. Another important finding is that viruses not detected despite regular updating of antiviral software caused only about 15% to 21% of virus problems reported in workgroups using antiviral software. The possible reasons for failure to detect include improper configuration of software and the inability of all known antivirus detectors to detect. A related implication is that a substantial amount of damage due to viruses could probably have been prevented by regular updating of antiviral software. We also used the CVES in the development of a simulation model for the spread of computer viruses in workgroups in order to analyze the effect of a notification process oil control. Our major finding is that the process of notification, whether by human behaviour or by technology, substantially reduces the impact of computer viruses in workgroups. For example, if a workgroup has a period of vulnerability when only 80% of its workstations are effectively using antiviral software, then even a 50% probability of notification of a detected virus substantially reduces the burden. Ali added benefit of maintaining an environment with high effective antiviral software usage and high levels of notification is that greater rates of communication rates events that can potentially transmit computer viruses within the workgroup actually become protective reduce the impact of computer viruses in the workgroup. Anecdotal observations also indicate that the process of notification is significant in controlling the spread of ""new"" viruses not yet detectable by software, although the process of notification from law enforcement authorities to workgroups was not in the simulation model. More formally, the reduced impact of computer viruses in a workgroup due to a protective effect of greater rate of communication events that can potentially transmit computer viruses corresponds to a situation when a computer virus introduced into the workgroup produces, on average, less than one copy in the workgroup. This threshold corresponds to the basic reproduction ratio in epidemiology that describes the spread of infectious disease.",2001,10.1016/S0167-4048(01)00812-4,no
"Trajectories in phase diagrams, growth processes, and computational complexity: How search algorithms solve the 3-satisfiability problem","Decision and optimization problems typically fall into one of two categories fur any particular solving algorithm. The problem is either solved quickly (easy) or demands an impractically long computational effort (hard). Here we show that some characteristic parameters of problems can be tracked Juring a run of the algorithm defining a trajectory through the parameter space. Focusing on 3-satisfiability, a recognized representative of hard problems, we analyze trajectories generated by search algorithms. These trajectories can cross well-defined phases, corresponding to domains of easy or hard instances, and allow one to successfully predict the times of resolution.",2001,10.1103/PhysRevLett.86.1654,no
Application of the electrical conductivity of concentrated electrolyte solutions to industrial process control and design: from experimental measurement towards prediction through modelling,"Different aspects of the conductivity of concentrated electrolyte solutions and their application to design and control of chemical processes are discussed and reviewed, with particular focus on the work of our research group during the last few years. An automated system based on the electrodeless conductivity technique for the measurement of the electrical conductivity of solutions as the basis of a titration strategy is described. The inadequacy of classical calibration methods when high conductivities are to be measured is highlighted and an alternative methodology is proposed. Our measurements on specific concentrated systems of interest in industrial applications and the development of general methodologies to treat experimental conductivity data at constant or varying temperature in order to get comprehensive inter- and extrapolation mathematical models are summarised. Theoretical approaches to explain the variability of conductivity data of electrolyte solutions with the solute concentration as well as the deviations from ideality found at higher concentrations are reviewed and critically evaluated. Solutions of mixed electrolytes are treated in a similar fashion. (C) 2001 Elsevier Science B.V. All rights reserved.",2001,10.1016/S0165-9936(00)00081-9,no
Applying CFD modelling in order to enhance water treatment reactors efficiency: example of the ozonation process,"Disinfection/oxidation processes area major step in the water treatment train. As high levels of inactivation or removal of pathogens and micropolluants are aimed at the design and optimized operating conditions are crucial issues for the reliability of the treatment and cost control. In this context, using computational fluid dynamics allows better evaluation of the efficiency of the treatment and guarantees its performance prior to full-scale design. Examples are given in this paper of the capability of the numerical tool in order to propose an optimized solution for a refurbishment or a new plant design. The full-scale realization confirmed the predicted quality benefit.",2001,,no
Modelling business processes with workflow systems: an evaluation of alternative approaches,"Effective business process management necessitates a consistent information flow between the participants in the process, the smooth integration of the flow of work, the timely sharing of data and information during the planning and implementation phases and harmonious support for the collaborative aspects of work. The recent trends in the development of advanced workflow management sq stems and technologies seem to be of crucial importance for facilitating these tasks within the process management context. However, workflowmanagement systems (WfMS) follow various approaces in modelling the flow of work and hence present varying functionalities when supporting enterprise processes, The present paper examines the ways in which workflow technology may facilitate the implementation of process management, reviews the pros and cons of adopting alternative workflow representation techniques in modelling business processes and provides guidance to managers as to the characteristics, the similarities and differences of the various workflow modelling schemes. (C) 2001 Elsevier Science Ltd. All rights reserved.",2001,10.1016/S0268-4012(01)00005-6,no
"Importance of predictive microbiology for risk minimization in food production processes - 1. Model development, application software, model validation","Efforts to develop Predictive Microbiology (PM), a rather recent discipline of food hygiene, are made in England (ROBERTS, BARANYI, et al.), the USA (BUCHANAN, WHITING, et al.) and Australia (MCMEEKIN, et al.) since about 15 years. From inoculation experiments in laboratory media, PM derives equations to describe quantitatively in the behaviour of microorganisms in foods in dependence of intrinsic and extrinsic factors (controlling factors). Meanwhile, numerous growth, survival and thermal inactivation (death) model have been elaborated for the most important foodborne pathogens. The Food MicroModel and the Pathogen Modeling Program are available as user friendly application software. Although all PM models are simplifications of the biological mechanisms and the presently available models still have their limitations, comparisons with independent data from literature indicate that predictions of most models are in the worst case fail-safe and their systematic errors do not exceed those of inoculated pack experiments. Once a model has been validated for a type of food, it can be applied at all stages of food production and distribution. PM models are already used to conduct HACCP studies and are powerful tools for microbiological risk assessment, in particular.",2001,,no
Application of process chemistry and SAR modelling to the evaluation of health findings of lower olefins,"Epidemiology studies show increased leukemia mortality among styrene butadiene rubber (SBR) workers but not among butadiene monomer production employees. A detailed review of the SBR manufacturing process indicates that sodium dime thyldithiocarbamate (DMDTC) introduced into the SBR manufacturing process for a period in the 1950s coincides with increased leukemia mortality. Using the Computer-Optimized Molecular Parametric Analysis of Chemical Toxicity (COMPACT), we assessed the enzyme (cytochrome P450) substrate specificity of an olefin series including 1,3-butadiene (BD) and also modeled its interaction with DMDTC. These analyses showed correlation of a structural/electronic parameter - the COMPACT radius - with the presence or absence of cytogenetic activity and also found that DMDTC would inhibit the oxidative metabolism of BD at least at high concentrations. Both DMDTC and its diethyl analog (DEDTC) bind with CYP 2E1 and CYP 2A6. Both of these isoforms are important in the initial oxidative metabolism of butadiene and other olefins. In co-exposure studies in mice of DMDTC with BD or with epoxybutene (EB), we found that there was a reduced increase in genotoxic activity based on micronuclei induction compared with BD or EB exposure alone. Treatment with DMDTC significantly increased the protein carbonyl contents of hepatic microsomes compared with that of controls, a finding that may be related to DMDTC's activity as a prooxidant. Co-exposure with DMDTC and EB increased hepatic microsomal carbonyls to levels significantly greater than those of DMDTC-treated mice, while EB administration in the absence of DMDTC did not change protein carbonyls relative to those of controls. The increase in hepatic microsomal protein carbonyls suggests that DMDTC may modulate EB metabolism towards the formation of reactive intermediates that react with proteins. The present molecular modeling and mechanistic studies suggest that co-exposure of BD and DMDTC is a plausible biological hypothesis regarding increased leukemia risk among SBR workers. (C) 2001 Elsevier Science Ireland Ltd. All rights reserved.",2001,10.1016/S0009-2797(01)00215-0,no
Non-linearity and error in modelling soil processes,"Error in models and their inputs can be propagated to outputs. This is important for modelling soil processes because soil properties used as parameters commonly contain error in the statistical sense, that is, variation. Model error can be assessed by validation procedures, but tests are needed for the propagation of (statistical) error from input to output. Input error interacts with non-linearity in the model such that it contributes to the mean of the output as well as its error. This can lead to seriously incorrect results if input error is ignored when a non-linear model is used, as is demonstrated for the Arrhenius equation. Tests for non-linearity and error propagation are suggested. The simplest test for non-linearity is a graph of the output against the input. This can be supplemented if necessary by testing whether the mean of the output changes as the standard deviation of the input increases. The tests for error propagation examine whether error is suppressed or exaggerated as it is propagated through the model and whether changes in the error in one input influence the propagation of another. Applying these tests to a leaching model with rate and capacity parameters showed differences between the parameters, which emphasized that statements about non-linearity must be for specific inputs and outputs. In particular, simulations of mean annual concentrations of solute in drainage and concentrations on individual days differed greatly in the amount of non-linearity revealed and in the way error was propagated. This result is interpreted in terms of decoherence.",2001,10.1046/j.1365-2389.2001.00366.x,no
Advances in the modeling of the quality of glass melting processes,"Essential process steps in the fusion of glass forming raw materials and in the purification of glass melts in industrial glass furnaces are: batch to glass melt conversion, sand grain dissolution in the fresh melt, removal of dissolved gases and gas bubbles, homogenization and thermal (viscosity) conditioning of the melt. These processes and the completeness of these stages depend on the temperature level, residence time, mixing conditions and the chemistry of the system. Besides these important steps in glass melting, glass quality can be affected due to contamination of the melt by refractory components, impurities in the batch or by evaporation processes. Today, descriptive so-called Computational Fluid Dynamic models exist for the prediction of the heat transfer, the temperatures and glass melt and combustion gas convection patterns in industrial glass furnaces. Extension of these models by a description of the relevant physical-chemical processes in the glass melt tanks, is required for the prediction of the quality of the glass melting process. Such extended models include the determination of the kinetics of sand dissolution in melts, the rate of glass melt degassing, homogenisation behaviour, refractory wear and incongruent evaporation of glass melt components. This paper presents an approach for modelling the most relevant process steps in glass melting and gives basic equations for estimating the required time to complete these essential steps in melting and purification (fining and homogenizing), depending on the flows and temperatures in the melt tank. These equations can be applied to design new glass melt concepts, to improve glass furnaces or to investigate the effect of the process conditions in existing furnaces on glass melt quality in terms of homogeneity, presence of contamination or inclusions. A future challenge in the field of glass technology will be the validation of these glass melt quality models and the application of such models for designing new glass melting procedures using new heating techniques, advanced fining (gas removal) methods and to apply model based control systems for optimization of the stability of glass melter operation. Validation includes the comparison of the statistical correlation between: a. process settings and by the model predicted glass quality in terms of homogeneity, sand grain or bubble removal efficiency on one hand side, with b. found correlations between the observed glass quality in terms of bubble or seed counts, stones, or cords and the actual process settings, on the other hand.",2001,,no
Heterogeneous processes involving nitrogenous compounds and Saharan dust inferred from measurements and model calculations,"Experimental data on aerosol chemical composition and gaseous concentrations in various African ecosystems have been obtained under the IGAC DEBITS AFRICA (IDAF) program. In this paper, data covering a complete wet and dry season (1996 and 1998) in the semiarid savanna of the Sahelian region of Niger are presented. The analysis of the aerosol chemical composition and the gas phase concentrations at the Banizoumbou station indicates two strong signatures: a nitrogenous component composed of nitric acid, ammonia, particulate ammonium, and nitrates; and a terrigenous component originating from semiarid and desert soils (calcium, carbonates, magnesium, potassium, sulfate). To further investigate the interactions between gas and particles and to help interpret the IDAF experimental data, these data are analyzed using a gas aerosol equilibrium model (Simulating Composition of Atmospheric Particles at Equilibrium (SCAPE)). The model is found to accurately represent the mean aerosol composition for the dry and the wet season of the studied region. It is found that heterogeneous processes involving terrigenous compounds are important and play a major role in partitioning semivolatile species, such as nitric acid, between the gas and aerosol phases. The important role of these heterogeneous processes in the atmospheric chemistry in the Sahelian region is discussed. To compare results obtained in the semiarid savanna of Niger and other African ecosystems, SCAPE model is also applied to humid savanna and forest using IDAF and Experiment for Regional Sources and Sinks of Oxidants (EXPRESSO) measurements.",2001,10.1029/2000JD900778,no
Dynamic evaluation of the dehydration response curves of foods characterized by a poultice-up process using a fish-paste sausage. II. A new tank model for a computer simulation,"For a computer simulation of dehydration curves of foods, a new tank model is proposed that uses fish-paste sausage for drying that is accompanied by a poultice-up process. The model is proposed separately and is dependent on the moisture contents in Regions I (W-0 > 100%-d.b.) and II (W-0 < 100%-d.b.) and described by a two-term exponential expression for the two-tank model in Region I and a single-term exponential expression for a single tank model in Region II. The dehydration constants that appeared in the two equations could be evaluated quantitatively by the TPD, NMR, and SEM data obtained. By using the model equations. all the dehydration curves obtained under any drying condition were simulated, and the calculated curves agreed satisfactorily with the experimental curves derived from various drying conditions.",2001,10.1081/DRT-100105288,no
A new novel method for solving ASML alignment fail (model error) in color filter process,"For better resolution and throughput concern on color filter process, we use ASML5500/100 for color filter production instead of 1X CANON, but it often suffered alignment fail (error code: model error) at the green layer. Some items have been studied: (1) pattern close to ASML PM mark; (2) level sensor issue (level sensor contamination, plate tilt, level lens contamination); (3) different process sequence; (4) open the clear-out window at passivation layer to reduce interference effect. All of these items are proved no obviously influence to induce model error. By checking the spectrum of the green photo-resist, we found that it is low transmittance at 633nm(1) (the wavelength that the ASML alignment laser uses). Raising the transmittance by reducing the thickness of green resist is proved useful to eliminate the occurrence of model error. On the other hand, the ATHENA (TM) provided by ASML which use red and green lights for alignment will get rid of the alignment failure 2.",2001,10.1117/12.425276,no
Modelling health care processes for eliciting user requirements: a way to link a quality paradigm and clinical information system design,"Healthcare institutions are looking at ways to increase their efficiency by reducing costs while providing care services with a high level of safety. Thus, hospital information systems have to support quality improvement objectives. The elicitation of the requirements has to meet users' needs in relation to both the quality (efficacy, safety) and the monitoring of all health care activities (traceability). Information analysts need methods to conceptualise clinical information systems that provide actors with individual benefits and guide behavioural. changes. A methodology is proposed to elicit and structure users' requirements using a process-oriented analysis, and it is applied to the blood transfusion process. An object-oriented data model of a process has been defined in order to organise the data dictionary. Although some aspects of activity, such as 'where', 'what else', and 'why' are poorly represented by the data model alone, this method of requirement elicitation fits the dynamic of data input for the process to be traced. A hierarchical representation of hospital activities has to be found for the processes to be interrelated, and for their characteristics to be shared, in order to avoid data redundancy and to fit the gathering of data with the provision of care. (C) 2001 Elsevier Science Ireland Ltd. All rights reserved.",2001,10.1016/S1386-5056(01)00203-9,no
Analysis of crown efficiency in a Common Beech thinning trial using a process-based model.,"In order to design a simulator of local, intensive silviculture to the benefit of future crop trees in Common Beech, an individual-tree growth model was built using ecophysiological assumptions. This linear model links stem growth and crown size. The proportionality factor is named crown efficiency. This macroscopic parameter may be obtained as a combination of several elementary ecophysiological parameters (photosynthetic efficiency, allocation coefficient to stem, respiration coefficients etc.). The simple structure of the model allows the analysis of crown efficiency variations in a thinning trial. We show that: thinning directly increase crown efficiency; canopy closure and climatic fluctuations lead to large differences of crown efficiency between successive periods; in order to show an effect of age, the mathematical structure of the model should be improved, e.g. by a better account of maintenance respiration and geometry of the annual ring.",2001,,no
Process modelling for air bending: validation by experiments and simulations,"In order to evaluate and justify the use of bending simulations in e.g. CAPP applications, the simulations have to be compared to realistic samples from shop floor practice. However, input data concerning e.g. material behaviour, sheet thickness and tool geometry can be controlled in an adequate, reproducible way if obtained under laboratory conditions. The bending setup leading to the experimental data is elaborated upon. The experiments mainly address the required punch displacement and the sheet length correction, being the main concerns in CAPP applications. The experiments are employed to evaluate results of simulations of the air bending process, In this case, the results of the equilibrium model, called the ABS model (air bending simulation) and a finite element method analysis (FEM) (both described in an accompanying publication), are involved in the experimental verification. Diverging trends in the analysis can indicate deficits in the simulations. Presumably, these aberrations can be reflected on the differing assumptions and principles applied in the models or assumptions concerning material behaviour. (C) 2001 Elsevier Science B.V. All rights reserved.",2001,10.1016/S0924-0136(01)00772-5,no
"Formulation and evaluation of IMS, an interactive three-dimensional tropospheric chemical transport model 1. Model emission schemes and transport processes","In part one of this series of papers on a new integrated modelling system (IMS), the interactive three-dimensional chemical transport model (CTM), we present a detailed description of the interactive emission scheme for biogenic species and outline the datasets used for anthropogenic species. In addition, we describe the transport scheme employed in this model. The biogenic emission schemes incorporate the high-resolution Olson World Ecosystem data (Olson, 1992), the satellite-sensed terrestrial vegetation data from AVHRR (A Very High Resolution Radiometer) (Brown et al., 1985), and the CZCS (Coastal Zone Color Scanner) data (Erickson and Eaton, 1993). These datasets provide seasonal variations in surface biogenic emissions. The emission schemes are tested against other estimates (e.g., GEIA) and equilibrium emissions. A comparison of terrestrial biogenic fluxes, both the spatial and temporal (seasonal) variation of modelled surface net primary production, is consistent with the geographical variations of the global vegetation index (GVI) distribution derived from AVHRR. The annual net primary production is 76000 Tg C yr(-1), which compares well with the 40500-78000 Tg C yr(-1) estimated by Melillo et al. (1993). This indicates that the model works well in capturing spatial and seasonal variations in the terrestrial vegetation. The modelled surface vegetation fluxes are verified against data from Guenther et al. (1995). While the comparison shows a generally good agreement in terms of the temporal and spatial distributions of isoprene (530 Tg yr(-1)), large discrepancies are seen over the tropical locations which often exhibit strong seasonality in rainfall and very small variation in temperature. These differences indicate that a large difference in the estimation between an empirical relation and an LSM calculation occurs if an area in which seasonal distribution of rainfall is the main factor which determines the type of vegetation. In this paper, we assess (results are discussed in following papers) the role of changing surface biogenic distributions in surface-to-atmosphere biogenic fluxes (both ocean-to-atmosphere and land-to-atmosphere).",2001,10.1023/A:1006411331967,no
Measurement and modelling of the film casting process 1. Width distribution along draw direction,"In the cast film process a polymer melt is extruded through a slit die, stretched in air and cooled on a chill roll. During the path in air, while the melt cools, a reduction of both thickness and width takes place; obviously, thickness and width reductions are functions of draw ratio and stretching distance. Width distribution along the draw direction was measured on a iPP resin supplied by Montell as function of both flow rate and take up velocity. Final film width was found to decrease as take up velocity increase and, surprisingly, as extrusion flow rate increases. Thus draw ratio increase, attained by either lowering extrusion flow rate or by rising take up velocity, can lead to either enlargement or reduction of final film width. The process of stretching in air was modelled with coupled one-dimensional equations of continuity and motion based on work of Barq, Haudin, Agassant, and Bourgin (Int. Poly. Process. 9 (1994) 350) the crystallinity generation term, according to the Nakamura non-isothermal model, was included in the equation of energy lumped along the thickness direction. The polymer was considered as a viscous fluid (non-Newtonian), the apparent viscosity being function of temperature and strain rate. Furthermore, the effect of crystallinity on viscosity was somehow accounted for. The model equations were solved numerically. A modified expression of heat transfer coefficient with respect to the model of Barq et at. (1994) was applied leading to a better agreement between model predictions and data with reference to width distribution along the draw direction and final film thickness. (C) 2001 Elsevier Science Ltd. All rights reserved.",2001,10.1016/S0009-2509(01)00286-X,no
Non-isothermal '2-D flow/3-D thermal' developments encompassing process modelling of composites: flow/thermal/cure formulations and validations,"In the manufacturing process of large geometrically complex components comprising of fibre-reinforced composite materials by resin transfer molding (RTM), the process involves injection of resin into a mold cavity filled with porous fibre preforms. The overall success of the RTM manufacturing process depends on the complete impregnation of the fibre mat by the polymer resin, prevention of polymer gelation during filling, and subsequent avoidance of dry spots. Since a cold resin is injected into a hot mold, the associated physics encompasses a moving boundary value problem in conjunction with the multi-disciplinary study of flow/thermal and cure kinetics inside the mold cavity. Although experimental validations are indispensable, routine manufacture of large complex structural geometries can only be enhanced via computational simulations, thus eliminating costly trial runs and helping the designer in the set-up of the manufacturing process. This study describes the computational developments towards formulating an effective simulation-based design methodology using the finite element method. The specific application is for thin shell-like geometries with the thickness being much smaller than the other dimensions of the part. Due to the highly advective nature of the non-isothermal conditions involving thermal and polymerization reactions, special computational considerations and stabilization techniques are also proposed. Validations and comparisons with experimental results are presented whenever available. Copyright (C) 2001 John Wiley & Sons, Ltd.",2001,10.1002/nme.85,no
"The Figura Sforzata: Modelling, power and the mannerist body (Evaluating the bent figure through the sculptural process)","In the wake of Michelangelo, a number of Italian artists - and notably those who composed their works using wax models - came to identify the act of design with that of bending bodies. This article looks at how this manner of thinking led sculptors, painters, and writers to articulate new visions and fantasies about their artistic 'mastery'. Focusing on the language contemporaries used when evaluating the bent figure, and considering associated ideas about memory, life study and poetic invention, this article explores the ways which sculptural process itself became a vehicle through which artists could promote their power.",2001,10.1111/1467-8365.00280,no
"Liquor-to-liquor differences in combustion and gasification processes: Simultaneous measurements of swelling and CO2 ,CO, SO2 and no formation reveals new data for mathematical models","In this paper we present for the very first time experimental black liquor particle combustion data with simultaneous profiles of the carbon release and the NO formation and particle swelling for a sulfate and a sulfite liquor, 2.4 and 3.6 mm droplets in O-2/N-2 environments at typical recovery boiler temperatures. Experimental results were compared with a detailed single particle combustion model for obtaining additional information on the process.",2001,,no
Comparison of different analysis models to measure plastic strains on sheet metal forming parts by digital image processing,"In this paper, a strain measurement system for sheet metal forming by digital image processing technology and its fabrication process are presented. The principal strains as well as forming limit diagram can be obtained by using the proposed system. There are two different approaches, namely the total least square optimization method and the multiple regression analysis method are studied. Comparing to the total least square optimization method, the multiple regression analysis method is simpler and faster within reasonable accuracy. The plastic strains, of deformed parts are calculated based on the non-deformed reference configuration being a circle and the deformed configuration being a curve-fitting ellipse. The strain measurement of sheet metal forming is very important and useful. For example, it can be used to construct forming limit diagram of sheet metal with different material properties as well as different process variables, such as strain path, strain rate and temperature. The measurement results can also be used to verify the numerical simulation results, such as finite element analysis. In the final of the paper, one industrial case study of fine stamping electronic part is discussed to demonstrate the proposed methodology.",2001,10.1117/12.429616,no
River water quality model no. 1 (RWQM1): II. Biochemical process equations,"In this paper, biochemical process equations are presented as a basis for water quality modelling in rivers under aerobic and anoxic conditions. These equations are not new, but they summarise parts of the development over the past 75 years. The primary goals of the presentation are to stimulate communication among modellers and field-oriented researchers of river water quality and of wastewater treatment, to facilitate practical application of river water quality modelling, and to encourage the use of elemental mass balances for the derivation of stoichiometric coefficients of biochemical transformation processes. This paper is part of a series of three papers. In the first paper, the general modelling approach is described; in the present paper, the biochemical process equations of a complex model are presented; and in the third paper, recommendations are given for the selection of a reasonable submodel for a specific application.",2001,,no
Comprehensive modeling of the glass container forming process,"Industrial glass forming is a complex sequence of unit processes that includes melting and refining the raw material, cooling and conditioning the molten glass in forehearths and the actual forming processes in the individual section machine. The sensitivity of glass to the processing history requires that comprehensive modeling be used to yield accurate design information in order to control the final thickness distribution in the bottle. Numerical modeling is used for the analysis and design of the glass press and blow process with the objectives of determining the sensitivity of the final product to process parameters and improving the product design at reduced cost and time. All the stages of the complete process were modeled and the result of each unit was transferred and used as input for the next unit to predict the final bottle thickness distribution. In addition to specific thermo-mechanical coupling capabilities, 'mesh-to-mesh' interpolation and remeshing techniques are available to allow a continuation of the calculations in spite of very severe mesh deformations. This work evidences the powerful modeling capabilities available today to address successfully the critical stages of the glass forming process.",2001,,no
Identifying the structure of business processes for comprehensive enterprise modeling,"It is one of the difficulties in enterprise modeling that we must deal with many fragmented pieces of knowledge provided by Various domain-experts, which are usually based on mutually different viewpoints of them. This paper presents a formal approach to integrate those pieces into enterprise-wide model units using Rough Set Theory (RST). We focus on business processes in order to recognize and identify the constituents or units of enterprise models, which would compose the model expressing the various aspects of the enterprise. We defined five model unit types of ""resource,"" ""organization,"" ""task,"" ""function,"" and ''behavior."" The first three types represent the static aspect of the enterprise, whereas the last two types represent the dynamic aspect of it. Those units are initially elicited from each domain-expert as his/her own individual model units, then they are integrated into enterprise-wide units using RST. Our approach is methodology-free, and any methodologies can include it in their early stage to identify what composes the enterprise.",2001,,no
Boreal forest CO2 exchange and evapotranspiration predicted by nine ecosystem process models: Intermodel comparisons and relationships to field measurements,"Nine ecosystem process models were used to predict CO2 and water vapor exchanges by a 150-year-old black spruce forest in central Canada during 1994-1996 to evaluate and improve the models. Three models had hourly time steps, five had daily time steps, and one had monthly time steps. Model input included site ecosystem characteristics and meteorology. Model predictions were compared to eddy covariance (EC) measurements of whole-ecosystem CO2 exchange and evapotranspiration, to chamber measurements of nighttime moss-surface CO2 release, and to ground-based estimates of annual gross primary production, net primary production, net ecosystem production (NEP), plant respiration, and decomposition. Model-model differences were apparent for all variables. Model-measurement agreement was good in some cases but poor in others. Modeled annual NEP ranged from -11 g C m(-2) (weak CO2 source) to 85 g C m(-2) (moderate CO2 sink). The models generally predicted greater annual CO2 sink activity than measured by EC, a discrepancy consistent with the fact that model parameterizations represented the more productive fraction of the EC tower ""footprint."" At hourly to monthly timescales, predictions bracketed EC measurements so median predictions were similar to measurements, but there were quantitatively important model-measurement discrepancies found for all models at subannual timescales. For these models and input data, hourly time steps (and greater complexity) compared to daily time steps tended to improve model-measurement agreement for daily scale CO2 exchange and evapotranspiration (as judged by root-mean-squared error). Model time step and complexity played only small roles in monthly to annual predictions.",2001,10.1029/2000JD900850,no
A nude mice model of human rhabdomyosarcoma lung metastases for evaluating the role of polysialic acids in the metastatic process,"PSA is an oncodevelopmental antigen usually expressed in human tumors with high metastatic potential. Here we set up a metastatic model in nude mice by using TE671 cells, which strongly express PSA-NCAM. We observed the formation of lung metastases when TE671 cells were injected intravenously, intramuscularly, and intraperitoneously, hut not subcutaneously. Intraperitoneous injections also induced peritoneal carcinosis, ascites, and liver metastases. To evaluate the putative role of PSA in the metastatic process we used a specific cleavage of PSA on NCAM by endoneuraminidase-N on intraperitoneous primary tumors. Mice with primary intramuscular tumors were taken as control. Repeated injections of endoneuraminidase-N led to a decrease in PSA expression in primary intraperitoneous nodules and ascites but not in intramuscular primary tumors. Endoneuraminidase-N also increased the delay in ascitic formation and decreased the number of lung or liver metastases in the case of intraperitoneous tumors but not in the case of intramuscular tumors. When metastases occurred in endoneuraminidase-N injected animals, they strongly expressed PSA-NCAM. Therefore, we established a relationship between PSA expression on the surface of primary tumor cells and the metastatic process.",2001,10.1038/sj.onc.1204176,no
Validation of a 5-log(10) reduction of Listeria monocytogenes following simulated commercial processing of Lebanon bologna in a model system,"Recently, numerous product recalls and one devastating outbreak that claimed 21 lives were attributed to Listeria monocytogenes in ready-to-eat meat products. Consequently, the Food Safety and Inspection Service published a federal register notice requiring manufacturers of ready-to-eat meat and poultry products to reassess their hazard analysis and critical control point plans for these products as specified in 9 CFR 417.4(a). Lebanon bologna is a moist, fermented ready-to-eat sausage. Because of undesirable quality changes, Lebanon bologna is often not processed above 48.9 degreesC (120 degreesF). Therefore, the present research was conducted to validate the destruction of L. monocytogenes in Lebanon bologna batter in a model system. During production, fermentation of Lebanon bologna to pH 4.7 alone significantly reduced L. monocytogenes by 2.3 log(10) CFU/g of the sausage mix (P < 0.01). Heating the fermented mix to 48.9 degreesC in 10.5 h destroyed at least 7.0 log(10) CFU of L. monocytogenes per g of sausage mix. A combination of low pH (5.0 or lower) and high heating temperatures (greater than or equal to 43.3 degreesC, 115 degreesF) destroyed more than 5 log(10) CFU of L. monocytogenes per g of sausage mix during the processing of Lebanon bologna. In conclusion, an existing commercial process, which was validated for destruction of Escherichia coli O157:H7, was also effective for the destruction of more than 5 log(10) CFU of L. monocytogenes.",2001,,no
Irregular input data in convergence acceleration and summation processes: General considerations and some special Gaussian hypergeometric series as model problems,"Sequence transformations accomplish an acceleration of convergence or a summation in the case of divergence by detecting and utilizing regularities of the elements of the sequence to be transformed. For sufficiently large indices, certain asymptotic regularities normally do exist, but the leading elements of a sequence may behave quite irregularly. The Gaussian hypergeometric series F-2(1)(a,b; c: z) is well suited to illuminate problems of that kind. Sequence transformations perform quite well fur most parameters and arguments. If. however, the third parameter c of a nonterminating hypergeometric series F-2(1) is a negative real number, the terms initially grow in magnitude like the terms of a mildly divergent series. The use of the leading terms of such a series as input data leads to unreliable and even completely nonsensical results. In contrast, sequence transformations produce good results if the leading irregular terms are excluded from the transformation process. Similar problems occur also in perturbation expansions. For example, summation results for the infinite coupling limit kj of the sextic anharmonic oscillator can be improved considerably by excluding the leading terms from the transformation process. Finally. numerous new recurrence formulas for the F-2(1) (a. b: c: z) are derived. (C) 2001 Elsevier Science B.V. All rights reserved.",2001,10.1016/S0010-4655(00)00175-2,no
Understanding enantioselective processes: A laboratory rat model for alpha-hexachlorocyclohexane accumulation,"Since cyclodextrin gas chromatography columns became popular for chiral separations, many researchers have noticed high enantiomeric ratios [ER: (+)-enantiomer/(-)enantiomer] for alpha -HCH in the brains of wildlife. This investigation used the laboratory rat as a model for these phenomena. Rats were either pretreated with phenabarbital (PB) or left untreated and then dosed with alpha -HCH. Animals were sacrificed after 1 or 24 h. The ER averaged 0.95 +/- 0.01 in blood, 1.29 +/- 3.02 in fat, and 0.77 +/- 0.004 in liver. ERs in brain ranged from 2.8 +/- 0.5 to 13.5 +/- 0.4. Both the tissue concentration distribution and the ERs agree well with those previously reported in wildlife. To determine whether high brain ERs were due to enantioselective metabolism or transport through the blood-brain barrier, alpha -HCH exposed brain and liver tissue slices were compared. Concentrations in the brain slices did not decrease with PB pretreatment but did decrease in the liver slices. Enantiomeric ratios in the brain slices averaged 1.11 +/- 0.02 and were 0.76 +/- 0.03 in liver slices for the PB pretreated rats. These data indicate that the enantioselective metabolism of alpha -HCH by the brain is not the mechanism responsible for high ERs in this tissue.",2001,10.1021/es001754g,no
Toward a comprehensive framework for software process modeling evolution,"Software process modeling has undergone extensive changes in the last three decades, impacting process' structure, degree of control, degree of visualization, degree of automation and integration. These changes can be attributed to several factors. This paper studies two of these factors, the time dimension and the interdisciplinary impact, and assesses their effect on the evolution of process modeling. A literature survey for software process modeling was carried out which provided evidence of how the time dimension and the interdisciplinary impact triggered process evolution and changes in methodology, technology, experience and business requirements. Finally, the paper concludes with a theoretical framework to serve as an illustrative model for the effects of the time dimension and interdisciplinary impact on process modeling evolution. This framework can serve as to develop more advanced models for technological forecasting in software process modeling evolution.",2001,10.1109/AICCSA.2001.934050,no
Information processing and insight: A process model of performance on the nine-dot and related problems,"The 9-dot problem is widely regarded as a difficult insight problem. The authors present a detailed information-processing model to explain its difficulty, based on maximization and progress-monitoring heuristics with lookahead. In Experiments 1 and 2, the model predicted performance for the 9-dot and related problems. Experiment 3 supported an extension of the model that accounts for insightful moves. Experiments 4 and 5 provided a critical test of model predictions versus those of previous accounts. On the basis of these findings, the authors claim that insight problem solving can be modeled within a means-ends analysis framework. Maximization and progress-monitoring heuristics are the source of problem difficulty, but also create the conditions necessary for insightful moves to be sought. Furthermore, they promote the discovery and retention of promising states that meet the progress-monitoring criterion and attenuate the problem space.",2001,10.1037//0278-7393.27.1.176,no
Model-based evaluation of two BNR processes - UCT and A(2)N,"The activity of denitrifying P-accumulating bacteria (DPB) has been verified to exist in most WWTPs with biological nutrient removal (BNR). The modified UCT process has a high content of DPB. A new BNR process with a two-sludge system named A(2)N was especially developed to exploit denitrifying dephosphatation. With the identical inflow and effluent standards, an existing full-scale UCT-type WWTP and a designed A2N process were evaluated by simulation. The used model is based on the Delft metabolical model for bio-P removal and ASM2d model for COD and N removal. Both processes accommodate denitrifying dephosphatation, but the A(2)N process has a more stable performance in N removal. Although excess sludge is increased by 6%, the A(2)N process leads to savings of 35, 85 and 30% in aeration energy, mixed liquor internal recirculation and land occupation respectively, as compared to the UCT process. Low temperature has a negative effect on growth of poly-P bacteria, which becomes to especially appear in the A2N process. (C) 2001 Elsevier Science Ltd. All rights reserved",2001,10.1016/S0043-1354(00)00596-0,no
Dynamic model simulations as a tool for evaluating the stability of an anaerobic process,"The association of a wall growth factor with a dynamic model based on Andrews' work (1969), without pH restrictions, is used herein to study the inhibition of methanogenesis by high concentrations of volatile acids. The model considers the methanogenic bacteria as being representative of the biological phase of the anaerobic digestion, and assumes a continuous feed of acetic acid to the continuously stirred anaerobic reactor. The model can be used for simulations on transient conditions, namely the effect of initial conditions on the start-up of a digester, as well as for studying the significant improvements in stability when wall growth occurs in the reactor. The effect of changing the feed characteristics to a digester was studied in two situations: with and without wall growth. The presence of wall growth allows a better behaviour of an anaerobic process in any case, namely when a step increase in the feeding substrate concentration or in flow rare is performed.",2001,,no
Computer modelling of intrinsic defects and migration processes in KY3F10,The computer modelling technique was used in the present work to study the intrinsic defects and the migration processes in KY3F10. The main intrinsic disorder was found to be F Frenkel pairs but at higher temperatures it is possible that KF pseudo-Schottky defects could also be present in the material. The ionic conduction process is mainly due to the fluorine ions migrating via both vacancy and interstitialcy mechanisms.,2001,10.1080/10420150108214144,no
Numerical modelling of defect formation on copper wire surfaces during the wire drawing process,"The crowsfoot defect, a characteristic surface cracking defect in wire drawing, usually occurs at fine wire diameters. This paper reports a numerical simulation of the wire drawing process where copper wire is reduced in diameter under varying conditions of friction, die angle and reduction ratios. The results of this analysis show that, contrary to existing theoretical models, the largest drawing stress and von Mises equivalent stress occur at the outer surface of the wire and not along the central axis as previously postulated.",2001,10.1243/0954406011520652,no
Application of isothermal and model-free isoconversional modes in DSC measurement for the curing process of the PU system,"The dynamic, isothermal, and model-free isoconversional modes of differential scanning calorimetry measurement were used to monitor the curing process of the polyurethane system. Conversions obtained from these three methods were in good agreement with one another, indicating that isothermal and model-free isoconversional modes can successfully be used for monitoring the curing process. For the isothermal mode, the highest reaction rate occurred at time zero, and autoacceleration was not observed for this system. From the model-free isoconversional mode, it was possible to calculate the activation energy changes during the curing process. (C) 2001 John Wiley & Sons, Inc.",2001,10.1002/app.1574,no
"Why is there an ERN/Ne on correct trials? Response representations, stimulus-related components, and the theory of error-processing","The ERN or Ne is a component of the event-related brain potential that occurs when human subjects make errors in reaction time tasks. It is observed in response-locked averages, time-locked to the execution of the incorrect response. Recent research has reported that this component is present on correct response trials, thereby challenging the idea that the component is specifically related to error-processing. In this paper, we argue that the ERN or Ne observed on correct trials can be attributed to one or both of two factors: either there is error-processing on correct trials, and/or the response-locked averages used to derive the ERN:Ne are contaminated by negative components evoked by the stimulus. For this reason, there is no reason to abandon theories that relate the ERN/Ne to error-processing. (C) 2001 Elsevier Science B.V. All rights reserved.",2001,10.1016/S0301-0511(01)00076-X,no
A methodology for measuring and modeling crystallographic texture gradients in processed alloys,"The explicit representation of internal material structure in alloy processing and in-service performance simulations is becoming increasingly prevalent. This paper presents a methodology for characterizing and representing a spatially-varying orientation distribution function (ODF) that can be used in processing and performance simulations for alloys containing texture gradients. We use thick AA 7050 aluminum plate, which is known to contain texture gradients, as a case study to demonstrate the methodology, which employs a finite element representation of the ODF initialized using individual lattice orientation measurements taken using the electron backscatter pattern (EBSP) technique. As expected, we find that the texture varies significantly through the plate thickness. We use the ODF to examine the effect of the varying texture on the resulting yield strength distribution as embodied by the average Taylor factor. We find that the predicted yield strength anisotropy is different at different locations through the thickness of the plate. We examine the optimal number of orientation measurements necessary for determining the ODF in the presence of this texture gradient. We find that as we increase the number of orientations, the ODF quickly becomes stable but eventually starts to change under the influence of the texture gradient. We also investigate spatial interpolation of the ODF using the finite element representation, We find that, as with finite element representations of other fields, interpolation accuracy depends on the variation of the held variable and the discretization of the domain. In this case, gradients in both physical space and orientation space affect the accuracy of the interpolation. Finally, the effects of the texture gradient on the mechanical response of the material is demonstrated by employing the ODFs taken from various locations through the thickness of the plate in polycrystal plasticity simulations of uniaxial tension and plane strain compression. (C) 2001 Elsevier Science Ltd. All rights reserved.",2001,10.1016/S0749-6419(00)00068-1,no
Process modeling in machining. Part II: validation and applications of the determined flow stress data,"The flow stress data, determined in Part I of the present study, is validated by using it as an input to the finite element method and analytical based computer programs to predict process variables in metal cutting. The predicted process variables in two-dimensional orthogonal turning and three-dimensional face milling operations, are compared with the published experimental data and the results of experiments conducted in the present work. The majority of the predictions have been found to be in reasonable agreement with the measured results. The comparisons have been discussed and, in the case of unsatisfactory agreement, the reasons for inaccurate predictions are reviewed. The how stress data of AISI H13 tool steel (46 HRC), determined in Part I is used in this study to investigate the influence of edge preparation on forces in the cutting and feed directions, tool stresses and cutting temperatures. It has been concluded that the hone-radius edge with a hone radius of 0.1 mm provides the maximum resistance to chipping and the chamfered edge (20 degrees x 0.1 mm) has the minimum flank and crater wears for the conditions used in the present study. (C) 2001 Elsevier Science Ltd. All rights reserved.",2001,10.1016/S0890-6955(01)00017-7,no
Measuring regional competitiveness in oilseeds production and processing in Nigeria: a spatial equilibrium modelling approach,"The four main resource regions of Nigeria, namely, the East, Middle-belt, West and North, differ in their endowments in the production and processing of oilseeds. The Federal Government has allocated resources to the oilseeds sub-sector across the regions without apparent rational basis. Consequently, huge imports of oilseed products, high and unaffordable oilseed product prices, and the establishment of new oil mills in the face of frequent shutdowns and chronic under-utilisation of existing mills, still characterise the sector. In this paper, a spatial equilibrium model of the Nigerian oilseeds economy is used to determine the optimal location and number of mills across the regions, and to establish regional competitiveness in aid of the planning and development of the oilseeds sub-sector. In the model, production (farming and milling) activities are tied to arable land and milling capacity constraints. The shadow prices of the arable land and milling capacities are used as a measure of competitiveness. Results indicate that, because of its central location, the Middle-belt is the most competitive in oilseeds production at a shadow value of N18,400 per hectare of cultivated land, followed by the North, West and East regions. However, the East is the most competitive in oilseeds milling, followed by the West, due to their close proximity to ports of export. These results suggest that transport cost is the main determinant of the viability of oilseeds production and milling in Nigeria. (C) 2001 Elsevier Science B.V. All rights reserved.",2001,10.1111/j.1574-0862.2001.tb00070.x,no
Microbial processes of CH4 production in a rice paddy soil: Model and experimental validation,"The importance of different anaerobic processes leading to CH4 production in rice paddies is quantified by a combination of experiments and model. A mechanistic model is presented that describes competition for acetate and H-2/CO2 inhibition effects and chemolithotrophic redox reactions. The model is calibrated with anaerobic incubation experiments with slurried rice soil, monitoring electron donors and electron accepters influencing CH4 production. Only the values for maximum conversion rates (V-max) for sulphate and iron reduction and CH4 production are tuned. The model is validated with similar experiments in which extra electron donors or electron accepters had been added. The differences between model estimates without kinetic parameter adjustments and experiment were not significant, showing that the model contains adequate process descriptions. The model is sensitive to the estimates of V-max, that are site dependent and to the description of substrate release, that drives all competition processes. For well-shaken systems, the model is less sensitive to chemolithotrophic reactions and inhibitions. Inhibition of sulphate reduction and methanogenesis during iron reduction can however explain acetate accumulation at the start of the incubations. Iron reduction itself is most probably retarded due to manganese reduction. Copyright (C) 2001 Elsevier Science Ltd.",2001,10.1016/S0016-7037(01)00563-4,no
Toward a comprehensive model of the school leaving process among Latinos,"The Latino population continues to have extremely high dropout rates compared to other racial and ethnic groups. This trend is particularly disturbing given the major demographic changes currently under way in this country, with a disproportionate share of the current and future growth of the U.S. population due to the Latino population. Failure to reduce the dropout rates of Latino youth will have serious consequences in the near future. This article provides an overview of the literature on Latino dropouts as well as research and data needs in the study of this phenomenon. The article also contains a brief discussion of key policy issues that can be addressed with the development of research and data sources focusing on Latino dropouts. The authors conclude with a discussion regarding the urgency in reducing the high Latino dropout rate.",2001,,no
Error detection in GPS observations by means of Multi-process models,"The main purpose of this article is to present the idea of using Multi-process models as a method of detecting errors in GPS observations. The theory behind Multi-process models, and double differenced phase observations in GPS is presented shortly. It is shown how to model cycle slips in the Multi-process context by means of a simple simulation. The simulation is used to illustrate how the method works, and it is concluded that the method deserves further investigation.",2001,,no
In-line moisture measurement during granulation with a four-wavelength near-infrared sensor: an evaluation of process-related variables and a development of non-linear calibration model,"The near-infrared set-up based on simultaneous detection of four wavelengths was applied for in-line moisture measurement during fluid bed granulation. In addition to the spectral response, several other process measurements describing the stare of the granulation were evaluated. The near-infrared moisture measurement is disturbed by the variation in physical properties of the sample (eg, temperature, particle size, bulk density). The factors explaining the non-linearity of spectral response during different phases of granulation could he extracted. Combining all this process information improved the prediction capability of the multivariate calibration models tested (partial least squares and artificial neural network (ANN)). The back-propagation neural network approach was found to have most predictive power with the independent test data. (C) 2001 Elsevier Science B.V. All rights reserved.",2001,10.1016/S0169-7439(01)00108-3,no
Improving the efficiency of existing water process tanks using Flow Through Curves (FTCs) and mathematical models,"The process efficiency of existing water tanks can be dramatically increased by performing simple geometrical modifications to improve flow conditions and increase the actual detention time. In the present work, the tank of Agios Stefanos, a significant component of the water supply system of the region of Attica (Greece), is examined. The geometry of the tank is modified, by placing an internal wall. To assess and quantify the effect of this modification, die shapes and the characteristics of the Flow Through Curves (FTCs) of the initial and 3 alternative modified geometries are compared. The FTCs are not derived experimentally, but computationally with a verified mathematical model. The alternative with the best FTC characteristics is proposed for construction.",2001,,no
A comprehensive study of the rational function model for photogrammetric processing,"The rational function model (RFM) has gained considerable interest recently mainly due to the fact that Space Imaging Inc. (Thornton, Colorado) has adopted the RFM1 as an alternative sensor model for image exploitation. The RFM has also been implemented in some digital photogrammetric systems to replace the physical sensor mode for photogrammetric processing. However, there have been few publications addressing the theoretical properties and practical aspects of the RFM until recently. In this paper a comprehensive study of the RFM is reported upon. Technical issues such as the solutions, feasibility; accuracy, numerical stability, and requirements for control information are addressed. Both the direct and iterative least-squares solutions to the RFM are derived, and the solutions under terrain-dependent and terrain-independent computation scenarios are discussed. Finally, evaluations of the numerous tests with different data sets are analyzed. The objective of this study is to provide readers with a comprehensive understanding of the issues pertaining to applications of the RFM.",2001,,no
Modeling and estimation of the spatial variation of elevation error in high resolution DEMs from stereo-image processing,The spatial variability of elevation errors in high-resolution digital elevation models (DEMs) derived from stereo-image processing is examined. Error models are developed and evaluated by examining the correlation between various DEM parameters and the magnitude of the observed DEM vertical error. DEM vertical errors were estimated using a dataset of more than 51 000 points of known elevation obtained from a kinematic global positioning satellite (GPS) ground survey. Elevation variability and the quality of the stereo-correlation match over small spatial scales were the dominant factors that determined the magnitude of the DEM error at any given location. The error models are strongly correlated with the magnitude of the DEM vertical error and are shown to adequately represent the full range of the observed error. The error models are used to estimate the magnitude of the vertical error for every point in the DEMs. The models are then used to predict the overall error in the DEMs. The results demonstrate that the error models can accurately quantify and predict the spatial variability of the DEM error.,2001,10.1109/36.964985,no
Modeling quantum measurement probability as a classical stochastic process,"The time-dependent measurement probabilities for the simple two-state quantum oscillator seem to invite description as a classical two-state stochastic process. It has been shown that such a description cannot be achieved using a Markov process. Constructing a more general non-Markov process is a challenging task, requiring as it does the proper generalizations of the Markovian Chapman-Kolmogorov and master equations. Here we describe those non-Markovian generalizations in some detail, and we then apply them to the two-state quantum oscillator. We devise two non-Markovian processes that correctly model the measurement statistics of the oscillator, we clarify a third modeling process that was proposed earlier by others, and we exhibit numerical simulations of all three processes. Our results illuminate some interesting though widely unappreciated points in the theory of non-Markovian stochastic processes. But since quantum theory does not tell us which one of these quite different modeling processes ""really"" describes the behavior of the oscillator, and also since none of these processes says anything about the dynamics of other (noncommuting) oscillator observables, we can see no justification for regarding any of these processes as being fundamentally descriptive of quantum dynamics. (C) 2001 American Institute of Physics.",2001,10.1063/1.1378791,no
Validation of the mathematical models of heat and mass transfer processes,"Theory of mathematical modeling of any physical or chemical process is commonly based on schemes leading to unique solution. Boundary and initial conditions together with physical properties of the thermodynamic system are treated as exactly known. The influence of different kinds of mathematical model simplifications on the accuracy of solution and reliability of the model are not usually analysed. The widely used procedure of model validation is based on direct comparison of analytical or numerical solution, unique in mathematical sense, with measurement results. The main feature of the method presented in this paper is that all experimental results are included into the mathematical model. Thus, because of the inevitable errors of measurements the system of model equations become internally contradicted as the number of unknown variables is less than the number of equations. In consequence, basic laws of energy and mass conservation are not satisfied. To adjust the experimental data to the mathematical model an orthogonal least squares method is proposed. Additionally, a posteriori errors calculated using the law of errors propagation, give important information about the accuracy of model solutions. The values of corrections of experimental results allow to formulate an objective criterion for the validation of the mathematical model. In the paper, mathematical models of heat and mass transfer processes are validated: nucleation and grain growth during solidification of binary system and mass transfer during autocatalytic dissolution of metallic copper in oxygen-containing ammonia solutions.",2001,,no
Modeling and control of thin film morphology using unsteady processing parameters: Problem formulation and initial results,"Thin film deposition is an industrially-import ant process to which control theory has not historically been applied. The need for control is growing as the size of integrated circuits shrinks, requiring increasingly tighter tolerances in thin film manufacture. In this work we formulate a lattice model of film growth as a control system and take the process parameters as inputs. In the evolution equation, nonlinear functions of the process parameters multiply linear vector fields, yielding a structure similar to a bilinear system. The process conditions in some deposition methods are inherently unsteady, which produces films with altered morphology. We use the model developed in this study to analyze the effects of fast periodic forcing on thin film evolution, With the method of averaging we develop new effective transition rates which may produce film properties unattainable with constant inputs. These effective rates are the convex hull of the set of rates associated with constant inputs. We present conditions on the convex hull for which the finite-time and infinite-time reachability sets cannot be expanded with fast periodic forcing. An example in which this forcing increases the reachability set and produces more desirable morphology is also presented.",2001,,no
Use of physical models for validating an Euler/Euler approach for sediment transport processes,This paper deals with the sedimentation and transport processes taking place in a rectangular horizontal channel in the laboratory. These model tests have served as a basis for the numerical simulation of sedimentation processes and the various factors involved. An Euler/Euler model for the numerical simulation of sedimentation effects and sediment transport is presented. The model solves the transient Navier-Stokes equations with the k-epsilon turbulence model. The main advantage of the model is its validity in regions of high and low sediment concentration. Bed changes are accounted for directly without any adaption of the computational grid. The effects of deposition and reentrainment of particles are investigated in a laboratory flume. The results from the numerical model are in good accordance with those coming from the physical model tests.,2001,,no
Sedimentary process modeling in radioecological water quality model,"This paper presents the diverse components needed to elaborate a fresh water radioecological quality model. In particularly, we propose to take into account the sedimentary dynamics process through erosion and deposition fluxes with a multi- class model. The validity of this approach is checked by comparison with the, experimental results obtained after resuspending sediment experiment realized in a small flume.",2001,,no
Environmental impact evaluation model for industrial processes,"This paper proposes a model to evaluate the environmental impact of manufacturing processes. This model uses the potency factor approach to classify environmental impacts into five ecological health impact groups and uses their toxicological, cancer, and physical effects as the bases to rate the seven groups of human health impacts. The environmental impacts in each impact group are reported, and their hazard scores on ecological and human health are determined. The model also generates a single score for the overall environmental impacts of a process. This single score system helps to identify, among all the viable processes, which is the most environmentally friendly process. This model can serve as a tool to highlight the potential environmental hazards of process operations and to provide information about environmental performance for decision making. The model has been developed into a computer software program, Environmental Impact Evaluation System, and is demonstrated by using the processes employed for the manufacture of paper bags.",2001,10.1007/s002670010183,no
Reliability modeling incorporating error processes for Internet-distributed software,"This paper proposes several improvements on the conventional software reliability growth models (SRGMs) to describe actual software development process by eliminating an unrealistic assumption that detected errors are immediately corrected. A key part of the proposed models is the ""delay-effect factor"", which measures the expected time lag in correcting the detected faults during software development. To establish the proposed model, we first determine the delay-effect factor to be included in the atonal correction process. For the conventional SRGMs, the delay-effect factor is basically non-decreasing. This means that the delayed effect becomes more significant as time moves forward. Since this phenomenon may not be reasonable for some applications, we adopt a bell-shaped curve to reflect human learning process in our proposed model Experiments on a real data set for internet-distributed software has been performed, and the results show that the proposed new model gives better performance in estimating the number of initial faults than previous approaches.",2001,,no
Modelling pesticide environmental fate: process understanding and knowledge gaps,"This paper reviews the state-of-the-art of pesticide environmental fate modelling, emphasizing interactive effects of non-linear and non-equilibrium processes affecting leaching to groundwater, and the incorporation and application of this knowledge in simulation models. The paper also highlights significant gaps in our current understanding of specific environmental compartments where more research is clearly needed.",2001,,no
"Latent models of family processes in African American families: Relationships to child competence, achievement, and problem behavior","This study explored the assessment of family processes for a sample of African American kindergarten children, parents, and teachers involved in the EARLY ALLIANCE prevention trial. Using modified versions of the Family Assessment Measure, the Family Adaptability and Cohesion Evaluation Scales, the Family Beliefs Inventory, and the Deviant Beliefs measure, internal consistency analyses along with exploratory and confirmatory factor analyses provided empirical support for a Cohesion factor (cohesion and communication), a Structure factor (support and organization), a Beliefs factor (on family purpose and child development), and a Deviant Beliefs factor. Regression analyses examined the relationship of these measures of family, processes to child social and academic competence, problem behavior, and early reading achievement. Family Structure (support and organization) was consistently related to parent-and teach er-reported competence and behavioral outcomes, providing support for this construct as an important aspect of family process. Family Cohesion and communication, along with Beliefs, were also related to youth competence and behavior. None of the family process variables added a unique contribution to the influence upon achievement for these kindergarten children beyond the role of parental education and income. This work begins to examine specific dimensions of family processes and their relationships to important adaptive and less adaptive child outcomes. Other dimensions may be identified and examined in future research with families of color.",2001,10.1111/j.1741-3737.2001.00967.x,no
A methodology for validation of process models used to simulate thermal tests at Yucca Mountain,"This study involved the development and application of a methodology for evaluation of process models developed to capture the thermal-hydrological (TH) and thermal-hydrological-mechanical (THM) responses from the thermal tests associated with the Yucca Mountain Project (YMP). These thermal tests, which are referred to as the Drift Scale Test (DST), the Single Heater Test (SHT), and the Large Block Test (LBT), have different areal extents (ranging from 10 to 3000 m(2)) and different electrical heater powers (ranging from 2.3 to 190 kW). All three thermal tests have instruments and sensors that measure the T-H-M responses during heating/cooling periods that range from one to eight years. In this study, quantitative evaluation using statistical measures is emphasized. Results indicate the TH process model simulated the thermal response quite well; however, evaluating the same process models for simulation of the hydrological response is more difficult because, in part, of inherent uncertainties in the hydrological measurements. Similarly, the THM model is also affected by uncertainties but adequately replicated the mechanical response.",2001,,no
A study of applying fuzzy analytic hierarchy process on management talent evaluation model,"This study is to construct a new managerial talent evaluation model for the assessors in IC packaging industry in Taiwan. The Fuzzy Analytic Hierarchy Process (FARP) method involved in evaluation to be able to obtain the information with more systematic and efficient ways to assist the related managerial competency activities. Techniques of the Borda function, FAHP method, fuzzy Delphi method and questionnaire survey were used in the study. Finally, an empirical study was conducted to examine the FAHP of management talent evaluation model effectively to promote quality of decision making and ran be referenced for further managerial talent related activities.",2001,,no
Toward a domain-theoretic modelling of measuring processes,"This work(1) provides for a general domain-theoretic framework for modelling measuring instruments and processes, with a geometric interpretation based on simplicial complexes, An analogy between the notions of approximation and computation in domain theory and the notion of measuring process performed with a measuring instrument is established. A qualitative domain of coherent simplexes of instrument readings is defined. The histories of the measuring processes performed in that domain are modelled in an adjoint qualitative domain of coherent simplicial complexes.",2001,,no
Flow front measurements and model validation in the vacuum assisted resin transfer molding process,"Through-thickness measurements were recorded to experimentally investigate the through thickness flow and to validate a closed form solution of the resin flow during the vacuum assisted resin transfer molding process (VARTM). During the VARTM process, a highly permeable distribution medium is incorporated into the preform as a surface layer and resin is infused into the mold, under vacuum. During infusion, the resin flows preferentially across the surface and simultaneously through the thickness of the preform, giving rise to a three dimensional-flow front. The time to fill the mold and the shape of the flow front, which plays a key role in dry spot formation, are critical for the optimal manufacture of large composite parts. An analytical model predicts the flow times and flow front shapes as a function of the properties of the preform, distribution media and resin. It was found that the flow front profile reaches a parabolic steady state shape and the length of the region saturated by resin is proportional to the square root of the time elapsed. Experimental measurements of the flow front in the process were carried out using embedded sensors to detect the flow of resin through the thickness of the preform layer and the progression of flow along the length of the part. The time to fil]. the part, the length of flow front and its shapes show good agreement between experiments and the analytical model. The experimental study demonstrates the need for control and optimization of resin injection during the manufacture of large parts by VARTM.",2001,10.1002/pc.10553,no
3D solid modeling of multilayer printed circuit boards,"To better integrate PCB engineering design and analysis, the software tool PCB-FEA has been developed to interface Board Station's (Mentor Graphics) layout data for automated thermal, structural and thermo-mechanical FE simulations with ANSYS. The main goal is to identify printed circuit board fatigue problems early in the design process. On account of densely populated PCBs, a two-step board-level/component-level modeling approach is used to keep the model in computationally acceptable limits. To consider conduction heat transfer in printed circuit boards, a CAD data based 3D solid model of the layered composite is proposed in this paper. The model is intended to replace traditional models that treat the board as a homogenous material with two different effective thermal conductivities.",2001,,no
A model for crosstalk noise evaluation in deep submicron processes,"To certify the correctness of a design. in deep submicron technologies the verification process has to cover some new issues. The noise introduced on signals through the crosstalk coupling is one of these emerging problems. In this paper, we propose a model to evaluate the peak value of the noise injected on a signal during the transition of its neighboring signals. This model has been used in a prototype verification tool and has shown a satisfying accuracy! within a reasonable computation delay.",2001,10.1109/ISQED.2001.915218,no
River Water Quality Model no. 1 (RWQM1): Case study II. Oxygen and nitrogen conversion processes in the River Glatt (Switzerland),"Various simplifications of the river water quality model no. 1 are applied to data sets from the river Glatt in Switzerland. In a first application, the biomass responsible for nitrogen and oxygen conversion processes is quantified based on known reaeration rates, measured concentrations of ammonia, nitrite and oxygen and assumed growth parameters of algae and bacteria. In a second application, the model is extended to calculate chemical equilibria of inorganic carbon compounds dissolved in the water and daily variations in pH. The influence of partially unknown inflow concentrations and of calcite precipitation on fluctuations in electrical conductivity and pH are discussed. In the last model, the processes of growth of sessile algae and bacteria, detachment of algae, and grazing by benthic organisms are introduced. Due to lack of data for quantifying these processes, this last model application is speculative. Nevertheless, it is interesting because it shows a direction to which river water quality modelling would have to proceed in order to increase its predictive capabilities.",2001,,no
Estimating consistency of geometric world models through observation of a localization process,"We present the Geometric Consistency Filter, a new approach to extraction of spatial information in the context of the navigation for autonomous mobile robots. Geometric Consistency Filter can detect a significant portion of local inconsistencies in geometric world models, which result from partially dynamic environments and a limited resolution of the sensing systems. Belief about consistency of portions of the world model is based on incremental fusion of evidence, which is obtained through analysis of the localization process. presented experiments demonstrate how the estimation of consistency can be useful for robust localization and updating of world models in dynamic environments.",2001,,no
Spice model quality: Process development viewpoint,"We show what is required of a good model, with emphasis from a process development point of view. The merits of good testchips, physical correctness of the model, and realistic skews (best/worst case) are investigated. Prediction of future technologies with some level of confidence is also looked at. A close working relationship between process integration, model engineering, and circuit design is considered. Advantages of models with well-behaved, smooth, glitchless characteristics and M ell-behaved derivatives are covered. Parameter correlations. parameter redundancy: and model documention round our the basic model discussions. Extra features are also covered, including noise models, high frequency characterization, diode models, parasitic bipolar models, etc. And finally the need for models to evolve with manufacturing improvements is highlighted.",2001,10.1109/ISQED.2001.915274,no
Understanding Internet users on double helical model of chance-discovery process,"A case is presented for the double helical processing of discovery system - human and automated data mining systems co-work, each progressing spirally toward the creative reconstruction of ideas. Especially, the discovery of what we call chances, significant novel events are to be realized in this process. The example shown here is an application to questionnaire analysis, for understanding new behaviors of the Internet users. The Internet users are born and bred with face-to-face human relations in the real worid, but their interactions with WWW are distilling new value-criteria, keeping personal real-world senses of rationality, empathy, ethics, etc. In our method for aiding the discovery, base on the double-helix model, the in-depth interaction of the Internet, the personalities and the behaviors of people came to be understood with revealing unnoticed value-criteria.",2002,10.1109/ISIC.2002.1157872,no
Caregiving effectiveness model evolution to a midrange theory of home care: A process for critique and replication,"A clinically relevant model, grounded in nursing theory, has evolved to become a midrange theory. This article describes the processes used to derive, validate, revise, and test the Caregiving Effectiveness Model. Testing of this midrange theory used prospective longitudinal research with family members caring for patients requiring lifelong, complex, technology-based home care. It presents the conceptual critiques and statistical procedures and discusses derivation of model-generated nursing interventions and implications for use of these validation processes in developing nursing knowledge. The article summarizes limitations of the model and presents recommendations for future research.",2002,,no
"New directions for research in personality and aging: A comprehensive model for linking levels, structures, and processes","A comprehensive lifespan model of personality is proposed that is based in developmental systems theory. The model integrates processes and structures within a level-of-analysis framework. The model builds on McAdams' (1995) delineation of three levels of personality but adds what is seen as necessary process constructs at each of the three structural levels he has identified. At Level I, the trait level, the parallel process construct is that of states. Level II constructs, or personal concerns, have their parallel in self-regulatory processes in service of individual goals. The Level III construct, the life story, has its process counterparts in social cognitive activities related to recounting life narratives, such as remembering, reminiscence, and storytelling. A brief overview of these six foci of personality demonstrates that each are firmly anchored in the aging research literature. It is argued that personality is most clearly revealed in later life and that this model allows examination of personality from both nomothetic and idiographic perspectives. (C) 2002 Elsevier Science (USA). All rights reserved.",2002,10.1016/S0092-6566(02)00012-0,no
Understanding a semiconductor process using a full-scale model,"A full-scale semiconductor manufacturing plant model was developed from a SEMATECH dataset using the computer software package EXTEND. The model was generated to study the complex interactions and characteristics of a semiconductor fabrication process. Equipment downtimes, process flow routes, and machine processing times were used to validate the model. Pilot runs of the model were used to determine simulation run times and data collection rates for the initial inventory and product cycle time measurements. The product cycle time results from the model at 95% capacity were within 63 hours (or 7%) of the SEMATECH cycle time measurements. These results demonstrate the accuracy of the simulation model built from the SEMATECH dataset. The full-scale model was set up to run special scenarios showing the effects of eliminating maintenance and changing product types. The full-scale model was compared to a small-scale model based on the same dataset to demonstrate the inadequacy of the validated small-scale model. A full-scale model is also useful for analyzing scheduling routines, detecting bottlenecks, and understanding machine relations in the semiconductor industry.",2002,10.1109/66.999607,no
"Review of methods for fitting time-series models with process and observation error and likelihood calculations for nonlinear, non-Gaussian state-space models","A key challenge for analyzing fisheries time-series data has been to incorporate sources of uncertainty such as process error, observation error, and model-structure uncertainty. Recent years have seen promising advances, in methods for handling the first two together in a state-space framework, but likelihood calculations for state-space models require high-dimensional integrals, which make their use computationally challenging. The first section of this paper reviews model-fitting methods that use a state-space model structure, including errors-in-variables methods, Bayesian methods that do and do not use the state-space likelihood, and the possibility of classical likelihood analysis with nonlinear, non-Gaussian state-space models. It also discusses the relationship between true likelihood calculations and errors-in-variables likelihoods, as well as the role of Monte Carlo methods in implementing Bayesian and/or state-space model analyses. The second section introduces a numerical method for calculating state-space likelihoods without Monte Carlo methods and gives examples in a classical maximum-likelihood framework. The method is applicable when the dimension of the state space at each time step is low. Although recent advances in model-fitting and analysis methods are promising, inferences from noisy data and complex processes will continue to be variable and uncertain.",2002,,no
Parched-Thirst: development and validation of a process-based model of rainwater harvesting,"A lack of technical knowledge has been identified as one of the primary factors preventing widespread adoption of rainwater harvesting amongst resource-poor farmers in semi-arid Tanzania. Experiments can fill this gap and help in the assessment and design of improved cropping systems, but the transfer of the results in time and space is difficult. A process-based, distributed agro-hydrological cropping systems model (Parched-Thirst) has been developed to enable the transfer of experimental results. It incorporates data pre-processors (climate generator, rainfall disaggregator and pedotransfer functions), soil moisture models, a rainfall-runoff model (including simple unit hydrograph runoff routing), the crop models Parch and Oryza_W, and analysis tools. The development process has been iterative, resulting in a model which is both very user-friendly and closely targeted to the needs of users. Results of a sensitivity analysis of the model suggest that predicted runoff is very sensitive to saturated hydraulic conductivity which is difficult to measure. Validation of model components and the model as a whole has been undertaken by third parties where possible as this lends the results greater credibility. Results suggest that the model is capable of reproducing observed data well over the range of conditions tested. (C) 2002 Elsevier Science B.V. All rights reserved.",2002,10.1016/S0378-3774(01)00187-1,no
Parallel processing and non-uniform grids in global air quality modelling,"A large-scale glob at air quality model, running efficiently on a single vector processor, is enhanced to make more realistic and more long-term simulations feasible. Two strategies are combined: non-uniform grids and parallel processing. The communication through the hierarchy of non-uniform grids interferes with the inter-processor communication. We discuss load balance in the decomposition of the domain, I/O, and inter-processor communication. A model shows that the communication overhead for both techniques is very low, whence non-uniform grids allow for large speed-ups and high speed-up can be expected from parallelization. The implementation is in progress, and results of experiments will be reported elsewhere.",2002,,no
Model-based evaluation of temperature and inflow variations on a partial nitrification-ANAMMOX biofilm process,"A mathematical model describing nitrification (nitritification plus nitratification) and anaerobic ammonium oxidation (ANAMMOX) combined in a biofilm reactor was developed. Based on this model, a previously proposed one-reactor completely autotrophic ammonium removal over nitrite (CANON) process was evaluated for its temperature dependency and behaviour under variable inflow. The temperature-dependency of growth rates of the involved organisms is described by an Arrhenius-type equation. If temperature decreases, the activities of the involved organisms decrease. This means that thicker biofilms are needed or the ammonium surface load (ASL) to the biofilm should be decreased to maintain full N-removal at lower temperatures. Although the growth rate of nitrite oxidisers is higher than that of ammonium oxidisers at lower temperatures, these organisms can be effectively competed out due to a lower oxygen affinity. Variable inflow or dissolved oxygen (DO) concentration negatively affect the N-removal efficiency due to an unbalance between applied ASL load and required oxygen concentration. A variation of the dissolved oxygen concentration in a small range ( +/-0.2g O-2/m(3)) has no significant influence on the process performance, which means that requirements on electrode sensitivity and a DO control scheme are not too stringent. A variable ASL has obvious influence on the process performance, at both constant and variable DO. A good adjustment of DO in accordance with the variable ASL is needed to optimise the N-removal efficiency. At T = 20degreesC, an N-removal efficiency of 88% is possible at ASL = 0.5 g NH4+ - N/m(2) d, in a biofilm of at least 0.7 mm. thickness and a DO level of 3 4 0.3 g O-2/m(3) in the bulk liquid. (C) 2002 Elsevier Science Ltd. All rights reserved.",2002,10.1016/S0043-1354(02)00219-1,no
Iterative error-based nonlinear PLS method for nonlinear chemical process modeling,"A novel nonlinear Partial Least Square (PLS) method is proposed to enhance modeling capability for nonlinear chemical processes. The proposed method incorporates a modified back-propagation algorithm for artificial neural network within Nonlinear Iterative Partial Least Square (NIPALS) algorithm for PLS methods, without deteriorating the robustness of PLS methods. The modified back-propagation algorithm iteratively updates the weights within the networks. The proposed method circumvents the pseudo-inverse calculation of the error-based neural network PLS proposed by Baffi et al. (1999b), and thus makes the weight updating procedure more stable and the solutions more accurate. The modeling capability of the proposed method was investigated through three case studies: 1) a pH neutralization process, 2) cosmetic data, and 3) an industrial crude column process. Simulation results showed that the proposed method represented more stable convergence and enhanced prediction power than those of the linear PLS, the neural network PLS, and the error-based neural network PLS.",2002,10.1252/jcej.35.613,no
Processing of ultra-high molecular weight polyethylene - Modelling the decay of fusion defects,"A problem in applications of ultra-high molecular weight polyethylene (UHMWPE) is the tendency for components to contain fusion defects, arising during processing of the as-polymerized powder. These defects have been implicated previously in failures of UHMWPE load-bearing surfaces, in knee and hip prostheses. Recent work of the authors has recognized two forms of defect: voids (Type 1) and particle boundaries deficient in diffusion by reptation (Type 2). To assist process and product design, a method has now been developed for predicting the decay of severity of Type 2 defects during processing,, for a component of given shape and process history. A new quantifier was introduced for characterizing the progress of diffusion at Type 2 defects in UHMWPE-the maximum reptated molecular mass (M) over bar. This was computed using results from reptation theory, embedded within a Finite Element thermal model of the process. The method was illustrated by simulating compression moulding trials already carried out experimentally by the same authors. It was discovered that (M) over bar never reached the viscosity average molecular mass of the polymer, indicating incomplete boundary diffusion, and explaining the previous observation of Type 2 defects even in fully-compacted, apparently perfect mouldings. The method described has potential as a design tool, especially for optimizing manufacture of UHMWPE prosthesis components.",2002,10.1205/026387602320224003,no
Parameter identification and model validation of adsorption and electron transfer processes using impedance,"A procedure to identify the parameter set of electrochemical models involving adsorption-desorption of intermediates species, employing impedance measurements, is presented. Two steps are considered. First, the theoretical structural identifiability analysis of the model is carried out to know the number of parameter sets. which give the same potential/current model behavior. Second, using steady state and electrochemical impedance measurements, the parameter identification procedure based on interval analysis is carried out to identify one of the possible solutions. Accordingly, all possible parameter sets giving the same potential/current behavior are obtained. The application of structural and parametric identification procedures is discussed for the Volmer-Heyrovsky model associated with the hydrogen evolution reaction (HER). (C) 2002 Elsevier Science Ltd. All rights reserved.",2002,10.1016/S0013-4686(02)00067-1,no
Production of Picea abies in south-east Norway in response to climate change: A case study using process-based model simulation with field validation,"A process-based model was used to simulate biomass production of Norway spruce under both current climate and climate change scenarios. The model was parameterized for Nordmoen in south-cast Norway using real climate data for the period 1987-1989. The model was applied to predict the biomass production responses to three climate change scenarios. The results showed that net primary production (NPP) increased by 7% under an elevated annual mean air temperature of 4degreesC from the current 10.1 t dry mass ha (-1) yr (-1). A doubled current ambient CO, concentration significantly increased NPP by 36%. The scenario of both elevated temperature and elevated CO2 concentration led to an increase in the NPP of 49%, higher than the sum of the two effects acting singly. The results also showed that forest production responses to climate change depend on the conditions of climate used for reference.",2002,10.1080/028275802317221064,no
A prospective study of quit attempts from alcohol problems in a community sample: Modeling the processes of change,"A prospective natural history study was conducted of problem drinkers who were thinking about quitting or reducing their alcohol consumption. Two primary constructs, cognitive appraisals and life events, were measured in a mailed-out baseline survey. A two month follow-up survey identified those who had made reductions in drinking. Regression analyses revealed some support for a cognitive appraisal explanation of change. Respondents who identified more anticipated costs of change were less likely to reduce their drinking. Plans for phase two of this research project - a one year follow-up to identify sustained change attempts - are discussed.",2002,10.1080/16066350290017220,no
Spectra of wavelet scale coefficients from process acoustic measurements as input for PLS modelling of pulp quality,"Acoustic and vibration signals are captured by simple standard accelerometers. These can often be mounted directly on operative process equipment, creating a completely non-invasive measurement system. The signals from the accelerometer are then amplified, digitized by an analogue-to-digital converter and stored in some suitable format in a PC. The method most often used for signal processing of acoustic data has been to apply variants of fast Fourier transform (FFT) on sampled data to produce a frequency domain representation. An alternative way tried here is to use the fast wavelet transform (FWT) in combination with FFT. The FWT has the advantage that it produces time-resolved representations and, on each time scale, different features can be extracted. However, in this case, time resolution has no meaning, since the starting points for data acquisitions were not fixed. The wavelet step can be seen as a series of pre-filters and it is here followed by FFT on coefficients at each wavelet scale. The results are compared to those obtained after FFT on the complete time series. We have used spectra of wavelet scale coefficients in an attempt to model pulp quality with PLS. In this case the number of points in the resulting wavelet multiresolution spectrum (WT-MRS) can be limited to a low number, e.g. 255 compared to 1025 with direct FFT on the time series. In the PLS modelling step the advantage is that the first two components describe Y much better than when using the conventional approach, e.g. 72% explained Y variance compared to 40%. A second advantage is that the model requires fewer coefficients. Copyright (C) 2002 John Wiley Sons, Ltd.",2002,10.1002/cem.731,no
Understanding and modelling flexibility in administrative processes,"Aiming to provide a platform for collaboration across agencies and to design appropriate IT support for the variety of administrative processes and decision making, concepts need to go beyond current approaches in business process modelling as well as workflow and record management. Drawing on these approaches, we suggest to focus on the unique tasks and activities of each actor involved and to present the relation of each individual contribution to the overall process as something tangible in order to support flexibility in the execution of administrative processes.",2002,,no
Validation of authentic performance assessment: A process suited for Rasch modeling,"An evaluation was conducted in order to develop, test, and validate a method for assessing critical thinking learning outcomes as related to scientific literature evaluation in a PharmD program. A 25-item authentic assessment was created using recommended guidelines and Bloom's hierarchical taxonomy. Construct and content validity were supported using classical test theory and Rasch modeling. A primer on using Rasch analysis is provided in conjunction with the specific results. Rasch analysis demonstrated good INFIT and OUTFIT statistics, with item difficulty ranging from -1.90 to 2.46 logits for 23 items. The KR20 for the assessment was 0.41 and item point-biserial correlations ranged from -0.05 to 0.25. Rasch modeling techniques provided information to evaluate individual item contributions not discerned by standard techniques such as KR20 or point-biserial correlations. This research supports the practical application of this type of authentic assessment and provides evidence for construct and content validity in measuring student performance.",2002,,no
Experimental evaluation of combined model reference adaptive controller in a pH regulation process,"An experimental evaluation of the combined model reference adaptive control (CMRAC) is presented in this paper. This adaptive control scheme was used to control a relatively complex process like the pH of a solution in a tank reactor at laboratory level. For comparison purposes, some very well-known control strategies were also implemented, which include PID control and standard model reference adaptive control (M RAC). Tracking and regulation capabilities of the control strategies studied were analysed and compared. Experimental results indicate that CMRAC behaves as well as the standard MRAC and a very well-tuned PID for a specific and known operating point. Advantages of the adaptive controllers are shown when the operating point changes. Copyright (C) 2002 John Wiley Sons, Ltd.",2002,10.1002/acs.674,no
Anti-CEA monoclonal antibody: Technetium-99m labeling and the validation process of a scintigraphic animal model with a non-cellular antigenic implant,"Animal models are currently used to verify the biodistribution of different radiopharmaccuticals before its clinical application in Nuclear Medicine; however, there may be some limitations. The utilization of labelled anti-tumor monoclonal antibodies (MoAb) in experimental models often requires implant of human antigens (usually a cellular implant), which cannot be achieved in immunocompetent animals. Our purpose was to label an anti-CEA MoAb with technetium-(99)m (Tc-99m) and to validate a simplified animal model using a non-cellular antigenic implant. MoAb was directly labelled with Tc-99m, after reduction with 2-mercaptoethanol. Labeling efficiency was checked by ascending chromatography and immunoreactive fraction was measured in plastic wells sensitized with the antigen. Radiopharmaccutical biodistribution was evaluated by dissection and scintigraphy in 5 mice groups; following the subcutaneous administration of Al(OH)(3), CEA adsorbed Al(OH)(3) and a control group evaluation. Labeling efficiency was 94 +/- 3%, which showed to be stable for 24 hr, with immunoreactive fraction above 50%. Invasive biodistribution evaluation showed prolonged blood retention, hepatic and renal uptake. A significant increase in uptake was observed in scintigraphic studies of animals with CEA-adsorbed Al(OH)(3) implants compared with the other groups (p<0.05). The non-cellular antigenic implant model simplifies the pre-clinical evaluation of labelled MoAb.",2002,,no
Measurements and modeling of the phase behavior of ternary systems of interest for the GAS process: I. The system carbon dioxide plus 1-propanol plus salicylic acid,"As a representative model system for the gas-anti-solvent (GAS) process, the phase behavior of the ternary system carbon dioxide+ 1-propanol + salicylic acid has been studied experimentally. For this purpose, carbon dioxide has been chosen as the anti-solvent gas, 1-propanol as the organic solvent, and salicylic acid (2-hydroxy benzoic acid) as the model drug. In each experiment, a solution of salicylic acid in I-propanol was expanded using carbon dioxide as the anti-solvent. A synthetic method was used for measuring bubble point curves, and the solid (salicylic acid)-liquid boundaries. Three-phase equilibrium data solid (salicylic acid)-liquid-vapor were obtained from intersection of two-phase isopleths vapor-liquid and solid-liquid. Results are reported for this ternary system at carbon dioxide concentrations ranging from 8.0 to 90.6 mol%, and within temperature and pressure ranges of 273-367 K and 1.0-12.5 MPa, respectively. It has been observed that the carbon dioxide concentration significantly affects the optimum operational conditions of the GAS process, i.e. at lower concentrations carbon dioxide acts as a co-solvent, while at higher concentrations it acts as an anti-solvent. Also, it is shown that at a proper temperature, it is possible to precipitate most of the dissolved solute with only a small change of the pressure. The Peng-Robinson equation of state as modified by Stryjek and Vera (PRSV EOS) has been used to model the ternary system. (C) 2002 Elsevier Science B.V. All rights reserved.",2002,10.1016/S0896-8446(02)00006-2,no
Modeling comprehension processes in software development,"As programs become more complex and larger, the sheer volume of information to be comprehended by developers becomes daunting. Software development is fraught with complexity that is difficult to identify a priori. Complexity is relative to the task, the developer's experience and the resources available. In this research, we identify comprehension processes applied in software development, the cognitive loads associated with these processes. We present an abstraction of the cognitive environment of the software developer, and introduce techniques to minimize the cognitive effort in the short-term and the long-term.",2002,10.1109/COGINF.2002.1039314,no
Applying a novel statistical process control model to platelet quality monitoring,"BACKGROUND: Many countries are implementing universal WBC reduction of blood components Thus, manufacturing procedures must include OC techniques to detect units that fail to meet established standards. STUDY DESIGN AND METHODS: A statistical process control model, based on the exponentially weighted moving average of the cumulative distribution function (CDF-EWMA), was developed to detect shifts in a mean and/or variance of a process. The model's parameters (weights) were optimized to maximize detection of an out-of-control process while minimizing sensitivity to autocorrelation. Validation was performed using a retrospective set of WBC-reduction data obtained from a blood bank. The WBC-reduction process was considered in control when there was 95-percent confidence that more than 95 percent of platelet concentrates would contain less than 1 x 10(6) WBCs (6.0 log WBC) as required by European standards. A sentry setting of 5.7 log WBCs was used to allow earlier detection of an out-of-control process. RESULTS: Graphic output of the CDF-EWMA model provided a continuous update of the probability that a WBC-reduction process was in control. Using the validation data, the model showed that the process was in control until Observation 332, at which point residual WBCs per unit increased. However, the first platelet concentrate to exceed specified criteria (Observation 346) occurred after the model detected that the process was out of control, demonstrating the forecasting value of this model. This deviation corresponded to an equipment failure in a single apheresis instrument. The Shewhart and EWMA techniques were similarly able to detect when the process was out of control using the test data. CONCLUSION: As a statistical process control model, the CDF-EWMA provides real-time estimation of the fraction of components meeting a regulatory limit. It is capable of detecting developing QC problems before units fail to meet regulatory requirements and is a potential alternative to other QC techniques for monitoring WBC reduction of blood components.",2002,10.1046/j.1537-2995.2002.00168.x,no
Towards the comprehension of oxygen storage processes on model three-way catalysts,"CeO2- and CeO0.63Zr0.37O2-supported noble metal catalysts were studied. Samples were fully characterized using TEM, XRD, N-2 adsorption and H-2 chemisorption. The oxygen storage process was investigated focusing on the evolution as a function of temperature of both the oxygen storage capacity (OSC) and the oxygen storage complete capacity (OSCC). Aging effect on OSC was also examined in details in the case of Rh catalysts. Finally, the major role of oxygen diffusion, partly influenced by the metal/support interface quality, was confirmed. (C) 2002 Elsevier Science B.V. All rights reserved.",2002,10.1016/S0920-5861(02)00005-6,no
Guideline validation in multiple trauma care through business process modeling,"Clinical guidelines can improve the quality of care in multiple trauma. In our Department of Trauma Surgery a specific guideline is available paper-based as a set of flowcharts. This format is appropriate for the use by experienced physicians but insufficient for electronic support of learning, workflow and process optimization. A formal and logically consistent version represented with a standardized meta-model is necessary for automatic processing. In our project we transferred the paper-based into an electronic format and analyzed the structure with respect to formal errors. Several errors were detected in 7 error categories. The errors were corrected to reach a formal and logically consistent process model. In a second step the clinical content of the guideline was revised interactively using a process modeling tool. Our study reveals that guideline development should be assisted by process modeling tools, which check the content in comparison to a meta-model. The meta-model itself could support the domain experts in formulating their knowledge systematically. To assure sustainability of guideline development a representation independent of specific applications or specific provider is necessary. Then clinical guidelines could be used for training, process optimization and workflow management additionally.",2002,,no
Gettering efficiencies and their dependence on material parameters and thermal processes: How can this be modeled?,"Driving forces of gettering reactions can be structured in a cause-effect diagram. For industrial applications, the classification of causes requires quantitative data on gettering efficiencies (GE), which can be provided by analyzing spiked wafers by using the ultra-trace profiling/inductively coupled plasma mass spectrometry (UTP/ICP-MS) method. Segregation gettering of 3d-metals in p/p+ and n/n+ epitaxial wafers was modeled by summarizing the stabilization energies of donors (p+), acceptors (n+) and dopant-metal pairs. Calculations and experimental findings were in quantitative agreement. GE of wafers with poly-silicon backsides were measured for Cu and modeled by a kinetic model within the uncertainty of the measurement. GE of bulk micro defects (BMDs) for Cu and Ni were measured for different thermal treatments and compared with a reaction- limited gettering mechanism, thus, a dependence of the GE on the total inner surface (TIS) of the BMDs. The TIS was reliably obtained by a combination of measured delta Oi values and modeled BMD-sizes. Our results of Ni and Cu gettering agree with a reaction limited mechanism, and the obtained threshold levels agree with the recently published value of Sueoka. et. al. The influence of the contamination method or the crystal growth technique on the gettering efficiencies is understood qualitatively, but still lacks a quantitative model.",2002,,no
Process-based modelling of the climatic forcing of fluvial sediment flux: some examples and a discussion of optimal model complexity,"During the complex and highly dynamic climate of the Late Quaternary, precipitation and temperatures were highly variable and generally did not change synchronously. Reconstructed fluvial sedimentological response to this climatic forcing was previously shown to be complex. This response can be understood only by accounting for all the relevant catchment processes. This paper reviews a number of simple numerical models that aim to enhance our understanding of the role of different processes in this climate-sedimentology system. The applicability of these models depends on the scale of the models involved, the required input data and the way that parameters are estimated. Simple process-based models perform best on Quaternary timescales because their data needs are low and parameters can be interpreted in physical terms, which enables an a priori estimation. Process-based models of fluvial dynamics may provide a methodology for the understanding of the mechanisms responsible for the dynamics of sediment flux to basins.",2002,10.1144/GSL.SP.2002.191.01.13,no
New product development process: Proposal for an innovative design modelling framework including actors evaluation of innovation costs and value,"Firms are facing very short and important innovation cycles, particularly in IT and Telecommunication sectors. Then a question appears: why do some innovations succeed whereas other fail. From offer's point of view, a way could be to evaluate impacts of a decision to innovate for each of the actors involved in this product trajectory. Therefore the goal of such an approach is reducing high Innovation development risks by integrating the diverse stakes of life cycle actors and by helping design teams to integrate the evolution of some key environmental processes. We introduce in this paper the characteristics of the Innovation Process and Engineering Design Phase for high level innovations. In this framework, we propose an Innovation Valuation Model integrating strategic and tactic impacts in term of value and cost.",2002,,no
Scale effects of Hortonian overland flow and rainfall-runoff dynamics: Laboratory validation of a process-based model,"Hortonian runoff was measured in the laboratory from uniform slopes of lengths of 1.5, 3.0, and 6.0 m for steady, high-intensity rainstorms with durations of 1.0 to 7.5 min. A clear reduction in runoff per unit slope length was found as slope lengths were increased. This effect becomes more pronounced with decreasing storm duration. The runoff data were used to validate a simple process-based model that combines the Philip-two-term infiltration equation with the kinematic wave overland flow principle. The predicted and experimental results agreed well. Laboratory findings were extrapolated with the aid of the model to slopes and rainfall durations similar to those found under West African conditions. The calculated reduction of runoff per unit length is similar to reported observations. Thus, this process-based model can largely explain the phenomenon of runoff reduction with increasing slope length. Copyright (C) 2002 John Wiley Sons, Ltd.",2002,10.1002/esp.356,no
Validation in process-like conditions of the kinetic and thermophysical modeling of a dicyanate ester/glass fibers composite,"In a previous paper, the kinetic behavior of a cyanate ester trimerization was studied. In this paper, the thermophysical properties (heat capacity, thermal conductivity) are measured and modeled (as a function of temperature and conversion) for the neat resin and for a glass fibers composite. These models (thermophysical and kinetics) are used to simulate the thermal transfers in an instrumented heated mold. The calculated local temperatures and surface heat fluxes appear to be in very good agreement with measurements, for both the neat resin and the composite. (C) 2002 Elsevier Science B.V. All rights reserved.",2002,10.1016/S0040-6031(02)00040-0,no
Problems in modeling evolutionary processes in close binaries,"In modeling the evolutionary processes of interacting binaries, a number of assumptions are invoked by many workers concerning the manner of the mass flow between the two stars. This article reviews the validity of the four assumptions most commonly used. namely, assumptions concerning (A) the critical Roche equipotential surface (the so-called Roche lobe), (B) accretion of matter from one star to its companion. (C) mass loss from the binary system, and (D), which is related to the last two items, whether the mass flow in a binary system is conservative or not. A large number of workers in this field appear to assume that the critical Roche equipotential surfaces (Roche lobes, in their lexicon) control the mass flow with the Lagrangian 1 point acting like a nozzle, channeling a rather well collimated jet of gas. Similarly, they also assume that accretion 'disks' are formed in this process and those 'disks' have properties very similar to solid disks with well defined surfaces and with precession, nutation etc. Finally. they assume, except in the most extreme case of rapid mass flow. conservative mass flow in which all mass lost by one star is completely accreted by the companion. These commonly invoked assumptions are critically examined from both observational data and relevant physics and are shown to be lacking realistic justification. We show what the concept of Roche equipotential surfaces actually tells us, and examine the manners of actual mass flow observed in selected binaries. (C) 2002 Elsevier Science B.V. All rights reserved.",2002,10.1016/S1387-6473(01)00152-X,no
An intelligent knowledge processing system on hydrodynamics and water quality modeling,"In order to aid novice users in the proper selection and application of myriad ever-complicated algorithmic models on coastal processes, needs arise on the incorporation of the recent artificial intelligence technology into them. This paper delineates an intelligent knowledge processing system on hydrodynamics and water quality modeling to emulate expert heuristic reasoning during the problem-solving process by integration of the pertinent descriptive, procedural, and reasoning knowledge. This prototype system is implemented using a hybrid expert system shell, Visual Rule Studio, which acts as an ActiveX Designer under the Microsoft Visual Basic programming environment. The architecture, solution strategies and development techniques of the system are also presented. The domain knowledge is represented in object-oriented programming and production rules, depending on its nature. Solution can be generated automatically through its robust inference mechanism. By custom-built interactive graphical user interfaces, it is capable to assist model users by furnishing with much needed expertise.",2002,,no
Modeling of the metal powder compaction process using the cap model. Part I. Experimental material characterization and validation,"In order to produce crack free metal powder compacts that respect both the dimensional tolerances and the mechanical strength requirements, both tooling design and compaction sequence have to be adequately determined. The finite element method, through the use of an appropriate constitutive model of the powder medium, has recently been used as an efficient design tool. The accuracy of this method highly depends on the faithfulness of the constitutive model and the quality of the material parameter set. Furthermore, in order for the simulation results to be reliable, they should be experimentally validated on real parts featuring density variations. Hence, the main concerns of this paper are the development of a standard calibration procedure for the cap material model as well as the development of a reliable technique for the experimental validation of the powder compaction simulation results. The developed calibration procedure, applied for the case of 316L stainless steel powders, is based on a series of isostatic, triaxial and uniaxial compaction tests as well as resonant frequency tests. In addition, a sensitivity study was performed in order to determine the relative importance of each factor and basic simulations served to validate the parameter set extraction procedure. On the other hand, a local density measurement technique was developed for the experimental validation of the model results. This technique is based on correlation with Vickers macro-hardness. Finally, an application featuring the compaction of a 316L stainless steel cylindrical component is presented to illustrate the predictive capabilities of the cap material model as well as the accuracy of the acquired material parameter set. (C) 2002 Published by Elsevier Science Ltd.",2002,10.1016/S0020-7683(01)00255-4,no
Measurement and modelling of the film casting process 2. Temperature distribution along draw direction,"In the case film process a polymer melt is extruded through a slit die, stretched in air and cooled on a chill roll. During the path in air the melt cools and a reduction of both thickness and width takes place; obviously, temperature distribution, thickness and width reductions are function of draw ratio and stretching distance. Temperature distribution along the draw direction was measured as function of flow rate during film casting experiments performed with an iPP resin. A non-contacting method of measurement, based on a narrow-band IR pyrometer, was adopted. A good qualitative agreement is shown between experimental temperature data and predictions of a model accounting of radiation emissivity dependence upon film thickness. Differences are consistent with discrepancies of film thickness evolution along draw direction, indeed the model slightly over predicts both film thickness reduction and, parallel, temperature decrease along the draw direction. (C) 2002 Elsevier Science Ltd. All rights reserved.",2002,10.1016/S0009-2509(02)00098-2,no
Methods of problem-oriented representation and data processing in resources of the hardware support of intellectual systems,"In the report the conceptual problems of rise of productivity and reliability of specialized computing devices (SCD) of intellectual systems are considered on the basis of representation and data processing with the help various problem-oriented machine arithmetics (POMA). The directions of development of methods of representation and data processing are considered, and also the new methods are offered and the known methods are essentially advanced.",2002,10.1109/ICAIS.2002.1048174,no
A modelling and control structure for product quality control in climate-controlled processing of agro-material,"In this paper a modelling and control structure for product quality control is presented for a class of operations that processes agro-material. This class can be characterised as climate-controlled operations, such as storage, transport and drying. The basic model consists of three parts. These are the quality and behaviour of the product, and its direct and indirect environment. This decomposition is reflected in the proposed control structure. The significance of the explicit inclusion of product behaviour is that the control is much more geared to the demands of the product. The applicability and potential advantages of the proposed structure are illustrated in different industrial cases, such as the storage of potatoes and the transport of apples in reefer-containers. (C) 2002 Elsevier Science Ltd. All rights reserved.",2002,10.1016/S0967-0661(02)00003-5,no
Markov models for the physical layer block error process in a WCDMA cellular system.,"In this paper we investigate the possibility of using Markov chains to model the error process in the data blocks delivered by the physical layer of a WCDMA (Wideband Code Division Multiple Access) cellular system. Starting from the results on the error statistics obtained from a suitable simulation tool, which jointly performs system and link level analysis, we first classify the-users on the basis of performance level and burstiness, then we provide some guidelines for the design of Markov models in the different system and channel conditions. The performance of an ARQ (Go-Back N) protocol at the link layer is taken as a reference to test the reliability of the proposed models. It is shown that the perspective of using simple error models in the analysis of upper-layer protocol is feasible in many cases.",2002,,no
A genetic neural fuzzy system-based quality prediction model for injection process,"In this paper, a genetic neural fuzzy system (GNFS) is presented and a hybrid learning algorithm divided into two stages is proposed to train GNFS. During first learning stage, Genetic algorithm is used to optimize the structure of GNFS and the membership function of each fuzzy term because of its capability of parallel and global search. On the basis of optimized training stage, the back-propagation algorithm (B-P algorithm) is chosen to update the parameters of GNFS to improve the system precision. The proposed GNFS is used to predict the weight of modeled part in injection process. The process of constructing quality prediction model for injection process based on GNFS is introduced. The results predicted by the constructed model show it can perform very well. The comparison between the presented GNFS and the other model based on regression and the neural network is made. The comparison verifies the proposed GNFS has superior performance and good generalization capability and also can apply to other industrial process. (C) 2002 Elsevier Science Ltd. All rights reserved.",2002,10.1016/S0098-1354(02)00092-3,no
A model structure for product quality in processing agro-material for process control purposes,"In this paper, a model structure is presented that captures product behaviour with respect to quality properties. Modelling product quality properties involves nominal (bulk) dynamic behaviour and the variation of these properties. Nominal behaviour is modelled using a limited number of basic reactions. To deal with the variation the presented model structure is extended with a three-step approximation procedure using discretised intervals. The model structure is suitable for control purposes and will contribute to closing the gap between product specialists and the system and control community. The applicability of the model structure and the possibility to describe quality properties is shown with existing models from the literature that show a good fit with the described model structure and by an industrial case study on potato storage. (C) 2001 Elsevier Science Ltd. All rights reserved.",2002,10.1016/S0260-8774(01)00052-8,no
Understanding weld modelling processes using a combination of trained neural networks,"In this paper, a set of neural networks has been trained for weld modelling processes with different architecture and training parameters. The set of neural networks is trained using actual weld data available in the literature. The performance of each neural network in this set is defined by two performance measures of interest, namely training error and generalization error. Instead of using one of the best networks from this set of trained networks, a method of combining the outputs of all the network from the set is proposed and is called the combined output (or output of the combined network). It is shown that the performance measures of interest obtained using this combined output is better than the performance measures of interest obtained by all the individual neural networks in the set.",2002,10.1080/00207540110091190,no
Cost-risk analysis in statistical validating simulation models of service processes,"In this paper, for validating computer simulation models of service processes, an uniformly most powerful invariant (UMPI) test is developed from the generalized maximum likelihood ratio (GMLR). This test can be considered as a result of a new approach to solving the Behrens-Fisher problem when covariance matrices of two multivariate normal populations (compared with respect to their means) are different and unknown. The test is based on invariant statistic whose distribution, under the null hypothesis, does not depend on the unknown (nuisance) parameters. The sample size and threshold of the UMPI test are determined from minimization of the weighted sum of the model builder's risk and the model user's risk. The proposed test could result in the saving of sample items, if the items of the sample are observed sequentially. In this paper we present the exact form of the proposed curtailed procedure and examine the expected sample size savings under the null hypothesis. The sample size savings can be bounded by a constant, which is independent of the sample size. Tables are given for the expected sample size savings and maximum sample size saving under the null hypothesis for a range of significance levels (a), dimensions (p) and sample sizes (n). The curtailed test considered in this paper represents improvement over the noticurtailed or standard fixed sample tests. Copyright (C) 2001 IFAC.",2002,,no
The relationship of input and output phonological processing: An evaluation of models and evidence to support them,"In this paper, we review studies of the relationship between input and output phonological processing and discuss the means by which an interactive activation model that assumes a single phonological network or functionally connected input and output phonological networks could account for apparent dissociations of these two pathways. Following this, we report data from 24 aphasic subjects with word processing deficits that indicate associations between input and output phonological processing. Input phonological measures correlated with output phonological measures, but not with output lexical-semantic measures. Input lexical-semantic measures did not significantly correlate with any of the output measures. We identified one subject, EF, who did not show this overall pattern. She performed well on two measures of phonological input processing (discrimination and rhyme judgements), but produced a high rate of phonological errors in picture naming. On an auditory lexical decision task, however, EF produced a high rate of false alarm errors (misperception of nonwords as words). False alarm errors have been attributed to a disturbance in input phonological processing. Consistent with this hypothesis, the rates of false alarm errors made by this group of subjects on the same auditory lexical decision task correlated with (1) input tasks that require maintaining activation of phonological representations and (2) a measure of output phonological processing (rates of phonologically related nonword errors in picture naming). These results are discussed with reference to current approaches to the study of input and output phonological processing and possible future investigations of this question.",2002,10.1080/02687040143000447,no
P removal from anaerobic supernatants by struvite crystallization: long term validation and process modelling,"In this work, a model for phosphorus crystallization in a fluidized bed reactor, able to describe the experimental results obtained during a semiscale pilot plant, is presented. In particular, the validity of the model proposed has been evenly extended with respect to a previous experiment, even at a lower contact time, and the length of each experiment has been increased, in order to verify the behaviour of the process for long term applications and to evaluate the maximum crystal growth of the system. Moreover, the state of the art of the available processes for phosphorus removal from wastewater is presented, together with a detailed review of the several models so far developed to describe the phosphorus crystallization mechanisms. (C) 2002 Elsevier Science Ltd. All rights reserved.",2002,10.1016/S0043-1354(01)00401-8,no
Quality characteristics of a model biscuit containing processed mango (Mangifera indica) kernel flour,"Mango (Mangifera indica) kernel was processed to flour and incorporated at 50% level of wheat flour substitution into biscuit (PMKWB) preparation. PMKWB was comparatively analysed with a commercial whole wheat biscuit (BRSCB) for several storage characteristics at 31 +/- 2degreesC and rate of peroxidation at 14, 31 and 58 +/- 2degreesC. Results showed that both biscuits had similar composition except in fat content. PMKWB was slightly more acidic than BRSCB due to the associated polyphenolic matter in the processed kernel flour used. BRSCB had more but negligible microbial load than PMKWB. On a 9-point hedonic scale, both biscuits were scored between 6.0 and 8.5 for attributes assessed though BRSCB scores were statistically higher (p < 0.05) than those of PMKWB. Both biscuits exhibited sigmoid-shaped Moisture adsorption isotherms with calculated monolayer moisture values of 4.0-4.5% (dry basis) using Brunauer-Emmett-Teller (BET) equation, and upper stability limits of 8.5-8.7% using the Henderson equation (rel. humidity, ca 52%). Both mathematical models fitted the biscuit isotherms well (rel. deviation, 3.0 to 4.5%). Peroxide values on the 90th day were 13, 21 and 56 meq/kg of PMKWB sample at 31, 14 and 58degreesC respectively. It was concluded that hygienically and commercially produced PMKWB would be shelf-stable for 3 months at temperatures not exceeding 31degreesC.",2002,10.1081/JFP-120005783,no
SODPM: a sequence-oriented decision process model for unstructured group decision problems,"Many approaches to the solutions of modern unstructured decision problems mainly involve modelling, information technology and group behaviour. The work of group decision- making can be viewed as a type of process plan that is reflected by its problem structure within which the thinking space is wide and innovative. This paper presents a Sequence- Oriented Decision Process Model (SODPM) that is based on the defined sequence of problem elements to help solve unstructured problems. A GDSSP (Group Decision Support System for Personnel Promotion) that embeds a predefined mechanism to perform decision process, decision model and decision choice is developed to experimentally demonstrate the SODPM. Empirical evaluation was conducted to derive the results for the research problems that include efficiency and group satisfaction. There were remarkable results: the SODPM can serve as a facilitative vehicle for opinion convergence and group satisfaction is highly positive. While the demonstrated domain for the example presented is personnel promotion, the proposed SODPM can be utilized to help solve the similar class of unstructured problems in other domains also.",2002,10.1080/01449290210121842,no
Integrating the safety dimension into quality management systems: a process model,"Many practitioners advocate that achieving quality and safety performance can help organizations foster a competitive edge. This is attributable to the minimization of financial loss, compliance with legislation, effective allocation of quality and safety responsibilities, and promotion of community goodwill. This paper discusses the factors affecting safety-quality integration in quality management systems. It also compares the compliance requirements of OHSAS 18001 occupational health and safety management standard with that of ISO 9001 (quality) and ISO 14001 (environmental) management standards. Incorporating the empirical findings from a recent study in Hong Kong, the paper examines the hypothesized links among company size, safety/quality awareness and interrelationship of various factors affecting safety/quality management practices. Finally, a process model of safety-focused quality management (SQM) is proposed along with an implementation guideline. The model is generic in nature and can be applied and modified to suit organizations of different business nature, sizes and locations.",2002,10.1080/09544120220135246,no
Modelling the data measurement process for the index of production,"Many statistical series that are available from official agencies, such as the Office for National Statistics in the UK and the Bureau of Economic Analysis in the USA, are subject to an extensive process of revision and refinement. This feature of the data is often not explicitly recognized by users even though it may be important to their use of the data. The starting-point of this study is to conceptualize and model the data measurement process as it is relevant to the index of production (IOP). The IOP attracts considerable attention because of its timely publication and importance as an indicator of the UK's industrial base. This study shows that there is one common stochastic trend (and one common factor in terms of observable variables) 'driving' 13 vintages of data on the IOP. Necessary and sufficient conditions are derived for the 'final' vintage of data on the IOP to be the permanent component of the series in the Gonzalo-G ranger sense, and the revisions to be the transitory components. These conditions are not satisfied for the IOP; hence, the permanent component is a function of all the published vintages.",2002,10.1111/1467-985X.00622,no
Understanding and modeling the processing-mechanical property relationship of bread crumb assessed by indentation,"Measuring fundamental mechanical parameters such as Young's modulus and critical stress is a straightforward and valid approach to evaluating the physical texture of breadcrumb. The objectives of this study were to evaluate whether such fundamental mechanical properties could be measured by indentation techniques such as the AACC crumb firmness method, and then to alter breadmaking conditions so as to model the relationship between these indentation mechanical properties as a function of crumb moisture content and crumb density. Bread was baked according to a short dough process using Canadian western red spring (CWRS) wheat flour. Factors considered in the design of experiments were proofing time, water absorption, crosshead speed, and in-denter diameter. Young's modulus and critical stress, measured with 12-and 20-mm, cylindrical indenters, were well covalidated with those obtained from a standard compression test. With increases in proofing time and water absorption, a more porous and compliant bread texture led to decreasing Young's modulus and critical stress. Our results revealed a good mapping of mechanical properties to crumb moisture content and density that were correlated to breadmaking conditions, thus permitting more precise prediction of the mechanical properties that determine bread texture.",2002,10.1094/CCHEM.2002.79.6.763,no
An approach for model validation in simulating sheet metal flanging processes,"Model validation has become an increasingly important issue to establish the trustworthiness of models in the era where numerical simulations have demonstrated their benefits in reducing development time and cost. Advanced numerical methods are continuously developed for solving more complex and challenging problems, and one of thorn is the sheet metal forming process, which involves large deformation, contact between tooling and sheet materials, nonlinear plasticity, and process uncertainties, etc. Therefore, any numerical simulation method is inevitably asked about its validity. This paper studied one of the typical shoot metal forming processes, flanging process, and examined the validity of two numerical models in predicting the final geometry or springback. The corresponding uncertainties in both material properties and process parameters were investigated and inputted to the propagation of uncertainty analysis, where metamodels were developed to effectively and efficiently compute the total uncertainty of the final geometry. Three validation frameworks were used to test the model validity for the sample flanging process. It was found that different conclusions could be drawn from different approaches. Therefore, it is highly recommended to take a combined approach for model validation.",2002,,no
Modified soul: A culturally sensitive process model for helping African-Americans achieve dietary guidelines for cancer prevention,"Modified Soul was tested over a 12-month period with a focus group of 10 African-American women, recruited through a social services organization serving an urban public housing project. Methods included focus group interviews (n = 4), observations and chemical analysis of traditional (n = 10) and modified soul food cooking practices (n = 10), and sensory evaluation discussions (n = 10). Four quarterly community center taste-test events were successfully planned and executed by the focus group. Over half of 21 dishes reduced in fat content by the focus group received taste-test ratings of ""very good"" from participants (it = 500). Based on AOAC (1984) methods, mean fat contents (grams per 100g portions) of modified vegetables (n = 4) and meats (n = 7) were reduced by 68% (from 7.7 +/- 4.7 to 2.5 +/- 1.6) and 49% (from 15.0 +/- 4.4 to 7.7 +/- 4.4), respectively (p < 0.001). Modified Soul has potential for empowering low-income African-Americans to reduce diet-related cancer risk in their communities.",2002,,no
Local semantics for the part-whole problem: An alternative CA model based on unserializable parallel processing,"On one hand, we can observe and describe behaviors of physical (non-living) systems as the machinery, where the systems obey certain rules in the time evolutions. On the other hand, many behaviors of living systems can be observed as if they cannot be described as mechanical processes on their own right. Does this confrontation between living and non-living systems suggest that the process of living systems contains something special that is essentially different from the mechanical (computational) process? In this paper, we propose the concept 'weak computation', and argue that processes of living and non-living systems are not essentially different when we look at both of them from the viewpoint of 'the weak computation'. We consider that such weak computations are executed in 'unserializable parallel processing systems (i.e., parallel processing systems irreducible to serial processing systems)'. In order to model 'the unserializable parallel processing', we modify the elementary cellular automata [1,2] as their local interaction can be not only reference but also interference.",2002,,no
"Models, algorithms, and software for data processing in measurements: Quality evaluation","Quality of measurement as a Complex system of various elements is investigated in several aspects. The mathematical models arc particularly studied. The first group includes models of objects Under investigation, models for measurement results and errors, measuring instruments characteristics. The models of transformations form another group, including algorithms and software for data processing,. Complete mathematical model of measurement is formulated as applied to the data processing stage. Several kinds of non-classic technique are discussed, including reprodicing kernel Hilbert space approach. Measurement quality analysis is based on the error decomposition and investigation in several aspects. Quality of algorithms and software is estimated in accordance with their role in measurement. The quality of the algorithms may Lie described by accuracy, stability and complexity characteristics: quality of the rnedia-resident software is described by this se with addition of the specific characteristics. The main task of the model quality analysis is the adequacy estimation. So a set of adequacy characteristics is formulated and investigated. General principles are derived for these groups of quality characteristics to be estimated.",2002,,no
Validated cost models for parallel OQL query processing,"Query cost models are widely used, both for performance analysis and for comparing execution plans during query optimisation. In essence, a cost model predicts where time is being spent during query evaluation. Although many cost models have been proposed, for serial, parallel and distributed database systems, surprisingly few of these have been validated against real systems. This paper presents cost models for the parallel evaluation of ODMG OQL queries, which have been compared with experimental results obtained using the Polar object database system. The paper describes the validation of the cost model for a collection of queries, using three join algorithms over the 007 benchmark database. The results show that the cost model generally both ranks alternative plans appropriately, and gives a useful indication of the response times that can be expected from a plan. The paper also illustrates the application of the cost model by highlighting the contributions of different features and operations to query response times.",2002,,no
An enquiry into rainfall data measurement and processing for model use in urban hydrology,"Rain data are collected all over the world because water is of paramount importance to all human life. WMO has provided standards for collection and standardized data processing of daily rainfall measurements, Currently no such standards are available for gauges with a resolution suitable for urban hydrology, where the resolution in time must not exceed a few minutes. The Group on Urban Rainfall under the International Water Association has made a comparison of national standards by means of a survey of 77 questions sent to 44 countries. The paper discusses the first results of the answers of the survey. Currently tipping bucket gauges are the dominating method of obtaining high resolution rain data, but the numbers of weighing gauges and radar measurements are rapidly growing. It is necessary to try to increase the awareness of documentation of current standards and to agree on standards for measurements and data processing on an international level in the future.",2002,,no
Model layout optimization for solid ground curing rapid prototyping processes,"Rapid prototyping technologies are capable of directly manufacturing physical objects from CAD models and have been increasingly used in product development, tool and die making and fabrication of functional parts. Solid ground curing (SGC) technology, one of the rapid prototyping technologies, is suitable of building multiple parts with different geometry and dimensions in batch production of rapid prototypes to minimize the cost of prototypes. However, the layout of CAD models in a graphic environment is time-consuming. Because of high cost of the resin, the layout of models in a batch is critical for the success of the SGC operations in any industrial environment. This paper presents the layout optimization using simulated annealing techniques. A software system was developed to assist Cubital operators to layout CAD models with various geometric shapes. The system accepts STL files from any solid modeling environment. Several examples are provided to illustrate the techniques and effectiveness of the approach. (C) 2002 Elsevier Science Ltd. All rights reserved.",2002,10.1016/S0736-5845(01)00022-9,no
Evaluation of six process-based forest growth models using eddy-covariance measurements of CO(2) and H(2)O fluxes at six forest sites in Europe,"Reliable models are required to assess the impacts of climate change on forest ecosystems. Precise and independent data are essential to assess this accuracy. The flux measurements collected by the EUROFLUX project over a wide range of forest types and climatic regions in Europe allow a critical testing of the process-based models which were developed in the LTEEF project. The ECOCRAFT project complements this with a wealth of independent plant physiological measurements. Thus, it was aimed in this study to test six process-based forest growth models against the flux measurements of six European forest types, taking advantage of a large database with plant physiological parameters. The reliability of both the flux data and parameter values itself was not under discussion in this study. The data provided by the researchers of the EUROFLUX sites, possibly with local corrections, were used with a minor gap-filling procedure to avoid the loss of many days with observations. The model performance is discussed based on their accuracy, generality and realism. Accuracy was evaluated based on the goodness-of-fit with observed values of daily net ecosystem exchange, gross primary production and ecosystem respiration (gC m(-2) d(-1) ), and transpiration (kg H(2) O m(-2) d(-1) ). Moreover, accuracy was also evaluated based on systematic and unsystematic errors. Generality was characterized by the applicability of the models to different European forest ecosystems. Reality was evaluated by comparing the modelled and observed responses of gross primary production, ecosystem respiration to radiation and temperature. The results indicated that: Accuracy . All models showed similar high correlation with the measured carbon flux data, and also low systematic and unsystematic prediction errors at one or more sites of flux measurements. The results were similar in the case of several models when the water fluxes were considered. Most models fulfilled the criteria of sufficient accuracy for the ability to predict the carbon and water exchange between forests and the atmosphere. Generality. Three models of six could be applied for both deciduous and coniferous forests. Furthermore, four models were applied both for boreal and temperate conditions. However, no severe water-limited conditions were encountered, and no year-to-year variability could be tested. Realism. Most models fulfil the criterion of realism that the relationships between the modelled phenomena (carbon and water exchange) and environment are described causally. Again several of the models were able to reproduce the responses of measurable variables such as gross primary production (GPP), ecosystem respiration and transpiration to environmental driving factors such as radiation and temperature. Stomatal conductance appears to be the most critical process causing differences in predicted fluxes of carbon and water between those models that accurately describe the annual totals of GPP, ecosystem respiration and transpiration. As a conclusion, several process-based models are available that produce accurate estimates of carbon and water fluxes at several forest sites of Europe. This considerable accuracy fulfils one requirement of models to be able to predict the impacts of climate change on the carbon balance of European forests. However, the generality of the models should be further evaluated by expanding the range of testing over both time and space. In addition, differences in behaviour between models at the process level indicate requirement of further model testing, with special emphasis on modelling stomatal conductance realistically.",2002,10.1046/j.1365-2486.2002.00471.x,no
"Representation primitives, process models and patient data in computer-interpretable clinical practice guidelines: A literature review of guideline representation models","Representation of clinical practice guidelines in a computer-interpretable format is a critical issue for guideline development, implementation, and evaluation. We studied 11 types of guideline representation models that can be used to encode guidelines in computer-interpretable formats. We have consistently found in all reviewed models that primitives for representation of actions and decisions are necessary components of a guideline representation model. Patient states and execution states are important concepts that closely relate to each other. Scheduling constraints on representation primitives can be modeled as sequences, concurrences, alternatives, and loops in a guideline's application process. Nesting of guidelines provides multiple views to a guideline with different granularities. Integration of guidelines with electronic medical records can be facilitated by the introduction of a formal model for patient data. Data collection, decision, patient state, and intervention constitute four basic types of primitives in a guideline's logic flow. Decisions clarify our understanding on a patient's clinical state, while interventions lead to the change from one patient state to another. (C) 2002 Elsevier Science Ireland Ltd. All rights reserved.",2002,10.1016/S1386-5056(02)00065-5,no
"Evaluation of the control of beet armyworm, Spodoptera exigua, with baculoviruses in greenhouses using a process-based simulation model","Scenario studies were carried out with a process-based model for control of wild-type and genetically modified baculoviruses in populations of Spodoptera exigua in glasshouse chrysanthemum (BACSIM). These scenario studies were used to evaluate the effectiveness of different spraying regimes, concentrations, UV-protection agents, and mean time to kill insects. In simulations on concentration and timing of S. exigua multicapsid nucleopolyhedrovirus (SeMNPV), good control efficacy was obtained with SeMNPV concentrations of 1 x 10(7) polyhedra/m(2) or higher. An early timing of virus applications, soon after egg deposition, is essential for effective control, especially at high temperatures, when the developmental rate of larvae is high and the window of opportunity for virus control is short. UV-protection agents contribute only marginally to effective biological control in the glasshouse because the decay rates of SeMNPV and Autographa californica multicapsid nucleopolyhedrovirus already ensure a period of exposure to active virus that is long enough for caterpillars to acquire a lethal concentration under practical conditions. The effect of genetic improvement towards shortening the survival time of infected larvae depends on the ability to time virus applications accurately and the mean time to kill the unmodified virus. Recombinant viruses with a lower mean time to kill are more effective than wild-type viruses when applied against second, third, and fourth instars because in these stages S. exigua infests a considerable number of plants and the rate of foliage consumption increases rapidly. When applied against first instars that infest only a small number of plants and have low consumption rates, recombinants will only be slightly more effective than wild-type viruses. In the case of an unmodified virus that already has a relatively high mean time to kill, a further reduction in the time to kill will not result in improved control. Thus, accurate monitoring followed by virus application is indispensable for adequate control and may even alleviate the need for fast killing viruses. (C) 2002 Elsevier Science (USA). All rights reserved.",2002,10.1016/S1049-9644(02)00032-4,no
In-hand sensory evaluation of textural characteristics in model processed cheese analogues,"Seven textural attributes of a range of model processed cheese analogues manufactured with different moisture contents and under different mixing speeds were investigated using in-hand sensory evaluation. A trained panel of nine judges and descriptive analysis were used. Significant differences between the cheeses were found for firmness (both in compression and upon cutting), stickiness and curdiness. Cheeses with lower moisture content were, in general, firmer, curdier and less sticky than cheeses with higher moisture content. Products with intermediate moisture content resulted, in general, in intermediate scores for those attributes. Mixing speed has significantly affected the curdiness scores (alpha=10%), as shown in the response surface regression analysis, but less in comparison to the moisture effect. At the confidence level of 5%, mixing speed was found not to be significant for any sensory attribute. Principal component analysis showed that firmness in compression, firmness in cutting, stickiness and curdiness were interrelated and together explained more than 50% of the variance in the experimental data. In spite of the relatively narrow range of textures studied and the unconventional evaluation approach, the trained sensory panel used was able to discriminate between the cheese analogues studied.",2002,10.1111/j.1745-4603.2002.tb01348.x,no
Sibling collusion and problem behavior in early adolescence: Toward a process model for family mutuality,"Sibling collusion is a process by which siblings form coalitions that promote deviance and undermine parenting. Collusive sibling processes were identified and measured using macro ratings of videotaped family interactions. Hypotheses were tested on a multiethnic sample of urban youth, with a target child identified as either ""high risk"" (n = 26) or ""normative"" (n = 26), and their families. Siblings in families with a high-risk target child showed reliably higher rates of collusion than those in families with a normative target child. Sibling collusion also accounted for variance in problem behavior after controlling for involvement with deviant peers. Findings suggest that deviant conduct forms a common ground among siblings, potentially amplifying risk of mutuality in problem behavior during early adolescence. These data also indicate that attention to sibling relationship processes is relevant to family interventions designed to mitigate the development of behavior problems.",2002,10.1023/A:1014753232153,no
Canadian Aerosol Module (CAM): A size-segregated simulation of atmospheric aerosol processes for climate and air quality models - 2. Global sea-salt aerosol and its budgets,"Size-segregated sea-salt aerosols were included as dynamic constituents in the third generation of the Canadian general circulation model (GCMIII) using the Canadian Aerosol Module (CAM). A 12-bin sectional model was used to simulate wind-dependent sea-salt aerosol generation at the surface, diffusive, and advective transports, as well as wet and dry removals as a function of particle size. The dependence of model-predicted atmospheric sea-salt concentration on wind speed is weaker than the u(3.41) dependence of sea-salt emissions but nevertheless still significant. A comparison of GCMIII surface winds with European Centre for Medium-Range Weather Forecasting reanalyzed winds was used to identify a globally averaged bias of modeled surface marine winds of 18%. After taking this into account, two-year simulations were performed to study the atmospheric sea-salt cycle. Comparisons of seasonal mean model predictions with observed sea-salt mass concentrations at 24 marine observatories were in reasonable agreement (generally better than a factor of 2). The annual global sea-salt flux is estimated to be 1.01x10(13) kg with similar to32% from the Northern Hemisphere and with 98% in supermicron particles. Although emissions to the atmosphere of submicron sea-salt particles larger than 0.2 mum radius make up only 2% of the total global emissions, they contribute significantly to the background aerosol mass and number concentrations in the marine atmosphere. Submicron sea-salt aerosols contribute substantially to the total optical depth of the atmosphere over the open oceans. They account for 5-15% of total sea-salt mass in air at the surface and 20% at 700 hPa. The global mass mean dry diameter of sea-salt aerosols over oceans is 2.8 mum at the surface and 1.9 mum in the midtroposphere. Concentrations are highest in the roaring forties of the Southern Hemisphere and over the northern oceans from October to March. Residence times of sea-salt aerosols were highly variable depending on size, vertical dispersion, and removal processes. The mean residence time for 7.7 mum and 0.4 mum particles in the marine boundary layer were in the range 0.3-10 hours and 80-360 hours, respectively.",2002,10.1029/2001JD002004,no
An iterative fuzzy model for cognitive processes involved in environment quality judgement,"Social surveys are commonly used to asses the quality of the living environment. Models allow quantifying the impact of future developments. Starting from basic knowledge on the cognitive process of human judgement a fuzzy model is developed, focussing on the aggregation of the impact of particular activities to a global judgement.",2002,,no
Studies and evaluations on interception processes during rainfall based on a tank model,"Some analyses are carried out with regard to canopy interception processes during rainfall events based on a tank model. A hypothesis, rainfall interception rate is proportional to the product of potential evaporation and rainfall intensity, is formed from past experimental data, and is applied to the data in this study. Computational equations are proposed to the interception rate and accumulative interception loss under constant rainfall intensity. Data from the Shirakawatani experimental forested catchment are used in order to examine the relationship between the interception rate and rainfall intensity, the ratio of the interception rate to rainfall intensity and potential evaporation, accumulative interception loss and the rainfall duration, and accumulative interception loss and accumulative rainfall. These regression relations show that interception processes are described by rainfall intensity and potential evaporation. An equation relating the aerodynamic resistance in the Penman-Monteith equation to rainfall intensity is proposed to explain the fact that the interception rate exceeds net radiation. (C) 2002 Elsevier Science B.V. All rights reserved.",2002,10.1016/S0022-1694(01)00506-6,no
Measurable differences between sequential and parallel diagnostic decision processes for determining stroke subtype: A representation of interacting pathologies,"Stroke diagnosis depends on causal subtype. The accepted classification procedure is a succession of diagnostic tests administered in an order based on prior reported frequencies of the subtypes. The first positive test result completely determines diagnosis. An alternative approach tests multiple concomitant diagnostic hypotheses in parallel. This method permits multiple simultaneous pathologies in the patient. These two diagnostic procedures can be compared by novel numeric criteria presented here. Thrombosis, a type of ischemic stroke, results from interaction between endothelium, blood flow and blood components. We tested for ischemic stroke on thirty patients using both methods. For each patient the procedure produced an assessment of severity as an ordered set of three numbers in the interval [0, 1]. We measured the difference in diagnosis between the sequential and parallel diagnostic algorithms. The computations reveal systematic differences: The sequential procedure tends to under-diagnose and excludes any measure of interaction between pathologic elements.",2002,,no
Unexpected vertical profiles over complex terrain due to the incomplete formulation of transport processes in the SAIMM/UAM-V air quality model,"The air quality model (AQM) package SAIMM/UAM-V Systems Applications International Mesoscale Model/Urban Airshed Model with Variable grid was applied to a domain with complex topography including Switzerland. The output pressure at a fixed height above sea level generated by one of the SAIMM post-processor showed unexpected variations of about 40 hPa over the domain. We found that these fluctuations were caused by an incorrect definition of the pressure reference height. After the change of this definition, realistic pressure values were obtained. Using the correct pressure field as input, we simulated the 3-dimensional mixing ratios of pollutants with UAM-V for the summer smog period of July 28-29, 1993. The values tended to increase with height above the surface and with surface elevation above sea level. The Swiss topography was mirrored in the mixing ratio fields. By using CO as a quasi-inert tracer, it became evident that transport phenomena such as advection and diffusion did not consider expansions or compressions due to pressure and temperature variations. After the conversion of the concentrations to a common reference pressure and temperature before the calculation of transport, these strange topographic features in the mixing ratios vanished completely. The CO mixing ratio was underestimated by 10-15 ppb (7-10%) in the lowest layer over the Swiss Plateau due the omission of compression or expansion. For O-3 and NO2, the differences were 0-5 ppb (0-10%) and 0-0.2 ppb (0-15%), respectively. (C) 2002 Elsevier Science Ltd. All rights reserved.",2002,10.1016/S1364-8152(02)00033-6,no
"Towards an integration of process measurements, archive analysis and modelling in geomorphology - the Karkevagge experimental site, Abisko area, northern Sweden","The analysis of Holocene geomorphic process activity demands long-term data sets, which are available for the Karkevagge catchment due to 50 years of intensive geomorphologic field studies. This data set is used in combination with additional field measurements, remote sensing and digital elevation model (DEM) analysis to provide input data for modelling Holocene valley development. On the basis of this information, geomorphic process units (GPUs) are defined by means of GIs modelling. These units represent areas of homogeneous process composition that transfer sediments. Since the data base enables the quantification of single processes, the interaction of processes within the units can also be quantified. Applying this concept permits calculation of recent sediment transfer rates and hence leads to a better understanding of actual geomorphic landscape development activity. To extrapolate these data in time and space the process-related sediments in the valley are analysed for depth and total volume, primarily using geophysical methods. In this fashion the validity of measured process rates is evaluated for the Holocene time scale. Results from this analysis are exemplified in a cross-profile showing some of the principal sediment units in the valley. For example, the measured modem rates on a slush torrent debris fan seem to represent the Holocene mean rate. This approach should also be suitable for revealing Holocene geomorphic landscape development in terms of climate change.",2002,10.1111/j.0435-3676.2002.00175.x,no
Combined hydraulic and biological modelling and full-scale validation of SBR process,"The biological reactions during the settling and decant periods of Sequencing Batch Reactors (SBRs) are generally ignored as they are not easily measured or described by modelling approaches. However, important processes are taking place, and in particular when the influent is fed into the bottom of the reactor at the same time (one of the main features of the UniFed process), the inclusion of these stages is crucial for accurate process predictions. Due to the vertical stratification of both liquid and solid components, a one-dimensional hydraulic model is combined with a modified ASM2d biological model to allow the prediction of settling velocity, sludge concentration, soluble components and biological processes during the non-mixed periods of the SBR. The model is calibrated on a full-scale UniFed SBR system with tracer breakthrough tests, depth profiles of particulate and soluble compounds and measurements of the key components during the mixed aerobic period. This model is then validated against results from an independent experimental period with considerably different operating parameters. In both cases, the model is able to accurately predict the stratification and most of the biological reactions occurring in the sludge blanket and the supernatant during the non-mixed periods. Together with a correct description of the mixed aerobic period, a good prediction of the overall SBR performance can be achieved.",2002,,no
Process-oriented catchment modelling and multiple-response validation,"The conceptual rainfall runoff model TAC (tracer-aided catchment model) has been developed based on the experimental results of tracer hydrological investigations at the mountainous Brugga and Zastler basins (40 and 18.4 km(2)). The model contains a physically realistic description of the runoff generation, which includes seven unit types each with characteristic dominating runoff generation processes. These processes are conceptualized by different linear and non-linear reservoir concepts. The model is applied to a period of 3.2 years on a daily time step with good success. In addition, an extensive model validation procedure was executed. Therefore, additional information (i.e. runoff in subbasins and a neighbouring basin, tracer concentrations and calculated runoff components) was used besides the simulated discharge of the basin investigated. This study shows the potential of tracer data for hydrological modelling. On the one hand, they are good tools to investigate the runoff generation processes. This is the basis for developing more realistic conceptualizations of the runoff generation routine. On the other hand, tracer data can serve as multi-response data to assess and validate a model. Copyright (C) 2002 John Wiley Sons, Ltd.",2002,10.1002/hyp.330,no
On the complexity and interpretability of support vector machines for process modeling,"The design of a support vector machine with Gaussian kernels is considered for modeling nonlinear processes. The structure is equivalent to a neuro-fuzzy system based on radial basis function network considering some restrictions. To improve the interpretability and reduce the complexity of the structure a hybrid learning scheme is proposed. First, the input-output data is supervised clustered according to a modified form of the Mountain Method for cluster estimation, the subtractive clustering. Then, support vector learning finds the number of centers, its positions and output layer weights of the structure. The proposed learning scheme is applied for modeling the Box-Jenkins furnace benchmark and the distributed collector field of a solar power plant.",2002,10.1109/IJCNN.2002.1007483,no
Communicating the complexity of computer-integrated operations - An innovative use of process modelling in a North East hospital Trust,"The English National Health Service has undergone unprecedented political, economic and technological change. This has resulted in a requirement for radical improvements in operational efficiency and effectiveness. An effective IT infrastructure supporting key operational processes an management reporting is now, seen as essential This paper outlines the findings from empirical research in a North East hospital Trust. The authors collaborated on a requirements analysis project to investigate and model business processes and information flows using an automated IDEFO software took A participatory action research framework was used, informed by principles derived from critical social theory, to describe the complexity of the situation and provide improved communication amongst stakeholders. The paper concludes by reflecting on the need for more ""ideal"" speech situations in order to deal with complex operations management problems where political, power and social issues can obstruct effective implementation of new computer-integrated operations.",2002,10.1108/01443570210420403,no
Understanding information processing in the immune system; Computer modeling and simulations,"The immune system protects our organisms against pathogens and aberrant cells. It achieves this capability by processing and evaluating a wide variety of molecular signals many of which are generated by its own agents. Even though the amount of experimental data describing single cellular mechanisms is rapidly growing a satisfactory understanding of the immune system's operations remains very difficult due to the complexity of the system. Simmune, a novel computer program, allows to investigate cellular cooperation within the immune system through simulations on the level of interactions between individual cells. Such simulations may allow to bridge the scale gap between experimentally accessible low level data and higher level multicellular processes which determine the appropriate response to pathogen challenges.",2002,,no
Applying concepts of fuzzy cognitive mapping to model: The IT/IS investment evaluation process,"The justification process is a major concern for those organisations considering the development of an information technology (IT)/information systems (IS) infrastructure, which in turn is putting the competitive advantage of many companies at risk. The reason for this centers around management's inability to evaluate the holistic implication of adopting new technology, both in terms of the benefit and cost portfolios. In attempting to investigate this, the authors of this paper identify a number of well-known project appraisal techniques used during the IT/IS investment justification process. In establishing a number of limitations inherent in traditional appraisal techniques, the concept of multivalent, or fuzzy logic, is used to demonstrate how inter-relationships can be modeled between key dimensions identified in a proposed conceptual model for investment evaluation. The authors highlight this using fuzzy cognitive mapping (FCM) as a technique to model each IT/IS evaluation factor (integrating strategic, tactical, operational and investment considerations). The use of an FCM is then shown to be as a complementary tool that can serve to highlight interdependencies between contributory justification factors. (C) 2002 Elsevier Science B.V. All rights reserved.",2002,10.1016/S0925-5273(01)00192-X,no
Measuring and modelling flux recovery during the chemical cleaning of MF membranes for the processing of whey protein concentrate,"The microfiltration of milk based products is used industrially as a partial sterilisation technique and to facilitate selective separation of components. This paper describes (i) an experimental apparatus, protocol and key results and (ii) the development of a mathematical model, describing the flux recovery occurring when a 2.0 mum sintered stainless steel flat-sheet membrane fouled with reconstituted whey protein concentrate (NATC) powder is cleaned using aqueous sodium hydroxide. The model describes the unsteady-state hydraulic resistance variation which occurs when surface and in-pore bound material undergoes morphological changes during caustic cleaning. The model relies on simultaneous removal and swelling processes occurring within the surface and in-pore deposits, respectively. The validity and applicability of the model is discussed, along with possible future modelling developments. (C) 2002 Elsevier Science Ltd. All rights reserved.",2002,10.1016/S0260-8774(01)00151-0,no
A mathematical modeling and validation study of NOx emissions in metal processing systems,"The model is based on separate calculations of prompt and thermal NOx and is accomplished using CFD code for flow, temperature and concentration fields in the combustion system. Thermal NOx is calculated with the Zeldovich model. The prompt NOx is considered to be independent of residence time and is computed with detailed kinetic data based GRI-Mech version 2.11 and CHEMKIN code by assuming that every computational cell is a perfectly stirred reactor. Three main parameters are considered to be critical in NOx production: 1) air equivalence ratio, 2) temperature, and 3) mixture dilution with combustion products. All of these parameters and methane oxidation reaction rates are readily available in every cell of the CFD domain. Once NOx emission index is computed by GRI-Mech in every cell, NOx reaction rate is easily evaluated by multiplying it with the methane oxidation reaction rate. The NOx concentration field is then calculated using known CFD transport parameters. Comparison of model predictions with measurements is made for a wide range of industrial natural gas-fired burners installed in combustion chambers and furnaces. The flame temperatures were in the range of 1400-2100 K, velocities were in the range of 10-200 m/s, burner nozzles were in the range of diameters 5-550 mm, and the combustion chamber or furnace internal equivalent size in the range of 0.05-3.5 m. Good agreement has been obtained in most cases.",2002,10.2355/isijinternational.42.1175,no
"Phototransformation of triclosan in surface waters: A relevant elimination process for this widely used biocide - Laboratory studies, field measurements, and modeling","The phototransformation of the widely used biocide triclosan (5-chloro-2-(2,4-dichlorophenoxy)phenol) was quantified for surface waters using artificial UV light and sunlight irradiation. The pH of surface waters, commonly ranging from 7 to 9, determines the speciation of triclosan (pK(a) = 8.1) and therefore its absorption of sunlight. Direct phototransformation of the anionic form with a quantum yield of 0.31 (laboratory conditions at 313 nm) was identified as the dominant photochemical degradation pathway of triclosan. Combining the photochemical parameters with actual meteorological data and field measurements allowed us to validate a model describing the behavior of triclosan in the water column of a Swiss lake (Lake Greifensee). From August to October 1999, direct phototransformation accounted for 80% of the observed total elimination of triclosan from the lake. The remaining major sink for triclosan was the loss in the outflow. Thus, during the summer season, direct phototransformation appears to be a major elimination pathway of triclosan in this lake. Based on absorption spectra and quantum yield data, the phototransformation half-lives of triclosan were calculated under various environmental conditions typical for surface waters. Daily averaged half-lives were found to vary from about 2 to 2000 days, depending on latitude and time of year.",2002,10.1021/es025647t,no
Evaluation of near-surface material properties using planar mesh type coils with post-processing from neural network model,"The possibility of employing a planar mesh type probe for the evaluation of near-surface material properties as well as to inspect the presence of defects has been investigated in this paper. The impedance of a planar type coil in proximity of any metal surface is a complex function of many parameters including near-surface material properties. A two-dimensional model of a planar mesh type probe has been developed for the analytical calculation of magnetic vector potential., flux-linkage and impedance. The impedance of the probe is used for the evaluation of the near-surface material properties. A simple neural network model has been developed for the post-processing of output parameters from the measured impedance data.",2002,,no
Quality inspection of electroplated materials using planar type micro-magnetic sensors with post-processing from neural network model,"The possibility of employing planar type micro-magnetic sensors for the evaluation of the quality of electroplated materials as well as to inspect the presence of defects in its near-surface is investigated. The impedance of a planar type micro-magnetic sensor in the proximity of any metal surface is a complex function of many parameters including near-surface material properties. A two-dimensional model of two types of planar micro-magnetic sensors having meander and mesh type configurations has been developed for the analytical calculation of magnetic vector potential, flux-linkage and impedance. The impedance of the sensor is used for the evaluation of the quality of the electroplated materials. Usually an off-line generated grid system is used for the online evaluation of near-surface material properties. Use of a simple neural network model is proposed for the post-processing of output parameters from the measured impedance data as an alternative to the grid system.",2002,10.1049/ip-smt:20020340,no
Reliability modeling and analysis of multi-station manufacturing processes considering the quality and reliability interaction,"The relationship between product quality and manufacturing system component reliability is very complex in a multi-station manufacturing process (MMP). In this paper, a general system reliability model is presented to integrate the product quality and manufacturing system component reliability information. Considering the unique complex characteristics of MMPs, a new QR-Chain model is proposed to study the propagation of the interaction between manufacturing system component reliability and product quality throughout all stations. An analytical solution for the system reliability of a general MMP can be obtained based on the proposed QR-Chain model. The upper bound of the system reliability is derived, which is much easier to obtain than the exact solution.",2002,,no
Evaluation of surface modification processes using a ternary XPS diagram,"The use of a ternary XPS diagram to follow surface modification processes involving three elements is described. The elemental composition is represented by a single data point on a plane instead of by two or three elemental ratios or percentages. Vectors are defined between the data points, and simple vector algebra is used to interpret the results. The extent of the surface change is determined by calculating the length of the vector from untreated to a treated composition point, and this leads to a value for the overall change in elemental composition. The direction of the vector indicates how the changes in the individual elemental percentages are related to each other, i.e. what elemental composition the surface is approaching. The ternary XPS diagram is demonstrated and compared with elemental percentages and ratios using XPS data from hydrogen microwave plasma-treated polydimethylsiloxane. Copyright (C) 2002 John Wiley Sons, Ltd.",2002,10.1002/sia.1417,no
MM5-CMAQ air quality modelling process analysis: Madrid case,"The usual application of three dimensional air quality models is to predict the spatial and temporal distributions of ambient air pollutants and other species. Third generation complex Eulerian models generate output concentrations by solving systems of partial differential equations. These equations define the time-rate of change in species concentrations due to a series of physical and chemical processes (deposition, emission, chemical reactions, horizontal advection, etc.). MM5-CMAQ is configured in such a way that we can have access to quantitative information on the effects of the chemical reactions and other atmospheric processes that are being simulated. In this contribution we have configured an application of the MM5-CMAQ Air Quality Modelling System over the Madrid (Spain) regional domain and study the different physical and atmospheric processes. We have developed an expert analysis software package - which is intended to extent and improve in future works - to analyze in detail in a straightforward manner, all the information provided by the MM5-CMAQ modeling system. The huge amount of the output process analysis files (800 Mb NetCDF binary format) requires using these robust visualization packages. The results show that all the chemical and physical processes are fully consistent and the application of such a analysis will be extremely useful for decision taken processes in environmental offices at cities and regional environmental departments. This tool is essential to analyze in detail the process near specific areas of interest (industrial plants, special traffic concentrations, etc.).",2002,,no
Modelling and evaluation of time aspects in business processes,"There is a need for modelling and performance evaluation techniques and tools for a fast and reliable design of workflow systems. This paper introduces a modelling methodology based on coloured stochastic Petri nets. It allows the integration of control flow, organizational, information related and timing aspects in one modelling framework. The processing delays include stochastic distributions in addition to deterministic times. Several workflows and the effects of constrained shared resources needed for different tasks can easily be described and analysed together. Control flow and organizational aspects are modelled separately in resource and workflow models. These models are automatically compiled into one model, which can then be used for qualitative analysis or performance evaluation. The proposed modelling and evaluation method is supported by the software tool TimeNET. An application example shows its use.",2002,10.1057/palgrave.jors.2601416,no
Quantifying electric power quality via fuzzy modelling and analytic hierarchy processing,"There is no one universal, even approximate. piece of data that entirely characterises, qualitatively or quantitatively, the status of the power quality at a utilisation point. This is due, on the one hand, to the great dimensionality of the parameters involved in the problem of power quality evaluation. On the other hand, the available data is featured with imprecision, uncertainty and vagueness, which renders it a very tedious and problematical task to assess the power quality level through one index. It is attempted to formulate one such comprehensive measure for the level of power quality at a loading point. Knowledge acquisition sessions. analytic hierarchy processing and fuzzy reasoning are the assistant tools employed to propose this now measure.",2002,10.1049/ip-gtd:20020006,no
Evaluating the quality of process models: Empirical testing of a quality framework,"This paper conducts an empirical analysis of a conceptual model quality framework for evaluating the quality of process models. 194 participants were trained in the concepts of the quality framework, and then used it to evaluate models represented in a workflow modelling language. A randomised, double-blind design was used, and the results evaluated using a combination of quantitative and qualitative techniques. An analysis was also conducted of the framework's likelihood of adoption in practice, which is an issue rarely addressed in IS design research. The study provides strong support for the validity of the framework and suggests that it is likely to be adopted in practice, but raises questions about its reliability. The research findings provide clear direction for further research to improve the framework.",2002,,no
Model for indirect measurements of LD-steelmaking process,"This paper describes a statistical approach to modelling the LD-steelmaking process. The steelmaking process belongs to very complicated metallurgical processes, in which the measurement of process variables is very difficult and economically challenging. Among these variables are mostly the concentration of carbon in hot metal and the temperature of bath. Both of them are very important for predicting the end of blowing. We can obtain an appropriate prediction by using a simulation model of the technological process. We recognize several approaches to making simulation models and bave chosen the statistical approach which counts selected process variables from regression equation step by step. An advantage of using a regression model is that we can eliminate interruption of blowing.",2002,,no
Learning and understanding a software process through simulation of its underlying model,"This paper describes the usage process simulation models for better understanding the dynamic effects of processes. First, a method to build a comprehensive descriptive model is presented, which can build the foundation for the simulation model. Based upon the understanding of the static process, parts of the dynamic model can be modeled can be created. The initial method presented was used to create a discrete event model for inspections. The paper discusses the benefits of simulation for learning and optimizing the feedback cycles for learning.",2002,,no
"Validation of a comprehensive process-based model for the biological control of beet armyworm, Spodoptera exigua, with baculoviruses in greenhouses","This paper describes the validation and sensitivity analysis of a process-based simulation model (BACSIM) for the control of beet armyworm, Spodoptera exigua, with baculoviruses. Model predictions are compared to results of independent greenhouse experiments in which second, third, or fourth instar larvae of S. exigua in chrysanthemum plots are treated with different concentrations of Autographa californica multicapsid nucleopolyhedrovirus (AcMNPV) and S. exigua MNPV (SeMNPV), two viruses with distinct differences in infectivity and mean time to kill. BACSIM provides robust predictions for the control of S. exigua populations in greenhouse chrysanthemum with both AcMNPV and SeMNPV. Mortality levels caused by AcMNPV and SeMNPV were generally predicted within a 25% margin of error compared to the observed values. None of the deviations was higher than 40%. All values of simulated foliage consumption, caused by S. exigua populations treated with AcMNPV or SeMNPV applications, fell within 95% confidence intervals of measurements. Simulated time to kill was, in general, lower than the measurements. This discrepancy may be caused by the behavior of S. exigua larvae which feed on the underside of chrysanthemum leaves where they are protected from polyhedra. This suggests that the larval foraging behavior may play an important role in the efficacy of baculovirus applications and should be further studied experimentally. This validated model can be used for the pre-trial evaluation of the efficacy of genetically modified baculoviruses as biological control agents and for the optimization of spraying regimes in chrysanthemum cultivation. (C) 2002 Elsevier Science.",2002,10.1006/bcon.2001.0990,no
"Quality improvement, setup cost and lead-time reductions in lot size reorder point models with an imperfect production process","This paper investigates the lot size, reorder point inventory model involving variable lead time with partial backorders, where the production process is imperfect. The options of investing in process quality improvement and Setup cost reduction are included, and lead time can be shortened at an extra crashing cost. The objective is to simultaneously optimize the lot size, the reorder point, the process quality. the setup cost, and the lead time. We first assume that lead-time demand follows a normal distribution and develop an algorithm to find the optimal solution. Then, we relax the assumption of normality to consider a distribution-free case where only the mean and standard deviation of lead-time demand are known. We apply the minimax distribution-free procedure to solve this problem. Furthermore. two numerical examples are given to illustrate the results.",2002,10.1016/S0305-0548(01)00051-X,no
Convergence rates of estimators in partial linear regression models with MA(infinity) error process,"This paper is concerned with a partial linear regression model with serially correlated random errors which are unobservable and modeled by a moving-average process of infinite order. We study a class of estimators for the linear regression coefficients as well as the function characterizing the non-linear part of the model, constructed based on general kernel smoothing and least squares methods. The law of iterated logarithm and strong convergence rates of these estimator are derived by truncating the moving-average error process, a procedure widely applied in the analysis of time series. Our results can-be used to establish uniform strong convergence rate of the estimators of autocovariance and autocorrelation functions of the error process.",2002,10.1081/STA-120017224,no
Interagency systems development and evaluation in the Pacific Islands: A process model for rural communities,"This paper offers a historical perspective and evaluation approach for technical assistance efforts in the Pacific provided by U.S. based agencies and two studies of outcome evaluation at the regional and local level. The first study focuses on the evaluation of outcomes of the introduction of the interagency concept and annual conferences to assist six U.S. Pacific jurisdictions to develop and implement interagency systems development at the regional level. The application and outcomes of the interagency concept at the local level in the Republic of Palau is the focus of the second study. The Palau Interagency Model, in particular, is discussed in depth as a model for change and improved interagency coordination. An interagency survey that included Likert-type open-ended formatted questions as used in both studies to identify interagency effectiveness and areas for improvement at the local and regional level. Evaluation studies were multilevel, including perspectives of administrators, providers, parents. and consumers in order to identify stages of interagency development that included building an infrastructure, systems design, staff training, and implementation of services for families and individuals with disabilities, A template of benchmarks for creating an effective interageny team system is offered for further study and replication.",2002,,no
A method for modeling and evaluating software maintenance process performances,This paper presents a method for modeling and evaluating the performances of telecommunication software maintenance process. The method can be used as a special technique within the given generic model as part of an organizational effort to upgrade process maturity and efficiency. It is based on the modeling of software maintenance process as queuing networks and applies process simulation to determine its performances. The method allows efficient comparison of the alternative process designs without the risks associated with the experiments in real life. Its activities can be implemented in various telecommunication software maintenance processes and other software processes to be formally presented and analyzed for improvement purposes. Implementation is presented in a case study with software maintenance process in a telecommunications company. The results show practicability and applicability of the method in the software organization with the aim of improving software maintenance process.,2002,10.1109/CSMR.2002.995786,no
"Handling of removable discontinuities in MINLP models for process synthesis problems, formulations of the Kremser equation","This paper presents a new method for handling removable discontinuities in non-convex mixed integer non-linear programming (MINLP) models for chemical process synthesis and design. First, the occurrences of different kinds of discontinuities in design equations are discussed. Then methods so far developed for handling discontinuities in an MINLP design environment are summarized. In the main part of the paper, a new method is presented for handling removable discontinuities. Our new method is compared to five conventional literature methods applied to three mass exchange network synthesis problems of different size, where the Kremser equation is used for calculating the number of equilibrium stages. In addition, a heat exchange network synthesis problem is considered where the logarithmic mean temperature difference is calculated rigorously. Our method proved to be much faster than the other methods examined and shows less sensitivity to the change of initial values in terms of optimal objective function value and solution time. (C) 2002 Elsevier Science Ltd. All rights reserved.",2002,10.1016/S0098-1354(02)00037-6,no
"Error modeling, sensitivity analysis and assembly process of a class of 3-DOF parallel kinematic machines with parallelogram struts","This paper presents an error modeling methodology that enables the tolerance design, assembly and kinematic calibration of a class of 3-DOF parallel kinematic machines with parallelogram struts to be integrated into a unified framework. The error mapping function is formulated to identify the source errors affecting the uncompensable pose error. The sensitivity analysis in the sense of statistics is also carried out to investigate the influences of source errors on the pose accuracy. An assembly process that can effectively minimize the uncompensable pose error is proposed as one of the results of this investigation.",2002,,no
Residual stress development during the composite patch bonding process: measurement and modeling,"This paper presents an experimental technique for monitoring residual stress development throughout the composite patch repair curing process. Using this technique, process-induced strains and specimen warpage during a number of different cure cycles were measured for a simulated single-sided composite patch repair of an aluminum substrate. Models for adhesive cure rate and glass transition behavior of the patch adhesive resin (FM 300-1 K) were combined with a simple bi-metallic strip model to predict specimen warpage and strain behavior during cure. Model predictions were compared with experimental measurements and were used to assist in the development of optimized cure cycles. Using these optimized cycles, it was found that it was possible to achieve significant (>20%) reductions in patch warpage and at the same time, minimize processing time and obtain a high final adhesive degree of care. Experimental observations suggest that an improved patch model incorporating adhesive viscoelastic behavior during cure would assist in achieving additional process improvements. (C) 2001 Elsevier Science Ltd. All rights reserved.",2002,10.1016/S1359-835X(01)00083-5,no
Manipulator kinematic error model in a calibration process through quaternion-vector pairs,Three-dimendional modeling of rotations and translations in kinematic calibration is most commonly performed using homogeneous transformations. This paper presents a new technique for the calibration of robots based on the quaternion-vector pair approach for the identification of geometrical errors.,2002,,no
Analytical modelling of transmit power control error process for S-UMTS.,"Transmit power control is indispensable in Direct Sequence Code Division Multiple Access (DS/CDMA) based systems such as Satellite Universal Mobile Telecommunication System (S-UMTS). The performance of fixed step-size dosed loop power control Is significantly affected by command delay due to processing and propagation. This calls for insights into the dynamics of transmit power control taking into account the effects of delays. In this paper, therefore, we have developed an analytical model for the dynamic behaviour of the error process as a stochastic difference equation. We have modelled the effects of delays in the presence of random external disturbances as a random walk process. Using the stochastic difference equation, we have developed an expression for the steady-state pdf of the power control error process as a lognormal distribution. However, we demonstrate that In cases where the quantisation errors of the power control command are significant, the error process departs from the lognormal distribution approximation. This shows that the generally assumed lognormal distribution for power control error process may not always be valid.",2002,,no
Understanding the process of information systems and ICT curriculum development - Three models,"University curricula in Information Systems and ICT need to be frequently updated to take account of changing technologies and the uses made of these technologies. Many writers on the theory of curriculum give most attention to approaches to curriculum change based on research, development and diffusion models. This paper outlines some objections to the application of models of this type by describing how a university curriculum in information systems and ICT is built and rebuilt, and offers instead three alternate models; one based on curriculum negotiations, another on innovation translation from actor-network theory, and the third on an ecological model. In our discussion of these models, each will be shown to allow for the inclusion. of a richer level of complexity than research, development and diffusion models when describing the curriculum development process.",2002,,no
Understanding the process of information systems and ICT curriculum development: Three models,"University curricula in information systems and ICT need to be frequently updated to take account of changing technologies and the uses made of these technologies. Many writers on the theory of curriculum give most attention to approaches to curriculum change based on research, development and diffusion models. This paper outlines some objections to the application of models of this type by describing how a university curriculum in information systems and ICT is built and rebuilt, and offers instead three alternate models; one is based on curriculum negotiations, another on innovation translation from actor-network theory, and the third on an ecological model. In our discussion of these models, each will be shown to allow for the inclusion of a richer level of complexity than research, development and diffusion models when describing the curriculum development process.",2002,,no
Achieving maintainability of process simulation software models,"Various computer languages, some endogenous to specific software simulation packages, are frequently used for building process simulation models. These models are extensively used, both in academia and industry, for the design, analysis, and continuous improvement of processes, Use of these simulation models is common in manufacturing, transportation and logistics, health care, government, and various service industries. These models may be used for ongoing process improvement over a lengthy period of time, sometimes exceeding a decade. Furthermore, due to employee turnover and shifts in responsibilities, these models may be successively revised by many different modelers and used by many different clients during their software lifetime. Such models represent significant investments both of model-development time and of client expertise and interface time. Therefore, maintainability of these models is necessary to ensure their economic viability and continuation of their evolution and usage during the lifetime of the process being modeled and improved. Here, we discuss methods, techniques, and insights whose application improves the maintainability of process simulation models, thereby increasing their long-range value to either business or research enterprises.",2002,,no
Estimates of blow-up time for a non-local problem modelling an Ohmic heating process,"We consider an initial boundary value problem for the non-local equation, u(t) = u(xx) + lambdaf(u)/(integral(-1)(1)f (u)dx)(2), with Robin boundary conditions. It is known that there exists a critical value of the parameter lambda, say lambda*, such that for lambda>lambda* there is no stationary solution and the solution u(x,t) blows up globally in finite time t*, while for lambda<&lambda;* there exist stationary solutions. We find, for decreasing f and for &lambda;>lambda* upper and lower bounds for t*, by using comparison methods. For f (u) = e(-u), we give an asymptotic estimate: t* similar to t(u)(lambda-lambda*)(-1/2) for 0<(&lambda;-&lambda;*) &MLT; 1, where t(u) is a constant. A numerical estimate is obtained using a Crank-Nicolson scheme.",2002,10.1017/S0956792501004831,no
Environmental influence on the measurement process in stochastic reduction models,"We consider the energy-driven stochastic state vector reduction equation for the density matrix, which for pure state density matrices can be written in two equivalent forms. We use these forms to discuss the decoupling of the noise terms for independent subsystems, and to construct 'environmental' stochastic density matrices whose time-independent expectations are the usual quantum statistical distributions. We then consider a measurement apparatus weakly coupled to an external environment, and show that in mean field (Hartree) approximation the stochastic equation separates into independent equations for the apparatus and environment, with the Hamiltonian for the apparatus augmented by the environmental expectation of the interaction Hamiltonian. We use the Hartree approximated equation to study a simple accretion model for the interaction of the apparatus with its environment, as part of a more general discussion of when the stochastic dynamics predicts state vector reduction, and when it predicts the maintenance of coherence. We also discuss the magnitude of decoherence effects acting during the reduction process. Our analysis supports the suggestion that a measurement takes place when the different outcomes are characterized by sufficiently distinct environmental interactions for the reduction process to be rapidly driven to completion.",2002,10.1088/0305-4470/35/4/301,no
A measure-valued process related to the parabolic Anderson model,"We consider the following stochastic partial differential equation: partial derivativeu(t)/partial derivativet = Deltau(t) + kappau(t)F(t,.). Here, t greater than or equal to 0, X epsilon R-d with d greater than or equal to 3, and kappa > 0. F(t, x) is a generalized Gaussian process, with covariance E [(F)over dot(t, x)(F)over dot(t, y)] = delta(t-s)/\x-y\(2). Our solution u(t)(dx) will exist as a nonnegative measure, provided u(0)(dx) is a nonnegative measure satisfying certain properties. We discuss various properties of the solutions. Useful tools include the Feynman-Kac formula, duality, and integral equations.",2002,,no
"Biological control of beet armyworm, Spodoptera exigua, with baculoviruses in greenhouses: Development of a comprehensive process-based model","We describe the development of a comprehensive process-based model simulating the epizootiology and agronomic efficacy of baculoviruses used for biological control of beet armyworm, Spodoptera exigua, in greenhouse chrysanthemum. The model is built to help understand, evaluate, and predict the effects of genetic modification, formulation, and application strategy of a baculovirus on its epizootiology and agronomic efficacy in a greenhouse cropping system. The model was constructed such that the impact of detailed aspects of the virus-host relationship, cropping system, spraying regime, and insect ecology on control could be evaluated. The resulting model is highly detailed, covers four spatial scales, from the leaf to the greenhouse, and addresses spatially explicit processes at the individual level and spatially averaged population processes. Submodels describe the crop production system and plant growth, the distribution and dispersal of insects within the crop canopy, spray deposition, uptake of virus, the probability of death and survival time of infected insects, and vertical an horizontal transmission. This paper describes the conceptual basis of the model and its parameterization by quantitative descriptions of model components. Model components are tested by comparison with dependent and independent experimental data. (C) 2002 Elsevier Science.",2002,10.1006/bcon.2001.0989,no
Validation of a model for process development and scale-up of packed-bed solid-state Bioreactors,"We have validated our previously described model for scale-up of packed-bed solid-state fermenters (Weber at al., 1999) with experiments in an adiabatic 15-dm(3) packed-bed reactor, using the fungi Coniothyrium minitans and Aspergillus oryzae. Effects of temperature on respiration, growth, and sporulation of the biocontrol fungus C. minitans on hemp impregnated with a liquid medium were determined in independent experiments, and the first two effects were translated into a kinetic model, which was incorporated in the material and energy balances of the packed-bed model. Predicted temperatures corresponded well with experimental results. As predicted, large amounts of water were lost due to evaporative cooling. With hemp as support no shrinkage was observed, and temperatures could be adequately controlled, both with C. minitans and A. oryzae, In experiments with grains, strong shrinkage of the grains was expected and observed. Nevertheless, cultivation of C. minitans on oats succeeded because this fungus did not form a tight hyphal network between the grains. However, cultivation of A. oryzae failed because shrinkage combined with the strong hyphal network formed by this fungus resulted in channeling, local overheating of the bed, and very inhomogeneous growth of the fungus. For cultivation of C. minitans on oats and for cultivation of A. oryzae on wheat and hemp, no kinetic models were available. Nevertheless, the enthalpy and water balances gave accurate temperature predictions when online measurements of oxygen consumption were used as input. The current model can be improved by incorporation of (1) gas-solids water and heat transfer kinetics to account for deviations from equilibrium observed with fast-growing fungi such as A. oryzae, and (2) the dynamic response of the fungus to changes in temperature, which were neglected in the isothermal kinetic experiments. (C) 2002 John Wiley & Sons, Inc. Biotechnol Bioeng 77: 381-393, 2002; DOI 10.1002/bit.10087.",2002,10.1002/bit.10087,no
A priori error estimates of a finite-element method for an isothermal phase-field model related to the solidification process of a binary alloy,We introduce a piecewise linear finite-element scheme with semi-implicit time discretization for an evolutionary phase field system modelling the isothermal solidification process of a binary alloy. This system can be written in a vectorial form as a nonlinear parabolic system. The convergence of the scheme with error estimate is then proved by introducing a generalized vectorial elliptic projector.,2002,10.1093/imanum/22.2.281,no
Errors of signal processing in digital terrain modelling,"We present new interpretation of three classes of errors in digital terrain models (DTMs) which can be sources of artefacts in DTM-based studies: (a) errors in interpolation of digital elevation models (DEMs) caused by the Gibbs phenomenon; (b) errors in DTM derivation from DEMs with 'enhanced' resolution due to noise increase after DEM differentiation; (c) errors in DTM derivation caused by displacement of a DEM grid. Explanation of artefact roots and ways to avoid them are carried out in the context of the theory of signal processing. The Gibbs phenomenon is a specific behaviour of some functions manifested as over- and undershoots near a jump discontinuity. Any DEM includes jump discontinuities of the elevation, such as escarpments and pronounced errors of DEM generation. There are four main ways to prevent or reduce DEM errors caused by the Gibbs phenomenon: (a) decreasing the jump discontinuity before DEM interpolation; (b) using interpolation functions which do not generate the Gibbs phenomenon; (c) omitting over- and undershoots after DEM interpolation; (d) filtering the Gibbs phenomenon. Derivation of topographic variables from DEMs marked by 'enhanced' resolution can lead to artefacts. If a DEM of this kind is interpolated by triangulation-based algorithms, triangular patterns may be revealed on maps of topographic variables. If an 'enhanced' resolution of DEM is achieved by the weighted average methods of interpolation, contour 'traces' may be seen on maps. This is because partial derivatives used to calculate some topographic variables are very responsive to high-frequency components of a DEM. To prevent these errors one should use a regular DEM with a grid space relating to an average distance between points in an irregular DEM. Displacement of a grid of points, wherein elevation values are interpolated or determined, influences the derivation of topographic variables (e. g. map design of horizontal and vertical curvatures). Some patterns break, merge, and change their width and length. Small dots, lines, and particles of big patterns can appear and disappear on maps. These effects should be taken into account in the application of these maps to DTM-based geological studies.",2002,10.1080/13658810210129139,no
H diffusion for impurity and defect passivation: A physical model for solar cell processing,"We propose a physical model for diffusion of H in Si containing impurities and defects. The diffusion occurs via several parallel mechanisms, involving complex formation (trapping) and dissociation (detrapping) at impurities and defects, hopping in lattice interstitial sites, and chargestate conversion. The role of bulk and process-induced traps is considered to explain observations from plasma, ion implantation, and PECVD-nitridation processes.",2002,10.1109/PVSC.2002.1190496,no
Evaluation of a chemical transport model for sulfate using ACE-2 observations and attribution of sulfate mixing ratios to source regions and formation processes,"[1] High resolution measurements of sulfate during the ACE-2 campaign (June-July 1997) permit detailed evaluation of the performance of a chemical transport models driven by analyzed meteorological data. At Tenerife, Canary Islands, (minimal proximate sources) the median ratio characteristic spread between model and observations, S-m/o similar to 1.3, was comparable to the spread of three sets of collocated observations and to the spread of observations at stations separated by similar to 13 km within a single model grid cell (1degrees x 1degrees). Somewhat greater S-m/o, similar to 1.6, at Sagres, Portugal is attributed to nonrepresentativeness of a single measurement site influenced by proximate sources. At Tenerife contributions from European, North American, and biogenic sources to sulfate mixing ratios are comparable, with North American sources dominating (up to similar to85%) under conditions of a strong Azores high; thus substantial contributions of sulfate, and by implication other aerosols, can result from long-range midlatitude transport across the Atlantic Ocean.",2003,10.1029/2003GL016942,no
Snow process modeling in the North American Land Data Assimilation System (NLDAS): 2. Evaluation of model simulated snow water equivalent,"[1] This is the second part of a study on the cold season process modeling in the North American Land Data Assimilation System ( NLDAS). The first part concentrates on the assessment of model simulated snow cover extent. In this second part, the focus is on the evaluation of simulated snow water equivalent ( SWE) from the four land surface models ( Noah, MOSAIC, SAC and VIC) in the NLDAS. Comparisons are made with observational data from the Natural Resources Conservation Service's Snowpack Telemetry ( SNOTEL) network for a 3- year retrospective period at selected sites in the mountainous regions of the western United States. All models show systematic low bias in the maximum annual simulated SWE that is most notable in the Cascade and Sierra Nevada regions where differences can approach 1000 mm. Comparison of NLDAS precipitation forcing with SNOTEL measurements revealed a large bias in the NLDAS annual precipitation which may be lower than the SNOTEL record by up to 2000 mm at certain stations. Experiments with the VIC model indicated that most of the bias in SWE is removed by scaling the precipitation by a regional factor based on the regression of the NLDAS and SNOTEL precipitation. Individual station errors may be reduced further still using precipitation scaled to the local station SNOTEL record. Furthermore, the NLDAS air temperature is shown to be generally colder in winter months and biased warmer in spring and summer when compared to the SNOTEL record, although the level of bias is regionally dependent. Detailed analysis at a selected station indicate that errors in the air temperature forcing may cause the partitioning of precipitation into snowfall and rainfall by the models to be incorrect and thus may explain some of the remaining errors in the simulated SWE.",2003,10.1029/2003JD003994,no
Curie-Weiss model of the quantum measurement process,"A Hamiltonian model is solved, which satisfies all requirements for a realistic ideal quantum measurement. The system S is a spin-1/2, whose z-component is measured through coupling with an apparatus A = M + B, consisting of a magnet M formed by a set of N >> 1 spins with quartic infinite-range Ising interactions, and a phonon bath B at temperature T. Initially A is in a metastable paramagnetic phase. The process involves several time-scales. Without being much affected, A first acts on S, whose state collapses in a very brief time. The mechanism differs from the usual decoherence. Soon after its irreversibility is achieved. Finally, the field induced by S on M, which may take two opposite values with probabilities given by Born's rule, drives A into its up or down ferromagnetic phase. The overall final state involves the expected correlations between he result registered in M and the state of S. The measurement is thus accounted for by standard quantum-statistical mechanics and its specific features arise from he macroscopic size of the apparatus.",2003,10.1209/epl/i2003-00150-y,no
Integrated process modeling and experimental validation of silicon carbide sublimation growth,"A model that integrates heat and mass transfer, growth kinetics, anisotropic thermal stresses is developed to predict the global temperature distribution, growth rate and dislocation distribution. The simulated temperature and growth rate are compared with the experimental measurements. The time-depending growth process, e.g., the variations of the growth rate, the growth interface shape, and the thermal stresses with time in the growing crystal are studied using the integrated model. The resolved shear stress and the von Mises stress are used to predict the dislocation density. The effects of geometric configuration and design parameters on the growth of crystal are also discussed. (C) 2003 Elsevier Science B.V. All rights reserved.",2003,10.1016/S0022-0248(03)00944-8,no
Metrological measurements and signal processing in SEM based on the model of signal formation,A Monte-Carlo model of an SEM image and spectra formation has been developed. Computer-aided measurement systems theory and morphological image analysis were applied for the signal processing. It is shown that the methods improve detectors' and sensors' characteristics. Morphological analysis increases the accuracy of the objects' size and position measurement. (C) 2003 Elsevier B.V. All rights reserved.,2003,10.1016/S0167-9317(03)00346-0,no
"Validation of a model for the absorption process of H2O(vap) by a LiBr(aq) in a horizontal tube bundle, using a multi-factorial analysis",A multi-factorial experimental test of H2O vapour absorption by a falling film of LiBr(aq) over an horizontal smooth tube absorber is presented. The response parameters of the study are the average convective heat transfer and mass transfer coefficients of the falling film. The response is expressed as a function of the factors used in the study. Also a mathematical model for the prediction of the performance of the absorber is exposed which takes into account the wetting effects. Finally the predictions from the model and the experimental multi-factorial runs are compared and the conclusions discussed. (C) 2003 Elsevier Science Ltd. All rights reserved.,2003,10.1016/S0017-9310(03)00121-2,no
Modeling of dopant and defect interactions in Si process simulators,"A multi-scale modeling, from ab-initio calculations, through Monte Carlo diffusion simulators, to the continuum models, is necessary to describe the complexity of dopant and defect interactions for process simulators. The advantages that make continuum simulators the standard in industrial applications are maintained only on the basis of the simplicity of the physical models. The inclusion of complex interactions in Kinetic Monte Carlo methods is not a problem from the computational point of view, but they demand a large number of parameters. The fabrication of small devices brings up complex physical mechanisms, for which atomistic simulations seem more appropriate than continuum methods.",2003,,no
Nonlinear process modeling based on just-in-time learning and angle measure,"A new just-in-time learning methodology by incorporating both distance measure and angle measure is developed. This result enhances the existing methods, where the available information of angular relationships between two data samples is not exploited. In addition, a new procedure of selecting the relevant data set is proposed. The proposed methodology is illustrated by a case study of modeling a polymerization reactor. The adaptive ability of the just-in-time learning is also evaluated.",2003,,no
The implementation and evaluation of a novel wideband dynamic directional indoor channel model based on a Markov process,"A novel stochastic wideband dynamic directional indoor channel model which incorporates both the spatial and temporal domain properties as well as the dynamic evolution of paths when the mobile moves is proposed based on the concept of a Markov process. The derived model is based on dynamic measurement data collected at a carrier frequency of 5.2GHz in several typical indoor environments. Analysis shows that multiple births and deaths are possible at any instant of time. Furthermore, correlation exists between the number of births and deaths. Thus, an M-step, 4-state Markov channel model is proposed in order to account for these two effects. The spatio- temporal variations of paths within their lifespan are taken into consideration by the spatio-temporal vector which was found to be well-modeled by a Gaussian, probability density function while the power variation can be modeled by a simple low pass filter. The implementation of the model is detailed and finally, the model validity is evaluated by-comparing key statistics of the simulation results with the measurement results.",2003,,no
Evaluation of a Process-Based Agro-Ecosystem Model (Agro-IBIS) across the US Corn Belt: Simulations of the Interannual Variability in Maize Yield,"A process-based terrestrial ecosystem model, Agro-IBIS, was used to simulate maize yield in a 13-state region of the U. S. Corn Belt from 1958 to 1994 across a 0.5 degrees terrestrial grid. For validation, county-level census [U. S. Department of Agriculture (USDA)] data of yield were detrended to calculate annual yield residuals. Daily atmospheric inputs from the National Centers for Environmental Prediction-National Center for Atmospheric Research (NCEP-NCAR) reanalysis were used in conjunction with the Climate Research Unit's (CRU's) monthly climate anomaly dataset at 0.5 degrees resolution and a weather generator to drive the model at a 60-min time step. Multiple simulations were used to quantify model sensitivity to hybrid selection (defined by growing degree-day requirements), planting date, and soil type. The calibration of raw yields and model capability to replicate interannual variability were tested. The spatial patterns of simulated mean bias error (mbe) of raw yields were largely unresponsive to variations in soil texture, optimum hybrid choices, and planting dates typical for the region. Simulated 15-yr mean yields had a significant bias that was higher than observations, with an mbe of 0.97 Mg ha(-1) and a root-mean-squared error (rmse) of 1.75 Mg ha(-1). Simulations of interannual maize yield variability appeared to have a relatively weak response to changes in soil type and were more responsive to a planting date than a hybrid selection. The correlation (r(2)) between observed and simulated yield residuals within individual 0.5 degrees grid cells ranged from 0.0 to 0.6, and increased as additional scenarios of land management decisions within 0.5 degrees grid cells were taken into account. The correlations appeared to have a weak but significant relationship to a reported harvested area (r(2) = 0.36). Model simulations produced a larger absolute magnitude of interannual variability than observations (positive and negative simulated yield residuals over the region averaged 18% and -21%, compared with 13% and -17% for observations), but spatial patterns were consistent. It was determined that the impact of irrigation on maize yield in the western Great Plains must be properly accounted for in future modeling scenarios to capture the increase (36%) in mean yields and significantly decreased interannual variability compared to rain-fed maize for future sensitivity studies.",2003,,no
Product quality trajectory tracking in batch processes using iterative learning control based on time-varying perturbation models,"A run-to-run model-based iterative learning control (ILC) strategy for the tracking control of product quality in batch processes is proposed. A linear perturbation model for product quality, linearized around the nominal trajectories, is identified from process operating data using linear regression. To address the problem of model-plant mismatches, model prediction errors in the previous batch run are added to the model predictions for the current batch run. On the basis of the modified predictions, an ILC law with direct error feedback can be explicitly obtained. The convergence of tracking error under ILC is analyzed. To overcome the detrimental effects of unmeasured disturbances and process variations, it is proposed in this paper that the perturbation model should be updated in a batchwise manner. After the completion of each batch, a batchwise perturbation model, linearized around the control trajectory for that batch, is identified. A forgetting factor is introduced so that data from the more recent batch runs are weighted more than data from earlier batch runs. The proposed technique is successfully applied to a simulated batch reactor and a simulated batch polymerization process.",2003,10.1021/ie034006j,no
A comprehensive robotic model for neural & acoustic signal processing in bats,"A scale-accurate robotic model of a bat biosonar head which reproduces all known functional characteristics of its biological paragon is being build in the CIRCE project. The heads of bats are a set of intelligent, adaptively controlled sonar antennae, where signal processing is carried out both in the acoustic and in the neural domain. The primary project goal is to identify hypotheses of high explanatory power for the function of these biological systems. The progress report given here outlines involved engineering challenges and explains design decisions in the areas of ultrasonic transducers, antennae shapes, actuation and neuromimetic signal processing.",2003,10.1109/CNE.2003.1196860,no
Canadian Aerosol Module: A size-segregated simulation of atmospheric aerosol processes for climate and air quality models - 1. Module development,"A size-segregated multicomponent aerosol algorithm, the Canadian Aerosol Module (CAM), was developed for use with climate and air quality models. It includes major aerosol processes in the atmosphere: generation, hygroscopic growth, coagulation, nucleation, condensation, dry deposition/sedimentation, below-cloud scavenging, aerosol activation, a cloud module with explicit microphysical processes to treat aerosol-cloud interactions and chemical transformation of sulphur species in clear air and in clouds. The numerical solution was optimized to efficiently solve the complicated size-segregated multicomponent aerosol system and make it feasible to be included in global and regional models. An internal mixture is assumed for all types of aerosols except for soil dust and black carbon which are assumed to be externally mixed close to sources. To test the algorithm, emissions to the atmosphere of anthropogenic and natural aerosols are simulated for two aerosol types: sea salt and sulphate. A comparison was made of two numerical solutions of the aerosol algorithm: process splitting and ordinary differential equation (ODE) solver. It was found that the process-splitting method used for this model is within 15% of the more accurate ODE solution for the total sulphate mass concentration and <1% accurate for sea-salt concentration. Furthermore, it is computationally more than 100 times faster. The sensitivity of the simulated size distributions to the number of size bins was also investigated. The diffusional behavior of each individual process was quantitatively characterized by the difference in the mode radius and standard deviation of a lognormal curve fit of distributions between the approximate solution and the 96-bin reference solution. Both the number and mass size distributions were adequately predicted by a sectional model of 12 bins in many situations in the atmosphere where the sink for condensable matter on existing aerosol surface area is high enough that nucleation of new particles is negligible. Total mass concentration was adequately simulated using lower size resolution of 8 bins. However, to properly resolve nucleation mode size distributions and minimize the numerical diffusion, a sectional model of 18 size bins or greater is needed. The number of size bins is more important in resolving the nucleation mode peaks than in reducing the diffusional behavior of aerosol processes. Application of CAM in a study of the global cycling of sea-salt mass accompanies this paper [Gong et al., 2002].",2003,10.1029/2001JD002002,no
"Economic evaluation on fuzzy analytic hierarchy process model for recycling Cu(In,Ga)Se-2 PV modules","According to recycling the toxicity or noble materials and environment protection, the economic evaluation on the project has been primarily made and studied in this letter using Fuzzy Analytic Hierarchy Process model under a multi-object guidelines. Based on the economic evaluation system, the cost-efficient and optimum materials recycling process could be considered. The key issues for recycling the Cu(In,Ga)Se-2 modules were also discussed in this letter.",2003,,no
Concepts for river water quality processes for an integrated river basin modelling,"Although there are various ways to model the bio-chemical processes in rivers, two main trends can be distinguished: the traditional QUAL2E-type models and the RWQM-type models, developed by extending and adapting the ASM equations. Both approaches have their weak and strong points. This paper focuses on the application of these concepts in an integrated modelling context. To this purpose, the models were adapted to respect the mass balances, to enable linkage to WWTP outputs and to diffuse pollution sources. Both approaches have been included in ESWAT - Extended Soil and Water Assessment Tool which was developed by the authors to allow for an integral modelling of the water quantity and quality processes in river basins. A comparison and evaluation of both quality models is performed for the river Dender (Belgium). It is shown that both models give a reasonable fit for dissolved oxygen and ammonia. However, the dynamics of the quality processes for the RWQM model are slower, due to the time that bacteria need to adapt to changes of the loads of organic matter.",2003,,no
A stochastic model to evaluate performance of testing and taping processes on an LSI production system,"An automated testing and taping process on a LSI production system is modelled as a variant of M/G/1 finite capacity queue with multiple transactions, where each package receives either a normal service or normal service plus postservice. The system has N spaces for waiting. We successfully define states of system to regard as those of a Markov renewal process even though some time points are not regeneration points. Using the model, we derive properties of stochastic behaviour of the system, in particular, throughput, process of overflows, busy period, and idle period as cost-effective tools for system evaluation. (C) 2003 Elsevier Ltd. All rights reserved.",2003,10.1016/S0895-7177(03)00348-0,no
Proposal of an empirical viscosity model for quality control in the polymer extrusion process,"An empirical model to evaluate the viscosities of thermoplastic polymers in the extrusion process for quality control is proposed in this article. The viscosity is calculated from the screw rotation speed, melt temperature, geometric dimensions of the extruder and material constants, which were experimentally determined. Polypropylene (PP) was used in this study to test the effectiveness of the model. Comparing the calculated results, it was found that the viscosities obtained with the present model are in agreement with those obtained by using an in-line rheometer. The result shows that the proposed method can be applied to control the polymer extrusion process without imbedding an in-line rheometer, which influences the output of the product. (C) 2003 Elsevier Science Ltd. All rights reserved.",2003,10.1016/S0142-9418(02)00163-0,no
Defect distribution model validation and effective process control,"Assumption of the underlying probability distribution is an essential part of effective process control. In this article, we demonstrate how to improve the effectiveness of equipment monitoring and process induced defect control through properly selecting, validating, and using the hypothetical distribution models. The testing method is based on probability plotting, which is made possible through order statistics. Since each ordered sample data point has a cumulative probability associated with it, which is calculated as a function of sample size, the assumption validity is readily judged by the linearity of the ordered sample data versus the deviate predicted by the assumed statistical model from the cumulative probability. A comparison is made between normal and lognormal distributions to illustrate how dramatically the distribution model could affect the control limit setting. Examples presented include defect data collected on SP1 the dark field inspection tool on a variety of deposited and polished metallic and dielectric films. We find that the defect count distribution is in most cases approximately lognormal. We show that normal distribution is an inadequate assumption, as clearly indicated by the non-linearity of the probability plots. Misuse of normal distribution leads to a too optimistic process control limit, sometimes 50% tighter than suggested by the lognormal distribution. An inappropriate control limit setting consequently results in an excursion rate at a level too high to be manageable. In contrast, lognormal distribution is positively skewed, which adequately takes into account the fact that defect count distribution is characteristic of a long tail. In essence, use of lognormal distribution is a suggestion that the long tail be treated as part of the process entitlement (capability) instead of process excursion. The adjustment of the expected process entitlement is reflected and quantified by the skewness of lognormal distribution, yielding a more realistic estimate (defect count control limit). It is of particular importance to use a validated probability distribution when the sample size is small. Statistical process control (SPC) chart is generally constructed on the assumption of normality of the underlying population. Although this assumption is not true, as we discussed in the previous paragraph, the sample average will follow a normal distribution regardless of the underlying distribution according to the central limit theorem. However, this practice requires a large sample, which is sometimes impractical, especially in the stage of process development and yield ramp-up, when the process control limit is and has to be a moving target, enabling a rapid and constant yield-learning with minimal amount of production interruption and/or resource reallocation. In this work, we demonstrate that a validated statistical model such as lognormal distribution enables us to monitor the progress in a quantifiable and measurable way, and to tighten the control limits smoothly and systematically. To do so, we use the verified model to make a deduction about the expected defect count at a predetermined deviate, say 3sigma. The estimate error or the range is a function of sample variation, sample size, and the confidence level at which the estimation is being made. If we choose a fixed sample size and confidence level, the defectivity performance is explicitly defined and gauged by the estimate and the estimate error.",2003,10.1117/12.485235,no
Process modeling based on atomistic understanding for state of the art CMOS device design,"Atomistic modeling methods axe emerging as a powerful tool to understand the physical behavior of complex systems. However, continuum process simulators are the core of state of the art TCAD simulators and substantial challenges must be overcome to make effective use of atomistic techniques. As device dimensions shrink into the sub 100 nm regime, one of the problems faced by device designers is the diffusion of phosphorus (P) from the source/drain region into extensions due to reduction of the source/drain sidewall distance. We use this problem to illustrate how accurate and physical process models can help understand these issues. In particular, we develop a continuum model for phosphorus (P) and fluorine (F) diffusion from our understanding at an atomic level. These models are then calibrated to predict more complex interactions between phosphorus and fluorine.",2003,,no
Application of an integrated modelling process to evaluate an automotive climate control system,"Automobile auxiliary loads significantly impact the fuel use of conventional and advanced vehicles. Improving the method for delivering conditioned air to the vehicle occupants is an effective way to reduce the amount of fuel used for climate control. A potential system for improved delivery of conditioned air is a distributed HVAC system consisting of under-seat heat and cooling units and ducting system. The goal at the U.S. Department of Energy's National Renewable Energy Laboratory (NREL) is to assess thermal comfort, fuel economy, and emissions by using an integrated modeling approach composed of CAD, computational fluid dynamics (CFD), and thermal comfort tools. This paper presents the results of applying NREL's vehicle integrated modeling process to the distributed HVAC system.",2003,,no
Brainstem mechanisms for analyzing temporal patterns of echolocation sounds: a model for understanding early stages of speech processing?,"Because of their stereotyped audio-vocal behavior and highly accessible brainstem circuitry, echolocating bats provide a good model system in which to study the neural mechanisms that underlie the analysis of temporal features of sound. This paper reviews the lower brainstem auditory circuitry and describes selected forms of information processing that are performed in the pathways of the lower brainstem, and auditory midbrain (inferior colliculus (IC)). Several examples of neural circuits in echolocating bats point out the ways in which inputs with different properties converge on IC neurons to create selectivity for specific temporal features of sound that are common to speech and echolocation. The initial transformations of auditory nerve input that occur in the lower brainstem pathways include a change in sign from excitatory input to inhibitory output, changes in discharge pattern, and the creation of delay lines. Convergence of multiple inputs on neurons in the IC produces tuning for temporal features of sound including duration, the direction of frequency sweeps, modulation rate, and interstimulus interval. The auditory cortex exerts control over some of this processing by sharpening or shifting neuronal filter properties. The computational processes that occur in the IC result in integration across a time scale that is consistent with the rate at which biological sounds are produced, whether they be echolocation signals or human speech components. (C) 2002 Elsevier Science B.V. All rights reserved.",2003,10.1016/S0167-6392(02)00100-0,no
The evaluation of model of deproteination process,"Chrome-tanned leather manufacture, so-called shavings are produced which represent a considerable volume of hazardous waste. One of the ways of its processing is the chemical mode (acid, alkaline or enzymatic hydrolysis). For a number of reasons, the method most suitable is the enzymatic, where a soluble collagen hydrolyzate and solid fraction are formed. The utilization of this solid waste of high chrome content requires, in the first place, reducing the content of protein fraction contaminating the material. The chrome cake was deproteinated by alkaline hydrolysis in a pressure vessel, where a 91% removal of protein from the solid fraction was achieved by conversion into soluble form. All processes are putted to test at laboratory plant. The measured and analyzed date are evaluated within mathematical model.",2003,,no
Guideline validation in multiple trauma care through business process modeling,"Clinical guidelines can improve the quality of care in multiple trauma. In our Department of Trauma Surgery a specific guideline is available paper-based as a set of flowcharts. This format is appropriate for the use by experienced physicians but insufficient for electronic support of learning, workflow and process optimization. A format and logically consistent version represented with a standardized meta-model, is necessary for automatic processing. In our project we transferred the paper-based into an electronic format and analyzed the structure with respect to format errors. Several errors were detected in seven error categories. The errors were corrected to reach a formally and logically consistent process model. In a second step the clinical content of the guideline was revised interactively using a process-modeling tool Our study reveals that guideline development should be assisted by process modeling toots, which check the content in comparison to a meta-model. The meta-model itself could support the domain experts in formulating their knowledge systematically. To assure sustainability of guideline development a representation independent of specific applications or specific provider is necessary. Then, clinical guidelines could be used for eLearning, process optimization and workflow management additionally. (C) 2003 Elsevier Science Ireland Ltd. All rights reserved.",2003,10.1016/S1386-5056(03)00057-1,no
A CC-based security engineering process evaluation model,"Common Criteria(CC) provides only the standard for evaluating information security product or system, namely Target of Evaluation (TOE). On the other hand, SSE-CMM provides the standard for Security Engineering Process Evaluation. Based on the CC, TOE's security quality may be assured, but its disadvantage is that the development process is neglected. SSE-CMM seems to assure the quality of TOE developed in an organization equipped with security engineering process, but the TOE developed in such environment cannot avoid CC-based security assurance evaluation. We propose an effective method of integrating two evaluation methods, CC and SSE-CMM, and develop CC-based assurance evaluation model, CC_SSE-CMM. CC_SSE-CMM presents the specific and realistically operable organizational security process maturity assessment and CC evaluation model.",2003,10.1109/CMPSAC.2003.1245332,no
Development of a recommendation system with multiple subjective evaluation process models,"Current BtoC recommendation services utilize consumers' purchased log as criteria for selecting information, yet it includes little information of the reason why he bought the items. Thus it is difficult to recommend the suitable information for each consumer. We have observed how each consumer judges his likes and dislikes on objects viewing them. We have modeled each consumer's evaluation process by relationships among physical features of objects, each consumer's subjective interpretations and preferences. Thus, based on the models, our system can estimate users' subjective evaluations and preferences from physical features of objects to perform a suitable recommendation. We have also build situated preference models according to the usage of the items. The recommendation system refers such situated preference models together with the subjective evaluation model as the consumer's decision-making model for selecting objects.",2003,10.1109/CYBER.2003.1253474,no
Cognitive and motivational processes underlying coping flexibility: A dual-process model,"Discriminative facility was proposed as a cognitive process and need for closure was proposed as a motivational process underlying coping flexibility. The dual-process model posits that need for closure influences discriminative facility, which in turn modifies coping flexibility and psychological adjustment. In Study 1, results of structural equation modeling provided support for the dual-process model. This model was further examined using experimental methods. (Study 2) and a prospective design (Study 3). Consistent with the dual-process model, results from all 3 studies showed that participants who were more motivated to seek alternative coping strategies tended to encode stressful, situations in a more differentiated way. These individuals used a greater variety of strategies to. fit different situational demands and Were better adjusted.",2003,10.1037/0022-3514.84.2.425,no
"Measurement and mathematical modelling of the heat transfer in the glass forming process, in consideration of the heat transfer coefficients and radiation influences","During forming, glass to mould contact temperatures are the significant factor for the final glass surface quality. For a good characterization of this contact condition the heat transfer by conduction and radiation needs to be described in detail. A laboratory testing unit was set up to investigate the influence of radiation emitted by different glass compositions and the influence of different mould materials used during forming. To support the experimental results, heat transfer conditions and temperatures were modelled at given boundary conditions using the CFD-Codc FLUENT software program applying discrete models. Measurement and modelling results show that the radiation influence on heat transfer is strongly dependent on the spectral absorption coefficient of the glass which affects heat flux densities and surface temperatures. Results show up to 15 % higher heat flux density for amber glass compared to flint glass. In addition, internal glass temperature distributions are strongly affected. If glass throughput, type of forming tools and mould cooling settings are carefully adapted to the optical characteristics of the glass, an optimal surface quality and physical strength can be achieved, which may lead to significant economic benefits. Due to the complex interactions of the parameters which influence glass to mould heat transfer mathematical modelling proves to be essential for future developments in forming technology.",2003,,no
Error associated with assuming a finite regular geometry as an infinite one for modeling of transient heat and mass transfer processes,"Error (epsilon) due to assuming a finite regular geometry as an infinite one was determined for transient heat and mass transfer processes. Dimensionless numbers Fourier (Fo) and Biot (Bi), and geometrical properties (type, shape, and size) were the possible effective parameters on epsilon. Types of geometries used were slab (circle, square, and rectangle) and rod (cylinder and square). The error decreased with decreasing To, with the epsilon-curves shifting in parallel to the Fo-axis in the decreasing direction with increasing Bi, and Bi greater than or equal to 100 was found to be infinite for all geometries. The error for circle and square slab geometries was same at all Bi. The size of the dimension through which the transfer occurs did not affect the error in all geometries. However, the size of the dimensions, through which the transfer occurs was neglected, affected the error in squared slabs. It increased with decreasing ratio of width over length, e.g. in the order of square slab and rectangular slab of 1 x 2, 1 x 5, and 1 x 10. The error was same for the cylindrical and square rods for Bi = 0.01 and 0.1. For Bi greater than or equal to 1, square rod had greater epsilon values than cylindrical rod at the same Fo-Bi. An error chart was constructed as a function of Fo, Bi, and geometrical properties that can be used to determine the error due to assuming a finite slab and rod geometry as an infinite one during transient heat or mass transfer processes. (C) 2003 Elsevier Science Ltd. All rights reserved.",2003,10.1016/S0260-8774(02)00470-3,no
Development of a program logic model to measure the processes and outcomes of a nurse-managed community health clinic,"Evaluation is an essential process that permits assessing the effectiveness and efficiency of planned programs. In implementing a new nurse-managed Community Health Clinic targeting services for the homeless and underserved, the stakeholders considered an evaluation process integral to the planning stage of the clinic as a whole as well as of all the different programs being offered. The program logic model was chosen and modified to guide evaluation. Work to develop the evaluation model and its components began before the clinic opened. This article describes the development of the modified program logic model, how it was modified, and the rationale for its modifications. We highlight the process of developing the evaluation model because we found limited descriptions of the process in the literature. The evaluation process itself will be evaluated on an ongoing basis to determine if it is capturing the evaluation needs of the clinic project accurately.",2003,10.1016/S8755-7223(03)00070-X,no
An optimal design for process quality improvement: modelling and application,"Existing research works on process quality improvement focus largely on the linkages between quality improvement cost and production economics such as set-up cost and defect rate reduction. This paper deals with the optimal design problem for process improvement by balancing the sunk investment cost and revenue increments due to the process improvement. We develop an optimal model based on Taguchi cost functions. The model is validated through a real case study in automotive industry where the 6-sigma DMAIC methodology has been applied. According to this research, the management can adjust the investment on prevention and appraisal costs on quality improvement that enhances process capability, reduces product defect rate and, as a result, generates remarkable financial return.",2003,10.1080/09537280310001626197,no
Cognitive efficiency and images of stratification: FK model revisited under condition of truncation of scanning process,"Fararo and Kosaka (1991) have formalized the mechanism concerning images of stratification and self-location in a class system. The Fararo=Kosaka model provided theoretical explanation of the finding of the phenomenon of middle classification in self-location in modern societies. However, the FK model failed to explain the actual variance of self-location within a class. This study attempts to improve the FK model by adding a new axiom. The new axiom is about the condition of the truncation of the scanning process of the others' status. Revised FK model explains the variance of self-location as a result of cognitive efficiency. Furthermore, this model enables us to consider social condition of emergence of ""upgrading bias"" (Fararo and Kosaka 1991).",2003,,no
Progress and problems in the modeling of non-ferrous processes,"Fifty years ago when Professor Yazawa began his career, the field of process simulation and modeling was in its infancy. Since then, tremendous progress has been made in our ability to model metal production flowsheets from mine to refined metal. This progress has come about by parallel developments in basic understanding of process chemistry and in computational techniques. This paper will review the progress made in process understanding and modeling in lead, zinc, and copper metallurgy from its inception to the present, and the role of Professor Yaiawa's work in furthering this progress. Specific examples will be given for the sintering of lead concentrates, slag fuming, zinc concentrate roasting, flash smelting of copper, and pyro-extraction of zinc. Factors limiting further progress will be described.",2003,,no
Pain self-management in the process and outcome of multidisciplinary treatment of chronic pain: Evaluation of a stage of change model,"For chronic pain patients, acceptance of a self-management approach for pain may influence success in treatment, and adopting such a perspective may be conceptualized as a stage of change model. For 65 chronic pain patients in multidisciplinary treatment programs, we examined whether pretreatment self-management stage, assessed with Pain Stage of Change Questionnaire subscales, affected improvements in outcomes, and whether changes in stage represented a therapeutic process factor. Results showed (a) low precontemplation, high contemplation, and high action attitudes at pretreatment predicted greater improvements in outcomes than the opposite pattern of attitudes; (b) pre-to midtreatment changes in precontemplation and contemplation attitudes predicted mid-to posttreatment changes in pain severity and interference, but not vice versa. Results support the usefulness of a stage model in conceptualizing patients' acquisition of a self-management approach to pain, and suggest that early-treatment progression across stages may lead to reductions in pain severity and lifestyle interference.",2003,10.1023/A:1025720017595,no
Extending production models to include process error in the population dynamics,"Four methods for fitting production models, including three that account for the effects of error in the population dynamics equation (process error) and when indexing the population (observation error), are evaluated by means of Monte Carlo simulation. An estimator that represents the distributions of biomass explicitly and integrates over the unknown process errors numerically (the NISS estimator) performs best of the four estimators considered, never being the worst estimator and often being the best in terms of the medians of the absolute values of the relative errors. The total-error approach outperforms the observation-error estimator conventionally used to fit dynamic production models, and the performance of a Kalman filter based estimator is particularly poor. Although the NISS estimator is the best-performing estimator considered, its estimates of quantities of management interest are severely biased and highly imprecise for some of the scenarios considered.",2003,10.1139/F03-105,no
Heat flux determination in the gas-tungsten-arc welding process by using a three-dimensional model in inverse heat conduction problem,"Heat input measurement during the welding process is a highly complex task, primarily because the welding arc is a non-uniform heat source. To solve this problem, various analytical and numerical approaches have been proposed. They can be divided into two categories, the direct and the inverse problems of heat transfer. The problem is considered direct when all the boundary conditions are given for the outside surface of the domain. In an inverse problem, information concerning one or more boundary conditions is unknown. Thus, an inverse problem requires the knowledge of temperature at a fixed point inside the domain, in order to provide the temperature profile at the surface. Here, a methodology is proposed to calculate the heat flux delivered to the workpiece during the welding process. The inverse-problem technique is based on the conjugated gradient method with an adjoint problem. The proposed model is based on three-dimensional heat transfer with spatial and temporal heat source variation. To assess the proposed technique, different welding conditions were used in the gas-tungsten-arc welding process. The arc heat input was estimated by temperature measurement at the surface at the rear of the weld, with ten thermocouples equally spaced along the middle line of the plate.",2003,10.1068/htjr086,no
Part dimensional error and its propagation modeling in multi-operational machining processes,"In a multi-operational machining process (MMP), the final product variation is an accumulation or stack-up of variation from all machining operations. Modeling and control of the variation propagation is essential to improve product dimensional quality. This paper presents a state space model and its modeling strategies to describe the variation stack-up in MMPs. The physical relationship is explored between part variation and operational errors. By using the homogeneous transformation approach, kinematic modeling of setup and machining operations are developed. A case study with real machined parts is presented in the model validation.",2003,10.1115/1.1532007,no
Virtual optical experiments. Part I. Modeling the measurement process,"In recent years, internal laser probing techniques that exploit the electro-optical and the thermo-optical effects have been introduced. Space-resolved and time-resolved measurements of charge-carrier and temperature distributions in the interior of semiconductor samples have thus become possible. For a profound analysis and the optimization of these measurement techniques, a physically rigorous model for simulating the entire measurement process is presented. The model includes the electrothermal device simulation of the sample's operating condition, the calculation of the resulting refractive-index modulations, the simulation of wave propagation through the device under test, the imaging lenses and aperture holes, and the simulation of the detector response. As an essential part of this model, a numerically efficient algorithm for simulating wave propagation in large computational domains has been developed. The decisive step is introduction of a suitably chosen set of computational variables that allows a significantly coarser discretization width without loss of accuracy. (C) 2003 Optical Society of America.",2003,10.1364/JOSAA.20.000698,no
Comparing different global positioning system data processing techniques for modeling residual systematic errors,"In the case of traditional Global Positioning System (GPS) data processing algorithms, systematic errors in GPS measurements cannot be eliminated completely, nor accounted for satisfactorily. These systematic errors can have a significant effect on both the ambiguity resolution process and the GPS positioning results. Hence this is a potentially critical problem for high precision GPS positioning applications. It is therefore necessary to develop an appropriate data processing algorithm which can effectively deal with systematic errors in a nondeterministic manner. Recently several approaches have been suggested to mitigate the impact of systematic errors on GPS positioning results: the semiparametric model, the use of wavelets, and new stochastic modeling techniques. These approaches use different bases and have different implications for data processing. This paper aims to compare the above three methods, in both the theoretical and numerical sense.",2003,10.1061/(ASCE)0733-9453(2003)129:4(129),no
Study on the efficiency of the digging process using the model of excavator bucket,"In the first part of the paper, an experimental program is presented investigating the soil cutting problem, with the application of vertical rigid walls at various widths as the working tools. It was found that when the tool width equalled the width of the soil bin, the soil cutting problems might be treated as plane strain processes. For the tools for which no interaction with the sidewalls of the bin was observed, the zone of the plane strain deformations occurred in the central part of the tools. In the second part of this paper a new experimental program of laboratory tests is presented, for a tool model in the shape of an excavator's bucket equipped with teeth. The experimental verification of the influence of teeth (number of teeth and the position of teeth on the bucket's inside lip) on the efficiency of the digging cycle is the aim of this paper. For high values of teeth spacing, the superposition of the plane strain deformation mechanism with three-dimensional failure modes within the soil was observed. For the low values of teeth spacing, the teeth did not act as separate three-dimensional objects but as one wide tool built up from several modules. As a consequence, the deformation pattern in front of such an assembly of teeth was again the plane strain deformation pattern. (C) 2003 ISTVS. Published by Elsevier Ltd. All rights reserved.",2003,10.1016/j.jterra.2003.12.003,no
Impacts of model calibration on high-latitude land-surface processes: PILPS 2(e) calibration/validation experiments,"In the PILPS 2(e) experiment, the Snow Atmosphere Soil Transfer (SAST) land-surface scheme developed from the Biosphere-Atmosphere Transfer Scheme (BATS) showed difficulty in accurately simulating the patterns and quantities of runoff resulting from heavy snowmelt in the high-latitude Torne-Kalix River basin (shared by Sweden and Finland). This difficulty exposes the model deficiency in runoff formations. After representing subsurface runoff and calibrating the parameters, the accuracy of hydrograph prediction improved substantially. However, even with the accurate precipitation and runoff, the predicted soil moisture and its variation were highly ""model-dependent"". Knowledge obtained from the experiment is discussed. (C) 2003 Elsevier Science B.V. All rights reserved.",2003,10.1016/S0921-8181(03)00006-7,no
An assessment of the emissions inventory processing systems EMS-2001 and SMOKE in grid-based air quality models,"In the United States, emission processing models such as Emissions Modeling System-2001 (EMS-2001), Emissions Preprocessor System-Version 2.5 (EPS2.5), and the Sparse Matrix Operator Kernel Emissions (SMOKE) model are currently being used to generate gridded, hourly, speciated emission inputs for urban and regional-scale photochemical models from aggregated pollutant inventories. In this study, two models, EMS-2001 and SMOKE, were applied with their default internal data sets to process a common inventory database for a high ozone (O-3) episode over the eastern United States using the Carbon Bond IV (CB4) chemical speciation mechanism. A comparison of the emissions processed by these systems shows differences in all three of the major processing steps performed by the two models (i.e., in temporal allocation, spatial allocation, and chemical speciation). Results from a simulation with a photochemical model using these two sets of emissions indicate differences on the order of +/-20 ppb in the predicted 1-hr daily maximum O-3 concentrations. It is therefore critical to develop and implement more common and synchronized temporal, spatial, and speciation cross-reference systems such that the processes within each emissions model converge toward reasonably similar results. This would also help to increase confidence in the validity of photochemical grid model results by reducing one aspect of modeling uncertainty.",2003,,no
Utility of relatively simple models for understanding process parameter effects on FSW,"In this paper two models of the Friciton Stir Welding process will be discussed. The first is a thermal model which is used to simulate temperature profiles in friction stir welds. The required inputs for this model are total input power, tool geometry, thermo-physical properties of the material being welded (density, conductivity, specific heat) welding speed and boundary conditions. The output from the model can be used to rationalize observed hardness and microstructure distributions. The second model is a fully coupled, 2-D fluid mechanics based process model that is used to make parametric studies of variations in properties of the material to be welded (mechanical and thermophysical) and variations in welding parameters. Results from this model provide insight regarding the effect of material properties on friction stir weldability and on potential mechanisms of defect formation. Both models are readily implemented on a Windows or similar computing platform. Limitations and critical issues for extension of the models will be discussed.",2003,,no
Capacity measure for finite state Markov modeling of the phase process in flat fading channels,"In this paper we investigate finite state Markov modeling of the phase process in flat fading channels. Phase modulations are often used in mobile communication systems. They rely on the fact that the receiver can estimate the channel phase with acceptable accuracy. In the absence of pilot signals or when the channel varies frequently this becomes a very challenging task. Inaccuracy in phase estimation results in dramatic performance degradation. Here we propose mapping of the channel phase process into a finite state Markov model. In particular we study the effect of number of phase quantization levels and thresholds on the capacity under different signal to noise ratio and fading correlation conditions. In BPSK signaling, non-uniform 4-state phase quantization will result in noticeable capacity enhancement under all signal to noise and fading correlation conditions, while in low signal to noise conditions 8-state channel outperforms 4-state channel slightly. Both constant envelope and Rayleigh envelope models are considered in the numerical capacity analyses. The capacity improvements are also compared with the improvements in finite state Markov mapping of the fading envelope.",2003,10.1109/ICTEL.2003.1191473,no
A probabilistic model for measurement processes,"In this paper, the probabilistic description of a (any) measurement process is addressed in general terms. A model is proposed which identifies two main sub-processes, named observation and restitution. The former accounts for transformations in the measuring system giving rise to the observable output, the latter includes data processing which yields the final measurement result. The observation process is represented as a parametrical probabilistic mapping, from the set of the possible values of the measurand into the set of the observable values. In this way it is possible to represent the effects of both random variations and systematic deviations that may affect the measurement process, within a coherent probabilistic framework. Consequently restitution is viewed as a probabilistic inversion of the mapping describing observation. Finally the overall probabilistic description of the whole measurement process is provided. The final result of measurement is expressed by a probability distribution over the set of the possible values of the measurand. (C) 2003 Elsevier Ltd. All rights reserved.",2003,10.1016/S0263-2241(03)00026-5,no
A complexity effective communication model for behavioral modeling of signal processing applications,"In this paper, we argue that the address space of memory regions that participate in inter task communication is over-specified by the traditional communication models used in behavioral modeling, resulting in sub-optimal implementations. We propose shared messaging communication model and the associated channels for efficient inter task communication of high bandwidth data streams in behavioral models of signal processing applications. In shared messaging model, tasks communicate data through special memory regions whose address space is unspecified by the model without introducing non determinism. Address space to these regions can be assigned during mapping of application to specific architecture, by exploring feasible alternatives. We present experimental results to show that this flexibility reduces the complexity (e.g., communication latency, memory usage) of implementations significantly (up to an order of magnitude).",2003,10.1109/DAC.2003.1219035,no
Combining data-based and process-based approaches to minimize the complexity of a reactive sulphate transport model,"In this study, advanced methods of nonlinear data analysis and mechanistic models are combined to investigate transport and turnover of airborne sulphate in the aquifer of a forested watershed. The objective is to understand why adjacent sites differ substantially with respect to short-term and long-term sulphate concentration time series. First, a groundwater model was parameterized as a basis for a coupled model. A detailed analysis revealed substantial uncertainties with respect to, e.g. residence times and the spatial pattern of groundwater discharge. Second, a time series of sulphate concentration in the catchment runoff was analysed using artificial neural networks. The chemical hydrograph provides spatially integrated information about the groundwater of the watershed. In addition, the pronounced short-term dynamics reflect the varying contribution of shallow and deep groundwater on stream discharge. The neural network revealed a substantial change in these dynamics during the last 14 years. Based on these results, only a few of a variety of candidate processes were selected for a mechanistic model of reactive sulphate transport in the aquifer. Sulphate transport and interaction with the matrix is described as an equilibrium sorption process. A very simplified model is used for water transport. Although none of the parameters was fitted by inverse modelling, the model matched the long-term dynamics of sulphate concentration. In addition, it succeeded in giving the envelope of observed short-term variability of sulphate concentration due to the varying contributions of different flow paths. It is concluded that using different process-based and. data-based modelling approaches in an iterative way can considerably help the optimization of hydrologic models with respect to the available data, thus avoiding some of the problems associated with overparameterized mechanistic models.",2003,,no
Alternative decision modelling techniques for the evaluation of health care technologies: Markov processes versus discrete event simulation,"Markov models have traditionally been used to evaluate the cost-effectiveness of competing health care technologies that require the description of patient pathways over extended time horizons. Discrete event simulation (DES) is a more flexible, but more complicated decision modelling technique, that can also be used to model extended time horizons. Through the application of a Markov process and a DES model to an economic evaluation comparing alternative adjuvant therapies for early breast cancer, this paper compares the respective processes and outputs of these alternative modelling techniques. DES displays increased flexibility in two broad areas, though the outputs from the two modelling techniques were similar. These results indicate that the use of DES may be beneficial only when the available data demonstrates particular characteristics. Copyright (C) 2002 John Wiley Sons, Ltd.",2003,10.1002/hec.770,no
Simulation and model validation of batch PHB production process using Ralstonia eutropha,"Mathematical modeling and simulation of microbial Polyhydroxybutyrate (PHB) production process is beneficial for optimization, design, and control purposes. In this study a batch model developed by Mulchandani et al., [1] was used to simulate the process in MATLAB environment. It was revealed that the kinetic model parameters were estimated off the optimal or at a local optimal point. Therefore, an optimization program was written using MATLAB codes to estimate those parameters again. It resulted in a significant improvement in the accuracy of Mulchandani's kinetic model. The batch model was evaluated using two batch experiments performed in this work and also Mulchandani's batch data when kinetic model parameter values estimated in this work were used. Visual comparisons between the model profiles and experimental data indicate that the model represents the process reasonably. A goodness of fit criterion used in this work and some similar researches proved higher accuracy of Mulchandani's model using this work's kinetic parameter values compared to other models. Theoretical model verification was also performed that lead to identification of the possible limitations of the model.",2003,,no
Using easel for modeling and simulating the interactions of cells in order to better understand the basics of biological processes and to predict their likely behaviors,"Modeling, simulation, visualization, and animation play a significant role in the study of bioinformatics. Research in this area is generally multidisciplinary in nature and collaboratively conducted by researchers with expertise in biology, bioinformatics, computer science, artificial intelligence, mathematics, and statistics. Easel programming language is used for modeling, simulation, visualization, and animation of interactions of cells in order to better understand the basics of biological processes and to predict their likely behaviors. This paper presents a computer science - modeling, simulation, visualization, and animation approach to such research. The paper provides a brief overview of the basic ideas in the ""Message Passing"" Easel program to demonstrate the transmission of signals between cells based on their physical proximity.",2003,10.1109/CSB.2003.1227392,no
Building and evaluation of a structured representation of pharmacokinetics information presented in SPCs: From existing conceptual views of pharmacokinetics associated with natural language processing to object-oriented design,"Objective: Develop a detailed representation of pharmacokinetics (PK), derived from the information in Summaries of Product Characteristics (SPCs), for use in computerized systems to help practitioners in pharmaco-therapeutic reasoning. Methods: Available knowledge about PK was studied to identify main PK concepts and organize them in a preliminary generic model. The information from 1,950 PK SPC- texts in the French language was studied using a morpho-syntactic analyzer. It produced a list of candidate terms (CTs) from which those describing main PK concepts were selected. The contexts in which they occurred were explored to discover co-occurring CTs. The regrouping according to CT semantic types led to a detailed object-oriented model of PK. The model was evaluated. A random sample of 100 PK texts structured according to the model was judged for completeness and semantic accuracy by 8 experts who were blinded to other experts' responses. Results: The PK text file contained about 300,000 words, and the morpho-syntactic analysis extracted 17,520 different CTs. The context of 592 CTs was studied and used to deduce the PK model. It consists of four entities: the information about the real PK process, the experimental protocol, the mathematical modeling, and the influence of factors causing variation. Experts judged that the PK model represented the information in 100 sample PK texts completely in 89% of cases and nearly completely in the other 11%. There was no distortion of meaning in 98% of cases and little distortion in the remainder. Conclusion: The PK model seems to be applicable to all SPCs and can be used to retranscribe legal information from PK sections of SPCs into structured databases.",2003,10.1197/jamia.M1193,no
"Sperm recovering efficiency, a mathematical model, is designed to objectively evaluate semen processing techniques and methods","Objective: To propose the sperm recovering efficiency (SRE) model, an equation that can be used to objectively quantify the technical skill of individual operators processing semen specimens in an andrology laboratory. Additionally, this equation can be used to compare different semen processing methods and protocols. This model integrates the semen parameters of initial sperm concentration, motility, morphology, and final sperm recovery to produce a unique number that can be used to compare results. Design: A theoretical and deductive analysis. Setting: Private assisted reproductive units. Patient(s): None. Intervention(s): None. Main Outcome Measure(s): Motile progressive and morphologically normal sperm concentration. Result(s): The SIZE is a mathematical equation that can be used to compare initial semen parameters to postpreparation concentration, progressive motility, and normal morphology. The result is a number between 0 and 1. The model constitutes an objective means for determining the efficiency of a particular technique (gradients, migration, or filtration) or to set up the best conditions for that technique (time, gravities, medium volume, and sample volume). Additionally, the SRE permits comparisons between different operators, media, lots, laboratory conditions, treatments, and sample parameters. It can be used as a tool in laboratory quality control and for multicenter studies. Conclusion(s): Sperm recovering efficiency (SRE) is an objective method to compare the results of semen processing techniques.",2003,10.1016/S0015-0282(03),no
Application of radar-measured rain data in hydrological processes modeling during the intensified observation period of HUBEX,"On the basis of Digital Elevation Model data, the raster flow vectors, watershed delineation, and spatial topological relationship are generated by the Martz and Garbrecht method for the upper area of Huangnizhuang station in the Shihe Catchment with 805 km(2) of area, an intensified observation field for the HUBEX/GAME Project. Then, the Xin'anjiang Model is applied for runoff production in each grid element where rain data measured by radar at Fuyang station is utilized as the input of the hydrological model. The elements are connected by flow vectors to the outlet of the drainage catchment where runoff is routed by the Muskingum method from each grid element to the outlet according to the length between each grid and the outlet. The Nash-Sutcliffe model efficiency coefficient is 92.41% from 31 May to 3 August 1998, and 85.64%, 86.62%, 92.57%, and 83.91%, respectively for the 1st, 2nd, 3rd, and 4th flood events during the whole computational period. As compared with the case where rain-gauge data axe used in simulating the hourly hydrograph at Huangnizhuang station in the Shihe Catchment, the index of model efficiency improvement is positive, ranging from 27.56 % to 69.39 %. This justifies the claim that radar-measured data are superior to rain-gauge data as inputs to hydrological modeling. As a result, the grid-based hydrological model provides a good platform for runoff computation when radar-measured rain data with highly spatiotemporal resolution axe taken as the input of the hydrological model.",2003,,no
Modeling and understanding different types of process design activities,"One of the major tasks addressed by the chemical industry is the design and revamping of production processes. Increasingly powerful computer-aided tools are available to face these complex tasks. Nevertheless, most design knowledge still rests in the minds of experienced designers. It is desirable to make it part of a computer support environment. Therefore, it is necessary to have a model of the design process. This contribution addresses this objective by introducing a model, based on the identification of three types of design activities: Synthesis, Analysis, and Decision. We discuss the intrinsic characteristics of each type of activity from two different view points: characteristic subactivities and products. Every type of activity consists of typical subordinate subactivities which are distinctive for the type and determine its behavior. On the other hand, activities operate on the results or products of the design process, called product data, including requirements, the representation of the design artifact itself, and arguments. These products also allow a specification of the three types. These ideas are exemplified by modeling the design of a separation system.",2003,,no
A practical approach to measuring and modelling paper fluorescence for improved colorimetric characterisation of printing processes,"Paper Fluorescent Whitening Agent combined with differences in relative Ultra Violet levels between instrument illuminants, and real world viewing illuminants, can be a significant source of error in characterising a printing process, and hence in the ability to accurately reproduce colored images in print. These errors can be dealt with using bi-spectral measurements, but a simpler approach that significantly reduces paper whitening agent induced errors is possible, while using more practical print characterisation instruments and procedures.",2003,,no
Phenolic removal in a model olive oil mill wastewater using Pleurotus ostreatus in bioreactor cultures and biological evaluation of the process,"Pleurotus ostreatus grown in bioreactor batch cultures in a model phenolic wastewater (diluted and sterilized olive oil mill wastewater-OMW), caused significant phenolic removal. Laccase, the sole ligninolytic enzyme detected in the growth environment, was produced during primary metabolic growth. The bioprocess was simulated with the aid of a mathematical model and the parameters of growth were determined. When the fungal biomass was increased in the reactor (during repeated batch experiments) the rate of reducing sugars consumption progressively increased, but a phenolic fraction seemed of being strongly resistant to oxidation. The toxicity of OMW against the seeds of Lepidium sativum and the marine Branchiopoda Artemia sp. was significantly decreased after biotreatment. On the contrary, the toxicity against the freshwater Branchiopoda Daphnia magna was not affected by the treatment, whereas on the soil and freshwater sediments Ostracoda Heterocypris incongruens was slightly decreased. Both treated and untreated OMWs, used as water for irrigation of lettuce and tomato plants, did not significantly affect the uptake of several nutrients by the cultivated plants, but resulted in a decrease in the plant yields, which was minimized when high OMW dilutions were used. As a conclusion, P. ostreatus is able to reduce phenolic content and toxicity of sterilized OMW, in bioreactor cultures. However, high OMW dilutions should be used, and/or additional treatment should be applied before use of the OMW in the environment, e.g. as water for irrigation. Further research should be done in order to transfer this technology under industrial conditions (e.g. by using unsterilized OMW). (C) 2003 Elsevier Ltd. All rights reserved.",2003,10.1016/S0043-1354(03)00313-0,no
"Confirmatory factor analysis of the WPPSI, WPPSI-R, and WISC-R: Evaluation of a model based on knowledge-dependent and processing-dependent subtests","Previous factor analyses of the Wechsler scales have supported a two-factor (WPPSI and WPPSI-R) and three-factor solution (WISC-R). The present paper explores the validity of a four-factor model across these three instruments. The four-factor model maintains Wechsler's original distinction between verbal and nonverbal domains but distinguishes between ""knowledge dependent"" and ""processing dependent"" factors within each domain. Four separate LISREL maximum likelihood confirmatory factor analyses were performed on the 'WPPSI, WPPSI-R, and WISC-R standardization samples. A best-fitting model was determined by comparing the present four-factor model to the traditional two-factor model that distinguished between Verbal and Performance and a three-factor model that parallels the WISC-R factor structure. The new four-factor model resulted in a significant improvement of fit compared to both the two-factor and three-factor models across the three samples. The clinical validity of the model was examined by analyzing the profile patterns of language-impaired children (N = 198) tested with the WPPSI and reading-impaired children (N = 230) tested with the WISC-R. Because processing-dependent. tests require more mapping than knowledge-dependent tests, the pattern ""knowledge dependent"" > ""processing dependent"" is predicted. This pattern was confirmed in both samples.",2003,10.1177/073428290302100101,no
Evaluation of modeling notations for basic software engineering in process control,"Process engineers and process control engineers need to discuss the functionality of a plant in an early phase of a project. A ""language"" to communicate between these different skilled engineers is necessary, which is based on the requirements of the process itself. The quality of the notion is strongly depending on an appropriate modeling concept for the process characteristics. The use of ICL (Idiomatic Control Language).and UML (Unified Modeling Language) will be compared and evaluated for,modeling in basic engineering. It addresses to process engineers, process control engineers and technicians. The model should be used for code generation for PLC or DCS systems.",2003,,no
Application of radar-measured rain data in hydrological processes modelling during the intensified observation period of HUBEX,"Raster flow vectors and watershed delineation were generated from the DEM for the upper area of Huangnizhuang station in the Shihe catchment intensified observation field for the HUBEX/GAME project. The Xinanjiang model was then applied for runoff production in each grid element where rain data measured by weather radar at Fuyang station were used as the input of the hydrological model after correction. Those elements are connected by flow vectors with the outlet of drainage catchment, in which runoff is routed by the Muskingum method from each grid element to the outlet. The Nash-Sutcliffe model efficiency coefficient is up to 92.41% from 31 May to 3 August 1998. The index of model efficiency improvement is 51.22% if compared with the case where raingauge data were used. This shows that radar-measured data are superior to raingauge data as the input of hydrological modelling.",2003,,no
Reactive-transport modeling as a technique for understanding coupled biogeochemical processes in surface and subsurface environments,"Reactive-transport models contribute significantly to the field of modern geosciences. A general mathematical approach to solving models of complex biogeochemical systems is introduced. It is argued that even though mathematical models for reactive-transport simulations can be developed at various levels of approximation, the approach for their construction and application to the various compartments of the hydrosphere is fundamentally the same. The workings of coupled transport-reaction systems are described in more detail by means of examples, which demonstrate the similarities in the approach. Three models of the carbon dynamics in redox-stratified environments are compared: porous media flow problems in a coastal sediment and in a contaminated groundwater system; and a surface flow problem in a eutrophic estuary. Considering the interdisciplinary nature of such models, a Knowledge Base System for biogeochemical processes is proposed. Incorporation of the proposed knowledge base in an appropriate modeling framework, such as the Biogeochemical Reaction Network Simulator, proves an effective approach to the modeling of complex natural systems. This methodology allows for construction of multicomponent reactive-transport models applicable to a wide range of problems of interest to the geoscientist.",2003,,no
Longitudinal write-process modeling including measured angular-dependent coercivity,"Recent angular VSM measurements indicate that the remanent coercivity of various types of longitudinal media has an applied field angular dependence that approximates the behavior of Stoner-Wohlfarth particles with easy axes oriented isotropically in-plane. This is in contrast to the Kondorsky style switching (in-plane field, domain wall switching) inherently assumed in most models of the write process. In this article, we incorporate the measured angular dependent switching into a Williams-Comstock style write model. We find that the vertical spacing dependence of the transition width or ""a"" parameter can be much lower than that expected when only the in-plane component of the head field participates in switching the media. This phenomena is verified experimentally by writing at different spacings and reading at a constant spacing. We find that the experimental data can only be matched if the measured angular dependent switching is taken into account. (C) 2003 American Institute of Physics.",2003,10.1063/1.1555379,no
Evaluation of six multinomial models of conscious and unconscious processes with the recall-recognition paradigm,"Six multinomial processing-tree models (W. H. Batchelder & D. M. Riefer, 1999), which include parameters representing conscious and unconscious memory processes, were tested using the recall-recognition paradigm. Data from 2 experiments were fit equally well by 3 of the 6 models. One model was an extension of the generate-recognize model (L. L Jacoby, 1998), and another was an extension of the non-high-threshold model (D. M. McBride & B. A. Dosher, 1999). The 3rd model was the source evaluation model (D. M. McBride & B. A. Dosher, 1999). Values of the parameters of 2 of these 3 models, excepting the non-high-threshold model. responded to experimental manipulations in accordance with the meaning of the parameters. The equivalence of models with regard to goodness-of-fit tests is discussed as is how experiments can be designed to demonstrate the superiority of one model over another. The potential usefulness of these models in the study of amnesia is considered.",2003,10.1037/0278-7393.29.5.779,no
A comprehensive fuzzy multi-objective model for supplier selection process,"Supplier selection is understood as one of the key processes in strategic decision making level in Supply Chains (SC). This paper develops a comprehensive multiple products and multiple suppliers model for this process. Moreover, variou's targets are discussed and analyzed in the form of objectives, in addition to related constraints. Such model development is fulfilled in a real-world situation with wide ranges of uncertainties. In this paper, a fuzzy decision making model is presented. In the proposed Fuzzy Multiple Objectives Decision Making (FMODM) model, all goals, constraints, variables and coefficients are fuzzy. It is shown that with the application of the fuzzy methodology, the complex multi-objective problem is converted to a single one that can be solved and interpreted easily.",2003,,no
How the brain solves the binding problem for language: a neurocomputational model of syntactic processing,"Syntax is one of the components in the architecture of language processing that allows the listener/reader to bind single-word information into a unified interpretation of multiword utterances. This paper discusses ERP effects that have been observed in relation to syntactic processing. The fact that these effects differ from the semantic N400 indicates that the brain honors the distinction between semantic and syntactic binding operations. Two models of syntactic processing attempt to account for syntax-related ERP effects. One type of model is serial, with a first phase that is purely syntactic in nature (syntax-first model). The other type of model is parallel and assumes that information immediately guides the interpretation process once it becomes available. This is referred to as the immediacy model. ERP evidence is presented in support of the latter model. Next, an explicit computational model is proposed to explain the ERP data. This Unification Model assumes that syntactic frames are stored in memory and retrieved on the basis of the spoken or written word form input. The syntactic frames associated with the individual lexical items are unified by a dynamic binding process into a structural representation that spans the whole utterance. On the basis of a meta-analysis of imaging studies on syntax, it is argued that the left posterior inferior frontal cortex is involved in binding syntactic frames together, whereas the left superior temporal cortex is involved in retrieval of the syntactic frames stored in memory. Lesion data that support the involvement of this left frontotemporal network in syntactic processing are discussed. (C) 2003 Elsevier Inc. All rights reserved.",2003,10.1016/j.neuroimage.2003.09.013,no
Development and experimental validation of a numerical thermal model for the evaluation of the depth of laser treated zone in the laser transformation hardening process,"The aim of this work is to develop a mathematical model to predict the depth of laser treated zone in the LTH process. The Fourier equation of heat conduction was solved by using the Finite Difference Method in cylindrical coordinates in order to study the temperature distribution produced in a workpiece and hence to obtain the depth to which hardening occurs. The theoretical simulations were compared with results produced experimentally by a CO2 laser operating in continuous wave, showing good agreement.",2003,,no
"Online batch recipe adjustments for product quality control using empirical models: Application to a nylon-6,6 process","The application of batch profile characterization tools to enhance process understanding by uncovering the signature of the primary disturbances on the profiles and its effect on the product quality is illustrated on a nylon-6,6 process. The historical profile data for the fixed recipe operation are systematically studied to understand the primary disturbances affecting the process, and it is shown that good online predictions of the final product quality are possible much before the completion of the batch from the available measurement profiles. A simple online recipe adjustment strategy based on the predicted quality deviation from the target is proposed. Results show that the recipe adjustments significantly reduce the variation in the final product quality. Issues in the use of empirical prediction models from recipe-based data are discussed, (C) 2003 ISA-The Instrumentation, Systems, and Automation Society.",2003,10.1016/S0019-0578(07)60135-9,no
Statistical models for processing experiments with repeated measurements in the same experimental unit,"The application of three methods of analysis of longitudinal data: analysis of univariate variance, analysis of multivariate variance and analysis of variance were studied for the indicator sum or average of the areas under the curve obtained in the figure of the clotting activity in time. The methods were applied in an experiment in which different types of fermentations were compared for the production of micorrenine aspartic protease. The clotting activity in the solution was analyzed in systematic samplings every 9 h from 27 h to 117 h. A matrix of correlation was obtained showing that 100% of the coefficients between the sampling hours (t1, t2, ... t11) were superior to 0.99, with highly significant results (P < 0.001) for both fermentations. The application of the univariate analysis of variance according to the split plot design was not possible due to the level of correlation between the sampling hours, as well as that of multivariate analysis for there were no sufficient observations in the estimation of its parameters. The analysis was made through the statistical models Wiks, Pillai, Lawley-Hotelling and Roy by reducing the sampling hours to just three. which evidenced significant interaction. The analysis of variance for the indicator sum or average of the areas had the same result as the analysis of multivariate analysis, despite the value the F (Fisher) was higher. This method is advantageous because it permits the application of a multiple range test after the rejection of the equality hypothesis.",2003,,no
Tools for evaluating Veterinary Services: an external auditing model for the quality assurance process,"The author describes the reasons why evaluation processes should be applied to the Veterinary Services of Member Countries, either for trade in animals and animal products and by-products between two countries, or for establishing essential measures to improve the Veterinary Service concerned. The author also describes the basic elements involved in conducting an evaluation process, including the instruments for doing so. These basic elements centre on the following: designing a model, or desirable image, against which a comparison can be made establishing a list of processes to be analysed and defining the qualitative and quantitative mechanisms for this analysis establishing a multidisciplinary evaluation team and developing a process for standardising the evaluation criteria.",2003,,no
A procedure for the design and evaluation of decentralised and model-based predictive multivariable controllers for a pellet cooling process,"The cooling zone of an induration furnace is a highly interactive multivariable process with strong nonlinearities and dissimilar dynamics. Linear controllers, implemented on a first-principles process model, are unable to properly control the unit in a wide operating range. This paper proposes a design procedure which considers relevant process characteristics, such as nonlinearity, interaction, directionality, and dynamics, for the synthesis of decentralised extended PIDs and model-based predictive controllers (MPCs). Linear controllers with variable transformations are used since the process model shows that a Hammerstein model can approximate the process nonlinear behaviour. The decentralised PIDs are tuned using efficient rules that take into account the process interaction. The performance of both control strategies is evaluated for set-point tracking, disturbance rejection, and robustness to modelling errors. Similar results are obtained for the gas temperature control, which is the most important process variable. Slightly better results are obtained with the MPC for the gas pressure, the fastest dynamic variable. (C) 2002 Elsevier Science Ltd. All rights reserved.",2003,10.1016/S0098-1354(02)00212-0,no
On the efficiency of classical RASTA filtering for continuous speech recognition: Keeping the balance between acoustic pre-processing and acoustic modelling,"The efficiency of classical RASTA filtering for channel normalisation was investigated for continuous speech recognition based on context-independent and context-dependent hidden Markov models. For a medium and a large vocabulary continuous speech recognition task, recognition performance was established for classical RASTA filtering and compared to using no channel normalisation, cepstrum. mean normalisation, and phase-corrected RASTA. Phase-corrected RASTA is a technique that consists of classical RASTA filtering followed by a phase correction operation. In this manner, channel bias is as effectively removed as with classical RASTA. However, for phase-corrected RASTA, amplitude drift towards zero in stationary signal portions is diminished compared to classical RASTA. The results show that application of classical RASTA filtering resulted in decreased recognition performance when compared to using no channel normalisation for all conditions studied, although the decrease appeared to be smaller for context-dependent models than for context-independent models. However, for all conditions, recognition performance was. significantly and substantially improved when phase-corrected RASTA was used and reached the same performance level as obtained for cepstrum mean normalisation in some cases. It is concluded that classical RASTA filtering can only be effective for channel robustness, if the impact of the amplitude drift towards zero can be kept as limited as possible. (C) 2002 Elsevier Science B.V. All rights reserved.",2003,10.1016/S0167-6393(02)00030-4,no
Modeling and validation of an electric conductor welding process,"The exothermic conductor welding method which combines casting and welding technology is a highly transient and dynamic process. It includes some very complicated mould filling, conductor melting and solidification problems. Numerical modeling has the potential to become a powerful tool to represent this process. A three dimensional model simulating the fluid flow during mould filling, the melting process of the conductors during filling and the solidification process has been developed in this research. The model assumes that the exothermic welding powder immediately bums and turns into liquid metal. The code uses this amount of high temperature liquid metal to start mould filling. The mould filling and solidification simulation is based on solving the Navier-Stokes-, the continuity- and energy conservation equations. A fluid function F has been used for tracking the free surface. The melting heat and latent heat have been taken into account during the melting and solidification of the conductors. A diffusion model is also included in the code. Finite volume approach has been used. An experimental system has been designed for validation purpose. The simulation results of melting profile in the electric conductor and prediction of shrinkage defects have been compared with experiments. A good agreement has been achieved. An example of applications has been shown in this paper.",2003,,no
The benchmark evaluation process: From experimental data to benchmark model,"The International Criticality Safely Benchmark Evaluation Project (ICSBEP) provides a handbook of descriptions, evaluations, and models of experiments with fissionable material. The ""International Handbook of Evaluated Criticality Safety Benchmark Experiments"" (ICSBEP Handbook) is useful for criticality. safety analysts and nuclear-data evaluators for validation of neutron transport codes and nuclear cross-section sets. Each of the four main parts of the ICSBEP document provides valuable information. The four parts are as follows: Part 1, detailed description of the experiment; Part 2, evaluation of experimental data to obtain parameter values that define the model and their uncertainties; Part 3, derivation and concise description of the benchmark model; and Part 4, sample calculation results. The ICSBEP Handbook provides a practical, standardized format for documenting nuclear experiments. Valuable, previously unknown data are often discovered during the evaluation process. Besides these discoveries, many other things have been learned during this first decade of evaluating and providing benchmark-models of experiments. The current method is described in order to improve understanding of what is required to evaluate benchmark experiments for validation purposes.",2003,,no
MRMAide (TM) model validation process,"The modeling and simulation community has developed many processes for certifying the creditability of simulation models. Many of these processes are implemented throughout the model development cycle and require comparison of the developed model with the system being simulated. With the increased focus on model reuse, these processes need to be tailored to address the development cycle of reusing existing creditable models. This paper outlines the validation process that certifies the creditability of simulation models produced by the MRMAide(TM) (Mixed Resolution Modeling Aide). MRMAide(TM) is a technology that semi-automates the development of model wrappers. These wrappers are used to resolve fidelity differences between models using mixed resolution modeling (MRM) techniques that allow for the reuse of existing simulation models. The MRMAide(TM) validation process extrapolates on existing processes that are implemented throughout the development cycle of a simulation model and addresses MRMAide(TM)'s development processes.",2003,10.1117/12.498035,no
"Development of three-dimensional hydrodynamic and water quality models to support total maximum daily load decision process for the Neuse River Estuary, North Carolina","The Neuse River Estuary was included on the North Carolina Department of Water Quality 303(d) list for nutrients, and was scheduled for total maximum daily load (TMDL) development by spring of 2001. The water quality target of the TMDL was determined by the state to be the chlorophyll-a concentrations in the estuary. EPA Region 4 partnered with the State of North Carolina to provide technical assistance and guidance for nutrient TMDL development in the Neuse River Estuary. The goal was the development of hydrodynamic and water quality models that are sufficient to simulate the complex circulation and water quality kinetics within the system, including salinity and temperature stratification, wind driven seiching, dissolved oxygen stratification, and longitudinal and lateral variations in nutrient and chlorophyll-a concentrations. A three-dimensional, hydrodynamic, and water quality model was developed in the estuary from Maw Point at the Pamlico Sound boundary, to upstream at Streets Ferry Bridge above New Bern, North Carolina. The complex three-dimensional hydrodynamics of the Neuse estuary were modeled using the Environmental Fluid Dynamics Code (EFDC). EFDC was applied with water surface elevation forcing at the downstream boundary, freshwater inflows at the upstream boundaries, and wind over the water surface of the modeled domain. Water surface elevation, flows, currents, salinity, and temperature were simulated using EFDC. The U.S. EPA Water Quality Analysis Simulation Program (WASP6) was applied for the water quality portion of the model. The eutrophication model of WASP was used to simulate the complex nutrient transport and cycling in the estuary. The purpose of the water quality model is to predict a response in chlorophyll-a and dissolved oxygen concentrations as a function of nutrient loadings and transport throughout the Neuse River Estuary. The model was used to evaluate various loading scenarios and the impact on water quality within the ""use support"" areas within the 303(d) listed segments. The hydrodynamic and water quality models were calibrated for 1998 and confirmed for 1999 and 2000. A comparison of the model simulations with the extensive dataset shows that the models are accurately simulating the longitudinal/seasonal distribution of the hydrodynamics, mass transport, and water quality. The water quality model was used to evaluate TMDL scenarios.",2003,10.1061/(ASCE)0733-9496(2003)129:4(295),no
An evaluation of characteristics of teams in association football by using a Markov process model,"The paper proposes a statistical model of a football match that is useful in providing insights into the characteristics of teams. We evaluate the characteristics by means of maximum likelihood estimators for factors such as home advantage, offensive and defensive strength and their interactions, by taking account of not only goals but also possession of the ball. We build the model on the basis of the real data from the 1999-2000 season of the English Premier League and illustrate the characteristics of the teams in ways that allow us to visualize offensive and defensive strength, preference for playing at home and relative success against particular opposition teams.",2003,10.1046/j.0039-0526.2003.00437.x,no
Understanding youth smoking behavior through modeling the smoking decision process: Lessons learned from a developing country,"The prevalence of youth smoking is well reported in the literature. However, most research so far has been conducted in developed countries, leaving the incidence of youth smoking in developing and less developed countries unexplored and to the mercy of the tobacco industry. The present study forms a prototype called the smoking decision process model on youth smoking behavior, which draws from existing literature. Through this model, researchers, health practitioners, and anti-smoking activist groups in various parts of the developing world can acquire a more holistic view of the factors behind youth smoking behavior in their respective societies and, in turn, focus on the most appropriate means to combat smoking. A case study application of the model is presented in this study, involving Cypriot youth 12 to 18 years of age.",2003,10.1111/j.1559-1816.2003.tb01945.x,no
Comparison of different analysis models to measure plastic strains on sheet metal forming parts by digital image processing,"The principal strains of sheet metals and their limitations while forming can be obtained by using a strain measurement system. A strain measurement may employ one of two different approaches: namely the total least square optimization method or the multiple regression analysis (MRA) method. With both methods plastic strains of deformed parts are calculated based on the nondeformed reference configuration designated by a circle and the deformed configuration, which is a curve-fitting ellipse. In the MRA method, the mathematical formula is simpler reducing the required computations than that of the total least square optimization method. While the formula has a greater margin of error, this margin proves less than significant in the practical application of the method's results. Information from the results of a strain measurement system can be used to determine the sheet metal's formability and strain path allowing engineers to determine at which point, the sheet metal will crack, They can then change the thickness and the material of the sheet metal, or modify its shape accordingly to maximize the metal's efficiency. Strain measurement systems possess practical advantages in their actual application: they improve the quality of sheet metal being produced by minimizing defects in sheet metal during production. One industrial case study of fine stamping electronic part is discussed to demonstrate the proposed methodology. (C) 2003 Elsevier Science Ltd. All rights reserved.",2003,10.1016/S0890-6955(02)00241-9,no
Modelling water quality from source water to tap by integration of process models,"The quality of drinking water delivered to the customer's tap is influenced by a number of processes; namely water treatment, disinfection and changes during transport of treated water via the distribution system. Drinking water guidelines stipulate water quality to be delivered at the customer tap. Recent studies are concerned with safe limits of chlorinated by-products, such as trihalomethanes (THM). As a result, the guideline for total THM (TTHM) in the US has been reduced to 0.08mg/L and further lowering is planned. In Australia, water utilities are examining technical feasibility of reducing TTHM and other chlorinated by-products such as haloacetic acids (HAA). To control the presence of microbial indicators (total coliform counts) a certain concentration of disinfectant is needed. Taste and odour requirements limit free chlorine concentration to less than 0.6mg/L. Analysis of the existing data suggests that the critical problem for maintaining acceptable water quality is to keep the concentration of chlorine within a certain operational window and simultaneously control concentration of disinfection by-products (TTHM and HAA). Chlorine decay, and TTHM and HAA formation, are determined by the dose of chlorine applied, and temperature, concentration and type of dissolved organic carbon (DOC) in water. DOC in treated water is influenced by the DOC concentration of source water and treatment processes applied for its reduction. The most economical and frequently used process for DOC reduction is coagulation with ferric or aluminium salts. To predict the quality of tap water a sequence of process models was developed and connected, integrating a DOC removal model, correlation of DOC and chlorine decay model parameters, a chlorine decay model, a TTHM formation model and the water transport system's hydraulics model. Such an integrated model has the capacity to predict a profile of chlorine and TTHM in the whole distribution system at any time. Possible applications of such an integrated model are also outlined including planning and operation. For example, water treatment plant optimisation, determination of compliance of chlorine operating window and maximum TTHM concentrations.",2003,,no
Evaluating aerosol/cloud/radiation process parameterizations with single-column models and Second Aerosol Characterization Experiment (ACE-2) cloudy column observations,"The Second Aerosol Characterization Experiment (ACE-2) data set along with ECMWF reanalysis meteorological fields provided the basis for the single column model (SCM) simulations, performed as part of the PACE (Parameterization of the Aerosol Indirect Climatic Effect) project. Six different SCMs were used to simulate ACE-2 case studies of clean and polluted cloudy boundary layers, with the objective being to identify limitations of the aerosol/cloud/radiation interaction schemes within the range of uncertainty in in situ, reanalysis and satellite retrieved data. The exercise proceeds in three steps. First, SCMs are configured with the same fine vertical resolution as the ACE-2 in situ data base to evaluate the numerical schemes for prediction of aerosol activation, radiative transfer and precipitation formation. Second, the same test is performed at the coarser vertical resolution of GCMs to evaluate its impact on the performance of the parameterizations. Finally, SCMs are run for a 24-48 hr period to examine predictions of boundary layer clouds when initialized with large-scale meteorological fields. Several schemes were tested for the prediction of cloud droplet number concentration (N). Physically based activation schemes using vertical velocity show noticeable discrepancies compared to empirical schemes due to biases in the diagnosed cloud base vertical velocity. Prognostic schemes exhibit a larger variability than the diagnostic ones, due to a coupling between aerosol activation and drizzle scavenging in the calculation of N. When SCMs are initialized at a fine vertical resolution with locally observed vertical profiles of liquid water, predicted optical properties are comparable to observations. Predictions however degrade at coarser vertical resolution and are more sensitive to the mean liquid water path than to its spatial heterogeneity. Predicted precipitation fluxes are severely underestimated and improve when accounting for sub-grid liquid water variability. Results from the 24-48 hr runs suggest that most models have problems in simulating boundary layer cloud morphology, since the large-scale initialization fields do not accurately reproduce observed meteorological conditions. As a result, models significantly overestimate optical properties. Improved cloud morphologies were obtained for models with subgrid inversions and subgrid cloud thickness schemes. This may be a result of representing subgrid scale effects though we do not rule out the possibility that better large-forcing data may also improve cloud morphology predictions.",2003,10.1029/2003JD003902,no
Numerical validation of the land surface process component of an LSP/R model,"The University of Michigan's land surface process/radiobrightness (LSP/R) model was developed as a step toward linking a traditional SVAT model to satellite microwave observations. The LSP model simulates land-air interactions and estimates surface fluxes, temperature and moisture profiles in soil and vegetation when forced with observed weather. These estimates are used by a microwave emission model, called the R model, that predicts terrain brightness temperatures. In this paper, we evaluate accuracy of the numerical methods used in the LSP model. Such rigorous tests were not conducted during the early development of the model. We describe three test-scenarios that included comparing the numerical solution with an analytic solution, evaluating coupled energy and moisture transport for a simple case, and calculating errors in energy and mass balance in the model for a realistic case using field observations. The original version of the model was modified to make it more applicable to the field conditions for the third test-case. Results from these tests demonstrate the physical self-consistency of the model and its successful implementation for the simple scenarios, and argue for its extendibility to more realistically complex cases. (C) 2003 Elsevier Science Ltd. All rights reserved.",2003,10.1016/S0309-1708(03)00048-4,no
Steel industry: simulation of production processes and product performance evaluation using finite element models,"The use of finite element models in the steel industry, for simulating production processes and for evaluating products service performance, is discussed.",2003,,no
Error tracking in Ikonos Geometric processing using a 3D parametric model,"Thirteen panchromatic (Pan) and multiband (XS) Ikonos Geo-product images over seven study sites with various environments and terrain were tested Using different cartographic data and accuracies with a 3D parametric model developed at the Canada Centre for Remote Sensing, Natural Resources Canada. The objectives of this study were, to define the relationship between the final accuracy and the number and accuracy of input data, to track error propagation during the full geometric correction process (bundle adjustment and ortho-rectification), and to advise on the applicability of the model in operational environments. When ground control points (GCPs) have an accuracy poorer than 3 in, 20 GCPs over the entire image are a good compromise to obtain a 3- to 4-m accuracy in the bundle adjustment. When GCP accuracy is better than 1 in, ten GCPs are enough to decrease the bundle adjustment error of either panchromatic or multiband images to 2 to 3 in. Because GCP residuals reflect the input data errors (map and/or plotting), these errors did not propagate through the 3D parametric model, and the internal accuracy of the geometric model is thus better (around a pixel or less). Quantitative and qualitative evaluations of ortho-images were thus performed with either independent check points or overlaid digital vector files. Generally, the measured errors confirmed the predicted errors or were even slightly better, and a 2- to 4-m positioning accuracy was achieved for the ortho-images depending upon the elevation accuracy (DEM and grid spacing). To achieve a better final positioning accuracy, such as 1 in, a DEM with an accuracy of 1 to 2 in and with a fine grid spacing is required, in addition to well-defined GGPS with an accuracy of I m.",2003,,no
Influence of image processing operators and optical models used on the quality of depth estimation,This article deals with a spatial approach to depth estimation by analysis of edges in images. A Depth from Defocus method is explained and the physical process is described. Theoretical developments are made to apply it on thin or thick edges and the influence of the used image processing operators and optical models is pointed out and discussed. Some results on images are presented to illustrate the method efficiency and the appropriateness of the used models in noisy context.,2003,,no
"Determining the optimal lot size for the finite production model with random defective rate, the rework process, and backlogging","This paper considers the effects of the reworking of defective items on the economic production quantity (EPQ) model with backlogging allowed. The classic EPQ model assumes that manufacturing facility functions perfectly during a production run. However, due to process deterioration or other factors, the production process may shift and produce imperfect quality items. In this study, a random defective rate is considered, and when regular production ends, the reworking of defective items starts immediately. Not all of the defective items are reworked, a portion of them are scrap and are discarded. Repairing and holding cost per reworked item and disposal cost per scrap item are included in the proposed mathematical modelling and analysis. The renewal reward theorem is utilized to deal with the variable cycle length, and the optimal lot size that minimizes the overall costs for the imperfect quality EPQ model is derived where backorders are permitted.",2003,10.1080/03052150310001597783,no
The novel use of a capability maturity model to evaluate the mechanical design process,"This paper describes a process improvement study undertaken at three sites of UK electromechanical engineering companies, using a derivation of the Carnegie-Mellon/SEI Systems Engineering Capability, Maturity Model(R) (SE-CMMSM called the Process Capability Model - Mechanical Design (PCM-MD). The model was applied within a traditional engineering discipline domain, namely mechanical design. The new assessment tool was piloted on a sample of nine mechanical engineers and eight design engineers as well as some ancillary functions, such as stress and thermal analysis. This y was expanded to take into account the views of the downstream manufacturing disciplines and was then subsequently rolled-out into the other companies. The results from these studied,; support the view that the SE-CMM can be adapted and used in CMM-type assessments,for mechanical design process benchmarking.",2003,,no
Functional modelling and performance evaluation for two class Diffserv router using stochastic process algebra,"This paper describes the use of stochastic process algebra to model and to evaluate the performance of a two class DiffServ router. This specification is done by means a set of powerful operators of Extended Markovian Process Algebra (EMPA) language, and then studied from the functional and the performance point of view.",2003,,no
Variability of in situ moisture measurements and implications for modeling hillslope processes,"This paper investigates changes in moisture conditions within regolith prone to shallow landslides. An intensive monitoring network was established to quantify the responses in soil water content, soil water potential, and positive pore-water pressures to rainfall events. The results show that the hydrological response of the soil is primarily a function of the storm characteristics, but this response can be modified by antecedent moisture conditions, topographic position, and heterogeneity of soil properties. The physical properties of the soil results in two modes of response in soil water conditions: a ""damped"" response resulting from matric flow, and a ""spiked"" response caused by preferential flow through macropores. Macropore flow is only significant during major storms, and when antecedent moisture conditions are high. Soil moisture conditions are highly variable in both time and space. Topography and soil properties contribute to this variability, but, since the soil properties are influenced by topography, separating the two effects is difficult. Likewise the piezometric response is affected by the variation in soil properties, which provide preferential flow paths through the regolith. Any response cannot be explained simply by rainfall and antecedent soil moisture. The high degree of temporal and spatial variability in measured soil moisture conditions, even at the detailed scale of this study, has important implications for field investigations and slope stability modeling, i.e. considerable variability has to be included in any modeling framework. Likewise the importance of preferential flowpaths, and temporal variation in flow dynamics, have a critical influence on runoff processes.",2003,10.2113/9.3.213,no
"Hardware/software co-design of complex embedded systems - An approach using efficient process models, multiple formalism specification and validation via co-simulation","This paper presents a hardware/software co-design approach where different specification languages can be used in parallel, allowing effective system co-modeling. The proposed methodology introduces a process model that extends the traditional spiral model so as to reflect the design needs of modern embedded systems. The methodology is supported by an advanced toolset that allows co-modeling and co-simulation using SDL, Statecharts and MATRIXx, and interactive hardware/software partitioning. The effectiveness of the proposed approach is exhibited through two application examples: the design of a car window lift mechanism, and the design of a MAC layer protocol for wireless ATM networks.",2003,10.1023/A:1022388018837,no
"Modeling, scheduling and simulation of product development process by extended stochastic high-level evaluation Petri nets","This paper presents a methodology for modeling and simulating product development process-based on the extended stochastic high-level evaluation Petri nets (ESHLEP-N). A product development process is composed of many design activities and the ESHLEP-N model can describe some special features of design activity in detail, such as randomness of its duration, uncertainty of its interruption and complexity of design iteration. Therefore, the ESHLEP-N model is employed to simulate a product development process. The initial product development plan obtained by a mathematical method beforehand is taken as the input of the simulation. Then the simulation procedure is proposed, along with four types of rules, i.e. activity-sequencing rules, resource-assigning rules, state-changing rules and the simulation-terminating rule, for scheduling the design activities. An example of the development process of an automobile drive system in concurrent engineering environment is presented to illustrate the method of the ESHLEP-N-based modeling, simulation procedure and scheduling rules. The simulation results show that the simulation procedure and the scheduling rules are effective. (C) 2003 Elsevier Science Ltd. All rights reserved.",2003,10.1016/S0736-5845(03)00027-9,no
The processing of experimental data for the modelling and optimisation of the technological processes in the MA courses in quality management,"This paper presents several modern techniques and methods for the processing of experimental data for the modelling and optimisation of the technological processes presented in the MA university courses: quality management and technical statistics. The complete algorithm presented in this paper is being approached and applied by students within the framework of certain concrete examples in industrial engineering. The fundamental concepts about the experimentation strategy play a well-defined role in the aggregate strategy with reference to the technological process or product optimisation. In this respect, this paper presents a concrete example of process optimisation with a wide range of feasibility in industrial engineering. By using proper software for the modelling, optimisation and assisted control of the process being analysed, MA postgraduate students in quality management come to pertinent and altogether important conclusions as regards the optimisation of that particular process. The results obtained in this particular research paper are worthy of consideration in the field of quality management.",2003,,no
Post-processing framework for enhanced UWB channel modeling from band-limited measurements,"This paper proposes a general framework that allows one to retrieve accurate UWB channel parameters from band-limited Vector Network Analyzer (VNA) measurements. It is demonstrated that a specific post-processing technique, including a Passband Hermitian Reconstruction (PHR) and an estimation algorithm based on the Frequency Domain Maximum Likelihood criterion (FDML), makes it possible to reach super-resolution from band-limited measurements. Whereas the short bandwidth results in a poor precision when estimating differential path delays with traditional processing, the proposed method provides much thinner temporal precision, resolution and even actual path polarity.",2003,,no
Evaluating the influence of implicit models of mental disorder on processes of shared decision making within community-based multi-disciplinary teams,"This paper reports findings from a qualitative study concerning the influence of implicit models of mental disorder on shared decision making within community-based mental health teams. One-hundred participants representing five distinct multi-agency groups: psychiatrists, community psychiatric nurses, approved social workers, patients and informal carers operating within Leicestershire, England were interviewed using a standard case vignette describing a person whose behaviour suggests he may have schizophrenia. The results showed that each of the study's multi-agency groups implicitly supports a complex range of model dimensions regarding the nature of schizophrenia, the appropriateness of specific forms of treatment and care, and their respective rights and obligations towards each other. The influence of these implicit model patterns on processes of shared decision making are discussed through evaluating their contribution to our understanding of the power relationships existing between various practitioner groups (including informal carers), and between practitioners and patients during clinical encounters. (C) 2002 Elsevier Science Ltd. All rights reserved.",2003,10.1016/S0277-9536(02)00156-9,no
The process management model of quality planning for the new product development in manufacturing industry,"This research will develop a quality planning model for the new product development, NPD, in manufacturing industry This model deploys the activities of NPD to each department in order to measure and control departmental performance. Once the cross-functional activities to be come out into the open, the neutral area can be bottomed so that the NPD operating system can be built up not only to accumulate organizational knowledge but also predict and prevent problems.",2003,,no
Snow process modeling in the North American Land Data Assimilation System (NLDAS): 1. Evaluation of model-simulated snow cover extent,"This study evaluates the cold season process modeling in the North American Land Data Assimilation System (NLDAS) and consists of two parts: (1) assessment of land surface model simulations of snow cover extent and (2) evaluation of snow water equivalent. In this first part, simulations of snow cover extent from the four land surface models (Noah, MOSAIC, Sacramento land surface model (SAC), and Variable Infiltration Capacity land surface model (VIC)) in the NLDAS were compared with observational data from the Interactive Multisensor Snow and Ice Mapping System for a 3 year retrospective period over the conterminous United States. In general, all models simulate reasonably well the regional-scale spatial and seasonal dynamics of snow cover. Systematic biases are seen in the model simulations, with consistent underestimation of snow cover extent by MOSAIC (-19.8% average bias) and Noah (-22.5%), and overestimation by VIC (22.3%), with SAC being essentially unbiased on average. However, the level of bias at the regional scale varies with geographic location and elevation variability. Larger discrepancies are seen over higher elevation regions of the northwest of the United States that may be due, in part, to errors in the meteorological forcings and also at the snow line boundary, where most temporal and spatial variability in snow cover extent is likely to occur. The spread between model simulations is fairly low and generally envelopes the observed data at the mean regional scale, indicating that the models are quite capable of simulating the general behavior of snow processes at these scales. Intermodel differences can be explained to some extent by differences in the model representations of subgrid variability and parameterizations of snow cover extent.",2003,10.1029/2002JD003274,no
Nonparametric methods of process discrimination and model validation using zero crossings,"We address two related situations. Process discrimination. One has many sample realizations from each of two stationary data generating processes. Without making parametric models of the processes, one wants to test whether the two data generating processes are the same. Model validation. One has a single realization generated by a stationary real-world process. One also has one or perhaps a few realizations generated by a model of the process. One wants to assess the validity of the model. Both these problems involve the comparison of two sets or pairs of time series data. Our solutions use a new statistic which measures the ""distance"" between two time series based on their zero crossings. This article describes the statistic and several Monte Carlo studies which establish its utility for process discrimination and model validation.",2003,10.1081/SAC-120017505,no
An experimental design criterion for minimizing meta-model prediction errors applied to die casting process design,"We propose the expected integrated mean-squared error (EIMSE) experimental design criterion and show how we used it to design experiments to meet the needs of researchers in die casting engineering. This criterion expresses in a direct way the researchers' goal to minimize the expected meta-model prediction errors, taking into account the effects of both random experimental errors and errors deriving from our uncertainty about the true model form. Because we needed to make assumptions about the prior distribution of model coefficients to estimate the EIMSE, we performed a sensitivity analysis to verify that the relative prediction performance of the design generated was largely insensitive to our assumptions. Also, we discuss briefly the general advantages of EIMSE optimal designs, including lower expected bias errors compared with popular response surface designs and substantially lower variance errors than certain Box-Draper all-bias designs.",2003,10.1111/1467-9876.00392,no
Using sensitivity analysis to validate a state Variablo model of the software test process,"We report on the sensitivity analysis of a state variable model.(Model S) proposed earlier. Model S captures the dominant behavior of the system test phase of the software test pro cess. Sensitivity analysis is a mathematical methodology to compute changes in the system behavior due to changes in system parameters or variables. This is particularly important when parameters are calibrated using noisy or small data sets. Nevertheless, by mathematically,quantifying the effects of parameter variations on the behavior of the model, and thereby the STP, one can easily and quickly evaluate the effect of such variations on the process performance without having to perform extensive simulations. In all cases studied, Model S behaved according to empirical observations which serves to validate the-model. It is also shown that sensitivity analysis can suggest structural improvements in a model when the model does not behave as expected.",2003,10.1109/TSE.2003.1199072,no
Measurements and modelling of axial emission profiles in abnormal glow discharges in argon: heavy-particle processes,"We report studies on argon glow discharges established between flat disc electrodes, at pressure x electrode separation (pd) values between 45 and 150 Pa cm, with special attention to heavy-particle processes including heavy-particle excitation induced light emission. The discharges are investigated experimentally and also through self-consistent hybrid modelling. The comparison of the experimental and computed light intensity distributions verifies the correctness of the model, which gives a detailed insight into the discharge operation. The efficiency of heavy-particle excitation shows a universal dependence on the reduced electric field. At the higher pd values the scaling of electrical characteristics and light emission intensity with electrode separation is verified, however, additional processes (radial losses of charged particles and reduction of the active cathode area) result in the violation of scaling at the lowest pd value when the discharge tube diameter is kept constant.",2003,10.1088/0022-3727/36/21/007,no
Evaluating the credibility of transport processes in simulations of ozone recovery using the Global Modeling Initiative three-dimensional model,"[1] The Global Modeling Initiative (GMI) has integrated two 36-year simulations of an ozone recovery scenario with an offline chemistry and transport model using two different meteorological inputs. Physically based diagnostics, derived from satellite and aircraft data sets, are described and then used to evaluate the realism of temperature and transport processes in the simulations. Processes evaluated include barrier formation in the subtropics and polar regions, and extratropical wave-driven transport. Some diagnostics are especially relevant to simulation of lower stratospheric ozone, but most are applicable to any stratospheric simulation. The global temperature evaluation, which is relevant to gas phase chemical reactions, showed that both sets of meteorological fields have near climatological values at all latitudes and seasons at 30 hPa and below. Both simulations showed weakness in upper stratospheric wave driving. The simulation using input from a general circulation model (GMI(GCM)) showed a very good residual circulation in the tropics and Northern Hemisphere. The simulation with input from a data assimilation system (GMI(DAS)) performed better in the midlatitudes than it did at high latitudes. Neither simulation forms a realistic barrier at the vortex edge, leading to uncertainty in the fate of ozone-depleted vortex air. Overall, tracer transport in the offline GMI(GCM) has greater fidelity throughout the stratosphere than it does in the GMI(DAS) .",2004,10.1029/2003JD004238,no
An intelligent model for the signorini contact problem in belt grinding processes,"A bottleneck in the real-time simulation of belt grinding processes is the calculation of the force distribution between the workpiece and the grinding wheel, which can be simplified by the Signorini contact problem. The Finite Element Method (FEM) is the conventional way of solving such a contact problem, but too computationally expensive to meet the real-time requirement. This paper demonstrates a new approach to model the Signorini contact problem based on learning. This new model approximates the FEM model so that it is not necessary to execute optimization for each contact in run time; hence the calculation time is dramatically reduced.",2004,,no
A comprehensive model on the transport phenomena during gas metal arc welding process,"A comprehensive mathematical model and the associated numerical technique have been developed to simulate the coupled, interactive transport phenomena between the electrode (droplets), the arc plasma, and the workpiece (weld pool) during a stationary axisymmetric gas metal are welding process. The simulation involves arc plasma generation, electrode melting, droplet formation, detachment, transfer, and impingement onto the workpiece, and weld pool dynamics. During transfer from the tip of the electrode to the workpiece, the droplet subjects to gravity, electromagnetic force, surface tension, and arc plasma drag force. Transient temperature and velocity distributions of the arc plasma, shapes of the electrode, droplet, and weld pool, and heat transfer and fluid flow in the weld pool are all calculated in a single, unified model. The predicted solidified weld bead shape compares favourably with the experimental result.",2004,10.1504/PCFD.2004.003789,no
Qualifying OPC Model robustness to reticle noise errors and FAB process changes,"A methodology has been developed to measure OPC model robustness as a function of systematic and statistical process variations. The analysis includes comparison of imaging solutions with several different OPC models generated for different writing tools and lithography process conditions. This approach allows for definition of OPC model tolerance in the continually changing R&D and production environment. The question of when it is absolutely necessary to regenerate OPC models and when application of ""the old"" OPC model is acceptable is answered This method has been applied at LSI Logic for qualifying a single OPC model for e-beam and laser reticle writing tools in back-end processes for the 0.13um technology node. The OPC model tolerance qualification takes additional time and engineering effort, but it provides pay back through comparable or better product performance and lower costs.",2004,10.1117/12.569580,no
Model-based evaluation of COD influence on a partial nitrification-Anammox biofilm (CANON) process,"A model evaluating COD influence on a partial nitrification-Anammox biofilm process is integrated on the basis of heterotrophic growth as described in ASM3, combined with a previously published model for the CANON process. This integrated model can simulate the activities of heterotrophs and autotrophs involved in a biofilm, and interactions between COD oxidation, denitrification, nitrification and Anammox can be evaluated. Simulations indicate that COD in the influent has no important influence on the trends in the partial nitrification-Anammox biofilm process. Besides full COD removal, a total nitrogen removal efficiency of about 90% can be expected for stable biofilm systems. Furthermore, Anammox is a major contributor to the total nitrogen removal in stable biofilm systems and conventional denitrification only takes a share of <20% in the total nitrogen removal.",2004,,no
Stochastic processes for modelling and evaluating atomic clock behaviour,A model for atomic clock errors is given by the stochastic differential equation. The probability that the clock error does not exceed a limit of permissible error is also studied by means of the survival probability.,2004,10.1142/9789812702647_0020,no
Numerical modeling of soil bioventing processes - Fundamentals and validation,"A numerical model of bioventing plants equipped with horizontally installed venting wells is developed. It is based on the equations for fluid flows in porous media. These equations are discretized according to the finite-volume method. Different boundary conditions are used in order to describe the influence of injection, extraction and passive wells in the domain under investigation. The numerical model is verified by laboratory- scale experiments, which are also described shortly. The proposed model permits a reasonable description of the flow field of pressurized air between the injection and extraction wells of a soil bioventing unit, even if the porous media are heterogeneous and the permeability of the soil varies. Hints for the calculation of analogous bioventing installations are deduced from the comparison of the numerical with experimental data.",2004,10.1023/B:TIPM.0000013324.41889.df,no
Quantitative fabric drape evaluation system using image processing technology (Part 1: Measurement system and geometric model),"A quantitative fabric drape evaluation system that uses image processing technology and simple instruments has been developed. The purpose of this research is to obtain detailed quantitative information on fabric drapability from digital images captured with a commercial digital camera (or a scanner). A two-dimensional geometric drape model transformed from a drape shadow image was defined with shape parameters such as the number of nodes, the position of the nodes, and their frequency and amplitude. Statistical information including maximum, average, and variance of drape shape parameters can be obtained by using frequency analysis, as well as drape coefficients. A three-dimensional drape shape can be regenerated from its captured two-dimensional drape images with a three-dimensional simulator. The hardware required to capture drape images consists of a digital USB camera, a frame cover, and a stand to attach the camera to a traditional Cusick type drape tester. All evaluation softwares are coded with Microsoft Visual C++ and operated under Microsoft Windows 9x or above.",2004,,no
A comprehensive model of the deformation process in the Nagamachi-Rifu Fault Zone,"A two-dimensional finite element model was constructed along a cross section almost perpendicular to the Nagamachi-Rifu Fault Zone, in order to clarify the stress accumulation process on an intraplate earthquake fault. We explain the surface deformations observed by the dense GPS network and leveling surveys using models with heterogeneities in the crust. These heterogeneities are identified from various geophysical surveys in the region. We found that the observed surface deformations cannot be explained by a model having a weak zone in the upper crust, but can be explained by models having a weak zone in the lower crust. Models having an aseismic fault or fault zone in the lower crust can reproduce the spatial pattern of the observed deformations, but amplitudes predicted by these models are smaller than those observed. The weak zone in the lower crust probably plays an important role in the stress accumulation process on the Nagamachi-Rifu fault zone.",2004,,no
Understanding spatial and temporal processes of urban growth: cellular automata modelling,"An understanding of the dynamic process of urban growth is a prerequisite to the prediction of land-cover change and the support of urban development planning and sustainable growth management. The spatial and temporal complexity inherent in urban growth requires the development of a new simulation approach, which should be process-oriented and have a strong interpretive element. In this paper the authors present an innovative methodology for understanding spatial processes and their temporal dynamics on two interrelated scales-the municipality and project scale-by means of a multistage framework and a dynamic weighting concept. The multistage framework is aimed at modelling local spatial processes and global temporal dynamics by the incorporation of explicit decisionmaking processes. It is divided into four stages: project planning, site selection, local growth, and temporal control. These four stages represent the interactions between top-down and bottom-up decisionmaking involved in land development in large-scale projects. Project-based cellular automata modelling is developed for interpreting the spatial and temporal logic between various projects that form the whole of urban growth. Use of dynamic weighting is an attempt to model local temporal dynamics at the project level as an extension of the local growth stage. As nonlinear function of temporal land development, dynamic weighting can link spatial processes and temporal patterns. The methodology is tested with reference to the urban growth of a fast growing city-Wuhan, in the People's Republic of China-from 1993 to 2000. The findings from this research suggest that this methodology can be used to interpret and visualise the dynamic process of urban growth temporally and transparently, globally and locally.",2004,10.1068/b2975,no
Computer Modeling of ion-exchange processes with application to hydrogeochemical problems,"Based on the analysis of published data, results of experimental investigations, and computer simulation, the constants of ion exchange reactions on clay minerals were determined and the factors affecting these constants were established. This information was applied to the solution of typical hydrogeochemical problems including (a) the calculation of concentrations of absorbed cations in clay, (b) the determination of the equilibrium compositions of water and clay, and (c) the evaluation of the role of ion exchange in the geochemical evolution of HCO3-Ca groundwater and water of the HCO3-Na composition. For the solution of geochemical evolution of HCO3 these problems, we used observations on real changes in the chemical compositions of groundwater in the Moldavian artesian basin.",2004,,no
Evaluation of a model for leaf to fruit transfer of radionuclides in processing tomato plants using an independent set of data,"Because of their varied possibilities of consumption, tomatoes are an important component of the human diet. This paper presents results of the evaluation of a dynamic model (Ventomod) for the short-term behaviour of radionuclides deposited on tomato plants following a direct contamination event. To check its forecasting capability in assessing the risk of radionuclide contamination of the human diet, it has been tested with an independent dataset on the leaf to fruit transfer Of Cs-134 in a typical Hungarian tomato variety, ""Dwarf of Kecskemet"". Data obtained from this pot experiment were used to evaluate model behaviour. Model constants were varied according to the differences between the Hungarian dataset and the one used to calibrate it. Results show that the model output well reproduces the observed activity of fruits for various levels of contamination and at different contamination dates. The main part of this report summarises the experimental protocol, compares the experimental results with model predictions generated by Ventomod and makes recommendations for both updating model parameters and undertaking further experimental work. (C) 2003 Elsevier Ltd. All rights reserved.",2004,10.1016/j.jenvrad.2003.08.005,no
Fuzzy models to deal with heterogeneous information in decision making problems in engineering processes,"Before implementing an engineering system in the design process are studied different proposals to evaluate and rank them. In this evaluation process several experts assess different aspects and criteria according to their knowledge and preference on them. These criteria may have different nature (quantitative, qualitative) and the experts could belong to different areas and have different knowledge on each criterium, so the assessments used to express the value on each criterium could be assessed with different types of information (numerical, linguistic, interval-valued). In such a case, to select the best proposal we must deal with this heterogeneous information to evaluate and rank the different proposals. In this contribution we show different fuzzy approaches for dealing with heterogeneous information.",2004,10.1142/9789812702661_0050,no
A new method to model process and quality control,"Between the needs and requirements of quality and process control tasks related to production processes in manufacturing and the computer aid currently offered using state-of-the-art tools and data models qualifying the final products, like CADs for example, a gap catches the eye: the disability of mapping the term 'quality' and all the information required to describe the properties of the produced component in question. Indeed, the widely used concept of tolerances in the CADs is quite not adequate to model complex effects appearing during the production process. Hence, to overcome these deficiencies of common computer aided models and software tools respectively, we propose to add a set of deformations applied to a CAD model, leading to a more flexible prototype for final releases of the manufactured component. Our approach strongly relies on the well-known technique of prototype systems, modeling the final product as deformed version of its underlying prototype in CAD, in which the transformations used may be dedicated via process simulation. The method proposed even enables a reconstruction of a produced component from measurement data to CAD, and hence, facilitates the tasks in process control.",2004,,no
Workload balancing on agents for business process efficiency based on stochastic model,"BPMS (Business Process Management Systems) is a revolutionary information system that supports designing, administrating, and improving the business processes systematically. BPMS enables execution of business processes by assigning tasks to human or computer agents according to the predefined definitions of the processes. In this paper, we model business processes and agents using a queueing network and propose a task assignment algorithm to maximize overall process efficiency under the limitation of agent's capacity. We first transform the business processes into queueing network models, in which the agents are considered as servers. With this complete, workloads of agents are calculated as server utilization and the task assignment policy can be determined by balancing the workloads. This will serve to minimize the workloads of all agents, thus achieving overall process efficiency. Another application of these results can be capacity planning of agents in advance and business process optimization in reengineering context. The simulation results and comparisons with other well-known dispatching policies show the effectiveness of our algorithm.",2004,,no
Rationality validation of business process model by simulation method,"Business process modeling has been adopted widely. Due to the complexity, it is very hard to validate the rationality of process models. The existing validation methods can only detect structural conflicts in process models. It is also very important to validate those objects related to the processes. This paper presents a rationality validation method which is based on discrete event simulation technology. It can detect three logic mistakes from business process models: structural deadlock, lack of synchronization and objects not matched each other. This method has extended the scope of rationality validation, and also enriches the contents that can be validated.",2004,,no
Evidence against the rescue of defective Delta F508-CFTR cellular processing by curcumin in cell culture and mouse models,"Curcumin, the yellow colored component of the spice turmeric, has been reported to rescue defective DeltaF508-cystic fibrosis transmembrane conductance regulator (CFTR) cellular processing in homozygous mutant mice, restoring nasal potential differences and improving survival (Egan, M. E., Pearson, M., Weiner, S. A., Rajendran, V., Rubin, D., Glockner-Pagel, J., Canny, S., Du, K., Lukacs, G. L., and Caplan, M. J. (2004) Science 304, 600-602). Because of the implied potential use of curcumin or similar compounds in the therapy of cystic fibrosis caused by the DeltaF508 mutation, we tried to reproduce and extend the pre-clinical data of Egan et al. Fluorometric measurements of iodide influx in Fischer rat thyroid cells expressing DeltaF508-CFTR showed no effect of curcumin (1-40 muM) when added for up to 24 h prior to assay in cells grown at 37degreesC. Controls, including 27degreesC rescue and 4 mM phenylbutyrate at 37degreesC, were strongly positive. Also, curcumin did not increase short circuit current in primary cultures of a human airway epithelium homozygous for DeltaF508-CFTR with a 27degreesC rescue-positive control. Nasal potential differences in mice were measured in response to topical perfusion with serial solutions containing amiloride, low Cl-, and forskolin. Robust low Cl- and forskolin-induced hyperpolarization of 22+/-3 mV was found in wild type mice, with 2.1+/-0.4 mV hyperpolarization in DeltaF508 homozygous mutant mice. No significant increase in Cl-/forskolin hyperpolarization was seen in any of the 22 DeltaF508 mice studied using different curcumin preparations and administration regimens, including that used by Egan et al. Assay of serum curcumin by ethyl acetate extraction followed by liquid chromatography/mass spectrometry indicated a maximum serum concentration of 60 nM, well below that of 5-15 muM, where cellular effects by sarcoplasmic/endoplasmic reticulum calcium pump inhibition are proposed to occur. Our results do not support further evaluation of curcumin for cystic fibrosis therapy.",2004,10.1074/jbc.M407308200,no
Modeling hydrodynamic and water quality processes in a reservoir,"Despite the progress in three-dimensional (3D) hydrodynamic, water quality, and sediment diagenesis models and their successful applications in estuaries and bays, few similar 3D modeling studies on lakes and reservoirs have been published. In this study, a 3D hydrodynamic and water quality model has been developed and applied to Lake Tenkiller, Oklahoma. The model includes coupled hydrodynamic, eutrophication, and sediment diagenesis processes. Lake Tenkiller is a manmade reservoir up to 45 meters deep. The lake measures 48 kilometers (km) long, up to 3 in wide, and 70 km(2) in area. Its major water quality issues include nutrient enrichment, eutrophication, and hypolimnetic oxygen depletion. With large lateral variations, the lake needs a 3D model to simulate the hydrodynamic and water quality processes in detail. The model has 198 horizontal grid cells and 10 vertical layers. Measured data at 14 stations from February 1986 to September 1986 are available for hydrodynamic and water quality model calibration, including water elevation, water temperature, dissolved oxygen, chlorophyll a, 5-day biochemical oxygen demand, orthophosphorus, and nitrate-nitrogen. Comparisons between the modeled results and the measured data for all parameters were satisfactory. Seasonal variations of water quality variables in the lake were well replicated. A series of test cases was conducted to illustrate the importance of 3D modeling of lake hydrodynamic and eutrophication processes. It is shown that lake stratification and wind forcing are two major hydrodynamic processes controlling the hypolimnetic oxygen depletion. A seiche signal with a period of 2.36 hours is found in the lake. Theoretical estimation, time series plots, and the Fourier analysis all consistently support this finding. The model is used as a tool for water quality management in the lake.",2004,,no
Perspectives on the drug court model across systems: A process evaluation,"Drug courts have been in existence since 1989, yet few process evaluations have appeared in the literature to help inform the discussion about their effectiveness. This article reports findings from a process evaluation of a drug court program in San Mateo, California. The evaluation was designed to document the history of the program, to examine program strengths and areas of improvement, to assess the roles and relationships among the various agencies involved and to describe the impact of the drug court on the justice and drug treatment systems. Methods included review of available drug court program documents, interviews with key stakeholders, and focus groups with drug court participants. The main findings were: support for the continuation of drug court, enhanced collaboration among all agencies, and an increased awareness of the needs of substance-using clients in the criminal justice system. Potential lessons for other drug courts include the importance of building strong collaborations and maintaining good communication, recognizing competing interests in developing procedures for drug court, and considering changes in eligibility criteria as experience with the drug court model expands.",2004,,no
Using more realistic data models to evaluate sensor network data processing algorithms,"Due to lack of experimental data and sophisticated models derived from such data, most data processing algorithms from the sensor network literature are evaluated with data generated from simple parametric models. Unfortunately, the type of data input used in the evaluation often significantly affect the algorithm performance. Our case studies of a few widely-studied sensor networks data processing algorithms demonstrated the need to evaluate algorithms with data across a range of parameters. In the end, we propose our synthetic data generation framework.",2004,,no
Psychiatric-psychotherapeutic inpatient treatment for depression. Process and outcome quality based on a model project in Baden-Wurttemberg,"During 1998-2000 a quality assurance program for diagnosis and treatment of depression was conducted in 24 hospitals for psychiatry and psychotherapy in Baden-Wurttemberg (southern Germany). Process and outcome quality of 3,000 depressive patients was documented at admission and discharge. The article focuses on therapeutic measures, duration, outcome, patient satisfaction, and their interactions. The results show that the patients satisfaction with the care received is very high and the pre-post effect sizes of inpatient treatment for depression are high.",2004,10.1007/s001115-004-1705-8,no
A new deterministic process based generative model for characterizing bursty error sequences,"Efficient and accurate generative models are of great importance for the design and performance evaluation of wireless communication protocols as well as error control schemes. In this paper, deterministic processes are utilized to derive a new generative model for the simulation of bursty error sequences encountered in digital mobile fading channels. The proposed deterministic process based generative model (DPBGM) is simply a properly parameterized and sampled deterministic process followed by a threshold detector and two parallel mappers. The target error sequence is generated by a computer simulation of a frequency hopping (FH) convolutionally coded Gaussian minimum shift keying (GMSK) transmission system with Rayleigh fading. Simulation results show that this generative model enables us to match very closely any given gap distribution (GD), error-free run distribution (EFRD), error cluster distribution (ECD), error burst distribution (EBD), error-free burst distribution (EFBD), block error probability distribution (BEPD), and bit error correlation function (BECF) of the underlying descriptive model.",2004,10.1109/PIMRC.2004.1368375,no
Modeling of glass making processes for improved efficiency: High temperature glass melt property database for modeling. US DOE project DE-FG07-96EE41262,"Electrical boosting of glass melters is commonplace and many specialty and optical glasses are melted completely by electricity. Increased fossil fuel prices and/or improved designs for electrically heated finers could extend this trend. Knowledge of melt electrical resistivity is essential for effective furnace modeling and design. We have measured electrical resistance at a frequency of 2 KHz in the range of 950 to 1450degreesC for about 150 glass compositions of container, float, low-expansion borosilicate, TV panel, wool and textile fiberglass-types using a simple two-probe method, and report here the ranges of resistivity and general composition-dependent behavior observed. Details of the specific glass compositions will be withheld, pending evaluation of the data by Center for Glass Research member companies.",2004,,no
Interactive modeling software for supervisory control of industrial processes with increased complexity,"Engineering systems are now becoming more and more complex and increasingly dependent on automation, giving rise to a fear that even the smallest error in the system may cause a disaster. Among various factors that contribute to the increased system complexity are the intrinsic non-linearity within the system and the scale enlargement. It is understood that even simple non-linearity may lead to complex phenomena such as non-linear oscillation, bifurcation and chaos that are hard to model, predict or control. Scale enlargement leads to well-known problems in supervisory, control like complicated operators' situation awareness, data overload during system malfunction, etc. In this paper, an interactive modeling method will be proposed to support the extraction of meaningful patterns from large amount of plant data using data mining techniques and the latest grey-box modeling approach. This method will be demonstrated over a Simple CSTR system.",2004,,no
Modelling the process of incoming problem reports on released software products,"For big software developing companies, it is important to know the amount of problems of a new software product that are expected to be reported in a period after the date of release, on a weekly basis. For each of a number of past releases, weekly data are present on the number of such reports. Based on the type of data that is present, we construct a stochastic model for the weekly number of problems to be reported. The (non-parametric) maximum likelihood estimator for the crucial model parameter, the intensity of an inhomogeneous Poisson process, is defined. Moreover, the expectation maximization algorithm is described, which can be used to compute this estimate. The method is illustrated using simulated data. Copyright (C) 2004 John Wiley Sons, Ltd.",2004,10.1002/asmb.517,no
TimeWrap - A method for automatic transformation of structured guideline components into formal process-representations,"Guideline and protocol representation languages have reached a level of complexity where auxiliary methods are needed to support the authoring of protocols in the particular language. Several approaches and methods exist that claim high knowledge about both, the medical context and the formal requirements. Therefore, we need knowledge-based methods to facilitate the human plan designer and create the protocols of the particular language as automated as possible. We present a three-step wrapper method, called TimeWrap, to extract information, in particular temporal issues, out of semi-structured data and integrate it in a formal representation. We illustrate our approach using the guideline-representation language Asbru and examples from guidelines to treat conjunctivitis.",2004,,no
Evaluation of a new process for ironmaking: a productivity model for the rotary hearth furnace,"In order to address the key issues of capital costs and CO2 emissions in ironmaking operations, a new process was proposed combining a Rotary Hearth Furnace (RHF) and a Bath Smelter. This paper describes the construction of a productivity model for the RHF based on previous studies concerning the reduction behaviour of pellets of carbon and iron oxides. The model was used to estimate changes in RHF productivity according to the type of carbon used in the RHF pellets, numbers of layers of pellets, final metallization degree of the direct reduced iron (DRI) produced, and initial sizes of the pellets. The results indicate that productivity gains between 33 and 46% can be achieved replacing coal with wood charcoal, a carbon source virtually free of net CO2 emissions. Also, the productivity of the RHF can be doubled by reducing the charge only up to 70% metallization. The model allows the study of changes in overall energy consumption due to changes in the extent of primary oxidation of the gas at the pellet level showing that the use of wood charcoal increases the total amount of carbon consumed by less than five percent relative to operations with coal.",2004,,no
Development of a control system using the fuzzy set theory applied to a browning process - a fuzzy symbolic approach for the measurement of product browning: development of a diagnosis model - part I,"In the food industry, controlling sensory properties on the manufacturing line represents a key issue for companies, because sensory properties influence consumer choice and preference. Pertinent control of a sensory property depends on the reliability of the measurements of the sensory variables involved in how it is perceived. The browning variable is an important sensory variable assessed by an operator to control the visual quality of browned products. In this paper, we propose a diagnosis model that relies on the fuzzy symbolic approach to ensure reliable measurement of the browning variable. The browning variable is broken down into descriptive variables, which are formalized by sensory indicators. These variables are assessed by the operators with repeatability indexes higher than 90%. The diagnosis model was validated on a database with a percentage of compatibility of 92% at a sensitivity level of half a symbol. (C) 2003 Elsevier Ltd. All rights reserved.",2004,10.1016/j.jfoodeng.2003.11.017,no
Modelling manufacturing process for quality feedback in the environment of concurrent engineering,"In this paper, a two-layer model is used to strengthen the quality assurance function in the process planning and operation monitoring stages of engineering design for small-volume production. The lower layer of the model, formulated by a dynamic time series, and the upper layer of the model, denoted by a static multivariable regression equation, are combined together. As for the dynamic counterpart of the model, the adaptive Kalman smooth algorithm (AKSA) mixed with a simplex algorithm is executed to predict the expectation and the range on the size of the part to be operated next. Regarding the static counterpart of the model, the nonlinear step-wise regression algorithm is applied to build up the relationship between the operation parameters and the parameters in the dynamic model. In view of the applicability of the two-layer model, an experiment is implemented and an integrated prototype system is designed and developed. From application in an enterprise, this system shows quite satisfactory behaviour.",2004,10.1080/0951192031000104455,no
Fuzzy modeling and prediction of cylindricity error in BTA deep hole boring process,"In this paper, first order Sugeno models are proposed to model the relationship between the different cutting parameters and the resulting cylindricity error. This gives the machine operator the opportunity to predict cylindricity for a given set of cutting parameters. Hence, the best parameters can be picked to achieve the best cylindricity. A second model shows that the cutting parameters should vary while drilling in order to reduce the whirling mode. As the amplitude of whirling is reduced, this leads to better cylindricity.",2004,,no
Managing quality in the e-service system: Development and application of a process model,"In this paper, we develop a process model for assessing and managing e-service quality based on the underlying components of the e-service system and, in turn, address the growing need to look in more detail at the system component level for sources of poor quality. The proposed process model is comprised of a set of entities representing the e-service system, a network defining the linking between all pairs of entities via transactions and product flows, and a set of outcomes of the processes in terms of quality dimensions. The process model is developed using Unified Modeling Language (UML), a pictorial language for specifying service designs that has achieved widespread acceptance among e-service designers. Examples of applications of the process model are presented to illustrate how the model can be use to identify operational levers for managing and improving e-service quality.",2004,,no
A modified Gaussian model-based low complexity pre-processing algorithm for H.264 video coding standard,"In this paper, we present a low complexity modified Gaussian model-based pre-processing filter to improve the performance of H.264 compressed video. Noisy video sequences captured by imaging system result in decline of coding efficiency and unpleasant coding artifacts due to higher frequency components. By incorporating local statistics and quantization parameter into filtering process, the spurious noise is significantly attenuated and coding efficiency is improved, leading to improvement of visual quality and to bit-rate saving for given quantization step size. In addition, in order to reduce the complexity of the pre-processing filter, the simplified local statistics and quantization parameter induced by analyzing H.264 transformation and quantization processes are introduced. The simulation results show the capability of the proposed algorithm.",2004,,no
Fictitious crack modeling of the fracture of a refractory ceramic: Fitting through experimental measurements of the displacement field around the fracture process zone,"In this paper, we present the modelling of the fracture of a refractory ceramic using the fictitious crack model. The parameters of the tension-softening curve of the modelling are obtained, not by numerical fitting of macroscopical data, but after observation of the displacement field around the fracture process zone. The modelling obtained is compared with experimental data and previous modelling made by other authors, giving a good fitting of the experimental values and a modelling at least as good as the one obtained by numerical fitting.",2004,,no
Calibration-free estimates of batch process yields and detection of process upsets using in situ spectroscopic measurements and nonisothermal kinetic models: 4-(dimethylamino)pyridine-catalyzed esterification of butanol,"In this paper, we report the use of an NIR fiber-optic spectrometer with a high-speed diode array for calibration-free monitoring and modeling of the reaction of acetic anhydride with butanol using the catalyst 4-(dimethylamino)pyridine in a microscale batch reactor. Acquisition of spectra at 5 ms/scan gave information relevant for modeling these fast batch processes with a single multibatch kinetic model. Nonlinear fitting of a first-principles model directly to the reaction spectra gave calibration-free estimates of time-dependent concentration profiles and pure component spectra. The amount of catalyst was varied between different batches to permit accurate estimation of its effect in the multiway model. A wide range of different models with increasing complexity could be fit to each batch individually with low residuals and apparent low lack of fit. However, only one model properly estimated the concentration profiles when all five batches were fitted simultaneously in a multiway kinetic model. Inclusion of on-line temperature measurements and use of an Arrhenius model for the estimated rate constant gave significantly improved model fits compared to an isothermal kinetic model. Augmentation of prerun batches with data from an additional batch permitted model-based forecasts of reaction trajectories, reaction yield, reaction end points, and process upsets. One batch with added water to simulate a process upset was easily detected by the calibration free process model.",2004,10.1021/ac035356i,no
A three-level model for educational evaluation process in a client/server environment,"In this paper, we will prove that the educational evaluation process can be described by a three-level model: the conceptual level, the implementation level and the user level. At the conceptual level, the automata formalism represents in a very formal way the educational process's environment including tests' structure and learner-system interaction. This formalism is implemented in XML to define a new test description language and allows designing an evaluation system running as an automaton. At the user level, we make use of the evaluation system and-produce XML documents in the test description language for questionnaires.",2004,,no
Model validation for mapping specification behaviors to processing elements,"Increase in system level modeling has given rise to a need for efficient functional validation of models above cycle accurate level. This paper presents a technique for checking functional equivalence of system level models, before and after the distribution of behaviors in the specification over components in the platform architecture. We derive a control flow graph from models written in system level design languages (SLDLs) and reduce it to a normal form representation using well defined rules. Two models having identical normal form are shown to be functionally equivalent. An equivalence checker based on the above concept is used to automatically check if the architecture level model is functionally equivalent to the specification model. As a result, the models generated for various mapping decisions do not have to be reverified using costly simulations.",2004,10.1109/HLDVT.2004.1431247,no
Gaussian process modeling in conjunction with individual patient simulation modeling: A case study describing the calculation of cost-effectiveness ratios for the treatment of established osteoporosis,"Individual patient-level models can simulate more complex disease processes than cohort-based approaches. However, large numbers of patients need to be simulated to reduce 1st-order uncertainty, increasing the computational time required and often resulting in the inability to perform extensive sensitivity analyses, A solution, employing Gaussian process techniques, is presented using a case study, evaluating the cost-effectiveness of a sample of treatments for established osteoporosis. The Gaussian process model accurately formulated a statistical relationship between the inputs to the individual patient model and its outputs. This model reduced the time required for future runs from 150 min to virtually instantaneous, allowing probabilistic sensitivity analyses to be undertaken. This reduction in computational time was achieved with minimal loss in accuracy. The authors believe that this case study demonstrates the value of this technique in handling 1st- and 2nd-order uncertainty in the context of health economic modeling, particularly when more widely used techniques are computationally expensive or are unable to accurately model patient histories.",2004,10.1177/027989X03261561,no
Integrated experimental-modelling study of microstructural development and kinematics in a blown film extrusion process: I. Real-time Raman spectroscopy measurements of crystallinity,"Integrated experimental-model ling studies on microstructural development and kinematics of blown film extrusion of low- and high-density polyethylenes (LDPE and HDPE) films are reported. First, the microstructural evolution along the machine direction of the film line is reported from Raman spectroscopy. The 1418 cm(-1) peak associated with wagging of the -CH(2) group in the crystalline phase was measured using mobile Raman probes. The changes in take-up ratio, blow-up ratio and freeze line height displayed varied levels of influence on the rate of crystallisation. The two polyethylenes develop significantly different extents of crystallinity in the final product form, but higher strain-rates led to faster kinetics and confirmed flow-enhanced crystallisation phenomenon in both polymers. The effect of processing conditions on the development of crystallinity in LDPE and HDPE was consistent with that reported earlier for linear low-density polyethylene (LLDPE), thus confirming the use of Raman spectroscopy as an efficient and reliable analytical technqiue for monitoring microstructure in real time. These results are compared with predictions from a flow-enhanced crystallisation model in a companion paper.",2004,10.1179/174328904X24844,no
Mixing evaluation in the RH process using mathematical modelling,"Mixing phenomena in a RH process has been studied numerically by solving the Navier Stokes equations along with the species concentration equation in a cartesian coordinate system comprising the geometry of the ladle and the snorkel fitted to it. The solution of the species concentration equation has been utilized to compute the mixing time in the RH ladle under different flow conditions. The numerical procedure and solution algorithm has been first verified by comparing the numerically obtained tracer dispersion curve, with the actual plant measurement, which agrees fairly well with each other. Mixing time for the RH process has been computed for different downleg snorkel size, snorkel immersion depth (SID) and steel velocity within the downleg and a non-dimensional mixing time correlation has been developed for the RH ladle taking the above three pertinent input parameters into considerations. The correlated non-dimensional mixing time equation predicts fairly well the computed result as well as the actual mixing time being observed in the plant.",2004,10.2355/isijinternational.44.82,no
Model-based evaluation on the conversion ratio of ammonium to nitrite in a nitritation process for ammonium-rich wastewater treatment,"Modeling for nitritation process was discussed and analyzed quantitatively for the factors that influence nitrite accumulation. The results indicated that pH, inorganic carbon source and Hydraulic Retention Time( HRT) as well as biomass concentration are the main factors that influenced the conversion ratio of ammonium to nitrite. A constant high pH can lead to a high nitritation rate and results in high conversion ratio on condition that free ammonia inhibition do not happen. In a CSTR system, without pH control, this conversion ratio can be monitored by pH variation in the reactor. The pH goes down far from the inlet level means. a strongly nitrite accumulation. High concentration of alkalinity can promoted the conversion ratio by means of accelerating the hitritation rate through providing sufficient inorganic carbon source(carbon dioxide). When inorganic carbon source was depleted, the nitritation process stopped. HRT adjustment could be an efficient way to make the nitritation system run more flexible, which to some extent can meet the requirements of the fluctuant of inlet parameters such as ammonium concentration, pH, and temperature and so on. Biomass concentration is the key point, especially for a CSTR system in steady state, which was normally circumscribed by the characteristics of bacteria and may also affected by aeration mode and can be increased by prolonging the HRT on the condition of no nitrate accumulation when no recirculation available. The higher the biomass concentration is, the better the nitrite accumulation can be obtained.",2004,,no
Modeling processes in oxide-silicate systems treated by plasma for the purpose of improving coating quality,"Modeling of high-temperature processes in oxide-silicate systems under the effect of plasma is performed. The research is implemented with plasma guns used in analytical chemistry for atom-emission analysis. The selection of these sources is determined by a simpler method for studying the interaction of plasma with a solid material surface. To refine the specified high-temperature mechanisms, thermodynamic modeling has been performed using the Astra software package.",2004,10.1023/B:GLAC.0000026776.73090.dd,no
Fuzzy model for safety evaluation process of new and old roads,"More than 50% of traffic fatalities occur on two-lane rural roads, and more than half of these fatalities occur on curved roadway sections. A large body of research can be used to analyze and evaluate the fundamental relationships between accident situation, highway geometric design, driving behavior, and driving dynamics. These factors form the basis for the development of three quantitative safety criteria used to evaluate the hazards of two-lane rural roads with respect to new designs; redesigns; restoration, rehabilitation, or resurfacing projects; and existing alignments. The safety criteria support the design engineer in classifying new or old roadway sections according to good (sound), fair (tolerable), and poor (dangerous) design practices. On the basis of observation of the actual variation in the accident rate with respect to road alignment, a fuzzy model was developed to classify roadway elements by using these safety criteria to obtain a more careful evaluation of highway design inconsistencies. For each criterion, the inconsistencies were included in three fuzzy sets (good, fair, poor) with differing degrees of membership. By defining linear membership functions, it was possible to obtain good results to classify road sections and then to determine a prioritization scale of maintenance interventions. The procedure can be applied to large databases of road networks to identify the more dangerous design elements that need interventions to improve highway safety and to allocate resources under limited budget conditions.",2004,,no
Advanced practice nursing model for comprehensive care with chronic illness - Model for promoting process engagement,New models of providing care to chronically ill persons are needed that can facilitate a more integrative approach to patient care. The purposes of this article are to describe the utilization of a theory synthesis process for development of a client-focused approach for advanced practice nurse (APN) management of chronic illness and to present the Model for Promoting Process Engagement. The model was developed as a theory-driven intervention to address complexities of chronic illness care. This APN practice model is the direct result of the synthesis of a number-of differing theoretical models developed by-the authors in previous individual research endeavors.,2004,,no
"An evaluation model for practice fields in teacher education: Contexts, process, outcomes, and transfer","Practice fields provide authentic learning experiences where users come to ""know about problems and practices"" faced in real professional practice and develop knowledge and skills to transfer to professional roles. Evaluating learning outcomes and transfer of knowledge and skills from training to use is complex. The evaluation needs to identify the contexts and implementation strategies that are effective for producing learning as well as facilitating their transfer to professional practice in real environments. This presentation describes an evaluation model based on multiple research studies that can be used for assessing learning outcomes and transfer from practice field systems involving interactive cases, computer-mediated discussions, and electronic performance support tools.",2004,,no
The measurement of viscosity in rubber mixing process based on fuzzy-GA modeling,"Rubber mixing is a complicated process, online measurement of viscosity is very difficult to achieve. To cope with the problem, a soft sensing approach based on fuzzy-GA modeling is proposed. During modeling, T-S fuzzy model is employed to approximate the non-linearity of rubber mixing process, an improved Gustafon-Kessel fuzzy clustering algorithm based on similarity assessing is proposed to determine the optimum number of clusters and real-coded GA (genetic algorithm) is adopted to optimize model parameters. All these techniques make the fuzzy model simple and accurate. Based on the approach, a test is conducted. The results show the proposed approach provides a result near laboratory measurement, and the error is lower and acceptable. It decreases time, involved with tests in laboratory and can be seen as a powerful tool for online measurement of viscosity.",2004,,no
Improving the understanding of a novel complex azeotropic distillation process using a simplified graphical model and simulation,"Sasol Technology and Linde AG have developed an innovative azeotropic distillation process. A light-boiling binary entrainer (ethanol and water) is used to recover hydrocarbons from a process stream that also contains oxygenated species. Conventional distillation cannot be used because of low-boiling azeotropes that exist between the hydrocarbons and oxygenates. The use of a binary entrainer is crucial to the success of the process concept. Each entrainer component on its own could not achieve the desired separation of octene and oxygenates. Extensive use has been made of graphical conceptualising techniques and steady-state simulations to aid process development and process synthesis. A single ternary diagram does not provide sufficient insight into the system. Hence, a paper pyramid model was constructed, by joining ternary diagrams to better visualise vapour-liquid equilibrium relationships of the four key components in this system. (C) 2003 Published by Elsevier B.V.",2004,10.1016/j.cep.2003.07.001,no
"Validation of simplified evaluation models for first peak power, energy, and total fissions of a criticality accident in a nuclear fuel processing facility by TRACY experiments","Simplified evaluation models are developed at the Japan Atomic Energy Research Institute (JAERI) to predict the first peak power, energy, and total fission numbers during a criticality accident for design and installation of a criticality alarm system and for quick response with measures to avoid excessive exposure of the general public. These models were first derived in previous papers only from theoretical considerations employing one-point reactor kinetic neutron behavior and thus are applicable to any geometrical shape of vessel containing fissile solution. Applicability concerning nuclide composition comes essentially from using empirical equations describing specific heat and density to give simplified forms of the models. The models developed originally for a stepwise reactivity insertion mode are shown in the current paper to approximately stand for the ramp reactivity insertion mode by giving their theoretical formation and are validated by applying experimental data from JAERIs Transient Experiment Critical Facility (TRACY) on a low-U-235-enriched uranium nitrate solution as well as CRAC experiments on high-U-131-enriched uranium nitrate solution together with past accident data, including the most recent JCO accident.",2004,,no
Definition and empirical validation of metrics for software process models,"Software companies are becoming more and more concerned about software process improvement, when they are promoting the improvement of the final products. One of the main reason of the growing interest in software metrics has been the perception that software metrics are necessary for software process improvement. Measurement is essential for understanding, defining, managing and controlling the software development and maintenance processes and it is not possible to characterize the various aspects of development in a quantitative way without having a deep understanding of software development activities and their relationships. In this paper a representative set of metrics for software process models is presented in order to evaluate the influence of the software process models complexity in their quality. These metrics are focused on the main elements included in a model of software processes, and may provide the quantitative base necessary to evaluate the changes in the software processes in companies with high maturity levels. To demonstrate the practical utility of the metrics proposed at model level, an experiment has been achieved which has allowed us to obtain some conclusions about the influence of the metrics proposed on two sub-characteristics of the maintainability: understandability and modifiability, which besides confirm the results of a subjective experiment previously performed.",2004,,no
SMCMM model to evaluate and improve the quality of the software maintenance process,"Software maintenance function suffers from a scarcity of management models that would facilitate its evaluation, management and continuous improvement. This paper is part of a series of papers that presents a Software Maintenance Capability Maturity Model (SMCMM). The contributions of this specific paper are: 1) to describe the key references of software maintenance; 2) to present the model update process conducted during 2003; and 3) to present, for the first time, the updated architecture of the model.",2004,10.1109/CSMR.2004.1281425,no
Evaluation of soil CO2 production and transport in Duke Forest using a process-based modeling approach,"Soil surface CO2 efflux is an important component of the carbon cycle in terrestrial ecosystems. However, our understanding of mechanistic controls of soil CO2 production and transport is greatly limited. A multilayer process-based soil CO2 efflux model (PATCIS) was used to evaluate soil CO2 production and transport in the Duke Forest. CO2 production in the soil is the sum of root respiration and soil microbial respiration, and CO2 transport in the soil mainly simulates gaseous diffusion. Simulated soil CO2 efflux in the Duke Forest ranged from 5 g CO2 m(-2) d(-1) in the winter to 25 g CO2 m(-2) d(-1) in summer. Annual soil CO2 efflux was 997 and 1211 g C m(-2) yr(-1) in 1997 and 1998, respectively. These simulations were consistent with the observed soil CO2 efflux. Simulated root respiration contributed 53% to total soil respiration. Soil temperature had the dominant influence on soil CO2 production and CO2 efflux while soil moisture only regulated soil CO2 efflux in the summer when soil moisture was very low. Soil CO2 efflux was sensitive to the specific fine root respiratory rate and live fine root biomass. Elevated CO2 increased annual soil CO2 efflux by 26% in 1997 and 18% in 1998, due mainly to the enhanced live fine root biomass and litterfall. On a daily to yearly basis, CO2 production is almost identical to CO2 efflux, suggesting that CO2 transport is not a critical process regulating daily and long-term soil surface CO2 effluxes in the Duke Forest. We also developed a statistical model of soil CO2 efflux with soil temperature and moisture. Daily soil CO2 efflux estimation by the statistical model showed a similar pattern to the simulated soil CO2 efflux, but the total annual CO2 efflux was slightly lower. While the statistical model is simple, yet powerful, in simulating seasonal dynamics of soil CO2 efflux, the process-based model has the potential to advance our mechanistic understanding of soil CO2 efflux variations in the current and future worlds.",2004,10.1029/2004GB002297,no
"The deductive phase of statistical analysis via predictive simulations: test, validation and control of a linear model with autocorrelated errors representing a food process","Statistical analysis consists of two phases: induction for model parameter estimation and deduction to make decisions on the basis of the statistical model. In the Bayesian context, predictive analysis is the key concept to perform the deductive phase. In that context, Monte-Carlo posterior simulations are shown to be extremely valuable tools to achieve for instance model selection and model checking. Example of predictive analysis by simulation is detailed for the linear model with Autocorrelated Errors which has been beforehand estimated by Gibbs sampling. Numerical illustrations are then given for a food process with data collected on line. Special attention is cast on the control of its anticipated behavior under uncertainty within Bayesian decision theory. (C) 2003 Elsevier B.V. All rights reserved.",2004,10.1016/S0378-3758(03)00191-5,no
Evaluation model for multi-process capabilities of stranded wire,"Stranded wire is the most important component of familiar mechanical equipment such as elevators, cable cars, and cranes. The quality of these products that are used on a daily basis are mainly affected by the tensile strength of stranded wire. In order to attain the purpose of economical design and a long life span of stranded wire, a less relaxation property of strand type is suitable for manufactured tools. Thus, the manufacturing industries of stranded wire need to reach the goals of high tensile strength and low relaxation. To ensure the required quality of stranded wire, the strand pull test and the long period relaxation test are two important quality assurance tests. There are three specific items of the tensile strength test that belong to the larger-the-better quality type. The quality type of the smaller-the-better is for the long period relaxation test. However, many existing methods are able to measure process capability for the product with a single quality characteristic although it cannot be applied to most products with multiple properties. Thus, the indices of C-pu and C-pl, for the larger-the-better and the smaller-the-better quality type respectively proposed by Kane [5], are quoted and combined to propose a new index to evaluate the quality of multiple characteristics of stranded wire in this article. The principle of statistics is then used to derive the one-to-one mathematical relationship of this new index and ratio of satisfactory production process. Finally, the procedure and criteria to evaluate the quality of stranded wire is proposed. This integrated multi-quality property capability analysis model can be used to evaluate the multi-process capabilities and provide continuous improvements on the manufacturing process of stranded wire.",2004,10.1007/s00170-003-1784-x,no
Placing assist features in layout using a process model,"Sub-resolution assist features (SRAFs) are non-printing features arranged on a mask layout to ""assist"" the lithographic performance of the lines intended to be printed on the wafer [1]. SRAFs typically are narrow lines located adjacent to the target figure edges. Current practice is to synthesize SRAFs with a rule-based methodology where the assist feature placement is dictated by combinations of feature width and spacing parameters. Optical behavior with off-axis illumination is complex and requires an elaborate set of SRAF synthesis rules. Creating and maintaining a robust set of placement rules guaranteed to work properly for arbitrary configurations is very difficult. Socha, et al, have demonstrated that the optimum configurations for SRAFs can be derived from the aerial image of the target layout configuration [2]. In this paper we show how SRAF synthesis can be optimally implemented in an OPC tool environment, leveraging lithography simulation.",2004,10.1117/12.569179,no
A real-time manufacturing/assembly system performance evaluation and control model with integrated sensory feedback processing and visualization,"The aim of this paper is to introduce a real-time performance evaluation and control model, for the purpose of reducing cost and improving the quality; and the real-time responsiveness of manufacturing/assembly systems and enterprises that host them. In order to make the approach practical, networking solutions with integrated sensory feedback processing and visualization methods are illustrated. In this paper we summarize the core concepts of our method and demonstrate some quality industrial solutions.",2004,10.1108/01445150410529946,no
"Mathematical model development, experimental validation and process optimization: retortable pouches packed with seafood in cone frustum shape","The aim of this research was to develop and validate a mathematical model coupled with an optimization technique for thermal processing of conduction-heated foods in retortable pouches in order to: (a) search for variable retort temperature profiles to minimize process time, and (b) search for variable retort temperature profiles to minimize quality gradient (thiamine) within the product. The model was validated utilizing Jack mackerel (Trachurus Murphyi) packed in retortable pouches. The conjugated gradient method was utilized as a search technique to find the best variable retort temperature profile to satisfy the specific objectives. The simulation results were in good agreement with the observed temperatures. The prediction errors obtained in the validation study were under 5%. Non-significant differences (P < 0.05) were found between the experimental and predicted values. Simulations showed that significant reduction in process time (20-30%) could be attained utilizing variable retort temperature profiles while maintaining product quality. (C) 2003 Elsevier Ltd. All rights reserved.",2004,10.1016/S0260-8774(03)00294-2,no
Examples of model validation for foundry processes virtual prototyping,The article concerns the problems of validation of thermal model describing the set of physical behaviors in real Casting - Mould system. This validation can be made by different ways including two kinds of information sources. The first source consists in observation and thermal parameter measurements in real time of casting process and the other one comes from testing and quality control results out of real casting process time. Two examples of validation by inverse problem solution for mould materials are presented.,2004,,no
Validation and improvement of computer modeling of the lost foam casting process via real time x-ray technology,"The Lost Foam Casting (LFC) process is relatively immature when compared to other casting processes. To make the process more robust, computer modeling is needed to reduce lead times and casting defects. In this study, metal filling of lost foam aluminum castings was visualized using state-of-art real time X-Ray technology. Metal filling of the lost foam casting was simulated using a commercial software package. The result of the computer simulation was compared with the results of the real time X-Ray visualization. Some of the computer simulation results agreed with the real time X-Ray visualization, but some did not. Based on other studies using real time X-Ray technology, new input variables, such as foam density variations, were developed for the computer modeling. With the new variables, modeling of the lost foam casting process becomes more realistic.",2004,,no
"CBR based design process, metrics modeling for upcoming project optimization","The metrics model was proposed to record a design process from five aspects (product metric component, organization metric component, resource metric component, activity metric component and checking metric component) that are indivisible in a design process and relevant to each other in the model. By providing a rather full and detailed record of tracks of design process, the model enable us to recognize and measure a design process thoroughly and thus form a case-base of finished design projects. These cases then will give us some guidance to optimize the upcoming project.",2004,,no
Validation of FEM-simulation for micro deep drawing process modeling,"The miniaturization of sheet deep drawing is one possible solution for the production of metallic microparts. However, when reducing the product size to dimensions around 1 mm and using foils down to 40 mum, a reduction of process reproducibility is observed, also in industrial manufacturing. Consequently differences between ""macro-deep drawing"" and ""micro-deep drawing"" were studied by Finite Element Analysis and experimental investigations. Implicit axisymmetric 2D simulation was used to study the influence of transverse anisotropy, friction and deviations of process-determining geometric factors, e.g. the drawing gap, on the micro deep drawing process for producing cups of 8 to 1 mm from CuZn37 foils with thicknesses from 300 to 80 mum. Deviations of tool geometry within the product tolerances of the tool can have a decisive influence on the deep drawing result. For example, the punch force changes by up to 12.8% if the drawing gap deviates from the nominal value by a few hundredths of a millimeter within typical manufacturing tolerances. The results of the implicit axisymmetric 2D simulation are used to check an explicit 3D model that additionally allows planar anisotropy in the deep drawing process to be taken into consideration in later experiments.",2004,,no
Modeling of direction-dependent processes using Wiener models and neural networks with nonlinear output error structure,"The modeling of direction-dependent dynamic processes using Wiener models and recurrent neural network models with nonlinear output error structure is considered. The results obtained are compared for several simulated first-order and second-order processes and using three different types of input signals: a pseudorandom binary signal, an inverse-repeat pseudorandom binary signal and a multisine (sum of harmonics) signal. Experimental results on a real system, namely an electronic nose system, are also presented to illustrate the applicability of the techniques discussed.",2004,10.1109/TIM.2004.827083,no
PLS-based optimal quality control model for TE process,"The necessary and sufficient conditions of the fault delectability under the PLS framework are discussed here. Then a new Partial Least Squares optimum detecting model (PLS-ODM) is proposed to improve the monitoring performance of the PLS method This PLS-ODM is deduced according to the fault subspace and the VP index of the process variables that could be used to estimate the importance of the measurements to the quality. Compared with traditional PLS, the PLS-ODM has the stronger fault detecting power, and could defect much weaker important process faults exactly in time. The application of it in the Tennessee Eastman (TE) Benchmark statistical quality control confirms that it's detection performance is improved remarkably to make it possible to avoid the process shutdown to save cost and stabilize the product quality.",2004,,no
"The system analysis, modelling and optimisation of securing processes of quality radio-electronic devices.",The paper discusses conceptual questions of modelling and optimizing off quality providing processes.,2004,,no
Spontaneous speech understanding in train timetable inquiry processing based on N-gram language models and finite state transducers,"The presented paper concerns the spoken language understanding in an information retrieval dialogue system. There are described methods of use the finite state transducers for conceptual semantic parsing and meaning extraction from speaker's utterances. In this case, the main aim of understanding is an identification of important semantic constituents and their interpretation within supposed frame structure. The key problem is to create an appropriate mapping between sequence of recognized words and concept based meaning that represents real-world entities. We propose a hierarchical semantic n-gram language model for parsing of a first initiative spontaneous speech train timetable inquiry. The design, implementation and evaluation of the model in experimental understanding system are described below.",2004,,no
Measurement process context effects in empirical tests of causal models,"The purpose of this research is to examine the issue of measurement context effects in empirical tests of attitudinal models. The specific issue examined concerns the degree to which the measurement process affects the objects of measurement (i.e., various attitudinal and related concepts). The primary method of investigation is a controlled experiment in which a model specified by Dodds, Monroe and Grewal [J Mark Res 28 (1991) 307] provides the theoretical model setting and in which measurement procedures are experimentally manipulated. Based upon the memory accessibility-diagnosticity theory specified by Feldman and Lynch [J Appl Psychol 73 (1988) 421] and the concept of spreading activation [Psychol Bull 103 (1988) 299; Anderson J. Language, memory, and thought. Hillsdale, NJ: Lawrence Erlbaum Associates, 1978; Anderson J. The architecture of cognition. Cambridge, MA: Harvard Univ. Press, 1983; Psychol Rev 82 (1975) 407; J Advertising 19 (1990) 3], the effects of context questionnaire items on answers to, and estimated relationships among, target questionnaire items are examined. The findings indicate that measurement context affected the linkages between the experimentally manipulated brand and price information and subsequent endogenous variables as well as linkages among the endogenous variables. (C) 2003 Elsevier Science Inc. All rights reserved.",2004,10.1016/S0148-2963(01)00300-9,no
Validation of a model of an expanding superheated steam flash dryer for cut tobacco based on processing data,"The role of drying in modern technology is not only to remove moisture but also to control solid properties in a predetermined way. The quality of cut tobaccos can be improved when they are expanded i.e., when their specific volume (filling power) is increased. This effect can be achieved by means of a flash dryer and the process is described in the article. The dryer uses superheated steam as a drying and expanding agent and is composed of a drying pipe, cyclone, circulating fan, and indirect steam superheater. The process atmosphere is virtually oxygen free (O-2 < 4.5%). A distributed parameter model is used to describe solid velocity, temperature, and moisture content as well as steam temperature and velocity along the dryer pipe. Experimentally obtained solid properties (specific heat, characteristic drying curve, particle density, and morphology) are incorporated in the model. Monodisperse particle population representation is used. The above model solutions are iterated in a closed loop lumped parameter model until a steady state solution is reached. Experimental data obtained during test runs of the industrial scale CLED (Closed Loop Expansion Dryer) on lamina cut tobacco throughputs up to 3000kg/h are used to validate the model. Several test runs were carried out at various processing parameters while both input and output parameters of cut tobacco (in/out moisture and temperature) and steam (in/out temperature, in/out O-2 contents) were measured. The experimental results compare satisfactorily well to model predictions. Optimum processing parameters, leading to improvement of physical properties of the cut tobaccos, have been obtained in the CLED process. This leads to significant mass savings in final products (cigarettes), ca. 10%. Moreover, substantial fuel economies, due to the closed steam cycle, make the superheated steam drying of cut tobacco even more attractive.",2004,10.1081/DRT-120028212,no
An experimental replica to validate a set of metrics for software process models,"The software process measurement plays an essential role in order to provide the quantitative basis necessary for software process improvement. Traditionally, this measurement has been focused in the project and product measurement, but nowadays software process models (SPM) are entities very relevant due to the increasing number of companies which model and manage their processes in order to reach high maturity levels. We have defined a set of metrics for software process models in order to evaluate the influence of the software process models complexity in their maintainability. These metrics are focused on the main elements included in a software process model. To demonstrate the practical utility of the metrics proposed a replica of an experiment has been achieved which has allowed us to obtain some conclusions about the influence of the metrics proposed on two sub-characteristics of the maintainability: understandability and modifiability, which besides confirm the results of a set of experiments previously performed in the context of a family of experiments.",2004,,no
Evaluating a split processing model of visual word recognition: Effects of orthographic neighborhood size,"The split fovea theory proposes that visual word recognition of centrally presented words is mediated by the splitting of the foveal image, with letters to the left of fixation being projected to the right hemisphere (RH) and letters to the right of fixation being projected to the left hemisphere (LH). Two lexical decision experiments aimed to elucidate word recognition processes under the split fovea theory are described. The first experiment showed that when words were presented centrally, such that the initial letters were in the left visual field (LVF/RH), there were effects of orthographic neighborhood, i.e., there were faster responses to words with high rather than low orthographic neighborhoods for the initial letters ('lead neighbors'). This effect was limited to leadneighbors but not end-neighbors (orthographic neighbors sharing the same final letters). When the same words were fully presented in the LVF/RH or right visual field (RVF/LH, Experiment 2), there was no effect of orthographic neighborhood size. We argue that the lack of an effect in Experiment 2 was due to exposure to all of the letters of the words, the words being matched for overall orthographic neighborhood count and the sub-parts no longer having a unique effect. We concluded that the orthographic activation found in Experiment 1 occurred because the initial letters of centrally presented words were projected to the RH. The results support the split fovea theory, where the RH has primacy in representing lead neighbors of a written word. (C) 2003 Elsevier Science (USA). All rights reserved.",2004,10.1016/S0093-934X(03)00164-0,no
Introducing coordinated care (2): evaluation of design features and implementation processes implications for a preferred health system reform model,"The study investigated why the goals of the Australian Coordinated Care trials for clients with complex care needs were not achieved. Significantly higher health service use and costs were incurred in the absence of clear evidence of improved client health outcomes. The validity of assumptions underpinning trial design and the success of implementation at each step in application of the model were examined. There were failures in both design and implementation. Many clients did not require care coordination. The funds pooling arrangements contributed to limited possibilities for service substitution and training of GP care coordinators was inadequate. Trial design did not focus on either clinical guidelines or consumer empowerment. Furthermore, the expectations of the overall national trial were unrealistic both in trial design and expected outcomes given the rigidities and realities of the Australian health care system. Broader system reform in the form of funds pooling and health services planning at the regional level, based on large populations, may be a more effective means to address problems of care coordination and an inflexible supply system. (C) 2004 Elsevier Ireland Ltd. All rights reserved.",2004,10.1016/j.healthpol.2004.02.001,no
Empirical validation of the triple-code model of numerical processing for complex math operations using functional MRI and group Independent Component Analysis of the mental addition and subtraction of fractions,"The suitability of a previously hypothesized triple-code model of numerical processing, involving analog magnitude, auditory verbal, and visual Arabic codes of representation, was investigated for the complex mathematical task of the mental addition and subtraction of fractions. Functional magnetic resonance imaging (fMRI) data from 15 normal adult subjects were processed using exploratory group Independent Component Analysis (ICA). Separate task-related components were found with activation in bilateral inferior parietal, left perisylvian, and ventral occipitotemporal areas. These results support the hypothesized triple-code model corresponding to the activated regions found in the individual components and indicate that the triple-code model may be a suitable framework for analyzing the neuropsychological bases of the performance of complex mathematical tasks. (C) 2004 Elsevier Inc. All rights reserved.",2004,10.1016/j.neuroimage.2004.03.021,no
Validation of chemical speciation model of inorganic arsenic in river waters with mobilisation processes,"The thermodynamic model of inorganic arsenic (acid-base, complexation, precipitation and redox equilibria) was validated with results obtained in arsenic mobilisation processes taking place in several river sediment samples of the Basque Country (North of Spain). Natural waters in contact with the sediment were collected at each sampling point, together with the sediment samples. These waters were analysed and then used as the extractant in the mobilisation studies. Considering pH, redox potential and total concentration values of the main cations and anions, the stoichiometric equilibrium constant were computed for each river water situation. The activity coefficient values of the inorganic arsenic species were estimated by means of the Modified Bromley Methodology (MBM). Both water and sediment for each sampling site were used in the mobility experiments, and the results were compared with the theoretical predictions of the thermodynamic model extrapolated to the chemical conditions of the sampling point. Moreover, a correlation analysis was also performed taking into account all the data and sampling sites; these results were also discussed and compared with the theoretical speciation obtained from the basic model of inorganic arsenic.",2004,10.3184/095422904782775108,no
Evaluation of Staphylococcus aureus growth potential in ham during a slow-cooking process: Use of predictions derived from the US Department of Agriculture Pathogen Modeling Program 6.1 predictive model and an inoculation study,"The U.S. Department of Agriculture has cautioned against slow cooking meat such that the interior temperature increases from 10degreesC (50degreesF) to 54.4degreesC (130degreesF) in greater than or equal to6 h. During a commercial ham-smoking process, the ham cold point is typically between 10 and 54.4degreesC for 13 h, but the ham is subsequently exposed to heating sufficient to eliminate vegetative pathogenic bacteria. Thus, production of heat-stable staphylococcal enterotoxin is the primary biological hazard. For this study, uncooked surface and uncooked ground interior ham were inoculated with a three-strain Staphylococcus aureus mixture, exposed to simulated surface and interior slow-cook conditions, respectively, and analyzed. periodically using the Baird-Parker agar and 3M Petrifilm Staph Express count plate methods. For the surface and interior conditions, S. aureus numbers increased by no more than 0.1 and 0.7 log units, respectively. Predictions derived from actual time and temperature data and S. aureus growth values from a computer-generated model (Pathogen Modeling Program 6.1, U.S. Department of Agriculture) were for 2.7 (ham surface) and 9.9 to 10.5 (ham interior) generations of S. aureus growth, indicating that use of model-derived growth values would not falsely indicate safe slow cooking of ham. The Baird-Parker method recovered significantly (P < 0.05) greater numbers of S. aureus than the Petrifilm Staph Express method. For hams pumped with brine to attain (i) 18% (wt/wt) weight gain, (ii) greater than or equal to2.3% sodium lactate, (iii) greater than or equal to0.8% sodium chloride, and (iv) 200 ppm ingoing sodium nitrite, slow-cooking critical limits of less than or equal to4 h between 10 and 34degreesC, : 5 h between 34 and 46degreesC, and less than or equal to5 h between 46 and 54.4degreesC could be considered adequate to ensure safety.",2004,,no
Understanding computational and simulation management of process modeling,The use of computational process modeling and simulation has become widespread as the real cost of computers has decreased and their capabilities have increased. However: many; science and engineering universitiy; faculties have failed to treat this area as a formal undergraduate discipline. The result has been generally a self-taught approach by professionals to both the practice and the management of modeling and simulation. This article describes process modeling and simulation techniques and applications.,2004,10.1007/s11837-004-0232-1,no
"Verification, validation and testing strategy planning supported by a process model","The Verification & Validation (V&V) process is an important place for requirements analysis and risk reduction in the product development processes. Though many companies struggle with budget and schedule overruns or low product qualhy, they do not pay enough attention to the fact that the V&V and testing processes are in most cases inadequately planned and thus the major causes for the delays and inadequate quality in the projects. This paper introduces a new, generic approach for VV strategy planning, which supports the early planning and continuous updating of the V&V strategy plans during product development. The Verification, Validation and Testing (VVT) Process Modeling Procedure uses systems engineering methods for VVT strategy planning and the monitoring of technical achievements in the whole product lifecycle. The VVT Process Modeling Procedure supports the selection of the optimal VVT strategy for the project based on the estimation of the total cost and duration of the VVT process and the achievable product performance through the applied VVT activities in the VVT process.",2004,,no
Measurement of aspheric optical surface by quantitative shadow method model developing with image-processing techniques,"This article introduces a new method based on knife-edge test to quantitatively characterize local surface deformations of aspheric surface. A set-up of image processing under computer controlling is placed behind of traditional shadow measure device for recording and analyzing the images. Through picking up the characteristics of images, the relationship is determined between illumination levels changing and deformations of surface based on the Fourier Optics principle, and the aspheric surface under test can be reconstructed. The simulation date indicate that the RMS accuracy can reach about 1lambdasquarelambda=632.8nmsquarein one dimension for one parabolic surface under manufacture, therefore this method is efficient and economic for measuring and reconstructing surface in pre-polishing state.",2004,,no
Effects of random defective rate and imperfect rework process on economic production quantity model,"This paper considers the economic production quantity (EPQ) model with a random defective rate and an imperfect rework process. The assumption of perfect quality production condition of the classical EPQ model is unrealistic. Due to process deterioration or other factors, production of defective items is inevitable. This study assumes that the defective rate is a random variable and all items produced are inspected. The imperfect quality items fall into two groups, the repairable and the scrap. The reworking of the repairable defective items starts when the regular production process finishes in each cycle. The rework process is also assumed to be imperfect, with a random scrap rate. Since the scrap items are produced during a production run, it follows that the production cycle length is not a constant. The renewal reward theorem is utilized to cope with the variable cycle length. A mathematical model is developed to minimize the overall costs and to derive the optimal lot size for the imperfect quality EPQ model where shortages are not permitted. A numerical example is provided to demonstrate its practical usage.",2004,,no
Speed estimation and control in a belt-driven system for a web production process. Part 1: system modelling and validation,"This paper develops a detailed mathematical model of a casting drum drive in a web production process. The drive uses a flat belt to connect the casting drum itself to a drive and a brake pulley. The model includes mechanical effects, such as creep and slip between the belt and pulleys, and its parameters are obtained by a combination of calculations and experimental tests. Experimental results from a laboratory-scale model of a commercial casting drum system are used to validate the model under steady-state and transient conditions. A companion paper describes the model's application in new controller strategies, which significantly reduce drum speed variations caused by web tension changes.",2004,10.1243/095440804322860609,no
A modified model for flexibility analysis in chemical engineering processes,"This paper discussed an extended model for flexibility analysis of chemical process. Under uncertainty, probability density function is used to describe uncertain parameters instead of hyper-rectangle, and chance-constrained programming is a feasible way to deal with the violation of constraints. Because the feasible region of control variables would change along with uncertain parameters, its smallest acceptable size threshold is presented to ensure the controllability condition. By synthesizing the considerations mentioned above, a modified model can describe the flexibility analysis problem more exactly. Then a hybrid algorithm, which integrates stochastic simulation and genetic algorithm, is applied to solve this model and maximize the flexibility region. Both numerical and chemical process examples are presented to demonstrate the effectiveness of the method.",2004,,no
Requirements engineering and process modelling in software quality management - Towards a generic process metamodel,"This paper examines the concept of Quality in Software Engineering, its different contexts and its different meanings to various people. It begins with a commentary on quality issues for systems development and various stakeholders' involvement. It revisits aspects and concepts of systems development methods and highlights the relevance of quality issues to the choice of a process model. A summarised review of some families of methods is presented, where their application domain, lifecycle coverage, strengths and weaknesses are considered. Under the new development era the requirements of software development change; the role of methods and stakeholders change, too. The paper refers to the latest developments in the area of software engineering and emphasises the shift from traditional conceptual modelling to requirements engineering and process metamodelling principles. We provide support for an emerging discipline in the form of a software process metamodel to cover new issues for software quality and process improvement. The widening of the horizons of software engineering both as a 'communication tool' and as a 'scientific discipline' (and not as a 'craft') is needed in order to support both communicative and scientific quality systems properties. In general, we can consider such a discipline as a thinking tool for understanding the generic process and as the origin of combining intuition and quality engineering to transform requirements to adequate human-centred information systems. We conclude with a schematic representation of a Generic Process Metamodel (GPM) indicating facets contributed by Software Engineering, Computer Science, Information Systems, Mathematics, Linguistics, Sociology and Anthropology. Ongoing research and development issues have provided evidence for influence from even more diverse disciplines.",2004,10.1023/B:SQJO.0000034711.87241.f0,no
Evaluation of methods for the process modeling of salt leaching processes,This paper introduces different methods in order to support the understanding and prognosis of dynamic salt leaching processes in a flooded potash shaft. Here geochemical computations as well as kinetic boundary conditions became part of the regarded data base. The process model has to consider the partly unknown geometrical conditions as well as the complexity of the geochemical processes. The complex intercorrelations of effects in the regarded geosystem make it necessary to include all available information. This requires the discussion of several methods of pre-processing and analysis. This survey will contribute to determine sustainable treatments of possible reclamation in the Stassfurt mining area.,2004,,no
Estimation of effectiveness of reforming reactor block using mathematical model of process,"This paper is about a problem of mathematical formulation of reactor block development taking into account ways of gas-feed flow. Influence of a configuration of reactor block on a structure of activity of the catalyst and speed of its deactivation is investigated. It is shown, that a presence of hydrodynamic irregularities diffusion gas-feed flow through catalyst bed results in different intensity of mass exchanged processes in peripheral and central zone of reactor, that reduce potential of catalyst activity and effectiveness of process. A change of feed flow direction from center to periphery is one of possible ways of partial removal of irregularities of concentration and temperature fields.",2004,,no
A new migration model based on the evaluation of processes load and lifetime on heterogeneous computing environments,"This paper presents a new model for evaluation of the positive and negative impacts related to the process migration on environments composed by heterogeneous capacity computers. On this model, a busy computer analyzes the occupation of each process and selects the more adequate for migration. The analysis and selection are done through a migration factor This factor reflects how much the busy computer will be freed and how much the destination computer will be overloaded, in view of the migration of each process. The migrated processes are the ones that present migration factors to enhance the environment load balancing. The results from the carried out experiments have proved this model contributions when compared to related work The contribution is the decrease in process average response time, which means higher performance.",2004,,no
INTEGRATED MODELING OF MICROWAVE FOOD PROCESSING AND COMPARISON WITH EXPERIMENTAL MEASUREMENTS,"This paper presents an integrated electromagnetic and thermal model for the microwave processing of food packages. The model is developed by combining the edge finite element formulation of the 3-D vector electromagnetic field in the frequency domain and the node finite element solution of the thermal conduction equation. Both mutual and one-way coupling solution algorithms are discussed. Mutual coupling entails the iterative solution of the electromagnetic field and the thermal field, because the physical properties are temperature-dependent. The one-way coupling is applicable when the properties are temperature independent or this dependence is weak. Mesh sensitivity and shape regularity for the edge element based formulation for computational electromagnetics are discussed in light of available analytical solutions for a simple wave guide. The integrated model has been used to study the electromagnetic and thermal phenomena in a pilot scale microwave applicator with and without the food package immersed in water. The calculated results are compared with the experimentally measured data for the thermal fields generated by the microwave heating occurring in a whey protein gel package, and reasonably good agreement between the two is obtained.",2004,,no
Framework and modeling of collaborative quality assurance processes by GSPN,"This paper sets forth the networked collaborative quality assurance process management system and analyzes the simulation method for process automation models. It also proposes planning and control strategy to operate such networked quality assurance system. For a quality system to be effective, quality processes and their associated tasks, authorizes, procedure and resources should be defined, deployed and executed in a consistent manner. The interactive quality assurance procedure driven by stochastic Petri net(GSPN) in the networked manufacturing is present. This technology aims at the collaboration of quality processes and evaluation of such system in networked manufacturing environment. This modeling framework based on GSPN can provide qualitative analysis and performance evaluation for continuous quality improvement. At the same time it is advantageous to define and verify the correctness of such processes. With this method, some properties such as the latency time, utility of resource and throughput of the whole system or its subnet are calculated by solving the steady state probability of GSPN. The result can provide the possibility by reconfiguration the quality process for establishing new forms of partnership with collaborators. At last, a simple application example is described and analyzed.",2004,,no
Re-evaluating Antarctic sea-ice variability and its teleconnections in a GISS global climate model with improved sea ice and ocean processes,"This study re-evaluated simulated Antarctic sea-ice variability and its teleconnections in a NASA Goddard Institute for Space Studies (GISS) coupled global climate model (CGCM) with improved sea-ice and ocean processes. With the improvements to the parameterizations of sea-ice dynamics and thermodynamics and of sub-grid-scale ocean processes, the new version of the GISS/CGCM does indeed do a better job in the representations of the local/regional ice-ocean interactions with regard to (1) the seasonal distributions of the Antarctic sea-ice edges (SIE), and (2) the vertical temperature and salinity structure in the upper Southern Ocean and surface air temperature (SAT) climatology in the southern high latitudes compared with the control version. However, these encouraging local/regional improvements do not extend to the simulations of the polar and extrapolar climate teleconnections. There is no obvious change to the simulations of the dominant spatial covarying patterns of the SAT variability on either the regional (southern high latitudes) or global scales. The simulated teleconnections between Antarctic SIE and global SAT still show the weak El Nino-southern oscillation like correlation pattern in the eastern tropical Pacific, though the new version generates a stronger tropical Indian component. Some dominant observed teleconnection patterns in the western extreme of the tropical Pacific and over the tropical continents (in-phase relationship between tropical South America and Africa) are still not well represented or are missed in the Antarctic SIE and global SAT lead/lag correlation maps and the empirical orthogonal function analysis on those correlation maps. The possible causes of the weak teleconnections in the improved GISS/CGCM are briefly discussed. Copyright (C) 2004 Royal Meteorological Society.",2004,10.1002/joc.1040,no
Experimental extraction of point defects parameters needed for 2-D process modeling using reverse modeling,"This work deals with the simulation of two-dimensional impurity diffusion in CMOS silicon devices. The Reverse Modeling method was used to determine the diffusion coefficient (D-I), surface recombination rate of defects (K-I) and the characteristics of the injecting source. Analysis showed similarity between D-I in 2-D system compared with the value obtained from non-patterned samples. The results for D-I and K-I are very well described by the Arrhenius expressions. D-I was found to be related to the substrate type e.g. EPI or CZ. The values of K-I related to the interface type, oxidizing or nonoxidizing (SiO2 or Si3N4).",2004,,no
Models and processes for the evaluation of COTS components,"This workshop summary presents an overview of the one-day International Workshop on Models and Processes for the Evaluation of COTS Components (MPEC'04), held in conjunction with the 26th International Conference on Software Engineering (ICSE'04). Details about MPEC'04 may be found at http://www.lsi.upc.es/events/mpec/.",2004,10.1109/ICSE.2004.1317523,no
The measurement of viscosity in rubber mixing process based on fuzzy modeling,"Viscosity is one of the key quantities in Rubber mixing process, online measurement of viscosity is very difficult to achieve. To cope with this problem, a soft sensing approach based on fuzzy modeling is proposed. During fuzzy modeling, T-S fuzzy model is employed to approximate the non-linearity of rubber mixing process, an improved Gustafon-Kessel fuzzy clustering algorithm based on similarity assessing is proposed to determine the optimum number of clusters. All these techniques make the fuzzy model simple and accurate. Based on it, test on BB370 internal mixer is carried out. The results show the proposed approach provides more accurate viscosity prediction than mathematical modeling approach. Compared with laboratory measurement, the error is small and acceptable. It improves production efficiency greatly and lays down the foundation for optimal control of viscosity.",2004,,no
A petri net application: Modelling a high complexity IC design process,"We present a design methodology formal description for high complexity digital ICs. The independent of technology stages as well as deendent of technology ones of the method are well defined and structured. For this purpose we have used Petri Nets with conditioned transitions. The design methodology links in a formal way the actual trends in logic synthesis and physical design, wit h a new stage in the chain of IC design: the high-level behaviour and discrete-time modelling of the system.",2004,,no
Modelling the PCR amplification process by a size-dependent branching process and estimation of the efficiency,We propose a stochastic modelling of the PCR amplification process by a size-dependent branching process starting as a supercritical Bienayme-Galton-Watson transient phase and then having a saturation near-critical size-dependent phase. This model allows us to estimate the probability of replication of a DNA molecule at each cycle of a single PCR trajectory with a very good accuracy.,2004,10.1239/aap/1086957587,no
Performance evaluation for multi-task processing system with software availability model,"We propose the performance evaluation method for the multi-task system with software reliability growth process. The time-dependent behavior of the system itself alternating between up and down states is described by the Markovian software availability model. We assume that the cumulative number of tasks arriving at the system and the processing time for a task follow the homogeneous Poisson process and the exponential distribution, respectively. Then we can formulate the distribution of the number of tasks whose processes can be complete with the infinite-server queueing model. From the model, several quantities for software performance measurement related to the task processing can be derived. Finally, we present several numerical examples of the quantities to analyze the relationship between the software reliability characteristics and the system performance measurement.",2004,10.1142/9789812702685_0068,no
Temporal codes and sparse representations: A key to understanding rapid processing in the visual system,"Where neural information processing is concerned, there is no debate about the fact that spikes are the basic currency for transmitting information between neurons. How the brain actually uses them to encode information remains more controversial. It is commonly assumed that neuronal firing rate is the key variable, but the speed with which images can be analysed by the visual system poses a major challenge Cor rate-based approaches. We Will thus expose here the possibility that the brain makes use of the spatio-temporal structure of spike patterns to encode information. We then consider how Such rapid selective neural responses can be generated rapidly through spike-timing-dependent plasticity (STDP) and how these selectivities can be used for visual representation and recognition. Finally, we show how temporal codes and sparse representations may very well arise one from another and explain some of the remarkable features of processing in the visual system. (C) 2005 Elsevier Ltd. All rights reserved.",2004,10.1016/j.jphysparis.2005.09.004,no
Role of the calibration process in reducing model predictive error,"[1] An equation is derived through which the variance of predictive error of a calibrated model can be calculated. This equation has two terms. The first term represents the contribution to predictive error variance that results from an inability of the calibration process to capture all of the parameterization detail necessary for the making of an accurate prediction. If a model is ""uncalibrated,'' with parameter values being supplied solely through ""outside information,'' this is the only term required. The second term represents the contribution to predictive error variance arising from measurement noise. In an overdetermined system, such as that which may be obtained through ""parameter lumping'' ( e. g., through the introduction of a spatial zonation scheme), this is the only term required. It is shown, however, that parameter lumping is a form of ""implicit regularization'' and that ignoring the implied first term of the predictive error variance equation can potentially lead to underestimation of predictive error variance. A model's role as a predictor of environmental behavior can be enhanced if it is calibrated in such a way as to reduce the variance of those predictions which it is required to make. It is shown that in some circumstances this can be accomplished through ""overfitting'' against historical field data. It can also be accomplished by giving greater weight to those measurements which carry the greatest information content with respect to a required prediction. This suggests that a departure may be necessary from the custom of using a single ""calibrated model'' for the making of many different predictions. Instead, model calibration may need to be repeated many times so that in each case the calibration process is optimized for the making of a specific model prediction.",2005,10.1029/2004WR003501,no
Auditory models for audio processing - Beyond the current perceived quality?,"A brief review is given about audio processing techniques that arc based on auditory models with an emphasis on applications of the Oldenburg perception model"". i.e.. objective assessment of subjective sound quality for speech and audio codecs, automatic speech recognition. SNR estimation, and hearing aids.",2005,10.1109/ASPAA.2005.1540199,no
Modelling carbon and water cycles in a beech forest Part II: Validation of the main processes from organ to stand scale,"A forest ecosystem model (CASTANEA) simulating the carbon balance (canopy photosynthesis, autotrophic and heterotrophic respirations, net ecosystem exchange, wood and root growth) and the water cycle (transpiration, soil evaporation, interception, drainage and soil water status) is tested with data from a young beech forest (Fagus sylvatica L.). For this purpose, the model validity is assessed by comparison between net CO2 and H2O fluxes simulated and measured by the eddy flux technique over one year. In addition, most of the sub-models describing the processes mentioned above are tested using independent measurements from the same forest stand: tree growth, branch photosynthesis, wood and soil respirations, sap flow and soil water content. Most of the input parameters (both weather and plant characteristics) are measured in the same experimental site (i.e. Hesse forest) independently of the validation dataset (none has been fitted to match the output data, except rainfall interception parameters); some are from other beech sites or from literature. Concerning the radiative transfer, the model reproduces the measured exponential PAR extinction and provides a good estimate of the net radiative budget, except during winter. At the branch scale, simulated photosynthesis and transpiration of sun-leaves are close to the measurements. We show also, using model simulations, that the seasonal decrease of measured net photosynthesis at the branch level could be explained by a decrease in leaf nitrogen content during the leafy season. At stand scale, a good correlation was obtained between simulated and observed fluxes both on a half-hourly basis and on a daily basis. Except at the end of the leafy season, the model reproduces reasonably well the seasonal pattern of both CO2 and H2O fluxes. Finally, even if there are some discrepancies between model estimations and fluxes measured at stand scale by eddy covariance, the model simulates properly both annual carbon and water balances when compared with the sum of the measured local fluxes. The remaining differences question the scaling up process when building such a model and the spatial footprint of eddy fluxes measurements. (c) 2005 Elsevier B.V. All rights reserved.",2005,10.1016/j.ecolmodel.2005.01.003,no
A comprehensive model of hydrogen transport into a solar cell during silicon nitride processing for fire-through metallization,"A mechanism for the transport of H into a Si solar cell during plasma-enhanced chemical vapor deposition (PECVD) of a hydrogenated silicon nitride (SiN:H) layer and its subsequent fire-through metallization process is described. The PECVD process generates process-induced traps, which ""store"" H at the surface of the solar cell. This stored H is released and diffuses rapidly into the bulk of Si during the high-temperature metallization-firing process. During the ramp-down, the diffused H associates with impurities and defects and passivates them. The firing step partially heals up the surface damage. The proposed model explains a variety of observations and experimental results.",2005,10.1109/PVSC.2005.1488311,no
Validation of 3-D electromagnetic-thermal model for microwave food processing,"A numerical model was developed to study the electromagnetic field and thermal phenomena during microwave heating food processing. Three-dimensional finite difference time domain and finite element integration is presented for the analysis of the electromagnetic field distribution and temperature distribution. Experimental measurements validated 3-D model of microwave food processing for the electromagnetic and thermal phenomena. The extensive numerical simulations were used to study effect of various processing parameters on the heating patterns in food package in a 915 MHz microwave applicator. Computed results are presented for the electric field distribution, power absorption and temperature profile for the food sterilization processing in Microwave-Circulated Water Combination (MCWC) heating system.",2005,10.1115/HT2005-72534,no
"A comprehensive, one-dimensional model of fiber processing","A one-dimensional model of fiber processing isformulated based on first principles. Several special cases are then considered, including an acetone, acetate, water, and air dry spinning process. Selected dry spinning numerical predictions are presented graphically and used to illustrate interesting aspects of the modeling.",2005,10.1080/01457630590951078,no
Evaluation of a physically-based model to simulate the runoff and erosion processes in a semiarid region of Brazil,"A physically-based distributed model called Catchment Hydrology Distributed Model-CHDM, developed by Lopes (1995), was tested in an experimental basin, located in a representative semiarid region in northeastern Brazil. This model is a refinement of an earlier model, WESP, which has been found to simulate infiltration, runoff, and erosion processes well, in one dimension, within small-sized basins. Different from WESP, which is based on a simple mass balance that does not take into account the limiting transport capacity of flow, CHDM has a built in choice of six different transport capacity relationships. The model was evaluated utilizing the runoff and erosion data collected for natural rainfall events in the Sume Experimental Basin, where erosion plots of 100 m(2) and micro-basins of about 0.5 ha were installed. The model proved to be consistent and useful for runoff and erosion prediction in the semiarid region of Brazil.",2005,,no
On the suitability of correctness criteria for business process models,"A popular requirement for the validation of workflow models is soundness. As soundness can not be easily seen on the model level, different correctness criteria have been proposed in the literature to bridge the gap between the modeling process and a executable workflow model. Well-structuredness and relaxed soundness are investigated in the paper. Relationships between the properties are derived.",2005,,no
Finite element modeling and experimental validation of cooling rates of large ready-to-eat meat products in small meat-processing facilities,"A two-dimensional axisymmetric transient heat conduction model was developed to simulate air chilling of large ready- to-eat meat products of ellipsoidal shape. A finite element scheme, using 1,600 linear triangular elements with 861 nodes, was implemented in Matlab 65 to solve the model. The model considered a variable initial temperature distribution and combined convective, radiative, and evaporative boundary conditions. Predicted values agreed well with experimental data collected in actual processing conditions. Validation of model performance resulted in maximum deviations of 2.54 degrees C and 0.29% for temperature and weight loss histories, respectively. The maximum temperature deviation (2.54 degrees C) occurred at the surface; however, for center temperature, the maximum deviation was lower (1.59 degrees C). The validated model was used to assess the extent of deviations from stabilization performance standards established by the Food Safety and Inspection Service (FSIS) caused by unexpected equipment failure or electrical power outage. A total of 48 simulations were also carried out to establish critical product sizes and operating conditions for compliance with FSIS Performance standards. It was concluded that, for cured meat products, small processors should be able to meet the stabilization requirements for any typical commercially available product size, under all simulated chilling conditions. Conversely, for non-cured meats, products should have a maximum weight of 2.25 kg (with typical dimensions of: major axis = 21.2 cm, minor axis = 13.9 cm) in order to comply with FSIS standards, particularly to meet the criteria of cooling between 54.4 degrees C to 266 degrees C. The validated model provides a useful quantitative tool for various food safety applications.",2005,,no
Genetic evaluation of mastitis resistance using a first-passage time model for Wiener processes for analysis of time to first treatment,"A Wiener process is a Brownian-motion process initiated in a certain state in a state space, and the first passage time is defined as the time of the process to reach a predefined absorbing state where the process stops. Time from 31 d prepartum to first treatment of clinical mastitis (CM) was modeled as first passage times of such Wiener processes. Two processes were used to allow for several risk factors, and for each process, initiation was at some arbitrary time point, in a certain health state with drift toward or away from absorption ( disease). The drift parameter of each process was expressed as linear functions of covariates ( year of calving and sire). First passage time was defined as the time from process initiation until the first health status process reached zero ( absorption). The model was fitted to records for 36,178 first-lactation daughters of 245 Norwegian cattle sires using a Bayesian approach and Markov chain Monte Carlo methods. Genetic evaluation of sires was carried out by calculating the posterior probability of no CM ( the value of the survival function) by d 331, i.e., 300 d after first calving. Alternatively, sire evaluation was based on the integrated area under the survival curve. These measures were highly correlated (0.999), which indicates a small degree of crossings of the sire-dependent survival curves. Hence, sire-specific hazards were close to proportional, resulting in a higher rank-correlation to sire evaluations from a survival model with proportional hazards than to the results from a multivariate threshold model.",2005,,no
Maintainability of software process models: An empirical study,"Adequate process modeling is one of the key factors for the success of software organizations. Currently, organizations face with a very high competition and consequently they have to continuously improve their processes. This improvement could require changes of the process models so it is important to evaluate the maintainability of these models to facilitate their evolution. In this paper we present the results obtained with the replication of an experiment to validate a set of metrics for Software Process Models (SPMs). The replicas were performed at the Italian Universities of Sannio (Italy) and Federico II (Naples). They are part of a family of experiments and have confirmed the results obtained in the original experiment carried out at the University of Castilla-La Mancha (Spain). As a result a set of useful indicators of the understandability and modifiability of the SPMs, which are two key subcharacteristics of their maintainability, have been obtained.",2005,10.1109/CSMR.2005.35,no
A framework for e-business quality modeling based on process interaction,"Among the factors that block e-business' development, quality of e-business is a critical problem to be resolved. Scientific quality modeling of e-business is important to improvement of e-business. Compared with traditional business, the shift of business interaction manner is the essential character of e-business. Constructing interaction model of e-business transaction process and then making quality analysis is a good way to improve the quality of e-business. In this paper we will develop a framework to deriving customer perceived quality model of e-business, which is based on the dynamic interaction process of e-business transaction.",2005,,no
Process model quality assessment by sensitivity analysis,"An accurate process model is the linchpin of model-based Optical Proximity Correction (OPC) and Resolution Enhancement Technique (RET) synthesis. The accuracy of the resulting mask layout can be no better than that of the model. Relatively good, first-principle mathematical models exist for some process steps, such as aerial image formation, but resulting silicon is a combination of many effects, including those less well understood. Accuracy can be assured only with models anchored to observed phenomena. Process models are usually a combination of first principle elements and phenomenological components with the ""right"" degrees to freedom to fit the overall process. The key challenge in generating, accurate models is to capture all process behavior over all conditions with a minimum number of empirical measurements. This means that models must extrapolate accurately from the specifics measured, and should be largely immune to empirical measurement noise. In this paper we describe a methodology in which to test model performance with respect to these criteria.",2005,10.1117/12.617197,no
The quantum measurement process in an exactly solvable model,"An exactly solvable model for a quantum measurement is discussed which is governed by hamiltonian quantum dynamics. The z-component (s) over cap (z), of a spin -1/2 is measured with an apparatus, which itself consists of magnet coupled to a bath. The initial state of the magnet is a metastable paramagnet, while the bath starts in a thermal, gibbsian state. Conditions are such that the act of measurement drives the magnet in the up or down ferromagnetic state according to the sign of s, of the tested spin. The quantum measurement goes in two steps. On a timescale 1/root N the off-diagonal elements of the spin's density matrix vanish due to a unitary evolution of the tested spin and the N apparatus spins; on a larger but still short timescale this is made definite by the bath. Then the system is in a 'classical' state, having a diagonal density matrix. The registration of that state is a quantum process which can already be understood from classical statistical mechanics. The von Neumann collapse and the Born rule are derived rather than postulated.",2005,,no
FEBUKO and MODMEP: Field measurements and modelling of aerosol and cloud multiphase processes,"An overview of the two FEBUKO aerosol-cloud interaction field experiments in the Thuringer Wald (Germany) in October 2001 and 2002 and the corresponding modelling project MODMEP is given. Experimentally, a variety of measurement methods were deployed to probe the gas phase, particles and cloud droplets at three sites upwind, downwind and within an orographic cloud with special emphasis on the budgets and interconversions of organic gas and particle phase constituents. Out of a total of 14 sampling periods within 30 cloud events three events (El, Ell and EIII) are selected for detailed analysis. At various occasions an impact of the cloud process on particle chemical composition such as on the organic compounds content, sulphate and nitrate and also on particle size distributions and particle mass is observed. Moreover, direct phase transfer of polar organic compound from the gas phase is found to be very important for the understanding of cloudwater composition. For the modelling side, a main result of the MODMEP project is the development of a cloud model, which combines a complex multiphase chemistry with detailed microphysics. Both components are described in a fine-resolved particle/drop spectrum. New numerical methods are developed for an efficient solution of the entire complex model. A further development of the CAPRAM mechanism has lead to a more detailed description of tropospheric aqueous phase organic chemistry. In parallel, effective tools for the reduction of highly complex reaction schemes are provided. Techniques are provided and tested which allow the description of complex multiphase chemistry and of detailed microphysics in multidimensional chemistry-transport models. (c) 2005 Elsevier Ltd. All rights reserved.",2005,10.1016/j.atmosenv.2005.02.004,no
Modelling dewatering behaviour through an understanding of solids fonnation processes. Part II - solids separation considerations,"An understanding of the mechanisms which control solids formation can provide information on the characteristics of the solids which are formed. The nature of the solids formed in turn impacts on dewatering behaviour. The 'upstream' solids formation determines a set of suspension characteristics: solids concentration, particle size distribution, solution ionic strength and electrostatic surface potential. These characteristics together define the suspension's rheological properties. However, the complicated interdependence of these has precluded the prediction of suspension rheology from such a fundamental description of suspension characteristics. Recent shear yield stress models, applied in this study to compressive yield, significantly reduce the empiricism required for the description of compressive rheology. Suspension compressibility and permeability uniquely define the dewatering behaviour, described in terms of settling, filtration and mechanical expression. These modes of dewatering may be described in terms of the same fundamental suspension mechanics model. In this way, it is possible to link dynamically the processes of solids formation and dewatering of the resultant suspension. This, ultimately, opens the door to improved operability of these processes. In part I of this paper [I] [Dustan AC, Cohen B, Petrie JG. 'Modelling dewatering behaviour through an understanding of solids formation processes. Part I-solids formation considerations', doi:10.1016/j.cis.2005.01.004] we introduced an integrated system model for solids formation and dewatering. This model was demonstrated for the upstream processes using experimental data. In this current paper models of colloidal interactions and dewatering are presented and compared to experimental results from batch filtration tests. A novel approach to predicting suspension compressibility and permeability using a single test configuration is presented and tested. (c) 2005 Elsevier B.V. All rights reserved.",2005,10.1016/j.cis.2005.01.003,no
Modelling dewatering behaviour through an understanding of solids fonnation processes. Part I - Solids formation considerations,"An understanding of the mechanisms which control solids formation can provide information on the characteristics of the solids which are formed. These characteristics will in turn impact on dewatering behaviour. In this paper a model for solids formation is proposed. The first part of the model considers the hydrodynamics in the precipitation vessel, from which a reactant mixing model is developed. Spatially variant solution conditions are quantified (dynamically) using an equilibrium speciation model. These calculations are performed in conjunction with an adsorption model, accounting for equilibria involving adsorbed species. The kinetics of solids formation, including nucleation, growth and aggregation, are described empirically using spatially variant supersaturation profiles. These, together with moment transformations of the solids population balance, describe the evolution of particle sizes throughout the precipitation process. Precipitation of nickel hydroxide is explored experimentally, and models developed are fitted to the results. Comments are offered on the impact of simplifications required for computational reasons, and assumptions required due to lack of information, on the accuracy of the model. In part H of this paper, the use of model outputs in predicting filtration behaviour is explored. (c) 2005 Elsevier B.V. All rights reserved.",2005,10.1016/j.cis.2005.01.004,no
Membrane formation by dry-cast process model validation through morphological studies,"Asymmetric membranes were prepared by dry-cast phase inversion technique from a cellulose acetate, acetone, water solution in order to assess the validity of the mathematical model recently developed by us. Based on the model predictions, general structural characteristics of the membranes were determined by plotting the composition paths on the ternary phase diagram and polymer concentration profile at the first moment of precipitation. Composition paths on the ternary phase diagram enable the assessment of whether a phase separation occurs and allow prediction of inception time and duration of the phase separation. The polymer distribution at the moment of precipitation provides a rough thickness of the high polymer concentration region near the interface and a pore distribution of the sublayer structure. The effects of polymer/nonsolvent ratio in the casting solution, the initial film thickness, evaporation temperature, relative humidity and velocity of air were investigated. Model predictions were compared with the morphological analysis conducted using scanning electron microscopy. Results show that diffusion formulation plays an important role in capturing the accurate structure of the membrane from the model predictions. (C) 2004 Elsevier B.V. All rights reserved.",2005,10.1016/j.memsci.2004.10.008,no
Intermediate care: for better or worse? Process evaluation of an intermediate care model between a university hospital and a residential home,"Background: Intermediate care was developed in order to bridge acute, primary and social care, primarily for elderly persons with complex care needs. Such bridging initiatives are intended to reduce hospital stays and improve continuity of care. Although many models assume positive effects, it is often ambiguous what the benefits are and whether they can be transferred to other settings. This is due to the heterogeneity of intermediate care models and the variety of collaborating partners that set up such models. Quantitative evaluation captures only a limited series of generic structure, process and outcome parameters. More detailed information is needed to assess the dynamics of intermediate care delivery, and to find ways to improve the quality of care. Against this background, the functioning of a low intensity early discharge model of intermediate care set up in a residential home for patients released from an Amsterdam university hospital has been evaluated. The aim of this study was to produce knowledge for management to improve quality of care, and to provide more generalisable insights into the accumulated impact of such a model. Methods: A process evaluation was carried out using quantitative and qualitative methods. Registration forms and patient questionnaires were used to quantify the patient population in the model. Statistical analysis encompassed T-tests and chi-squared test to assess significance. Semi-structured interviews were conducted with 21 staff members representing all disciplines working with the model. Interviews were transcribed and analysed using both 'open' and 'framework' approaches. Results: Despite high expectations, there were significant problems. A heterogeneous patient population, a relatively unqualified staff and cultural differences between both collaborating partners impeded implementation and had an impact on the functioning of the model. Conclusion: We concluded that setting up a low intensity early discharge model of intermediate care between a university hospital and a residential home is less straightforward than was originally perceived by management, and that quality of care needs careful monitoring to ensure the change is for the better.",2005,10.1186/1472-6963-5-38,no
Simulation Modeling of Iron Ore Product Quality for Process and Infrastructure Development,"BHP Billiton has several iron ore mining operations in the inland Pilbara region of Australia. Iron ore is mined from open-cut pits, railed several hundred kilometres to two port processing facilities at Port Hedland. Here the ore is processed, blended and the lump product rescreened ready for shipment, mainly to Asia. Customers use the ore as principal feed in steel production. Figure 1 shows a simplified ore flow, although many operational variations exist. The recent sharp increase in iron ore demand has required a review of capacity. To deliver increased tonnage of ore safely, at minimum cost and acceptable quality, processes need to be upgraded, existing mines and infrastructure expanded, and new mines brought on line. Expansion assessment requires informed choices involving alternative options of operation and infrastructure development that differ greatly in capital and operating costs. Multiple choices exist between alternative mining practices, ore processing, stockpiling, railing and ship loading operations. Any expansion option must be assessed to understand the effect on product quality. Customers assess quality by both cargo grade, and inter-cargo grade variability. As well as iron, several impurities, principally phosphorus, silica, and alumina are important. To gauge the effect of expansion options on shipped product quality, simulation models have been built, enabling mining and handling configurations to be studied, for appropriate time periods. Since ore production grade shows complex serial and cross correlations, totally synthetic data cannot be constructed. Production was therefore simulated from historic data of geologically similar ore, statistically adjusted to match potential operations. To help company personnel use the simulation models, they were written as Excel T workbooks, driven by Visual Basic (VBA) macros, making full use of the provided graphical capabilities. The simulation models described in this paper have been extensively used to evaluate alternative expansion and development options. The company is using the results from the models to assess the effect on product quality for many processing and equipment options in determining it expansion direction.",2005,,no
A state-activity-state process model for defects management in power plants,"Business process modeling is a main research area of workflow technology. In this article, an innovated modeling approach, which models business flow with State-Activity-State (SAS, in short) nets, is brought forth and used to model defects management process of power plants. In order to set up the SAS model of defects management process, typical business processes in two Chinese power plants are evaluated, and the state set and activity set used for SAS net are put forward. The defects management process based on SAS model is flexible enough to be used in different power plants with different defects business flows.",2005,,no
Factors and measures of business process modelling: model building through a multiple case study,"Business process modelling has gained widespread acceptance as a valuable design and management technique for a variety of purposes. While there has been much research on process modelling techniques and corresponding tools, there has been little empirical research into the success factors of effective process modelling, and the post hoc evaluation of process modelling success. This paper reports on the first attempt to identify process modelling success factors and measures, as empirically evidenced in case studies of nine process modelling projects in three leading Australian organizations.",2005,10.1057/palgrave.ejis.3000546,no
Using event-based process modelling to support six sigma quality,"Collaborative and operational process visibility requires a process performance monitoring framework that provides traceability in near real-time. Process variability produces non-conforming or defective products and services. The focus of a Six Sigma quality programme is to reduce variability using statistical methods to highlight variance. Business interactions may be modelled using an event-based process model. In addition, business object information may be packaged with enterprise events. Process models lack the performance and quality information necessary for complete process lifecycle management and portability across disparate process management systems. This paper will describe the combination of process model, event and defect information to support a Six Sigma reporting mechanism for achieving process transparency. A proposition to supplement current process definition languages with such performance information is also included.",2005,10.1109/DEXA.2005.196,no
Consistency analysis of interorganizational processes based on activity diagrams,"Consistency property describes the relation between the communication pattern and process logic of interorganizational processes. And the Petri net and message sequence chart are applied as the modeling tool for the communication pattern and process logic of interorganizational processes respectively. But these two different modeling tools bring confusions and make it is hard to analyze the consistency property of interorganizational processes. The modeling method of interorganizational processes based on activity diagrams is proposed in this work. The formal semantics of activity diagrams and the standardized mapping technique from activity diagrams to message sequence charts are introduced Based on activity diagrams, the N-consistency property of interorganizational processes is defined and verified",2005,10.1109/ICEBE.2005.41,no
Modelling and validated simulation of solvent-gradient simulated moving bed (SG-SMB) processes for protein separation,"Continuously operated chromatographic processes like the Simulated Moving Beds (SMB) are well established for the purification of hydrocarbons, fine chemicals as well as pharmaceuticals. With respect to discontinuous batch chromatography they have proven their ability to improve the process performance in terms of productivity, eluent consumption and product concentration especially for larger production rates. These processes are operated under isocratic conditions where the composition of the mobile phase remains constant. Non isocratic processes are necessary if e.g. bio-products like proteins with a large difference in their affinity have to be separated. Therefore the implementation of solvent gradients is an important approach to improve the performance of SMB processes. The advantages of solvent gradient SMB processes are achieved by a higher complexity with respect to layout and operation which makes an empirical design nearly impossible. Therefore process simulation based on validated rigorous models is necessary for process design and optimisation.",2005,,no
Viscosity measurement by cylindrical compression for numerical modeling of precision lens molding process,"Cylindrical compression tests were conducted on two different optical glass grades for determining the high-temperature viscosity and elastic parameters. Numerical simulations of the compression tests and the precision lens molding process were performed by incorporating the data obtained from the compression tests using a commercial finite-element method program. Excellent agreement between the viscosity data from the compression test and the beam bending test was obtained, and a good comparison between the measured and predicted deformation load results was also observed. Further issues that have emerged from this research that would be relevant to the ongoing research on the numerical modeling of the precision aspherical lens molding process include determination of hightemperature elastic properties of glass (i.e. elastic and shear modulus) and friction characterization at the glass-mold interface.",2005,10.1111/j.1551-2916.2005.00477.x,no
Process evaluation of an integrated health promotion/occupational health model in WellWorks-2,"Disparities in chronic disease risk by occupation call for new approaches to health promotion. WellWorks-2 was a randomized, controlled study comparing the effectiveness of a health promotion/occupational health program (HP/OHS) with a standard intervention (HP). Interventions in both studies were based on the same theoretical foundations. Results from process evaluation revealed that a similar number of activities were offered in both conditions and that in the HP/OHS condition there were higher levels of worker participation using three measures: mean participation per activity (HP: 14.2% vs. HP/OHS: 21.2%), mean minutes of worker exposure to the intervention/site (HP: 14.9 vs. HP/OHS: 33.3), and overall mean participation per site (HP: 34.4% vs. HP/ OHS: 45.8%). There were a greater number of contacts with management (HP: 8.8 vs. HP/OHS: 24.9) in the HP/ OHS condition. Addressing occupational health may have contributed to higher levels of worker and management participation and smoking cessation among blue-collar workers.",2005,10.1177/1090198104264216,no
Fuzzy adaptive networks in machining process modelling: dimensional error prediction for turning operations,"Due to the complexity of machine tool structure and the cutting process, the dynamics of machining processes are still not completely understood. This is especially true for high-speed machining processes. To model and control these complex processes, new approaches, which can represent complex phenomenon combined with learning ability, are needed. The combined neural-fuzzy approach appears ideally suited for this purpose. To illustrate the approach, the recently developed fuzzy adaptive networks are used to model dimensional error in turning operations. The fuzzy adaptive network has both the learning ability of a neural network and the linguistic representation of a complex, not well understood or vague phenomenon. An approximate model representing the influences of machining parameters on dimensional error is first established. This model is then improved by learning with the given training data. The improved models are verified by the use of test data, which are obtained by the use of actual experiments.",2005,10.1080/0007540500031964,no
In-line Process Analytical Technology based on qualitative near-infrared spectroscopy modeling - An innovative approach for the pharmaceutical quality control,"Due to the increasing competition in the pharmaceutical industry the cost effectiveness of manufacturing becomes more and more important. Process Analytical Technologies (PAT) are endeavored to increase the process understanding and thereby reduce the risk of producing poor quality. Furthermore, an integrated quality control shortens the process times being one of the main cost drivers. Accordingly it was attempted to replace the Karl Fischer titration of individual sealed vials of an established lyophilization process by a high-speed in-line near-infrared inspection of entire batches. Since a simple classification result is sufficient to test against a certain acceptance limit, a quantitative water determination was replaced by an innovative qualitative chemometric calibration. This reliable method was robust and fast but reduced the validation effort remarkably; however a threshold challenge was introduced as a new criterion.",2005,,no
A strategy for process-oriented validation of coupled chemistry-climate models,Evaluating CCMs with the presented framework will increase our confidence in predictions of stratospheric ozone change.,2005,10.1175/BAMS-86-8-1117,no
Application of scatterometry for evaluation of lithographic process and OPC model generation,"Evaluation and qualification of lithographic exposure tools is a crucial step in establishing high volume manufacturing processes for IC manufacturers. The data sampling offered by scatterometry can be as dense as that from ECD (electrical CD) for the qualification of the tool. In this paper, the CDs obtained from scatterometry measurements are compared with those obtained by ECD (electrical CD) measurements to show the cross-slit and cross-scan tool characteristics. Since scatterometry is still an order of magnitude slower than ECD, data from various sampling plans will be compared. Another important consideration of this study is to use scatterometry to generate OPC (optical proximity correction) models for the 45nm and 32nm nodes. An accurate measurement of the process to fit the model becomes very crucial in the very deep sub-micron regime. Currently, SEM measurements are performed but they are slow and their precision is not adequate. In this paper, scatterometry measured data will also be compared with SEM data for OPC model fit.",2005,10.1117/12.600183,no
Wildfire hazard evaluation through a space-time point process conditional intensity model,"Fire departments all over the world often use numerical indices to aid in wildfire management and hazard assessment. These indices are designed to summarize local meteorological and fuel information and provide an estimate of the current risk of fire. In this paper we evaluate the effectiveness of the Burning Index (BI) (a collection of numerical indices designed to be used for fire planning and management) for predicting wildfire occurrences in the Valencian Community (Spain) using space-time point process models. The models are based on a particular decomposition of the conditional intensity, with separate terms to describe spatial and seasonal variability as well as contributions from the BI.",2005,,no
Multiple target tracking with possibly merged measurements modeled by point processes,"Generally sensor resolutions are finite, and hence any measurement may originate from two or more objects. Recent advances in sensor signal processing technology have made it possible to estimate the number of objects that may originate from a given detection for a certain type of sensor. With these new methods, it may be possible to associate each, possibly merged, measurement with state components of an unknown number of objects from which the measurement originates. One suitable mathematical model for an unknown number of object states is a finite point process that can be defined by a family of permutable (symmetric) probability distributions together with the probability of the number of the objects. In this paper we will show how general multiple hypothesis tracking systems, usually designed with the assumption that measurements are all resolved, may be extended to process possibly merged measurements modeled by point processes.",2005,,no
Evaluating a rapid simulation modelling process (RSMP) through controlled experiments,"In recent years there has been an increasing interest in simulation modelling to study, control, and manage software development processes. Despite this, little attention has been paid towards the simulation modelling process itself We have developed a rapid simulation modelling process (RSMP) based on our study of experienced simulation modellers. This paper reports a set of controlled experiments to evaluate the usability and utility of the RSMP. The results from this show that the RSMP has proved to be a usable and useful approach for developing software process simulation models. The results show that the RSMP is likely to bring discipline to software process simulation modellers and improve the quality of the model they produce.",2005,10.1109/ISESE.2005.1541841,no
Defect-diffusion-stress relationships in modeling the oxidation and degradation processes of alumina formers: A brief survey,"In the modeling of the oxidation and degradation processes of the superior metallic materials in terms of the high temperature oxidation resistance, being the so-called alumina formers, the matter transport only via grain boundaries of alpha-Al2O3 is usually taken into account. This paper indicates that such an approach needs to be re-visited. It is pointed out that the following factors should be taken into account (i) diffusion via point defects in the unstable alumina polymorphs (gamma, delta, theta) which grow during the early oxidation stages; (ii) dislocation-related processes; (iii) formation of the oxide in three-dimensional defects, being the cracks in the oxide layer; (iv) outward oxide growth resulting from the formation of the new oxide within the existing layer. The mentioned above effects as well as the other ones, related to the interfacial and stress-induced processes are illustrated and discussed. It is pointed out that alumina formers constitute a wide group of materials and several categories should be distinguished among them with respect to their composition and production route on one hand, and the oxidation and degradation mechanisms, on the other hand. It is shown that the comprehensive approach enables better and self-consistent modeling of the oxidation and degradation mechanisms of alumina formers.",2005,,no
Estimation of bioreactor efficiency through structured hydrodynamic modeling case study of a Pichia pastoris fed-batch process,"In this article, two theories are unified to investigate the effect of hydrodynamics on a specific bioprocess: the network-of-zones (NOZ) hydrodynamic structured modeling approach (developed by several researchers but applied to only a few bioprocesses) and the effectiveness factor eta approach. Two process scales were investigated (20 and 500 L), and for each, hydrodynamics were quantified using an NOZ validated by homogeneity time measurements. Several impeller combinations inducing quite different hydrodynamics were tested at the 20-L scale. After this step, effectiveness factors were determined for each fermentation run. To achieve this, a perfectly mixed microbial kinetic model was evaluated by using simple Monod kinetics with a fed-batch mass balance. This methodology permitted determination of the effectiveness factor with more accuracy because of the relation with the perfect case deduced from the Monod kinetics. It appeared that for the small scale, eta decreased until reaching a value of approx 0.7 (30% from the ideal case) for the three impeller systems investigated. However, stirring systems that include hydrofoils seemed to maintain higher effectiveness factors during the course of the fermentation. This effect can be attributed to oxygen transfer performance or to homogenization efficiency exhibited by the hydrofoils. To distinguish the oxygen transfer from the homogenization component of the effectiveness factor, these phenomena were analyzed separately. After determining the evolution of eta(O2), linked to oxygen transfer for each of the fermentation runs, the NOZ model was employed to quantify substrate gradient appearance. After this step, another effectiveness factor, eta(mix), related to mixing was defined. Consequently, it is possible to distinguish the relative importance of the mixing effect and oxygen transfer on a given bioprocess. The results have highlighted an important scale effect on the bioprocess that can be analyzed using the NOZ model.",2005,,no
Using an expert panel to validate a requirements process improvement model,In this paper we present components of a newly developed software process improvement model that aims to represent key practices in requirements engineering (RE). Our model is developed in response to practitioner needs highlighted in our empirical work with UK software development companies. We have now reached the stage in model development where we need some independent feedback as to how well our model meets our objectives. We perform this validation through involving a group of software process improvement and RE experts in examining our RE model components and completing a detailed questionnaire. A major part of this paper is devoted to explaining our validation methodology. There is very little in the literature that directly relates to how process models have been validated. therefore providing this transparency will benefit both the research community and practitioners. The validation methodology and the model itself contribute towards a better understanding of modelling RE processes. (c) 2004 Elsevier Inc. All rights reserved.,2005,10.1016/j.jss.2004.06.004,no
Fractionation process in TREF systems: Validation of thermodynamic model and calculation procedure by Raman LAM studies,"In this paper, possible sources for the unexpected distributions of crystalline sequence lengths calculated from temperature rising elution fractionation (TREF) calibration experiments, as reported in a previous work, are investigated. With this aim, chain folding and cocrystalization phenomena were explored in the conditions of crystallization as used for TREF or crystallization analysis fractionation (CRYSTAF). Slow crystallizations were performed from xylene solutions of model low molecular weight ethylene homopolymers with narrow molecular weight distributions. The same experiments were performed with homopolymers having narrow molecular weight distributions and with blends having wide molecular weight distributions. The resulting distributions of the lengths of crystalline methylene sequences were directly studied by Raman in the so-called longitudinal acoustic mode (LAM) and by DSC. For ethylene homopolymers with molecular weights below 2000 g/mol, the results from Raman LAM indicate that slow crystallization in TREF or CRYSTAF systems occurs in the extended-chain mode. For higher molecular weights, evidence of chain folding was found. In the case of blends, independent crystallization was observed for each molecular weight when the molecular weight ranges used for the blends are relatively narrow. Cocrystallization was observed when this range was increased. Overall, these results strongly support the inverse technique calculation procedure developed by our group for the calculation of distributions of lengths of crystallizable sequences from TREF spectra. In this context, the results confirm that the unexpected crystallizable sequence lengths found in our previous work really exist and can be associated to chain folding or cocrystallization phenomena. (c) 2005 Wiley Periodicals, Inc.",2005,10.1002/polb.20582,no
Customer-oriented improvement and evaluation of supply chain processes supported by simulation models,"In this paper, we explain why customer orientation is important for evaluation and improvement of supply chain process, and we show the interdependencies with performance measures which should be taken into account so as to integrate the requirements of supply chain performance management. Then, we describe how process improvements can be dynamically evaluated under consideration of customer orientation and supported by an integrated usage of discrete-event simulations models and system dynamics models. Finally, we illustrate the use of selected performance measures as well as indicators and our models by a specific process improvement (postponement) conducted by an electronic manufacturer in the telecom industry. (c) 2004 Elsevier B.V. All rights reserved.",2005,10.1016/j.ijpe.2004.07.004,no
Markov models for the physical layer block error process in a WCDMA cellular system,"In this paper, we investigate the possibility of using Markov chains to model the error process in the data blocks delivered by the physical layer of wideband code division multiple access a (WCDNM) cellular system. Suitable Markov models (NM) are designed to fulfill the two following objectives: First, an upper layer protocol supplied by the output obtained from the MM should behave as if it were running on the actual physical layer; second, MM parameters should be linked via simple relationships to the main physical layer parameters. Starting from the results on the error statistics obtained from a suitable simulation tool which jointly performs system and link level analysis, we first classify the users on the basis of performance level and burstiness, and then, we provide some guidelines for the design of Markov models in the different system and channel conditions. The performance of an automatic repeat request (ARQ) (Go-Back N) protocol at the link layer is taken as an example to test the accuracy of the proposed models. It is shown that the perspective of using simple error models in the analysis of upper layer protocols is feasible in many cases.",2005,10.1109/TVT2005.858188,no
The in vitro acute skin irritation of chemicals: Optimisation of the EPISKIN prediction model within the framework of the ECVAM validation process,"In view of the increasing need to identify non-animal tests able to predict acute skin irritation of chemicals, the European Centre for the Validation of Alternative Methods (ECVAM) focused on the evaluation of appropriate in vitro models. In vitro tests should be capable of discriminating between irritant (1) chemicals (EU risk: R38) and non-irritant (NI) chemicals (EU risk: ""no classification""). Since major in vivo skin irritation assays rely on visual scoring, it is still a challenge to correlate in vivo clinical signs with in vitro biochemical measurements. Being particularly suited to test raw materials or chemicals with a wide variety of physical properties, in vitro skin models resembling in vivo human skin were involved in prevaliclation processes. Among many other factors, cytotoxicity is known to trigger irritation processes, and can therefore be a first common event for irritants. A refined protocol (protocol(15min-18hours)) for the EPISKIN model had been proposed for inclusion in the ECVAM formal validation study. A further improvement on this protocol, mainly based on a post-treatment incubation period of 42 hours (protocol(15min-42hours)), the optimised protocol, was applied to a set of 48 chemicals. The sensitivity, specificity and accuracy with the MTT assay-based prediction model (PM) were 85%, 78.6% and 81.3% respectively, with a low rate of false negatives (12%). The improved performance of this optimised protocol was confirmed by a higher robustness (homogeneity of individual responses) and a better discrimination between the I and NI classes. To improve the MTT viability-based PM, the release of a membrane damage marker, adenylate kinase (AK), and of cytokines IL-1 alpha and IL-8 were also investigated. Combining these endpoints, a simple two-tiered strategy (TTS) was developed, with the MTT assay as the first, sort-out, stage. This resulted in a clear increase in sensitivity to 95%, and a fall in the false-positive rate (to 4.3%), thus demonstrating its usefulness as a ""decision-making"" tool. The optimised protocol proved, both by its higher performances and by its robustness, to be a good candidate for the validation process, as well as a potential alternative method for assessing acute skin irritation.",2005,,no
"Stage of change transitions and processes of change, decisional balance, and self-efficacy in smokers: A transtheoretical model validation using longitudinal data","Interactions were examined between stage of change transitions and intraindividual increases or decreases in the processes of change, pros and cons of smoking, and situational temptations longitudinally. A total of 786 ever smokers was assessed 2 times, 6 months apart, with respect to the transtheoretical model (TTM) constructs. Two significant discriminant functions within initial precontemplators and 1 significant function within initial contemplators were found. Ten out of 15 TTM variables contributed to at least 1 function. The functions mainly distinguished between preabstinence (precontemplation, contemplation, or preparation) and abstinence (action or maintenance) stages of change, that is, between current and former smokers. This is one of the few studies providing a longitudinal validation of the postulates of the TTM.",2005,10.1037/0893-164X.19.1.3,no
The comprehension and production of pantomimes from a cognitive model of processing,"Introduction. At present in the literature there are basically two different perspectives to explain the relation between the capacity to discriminate pantomimes and the capacity to produce them, within the framework of present-day models concerning praxes. The first of these positions suggests the need for two separate systems (action lexicons), one for discrimination (action input lexicon) and another for gestural production (action output lexicon). The other alternative holds that the structures used for production are the same as those responsible for discriminating gestures. The aim of this study is to report the cases of two patients with complementary dissociations between the discrimination and the production of pantomimes. Case reports. We describe the performance of two female patients evaluated with a cognitive battery for evaluating apraxias. Patient 1 had difficulties in discriminating pantomimes with relative preservation when executing them, whereas patient 2 found it hard to perform pantomimes with relative preservation when discriminating them. Conclusions. The findings, that is, the presence of a double dissociation between discrimination and performance of pantomimes, add support to the proposed existence of two different systems, one for the discrimination and one for the production of gestures.",2005,,no
Deriving requirements from process models via the problem frames approach,"Jackson's problem frames is an approach to describing a recurring software problem. It is presumed that some knowledge of the application domain and context has been gathered so that an appropriate problem frame can be determined. However, the identification of aspects of the problem, and its appropriate 'framing' is recognised as a difficult task. One way to describe a software problem context is through process modelling. Once contextual information has been elicited, and explicitly described, an understanding of what problems need to be solved should emerge. However, this use of process models to inform requirements is often rather ad hoc; the traceability from business process to software requirement is not always as straightforward as it ought to be. Hence, this paper proposes an approach for deriving and contextualising software requirements through use of the problem frames approach from business process models. We apply the approach on a live industrial e-business project in which we assess the relevance and usefulness of problem frames as a means of describing the requirements context. We found that the software problem did not always match easily with Jackson's five existing frames. Where no frame was identified, however, we found that Jackson's problem diagrams did couch the requirements in their right context, and thus application of the problem frames approach was useful. This implies a need for further work in adapting a problem frames approach to the context of e-business systems. (c) 2004 Elsevier B.V. All rights reserved.",2005,10.1016/j.infsof.2004.09.002,no
Emotional and temporal aspects of situation model processing during text comprehension: An event-related fMRI study,"Language comprehension in everyday life requires the continuous integration of prior discourse context and general world knowledge with the current utterance or sentence. In the neurolinguistic literature, these so-called situation model building processes have been ascribed to the prefrontal cortex or to the right hemisphere. In this study, we use whole-head event-related fMRI to directly map the neural correlates of narrative comprehension in context. While being scanned using a spin-echo sequence, 20 participants listened to 32 short stories, half of which contained globally inconsistent information. The inconsistencies concerned either consistent temporal or chronological information or the emotional status of the protagonist. Hearing an inconsistent word elicited activation in the right anterior temporal lobe. The comparison of different information aspects revealed activation in the left precuneus and a bilateral frontoparietal network for chronological information. Emotional information elicited activation in the ventromedial prefrontal cortex and the extended amygdaloid complex. In addition, the integration of inconsistent emotional information engaged the dorsal frontomedial cortex (Brodmann's area 8/9), whereas the integration of inconsistent temporal information required the lateral prefrontal cortex bilaterally. These results indicate that listening to stories can elicit activation reflecting content-specific processes. Furthermore, updating of the situation model is not a unitary process but it also depends on the particular requirements of the text. The right hemisphere contributes to language processing in context, but equally important are the left medial and bilateral prefrontal cortices.",2005,10.1162/0898929053747658,no
"Functionally graded mold inserts by laser-based flexible fabrication: processing modeling, structural analysis, and performance evaluation","Laser-based flexible fabrication (LBFF), a novel solid freeform fabrication (SFF) method-based on the principles of laser cladding (LC), was developed to produce functionally graded mold (FGM) inserts using shaped laser beams, quasi-coaxial nozzle for powder delivery, and functionally graded materials. As a case study of this innovative method, a hollow square mold insert was fabricated with additive layers of H13 steel, Ni/Cr alloy, and TiC using circular and rectangular beam (RB) profiles. Finite element analysis using ANSYS (R) was applied to determine temperature fields and thermal gradients associated with circular and rectangular beams. The rnicrostructures and interfaces were examined using a scanning electron microscope (SEM), and related to the temperature gradients. Characterization results show a nearly full-density mold with excellent integrity, beneficial microstructures, strong interfaces, and high hardness. In addition, the mold insert was tested and compared to H13 steel molds in a thermal fatigue environment for ability to resist crack initiation, thermal strain, and oxidation. (c) 2004 Elsevier B.V. All rights reserved.",2005,10.1016/j.jmatprotec.2004.08.029,no
Toward validation of process criteria for high-pressure processing of orange juice with predictive models,"Mathematical models were developed to predict time to inactivation (TTI) by high-pressure processing of Salmonella in Australian Valencia orange juice (pH 4.3) and navel orange juice (pH 3.7) as a function of pressure magnitude (300 to 600 MPa) and inoculum level (3 to 7 log CFU/ml). For each model, the TTI was found to increase with increasing inoculum, level and decrease with increasing pressure magnitude. The U.S. Food and Drug Administration Juice Hazard Analysis and Critical Control Point Regulation requires fruit juice processors to include control measures that produce a 5-log reduction of the pertinent microorganism of public health significance in the juice. To achieve a 5-log reduction of Salmonella in navel orange juice at 20&DEG; C, the models predicted hold times of 198, 19, and 5 s at 300, 450, and 600 MPa, respectively. In Valencia orange juice at 20&DEG; C, a 5-log reduction of Salmonella was achieved in 369, 25, and 5 s at 300, 450, and 600 MPa, respectively. At pressures below 400 MPa, Salmonella was more sensitive to pressure in the more acidic conditions of the navel orange juice and TTIs were shorter. At higher pressures, little difference in the predicted TTI was observed. Refrigerated storage (4&DEG; C) of inoculated navel orange juice treated at selected pressure/time/inoculum combinations showed that under conditions in which viable Salmonella was recovered immediately after high-pressure processing, pressure-treated Salmonella was susceptible to the acidic environment of orange juice or to chill storage temperature. These TTI models can assist fruit juice processors in selecting processing criteria to achieve an appropriate performance criterion with regard to the reduction of Salmonella in orange juice, while allowing for processing flexibility and optimization of high-pressure juice processing.",2005,,no
Modelling and evaluation of the micro abrasive blasting process,"Micro abrasive blasting (MAB) is becoming an important machining technique for the cost effective fabrication of micro devices. The material removal process is based on the erosion of a mask-protected brittle substrate by an abrasive-laden air jet. To exploit the potentials of this technique for applications of industrial interest, the blasting process has to become more efficient and better predictable. Therefore, in this paper micro-abrasive blasting is analysed by means of a set of models containing different sub-models for the particle jet, the erosion mechanisms of a single particle impact and the machining results. A one-dimensional isentropic flow model was developed to calculate the particle exit velocity of each individual particle in the airflow for two different types of nozzles: a converging cylindrical and a new developed line shaped Laval-type. The particle size and its position within the air jet are based on probability distribution functions. The result is a nozzles characteristic energy intensity distribution of the particle beam. Subsequently, classical indentation fracture mechanics is used to model the interaction between incoming particles and the substrate surface. The simulation shows that the Laval-type nozzle is able to increase the particle velocity with more than 30% compared to the converging nozzle. Also the blasting profile is more uniform with a relatively flat bottom. Experimental verifications of the particle velocities using particle image velocimetry (PIV) and measurements of the roughness and the shape of the blasting profile demonstrate that the presented model is capable to predict accurately the blasting performance of both nozzles types. (c) 2005 Elsevier B.V. All rights reserved.",2005,10.1016/j.wear.2005.01.045,no
Fabrication of microstructured optical fibers - Part I: Problem formulation and numerical modeling of transient draw process,"Microstructured optical fibers (MOFs) achieve their desired performance via a pattern of holes that run along the length of the fiber. Varying the hole pattern allows a variety of optical effects to be produced. However, the original hole pattern within the preform may not be accurately transferred to the finished fiber due to the combined impact of material properties and the drawing conditions experienced during fabrication. In this two-part paper, the processes of drawing MOFs having arbitrary cross-sectional hole structures will be analyzed for the case of Newtonian materials. Part I presents a modeling formalism to describe the drawing processes, followed by a scaling analysis on a representative case, i.e., the nonisothermal drawing of an axisymmetric annular hollow fiber, to reveal the major factors influencing the drawing of both silica and polymer MOFs. By treating the primary draw process (i.e., from preform to intermediate cane) in fabricating polymer MOFs as a transient, isothermal problem, numerical simulations were carried out for an illustrative five-hole structure. The results revealed the central importance of any steep neck-down region on hole-shape deformation as well as the importance of forces additional to those associated with surface tension effects. Both experimental observations and numerical modeling show that a diversity of hole ""activities"" (both in a hole's relative size and shape) can occur when drawing MOFs. Part 11 will extend both the analysis and numerical modeling with a focus on the steady-state continuous draw process (i.e., from preform or cane to fiber). In parallel with this analysis, we also present experimental results for the drawing of polymethylmethacrylate (PMMA) MOFs.",2005,10.1109/JLT.2005.850055,no
"The development of science teachers' knowledge on models and modelling: promoting, characterizing, and understanding the process","Models play an important role in science education. However, previous research has revealed that science teachers' content knowledge, curricular knowledge, and pedagogical content knowledge about models and modelling are often incomplete or inadequate. A research project was conducted that aimed at promoting and gaining an understanding of the development of beginning science teachers' knowledge in this area. The teachers, who were enrolled in a teacher's training programme, participated in a special course on models and modelling, and conducted a research project in their classes about this theme. The data gathered in this project support the discussion to the extent that both the activities of the course and the conducting of and reflection on their research projects contributed to the development of teachers' knowledge. From the results, we propose some guidelines for educational researchers interested in this area.",2005,10.1080/0950069042000323773,no
Modeling field-scale multiple tracer injection at a low-level waste disposal site in fractured rocks: Effect of multiscale heterogeneity and source term uncertainty on conceptual understanding of mass transfer processes,"Multiple factors may affect the scale-up of laboratory multi-tracer injection into structured porous media to the field. Under transient flow conditions and with multiscale heterogeneities in the field, previous attempts to scale-up laboratory experiments have not answered definitely the questions about the governing mechanisms and the spatial extent of the influence of small-scale mass transfer processes such as matrix diffusion. The objective of this research is to investigate the effects of multiscale heterogeneity, mechanistic and site model conceptualization, and source term density effect on elucidating and interpreting tracer movement in the field. Tracer release and monitoring information previously obtained in a field campaign of multiple, conservative tracer injection under natural hydraulic gradients at a low-level waste disposal site in eastern Tennessee, United States, is used for the research. A suite of two-pore-domain, or fracture-matrix, groundwater flow and transport models are calibrated and used to conduct model parameter and prediction uncertainty analyses. These efforts are facilitated by a novel nested Latin-hypercube sampling technique. Our results verify, at field scale, a multiple-pore-domain, multiscale mechanistic conceptual model that was used previously to interpret only laboratory observations. The results also suggest that, integrated over the entire field site, mass flux rates attributable to small-scale mass transfer are comparable to that of field-scale solute transport. The uncertainty analyses show that fracture spacing is the most important model parameter and model prediction uncertainty is relatively higher at the interface between the preferred flow path and its parent bedrock. The comparisons of site conceptual models indicate that the effect of matrix diffusion may be confined to the immediate neighborhood of the preferential flow path. Finally, because the relatively large amount of tracer needed for field studies, it is likely that source term density effect may exaggerate or obscure the effect of matrix diffusion on the movement of tracers from the preferred flow path into the bedrock. (C) 2005 Elsevier B.V All rights reserved.",2005,10.1016/j.jconhyd.2004.12.002,no
Evaluation of nanofiltration processes for brackish water treatment using the DSPM model,"Nanofiltration (NF) is a membrane process particularly well Suited for the treatment of brackish waters to obtain drinking-water or reuse industrial wastewater. Efficient design of NF systems requires the simulation using mass transfer models to obtain permeate flux and component rejection. The most accurate models depend on feed composition and operating parameters of the system; unlike phenomenological models, these models are difficult to implement because of their complex mathematical structure. This paper presents an iterative solution of the Donnan-Steric Pore Model (DSPM) and its integration into an overall model to predict NF performance. The developed computer model has shown to be an effective tool to determine Suitable NF system configurations and membranes for a particular brackish water problem.",2005,,no
Nonlinear software sensor for monitoring genetic regulation processes with noise and modeling errors,"Nonlinear control techniques by means of a software sensor that are commonly used in chemical engineering could be also applied to genetic regulation processes. We provide here a realistic formulation of this procedure by introducing an additive white Gaussian noise, which is usually found in experimental data. Besides, we include model errors, meaning that we assume we do not know the nonlinear regulation function of the process. In order to illustrate this procedure, we employ the Goodwin dynamics of the concentrations [B. C. Goodwin, Temporal Oscillations in Cells (Academic, New York, 1963)] in the simple form recently applied to single gene systems and some operon cases [H. De Jong, J. Comput. Biol. 9, 67 (2002)], which involves the dynamics of the mRNA, given protein and metabolite concentrations. Further, we present results for a three gene case in coregulated sets of transcription units as they occur in prokaryotes. However, instead of considering their full dynamics, we use only the data of the metabolites and a designed software sensor. We also show, more generally, that it is possible to rebuild the complete set of nonmeasured concentrations despite the uncertainties in the regulation function or, even more, in the case of not knowing the mRNA dynamics. In addition, the rebuilding of concentrations is not affected by the perturbation due to the additive white Gaussian noise and also we managed to filter the noisy output of the biological system.",2005,10.1103/PhysRevE.72.011919,no
Distance measures for nonparametric weak process models,"Nonparametric versions of hidden Markov models, what we call weak models, are robust for process detection and easy to construct, as the assumption of knowing precise probabilities in HMMs is weakened to {0,1}-values of reachabilities. Weak models are shown to be equivalent to DFAs/NFAs. The concept of minimal unifilar weak model (mu-WM) is introduced. The spectral radius of the transition matrix of mu-WM determines the growth rate of acceptable observation sequences. An absolute weak model distance is defined for model clustering purpose, while a relative distance is a measure of how fast the performance of detection gets improved as more observations arrive. Convergence of the distance measures is proved.",2005,,no
Measuring the effectiveness of a collaborative for quality improvement in pediatric asthma care: Does implementing the chronic care model improve processes and outcomes of care?,"Objective.-To examine whether a collaborative to improve pediatric asthma care positively influenced processes and outcomes of that care. Methods.-Medical record abstractions and patient/parent interviews were used to make pre- and postintervention comparisons of patients at 9 sites that participated in the evaluation of a Breakthrough Series (BTS) collaborative for asthma care with patients at 4 matched control sites. Setting.-Thirteen primary care clinics. Patients.-Three hundred eighty-five asthmatic children who received care at an intervention clinic and 126 who received care at a control clinic (response rate = 76%). Intervention.-Three 2-day educational sessions for quality improvement teams from participating sites followed by 3 ""action"" periods over the course of a year. Results.-The overall process of asthma care improved significantly in the intervention group but remained unchanged in the control group (change in process score +13% vs 0%; P < .0001). Patients in the intervention group were more likely than patients in the control group to monitor their peak flows (70% vs 43%; P < .0001) and to have a written action plan (41% vs 22%; P = .001). Patients in the intervention group had better general health-related quality of life (scale score 80 vs 77; P = .05) and asthina-specific quality of life related to treatment problems (scale score 89 vs 85; P < .05). Conclusions.-The intervention improved some important aspects of processes of care that have previously been linked to better outcomes. Patients who received care at intervention clinics also reported higher general and asthma-specific quality of life.",2005,10.1367/A04-106R.1,no
A process based model for measuring process quality attributes,Organizations frequently use product based organizational performance models to measure the effects of information system (IS) on their organizations. This paper introduces a complementary process based approach that is founded on measuring business process quality attributes. These quality attributes are defined on the basis of ISO/IEC 9126 Software Product Quality Model. The new process quality attributes are applied in an experiment and results are discussed in the paper.,2005,,no
Model-based evaluation of oxygen consumption in a partial nitrification-Anammox biofilm process,"Previous modelling studies indicated that DO was a key factor controlling a partial nitrification Anammox biofilm process (CANON). As a design parameter, therefore, aeration becomes critical in engineering. Aeration depends on oxygen consumption (OC) taking place in the CANON system which is determined by nitrification, ordinary denitrification, Anammox and even biofilm thickness. Due to the presence of substrate gradients in a biofilm, the amount of OC is difficult to assess with experiments. For that reason, modelling technique was used here to evaluate OC with an expanded CANON model with heterotrophic growth/denitrification involved. Simulations demonstrated that different process/biofilm parameters created different curves of OC, which could help engineers or operators have a better insight into the topic of OC in process design or practical operation.",2005,,no
CFD-based modeling of the hydrodynamic production process of polymeric microfibers and an experimental evaluation,"Previously, we have developed methods to continuously fabricate polymeric microfibers via micro-scale hydrodynamic phenomena and 'on the fly' photo-polymerization. For the production of microtibers with desired diameters by regulating the sample and sheath flow rate, an appropriate numerical model is required so that the laborious trial-and-error process that characterizes the search for optimal production conditions can be minimized. In this paper, a CFD-based model of the hydrodynamic process is developed. We employed commercially available CFX-5.7 as a CFD tool. An evaluation of the established model was carried out with regard to variation of the flow conditions and the viscosity. First, the simulated sample flow patterns in the sheath flow are obtained and their qualitative and quantitative shapes are similar to the experimentally acquired ones. Second, the diameters of the microfiber under various flow conditions were measured from the CFD model and experiment. An improvement in the prediction of the fibers' diameter (40% improvement at a 2.4 &mu; l min(-1) sample flow rate compared with the analytical results) was found. The effect of other parameters such as the viscosity of the sample and sheath flow was also investigated using the CFD-based simulation model.",2005,10.1088/0960-1317/15/3/023,no
MOCVD process modeling using in-situ reflectance test structure measurements for process control improvement,Process control methodology based on in-situ reflectance measurements can be used with a process model to reduce calibration runs in the development and manufacture of InGaAlAs lasers and modulators.,2005,,no
A family of experiments to validate metrics for software process models,"Process modelling is a key activity of software process management and it is the starting point for enacting, evaluating and improving software processes. The Current competitive marketplace calls for the continuous improvement of processes and therefore. it is fundamental to have software process models with a high maintainability. In this paper we introduce a set of metrics for software process models and discuss how these can be used as maintainability indicators. In particular, we report the results of a family of experiments that assess relationships between the structural properties, as measured by the defined metrics, of the process models and their maintainability. (c) 2004 Elsevier Inc. All rights reserved.",2005,10.1016/j.jss.2004.11.007,no
Comparing and evaluating process-based ecosystem model predictions of carbon and water fluxes in major European forest biomes,"Process-based models can be classified into: (a) terrestrial biogeochemical models (TBMs), which simulate fluxes of carbon, water and nitrogen coupled within terrestrial ecosystems, and (b) dynamic global vegetation models (DGVMs), which further couple these processes interactively with changes in slow ecosystem processes depending on resource competition, establishment, growth and mortality of different vegetation types. In this study, four models - RHESSys, GOTILWA+, LPJ-GUESS and ORCHIDEE - representing both modelling approaches were compared and evaluated against benchmarks provided by eddy-covariance measurements of carbon and water fluxes at 15 forest sites within the EUROFLUX project. Overall, model-measurement agreement varied greatly among sites. Both modelling approaches have somewhat different strengths, but there was no model among those tested that universally performed well on the two variables evaluated. Small biases and errors suggest that ORCHIDEE and GOTILWA+ performed better in simulating carbon fluxes while LPJ-GUESS and RHESSys did a better job in simulating water fluxes. In general, the models can be considered as useful tools for studies of climate change impacts on carbon and water cycling in forests. However, the various sources of variation among models simulations and between models simulations and observed data described in this study place some constraints on the results and to some extent reduce their reliability. For example, at most sites in the Mediterranean region all models generally performed poorly most likely because of problems in the representation of water stress effects on both carbon uptake by photosynthesis and carbon release by heterotrophic respiration (R-h). The use of flux data as a means of assessing key processes in models of this type is an important approach to improving model performance. Our results show that the models have value but that further model development is necessary with regard to the representation of the some of the key ecosystem processes.",2005,10.1111/j.1365-2486.2005.01036.x,no
Speculative parallel processing applied to modelling of initial problems,"Purpose - To present a new parallel method for solving differential equations that describe transient states in physical systems. Design/methodology/approach - The proposed speculative method first solves a differential equation with a large integration step to determine initial data for parallel computations in sub-intervals of time, then speculatively computes in parallel solutions in all the sub-intervals with a smaller integration step and finally composes the final solution from the speculatively computed ones. The basic numerical method applied is the well-known Runge-Kutta algorithm. Findings - The speculative method allows important reduction of the computation time of sequential algorithms. The speed-up of the speculative method that we propose, as compared to the sequential execution, depends on the number of sub-intervals that are defined inside the total analysed time interval. The speed-up increases almost linearly with the number of sub-intervals. The good accuracy of computations in the presented example was obtained. Research limitations/implications - The proposed method can be applied to non-linear systems without discontinuity points and to stable systems (i.e. systems insensitive to the selection of initial conditions). Practical implications - The method can be especially applied for long-lasting computations with a slow convergence of state variables values along with the decrease of integration steps. Originality/value - The paper presents an original parallel method for solving differential equations, which significantly speeds up transient states analysis in physical systems.",2005,10.1108/03321640510571093,no
Activity diagram patterns for modeling quality constraints in business processes,"Quality management is an important aspect of business processes. Organizations must implement quality requirements, e.g., according to standards like ISO 9001. Existing approaches on business process modeling provide no explicit means to enforce such requirements. UML Activity Diagrams are a well recognized way of representing those business processes. In this paper, we present an approach for enforcing quality requirements in such business processes through the application of process quality patterns to Activity Diagrams. These patterns are defined using a pattern description language, being a light-weight extension of UML Activity Diagrams. Accordingly, such patterns can be used in forward-engineering of business processes that incorporate quality constraints right from the beginning.",2005,,no
Unified problem modeling language for knowledge engineering of complex systems-part II - Application in real-time alarm processing,"Real time applications to control industrial, medical, scientific, consumer, environmental and other processes is rapidly growing. Today such systems can be found in nuclear power stations, computer-controlled chemical plants, flight control, etc. This growth, however, has also brought to the forefront some of problems with the existing technologies. In domains like real-time alarm processing in a power system control centre existing technologies like expert systems cannot efficiently cope with. These problems have pushed for research into new techniques which could be used for solving these problems. The problems range from among other aspects, the enormous size of the power system and the fast response time constraints in emergency situations. In this paper we describe the application of the Intelligent Multi-Agent Hybrid Distributed Architecture for real-time alarm processing in a power system control centre. We show how the IMAHDA architecture is able to model the complexity and size of the power system as well as meet the desired response time constraints. Implementation of a large scale real time system like alarm processing involves realization of various objectives. These include methodology related objectives, domain related objectives, and management related objectives. This paper also describes the realization of these objectives.",2005,10.1007/s00500-004-0404-5,no
Construction and evaluation of Kansei information processing systems based on K-Model,"Recently, the report which reflects Kansei of the user in the multi-modal interface has been made. With this, we propose a K-Model which realizes efficient treatment and Kansei information processing by the dynamic system. In a K-Model, the Kansei Information processing is carried out by Multi-Agent Systems. We have applied this GM to as a Kansei information processing algorithm. In this paper, Kansei information processing systems(KIPS) have been constructed and based on the K-Model. Kansei information of the user is extracted from the optional input system. The KIPS can be define as the system. which outputs the action reflecting the result. Here, the system which calculates in real-time Kansei information from the voice as examples of the KIPS to be constructed. As an evaluation, the following were investigated : model accuracy and processing time and Kansei element discrimination rates of constructed systems. As a result, the effectiveness of a K-Model has been discussed.",2005,,no
Scatterometer sensitivity for statistical process control: Importance of modeling for in-direct measurements,"Scatterometer-based control in semiconductor processing requires careful thin-film model selection to obtain the best repeatability, reproducibility, linearity, and bias. A ""working,"" standard based on process capability is used to gather data that is then re-matched with different models for comparison. This study provides a framework for model comparisons and examines the effects of the index of refraction and model complexity, or the number of trapezoids, on measurement system capability for control of a polysilicon gate etch process. Measurement capability for the gate profile is needed in addition to gate length. Results from a single-wavelength, multiple angle-of-incidence scatterometer are compared to reference data from atomic force and transmission electron microscopy. The polysilicon index of refraction has a strong effect on measurement scaling and repeatability. A counter-intuitive result is that a single-trapezoid model can perform better than a double-trapezoid model for linearity and bias.",2005,,no
Systematic measurements of proton- and alpha-capture cross sections relevant to the modelling of the p process.,"Several in-beam cross section measurements of proton- as well as a-capture reactions in the Se-Sb region have been carried out to obtain global input parameters for Hauser-Feshbach (HF) calculations. In total, 20 (p,gamma) and 7 (alpha, gamma) reactions were measured. We compare some of these results with Hauser-Feshbach calculations using various optical model potentials and nuclear level densities.",2005,10.1016/j.nuclphysa.2005.05.092,no
Design and evaluation of an educational software process simulation environment and associated model,"Simulation is an educational tool that is commonly used to teach processes that are infeasible to practice in the real world. Software process education is a domain that has not yet taken full advantage of the benefits of simulation. To address this, we have developed SimSE, an educational, interactive, graphical environment for building and simulating software engineering processes in a game-like setting. We detail the design of SimSE, present an initial simulation model of a waterfall process that we developed, and describe an experiment that we conducted to evaluate the educational potential of SimSE and its initial model.",2005,10.1109/CSEET.2005.16,no
Defect charge states for classical modeling of diffusion processes in insulators,"Some point defect properties can be accurately modeled by describing the ions classically, in terms of the shell model. This is particularly the case for point-defect diffusion in strongly ionic crystals. First, we discuss the issue of what ionic charge should be attributed to shell-model ions, including the possibility of partly covalent materials. We then discuss the issue of what defect charge states are likely to be of experimental interest and at the same time amenable to classical modeling. Finally, we discuss the rather common case where the defect charge is not well localized at a single ionic site, and where the electronic charge distribution of the defect and its neighbors will not remain fixed throughout a diffusion step.",2005,10.1080/10420150500429448,no
Application of Gaussian error propagation principles for theoretical assessment of model uncertainty in simulated soil processes caused by thermal and hydraulic parameters,"Statistical uncertainty in soil temperature and volumetric water content and related moisture and heat fluxes predicted by a state-of-the-art soil module [ embedded in a numerical weather prediction (NWP) model] is analyzed by Gaussian error-propagation (GEP) principles. This kind of uncertainty results from the indispensable use of empirical soil parameters. Since for the same thermodynamic and hydrological surface forcing and mean empirical parameters a soil module always provides the same mean value and standard deviation, uncertainty is first theoretically analyzed using artificial data for a wide range of soil conditions. Second, NWP results obtained for Alaska during a July episode are elucidated in relation to the authors' theoretical findings. It is shown that uncertainty in predicted soil temperature and volumetric water content is of minor importance except during phase transition. Then the freeze-thaw term dominates and leads to soil temperature and moisture uncertainties of more than 15.8 K and 0.212 m(3) m(-3) in mineral soils. Heat-flux uncertainty is of the same order of magnitude as typical errors in soil-heat-flux measurements. Uncertainty in the pore-size distribution index dominates uncertainty for all state variables and soil fluxes under most conditions. Uncertainties in hydraulic parameters ( saturated hydraulic conductivity, pore-size distribution index, porosity, saturated water potential) affect soil-temperature uncertainty more than those in thermal parameters ( density and specific heat capacity of dry soil material). Based on a thermal conductivity approach alternatively used, it is demonstrated that GEP principles are indispensable for evaluating parameterized soil-transfer processes. Generally, statistical uncertainty decreases with depth. Close beneath the surface, the uncertainty in predicted soil temperature, volumetric water content, and soil-moisture and heat fluxes undergoes a diurnal cycle.",2005,10.1175/JHM455.1,no
Quality-reliability chain modeling for system-reliability analysis of complex manufacturing processes,"System reliability of a manufacturing process should address effects of both the manufacturing system (MS) component reliability, and the product quality. In a multi-station manufacturing process (MMP), the degradation of MS components at an upstream station can cause the deterioration of the downstream product quality. At the same time, the system component reliability can be affected by the deterioration of the incoming product quality of upstream stations. This kind of quality & reliability interaction characteristics can be observed in many manufacturing processes such as machining, assembly, and stamping. However, there is no available model to describe this complex relationship between product quality, and MS component reliability. This paper, considering the unique complex characteristics of MMP, proposes a new concept of quality & reliability chain (QR-Chain) effect to describe the complex propagation relationship of the interaction between MS component reliability, and product quality across all stations. Based on this, a general QR-chain model for MMP is proposed to integrate the product quality with the MS component reliability information for system reliability analysis. For evaluation of system reliability, both the exact analytic solution, and a simpler upper bound solution are provided. The upper bound is proved to be equal to the exact solution if the product quality does not have self-improvement, which is generally true in many MMP Therefore, the developed QR-chain model, and its upper bound solution can be applied to many MMP.",2005,10.1109/TR.2005.0853441,no
Methodology for supply chain system modeling and process evaluation,"The aim of this paper is to present a modeling methodology in order to evaluate financial and physical performance in Supply Chain domain. This methodology is applied to a real industrial case study in a Supply Chain company. In this case study, we compare two production strategies, a push and pull strategy by combining heuristic and simulation.",2005,,no
The Bayesian statistical model for process capability evaluation,"The classic process capability indices have a flaw that there is no assessment on error distributions. However, the distributions of these estimates ate usually so complicated that it is difficult to obtain good interval estimates. In this paper we adopt a Bayesian approach to obtain an interval estimation, particularly for the index C(pm). The posterior probability, i.e., the process under investigation is capable, is derived; then the credible interval, a Bayesian analogue of the classical confidence interval, is also obtained. To make this Bayesian procedure convenient for practitioners to implement on manufacturing processes, we tabulate the minimum values of C(pm).",2005,,no
The computational implementation of the landscape model: Modeling inferential processes and memory representations of text comprehension,"The complexity of text comprehension demands a computational approach to describe the cognitive processes involved. In this article, we present the computational implementation of the landscape model of reading. This model captures both on-line comprehension processes during reading and the off-line memory representation after reading is completed, incorporating both memory-based and coherence-based mechanisms of comprehension. The overall architecture and specific parameters of the program are described, and a running example is provided. Several studies comparing computational and behavioral data indicate that the implemented model is able to account for cycle-by-cycle comprehension processes and memory for a variety of text types and reading situations.",2005,10.3758/BF03192695,no
Processes and problems in the formative evaluation of an interface to the foundational model of anatomy knowledge base,"The Digital Anatomist Foundational Model of Anatomy (FMA) is a large semantic network of more than 100,000 terms that refer to the anatomical entities, which together with 1.6 million structural relationships symbolically represent the physical organization of the human body. Evaluation of such a large knowledge base by domain experts is challenging because of the sheer size of the resource and the need to evaluate not just classes but also relationships. To meet this challenge, the authors have developed a relation-centric query interface, called Emily, that is able to query, the entire range of classes and relationships in the FMA, yet is simple to use by a domain expert. Formative evaluation of this interface considered the ability of Emily to formulate queries based on standard anatomy examination questions, as well as the processing speed of the query engine. Results show that Emily is able to express 90% of the examination questions submitted to it and that processing time is generally 1 second or less, but can be much longer for complex queries. These results suggest that Emily will be a very useful tool, not only for evaluating the FMA, but also for querying and evaluating other large semantic networks.",2005,10.1197/jamia.M1401,no
Inverse modeling of oxid deposition using measurements of a TEOS CVD process,The goal of this paper is to identify simulation models for the deposition of silicon dioxide layers from TEOS (Tetraethoxysilane) in a CVD (Chemical Vapor Deposition) process and to calibrate the parameters of these models by comparing simulation results to SEM (Scanning Electron Microscope) images of deposited layers in trenches with different aspect ratios. We describe the three models used and the parameters which lead to the best results for each model which allows us to draw conclusions on the usefulness of the models.,2005,,no
Redintegration and the benefits of long-term knowledge in verbal short-term memory: An evaluation of Schweickert's (1993) multinomial processing tree model,"The impact of four long-term knowledge variables on serial recall accuracy was investigated. Serial recall was tested for high and low frequency words and high and low phonotactic frequency nonwords in 2 groups: monolingual English speakers and French-English bilinguals. For both groups the recall advantage for words over nonwords reflected more fully correct recalls with fewer recall attempts that consisted of fragments of the target memory items (one or two of the three target phonemes recalled correctly); completely incorrect recalls were equivalent for the 2 list types. However, word frequency (for both groups), nonword phonotactic frequency (for the monolingual group), and language familiarity all influenced the proportions of completely incorrect recalls that were made. These results are not consistent with the view that long-term knowledge influences on immediate recall accuracy can be exclusively attributed to a redintegration process of the type specified in Schweickert's (1993) multinomial processing tree model of immediate recall. The finding of a differential influence on completely incorrect recalls of these four long-term knowledge variables suggests instead that the beneficial effects of long-term knowledge on short-term recall accuracy are mediated by more than one mechanism. (C) 2004 Elsevier Inc. All rights reserved.",2005,10.1016/j.cogpsych.2004.07.001,no
Simultaneous modelling of process variables and raw material properties as measured by NIR. A case study from cellulose production,"The main goal of the present paper was to investigate the potential of near infrared spectroscopy (NIR) to be used in modelling of pulp properties in the paper industry. An experimental design based on a split plot structure was used for generating the data. The factors considered were cooking recipe, cooking time and chips quality and the response was Kappa No. in sulphite pulp. NIR spectra were measured on the chip samples, and transformed to principal components. The scores from these components were used in an ANOVA model together with the other design variables. The first step in the modelling work was to establish a benchmark model, which included only chips category, and not the NIR spectra themselves. Then the scores from the principal component analysis were included in the model. One principal component was found to be significant for the prediction of Kappa No. Prior to the model building process, a thorough investigation of the principal component analysis was performed, including a discriminant analysis of the scores. The main conclusions from this work are that it is possible to categorize chips according to scores on corresponding NIR spectra, and to replace chips category with these scores in ANOVA models for Kappa No. in sulphite pulp. (c) 2005 Published by Elsevier B.V",2005,10.1016/j.chemolab.2005.04.002,no
"An integrated model-based approach for evaluating and improving safety instruments in continuous process from the viewpoints of maintainability, reliability and availability - Case study: Offshore industry","The objective of this paper is to present a framework for assessing and improving safety equipment items performance based on maintainability, reliability and availability. The main idea is to employ Principal Component Analysis (PCA) and Importance Analysis (IA) to provide insight on safety equipment items performance. The validity of the model is verified and validated by Data Envelopment Analysis (DEA). Furthermore, a non-parametric correlation method, namely, Spearman correlation experiment shows a high level of correlation between the findings of PCA and DEA. At first PCA is used for assessing the performance of safety equipment items and ranking them. IA is then performed for the worst safety equipment item which could have most impact on the overall system effectiveness to classify their components based on the Component Criticality Measures (CCM). The analysis of the classified components can ferret out the leading causes and common-cause events to pave a way toward decreasing failure interdependency and magnitude of incidents which ultimately maximize overall operational effectiveness.",2005,10.1252/jcej.38.436,no
Dynamic measurement and mathematical modeling of the temperature history on hot dog surfaces during vacuum-steam-vacuum processes,"The objective of this study was to develop an instrumentation system to measure the surface temperature of hot dogs during VSV processes. Results indicated that the pressure in the treatment chamber responded immediately and accurately to the events of VSV. The surface temperature history, however, followed an exponential trend after saturated steam was flushed into the treatment chamber. A mathematical model was developed to simulate the surface temperature history during steam pasteurization processes. According to the model, a 5-log reduction in L. innocua inoculated onto the surface of hot dogs could be achieved using 110 degrees C steam for 0.1 s, provided that the surface was perfectly smooth and bacteria were all distributed on the surface. However, bacteria still survived the VSV treatment even when higher temperatures were used. The incomplete destruction of bacteria on hot dog surfaces using current VSV processes may be due to the fact that the pores are filled with water and heat must penetrate into a certain depth under the surface of hot dogs in order to eliminate L. monocytogenes. This study suggested using a single long steam treatment cycle, instead of multiple short VSV cycles, for a complete destruction of bacteria hidden beneath the surface of ready-to-eat solid foods. Published by Elsevier Ltd.",2005,10.1016/j.jfoodeng.2004.09.030,no
Multilevel phenomenological modelling approach to support the evaluation and generation of intensified processes,"The paper presents a first approach for the systematisation of the fundamentals contained in the PI developments to support their implementation into new process designs. An initial methodology based on a multilevel analysis is proposed, starting with the identification and integration of the main tasks (process stages) by the material involved, functions to be delivered and appropriate process conditions. A more detailed level deals with the refinement of each task using a phenomenological modelling approach which aids in the construction and analysis of models whose ultimate purpose is providing the insights in the process performance and understanding on how cooperative phenomena can be manipulated for the accomplishment of the design strategies that promote the intensification of processes. Within the fundamental of the proposed phenomenological modelling approach that guide the model structuring, the process tasks are decornposed into the relevant phases and physicochemical phenomena involved, identifying the connections and influences amongst them and evaluating the individual rates. The intensification of rate-limiting phenomena is finally achieved by diverse strategies derived from PI principles, dealing with the manipulation of driving forces.",2005,,no
Application of microstructural modelling for quality control and process improvement in hot rolled steels,"The prediction of microstructures and mechanical properties is an important point for the control of steel properties and quality. The present paper describes an integrated model developed at IRSID, Arcelor group, to predict the microstructure and mechanical properties of hot rolled steel, including the microstructure of C-Mn and C-Mn-Nb steels, the strength, the cooling curve affected by heat evolution due to transformation and precipitation strengthening. Online and offline applications of this model are presented. The practical advantages of the model are numerous, including help in designing and optimising metallurgical routes or participation in plant quality control systems.",2005,10.1179/174328105X45893,no
A New Method For Measuring Software Processes Within Software Capability Maturity Model Based On the Fuzzy Multi-Agent Measurements,"The present article discusses and presents a new comprehensive approach aimed at measuring the maturity and quality of software processes. This method has been designed on the basis of Software Capability Maturity Model (SW-CMM)(1) and the Multi-level Fuzzy Inference Model and is used as a measurement and analysis tool. Among the most important characteristics of this method one can mention simple usage, accuracy, quantitative measures and comparability. Fuzzy logic-based tools are designed to provide such functions.",2005,,no
Modeling in-network processing and aggregation in sensor networks: Algorithms and evaluation,"The rapid advances in processor, memory and radio technology have enabled the development of distributed networks of sensor nodes capable of sensing and communicating using wireless media. The basic operation in sensor networks is the systematic gathering and transmission of sensed data to the end-user. The severe energy constraints and limited computing capabilities of the sensors present major challenges to its design. In this paper, we propose two new protocols for in-network processing and data aggregation, DEEPADS (Distributed Energy-efficient Protocol for Aggregation of Data in Sensor Networks) and C-DEEPADS (Clustered-DEEPADS) that maximize the lifetime of the sensor network. Simulation results show that our protocols perform better than the existing approaches: Directed diffusion[1], LEACH[3], PEDAP[5] and PEDAP-PA[5]. The two-tier hierarchical approach C-DEEPADS is optimal in terms of maximizing the system lifetime as well as reducing the end-to-end latency.",2005,,no
Measurements and semi-empirical model describing the onset of powder formation as a function of process parameters in an RF silane-hydrogen discharge,"The transition pressure above which powder formation takes place was experimentally determined in a parallel plate RF silane-hydrogen plasma as a function of the process parameters-power, temperature, gas flow and hydrogen dilution-using the dc-bias voltage as powder formation indicator. The resulting empirical scaling law describes in what conditions powders are formed and in what conditions the plasma is powder-free. Second, a semi-empirical model was developed that treats the nano-particle density in the plasma. This model was applied to analytically describe the transition pressure above which nano-particle coagulation takes place as a function of process parameters. The resulting modelled scaling law shows good correspondence with the experimentally found scaling law. Finally, a series of amorphous silicon films was deposited. The reflection-transmission spectra of the films were measured and modelled through Tauc-Lorentz formalism. The optical analysis shows that at around the plasma transition pressure there occurs also a transition in the properties of the deposited material.",2005,10.1088/0022-3727/38/14/013,no
Fault location on shielded cables: Electromagnetic modelling and improved measurement data processing,"This paper analyses the performance of the time domain reflectometry (TDR) technique when it is applied to the location of faults on cables. Different types of defects, which can arise on coaxial cables, are modelled and their reflection coefficients are calculated. The results, validated by experimental data, are compared to the typical noise that affects this kind of measurement to assess the sensitivity of the technique. Shield damage to a shielded twisted pair and a shielded multiwire cable is also examined: these examples represent more critical situations because the effect of cable and mismatching losses limits the dynamic range of the measurement. In both cases, the TDR allows us to locate faults that strongly affect the propagation of the signal on the transmission line, but the reflections produced by small defects are often masked by the noise. A data processing technique based on the comparison with a reference measurement and the use of the statistical correlation are then implemented. In this way, the reflections produced by the faults are much more visible and an overall enhancement of the technique sensitivity is achieved.",2005,10.1049/ip-smt:20045035,no
Influence of model validation on proper selection of process models - an industrial case study,"This paper considers the design and validation of a model of an industrial batch process in TiO2 production. The model will be used for flexible recipe control, which is a model-based approach used in control and optimisation of batch processes. Because of insufficient knowledge and a lack of proper data, different process models were developed: a semi-empirical dynamic model based on chemical kinetics laws, and several experimental black-box models. In the paper the models are validated and mutually compared. Validation of models has shown that besides comparing the model and the process output behaviour, additional measures considering also the model input error should be introduced for proper model validation related to the model use. In our case, introducing additional measures also contributed to improvement of the model design procedure, so that a simple yet satisfactory black-box model was obtained despite a small amount of process data. (c) 2004 Elsevier Ltd. All rights reserved.",2005,10.1016/j.compchemeng.2004.11.013,no
Numerical modeling of a discontinuous incineration process with on-line validation,"This paper describes the dynamic model of the hot section of an incineration plant with steam production. The model is based on a first-principles approach that starting from material, energy, and momentum balances leads to the integration of a differential-algebraic system of 132 equations. The hot section of the plant comprises a furnace, a postcombustion chamber, and a waste heat boiler. The reciprocating grate kiln represents one of the most important features of the process, where the heterogeneous combustion of waste takes place. The waste, coming from diverse collections, is transferred from the pit into a vertical hopper and enters into the furnace pushed by a discontinuous feeding grate. Also the burning waste is discontinuously moved through the primary combustion chamber by reciprocating grates. These discontinuous features are mainly responsible for the high oscillations of process variables. The model was implemented for control purposes with the aim of reducing oscillations and offsets through a model-based control system. The paper describes thoroughly the key points that were addressed in structuring and solving the numerical detailed model. Finally, a validation procedure sketches a quantitative comparison between experimental measured data and simulated values. The measured process variables were acquired on-line from the real-time database running on the distributed control system.",2005,10.1021/ie0495473,no
"Evaluating a 16-bit YCBCR (6 : 5 : 5) color representation for low memory, embedded video processing","This paper examines a 16-bit YCbCr (6:5:5) color representation for limited-memory, embedded video-processing applications. The 16-bit format, combined with YCbCr 4:2:0 video streams, reduces storage requirements by 29% over a 4:2:0 chroma format and 65% over a baseline 24-bit YCbCr format while providing satisfactory image quality and sufficient information for motion estimation.",2005,10.1109/ICCE.2005.1429777,no
A fault diagnosis method for polymeric reaction process based on soft measuring hybrid model,"This paper gives environmental/economic load dispatch model which considers cost and emission function coefficients with uncertainties and the constraints of ramp rate. The uncertainties are represented by fuzzy numbers, and the model is known as fuzzy dynamic environmental/economic load dispatch model (FDEELD). A novel weighted ideal point method (WIM) is proposed to solve the FDEELD problem. The WIM convert the FDEELD problem into the single objective fuzzy nonlinear programming problem. Hybrid evolutionary algorithm with quasi-simplex techniques is used to seek optimal solution of the single objective problem corresponding to the FDKKLD; fuzzy number ranking method is applied to compare fuzzy weighted objective function. The experimental results shows that FDEELD is more practical model; the algorithm and techniques propose are efficient to solve the FDEELD problem.",2005,,no
Economic statistical process control for multivariate quality characteristics under Weibull shock model,This paper is an extension of research conducted by Banerjee and Rahim (Technometrics 29(6) (1988) 404). Their general approach is now applied to a multivariate control chart instead of a univariate control chart. A cost model for the economic statistical design of a Hotelling T-2 control chart is derived to deal with situations involving a Weibull shock model with an increasing failure rate. Application of the proposed T-2 control chart design is demonstrated through a numerical example. A sensitivity analysis of the model is performed. (c) 2005 Elsevier B.V. All rights reserved.,2005,10.1016/j.ijpe.2004.05.014,no
Improved identification of error-in-variable models for industrial process,"This paper is concerned with the identification of linear parametric models of multivariate processes using Total Least Squares (TLS). Since the recorded reference data of the industrial process such as in coal fired power plants are usually corrupted by measurement uncertainty, this identification represents an Error-in-Variable (EIV) problem. TLS is known to address the EIV problem but fails to estimate the covariance matrix of the variable set that is used to predict the process output variables. In this work, a novel TLS technique is introduced, which (i) provides a consistent estimation of this matrix and (ii) is computationally more efficient than conventional TLS algorithms, and thus could be used for online data measurement and processing. These benefits were illustrated using a simulated SISO example and by application to in two application studies that relate to the simulation of a single-input single-output first order lag and recorded data from a power plant boiler process.",2005,,no
A fault diagnosis method for polymeric reaction process based on soft measuring hybrid model,"This paper presented a fault diagnostic method for polymeric reaction process by means of the technique of adopted fuzzy pattern recognition. Based on soft measuring hybrid model, a threshold value principle and maximum membership degree principle are combined to diagnose faults. The fault diagnostic method is used for a typical polymeric reaction productive process-Polyacrylonitrile productive process, and it is proved that it can not only get accurate diagnosis results but also rectify the output of the hybrid model with the help of the information from morbid symptom set.",2005,,no
Ontological support in modeling learners' problem solving process,"This paper presents a new model for simulating procedural knowledge in the problem solving process with our ontological system, InfoMap. The method divides procedural knowledge into two parts: process control and action performer. By adopting InfoMap, we hope to help teachers construct curricula (declarative knowledge) and teaching strategies by capturing students' problem-solving processes (procedural knowledge) dynamically. Using the concept of declarative and procedural knowledge in intelligent tutoring systems, we can accumulate and duplicate the knowledge of the curriculum manager and student.",2005,,no
Converting measurement data to process knowledge by using three-dimensional CFB furnace model,"This paper presents a practice of how a three-dimensional circulating fluidized bed furnace model is used to convert scarce profile measurement data to a detailed description of the furnace process. Conventional test balance measurements define the overall material and energy balance but provide little information of the combustion process inside the furnace. More information has been obtained by extensive profile measurements of pressure, temperature and gas composition. In large-scale commercial units, the dimensions are such that profile measurements can encompass only a small proportion of the whole furnace. The measured profiles have been utilized in the validation and fine-tuning of a three-dimensional model, which has been applied to determine the process variables at the rest of the furnace. By validating the model parameters and submodels with the available measurement data, an accurate description of the whole furnace process has been achieved. A valid model has then been used to study and optimize the performance of the boiler. In addition, when the validation process is repeated for a variety of boilers, the understanding of the combustion phenomena is improved in the form of more accurate and generic submodels and correlations. This improves the prediction capability of the model, consequently, the three-dimensional model can also effectively be used to support design and analysis of new units.",2005,,no
Performance evaluation of a parallel pipeline computational model for space-time adaptive processing,"This paper presents further results on the design and implementation of various optimizations based on our earlier work of developing a parallel pipelined model for the computational intensive applications that have multiple processing tasks. Performance evaluation of this model was done by using a real-time airborne radar application that employs a Space-Time Adaptive Processing (STAP) algorithm. This paper focuses on the following four issues: (1) The tradeoffs between increasing the throughput and reducing the latency are examined in more detail when allocating processors among different processing tasks. (2) A multi-threaded design is incorporated into the pipeline model and implemented on a massively parallel computer with symmetric multi-processor nodes, which shows enhanced performance. (3) The disk I/O is incorporated into the parallel pipeline to study its effect on performance in which two I/O task designs have been implemented: embedding I/O in the pipeline or having a separate I/O task. By using a double buffering approach together with the asynchronous I/O, the overall pipeline performance scales well as the number of processors increases. (4 From the comparison of the two I/O implementations, it is discovered that the latency may be improved when merging multiple tasks into a single task. The effect of reorganizing the task structure of the pipeline is discussed in detail. All the performance results shown in this work demonstrate the linear scalability the parallel pipeline model can achieve using a production radar application. Although this paper focuses on the implementation of the parallel pipeline model and uses the results from a STAP application to Support the claims of the discovered properties for this pipeline, this model is also applicable to many other types of applications with similar computational characteristics.",2005,10.1007/s11227-005-0039-z,no
Managing quality in continuous casting process using product quality model and simulated annealing,"This paper proposes an approach using quality loss functions and multicriteria optimization procedure to determine appropriate process parameter values for producing quality products in a continuous casting system. Quality of a continuously cast product depends on the process conditions during the casting process. From the information available in the literature and industry, 17 critical quality conditions have been identified which have to be satisfied to prevent defect formation during casting. Each of these process conditions are connected to one or more than one process parameters and the process parameter values are to be selected such that all these 17 quality conditions are satisfied simultaneously. The optimization procedure suggested in the present study involves representing the relationship between the responses and the process variables in the form of an objective function and finding the best set of values for the process variables. The objective function is formulated as a loss function, which establishes a functional relationship between the input variables or process parameters and the quality criteria. The optimum values of the process parameters were obtained by minimizing the loss function using simulated annealing optimization algorithm. The details are presented in this paper with the help of a specific case. (c) 2004 Elsevier B.V. All rights reserved.",2005,10.1016/j.jmatprotec.2004.09.073,no
Integrated data processing for PSP/TSP/(SF)-F-3 and model deformation measurements,"This paper reviews data processing algorithms used for data registration with Pressure Sensitive Paint (PSP), Shear and Stress Sensitive Film (S3F) and model deformation estimation. Accuracy of the registration algorithm was experimentally estimated in production environment. The paper also includes results of the simultaneous PSP measurement and model deformation estimation of UCAV model and comparison of pressure fields recovered using PSP and S3F technique on the half delta wing model.",2005,10.1109/ICIASF.2005.1569914,no
Design process error-proofing: Benchmarking gate and phased review life-cycle models,"This report describes a number of product development life-cycle approaches theorized academically and observed in industry. The design process is an important part of the product life-cycle as decisions made then have the greatest impact on product quality and cost. Process formalization and improvement are a necessary step in enabling product development robustness and error-proofing. Process models such as gate approaches or phased review processes are used to structure and focus the process. Through benchmarking organizations such as General Electric, ABB, and NASA, comparisons on the focus and effectiveness of various methods can be made. The paper concludes with lessons on the implementation of such process models and future research to better implement and improve them.",2005,,no
Structural model of gelation processes of a sodium silicate sol destabilized by calcium ions: combination of SAXS and rheological measurements,"This study deals with the physico-chemical processes involved in the formation of basic fractal silica gels derived from a sodium silicate sol destabilized by calcium ions. Using small-angle X-ray scattering and dynamic rheological measurements, structural and viscoelastic properties have been investigated in situ during aggregation and gelation processes. The experimental results lead to a consistent model that describes the structural features and aggregation mechanisms involved in the formation of these gets. (C) 2004 Elsevier B.V. All rights reserved.",2005,10.1016/j.jnoncrysol.2004.11.019,no
A family process model of problem behaviors in adolescents,"This study examines the ways in which different family processes and personal experiences of social contexts are related to the adjustment of adolescents in a subsample of 755 mother-child dyads drawn from the National Survey of Families and Households. Structural equation modeling was employed to examine a model in which joint family contexts (socioeconomic resources), mothers' and adolescents' experiences of outside-family contexts (perceived social network quality and experience of school stress, respectively), and individual characteristics of mothers (distress) were expected to relate to adolescents' externalizing and internalizing behaviors through their association with within-family contexts (mother-adolescent conflict, family warmth). This conceptual model was supported by the data. Pathways were consistent for boys and girls.",2005,10.1111/j.0022-2445.2005.00008.x,no
"An integrated model of product, production process and quality information for failure prediction","This study proposes a methodology for predicting product failures occurring due to known failures and causalities. The prediction is based on failure propagation in an integrated model of a product, production process, and quality information (PPQ). This methodology enables a designer to predict new failures. One of the concepts for failure prediction is the PPQ for compiling referring to quality-related issues. The other is an analysis methodology of the failure propagation for predicting new PPQ-based failures. The failure propagation indicates the possible failures from the production process and their causes. Three critical issues for system implementation are discussed in detail: 1) integrated PPQ model; 2) methodology for knowledge reuse by developing a PPQ-based library, and 3) an analysis methodology for failure prediction. The prototype system of the PPQ model is implemented for failure prediction in the example of a production system of Auto-Breaker.",2005,,no
Triangulation error resistive modelling and 3D shape processing,"This study proposes a topologically robust and ranging error resistive approximate modelling procedure suitable for quantitative 3D shape processing. A geometric model with desired matrix-format meshing is directly reconstructed based on a solid modelling approach The radial distance of each scanning point from the axis of the cylindrical coordinates is measured using a laser triangulation sensor. The angular and vertical positions of the laser beam are two other coordinate values of the modelling. A face (mesh) array listing (topology), which defines the sampling point connectivity and the shape of the mesh, is assigned to meet the desired sampling. The topologically stable organized meshing, and hence, an accurate approximation is then accomplished. It is free from the noise-originated misconnection and shape ambiguity, which is unavoidable in the widely used ICP (Iterative Closest Point) modelling. This proposal allows a simple, practical and versatile 3D shape processing, and modification, for instance, for cultural heritage retrieval and virtual training.",2005,,no
Intercomparison of the community multiscale air quality model and CALGRID using process analysis,"This study was designed to examine the similarities and differences between two advanced photochemical air quality modeling systems: EPA Models-3/CMAQ and CALGRID/CALMET. Both modeling systems were applied to an ozone episode that occurred along the 1-5 urban corridor in western Washington and Oregon during July 11-14, 1996. Both models employed the same modeling domain and used the same detailed gridded emission inventory. The CMAQ model was run using both the CB-IV and RADM2 chemical mechanisms, while CALGRID was used with the SAPRC-97 chemical mechanism. Output from the Mesoscale Meteorological Model (MM5) employed with observational nudging was used in both models. The two modeling systems, representing three chemical mechanisms and two sets of meteorological inputs, were evaluated in terms of statistical performance measures for both 1- and 8-h average observed ozone concentrations. The results showed that the different versions of the systems were more similar than different, and all versions performed well in the Portland region and downwind of Seattle but performed poorly in the more rural region north of Seattle. Improving the meteorological input into the CALGRID/CALMET system with planetary boundary layer (PBL) parameters from the Models-3/CMAG meteorology preprocessor (MCIP) improved the performance of the CALGRID/CALMET system. The 8-h ensemble case was often the best performer of all the cases indicating that the models perform better over longer analysis periods. The 1-h ensemble case, derived from all runs, was not necessarily an improvement over the five individual cases, but the standard deviation about the mean provided a measure of overall modeling uncertainty. Process analysis was applied to examine the contribution of the individual processes to the species conservation equation. The process analysis results indicated that the two modeling systems arrive at similar solutions by very different means. Transport rates are faster and exhibit greater fluctuations in the CMAQ. cases than in the CALGRID cases, which lead to different placement of the urban ozone plumes. The CALGRID cases, which rely on the SAPRC97 chemical mechanism, exhibited a greater diurnal production/loss cycle of ozone concentrations per hour compared to either the RADM2 or CBIV chemical mechanisms in the CMAQ cases. These results demonstrate the need for specialized process field measurements to confirm whether we are modeling ozone with valid processes.",2005,10.1021/es048403c,no
Models and processes for the evaluation of off-the-shelf components - MPEC'05,"This workshop summary presents an overview of the one-day International Workshop on Models and Processes for the Evaluation of off-the-shelf Components (MPEC'05), held in conjunction with the 27th International Conference on Software Engineering (ICSE'05). Details about MPEC'05 may be found at http://www.lsi.upc.edu/events/mpec/.",2005,,no
"Modeling, evaluation and control of a road image processing chain","Tuning a complete image processing chain (IPC) remains a tricky step. Until now researchers focused on the evaluation of single algorithms, based on a small number of test images and ad hoc tuning independent of input data. In this paper we explain how, by combining statistical modeling with design of experiments, numerical optimization and neural learning, it is possible to elaborate a powerful and adaptive IPC. To succeed, it is necessary to build a large image database, to describe input images and finally to evaluate the IPC output. By testing this approach on an IPC dedicated to road obstacle detection, we demonstrate that this experimental methodology and software architecture ensure a steady efficiency. The reason is simple: the IPC is globally optimized, from a large number of real images (180 out of a sequence of 30 000) and with adaptive processing of input data.",2005,,no
The relevance of the social information processing model for understanding relational aggression in girls,"Two studies examined whether social information-processing variables predict relational aggression in girls. In Study 1, fourth-through sixth-grade girls reported their intent attributions, social goals, outcome expectancies for relational aggression, and the likelihood that they would choose a relationally aggressive response in response to vignettes depicting ambiguous relational provocation situations. In Study 2, girls reported their intent attributions, evaluations of relational aggression, and likelihood of choosing a relationally aggressive response in response to ambiguous relational provocation vignettes. Social-cognitive variables failed to relate significantly to peer nominations of relational aggression in predicted ways in either study, challenging the relevance of the social information-processing model for girls.",2005,10.1353/mpq.2005.0010,no
Mathematical modeling and optimization of two-layer sintering process for sinter quality and fuel efficiency using genetic algorithm,"Two-layer sintering by charging a green sinter mix with normal coke rate in the upper layer and reduced coke rate in the lower layer can substantially reduce the coke rate and improve the sinter quality by producing more uniform thermal profile throughout the bed height. The two-layer sintering process has been analyzed by numerical simulation using a detailed CFD-based model, considering all the important phenomena (i.e., gas-solid reaction, melting and solidification, flow through porous bed, heat, and mass transfer etc.). A genetic algorithm optimization technique is then applied to evaluate the optimum coke rate in the two layers of the bed to produce the ideal thermal profile and melting fraction in the sinter bed for optimum sinter quality. By this optimization method a high-quality sinter with minimum return fines can be achieved along with reduced coke rate. Application of genetic algorithm for this type of process optimization has several advantages over traditional optimization techniques, because it can identify the global optimum condition and perform multiobjective optimization very easily for a complex industrial process such as iron ore sintering.",2005,10.1081/AMP-200053418,no
Combining data mining tools with health care models for improved understanding of health processes and resource utilisation,"Variability and uncertainty are inherent characteristics of most health care processes. Patient pathways and dwelling times even within the same process typically vary from patient to patient, such as the flow of patients through a particular health care provider or patient progression through the natural history of a given disease. The challenge for the OR modeller is to adequately handle and capture the stochastic features within developed models. This paper will discuss the benefits of combining patient classification tools (data mining techniques) with developed OR models, such as simulation tools, to more accurately capture patient outcomes, risks and resource needs. Illustrative applications will demonstrate the approach.",2005,,no
Multi-operational machining processes modeling for sequential root cause identification and measurement reduction,"Variation propagation modeling has been proved to be an effective way for variation reduction and design synthesis in multi-operational manufacturing processes. However previously developed approaches for machining processes did not directly model the process physics regarding how fixture, and datum, and machine tool errors generate the same pattern on part features. Consequently, it is difficult to distinguish error sources at each operation. This paper formulates the variation propagation model using the proposed equivalent fixture error concept. With this concept, datum error and machine tool error are transformed to equivalent fixture locator errors at each operation. As a result, error sources can be grouped and root cause identification can be conducted in a sequential manner The case studies demonstrate the model validity through a real cutting experiment and model advantage in measurement reduction for root cause identification.",2005,10.1115/1.1948403,no
The density process of the minimal entropy martingale measure in a stochastic volatility model with jumps,"We derive the density process of the minimal entropy martingale measure in the stochastic volatility model proposed by Barndorff-Nielsen and Shephard [2]. The density is represented by the logarithm of the value function for an investor with exponential utility and no claim issued, and a Feynman-Kac representation of this function is provided. The dynamics of the processes determining the price and volatility are explicitly given under the minimal entropy martingale measure, and we derive a Black & Scholes equation with integral term for the price dynamics of derivatives. It turns out that the minimal entropy price of a derivative is given by the solution of a coupled system of two integro-partial differential equations.",2005,10.1007/s00780-005-0161-z,no
Estimation of the PCR efficiency based on a size-dependent modelling of the amplification process,"We propose a stochastic modelling of the PCR amplification process by a size-dependent branching process starting as a supercritical Bienayme-Galton-Watson transient phase and then having a saturation near-critical size-dependent phase. This model based on the concept of saturation allows one to estimate the probability of replication of a DNA molecule at each cycle of a single PCR trajectory with a very good accuracy. To cite this article: N. Lalam et al., C. R. Acad. Sci. Paris, Ser. 1341 (2005). (c) 2005 Academie des sciences. Published by Elsevier SAS. All rights reserved.",2005,10.1016/j.crma.2005.09.029,no
Analog VLSI layout design of advanced image processing for artificial vision model,"We propose herein an artificial vision model for the motion detection which uses analog electronic circuits and designed the analog VLSI layout. The proposed model is comprised of four layers. The model was shown to be capable of detecting a movement object in the in the 2-dimensional image. Moreover, the proposed model can be used to detect two or more objects, which is advantageous for detection in an environment in which several objects are moving in multiple directions simultaneously. The number of elements in the model, is reduced in its' realization using the integrated devices. The proposed model is robust with respect to fault tolerance. Moreover, the connection of this model is between adjacent elements, making hardware implementation easy.",2005,,no
Output error analysis in model-reference adaptive data processing,"Zero shift in the measured shock signals brings significant errors to the test results. Model-reference adaptive data processing eliminates the errors to a lower level. There are still output errors in the data processing owing to the incomplete compensation. Based on the prototype shock models and the noise models, the output error analysis on the adaptive data processing is conducted by an inversion algorithm. Results of the analysis can be taken as a measure in the evaluation of reference model bank, and also be helpful to the construction of the reference model bank.",2005,,no
ILAS data processing for stratospheric gas and aerosol retrievals with aerosol physical modeling: Methodology and validation of gas retrievals,"[1] This paper presents initial results of simultaneous gas and aerosol retrievals from Improved Limb Atmospheric Spectrometer (ILAS) observations taken between November 1996 and June 1997. The solar occultation measurements were processed by an inversion method that included aerosol physical modeling and permitted simultaneous retrieval of O-3, HNO3, CH4, H2O, NO2, N2O, N2O5, ClONO2, CFC-11, and CFC-12 trace species and particle volume size distributions for key aerosol/polar stratospheric cloud (PSC) components such as liquid ternary solution, nitric acid trihydrate, nitric acid dihydrate, and water ice. The retrieval method was designed for the version 7.0 ILAS data processing algorithm. Gas retrieval results for the entire ILAS data set were compared to results from the earlier version 6.0 retrieval algorithm that was based on aerosol/PSC contribution estimates in the gas window channel. Gas data from nearby balloon-borne validation measurements and new data on the aerosol retrievals helped explain discrepancies between the two algorithms. The new version 7.0 methodology proved effective for simultaneous retrievals of all trace gases and showed a significant advantage when retrieving CH4, H2O, NO2, N2O, and CFC-12 from PSC observations.",2006,10.1029/2005JD006543,no
Model reduction and physical understanding of slowly oscillating processes: The circadian cycle,"A differential system that models the circadian rhythm in Drosophila is analyzed with the computational singular perturbation (CSP) algorithm. Reduced nonstiff models of prespecified accuracy are constructed, the form and size of which are time-dependent. When compared with conventional asymptotic analysis, CSP exhibits superior performance in constructing reduced models, since it can algorithmically identify and apply all the required order of magnitude estimates and algebraic manipulations. A similar performance is demonstrated by CSP in generating data that allow for the acquisition of physical understanding. It is shown that the processes driving the circadian cycle are (i) mRNA translation into monomer protein, and monomer protein destruction by phosphorylation and degradation (along the largest portion of the cycle); and (ii) mRNA synthesis (along a short portion of the cycle). These are slow processes. Their action in driving the cycle is allowed by the equilibration of the fastest processes; (1) the monomer dimerization with the dimer dissociation (along the largest portion of the cycle); and (2) the net production of monomer+dimmer proteins with that of mRNA (along the short portion of the cycle). Additional results (regarding the time scales of the established equilibria, their origin, the rate limiting steps, the couplings among the variables, etc.) highlight the utility of CSP for automated identification of the important underlying dynamical features, otherwise accessible only for simple systems whose various suitable simplifications can easily be recognized.",2006,10.1137/060649768,no
Model-based process evaluation of nanoparticle precipitation in Microemulsions,A model-based process evaluation using deterministic population balance equation (PBE) as well as stochastic Monte Carlo simulation (MCS) is applied for a microemulsion-based nanoparticle precipitation process. The two approaches are compared and show their advantages for different tasks in process design and control. PBE can be used for real-time analysis and process control when sufficiently supplied with appropriate model behaviour and the corresponding parameters. MCS is helpful to identify important basic mechanisms of the process and can be applied for model validation. Experiments for barium sulphate nanoparticle precipitation in a technical microemulsion are used to compare efficiency and accuracy of both approaches.,2006,,no
Use of a simple mathematical model to evaluate dipping and MAP effects on aerobic respiration of minimally processed apples,"A new general respiratory model based on the Michaelis-Menten type enzyme kinetics was tested to describe changes in aerobic respiration of minimally processed apple. 'Golden Delicious' apple slices, washed in distilled water or dipped in an antioxidant solution (1% ascorbic acid, 1% citric acid), were packed in multilayer pouches in passive or active modified atmosphere (respectively air and 90% N2O, 5% CO2, 5% O-2). During four days of storage at 4 degrees C, O-2 and CO2 concentrations in the package headspace were monitored. The proposed model successfully fitted the experimental data, adequately describing the aerobic respiration of apple slices. Results suggest that both dipping treatment and active modified atmosphere affect the respiratory activity of the packed product. In particular, dipping reduced both the initial oxygen respiration rate (for about 10 h of storage) and the inhibitory effect that CO, had on O-2 consumption of packed product. The active modified atmosphere decreased the rate of oxygen consumption compared with passive MA, in particular at the beginning of storage. (c) 2005 Elsevier Ltd. All rights reserved.",2006,10.1016/j.jfoodeng.2005.05.034,no
Modelling and measurement of sand transport processes over full-scale ripples in oscillatory flow,"A new series of laboratory experiments was performed in the Aberdeen Oscillatory Flow Tunnel (AOFT) and the Large Oscillating Water Tunnel (LOWT) to investigate time-averaged suspended sand concentrations and transport rates over rippled beds in regular and irregular oscillatory flow. The wave-induced oscillatory near-bed flows were simulated at full-scale. Five series of experiments were carried out. During the two AOFT experimental series, ripple dimensions, ripple migration rates and net sand transport rates were measured under regular and irregular asymmetric flow for two different sand types. The three LOWT experimental series focussed on measurements of the ripple dimensions, ripple migration rates, time-averaged suspended sand concentrations and net sand transport rates under regular asymmetric and irregular weakly asymmetric flow for two different sand types. From analysis of new and other full-scale data, it is concluded that the lower part of the time- and bed-averaged concentration profile (up to two times the ripple height above the ripple crest level) has an exponential profile. A new reference concentration formula is proposed based on the formula of Bosman and Steetzel [Bosman, J.J., Steetzel, H.J., 1986. Time- and bed-averaged concentration under waves. Proc. 20th ICCE Taipei, ASCE, pp. 986-1000], which includes the grain-size influence. Furthermore, it is shown that the concentration decay length is strongly related to the ripple height and that the simple formula R-c = 1.27 eta gives good agreement with the data. A new transport model is proposed for the wave-related net transport over full-scale ripples based on a modified half wave cycle concept of Dibajnia and Watanabe [Dibajnia, M., Watanabe, A., 1992. Sheet flow under nonlinear waves and currents. Proc. 23rd ICCE Venice, ASCE, pp. 20152028; Dibajnia, M., Watanabe, A., 1996. A transport rate formula for mixed sands. Proc. 25th ICCE Orlando, ASCE, pp. 3791-3804]. The magnitudes of the half wave cycle transport contributions are related to the gain-related Shields parameter, the degree of wave asymmetry and a newly defined vortex suspension parameter P, which is the ratio between the ripple height and the median grain-size. The new model has been calibrated using transport data from the new regular flow experiments and has subsequently been validated using other data, including measurements from irregular flow experiments. The new model is seen to perform better overall than existing practical models for ripple regime net sand transport. (c) 2006 Elsevier B.V. All rights reserved.",2006,10.1016/j.coastaleng.2006.02.002,no
A simple model to predict compound loss processes in aquatic ecotoxicological tests: calculated and measured triphenyltin levels in water and biota,"A new simple model, based on the fugacity approach, has been developed to provide a predictive tool useful in the planning phase of aquatic ecotoxicological tests for assessing the actual daily concentrations in water. In our experiments, three nominal concentrations (100, 225, and 500 ng L-1) of triphenyltin chloride (TPT-Cl) were employed for an exposure period of 28 days in 50 L aquaria with the echinoderm Antedon mediterranea as test species. Extracts from water and biota samples collected during the experiments were analysed by GC-MS/MS, after the extraction/derivatization step. An indicative mean BCF (V/V) on a fresh weight base of 3.5 x 10(4) +/- 0.8 x 10(4) (standard deviation) could be calculated for A. mediterranea. Three different compartments (air, water and biota) and main advection/reaction processes are taken into account in the model design, and the comparison between predicted and measured concentrations in both water and biota for the three concentrations tested confirmed that the assumptions given in our model application were valid and useful for further applications.",2006,10.1080/03067310500249997,no
Process models representing knowledge for action: a revised quality framework,"A semiotic framework for evaluating the quality of conceptual models was proposed by (Lindland OI, Sindre G and Solvberg A (1994) Understanding Quality in Conceptual Modelling, IEEE Software 11(2), 41-49) and has later been extended in several works. While the extensions have fixed some of the limitations of the initial framework, other limitations remain. In particular, the framework is too static in its view upon semantic quality, mainly considering models, not modelling activities, and comparing these models to a static domain rather than seeing the model as a facilitator for changing the domain. Also, the framework's definition of pragmatic quality is quite narrow, focusing on understanding, in line with the semiotics of Morris, while newer research in linguistics and semiotics has focused beyond mere understanding, on how the model is used and impact its interpreters. The need for a more dynamic view in the semiotic quality framework is particularly evident when considering process models, which themselves often prescribe or even enact actions in the problem domain, hence a change to the model may also change the problem domain directly. This paper discusses the quality framework in relation to active process models and suggests a revised framework based on this.",2006,10.1057/palgrave.ejis.3000598,no
Evaluating parameterizations of the autoconversion process using a single-column model and Atmospheric Radiation Measurement Program measurements,"A single-column model is used to evaluate the performance of two types of autoconversion parameterizations. The model results are compared to data collected at the Atmospheric Radiation Measurement Program's Southern U. S. Great Plains site. The model is run over a period covering 2 years (2000-2001), and the results are analyzed for time periods varying from hourly to seasonal. During a relatively short 27-hour period during March 2000 characterized primarily by shallow frontal clouds, modeled values of cloud liquid water were better simulated using a Manton-Cotton-type autoconversion parameterization. However, over longer timescales representing a multitude of different cloud types and meteorological conditions, a Sundqvist-type parameterization produced better results. Analysis of the model results indicates that the Manton-Cotton-type parameterization does better during periods when shallow clouds are present without any overlying clouds, while the Sundqvist-type parameterization is preferred during periods when high and low clouds coexist. A possible explanation is that precipitation from high clouds may not be represented well by the SCM, thus affecting the precipitation formation rates in any lower clouds. Sensitivity tests using the Manton-Cotton parameterization indicate that the autoconversion rate is sensitive to the specification of the cloud droplet number concentration (N-c). The single-column model, as well as many general circulation models, specify N-c as a constant value. However, limited in situ measurements suggest that N-c varies significantly in time. The mean modeled top-of-atmosphere cloud radiative forcing during the 2-year period 2000-2001 differed by 3 W m(-2) as the cloud droplet concentration was varied between minimum and maximum values suggested by the in situ measurements. These results imply that model-produced hydrological cycle and cloud-radiation interactions could be better modeled using an accurate time-dependent measure of the cloud droplet concentration.",2006,10.1029/2005JD006296,no
Efficiency of polymerase chain reaction processes: A stochastic model,"A stochastic model for the efficiency of polymerase chain reaction (PCR) processes is presented. The model is based on the assumption that the i'th nucleotide incorporation on the DNA template takes tau(i) seconds, where tau(i), has an exponential distribution with mean 1/lambda(i). In this paper, given lambda(i) that can be obtained from the primer-template sequence, temperature profile, enzyme rate, and other assay conditions [3], we calculate the efficiency of a multi-step PCR process using the distribution of the summation of non-identical exponential distributions.",2006,10.1109/GENSIPS.2006.353143,no
3-D inverse problem continuous model for thermal behavior of mould process based on the temperature measurements in plant trial,"A three-dimensional mathematical model for round billet continuous casting of steel in the mould, adopting the surface revolution theory, together with an appropriate inverse problem method from the measured temperatures by the thermocouples buried in various locations in the mould, has been developed to elucidate the real thermal behavior of mould process in plant production, including the mould heat transfer, steel solidification, and slag film distribution, with the objective of understanding the relationship among them. The results show that the vertical component of heat condition and the slag rim at meniscus decreases mould temperatures and the former also lower the level of the peak temperature, and the minimum thickness of solid slag film about 80 mm below meniscus results in the peak mould heat flux and temperature. The magnitude and profile of mould heat transfer around the perimeter between the shell and mould, having corresponding relationship with solid slag, determines that of shell thickness. And the channel the liquid slag entering at meniscus determines the mould heat transfer to some extent.",2006,,no
Integrated chemical-physical processes kinetic modelling of multiple mineral precipitation problems,"A three-phase (aqueous/gas/solid) mixed weak acid/base chemistry kinetic model is applied to evaluate the processes operative in the aeration treatment of swine wastewater (SWW) and sewage sludge anaerobic digester liquor (ADL). In both applications, with a single set of constants (except for the aeration rates which are situation specific), close correlation could be obtained between predicted and measured data, except for the Ca concentration-time profile in the SWW. For this wastewater, the model application highlighted an inconsistency in the measured Ca data which could not be resolved; this illustrates the value of a mass balance-based model in evaluating experimental data. From the model applications, in both wastewaters the dominant minerals precipitating are struvite and amorphous calcium phosphate (ACP), which precipitate simultaneously competing for the same species, P. The absolute and relative masses of the two precipitants are governed by the initial solution state (e.g. total inorganic C (C-T), Mg, Ca and P concentrations), their relative precipitation rates (struvite > ACP) and the system conditions imposed (aeration rates and time applied). It is concluded that the kinetic model is able to predict correctly the time-dependent weak acid/base chemistry reactions and final equilibrium state for situations where multiple minerals competing for the same species precipitate simultaneously or sequentially, a deficiency in traditional equilibrium chemistry-based algebraic models.",2006,10.2166/wst.2006.407,no
Automated correction of surface obstruction errors in digital surface models using off-the-shelf image processing,"Airborne topographic data collection requires removal of errors that arise due to surface features that obstruct the ground from the sensor. Typically, this has been based on manual correction and/or automated filtering. To some degree, the latter has provided a method for identifying and removing unwanted surface obstructions in large topographic data-sets. However, the algorithms used are unintelligent in that they cannot reliably differentiate between the various types of obstructions and the ground. If coincident optical support imagery is available, the use of intelligent correction routines becomes possible. This paper describes an automated approach for removing obstruction errors using optical support imagery and simple image processing routines. Orthorectification and classification of support imagery enable obstruction errors to be identified in the digital surface model (DSM) and corrected intelligently to produce a digital terrain model (DTM). The results show that support imagery can be used with basic image processing routines to remove obstructions intelligently and automatically from large topographic data-sets. Since the approach can differentiate between types of obstructions, the removal of each type of error can be customised, making this a very flexible approach to topographic data correction.",2006,10.1111/j.1477-9730.2006.00398.x,no
Modelling biological processes under anaerobic conditions through integrating titrimetric and off-gas measurements - applied to EBPR systems,"An innovative method for modelling biological processes under anaerobic conditions is presented and discussed. The method is based on titrimetric and off-gas measurements. Titrimetric data is recorded as the addition rate of hydroxyl ions or protons that is required to maintain pH in a bioreactor at a constant level. An off-gas analysis arrangement measures, among other things, the transfer rate of carbon dioxide. The integration of these signals results in a continuous signal which is solely related to the biological reactions. When coupled with a mathematical model of the biological reactions, the signal allows a detailed characterisation of these reactions, which would otherwise be difficult to achieve. Two applications of the method to the enhanced biological phosphorus removal processes are presented and discussed to demonstrate the principle and effectiveness of the method.",2006,10.2166/wst.2006.020,no
Comparison of numerical and physical models for understanding shear fracture processes,"An understanding of the formation of shear fractures is important in many rock engineering design problems. Laboratory experiments have been performed to determine the Mode II fracture toughness of Mizunami granite rock samples using a cylindrical `punch-through' testing device. In this paper we attempt to understand and interpret the experimental results by numerical simulation of the fundamental shear fracture initiation and coalescence processes, using a random array of displacement discontinuity crack elements. It is found that qualitative agreement between the experimental and numerical results can be established, provided that shear-like micro-scale failure processes can be accommodated by the failure initiation rules that are used in the numerical simulations. In particular, it is found that the use of an exclusively tension-driven failure initiation rule does not allow the formation of macro-shear structures. It is apparent, also, that further investigation is required to determine how consistent rules can be established to link micro-failure criteria to equivalent macro-strength and toughness properties for a macro-shear slip surface.",2006,10.1007/s00024-006-0055-9,no
"Research on the Representation of the Programming Task Process of the Problem of ""farmer's dilemma""","Anatomizing and describing knowledge task process (KTP) is the first step to improve the efficiency of knowledge work. From the angle of method optimizing and time-test, and on the basis of basic logic of motion time analysis, this paper interpreted knowledge task as a series of mind operations, and a procedure of information processing and computing in human's brain. Mind operation is considered as the therblig and algorithm of knowledge task. A programming process of ""Farmer's dilemma"" was given as an example to illustrate and analyze KTP by the concept of mind operation with the principle of scientific management and computing theory. This paper characterized the way of analyzing KTP by mind operation and described the paradigm of such analysis in the mean time.",2006,,no
A case-based reasoning method for processing model recognition and reuse in program comprehension,"Applying Cased-based reasoning (CBR) method in program understanding provides a practical route towards more powerful software engineering technology. A CBR approach to the recognition of model component is presented and the whole reasoning process of the recognition is presented, including a case representation method and a matching algorithm. A prototype system named Process Model Component Recognition & Reuse (PMCRR) is developed to implement model transformation and reconstruction. At last, an example is illustrated to check the efficiency of CBR method.",2006,10.1109/ICCIAS.2006.294101,no
Methodological guidelines for SQA in development process - An approach based on the SPICE model,"As far as international standards for promoting Software Process Quality are concerned, one of the most popular and accepted is ISO 15504 (or SPICE model). On the other hand, since a development methodology must guide the main activities in software development, it is necessary that this one fulfils some Quality Base Practices to guarantee a high-level product. The purpose of this research is analyzing a set of five methodologies widely used by developers, to identify its adjustment with respect to the aforementioned standard. This analysis allowed us: (1) determining the degree of alignment of these methodologies with respect to the SPICE model, and (2) proposing a synthesis of methodological guidelines, based on the best practices obtained from these methodologies, that supports the characteristics contained in the studied standard.",2006,,no
Integration of the STEP-based assembly model and XML schema with the fuzzy analytic hierarchy process (FAHP) for muti-agent based assembly evaluation,"Assemblability analysis and evaluation plays a key role in assembly design, operation analysis and planning. In this paper, we propose an integrated intelligent approach and framework for evaluation of assemblability and assembly sequence for electro-mechanical assemblies (EMAs). The approach integrates the STEP (STandard for the Exchange of Product model data, officially ISO 10303)-based assembly model and XML schema with the fuzzy analytic hierarchy process for assembly evaluation. The evaluation structure covers not only the geometric and physical characteristics of the assembly parts but also the assembly operation data necessary to assemble the parts. The realization of the integration system is implemented through a multi-agent framework. Through integration with the STEP-based product modeling agent system, CAD agent system and assembly planning agent system, the developed assembly evaluation agent system can effectively incorporate, exchange, and share concurrent engineering knowledge into the preliminary design process so as to provide users with suggestions for improving a design and also helping obtain better design ideas. The applications show that the proposed approach and system are feasible.",2006,10.1007/s10845-006-0031-3,no
Evaluation of management alternatives for an agricultural watershed in a sub-humid subtropical region using a physical process based model,"Assessments of potential environmental impacts of non-point source (NPS) pollutants at local and regional scales are necessary as a basis for effective management strategies to protect precious resources such as land and water. Intensive watershed scale study is therefore necessary to develop management strategies for abating the agricultural NPS pollution. The major goal of the present study was to identify the critical areas of an agricultural watershed and recommend the best management practices using a physical process based watershed scale model, soil water assessment tool (SWAT). A 973-ha agricultural watershed located in Midnapore district of West Bengal state in eastern India was monitored to quantify the hydrologic parameters such as runoff, sediment yield and the quality of surface water. The model was calibrated and validated using observed hydrologic and water quality data of the watershed monitored during the rainy seasons of 2002 and 2003, respectively. Besides these data, micro-meteorological data, topographical map, soil map, land resources data and remote sensing data (satellite imagery) of the watershed were used as input to the model. The study revealed that SWAT model simulates daily runoff, sediment yield and nutrient concentration in runoff satisfactorily throughout the entire rainy season, as evident from standard statistical tests. The calibrated model was then successfully used for identifying the critical sub-watersheds and for development of best management practices. Several simulations were performed to determine the best out of the 48 different combinations of treatments for the management of the critical sub-watersheds. The study revealed that no other crop could replace the rice (Oryza sativa) crop during the rainy season and the existing conventional tillage practice needs to be replaced by conservation tillage in order to minimize the sediment yield and nutrient losses. Fertilizer application rate of 80:60 kg ha(-1) of N:P is also recommended to minimize the surface water pollution in the watershed due to NO3-N and P. (c) 2005 Elsevier B.V. All rights reserved.",2006,10.1016/j.agee.2005.08.032,no
Modeling simulation and experimental validation for mold filling process,"Based on the continuum equation, momentum conservation and energy conservation equations, the numerical model of turbulent flow filling was introduced; the 3-D free surface vof method was improved. Whether or not the numerical simulation results are reasonable, it needs corresponding experimental validations. General experimental techniques for casting fluid flow process include: thermocouple tracking location method, hydraulic simulating method, heat-resistant glass window method and X-ray observation etc. The hydraulic analogue experiment with DPIV technique is arranged to validate the fluent flow program for low-pressure casting with 0.1 X 10(5) Pa and 0.6 X 10(5) Pa pressure visually. By comparing the flow head, liquid surface, flow velocity, it is found that the filling pressure value influences the flow state strongly. With the increase of the filling pressure, the fluid flow state becomes'unstable, the flow head becomes higher, and the filling time is reduced. The simulated results are accordant with the observed results approximately, which can prove the reasonability of our numerical program for filling process further.",2006,,no
Scenario-based comparison and evaluation: issues of current business process modelling languages,"Business process modelling (BPM) is an important way to understand business processes and provides a basis for further improvement and integration. In this paper, five business process modelling languages are used to model a business scenario and their advantages and disadvantages are discussed. Some process characteristics are derived from real scenarios and used for evaluating current business process modelling languages. The evaluation results reveal that none of the business process modelling languages studied can fulfil the modelling requirements. The limitations of current business process modelling languages are further investigated and future research directions are discussed.",2006,10.1243/095440505X32409,no
Adopting the cognitive complexity measure for business process models,"Business process models, often modelled using graphical languages like UML, serve as a base for communication between the stakeholders in the software development process. To fulfill this purpose, they should be easy to understand and easy to maintain. For this reason, it is useful to have measures that can give us some information about understandability, analyzability and maintainability of a business process model. Shao and Wang have proposed a cognitive complexity measure[19]. It can be used to estimate the comprehension effort for understanding software. This paper discusses how these research results can be extended in order to analyze the cognitive complexity of graphical business process models.",2006,10.1109/COGINF.2006.365702,no
A process-based rejectionist framework for evaluating catchment runoff model structure,"Complex hydrological descriptions at the hillslope scale have been difficult to incorporate within a catchment modeling framework because of the disparity between the scale of measurements and the scale of model subunits. As a result, parameters represented in many conceptual models are often not related to physical properties and therefore cannot be established prior to a model calibration. While tolerable for predictions involving water quantity, water quality simulations require additional attention to transport processes, flow path sources, and water age. This paper examines how isotopic estimates of residence time may be used to subsume flow path process complexity and to provide a simple, scalable evaluative data source for water quantity- and quality-based conceptual models. We test a set of simple distributed hydrologic models (from simple to more complex) against measured discharge and residence time and employ a simple Monte Carlo framework to evaluate the identifiability of parameters and how the inclusion of residence time contributes to the evaluative process. Results indicate that of the models evaluated, only the most complex, including an explicit unsaturated zone volume and an effective porosity, successfully reproduced both discharge dynamics and residence time. In addition, the inclusion of residence time in the evaluation of the accepted models results in a reduction of the a posteriori parameter uncertainty. Results from this study support the conclusion that the incorporation of soft data, in this case, isotopically estimated residence times, in model evaluation is a useful mechanism to bring experimental evidence into the process of model evaluation and selection, thereby providing one mechanism to further reconcile hillslope-scale complexity with catchment-scale simplicity.",2006,10.1029/2005WR004247,no
A study on Knowledge Process Project and its process model - Complexity theory perspective,"Complexity theory provides new perspective from which we can observe the Knowledge Process Project (KPP), and allows us to build its process model in a nonlinear, chaotic way. This study first advanced the concept of KPP, compared it with Craftwork Process Project (CPP); and then builds its process model, based on organization Change-Tolerance; at last uses a real case to exam the model and extract several management implications.",2006,,no
A context-driven software comprehension process model,"Comprehension is an essential part of software evolution. Only software that is well understood can evolve in a controlled manner. In this paper, we present a formal process model to support the comprehension of software systems by using Ontology and Description Logic. This formal representation supports the use of reasoning services across different knowledge resources and therefore, enables us to provide users with guidance during the comprehension process that is context sensitive to their particular comprehension task. As part of the process model, we also adopt a new interactive story metaphor, to represent the interactions between users and the comprehension process.",2006,10.1109/SOFTWARE-EVOLVABILITY.2006.1,no
"Note on ""effects of random defective rate and imperfect rework process on economic production quantity model""",Conventional approaches for deriving optimal production lot size are by using the differential calculus on the production-inventory cost function with the need to prove optimality first. Recent articles proposed the algebraic approach to the solution of classic economic order quantity and economic production quantity (EPQ) model without reference to the use of derivatives. This note extends it to an EPQ model taking the random defective rate and imperfect rework process into consideration. We demonstrate that the optimal lot size can be solved algebraically and the expected inventory cost can be derived immediately as well. [DOI: 10.1115/1.2039948],2006,10.1115/1.2039948,no
Data quality evaluation process and methods of natural environment conceptual model in simulation systems,"Currently in almost simulation systems applying environment models, data quality of natural environment is not sufficiently evaluated. In order to address this problem, the process and method of data quality evaluation are proposed for Natural Environment Conceptual Model(NECM). Firstly, a process of 4 steps is presented to evaluate data quality of NECM. Secondly, indexes and factors of data quality evaluation are given according to the characteristics of NECM. Thirdly, in order to meet the requirement of this data evaluation, the Analytic Hierarchy Process(AHP) method is improved and then applied to evaluate the data quality. Finally, the results of data quality evaluation are given and analyzed.",2006,,no
Error-proof process network model,"Data flow process networks are frequently used in streaming multimedia applications. Modified Kahn process network analyzed in this paper is a kind of data flow process networks. It is a computation model in which many concurrent processes can be executed simultaneously. In real time digital signal processing applications execution time often is infinite. However failures of implementation hardware can occur. In our work dynamic run-time reconfiguration is introduced into process network which ensures hardware error handling. After dynamic reconfiguration, network execution results may become non deterministic, but this helps avoiding critical termination of network execution. In this work Error-Proof Process Network (EPPN) model is presented. EPPN is based on Kahn process network model and ensures the possibility to reconfigure network in case of hardware failure.",2006,,no
Building inferential estimators for modeling product quality in a crude oil desalting and dehydration process,"Desalting/dehydration plants (DDP) are often installed in crude oil production units in order to remove water-soluble salts from an oil stream. This paper describes the development of simple inferential estimators for product quality of the desalting/dehydration process. The inferential estimators were constructed to capture the relationship between the product quality of the plant and the process input variables. Five input process variables that are known to influence product quality were considered. These include temperature, settling time, mixing time, chemical dosage, and dilution rate. The product quality of the desalting/dehydration process was identified by the salt removal and water cut efficiencies. Hence, inferential estimators were used to infer the salt removal and water cut efficiencies from the five input process variables. These inferential estimators were constructed based on the application of both multiple linear and principal component analysis as well as non-linear regression. The results indicate that the settling time and dilution water were the common variables in estimating both the salt removal and water cut efficiencies. On the other hand, temperature contributed insignificantly in predicting the two efficiencies. Furthermore, the inferential model predictions were compared with the experimental readings. It was found that the actual dependence of the performance of the desalting/dehydration process on process parameters could not be described only by linear relationships. Addressing the non-linearity of the process variables overcame the problem of inaccurate predictions. Future studies based on the use of computational intelligence techniques and design of experiments to get better models are suggested as well as the use of response surface methodologies to determine the set of parameters that will optimize the process efficiencies. (c) 2006 Elsevier B.V. All rights reserved.",2006,10.1016/j.cep.2006.01.004,no
Modeling and optimization of process parameters for defect toleranced drilling of GFRP composites,"Drilling is a frequently practiced machining process for fiber-reinforced plastics (FRP) in industry owing to the need for component assembly in mechanical structures. Drilling experiments are performed on a (0/ +/- 45/ 90)(2s) 3-mm-thick glass fiber-reinforced laminate using 4-, 6- and 8-mm-diameter HSS drills. The machining response of the quasi-isotropic laminate was studied by monitoring the thrust and torque. The performance of the HSS drills for different cutting conditions was studied by measuring the tool wear. Delamination due to drilling is a major concern in machining a composite laminate and is analyzed by using linear elastic fracture mechanics, classical plate bending theory, and the mechanics of composites. A mechanical model for evaluating the critical thrust at which delamination is initiated at different ply locations has been used, and the critical thrust force at the onset of delamination has been found to be 70 N. The present work analyzes data on the thrust force, torque, and tool life by using a group method data handling (GMDH) algorithm. An optimization algorithm using simulated annealing with a performance index is then applied to search for the optimal process parameters for delaminatien constrained drilling.",2006,10.1080/10426910500411587,no
A methodology for modeling inter-company supply chains and for evaluating a method of integrated product and process documentation,"Due to the increasing complexity in products and in the resulting enterprise processes, new concepts for process optimization are necessary. One innovative concept, resulting from the requirements of the automotive industry, is the method of an integrated product and process documentation. In order to evaluate the benefits of an inter-company use of this documentation approach a simulation-based decision support system using a modular modeling concept for intra- and inter-company process chains has been developed. This concept is based on the Supply Chain Operations Reference-model (SCOR-model). The developed concept allows the evaluation of different configurations of process chains with different sets of parameters describing realistic production and inventory processes. Focussed on supply chains with high variant and complex products within the automotive industry first scenarios have been built up and the benefits of this integrated documentation approach can be visualized. This approach is the future basis for optimization methods coming from control theory. (c) 2005 Elsevier B.V. All rights reserved.",2006,10.1016/j.ejor.2005.02.006,no
Modeling the sensory qualities of processed apples after controlled atmosphere storage and frozen storage,"Durational effects of controlled atmosphere storage (CA) and frozen storage on apple quality were studied for commercially processed pies made from Northern Spy, Idared and Nova Spy apples. Sensory tests on apple pie quality indicated that the duration of apples in CA significantly affected the quality of the processed apple. Extending the CA duration of the apples to 45 weeks resulted in a pie filling with reduced apple flavor, tartness and astringency, and with increased sweetness and off-flavor. To minimize flavor/taste changes throughout the processing season, close attention must be given to the product's sugar/acid formulation: by week 27 for Idared apples and by week 33 for Northern Spy and Novaspy apples. The texture and appearance of processed Novaspy and Idared fruits were affected earlier in CA than Northern Spy apples. The frozen product was relatively stable for all three varieties, especially for apples processed early in the storage season (15-27 weeks). A strategy that utilizes short- to medium-term CA with longer-term frozen storage would optimize quality across the season.",2006,10.1111/j.1745-4557.2006.00060.x,no
Modeling of reconfiguration control applications based on the IEC 61499 reference model for Industrial Process Measurement and Control Systems,"Future manufacturing is envisioned to be highly flexible and adaptable. New technologies for efficient engineering of reconfigurable systems and their adaptations are preconditions for this vision. Without such solutions, engineering adaptations of Industrial Process Measurement and Control Systems (IPMCS) will exceed the costs of engineered systems by far and the reuse of equipment will become inefficient. In this work a new approach for the reconfiguration of IEC 61499 based control application and the corresponding modeling is proposed This new method significantly increases engineering efficiency and reuse in component-based IPMCSs control.",2006,,no
Modelling of manufacturing process complexity,"Gaining momentum in several fields of study is the recognition of the need for a viewpoint that includes the human element as an integral part of the modem production system beyond traditional ergonomics. The ""intellectual capital"" is as much of a resource as money, materials, software and hardware. A model that considers the human players in tandem with the physical elements is needed to provide insights into the sensitivities of the manufacturing system. Using Systems Analysis and Design methods, a framework has been developed, which is valid for different perspectives and environments, to assess the elements of manufacturing complexity. The manufacturing complexity index allows people with diverse backgrounds to rapidly evaluate alternatives and risks with respect to the product, process or operation tasks. In this paper, the technique for evaluating the process complexity metric is presented. An analysis is performed comparing the relative process complexity for a power steering pump bracket that is manufactured in a CNC machining cell and a dedicated line. The areas of complexity are clearly evident. This provides insight for risk assessment as this systematic approach can be used to ""mathematically"" show tradeoffs for each important criterion during the design stages.",2006,10.1007/1-84628-210-1_35,no
Understanding T-ingot horizontal DC casting using process modelling,"Horizontal continuous casting process has been successfully implemented in Alcan for the production of T-ingots of primary aluminium and foundry alloys. Ability to achieve increased productivity targets and reduce production costs relies on a fundamental understanding of key process characteristics and operating parameters. Thanks to the long-standing experience in vertical DC Casting, numerical modelling appeared as a powerful approach to understand phenomena related to metal flow, solidification and ultimately defect formation. As part of a collaborative R&D program, a global model of horizontal casting process, integrating specialized sub-models on critical aspects of the process such as meniscus dynamics, is being developed. Experimental characterization of primary and secondary cooling is performed in parallel with modelling work to provide the information necessary to properly characterize mould heat transfer. This paper will present the development of a 3D process model of T-ingot casting along with its application to solve specific process challenges that have emerged during the first years of production in the plant.",2006,,no
Validation of the hydrological processes in a hydrological model,"Hydrological models are often required to model watersheds where the conditions change over time. Calibration and validation of these models is a difficult process that requires validation of each of the major hydrological processes within the model. This paper presents the calibration, validation, and sensitivity analysis of the WATFLOOD hydrological model. The calibration process is usually based on streamflow and may involve an implicit validation of the hydrological processes when the internal state variables are monitored to ensure that the model operates realistically. This paper presents explicit validations of several internal state variables (soil moisture, evaporation, snow accumulation and snowmelt, and groundwater flow) and the statistical characteristics of the streamflow. The WATFLOOD model is shown to track each of these variables with sufficient accuracy for operational use of the model. In addition, several behavioral sensitivity checks are presented to show that the model behaves in a realistic manner. This paper provides a broadly based methodology for calibration and validation of a distributed hydrological model.",2006,10.1061/(ASCE)1084-0699(2006)11:5(451),no
Recent advances and current problems in modelling surface processes and their interaction with crustal deformation,"I present a brief summary of recent advances in the field of computational geomorphology and various attempts to couple numerical models of landscape evolution to models of crustal/lithospheric deformation. The most commonly used formulations for the various physical processes at play during surface erosion, transport and deposition are presented, as well as an outline of how they have been incorporated in a variety of numerical schemes. I also explain how the coupling between erosion and tectonics has been performed under various simplifying assumptions. Determining, the rate constants for each of the proposed landforming mechanisms remains a difficult challenge that has recently been helped by the advent of new low temperature thermochronometers and exposure dating by cosmogenic radionuclides. I demonstrate how the information contained in the relationship between age and elevation can be used to provide constraints on the 'age' of' a landscape, as well as how important rate information can be extracted from various datasets by using simple modelling techniques. This paper demonstrates why the field of computational geomorphology needs to harmonize the various parameterizations (often the legacy of empirical relationships derived from observations at the human scale), quantitative estimates of the value of the numerous rate parameters and improvement of the numerical techniques.",2006,10.1144/GSL.SP.2006.253.01.16,no
Measuring the impact of a Web-based data query system: The logic model as a tool in the evaluation process,"In brief review of program evaluation perspectives, including the concept of a logic model and its applicability to the evaluation of Web-based data query systems (WDQSs), is presented. A logic model is used to flesh out evaluation components of a WDQS, including inputs, constraints, program activities, program outputs, and outcomes. For each component, a list of potential items is presented for inclusion in the model, along with examples of initial, intermediate, and ultimate outcomes for a WDQS, Program evaluation is a process that is important to conduct both early in WDQS development to promote clarity of its vision and objectives and throughout the course of the WDQS implementation and maintenance to ensure that its objectives are being met. Should the WDQS not be producing desirable results, the logic model provides a road map for understanding which activities are not meeting the WDQS's objectives so that the course of WDQS development, implementation, and maintenance may be altered to improve the probability of reaching desirable outcomes.",2006,,no
Numerical modeling of the transport process of negative-ion plasmas in photodetachment measurement,"In order to measure negative-ion temperature, the experimental result of the negative-ion recovery ratio from the two-laser photodetachment is usually fitted with the theoretical curve derived from the ballistic kinetic theory of negative ions. The particle-in-cell (PIC) code with a one-dimensional slab model has been developed to analyze the effect of the ambipolar electric field, which is neglected in the ballistic theory, on the negative-ion recovery. The preliminary results of the PIC simulation indicate that our PIC code is a useful tool for the analyses of the negative-ion recovery in the photodetachment. (c) 2006 American Institute of Physics.",2006,10.1063/1.2171757,no
A service quality management model based on process approach,"In order to strive towards higher levels of customer satisfaction, managers within service industries need to increasingly construct the quality management system to improve service quality. However, there were few studies paying attention to how to implement a quality management system based on the indicators of proposed service quality scales. Moreover, it is seldom considered that the priority of the indicators maybe vary from one stage to another stage in service processes. If this fact is neglected, then customer requirements may be not met effectively. The proposed model, Quality Train Model (QTM), mainly presents three key concepts as customer focus, process approach, and Deming's wheel. The methodology of QTM consists of six steps: initiating, identifying, planning, implementing, diagnosing and recovering.",2006,,no
Modelling a multitype branching brownian motion: Filtering of a measure-valued process,"In the framework of marked trees, a multitype branching brownian motion, described by measure-valued processes, is studied. By applying the strong branching property, the Markov property and the expression of the generator are derived for the process whose components are the measure-valued processes associated to each type particles. The conditional law of the measure-valued process describing the whole population observing the cardinality of the subpopulation of a given type particles is characterized as the unique weak solution of the Kushner-Stratonovich equation. An explicit representation of the filter is obtained by Feyman-Kac formula using the linearized filtering equation.",2006,10.1007/s10440-006-9014-9,no
Evaluation of the inverse modelling process for heterogeneous porous media through laboratory air injection tests,"In the modelling process, inversion techniques are generally used to estimate unknown parameters. However, we need to determine the model structure and other conditions before solving inverse problems, and there are many alternatives, such as the inversion method, the number of parameters, and the weightings in objective functions. The aim of this study is to discuss the process so that the alternatives can be assessed by evaluating their reliability. Multi-well transient flow tests, such as hydropulse testing, had been proposed for the treatment of heterogeneous media in subsurface flow modelling using data obtained through laboratory air injection tests for a dried heterogeneous sandstone plate. The numerical inversion method includes a quasi-Newton method and an adjoint-state method for pressure change rate matching that can deal with various types of objective functions. Porosity at the injection and observation points can be treated as different unknown parameters. Data quality and quantity are also important factors for the inverse problem. Several injection patterns were carried out to assess the observation pattern. The bootstrap re-sampling method was used to evaluate the reliability of the inversion results. We prefer this method because it is not necessary to assume a probability density function and because the method can estimate prediction errors. Using the estimated reliability indices, we can assess the best model, i.e. the adequate combination of injection pattern and model structure and objective function, etc.",2006,,no
Creating a mathematical model from the realization of the measuring of a transient process of a mechanical system,"In the paper the method of creating the dynamical model of the mechanical system is considered. The transient process measured after the hammer blow is used as input data. Prony method with its improvements is used for the estimation of unknown parameters of the model. Using the FFT and IFFT procedures the sample data is decomposed into two parts: a high frequency component and a low frequency component. Each part is investigated separately. While the high frequency component was modeled by using a simple six poles model, the low frequency component was modeled by a single multiple pole. The correctness of the model was checked by RMSE criteria. The RMSE of the low frequency component model versus the multiplicity of the pole was investigated. The results show good agreement with experimental data. The full final model was used to get the amplitude-frequency response of the system.",2006,,no
Vital process of concrete based on fuzzy analytic hierarchy process and evaluation model of compatibility with environment,"In this article, the author proposed a new concept of the whole vital process of concrete and its compatibility with environment. Different actors which have influence on relations between the whole vital process of concrete and environment were studied and discussed. Several feasibility measures to. improve relations between the whole vital process of concrete and environment were put forward; also for the first time, the authors use AHP and fuzzy mathematics to offer a compatibility evaluation model of the vital process of concrete with environment. At last, an example was given, which illustrated an evaluation process based on fuzzy-AHP in detail. The model will be a useful tool for the researchers or decision makers to evaluate the relationship between concrete and environment, which may be helpful to sustainable development of concrete material and technology.",2006,,no
On a semiparametric regression model whose errors form a linear process with negatively associated innovations,"In this article, we are concerned with the regression model y(i) = x(i) beta + g (t(i)) + V-i (1 <= i <= n), where the known design points (x(i), t(i)), the unknown slope parameter beta, and the nonparametric component g are non-random and where the correlated errors V-i = Sigma(infinity)(j=-infinity)c(j)e(i-j), with Sigma(infinity)(j=-infinity)\c(j)\ < infinity and negatively associated e(i), are random variables. Under appropriate conditions, we study the asymptotic normality for the least squares estimator of beta and the nonparametric estimator of g(.). Moreover, strong convergence rates of these estimators are considered. Our results show that the nonparametric estimator of g(.) can attain the optimal convergence rate.",2006,10.1080/02331880600688163,no
Effect of cross correlations in error terms on the model selection criteria for the stationary VAR process,"In this paper we investigate the finite sample properties of several model selection criteria in case of bivariate Vector AutoRegressions (VARs) of order one and two by a simulation study, particularly focusing on the effects of the degree of cross correlation in the error terms in combination with the values of more or less extreme values of the characteristic roots of the VAR-process. The Monte Carlo experiments show that the degree of cross correlation has an influence on the model selection criteria.",2006,10.1080/13504850500392974,no
Towards a suite of metrics for business process models in BPMN,"In this paper we present a suite of metrics for the evaluation of business process models using BPMN notation. Our proposal is based on the FMESP framework, which was developed in order to integrate the modeling and measurement of software processes. FMESP includes a set of metrics to provide the quantitative basis necessary to know the maintainability of the software process models. This previously existent proposal has been used in this work as the starting point to define a set of metrics for the evaluation of the complexity of business process models defined with BPMN. To achieve this goal, the first step has been to adopt the metrics of FMESP, which can be directly used to measure business process models, and then, new metrics have been defined according to the particular aspects of the business processes and BPMN notation.",2006,,no
Can we assess the model complexity for a bioprocess: theory and example of the anaerobic digestion process,"In this paper we propose a methodology to determine the structure of the pseudo-stoichiometric coefficient matrix Kin a mass balance based model, i.e. the maximal number of biomasses that must be taken into account to reproduce an available data set. It consists in estimating the number of reactions that must be taken into account to represent the main mass transfer within the bioreactor. This provides the dimension of K The method is applied to data from an anaerobic digestion process and shows that even a model including a single biomass is sufficient. Then we apply the same method to the '' synthetic data '' issued from the complex ADM1 model, showing that the main model features can be obtained with two biomasses.",2006,10.2166/wst.2006.010,no
Monitoring the experiment process and diagnosing the experiment mistakes made by students with Petri net modeling,"In this paper, Petri net theory is taken into analyzing the Virtual Experiment (VE) in the Virtual Experiment Environment (V.E.E.). After applied the Petri net to V.E.E., the V.E.E. then can monitor the experiment progress and measure the learning effects of students when the students are doing experiment. Petri net is an approach that can be used as a visual-communication aid similar to flow charts, block diagrams, and networks. Therefore, the experiment steps are easily modeled by Petri net. According to the data which is collected by the V.E.E., there are two major error types that a student might make during an experiment. The first error type is the procedural error and the other one is measuring error. In the end of this paper, an experiment system is implemented. There were 72 3(rd) year students in junior high school involved in the experiment. In this experiment, we did both of pre-test and post-test in order to make sure the Petri net-enhanced V.E.E. is useful.",2006,,no
Optimizing the manufacturing supply chain for distilling process: Discussion based on the inverse minimum cost flow problem model (ID : 3-012),"In this paper, we introduce the inverse optimization problems' study into analyzing of the supply chain quantitatively. An overview of the general inverse minimum cost flow problem and its solving algorithms is presented. And based on the minimum cost flow problem model'of the manufacturing supply chain for distilling process, the orientation and/or goals for strategies on how to improving the supply chain's efficiency and effectiveness -- so as to realize the whole supply chain system's optimization -- are discussed quantitatively by management innovation. And in the end, a numerical example is given.",2006,,no
A decisional model for the performance evaluation of the logistic process: application to the hospital supply chain,"In this paper, we propose a generic decisional model allowing to estimate in a total way (physical flows, financial flows) plannings for any system contained in supply chain. We present how to integrate this model in a software suite dedicated to supply chain and call this type of software presenting a global decisional solution Advanced Budgeting and Scheduling. To show the generic character of the decisional approach, we apply the chaining of models suggested to the logistic process of a consultation ambulatory unit of a Hospital Supply Chain.",2006,10.1109/ICSSSM.2006.320502,no
Sequential empirical process in autoregressive models with measurement errors,"in this paper, we study the weak convergence of the sequential empirical process based on the residuals from autoregressive models with measurement errors. It is shown that the sequential empirical process converges weakly to the sum of a Gaussian process which is the limit of a sequential empirical process of certain p-dependent random variables and an additional term depending on the parameter estimators of the model. As an application, we discuss the change point problem in the distribution of the error process in the autoregressive model. We present the numerical result of a simulation study for an asymptotically distribution-free test. (c) 2005 Elsevier B.V. All rights reserved.",2006,10.1016/j.jspi.2005.07.010,no
Shared mental models and operational effectiveness: Effects on performance and team processes in submarine attack teams,"In this study submarine attack crews were studied during simulated attack operations. The aim of the study was to test whether knowledge about team members had an effect on performance and team processes. The design controlled for skills of the different operators. Briefly, this study demonstrated that knowledge about team members adds to performance, over and above the contribution from operational skills. This was evident for number of hits on target, amount of information exchange, and the type of information changed to a more controlling type of interaction when the attack teams operated. In addition, the data indicated less physiological arousal in teams with known team members. We attributed this effect to the shared mental models of team members when the attack teams operated under a condition of known team members.",2006,,no
A new force distribution calculation model for high-quality production processes,"Industrial robots have been introduced to the belt grinding of free-form surfaces in order to obtain high-quality products and high-efficiency. One of the critical problems of high-precision belt grinding is to compute the force distribution in the contact area between the workpiece and elastic grinding wheel. The finite element method (FEM) is the traditional way to solve such a contact problem. However, the FEM model is too time-consuming. Normally, a single calculation takes several minutes on a powerful PC, which is unacceptable for real-time simulations and on-line robot control. A new model based on a neural network (NN) technique is developed instead of the FEM model to calculate the force distribution. The new model approximates the old FEM model with an acceptable tolerance but can be executed much faster than FEM model. With this new model, real-time simulation and on-line robot control of grinding processes can be further conducted.",2006,10.1007/s00170-004-2229-x,no
Quality healthcare management and well-being through INTERLIFE services: New processes and business models,"INTERLIFE is a revolutionary product that can radically change the way healthcare services are offered by introducing new means for quality health care management by the healthcare providers, and by improving the patients', health providers' and citizens' quality of life. INTERLIFE is a technological and medical knowledge management and processing infrastructure able to support an early discharge and a continuous home monitoring service thus leading to reduction hospitalisation rates and to the increased efficacy of healthcare service delivery of patients suffering from chronic diseases such as CHF, COPD and Diabetes as well as a special category of acute health care related patients. Six test sites are participating in the validation trials, more specifically EAP Sardenya and MUTUAM in Spain, Hippokrateion Hospital and AHEPA Hospital in Greece, RAMIT in Belgium and University of Regensburg Medical Centre in Germany.",2006,10.1109/DDHH.2006.1624809,no
Evaluation of processed bovine cancellous bone matrix seeded with syngenic osteoblasts in a critical size calvarial defect rat model,"Introduction: Biologic bone substitutes may offer alternatives to bone grafting procedures. The aim of this study was to evaluate a preformed bone substitute based on processed bovine cancellous bone (PBCB) with or without osteogenic cells in a critical size calvarial defect rat model. Methods: Discs of PBCB (Tutobone (R)) were seeded with second passage fibrin gel-immobilized syngenic osteoblasts (group A, n = 40). Cell-free matrices (group B, n = 28) and untreated defects (group C; n=28) served as controls. Specimens were explanted between day 0 and 4 months after implantation and were subjected to histological and morphometric evaluation. Results: At I month, bone formation was limited to small peripheral areas. At 2 and 4 months, significant bone formation, matrix resorption as well as integration of the implants was evident in groups A and B. In group C no significant regeneration of the defects was observed. Morphometric analysis did not disclose differences in bone formation in matrices from groups A and B. Carboxyfluorescine-Diacetate-Succinimidylester (CFDA) labeling demonstrated low survival rates of transplanted cells. Discussion: Osteoblasts seeded into PBCB matrix display a differentiated phenotype following a 14 days cell culture period. Lack of initial vascularization may explain the absence of added osteogenicity in constructs from group A in comparison to group B. PBCB is well integrated and represents even without osteogenic cells a promising biomaterial for reconstruction of critical size calvarial bone defects.",2006,10.2755/jcmm010.003.06,no
Evaluation of immunochromatographic test kits for food allergens using processed food models,"It has been mandatory to label five allergenic substances (AS; egg, milk, wheat, buckwheat and peanut) in all processed foods, since April 2002 in Japan. Two kinds of ELISA kits have been provided as screening test kits for the Japanese official method. The kits have many advantages but some disadvantages, i.e., the kits are not necessarily suitable for daily monitoring in food manufacturing plants, because they require various analytical equipments and the use of complicated procedures. To overcome these drawbacks, we have developed other diagnostic kits based on immunochromatography that should enable more rapid and simple screening for food allergens. Then we examined the performance of these immunochromatographic test kits (IC kits) in terms of sensitivity, repeatability and cross-reactivity to AS proteins in 11 kinds of food models with various heating conditions and physical properties. We also examined processed food models including AS protein of constant concentration, using the IC kits and ELISA kits, and compared the results. The IC kits detected AS proteins at 5 mu g/g in the extracts from processed food models, and provided highly reproducible results. Cross-reactivity among the AS proteins was not observed. The results obtained using the IC kits showed performance equivalent to that of the ELISA kits we examined in unheating processed food models including AS proteins of constant concentration. The IC kits should be more suitable for daily monitoring in food manufacturing plants.",2006,10.3358/shokueishi.47.66,no
Towards a crystalline product quality prediction method by combining process modeling and molecular simulations,"It is shown for the first time that a combination of molecular simulations and process modeling can be used to predict crystalline product quality aspects such as the polymorphic fraction and crystal size distribution. Using molecular simulations the interfacial energy of 2D nuclei and the step free energy of 2D nuclei on crystal surfaces of different polymorphs are determined. These result in the nucleation and growth behavior of the polymorphs as a function of the supersaturation. The supersaturation development during polymorph crystallizations can be accounted for using dynamic process modeling. Furthermore, this study clearly demonstrates that a deeper understanding of polymorph crystallization can be obtained by combining research on a molecular scale with that on a process scale. Molecular simulations show, for instance, that there is no simple relationship between experimental solubility and apparent interfacial energy of the polymorphs.",2006,10.1002/ceat.200500379,no
Towards anchoring software measures on elements of the process model,"It is widely accepted that software measurement should be automated by proper tool support whenever possible and reasonable. While many tools exist that support automated measurement, most of them lack the possibility to reuse defined metrics and to conduct the measurement in a standardized way. This article presents an approach to anchor software measures on elements of the process-model. This makes it possible to define the relevant software measures independently of a concrete project. At project runtime the work breakdown structure is used to establish a link between the measurement anchor points within the process model and the project entities that actually have to be measured. Utilizing the project management tool Maven, a framework has been developed that allows to automate the measurement process.",2006,,no
A comprehensive model of stress generation and relief processes in thin films deposited with energetic ions,"Levels of intrinsic stress in thin films deposited using energetic condensation of ions and/or atoms are known to depend strongly on the energy of the depositing species. At low energies (up to a few eV) thin films are often observed to have a large number of voids and show tensile stress. As the energy increases, the stress is observed to rise to a maximum at all intermediate energy-usually at a few tens to a few hundreds of eV. Thereafter as the energy is increased further, the intrinsic stress is found to decrease. Variations on this ""universal"" behaviour occur when species of two quite different energies are deposited whereupon the higher of the two energies seems to determine the level of intrinsic stress even when the fraction of ions at that energy is comparatively small. This finding has led to the use of the plasma immersion ion implantation (Pill) method during film deposition to produce coatings with low stress. Although 98-99% of the ions impacting the substrate have energies in the range which produces very high intrinsic stress, the intrinsic stress in the film is determined predominantly by the 1-2% of ions with high energy (kV and above). Recent experiments show that stress relief call be achieved in films deposited with high levels of stress by post-deposition ion implantation with a non-condensing species, Such as argon. We carried Out in-situ ellipsometric Studies during the implantation and relaxation process, and found that the thickness of the film increased. The increase in thickness in carbon was consistent with a transition of material in the treated volume from sp(3) rich to sp(2) rich chemical bonding. Atomistic simulations also indicate that the film expands by raising the surface when stress is relieved by energetic bombardment. Experiments with a variety of materials show that a phase transition or change in preferred orientation occurs during the stress relief process. This paper will explore the physical processes involved in determining stress levels in thin films using experimental results together with atomistic simulation. Experimental results of intrinsic stress as a function of ion energy when de bias is applied, so that a high proportion of the depositing species are affected, are compared to results of PIII&D processes where only a small proportion of ions are affected by the applied bias. Mechanisms and simple models of stress generation and relief are proposed and compared with experiment. The carbon system is examined in detail and the effects of thermal annealing (also found to reduce stress but with different effects on microstructure) are compared with that of high energy ion bombardment. The differences are explained in terms of the model presented. (c) 2005 Published by Elsevier B.V.",2006,10.1016/j.surfcoat.2005.02.161,no
Approaches for model validation: Methodology and illustration on a sheet metal flanging process,"Model validation has become an increasingly important issue in the decision-making process for model development, as numerical simulations have widely demonstrated their benefits in reducing development time and cost. Frequently, the trustworthiness of models is inevitably questioned in this competitive and demanding world. By definition, model validation is a means to systematically establish a level of confidence of models. To demonstrate the processes of model validation for simulation-based models, a sheet metal flanging process is used as an example with the objective that is to predict the final geometry, or springback. This forming process involves large deformation of sheet metals, contact between tooling and blanks, and process uncertainties. The corresponding uncertainties in material properties and process conditions are investigated and taken as inputs to the uncertainty propagation, where metamodels, known as a model of the model, are developed to efficiently and effectively compute the total uncertainty/variation of the final configuration. Three model validation techniques (graphical comparison, confidence interval technique, and r(2) technique) are applied and examined; furthermore, strength and weakness of each technique are examined. The latter two techniques offer a broader perspective due to the involvement of statistical and uncertainty analyses. The proposed model validation approaches reduce the number of experiments to one for each design point by shifting the evaluation effort to the uncertainty propagation of the simulation model rather than using costly physical experiments.",2006,10.1115/1.1807852,no
Modeling and representation of geometric tolerances information in integrated measurement processes,"Modeling and representation of geometric tolerances information across an enterprise is viable due to the advances in Internet technologies and increasing integration requirements from industry. In Integrated Measurement Processes (IMP), geometric tolerances data model must support different models from several well-defined standards: including ASME Y14.5M- 1994, STEP, DMIS and others. In this paper, we propose a layered conformance level geometric tolerances representation model. This model uses the widely applied ASME Y14.5M-1994 as its foundation layer by abstracting most information from this standard. The additional geometfic tolerances information defined by DMIS and STEP is incorporated into this model to form corresponding conformance layers that support IMP. Thus, different application domains in an enterprise can use this data model to exchange product information. This model is further transformed with XML Schema that can be used to generate XML instance file to satisfy geometric tolerances representation requirements in IMP. (c) 2005 Elsevier B.V. All rights reserved.",2006,10.1016/j.compind.2005.09.004,no
Applicable models of industrial processes based on process understanding: Acrylamide prediction,"Modelling is widely used in food processing, and a large number of papers present a variety of models for the often complex physical, chemical and biological processes going on in food products. This extended abstract presents a modelling approach that focuses on the transparency of the model constants, to give the model robustness and better applicability. Based on a discussion of different modelling approaches an example is given showing that the well known thermal inactivation kinetic model can also be used for predicting the formation of acrylamide as a function of time and temperature in a cereal product in an industrial environment. The model constants (only two) specifically relate to the process investigated.",2006,,no
A validated through process model to predict the fatigue life of a cast A356 automotive wheel,Models describing the critical steps in the manufacturing process of aluminum alloy wheels were integrated with a model of in-service loading to predict component fatigue performance. A multiscale solidification model was coupled with models of the subsequent heat treatment and machining to predict the residual stress distribution in a finished wheel. A second multiscale model was incorporated into a service model to relate the pore size and local stress state to fatigue performance. The predictions resulting from this through process modeling approach have been validated by comparisons with a series of measurements: 1) thermocouple measurements during casting; 2) characterization of pore size and distribution using X-ray microtomography and optical metallography; 3) residual strain measurements on finished wheels; 4) in-service strain measurements during a rotating bend test; and 5) in-service fatigue performance during rotating bend tests.,2006,,no
Evaluating uncertainty introduced to process-based simulation model estimates by alternative sources of meteorological data,"Models that represent biophysical processes in hydrology, ecology and agricultural systems, when applied at specific locations, can make estimates with significant errors if meteorological input data are not representative of the sites. This is particularly important where the estimates from the models are used for decision support, strategic planning and policy development, due to the impacts of introduced uncertainty. This paper investigates the impacts of meteorological data sources on a cropping systems simulation model's estimate of crop yield, and quantifies the uncertainty that arises when site-specific weather data are not available. In the UK, as elsewhere, many meteorological stations record precipitation and air temperature, but very few also record solar radiation, a key driving input data set. The impacts of using on-site observed precipitation and temperature with estimated solar radiation, and off-site entirely observed meteorological data was tested on the model's yield estimates. This gave two scenarios: on-site observed versus partially modelled data; and on-site observed versus substitute data from neighbouring sites, for 24 meteorological stations in the UK. The analysis indicates that neighbouring meteorological stations can often be an inappropriate source of data. Of the 24 stations used, only 32% of the nearest neighbours were able to provide the best substitute off-site data. On-site modelled data provided better results than observed off-site data. The results demonstrate that the range of alternative data sources tested are capable of having both acceptable and unacceptable impacts on model performance across a range of assessment metrics, i.e. on-site data sources each produced yield over- or underestimate errors greater than 2 t ha(-1). A large amount of uncertainty can be introduced to the model estimates due to the data source. Therefore, the applications of models that represent biophysical process where meteorological data are required, need to include the quantification of input data errors and estimate of the uncertainty that imperfect data will introduce. (c) 2005 Elsevier Ltd. All rights reserved.",2006,10.1016/j.agsy.2005.07.004,no
Automatic target detection and recognition in the process of interaction between visual and object buffers of scene understanding system based on network-symbolic models - art. no. 62340A,"Modem computer vision systems lack human-like abilities to understand the visual scene, detect, and unambiguously identify and recognize objects. Bottom-up grouping can rarely be effective for real world images if applied to the whole image without having clear criteria of how to further combine obtained small distinctive neighbor regions into meaningful objects. ATR systems that are based on the similar principles become dysfunctional if a target doesn't demonstrate remarkably distinctive and contrasting features that allow for unambiguous separation from background and identification. However, human vision unambiguously separates any object from its background and recognizes it, using a rough but wide peripheral system that tracks motions and regions of interests, and narrow but precise foveal vision that analyzes and recognizes the object in the center of a selected region of interest, and visual intelligence that provides scene and object contexts and resolves ambiguity and uncertainty in the visual information. Biologically-inspired Network-Symbolic models convert image information into an ""understandable"" Network-Symbolic format, which is similar to relational knowledge models. The equivalent of interaction between peripheral and foveal systems in the network-symbolic system is achieved via interaction between Visual and Object Buffers and top-level knowledge system. This interaction provides recursive rough context identification of regions of interest in the visual scene and their analysis in the object buffer for precise and unambiguous separation of the target from clutter with following the recognition of the target.",2006,10.1117/12.662241,no
Processing of visual information in the visual and object buffers of scene understanding based on network-symbolic models - art. no. 624603,"Modem computer vision systems suffer from the lack of human-like abilities to understand a visual scene, detect, unambiguously identify and recognize objects. Bottom-up fine-scale segmentation of image with grouping into regions can rarely be effective for real world images if applied to the whole image without having clear criteria of how further to combine obtained small distinctive neighbor regions into meaningful objects. On a certain scale, an object or a pattern can be perceived just as an object or a pattern rather than a set of neighboring regions. Therefore, a region of interest, where the object or pattern can be located, must be established first. Rough but wide peripheral human vision serves to this goal, while narrow but precise foveal vision analyzes and recognizes the object from the center of the region of interest after separating it from its background. Unlike the traditional computer vision models, biologically-inspired Network-Symbolic models convert image information into an ""understandable"" Network-Symbolic format, which is similar to relational knowledge models. The equivalent of interaction between peripheral and foveal systems in the network-symbolic system is achieved via interaction of the Visual and Object Buffers and the top-level knowledge system. This article describes the principles of data representation and processing of information in Visual and Object buffers that allow for scene analysis and understanding with identification and recognition of objects in the visual scene.",2006,10.1117/12.662168,no
Conformance testing: Measuring the fit and appropriateness of event logs and process models,"Most information systems log events (e.g., transaction logs, audit traits) to audit and monitor the processes they support. At the same time, many of these processes have been explicitly modeled. For example, SAP R/3 logs events in transaction logs and there are EPCs (Event-driven Process Chains) describing the so-called reference models. These reference models describe how the system should be used. The coexistence of event logs and process models raises an interesting question: ""Does the event log conform to the process model and vice versa?"". This paper demonstrates that there is not a simple answer to this question. To tackle the problem, we distinguish two dimensions of conformance: fitness (the event log may be the result of the process modeled) and appropriateness (the model is a likely candidate from a structural and behavioral point of view). Different metrics have been defined and a Conformance Checker has been implemented within the ProM Framework.",2006,,no
"Optimization and modeling in the co-processing of wastes in cement industry comprising cost, Quality and environmental impact using SQP, Genetic Algorithm, and differential evolution","Nowadays the high degree of the industrial activity as well as the increasing society life standard have been accompanied by a growing waste generation which represents one of the most serious environmental problems. The possibility use of some industrial wastes in the cement production, as an alternative source of secondary raw materials, as well as alternative secondary fuels have been a viable path to reduce the cement industries production cost. The main concerns about the use of these fuels are the effects in the cement performance and the environmental impacts that they can cause. Through an optimization model the influence of these fuels in the cement Portland properties are analyzed. In this model the Sequential Quadratic Programming (SQP), Genetic Algorithm (GA) and Differential Evolution (DE) are applied taking account the raw material, fuels cost, clinker quality and the environmental impact, such as the consumption of the energy requested in the grinding for the cement production.",2006,,no
FMESP: Framework for the modeling and evaluation of software processes,"Nowadays, organizations face with a very high competitiveness and for this reason they have to continuously improve their processes. Two key aspects to be considered in the software processes management in order to promote their improvement are their effective modeling and evaluation. The integrated management of these key aspects is not a trivial task, the huge number and diversity of elements to take into account makes it complex the management of software processes. To case and effectively support this management, in this paper we propose FMESP: a framework for the integrated management of the modeling and measurement of software processes. FMESP incorporates the conceptual and technological elements necessary to ease the integrated management of the definition and evaluation of software processes. From the measurement perspective of the framework and in order to provide the support for the software process measurement at model level a set of representative measures have been defined and validated. (C) 2006 Elsevier B.V. All rights reserved.",2006,10.1016/j.sysarc.2006.06.007,no
Evaluating treatment process redesign by applying the EFQM Excellence Model,"Objective. To evaluate a treatment process redesign programme implementing evidence-based treatment as part of a total quality management in a Dutch addiction treatment centre. Method. Quality management was monitored over a period of more than 10 years in an addiction treatment centre with 550 professionals. Changes are evaluated, comparing the scores on the nine criteria of the European Foundation for Quality Management (EFQM) Excellence Model before and after a major redesign of treatment processes and ISO certification. Results. In the course of 10 years, most intake, care, and cure processes were reorganized, the support processes were restructured and ISO certified, 29 evidence-based treatment protocols were developed and implemented, and patient follow-up measuring was established to make clinical outcomes transparent. Comparing the situation before and after the changes shows that the client satisfaction scores are stable, that the evaluation by personnel and society is inconsistent, and that clinical, production, and financial outcomes are positive. The overall EFQM assessment by external assessors in 2004 shows much higher scores on the nine criteria than the assessment in 1994. Conclusion. Evidence-based treatment can successfully be implemented in addiction treatment centres through treatment process redesign as part of a total quality management strategy, but not all results are positive.",2006,10.1093/intqhc/mzl033,no
Model for defining and reporting reference-based validation protocols in medical image processing,"Objectives Image processing tools are often embedded in larger systems. Validation of image processing methods is important because the performance of such methods can have an impact on the performance of the larger systems and consequently on decisions and actions based on the use of these systems. Most validation studies compare the direct or indirect results of a method with a reference that is assumed to be very close or equal to the correct solution. In this paper, we propose a model for defining and reporting reference-based validation protocols in medical image processing. Materials and methods The model was built using an ontological approach. Its components were identified from the analysis of initial publications (mainly reviews) on medical image processing, especially registration and segmentation, and from discussions with experts from the medical imaging community during international conferences and workshops. The model was validated by its instantiation for 38 selected papers that include a validation study, mainly for medical image registration and segmentation. Results The model includes the main components of a validation procedure and their inter-relationships. A checklist for reporting reference-based validation studies for medical image processing was also developed. Conclusion The proposed model and associated checklist may be used in formal reference-based validation studies of registration and segmentation and for the complete and accurate reporting of such studies. The model facilitates the standardization of validation terminology and methodology, improves the comparison of validation studies and results, provides insight into the validation process, and, finally, may lead to better quality image management and decision making.",2006,10.1007/s11548-006-0044-6,no
Psychological processes underlying the development of a chronic pain problem - A prospective study of the relationship between profiles of psychological variables in the fear-avoidance model and disability,"Objectives: Understanding the psychological processes that underlie the development of a chronic pain problem is important to improve prevention and treatment. The aim of this study was to test whether distinct profiles of variables within the fear-avoidance model could be identified and could be related to disability in a meaningful way. Methods: In 81 persons with a musculoskeletal pain problem, cluster analysis was used to identify subgroups with similar patterns on fear and avoidance beliefs, catastrophizing, and depression. The clusters were examined cross-sectionally and prospectively on function, pain, health care usage, and sick leave. Results: Five distinct profiles were found: pain-related fear, pain-related fear + depressed mood, medium pain-related fear, depressed mood, and low risk. These subgroups were clearly related to outcome. In contrast to the clusters ""medium pain-related fear"" and ""low risk,"" the majority of those classified in the clusters ""pain-related fear,"" ""pain-related fear + depressed mood,"" and ""depressed mood"" reported long-term sick leave during follow-up. The subjects in the clusters with high scores on the depression measure reported the highest percentage of health care usage during follow-up (70% in the ""pain-related fear + depressed mood"" group and 42% in the ""depressed mood"" group reported > 10 health care visits). Conclusions: Distinct profiles of psychological functioning could be extracted and meaningfully related to future disability. These profiles give support to the fear-avoidance model and underscore the need to address the psychological aspects of the pain experience early on.",2006,10.1097/01.ajp.0000159582.37750.39,no
Quantitative model for evaluating the quality of an automotive business process,"One of the key issues to business process control is the identification of measurable process attributes. For manufacturing processes these are typically physical parameters of the process (e.g. temperature, set points) or physical attributes of the manufactured product (e.g. dimension, functional performance). However, for business processes the metrics are more abstract. The challenge has been to develop metrics that capture the contributing subtle and hard to measure factors for business process control. This paper presents an analytical model that uses the weights-of-evidence concept to convert answers to audit or self-assessment questions into a single numerical process quality index. This index is used to forecast process success or failure and monitor its performance from start to end. The application of the approach is illustrated with an automotive industry product development sub-process where the process performance metric is the field warranty data, i.e. incidents per thousand vehicles (IPTV). The analytical model converts process self-assessment (failure mode and effect analysis) questions into a single numeric process quality index. The validity of the model is reflected in the strength of the correlation between the index and the IPTV results. Also, in this paper a measure is developed for identifying critical process quality assessment questions. This measure quantifies the deviation in the automotive business process that should have more focus. The significance of the analytical model proposed in this research is that the project managers or quality assurance auditors may be able to use the metric to predict product quality at any point in the product development process.",2006,10.1080/00207540500371949,no
The condition of uniqueness in manufacturing process representation by performance/quality indicators,"One of the most critical aspects in operations management is making firm goals representable. This is usually done by translating the organization results and objectives in to 'performance measures'. The scientific literature shows many applications in different fields such as quality, production, logistics, marketing, etc. Nevertheless, a general theory formalizing the basic and application concepts is still lacking. This paper represents a first attempt to provide a mathematical structure to the concept of an indicator. A set of basic definitions is introduced with the aim of giving a rigorous explanation of the concept of a 'performance indicator'. Particular attention is dedicated to the condition of 'uniqueness'. When dealing, for example, with performance evaluations of a given manufacturing plant, a practical way is to define some indicators which make tangible the different aspects of the system at hand. In this case, indicators such as throughput, defectiveness, output variability, efficiency, etc. are commonly employed. However, going on into the problem, many questions arise: 'How many indicators shall we use?"". 'Is there an optimal set?' 'Is this set unique?' 'If not, what is the best one (if it exists)?', 'Can all these indicators be aggregated in a unique one?"", 'Are indicators the same as measurements?', and so on. The aim of this paper is to give an answer to all these questions. A general theory which specifically faces this topic is presented. All the introduced concepts are explained and discussed by the use of practical examples. Copyright (C) 2006 John Wiley & Sons, Ltd.",2006,10.1002/qre.762,no
Auto CD-SEM edge-placement error for OPC and process modeling,"OPC models for leading-edge lithography must be calibrated with empirical data, and the measure of mismatch between design-intent and CD-SEM data is termed edge-placement error (EPE). A new methodology of EPE measurement uses, an automated CD-SEM as part of an OPC model building and process qualification flow. Design-based classification of edges and EPE defines many orientations such as line-end-adjacent, line-end, corner, and other critical gate edges. Also, the CD-SEM edge contours can be converted into an OASIS file for the construction of a process variability band (PVband) to quantify variability for all two-dimensional (2D) contexts.",2006,,no
Toward an integrated open source quality model a process-based approach,"Open source software (OSS) has recently become more and more interesting for companies, organizations and public administration. However, the development of open source is often perceived by companies as being of low quality, due to its economy, team structure or philosophy. In general, the quality of software development processes is assessed by the utilization of quality models, frameworks or maturity models. While in the closed source world there exists a variety of models and standards (e.g. CMMI, SPICE), much less research is known taking the characteristics of the open source development processes into account. The few existing approaches focus their views mainly on small sets of practices varying from approach to approach. An integrated model based on existing approaches is not known. This paper fills this gap by specifying an appropriate model.",2006,,no
Oxygen uptake rate measurements both by the dynamic method and during the process growth of Rhodococcus erythropolis IGTS8: Modelling and difference in results,"Oxygen uptake rate (OUR) during growth of Rhodococcus erythropolis IGTS8, a natural desulphurizing bacterium, is measured using two experimental methods for the same set of experiments performed in a 2 L work volume commercial bioreactor. The experimental OUR values obtained by the usual ""dynamic method"" were compared to those obtained by the ""process method"" after fitting the values of oxygen concentration measured during cell growth process. Experimental OUR values obtained from the 'process method' are always higher than those from the `dynamic method'. This discrepancy is explained by the cell economy principle: during the few minutes employed to measure OUR using the dynamic method, the cells do not use oxygen to synthesize 4S desulphurization route enzymes. The experimental values obtained by both methods are modelled to describe oxygen consumption using three parameters: consumption for growth (Y-OX) and for maintenance (m(O2)), as usual, and the oxygen consumption to synthesize Dsz enzymes by means of a yield of oxygen into 'biodesulphurization capability' (Y-OBDS). Oxygen concentration profile during growth is closely described using the final model proposed. (c) 2006 Elsevier B.V. All rights reserved.",2006,10.1016/j.bej.2006.09.025,no
Model-based evaluation of struvite recovery from an in-line stripper in a BNR process (BCFS (R)),"Phosphate removal and recovery can be combined in BNR processes, This may be realised by struvite precipitation from the supernatant of the sludge in anaerobic compartments. This can be beneficial for either improving bio-P removal effluent quality or lowering the influent COD/P ratio required for bio-P removal. For this reason, a patented BNR process, BCFS (R), was developed and applied in The Netherlands. Several questions relating to P-recovery and behaviour of the system remain unclear and need to be ascertained. For this purpose, a modelling technique was employed in this study. With the help of a previous developed model describing carbon oxidation and nutrient removal, three cases were fully simulated. The simulations demonstrated that there was an optimal stripping flow rate and P-recovery would increase in costs and bio-P activity might be negatively affected due to decreased bio-P efficiency if this value was exceeded. The simulations indicated that the minimal CODbiod/P ratio required for the effluent standard (1g P/m(3)) could be lowered from 20 to 10 with 36% of P-recovery. A simulation with dynamic inflow revealed that the dynamic influent loads affected slightly the anaerobic supernatant phosphate concentration but the effluent phosphate concentration would not be affected with regular P-recovery.",2006,10.2166/wst.2006.092,no
Kinetic model of energy transfer processes between low-coordinated ions on MgO by photoluminescence decay measurements,"Photoluminescence decay studies of emitting species on MgO nonocubes at room temperature provide evidence of three surface species characterized by an excitation and emission wavelength couple {lambda(exc);lambda(em)}. Species A corresponds to {lambda(exc)=240 nm; lambda(em)=380nm}, whereas the couple {lambda(exc)=280nm; lambda(em)=470nm} is assigned to two species: B and B', the former is involved in energy transfer from excited state A* and the latter in direct emission from excited state B'* A simple model for energy trans-fer from species A* to B is proposed. The numerical resolution of equations corresponding to this model is in good agreement with experimental data. This method quantifies the kinetics of intrinsic emission and energy transfer processes. Lifetime values indicate that phosphorescence is taking place, and species A, B and B' are identified as edge O(4O)(2-) corner O(3C)(2-) and kink O(3C)(2-) oxide ions respectively.",2006,10.1002/cphc.200500580,no
Validation of a compartmental population balance model of an industrial leaching process: The Silgrain (R) process,"Population balance (PB) models have become the most widely used tool for dynamic modeling of particulate processes. The application of PB models for prediction purposes is attracting significant interest. A parameter estimation and a quantitative validation of PB models should be carried out before the model can be applied for prediction. PB models are large-scale and nonlinear in the parameters. Moreover, the availability of measurements is typically limited, especially at industrial level, which makes the parameters poorly identifiable from experimental data. This paper shows how a systematic method for analyzing parameter sensitivity and collinearity among parameters, provides a subset of parameters that can easily be identified from the available data. A compartmental PB model of an industrial hydrometallurgical leaching plant is developed. Parameter identifiability of the model parameters is analyzed, and experimental data from the industrial plant are used to identify the corresponding subset of parameters and to verify some of the main assumptions of the model. (c) 2005 Elsevier Ltd. All rights reserved.",2006,10.1016/j.ces.2005.01.047,no
Statistical validation and an empirical model of hydrogen production enhancement found by utilizing a controlled acoustic field in the steam-reforming process,"Preliminary studies have shown that a controlled sound field can produce enhancement of hydrogen production in a steam-reforming process. This paper expands on preliminary investigations to explain a statistical empirical model that was developed for space velocity parameters of reactor length and flow rate as well as for acoustic sound pressure level. The effects of these parameters are shown on the output of fuel conversion within a methanol-steam reformer. The experimental setup is discussed as well as the factorial method employed. Although the results of this model are particular to the experimental setup specified, this exercise statistically proves the validity of acoustic enhancement of steam reformation. The empirical model is derived and uncertainty analysis of the model takes place. Curvature was investigated for the model and was found significant for the parameter of reactor length. Linear model relationships were found adequate for sound pressure level and flow rate within the limits investigated. Preliminary results regarding the effect of frequency on the process are also discussed. (c) 2006 International Association for Hydrogen Energy. Published by Elsevier Ltd. All rights reserved.",2006,10.1016/j.ijhydene.2006.01.003,no
Simultaneous Dynamic Validation/Identification of Mechanistic Process Models and Reconciliation of Industrial Process Data,"Process models are subject to parametric uncertainty and raw process-instrumentation data are corrupted by systematic and random errors. In this work, we present a framework for dynamic parameter estimation and data reconciliation aiming at integrating model-centric support tools and industrial process operations. A realistic case-study for the rectification of the mass balance of an industrial continuous pulping system is presented. The incentive for gross-error estimation during model-based production accounting and inventory analysis is demonstrated.",2006,,no
Total Quality Management and process modeling for PLM in SME,"Product Lifecycle Management (PLM) as a business strategy is becoming a must not only for big companies but also for small and medium enterprises SME's that consider product development a core competency. However, a PLM solution deeply impacts the business process and requires the analysis and, if necessary, the re-engineering of the process itself. This paper presents the application of process modeling and simulation techniques for the implementation of a PDM/PLM system within a SME using Total Quality Management procedures to integrate process analysis. Two As-Is models have been realized: the first, extracting the process knowledge from Total Quality Management procedures, and the second, interviewing a company's staff. A gap analysis has been carried out to identify first, which aspects of the process could be modified and improved introducing a PDM system, and then to forecast a complete extension to the PLM paradigm. A re-engineered process, described by the To-Be model, has been designed and compared with the As-Is models using a discrete events simulator. On the basis of simulation results, considerations have been drawn related both to the new process asset and its future evolution to full implementation of the PLM paradigm.",2006,10.1007/1-84628-210-1_28,no
An evaluation approach to engineering design in QFD processes using fuzzy goal programming models,"Quality function deployment (QFD) is a product development process used to achieve higher customer satisfaction: the engineering characteristics affecting the product performance are designed to match the customer requirements. From the viewpoint of QFDs designers, product design processes are performed in uncertain environments, and usually more than one goal must be taken into account. Therefore, when dealing with the fuzzy nature in QFD processes, fuzzy approaches are applied to formulate the relationships between customer requirements (CRs) and engineering design requirements (DRs), and among DRs. In addition to customer satisfaction, the cost and technical difficulty of DRs are also considered as the other two goals, and are evaluated in linguistic terms. Fuzzy goal programming models are proposed to determine the fulfillment levels of the DRs. Differing from existing fuzzy goal programming models, the coefficients in the proposed model are also fuzzy in order to expose the fuzziness of the linguistic information. Our model also considers business competition by specifying the minimum fulfillment levels of DRs and the preemptive priorities between goals. The proposed approach can attain the maximal sum of satisfaction degrees of all goals under each confidence degree. A numerical example is used to illustrate the applicability of the approach. (C) 2004 Elsevier B.V. All rights reserved.",2006,10.1016/j.ejor.2004.10.004,no
A pattern-driven development process for quality standard-conforming business process models,"Quality management is a hot issue in most organisations and must be considered in the business processes of the organisation. Existing approaches on business process modelling provide neither explicit strategies to model quality requirements on business processes nor do they provide explicit support for the construction of business processes satisfying such quality requirements. In this paper, we present a pattern driven development process for modelling business processes with respect to given quality constraints. We introduce a visual pattern specification language based on UML Activity Diagrams that enables the expression of quality constraints as patterns. These patterns can be used in a forward-engineering development process which supports the business process designer in constructing business processes by applying patterns. Thereby, quality constraints can be integrated into the design of business processes seamlessly.",2006,,no
Understanding and modeling of thermofluidic processes in catalytic combustion,"Recent advances in the modeling of chemistry and transport in catalytic combustion, which have been fostered by complementary developments in in situ measurements of gas-phase thermoscalars, are reviewed. Key issues such as the implementation of proper sub-models for surface kinetics, low temperature gas-phase kinetics and interphase transport are presented, with emphasis on fuel-lean and fuel-rich steady catalytic combustion. The advent of in situ measurements of major and minor gas-phase species concentrations over the catalyst boundary layer has led to reactor configurations that can tolerate large transverse gradients thus allowing for kinetic investigations at high temperatures and realistic reactant compositions. It is shown that those measurements, when used in conjunction with multidimensional modeling, can elucidate the underlying hetero-/homogeneous kinetics and their interactions at industrially-relevant operating conditions. Turbulent transport, an issue of particular interest in gas-turbines, is also addressed. Experiments and simulations have shown that key to the aptness of near-wall turbulence models is their capacity to capture the strong flow laminarization induced by the heat transfer from the hot catalytic walls. New modeling directions that include direct numerical simulation (DNS) for transient catalytic combustion and lattice Boltzmann (LB) discrete velocity models for intraphase transport are briefly outlined. (c) 2006 Elsevier B.V. All rights reserved.",2006,10.1016/j.cattod.2006.06.047,no
A quantitative evaluation model using the ISO/IEC 9126 quality model in the component based development process,"Recently, software quality evaluation based on ISO/IEC 9126 and ISO/IEC 14598 has been widely accepted in various areas. However, these standards for software quality do not provide practical guidelines to apply the quality model and the evaluation process of software products. Thus, we present a quantitative evaluation model using the ISO/IEC 9126 quality model in the Component Based Development (CBD) process. Particularly, our evaluation mod(,l adopts a quantitative quality model which uses the weights of quality characteristics obtained through carefully selected questionnaires for stakeholder and Analytic Hierarchical Process (AHP). Moreover, we have also examined the proposed evaluation model with applying the checklists for the artifacts of the CBD to a small-scale software project. As a result, we believe that the proposed model will be helpful for acquiring the high quality software.",2006,,no
Web page recommendation using a stochastic process model,"Recommending Web pages based on the access patterns by previous visitors to a user visiting some Web site can be helpful if they are is not familiar with the site. To provide a proper recommendation service for users, it is necessary to efficiently mine the Web page access patterns from a huge amount of Web server log data. This paper presents an efficient Web page recommendation scheme using a stochastic process model based on the time-homogeneous DTMC (Discrete-Time Markov Chain). It views a series of Web page access as a discrete-time stochastic process and constructs a state transition matrix based on the relative frequency of moving from one page to another in the Web site. Then it uses the initial state transition matrix to compute the probability for each Web page to be accessed after some number of page references from the starting Web page and selects a page with the largest probability value for recommendation. Although this process involves a simple matrix multiplication, its computational overhead could be expensive if the number of pages in the Web site is very large. Our approach employs the PageGather algorithm to find clusters of closely related ones among all the Web pages and uses each of these clusters as a state in the state transition matrix. As a result, the size of the transition matrix can be considerably reduced and so is the computation time without sacrificing the effectiveness of recommendation too much. We demonstrate the effectiveness and efficiency of our recommendation scheme with a series of experiments.",2006,10.2495/DATA060241,no
Empirical process based on the recursive residuals in functional measurement error models,"Recursive residuals are a linear transformation of ordinary residuals. They have frequently been suggested for testing model fit and model assumptions in linear regression. In this paper, we generalize the theory of the empirical process based on the residuals in the measurement error models to the recursive residuals in the measurement error models. We prove that recursive residuals in these models are asymptotically independent and identically distributed and show that, in this case, the weak convergence properties of the standardized residuals hold for the studentized recursive residuals. Furthermore, we look at some tests for goodness-of-fit based on the weak convergence of the empirical distribution of the recursive residuals. We justify the use of goodness-of-fit tests based on recursive residuals by carrying out a parametric bootstrap simulation study.",2006,10.1080/02331880600589262,no
Modelling and processing measurement uncertainty within the theory of evidence,"RFVs are variables defined within the theory of evidence, which are suitable for the representation of measurement results together with the associated uncertainty, whichever is its nature. This paper proposes a suitable mathematics for processing RFVs, which considers the different nature of the uncertainty effects. This allows to process measurement algorithms in terms of RFVs, so that the final measurement result (and all associated available information) is directly obtained as a RFV.",2006,,no
Modeling attitude to risk in human decision processes: An application of fuzzy measures,"Several models of the human decision process have been proposed, classical examples of which are utility theory and prospect theory. In recent times, the theory of fuzzy measures and integrals has emerged as an alternative meriting further investigation. Specifically, one is interested in the degrees of disjunction and conjunction and the veto and favor indices that represent the tolerance measure of the decision maker. Though several theoretical expositions have appeared in contemporary literature, empirical studies applying these concepts to the real world are scarce. This paper reports two studies based on a model of strategic telecommunication investment decisions from a research work involving a survey of executives. The first study involves building fuzzy models corresponding to each individual decision maker with the results grouped based on the decision makers' propensity to risk as determined by their degrees of disjunction. The Shapley indices and the interaction effects are determined for each pooled data set corresponding to each group. To contrast this approach with those of conventional nomothetic comparisons of decision policies, the decision makers are grouped based on a clustering analysis of the individual linear regression models. The data for each cluster are pooled and the fuzzy measures learned from the data set are analyzed for comparison purposes. The results not only serve as a demonstration of fuzzy measure analysis as a viable approach to studying qualitative decision making, but also provide useful methodological insights into applying fuzzy measures to strategic investment decisions under risk. (c) 2006 Elsevier B.V. All rights reserved.",2006,10.1016/j.fss.2006.06.010,no
Optophysical measurements - Modeling the process of propagation of a laser beam using the Kirchhoff method,Simultaneous measurement of the distribution of intensity and phase of a laser beam in ail arbitraty cross-section and modeling of the behavior of the beam along the axis of propagation by means of the Kirchhoff method are proposed.,2006,10.1007/s11018-006-0153-1,no
A proposal and empirical validation of metrics to evaluate the maintainability of software process models,"Software measurement is essential for understanding, defining, managing and controlling the software development and maintenance processes and it is not possible to characterize the various aspects of development in a quantitative way without having a deep understanding of software development activities and their relationships The current competitive marketplace calls for the continuous improvement of processes and as consequence companies have to change their processes in order to adapt to these new emerging needs. It implies the continuous change of their software process models and therefore, it is fundamental to facilitate the evolution of these models by evaluating its easiness of maintenance (maintainability). In this paper we introduce a set of metrics for software process models and discuss how these can be used as maintainability indicators. In particular, we report the results of a family of experiments that assess relationships between the structural properties, as measured by the metrics, of the process models and their maintainability. As a result a set of useful metrics to evaluate the software process models maintainability, have been obtained.",2006,10.1109/IMTC.2006.328377,no
Backward representation of Markov jump processes and related problems. II. Optimal nonlinear estimation,Solutions of the problem of optimal nonlinear smoothing (interpolation) of the state of a special Markov jump process from indirect observations in Wiener noise were obtained. The optimal nonlinear estimates were examined and compared with the corresponding optimal linear estimates described in Part I.,2006,10.1134/S0005117906090098,no
Objective and subjective evaluation of seven selected all-pole modelling methods in processing of noise corrupted speech,"Spectral modelling properties of seven selected all-pole modelling methods were compared by using both objective and subjective tests. Model behaviour was evaluated with vowel sounds corrupted by uncorrelated Gaussian and Laplacian background noise. Objective tests were computed with the logarithmic spectral differences (SD2) and subjective speech quality was assessed with the Degradation Category Rating (DCR) listening test. In both tests, the WLPC method, where the weighting function was the short time energy of the speech signal, gave the best results. The correlation between the objective and subjective results was found to be remarkably strong.",2006,,no
A New Signal-to-Noise-Ratio Based Stochastic Model for GNSS High-Precision Carrier Phase Data Processing Algorithms in the Presence of Multipath Errors,"Stochastic modelling is widely used to mitigate errors that are not completely modelled in the functional relationships used as the basis for modern GNSS phase processing algorithms. For short to medium baselines such relationships typically include corrections and/or parameters for multipath, ionospheric and tropospheric errors, and stochastic models therefore perform the role of remedying imperfections in these. Most simple stochastic models used in practice are primarily based on satellite elevation angle: as this angle decreases the weight of the measurement is decreased due to the assumption that the noise level of the functional model imperfections will increase. This generally works well for imperfections in the ionospheric and tropospheric models as the size of the total errors in both cases can be directly related to elevation angle, but imperfections in multipath error models are not always simply elevation angle dependent. This is because phase multipath errors are very sensitive to the exact satellite-reflector-antenna geometry. Even phase measurements to satellites with high elevation angles can have large multipath errors if there is a reflecting surface high above the antenna (e. g. in the case of a tall building) or if a reflector is tilted with respect to the antenna. Also changes in the satellite-reflector-antenna geometry (brought about either by antenna or satellite motion) produce cyclic rather than approximately linear changes in the size of the phase multipath error. For instance a change in the additional path length of a reflected signal of only half a wavelength (9.5cm for GPS L1) will lead to a phase shift (phase difference between the direct and reflected signals) of the same magnitude but opposite sign. Therefore, whilst elevation angle dependent stochastic models can be used to reduce the effect of ionospheric and tropospheric residuals on positioning solutions, they may not mitigate fully phase multipath errors. To counteract these problems there have been a number of investigations into the use of the signal-to-noise ratio (SNR) to drive GNSS phase processing stochastic models. This is because SNR is well-known to be correlated to multipath error. However time series plots of phase multipath errors and SNRs always show that there is an about 90 degrees phase difference between the two, and existing methods do not take this into account leading to phase data without multipath sometimes being incorrectly down-weighted. This paper introduces and tests a new SNR-based stochastic modelling approach that does not suffer from this disadvantage. Two real data sets have been collected in well-controlled experimental sites, one in Laboratoire Central des Ponts et Chaussees (LCPC) near Nantes in France, and the other on campus of the University of Nottingham in UK. The data sets have been used to assess the performance of the proposed stochastic model in the presence of multipath errors. Positioning errors obtained with the proposed stochastic model are compared with those from standard least squares solutions. An improvement in accuracy of between 26% and 35% has been found with these data sets, with the level of improvement depending on the amplitudes and frequencies of multipath errors, and the phase coherence of SNRs and pseudorange multipath errors.",2006,,no
"The process of understanding to ""medium effect"" and its verification with modeling","The ""Medium Effect"" this paper introduces is a physical phenomenon of magnetic field distribution contrasts caused by different media around the magnetic body and the variation of media movement status, including in air, static water, static salted water. The paper also discusses the thinking and understanding process to the phenomenon of ""Medium Effect"" discovered in engineering explorations and the investigation to confirm it through modeling experiments. In general teaching materials of magnetic exploration, the adjacent media surrounding magnetic body are usually considered to be background Field, and without considering the media interference with the magnetic field distribution caused by magnetic body. The discovery and verification of ""Medium Effect"" can create some certain effects of applications, such as the study of geophysics, magnetic theory of physics, searching and position to magnetic targets under water, and magnetic survey in water areas.",2006,,no
Physically-based modelling of hydrological processes in a tropical headwater catchment (West Africa) - process representation and multi-criteria validation,"The aim of the study was to test the applicability of a physically-based model to simulate the hydrological processes in a headwater catchment in Benin. Field investigations in the catchment have shown that lateral processes such as surface runoff and interflow are most important. Therefore, the 1-D SVAT-model SIMULAT was modified to a semi-distributed hillslope version (SIMULAT-H). Based on a good database, the model was evaluated in a multi-criteria validation using discharge, discharge components and soil moisture data. For the validation of discharge, good results were achieved for dry and wet years. The main differences were observable in the beginning of the rainy season. A comparison of the discharge components determined by hydro-chemical measurements with the simulation revealed that the model simulated the ratio of groundwater fiuxes and fast runoff components correctly. For the validation of the discharge components of single events, larger differences were observable, which was partly caused by uncertainties in the precipitation data. The representation of the soil moisture dynamics by the model was good for the top soil layer. For deeper soil horizons, which are characterized by higher gravel content, the differences between simulated and measured soil moisture were larger. A good agreement of simulation results and field investigations was achieved for the runoff generation processes. Intertlow is the predominant process on the upper and the middle slopes, while at the bottom of the hillslope groundwater recharge and - during the rainy season - saturated overland flow are important processes.",2006,,no
"Representation of non-Gaussian, finite-states, reciprocal processes: the 1-D problem with cyclic boundary conditions","The aim of this paper is to propose a nonlinear approach in modeling and estimation for non-Gaussian discrete-index reciprocal processes. The sub-class of the finite-states processes is taken under consideration. Such a class of processes seems to be a suitable setup in many applications, and in particular it appears well suited for image-processing. For this class of processes a stochastic realization is shown to exist in the form of a fixed-degree polynomial model. In this perpective the present paper extends to a non-Gaussian case some, well known in the literature, representation results about Gaussian reciprocal processes.",2006,10.1109/CDC.2006.377715,no
The modeling and simulation of quality in process EDM cutting with wire electrode,"The article deals with mathematically modelling relations of technological parameters and quality parameters. Competition and scientific progress requires introduction Of technologies that perform challenging claims of modem production in automation field, from economy, environmental and energy efficiency point of view. Nonconventional cutting methods represent all of these claims. The EDM machining techniques are considered to be flexible tools in the processing of a wide range of materials without time loss by tool changing. In spite of great research effort and good knowledge in the field of progressive methods of cutting, there are still number of unexplained facts. One of them is influence of process factors on workpiece surface quality.",2006,,no
Job choice model to measure behavior in a multi-stage decision process,The article presents a job choice model allowing to measure the importance of items of employer images in a multi-stage decision process. Based on scientific research a model for the multi-stage decision process is presented that contains details on how decisions are made on each stage. A method using logistic regression to empirically validate the model is described and compared to an alternative method. The results of applying the method are presented and discussed.,2006,10.1007/3-540-31314-1_71,no
Effects of prototype and exemplar fit on brand extension evaluations: A two-process contingency model,"The brand extension literature suggests that consumers will favorably evaluate a brand extension when (a) it has high fit with the brand and (b) the brand has positive evaluations. We suggest that when a brand operates in multiple product domains, extension evaluations are more complex than have been conceptualized, and favorable consumer responses may result even in the absence of the above two conditions. Our two-process contingency model proposes two dimensions of fit (brand prototype fit and product exemplar fit) and two evaluative processes (top-down and parallel attitude transfer) that drive extension evaluations in different ways, depending on the level of cognitive resources. Three empirical studies found consistent support for the model.",2006,10.1086/504134,no
Development of a multi-scale packing methodology for evaluating fate and transport processes of explosive-related chemicals in soil physical models - art. no. 62171U,"The development of the scalable systems and methods involves proper reproduction of soil composition, lithology and structures, appropriate placement of boundary conditions, suitable simulation of representative environmental conditions, and the use of representative sampling systems. This paper evaluates the effect of different packing methods with a tropical sandy soil for obtaining a uniform and homogeneous packing so that these characteristics are comparable across all scales and dimensions. The packing methods used include piston-driven dry packing, piston-driven wet packing, and gravity-driven sedimentation packing. For dry and wet packing, the procedure consisted on the iterative addition of soil layers, mixing and compaction. Sedimentation packing involved the preparation of soil slurry and allowing its deposition under gravity. The systems were evaluated for consistent bulk density, porosity, homogeneity, and soil dispersivity. Preliminary results exhibit satisfactory bulk density and porosity values for the piston-driven methods, ranging from 1.59 to 1.64 g cm(-3) and from 42 to 44%, respectively. Sedimentation packing results in fair homogeneity and gradation, while dry packing develops heterogeneous layering. Transport parameters were also evaluated resulting in consistent dispersivity values for wet piston-driven packing ranging from 0.09-0.19 cm. Wet piston-driven packing is recommended as they yielded the most reproducible results for tropical sandy soils. The reproducibility of the recommended method is tested and proven in other physical models of different scales and dimensions. The method herein developed are, therefore, applicable for the development of representative multidimensional physical models designed to simulate soil and environmental fate and transport processes occurring in field conditions where landmines and other explosive devices are present.",2006,10.1117/12.665879,no
Dynamic modeling and experimental validation of the MMA cell-cast process for plastic sheet production,"The dynamic modeling and experimental validation of the model response of the poly( methyl methacrylate) (PMMA) cell-cast process for plastic sheet production is presented. It is based on the straightforward initiation, propagation, and termination reaction polymerization mechanism. The sheet molding process was modeled by a two-dimensional dynamic mathematical model able to predict conversion, temperature, and molecular weight averages. The mathematical model is cast as a partial differential equations (PDEs) system that is discretized using the numerical method of lines. The resulting set of ordinary differential equations, representing the heat and mass balances for this polymerization system, are then solved by standard ordinary differential equations (ODE) solvers. Comparison of the model prediction capabilities against experimental temperature measurements taken at the extremes of the PMMA sheet being produced are presented.",2006,10.1021/ie060206u,no
The Development of Pragmatic Quality Model for Software Product Certification Process,"The emergence of Multimedia Super Corridor (MSC) in Malaysia in 1996 was the starting point for the blooming of software and ICT related companies. Despite that, not much attention was given to the quality of the software product that was being developed by different categories of companies. These companies could not justify the quality of their products to the users and the users are left with uncertainties on the quality of the software. Our previous survey identified the need for independent soft-ware assessment and certification. Therefore, we propose a conceptual model of soft-ware certification process by product quality approach. It is designed to be applied by an independent certification body or and appointed institution. The main focus of this paper is the development of pragmatic quality model that will be applied in the certification process.",2006,,no
Evaluation model for information systems benefits in construction management processes,"The importance of information systems (IS) implementation in construction management has grown rapidly, and therefore, a more practical method for IS planning is required. Prioritizing the construction management processes in which IS will be implemented is one of the key processes in IS planning, and for the prioritization purpose, the benefits of IS implementation must be examined. This research suggests an evaluation model for IS benefits in construction management processes. The model is based on the evaluation of IS implementation benefits at the construction management task level, and it is postulated that the benefits are composed of the effect and possibility of IS implementation. The research suggests four basic measures as the fundamental evaluation criteria and represents an overall evaluation process in the model. A case study is performed to validate this model via a statistical analysis, and the result shows that the four measures can explain at least 58% of the variance of expected benefit from IS implementation. The contributions of this paper can be summarized in two aspects. First, researchers may further enrich the understanding of IS benefits realization mechanism with the factors. Second, industry practitioners may use the model in their IS planning task.",2006,10.1061/(ASCE)0733-9364(2006)132:10(1114),no
UVB/UVC induced processes in model DNA helices studied by time-resolved spectroscopy: Pitfalls and tricks,"The investigation of model DNA helices using time-resolved absorption and fluorescence spectroscopy with UVB/UVC excitation knows currently an increasing interest due, in particular, to the use of femtosecond spectroscopy. The study of such complex and fragile systems presents specific difficulties which are not encountered in the experiments performed on the monomeric DNA units. They are related both to the quality of the DNA helices and their sensitivity towards UV radiation. The present paper tackles some of these problems (pitfalls) and describes experimental protocols developed in our laboratory in order to overcome them (tricks). We focus on experiments carried out by fluorescence upconversion spectroscopy, time-correlated single photon counting and nanosecond flash photolysis. We illustrate our experience with examples obtained for double helices containing only adenine-thymine base pairs. We consider this report of pitfalls and tricks, which is far from being complete, as a first step towards a codification of rules for time-resolved studies with model DNA helices. This codification has to be established by common agreement of the various groups working in the field. (c) 2006 Elsevier B.V. All rights reserved.",2006,10.1016/j.jphotochem.2006.05.029,no
"Evaluation of Karhunen-Loeve, spectral, and sampling representations for Stochastic processes","The Karhunen-Loeve, spectral, and sampling representations, referred to as the KL, SP, and SA representations, are defined and some features/limitations of KL-, SP, and SA-based approximations commonly used in applications are stated. Three test applications are used to evaluate these approximate representations. The test applications include (1) models for non-Gaussian processes; (2) Monte Carlo algorithms for generating samples of Gaussian and non-Gaussian processes; and (3) approximate solutions for random vibration problems with deterministic and uncertain system parameters. Conditions are established for the convergence of the solutions of some random vibration problems corresponding to KL, SP, and SA approximate representations of the input to these problems. It is also shown that the KL and SP representations coincide for weakly stationary processes.",2006,10.1061/(ASCE)0733-9399(2006)132:2(179),no
Photocatalytic degradation of model organic pollutants on an immobilized particulate TiO2 layer - Roles of adsorption processes and mechanistic complexity,"The kinetics of photocatalytic degradation of four different model organic compounds, formic acid (FA), oxalic acid (OA), 4-chlorophenol (4-CP) and the herbicide monuron (3-(4-chlorophenyl)-1,1-dimethylurea) in a self-constructed batch-mode plate photoreactor with a thin flow of contaminated aqueous solution circulating over an illuminated particulate layer of TiO2 P25 (Degussa) was compared. Both OA and FA were adsorbed on TiO2 surface; their mineralization, induced by direct transfer of photogenerated holes, proceeded in a single step, without observable intermediates, following approximately zero order kinetics. Numerical simulations were performed using a newly proposed kinetic model based on the photostationary state assumption. The model allowed an explanation of the observed reaction order as well as the comparison of independent with competitive adsorption of organic compound and oxygen on the photocatalyst surface, yielding a better fit for the case of competition. 4-CP and monuron, which were not adsorbed under the conditions used, were degraded through the action of photogenerated hydroxyl radicals. Their degradation proceeded with lower photoefficiency than for the adsorbed compounds (FA and OA). While the mineralization of both 4-CP and monuron followed zero order kinetics, their degradation was close to first order. The different reaction orders were consistently explained using the pbotostationary state approach. (c) 2005 Elsevier B.V. All rights reserved.",2006,10.1016/j.apcatb.2005.11.007,no
Modeling the impact of layout variation on process stress in Cu/low k interconnects,The layout dependence of process stress in Cu/low k interconnects are examined using various stress sources and layout patterns. The anisotropic grain growth stress model is compared with the conventional isotropic intrinsic stress model and the latter is found to underestimate stress concentrations in the dielectric regions near metal line ends. Both the grain growth stress in copper and the thermal mismatch stress in copper and low k dielectrics are considered in the layout dependence study. The results demonstrate that accurate stress evaluation in interconnect structures has to employ geometrical models that include layout variations. Capabilities are developed to extract these geometrical models directly from layout analysis.,2006,,no
Petri net model for enterprise's organizational efficiency of marketing environment risk management process,"The marketing environment risk management process is a complex system involving many independent organizations. Using the four-stage organizational information management principle, this paper gives a simplified enterprise's marketing environment risk management process. Based on this, the enterprise's marketing environment risk management process can be modeled by Petri net tool. Furthermore, Petri net model and simulations were established for this process. And the research indicates that both the above thought and the above technical means are feasible. Finally, the organizational efficiency was analyzed using the Markov state transfer probability method.",2006,,no
Backward representation of Markov jump processes and related problems. I. Optimal linear estimation,The Martingale representation for a class of special Markov jump processes in reverse time is derived and applied to study optimal linear filtering and smoothing of states of nonlinear observation systems.,2006,10.1134/S0005117906080042,no
MODELLING MEASUREMENT PROCESSES IN COMPLEX SYSTEMS WITH PARTIAL DIFFERENTIAL EQUATIONS: FROM HEAT CONDUCTION TO THE HEART,"The modelling of a measurement process necessary involves a mathematical formulation of the physical laws that link the desired quantity with the results of the measurement and control parameters. In simple cases, the measurement model is a functional relationship and can be applied straightforward. In many applications, however, the measurement model is given by partial differential equations that usually cannot be solved analytically. Then, numerical simulations and related tools such as inverse methods and linear stability analysis have to be employed. Here, we illustrate the use of partial differential equations for two different examples of increasing complexity. First, we consider the forward and inverse solution of a heat conduction problem in a layered geometry. In the second part, we show results on the integrative modelling of the heart beat which links the physiology of cardiac muscle cells with structural information on their orientation and geometrical arrangement.",2006,10.1142/9789812774187_0001,no
An intelligent modeling system to improve the machining process quality in CNC machine tools using adaptive fuzzy Petri nets,"The paper first presents an AND/OR nets approach for planning of a computer numerical control (CNC) machining operation and then describes how an adaptive fuzzy Petri nets (AFPNs) can be used to model and control all activities and events within CNC machine tools. It also demonstrates how product quality specification such as surface roughness and machining process quality can be controlled by utilizing AFPNs. The paper presents an intelligent control architecture based on AFPNs with learning capability for modeling a CNC machining operation and control of machining process quality. In this paper it will be shown that several ideas and approaches proposed in the field of robotic assembly are applicable to the planning procedure modeling with minor modifications. Graph theories, Petri nets, and fuzzy logic are powerful tools which are employed in this research to model different feasible states for performing a process and to obtain the best process performance path using exertion of the process designer's criteria.",2006,10.1007/s00170-005-2551-y,no
Second-order residual analysis of spatiotemporal point processes and applications in model evaluation,"The paper gives first-order residual analysis for spatiotemporal point processes that is similar to the residual analysis that has been developed by Baddeley and co-workers for spatial point processes and also proposes principles for second-order residual analysis based on the viewpoint of martingales. Examples are given for both first- and second-order residuals. In particular, residual analysis can be used as a powerful tool in model improvement. Taking a spatiotemporal epidemic-type aftershock sequence model for earthquake occurrences as the base-line model, second-order residual analysis can be useful for identifying many features of the data that are not implied in the base-line model, providing us with clues about how to formulate better models.",2006,10.1111/j.1467-9868.2006.00559.x,no
Highlighting the model code selection and application process in policy-relevant water quality modelling,"The paper illustrates and discusses some rather simple and pragmatic means to enhance the quality of model-based knowledge input for decision-making. Firstly, we apply a stepwise description of the modelling process to describe our ""case story"", in which we first define modelling objectives and then use the benchmark criteria approach to evaluate model codes and to select an appropriate model code to be set up, analysed, and applied to simulate the long-term (decades) phosphorus-phytoplankton interactions and thus the eutrophication problem in Lake Vansjo-Storefjorden, Norway. Results from the simulations with the MyLake model indicate that at least 50% reduction in phosphorus loads are needed in order to reach the strictest environmental goal in the long-term. We believe that by: (1) following (and documenting) the step-by-step modelling process outlined in this paper, by (2) using available (internet-based) inventories to get an overview of existing model codes, and by (3) performing and documenting the model code selection with help of the benchmark criteria approach, at least some of the barriers to model use in environmental management can be removed.",2006,10.1016/j.ecolmodel.2005.10.031,no
Fuzzy analytical hierarchy process for evaluating and selecting a vendor in a supply chain model,"The paper proposes a structured model for evaluating vendor selection using the analytical hierarchy process (AHP) and fuzzy AHP. The model is developed using evidence from an empirical study. The paper aims to demonstrate how the model can help in solving such decisions in practice. A usability evaluation of the AHP-based model with three vendors is discussed. It also examines the structure of the decision hierarchy, whether it can represent vendor selection decisions in reality and whether it covers all key factors affecting vendor selection choices. The effectiveness of the AHP model is illustrated using a company in the southern part of India and the results validated using fuzzy AHP.",2006,10.1007/s00170-005-2562-8,no
Validating instrument models through the calibration process - art. no. 62970O,"The performance of modem IR instruments is becoming so good that meeting science requirements requires an accurate instrument model be used throughout the design and development process. The huge cost overruns on recent major programs are indicative that the design and cost models being used to predict performance have lagged behind anticipated performance. Tuning these models to accurately reflect the true performance of target instruments requires a modeling process that has been developed over several instruments and validated by careful calibration. The process of developing a series of Engineering Development Models is often used on longer duration programs to achieve this end. The accuracy of the models and their components has to be validated by a carefully planned calibration process, preferably considered in the instrument design. However, a good model does not satisfy all the requirements to bring acquisition programs under control. Careful detail in the specification process and a similar, validated model on the government side will also be required. This paper discusses the model development process and calibration approaches used to verify and update the models of several new instruments, including Geosynchronous Imaging Fourier Transform Spectrometer (GIFTS) and Far Infrared Spectroscopy of the Troposphere (FIRST).",2006,10.1117/12.684348,no
Error cancellation modeling and its application to machining process control,"The product quality in a machining process can be affected by datum surface imperfections fixture locator errors, and machine tool errors. It has been previously observed that these errors can cancel out one another for certain features. The mathematical modeling and analysis of this phenomenon is currently an open issue. We use the concept of an Equivalent Fixture Error (EFE) embedded into a modeling methodology to obtain insights into this fundamental phenomenon and achieve an improved process control. Based on our process fault model we develop a sequential root cause identification procedure and EFE compensation methodology. A case study is presented to demonstrate the proposed diagnostic procedure. A simulation study is also performed to illustrate the error compensation procedure.",2006,10.1080/07408170500333392,no
Biodegradation process of alpha-TCP particles and new bone formation in a rabbit cranial defect model,"The purpose of the present study was to observe the biodegradation process of pure alpha-tricalcium phosphate (alpha-TCP) particles and to determine the efficacy of alpha-TCP as a space maintainer in a bone defect. We used 14 rabbits and prepared two Cranial bone defects in each rabbit. One defect was left empty as a control, whereas the other was filled with alpha-TCP particles about 300 mu m in diameter. Animals were sacrificed at 1 week, 4 weeks, and 8 weeks. The cranial bone was then embedded either in paraffin wax for the preparation of decalcified specimens, or in polyester resin for the preparation of nondecalcified specimens. All specimens were evaluated histologically and histomorphometrically. As a consequence of the degradation of alpha-TCP, a ""reticulate structure"" appeared in the particles at 1 week and new bone was observed in this structure at 8 weeks. The amount of new bone between the control and experimental groups was not significantly different at any of the time points. However, in the experimental group, new bone at the surface of alpha-TCP was evident even in the center of the defect whereas fibrous connective tissue was dominant in the control group. These results indicate that alpha-TCP is a degradable osteoconductive material that is able to act as a space maintainer for bone regeneration when applied to a bone defect. While there was no significant difference in total bone formation between the experimental and negative control groups, the space-maintaining and osteoconductive properties of the particles may result in more complete bone formation in longer-term studies. (c) 2006 Wiley Periodicals, Inc.",2006,10.1002/jbm.b.30540,no
Data quality management using business process modeling,"The quality of data contained in the enterprise information systems has significant impact, both from the internal business decision-making perspective and the external regulatory and shareholder obligations. This paper addresses data quality assessment in business processes by proposing a modeling framework to quantify the data quality in an information processing system. We present a business process modeling framework for data quality analysis and develop the mathematical formulation for error propagation. This is overlaid with a business controls framework where the placement and effectiveness of the controls alter the propagation of errors. This framework enables the estimation and management Of data quality when faced with changes in various aspects of the business process. It also allows the formulation of optimization problems that trade off the cost of business controls with the level or cost of the resultant data quality. We illustrate the modeling framework and analyses with a revenue management process.",2006,10.1109/SCC.2006.41,no
Cloud processing of gases and aerosols in a regional air quality model (AURAMS),"The representation of cloud chemistry and scavenging processes in a new multiple-pollutant (unified) regional air-quality modelling system, AURAMS, is described. Aqueous-phase chemistry and scavenging processes are coupled explicitly with microphysical fields from the meteorological driver model and the size- and chemical-composition-resolved aerosols in AURAMS. The impact of aqueous-phase oxidation on regional aerosols (primarily sulphate), both in terms of mass and size distribution, is examined based on model simulations of a 1-week period over eastern North America. It is shown that aqueous-phase oxidation contributes about 30% to 40% of the total atmospheric sulphate production in this case. Cloud chemistry is also shown to modify the aerosol size distribution, which in turn can either enhance or reduce aerosol scattering efficiency in different geographic regions depending on where on the aerosol size spectrum the mass is added. The study also indicates that precipitation evaporation can be an important process in terms of tracer redistribution in the vertical. Whether and how to treat tracer release from precipitation evaporation can have a significant impact on model predictions of near-surface ambient tracer concentrations and wet deposition fluxes. Comparison between observations and AURAMS predictions shows that modelling cloud processing of gas and aerosols depends critically on the meteorological driver model's ability to predict cloud microphysics fields. In this case, the model underpredicted precipitation amount for the study period, which contributed to the underestimation of wet deposition and in turn may also have impacted the modelled ambient tracer concentrations. Crown Copyright (c) 2006 Published by Elsevier B.V. All rights reserved.",2006,10.1016/j.atmosres.2005.10.012,no
Using fuzzy analytical hierarchy process for multi-criteria evaluation model of high-yield bonds investment,"The returns and risks of Eligh-Yield Bond (HYB) lie between the stocks and Treasury bonds. In view of investment opportunities and the rate of return, the advantages of HYB are both lower risks and higher shares. Therefore, HYB has become one of important components in the portfolios. The purpose of this study is to find evaluation factors and their weights to aid the selection of HYB. The primary criteria to evaluate HYB are established by the literatures survey with Fuzzy Delphi Method (FDM), and then Fuzzy Analytic Hierarchy Process (FARP) is employed to calculate the weights of these criteria, so as to build the Fuzzy Multi-criteria model of HYB investments. The results indicate a greatest weight on the dimension of economic environment, and three primary evaluation criteria are: (1) spread versus Treasuries, (2) callability, and (3) default rate.",2006,,no
Model for an irreversible bias current in the superconducting qubit measurement process,"The superconducting charge-phase ""quantronium"" qubit is considered in order to develop a model for the measurement process used in the experiment of Vion [Science 296, 886 (2002)]. For this model we propose a method for including the bias current in the readout process in a fundamentally irreversible way, which to first order is approximated by the Josephson junction tilted-washboard potential phenomenology. The decohering bias current is introduced in the form of a Lindblad operator and the Wigner function for the current-biased readout Josephson junction is derived and analyzed. During the readout current pulse used in the quantronium experiment we find that the coherence of the qubit initially prepared in a symmetric superposition state is lost at a time of 0.2 ns after the bias current pulse has been applied, a time scale that is much shorter than the experimental readout time. Additionally we look at the effect of Johnson-Nyquist noise with zero mean from the current source during the qubit manipulation and show that the decoherence due to the irreversible bias current description is an order of magnitude smaller than that found through adding noise to the reversible tilted-washboard potential model. Our irreversible bias current model is also applicable to persistent-current-based qubits where the state is measured according to its flux via a small-inductance direct-current superconducting quantum interference device.",2006,10.1103/PhysRevA.74.062302,no
"Evaluation of the Tom-Cast model for prediction of early blight, Septoria leaf spot and anthracnose fruit rot in processing tomatoes in south-eastern Australia","The Tom-Cast disease forecasting model, based on leaf wetness and temperature, predicted that in a dry season fungicide sprays were unnecessary for control of early blight, Septoria leaf spot and anthracnose fruit rot, in processing tomato crops. Up to 9 applications of fungicide are made per season to Australian processing tomato crops, with most growers applying 3 to 4 sprays. The model was evaluated in crops at three locations over a two-year period by comparison with the growers' normal spray application practices. The adoption of the Tom-Cast model by processing tomato growers could save the Australian processing tomato industry an estimated $150,000 to $200,000 a year in fungicide costs.",2006,,no
Metrics-based software reliability models using non-homogeneous Poisson processes,"The traditional software reliability models aim to describe the temporal behavior of software fault-detection processes with only the fault data, but fail to incorporate some significant test-metrics data observed in software testing. In this paper we develop a useful modeling framework to assess the quantitative software reliability with time-dependent covariate as well as software-fault data. The basic ideas employed here are to introduce the discrete proportional hazard model on a cumulative Bernoulli trial process, and to represent a generalized fault-detection processes having time-dependent covariate structure. The resulting stochastic models are regarded as combinations of the proportional hazard models and the familiar non-homogeneous Poisson processes. We compare these metrics-based software reliability models with some typical non-homogeneous Poisson process models, and evaluate quantitatively both goodness-of-fit and predictive performances from the viewpoint of information criteria. As an important result, the accuracy on reliability assessment strongly depends on the kind of software metrics used for analysis and can be improved by incorporating time-dependent metrics data in modeling.",2006,,no
Assessing productivity and carbon sequestration capacity of Eucalyptus globulus plantations using the process model forest-DNDC: Calibration and validation,"The tree growth sub-module (PnET) of the mechanistic model Forest-DNDC was calibrated and validated for plantation grown Eucalyptus globulus. Forest-DNDC describes the biogeochemical cycles of C and N and can assist in estimating soil-borne greenhouse gas fluxes. For validation of the forest growth sub-module, data from commercial forest plantations in south-eastern Australia was used. Growth predictions agreed well with growth measurements taken at age 6 years from 28 permanent sample plots, with an average prediction error of - 1.62 t C ha(-1) (-3.19%). Differences between predicted and measured aboveground C stocks ranged between -23.5 and 12.6 t C ha-1, which amounted to a relative root mean square error in prediction of 17.9%. Correlation between modelled and measured C in standing biomass was good (r(2) = 0.73), with a Nash-Sutcliffe coefficient of model efficiency, ME=0.65. The results obtained from the validation test reveal that Forest-DNDC can predict growth of E. globulus to a high level of precision across a broad range of climatic conditions and soil types. Forest-DNDC performed satisfactorily in comparison to other growth and yield models that have already been calibrated for E. globulus (e.g. BIOMASS, 3-PG, PROMOD, or CABALA). In contrast to these growth and yield models, Forest-DNDC can additionally estimate total greenhouse gas budgets. The slightly lower precision of Forest-DNDC in comparison with specific management models, such as CABALA, are compensated for by the simple input requirements and application to regional situations. (c) 2005 Elsevier B.V All rights reserved.",2006,10.1016/j.ecolmodel.2005.07.021,no
Extending the UML 2 activity diagram with business process goals and performance measures and the mapping to BPEL,"The UML 2 Activity Diagram is designed for modelling business processes, but does not yet include any concepts for modelling process goals and their measures. We extend the UML 2 Activity Diagram with process goals and performance measures to make them conceptually visible. Additionally, we provide a mapping to BPEL to make the measures available for execution and monitoring. This profile and its mapping are tested with an example business process.",2006,,no
Web searching behavior: Constructing a process model of information evaluation based on a survey of college students,"The World Wide Web has had an unprecedented impact on the creation and utilization of information. As the amount of information available on the Web has continued to increase, it has become more difficult to decide whether certain information is reliable or not. This paper analyzes the information seeking behavior of Web users and examines their information evaluation processes. It is based on experimental research on students from a two-year college program. The results of the study are intended to be used by libraries to enhance user education. The paper reviews previous studies, examining studies on Web searching tendencies and the evaluation of Web resources in accordance with their methodologies. We discuss the simplicity of Web searching patterns and examine the factors that influence information evaluation. We point out the limitations of the research methods that have been used in the past, and describe the reasons for adopting the protocol analysis method in this investigation. We used an observational method and protocol analysis to investigate the students' behavior as they used search engines and the OPAC (Online Public Access Catalogue) to find information. We divided the students into two groups: students who have used the Web for more than five years and students who have used the Web for less than two years. In order to provide a basis for comparison, we repeated this research on university students. We came to the following conclusions. Web searching is a repeated behavior involving the repetition of a simple, regular pattern. The students were able to filter out unnecessary information quickly as they evaluated search results. Experience affected information seeking performance and evaluation, and a more pronounced effect was noted with students who had a longer history of experience with the Web. Students had a tendency to evaluate Web resources based on visual factors and personal experiences, but they lacked skills in determining the quality of the content. We found the same tendencies in university students. Finally, we integrated these findings to construct a process model of information evaluation for Web searching.",2006,,no
An industry-based evaluation of process modeling techniques,"There are many ways to model software development processes. This paper reports a feature analysis of four process modeling techniques using criteria specified by a software development organization. The evaluation used a single process, peer review, modeled using all four techniques. Performing the modeling activity highlighted the usefulness of the modeling activity and the usefulness of metamodels in structuring processes.",2006,,no
Energy consumption in batch thermal processing: model development and validation,"Thermal processing is an important method of food preservation in the manufacture of canned foods, retortable pouches, trays and bowls (retortable shelf-stable foods). The aim of this research was to develop a mathematical model to estimate total and transient energy consumption during the heat processing of retortable shelf-stable foods. The transient energy balance for a system defined as the steam and its water condensate in the retort requires no work term. The heat transfer terms include radiation and convection to the cook room environment, and heat transfer to the food in the cans. Mass and energy balance equations for the system were solved simultaneously, and the equation describing heat transfer in the food material was solved numerically using an explicit finite difference technique. Correlations valid in the range of interest (100 degrees C through 140 degrees C) were utilized to estimate the thermodynamic properties of steam, condensate, and food product. Depending upon selected conditions, retort insulation will account for a 15-25% energy reduction. In addition, initial temperature could reduce the peak energy demand in the order of 25-35%. These models should be useful in searching for optimum scheduling of retort battery operation in the canning plant, as well as in the optimising process conditions, to minimize energy consumption. (c) 2005 Elsevier Ltd. All rights reserved.",2006,10.1016/j.jfoodeng.2005.01.040,no
Evaluation of sampling-based methods for sensitivity analysis: Case study for the E-Coli food safety process risk model,"This article evaluates selected sensitivity analysis methods applicable to risk assessment models with two-dimensional probabilistic frameworks, using a microbial food safety process risk model as a test-bed. Six sampling-based sensitivity analysis methods were evaluated including Pearson and Spearman correlation, sample and rank linear regression, and sample and rank stepwise regression. In a two-dimensional risk model, the identification of key controllable inputs that can be priorities for risk management can be confounded by uncertainty. However, despite uncertainty, results show that key inputs can be distinguished from those that are unimportant, and inputs can be grouped into categories of similar levels of importance. All selected methods are capable of identifying unimportant inputs, which is helpful in that efforts to collect data to improve the assessment or to focus risk management strategies can be prioritized elsewhere. Rank-based methods provided more robust insights with respect to the key sources of variability in that they produced narrower ranges of uncertainty for sensitivity results and more clear distinctions when comparing the importance of inputs or groups of inputs. Regression-based methods have advantages over correlation approaches because they can be configured to provide insight regarding interactions and nonlinearities in the model.",2006,10.1080/10807030600977251,no
A meta-modeling approach for sequence diagrams to Petri Nets transformation within the requirements validation process,"This paper deals with the transformation of UML Sequence Diagrams into Petri Nets. The involved SD are those joined to the Use Cases dynamic description. In other words, they concern the requirements modeling. The approach is seen in a more general context of heterogeneous system design. It endeavors to integrate, early in the requirement modeling process, a formal model within a semi-formal one, in this case Petri nets with UML SD. The SD to PNs transformation is meta-modeling oriented within the MDA approach, specifically for PIM design. So, a meta-model for SD and one for PNs are first defined. Then, the meta-model of the transformation is done, and the transformation rules may be deduced. As a result, the approach allows a partial automation of the transformation, and subsequently the verification and validation of the system requirements.",2006,,no
A process-based model of end-of-life electronics recycling - Driving eco-efficiency-informed decisions,"This paper describes a model of end-of-life electronics recycling that uses process-based cost modeling and value-based metrics to determine the economic and value-recovery effectiveness of a processing operation. The process-based cost models combine engineering process models, operational models, and an economic framework to map from details of product and process to operating costs. The value-weighted mass recovery assessments, when compared against simple mass recovered, provide a better estimate of both environmental impact and retained quality. The combination of these two aspects offer several advantages over existing modeling methodologies: they consider the variety of products a recycler processes, are driven by data that a recycler collects, and can respond to rapidly changing conditions. Background on these aspects is provided, followed by a description of the modeling framework. An example case involving cathode ray tubes is presented to depict the type of analyses possible using the model.",2006,10.1109/ISEE.2006.1650050,no
Evaluating stochastic train process time distribution models on the basis of empirical detection data,"This paper evaluates several commonly applied probability distribution models for stochastic train process times based on empirical data recorded in a Dutch railway station, The Hague Holland Spoor. An initial guess of model parameters is obtained by the Maximum Likelihood Estimator (MLE). An iterative procedure is then followed, in which large delays are omitted one by one and the distribution parameters are estimated correspondingly using the MLE method. The parameter estimation is improved by minimizing the Kolmogorov-Smirnov (K-S) statistic where of course the empirical distribution is still based on the complete data set. A local search is finally performed in the neighbourhood of the improved model parameters to further optimize the estimation. To evaluate the distribution models, we compare the K-S statistic among the fitted distributions with optimized parameters using the one-sample K-S goodness-of-fit test at a commonly adopted significance level of alpha = 0.05. It has been found that the log-normal distribution can be generally considered as the best approximate model among the candidate distributions for both the arrival times of trains at the platform and at the approach signal of the station. The Weibull distribution can generally be considered as the best approximate distribution model for non-negative arrival delays, departure delays and the free dwell times of late arriving trains. The shape parameter of the fitted distribution is generally smaller than 1.0 in the first two cases, whereas it is always larger than 1.0 in the last case. These distribution evaluation results for train process times can be used for accurately predicting the propagation of train delays and supporting timetable design and rescheduling particularly in case of lack of empirical data.",2006,10.2495/CR060621,no
Queuing models for field defect resolution process,"This paper explores a novel application of queuing theory to the corrective software maintenance problem to support quantitative balancing between resources and responsiveness. Initially, we provide a detailed description of the states a defect traverses from find to fix and a definition and justification of mean time to resolution as a useful process metric. We consider the effect of queuing system structures, priority levels and priority disciplines on the differential mean times to resolution of defects of different severities. We find that modeling the defect resolution capacity of a software engineering group as n identical M/M/1 servers provides a flexible and realistic approximation to the queuing behavior of four different organizations. We consider three queuing disciplines. Though purely preemptive and non-preemptive prioriry disciplines may be suited for other groups, our data was best fit by a mixed discipline, one in which only the most severe defects preempt ongoing service activities of lesser severities. We provide two examples of the utility of such a model: Given the reasonable assumption that the most severe defects have the highest impact on reliability, we find that the reduction of the resolution time for these defects must come from changes reducing the service time. On the other hand the effect of additional engineering resources on the resolution time of less severe defects is easily computed and can be significant.",2006,,no
"A multi-electrode capacitance probe for phase detection in oil-water separation processes: design, modelling and validation","This paper focuses on the development of a multi-electrode capacitance probe for interface measurement and phase detection within industrial three-phase separators, used in oil and gas extraction and oil refining processes. The sensors are constructed using printed circuit board (PCB) technology and are embedded within stainless steel casings forming the structure of the probe. The design process was aided by the finite element solutions of the three-dimensional electrostatic problem. A number of solutions were obtained which predict the probe readings for various configurations of phases. Validation of the probe performance was performed using a combination of laboratory and industrial tests in a real scale separator, both with representative process media. A good agreement between the experiments and modelling is shown.",2006,10.1088/0957-0233/17/4/038,no
The value of setup cost reduction and process improvement for the economic production quantity model with defects,"This paper investigates the effect of imperfect yield on economic production quantity decisions. The production system is assumed to produce some time-varying proportion of defective parts which can be repaired at some unit cost. For a general defect rate function, we develop results that characterize the optimal run length and expected total cost and how these objects are affected by the cost parameters. Two kinds of investments in process improvements are considered: (i) reducing setup costs and (ii) improving process quality. We develop expressions for the marginal value of both setup cost reduction and process improvement and discuss the relationship between these marginal values and problem cost parameters. We show that any investment in setup cost reduction will result in a reduction in the number of defects produced, and the total number of defects can increase or decrease with an investment in quality improvement. (c) 2005 Elsevier B.V. All rights reserved.",2006,10.1016/j.ejor.2004.11.024,no
Challenges in modeling hydrologic and water quality processes in riparian zones,"This paper presents key challenges in modeling water quality processes of riparian ecosystems: How can the spatial and temporal extent of water and solute mixing in the riparian zone be modeled? What level of model complexity is justified? How can processes at the riparian scale be quantified? How can the impact of riparian ecosystems be determined at the watershed scale? Flexible models need to be introduced that can simulate varying levels of hillslope-riparian mixing dictated by topography, upland and riparian depths, and moisture conditions. Model simulations need to account for storm event peak flow conditions when upland solute loadings may either bypass or overwhelm the riparian zone. Model complexity should be dictated by the level of detail in measured data. Model algorithms need to be developed using new macro-scale and meso-scale experiments that capture process dynamics at the hillslope or landscape scales. Monte Carlo simulations should be an integral part of model simulations and rigorous tests that go beyond simple time series, and point-output comparisons need to be introduced. The impact of riparian zones on watershed-scale water quality can be assessed by performing simulations for representative hillslope-riparian scenarios.",2006,10.1111/j.1752-1688.2006.tb03818.x,no
Analytical urban storm water quality models based on pollutant buildup and washoff processes,"This paper presents methodology and major procedures for the development of analytical urban storm water quality models following derived probability distribution theory, which involves conceptualization of the three major components, i.e., the rainfall-runoff transformation, pollutant buildup, and washoff processes. In this study, two different types of the rainfall-runoff transformations are employed in an attempt to improve model performance by considering spatial variations of parameters associated with runoff generation mechanisms. By integrating different types of the rainfall-runoff transformations and pollutant buildup function with washoff function, two different types of pollutant washoff load models are formulated. Thereby, the probability distributions of the rainfall characteristics are mathematically transformed to create system storm water quality control measures, such as the average pollutant event mean concentration and long-term pollutant loads to receiving waters. These storm water quality control measures are closed-form analytical models and can be employed as alternatives to continuous simulation models for the evaluation of long-term system behavior. The results from case study reveal that with appropriately formulated rainfall-runoff transformation along with pollutant buildup and washoff functions, analytical storm water quality models are capable of providing comparable results to observed data and can serve as effective tools for storm water quality control analysis.",2006,10.1061/(ASCE)0733-9372(2006)132:10(1314),no
Data processing and compact representation of measured isotropic spectral BRDF,"This paper presents the methods for both data processing and compact representation of measured isotropic spectral BRDF. For the data processing, we develop a numerical method for filtering the noises, re-sampling the data from non-uniform sampling to uniform sampling, and interpolation. For the compact representation, we propose a method to represent the spectral BRDF in both the spectral and spatial domains. In spectral domain, for each pair of the incident and outgoing directions, we represent the spectral BRDF with Fourier coefficients. In spatial domain, for all the outgoing directions of a given incident direction, we represent the same-order Fourier coefficients either directly using a linear combination of spherical harmonics or a linear combination of spherical harmonics and a Gaussian, depending on their angular dependencies. Three Gaussian expressions are presented. Numerical studies are given for a measured isotropic spectral BRDF.",2006,,no
Numerical modelling and experimental validation of steel deep drawing processes - Part II: Applications,"This paper presents the modelling and experimental validation of three different deep drawing applications: the Erichsen test, a cylindrical cup test and an industrial sheet metal forming process. The sheet forrning material considered in the study is the EK4 steel characterized in Part I of this work. A finite element analysis of the deformation process is performed with a large strain hyperelastic shell formulation including the Hill-48 associate plasticity model. The experimental validation of the results provided by the simulation encompasses the punch force evolution together with the in-plane principal deformations and thickness distributions of the final deformed part. (c) 2005 Elsevier B.V. All rights reserved.",2006,10.1016/j.jmatprotec.2005.11.016,no
Analytical hierarchy process approach to rank measures for structural complexity of conceptual models,"This paper presents the result of a controlled experiment conducted to determine the relative importance of some measures, identified in research, for the structural complexity of Entity-Relationship (ER) models. The relative importance amongst these measures is calculated by applying the Analytical Hierarchy Process approach. The results reveal that the number of relations in an ER diagram are of the highest importance in measuring the structural complexity in terms of understandability, analyzability and modifiability; whereas, the number of attributes do not play an important role. The study presented here can lead to developing quantitative metrics for comparing the quality of alternative conceptual models of the same problem.",2006,,no
Ceramic tile process modeling for quality improvement using ANN,"This paper proposes a method of improving the quality and timelines of ceramic products through development of neural network model relating multiple quantitative and qualitative design parameters to product quality. Various processes in a ceramic tile manufacturing company were studied in this research. An intelligent software module to demonstrate the usefulness of such an approach is developed. The input variables describe the raw materials, ambient conditions and line settings. The neural network predictive model assigns one quality value to the final product based on the input data. This prediction can be used to make a priori adjustments to materials and line settings so that a product of required quality is produced without trial and error.",2006,,no
Measurement and modeling of stimulus-evoked electromyography in lengthened and shortened muscles for spinal cord injured subjects during an electrically-elicited fatigue process,"This study compares the amplitude and temporal features of stimulus-evoked electromyography (EMG) of paralyzed muscle, rectus femoris (RF), in both lengthened and shortened positions of six spinal cord injured (SCI) subjects during an electrically elicited fatigue process. The torque output and evoked EMG were fitted by hyperbolic tangent functions from which their amplitude residual levels and temporal inflection times can be extracted. Furthermore, a structural EMG model of Fuglevand et al (1992 Biol. Cybern. 67 143-53) was modified to include type I (slow twitch) and type II (fast twitch) of motor unit (MU) fibers with viable parameters obtained from paralyzed muscles to observe their amplitude and temporal changes. Our results showed that the amplitude of stimulus-evoked EMG decreased earlier in the lengthened muscle with a shorter inflection time (48.53 +/- 8.7 s versus 55.13 +/- 4.03 s) than that of the shortened position during 120 s of stimulation time (p < 0.05). Similarly, the peak-to-peak duration (PTPd) of the evoked EMG increased faster at an earlier time to a higher asymptotical value in lengthened muscle (2.23 +/- 0.74 versus 1.77 +/- 0.54), compared to that of a shortened one (p < 0.05). These observations coincided with the higher rising rate and larger final value of the temporal coefficients, i.e.,longer duration, in both type I and II MUs of lengthened muscles. From the observation of all parameters, the fatigue process in lengthened muscle proceeds faster than that in shortened muscle.",2006,10.1088/0967-334/27/12/006,no
A comparison of reasoning processes in a collaborative modelling environment: Learning about genetics problems using virtual chat,"This study investigated the possible activation of different types of model-based reasoning processes in two learning settings, and the influence of various terms of reasoning on the learners' problem representation development. Changes in 53 students' problem representations about genetic issue were analysed while they worked with different modelling tools in a synchronous network-based environment. The discussion log-files were used for the ""microgenetic"" analysis of reasoning types. For studying the stages of students' problem representation development, individual pre-essays and post-essays and their utterances during two reasoning phases were used. An approach for mapping problem representations was developed. Characterizing the elements of mental models and their reasoning level enabled the description of five hierarchical categories of problem representations. Learning in exploratory and experimental settings was registered as the shift towards more complex stages of problem representations in genetics. The effect of different types of reasoning could be observed as the divergent development of problem representations within hierarchical categories.",2006,10.1080/09500690500438670,no
Numerical modelling and experimental validation of steel deep drawing processes - Part I. Material characterization,"This work presents an experimental characterization of the mechanical behaviour of the EK4 deep drawing steel. The experimental procedure encompasses spectrometry, rnetalography, tension testing and hardness measurements. Special attention is devoted to the derivation of the elastic and plastic parameters involved in the assumed constitutive model based on the anisotropic Hill-48 yield criterion. The simulation of the deformation process during the whole tensile test is subsequently performed with the aim of assessing the adequateness of the proposed methodology. It should be mentioned that the material parameters obtained with this procedure are the basic data for the modelling and experimental validation of different deep drawing applications presented in Part II of this work. (c) 2005 Elsevier B.V. All rights reserved.",2006,10.1016/j.jmatprotec.2005.11.015,no
"The effects of the model errors generated by discretization of ""on-off"" processes on VDA","Through an idealized model of a partial differential equation with discontinuous ""on-off"" switches in the forcing term, we investigate the effect of the model error generated by the traditional discretization of discontinuous physical ""on-off"" processes on the variational data assimilation (VDA) in detail. Meanwhile, the validity of the adjoint approach in the VDA with ""on-off"" switches is also examined. The theoretical analyses illustrate that in the analytic case, the gradient of the associated cost function (CF) with respect to an initial condition (IC) exists provided that the IC does not trigger the threshold condition. But in the discrete case, if the on switches (or off switches) in the forward model are straightforwardly assigned the nearest time level after the threshold condition is (or is not) exceeded as the usual treatment, the discrete CF gradients (even the one-sided gradient of CF) with respect to some ICs do not exist due to the model error, which is the difference between the analytic and numerical solutions to the governing equation. Besides, the solution of the corresponding tangent linear model (TLM) obtained by the conventional approach would not be a good first-order linear approximation to the nonlinear perturbation solution of the governing equation. Consequently, the validity of the adjoint approach in VDA with parameterized physical processes could not be guaranteed. Identical twin numerical experiments are conducted to illustrate the influences of these problems on VDA when using adjoint method. The results show that the VDA outcome is quite sensitive to the first guess of the IC, and the minimization processes in the optimization algorithm often fail to converge and poor optimization retrievals would be generated as well. Furthermore, the intermediate interpolation treatment at the switch times of the forward model, which reduces greatly the model error brought by the traditional discretization of ""on-off"" processes, is employed in this study to demonstrate that when the ""on-off"" switches in governing equations are properly numerically treated, the validity of the adjoint approach in VDA with discontinuous physical ""on-off"" processes can still be guaranteed.",2006,,no
Modeling of mold filling process of Al casting and validation with X-ray in-situ observation,"To describe the shape and location of free surfaces in the mold filling simulation, parameters including volume filled ratio, surface dimensionless distance, and surface filled ratio for the DFDM (Direct Finite Difference Method) elements were proposed. A model of mold filling process was established, considering the mass, momentum and heat transfer in the vicinity of free surface. It had been applied to an experimental Al casting. In-situ observation and record of actual mold filling process of the casting with special X-ray apparatus were carried out, and were used to validate the simulation results. The validation showed that the model could predict the mold filling process well. It was found that back pressure of gas in the mould cavity had minor effect on change of liquid flow when using pressurized feeding system and sand mould with good air permeability.",2006,,no
Modeling of autonomous problem solving process by dynamic construction of task models in multiple tasks environment,"Traditional reinforcement learning (RL) supposes a complex but single task to be solved. When a RL agent faces a task similar to a learned one, the agent must relearn the task from the beginning because it doesn't reuse the past learned results. This is the problem of quick action learning, which is the foundation of decision making in the real world. In this paper, we suppose agents that can solve a set of tasks similar to each other in a multiple tasks environment, where we encounter various problems one after another, and propose a technique of action learning that can quickly solve similar tasks by reusing previously learned knowledge. In our method, a model-based RL uses a task model constructed by combining primitive local predictors for predicting task and environmental dynamics. To evaluate the proposed method, we performed a computer simulation using a simple ping-pong game with variations. (c) 2006 Elsevier Ltd. All rights reserved.",2006,10.1016/j.neunet.2006.05.037,no
"Simultaneous layout, process and model optimization within an integrated Design-for-Yield environment - art. no. 62832S","Trends in the design feature shrinking that outrun the progress in the lithography technologies require critical efforts in the layout, process, and model development. Printing a layout is no longer a problem only for the lithographers; it has penetrated into the layout stage as well. Layout patterns are getting more aggressive, raising serious printability concerns. This requires very accurate models to analyze the manufacturability issues. This also often requires simultaneous analysis and optimization of both layout and the process. Most advanced layout patterns are extremely hard to manufacture and consequently run into the risk of re-spins. Therefore, an early pre-tapeout analysis and troubleshooting of various layout, process, and RET issues has become a very important task. Our paper gives examples of how these and other related issues can be addressed using a commercially available Design-for-Yield integrated environment.",2006,10.1117/12.681806,no
Evaluation of errors in feedback control based on persistence prediction in model-based process controller system for deep sub-100 nm gate fabrication,"Using model-based advanced process control (APC) in the gate etching process, gate linewidth variations that are caused by variations in the photo resist linewidth are reduced by feedforward control of the photo resist linewidth that is obtained by a critical dimension (CD) measurement tool. Both long-term process drift in the etching process and long-term instability of the CD measurement tool lead to variations in the process model for APC. These process model variations result in gate linewidth variations and are attenuated by feedback control using the etch CD bias from the previously processed lot. We used a lot-to-lot model-based APC system and a critical dimension scanning electron microscope (CD-SEM) in a deep sub-100nm gate fabrication line to investigate long-term variations in the process model over a period of six months, and to study how much the variations are reduced by feedback control based on persistence prediction methods using the etch CD bias for the pilot wafer from the currently processed lot. The process model uses the linear relationship between the etch CD bias and the gas mixture ratio at the gate linewidth plasma trimming step, and the gradient is almost constant over the term. The spread in the long-term variation in the process model was 8.12 nm. The variation mainly included process drift in the plasma etcher, and the persistence of the etch CD bias in the lot after processing was significantly lost beyond approximately one week. When using feedback control based on the persistence prediction method using the etch CD bias for the pilot wafer from the currently processed lot, the standard deviation of the feedback error was 0.50 nm of l sigma. On the other hand, when calculating feedback control with the lot-mean etch CD bias from the lot processed immediately before the currently processed lot, the standard deviation of the feedback error was estimated to be 0.82 nm of 1 sigma. When performing feedback with the etching shift value of the pilot wafers, the percentage of lots whose post etch CD value fell within the range of +/- 1 nm from the target value was higher by 20% than the simulated value of the percentage of lots in which feedback was calculated with the etch CD bias of the previously processed lots.",2006,10.1143/JJAP.45.7645,no
Damage-layer-mediated H diffusion during SiN : H processing: A comprehensive model,"We describe a refined surface-damage model for H transport during SiN:H processing to explain recent observations in H passivation. We propose that surface-damaged layers act not only as a source of trapped H, but can also mediate H between Si and SiN:H. This capability enables exchange of H between SiN:H and Si, making passivation characteristics dependent on the surface damage as well as composition of the nitride. It also explains many other observations.",2006,,no
A process model for collaborative problem solving in Virtual Communities of Practice,"We present a model for collaborative problem solving in Virtual Communities of Practice. The model applies a repository, comprising resources, properties and statements, to underpin the process of problem-solving. The process allows for formulating, exploring, matching and gradually refining abstract problem descriptions (one kind of resource) into the corresponding concrete solutions (another kind of resource), expanding the underlying repository with new resources, properties and statements in the process. The model is defined formally, its usefulness is argued with a simple case study and a possible implementation is described using Semantic Web.",2006,,no
Using process algebra to validate behavioral aspects of object-oriented models,"We present in this paper a rigorous and automated based approach for the behavioral validation of control software systems. This approach relies on metamodeling, model-transformations and process algebra and combines semi-formal object-oriented models with formal validation. We perform the validation of behavioral aspects of object-oriented models by using a projection into a well-defined formal technical space (Finite State Process algebra) where model-checkers are available (we use LTSA; a model checker for Labeled Transition Systems). We then target an implementation platform, which conforms to the semantics of the formal technical space; in turn, this ensure conformance of the final application to the validated specification.",2006,,no
Decisional model for the performance evaluation of the logistic process: Application on consultation ambulatory unit of a hospital supply,"We propose a generic decisional model allowing to estimate in a total way (physical flows, financial flows) plannings for any system contained in supply chain. We present how to integrate this model in a software suite dedicated to supply chain and call this type of software presenting a global decisional solution Advanced Budgeting and Scheduling. To show the generic character of our approach, we apply the chaining of models suggested to the logistic process of a consultation ambulatory unit of a Hospital Supply Chain.",2006,,no
"Stochastic domination: the contact process, Ising models and FKG measures","We prove for the contact process on Z(d), and many other graphs, that the upper invariant measure dominates a homogeneous product measure with large density if the infection rate; is sufficiently large. As a consequence, this measure percolates if the corresponding product measure percolates. We raise the question of whether domination holds in the symmetric case for all infinite graphs of bounded degree. We study some asymmetric examples which we feel shed some light on this question. We next obtain necessary and sufficient conditions for domination of a product measure for ""downward"" FKG measures. As a consequence of this general result, we show that the plus and minus states for the Ising model on Z(d) dominate the same set of product measures. We show that this latter fact fails completely on the homogeneous 3-ary tree. We also provide a different distinction between Z(d) and the homogeneous 3-ary tree concerning stochastic domination and Ising models; while it is known that the plus states for different temperatures on Z(d) are never stochastically ordered, on the homogeneous 3-ary tree, almost the complete opposite is the case. Next, we show that on Z(d), the set of product measures which the plus state for the Ising model dominates is strictly increasing in the temperature. Finally, we obtain a necessary and sufficient condition for a finite number of variables. which are both FKG and exchangeable, to dominate a given product measure. (c) 2005 Elsevier SAS. All rights reserved.",2006,10.1016/j.anihpb.2005.04.002,no
A semi-implicit monotone difference scheme for an initial-boundary value problem of a strongly degenerate parabolic equation modeling sedimentation-consolidation processes,"We prove the convergence of a semi-implicit monotone finite difference scheme approximating an initial-boundary value problem for a spatially one-dimensional quasilinear strongly degenerate parabolic equation, which is supplied with two different inhomogeneous flux-type boundary conditions. This problem arises in the modeling of the sedimentation-consolidation process. We formulate the definition of entropy solution of the model in the sense of Kruzkov and prove convergence of the scheme to the unique BV entropy solution of the problem, up to satisfaction of one of the boundary conditions.",2006,10.1090/S0025-5718-05-01787-4,no
Optimizing a process-based ecosystem model with eddy-covariance flux measurements: A pine forest in southern France,"[1] We design a Bayesian inversion method (gradient-based) to optimize the key functioning parameters of a process-driven land surface model (ORganizing Carbon and Hydrology In Dynamic EcosystEms (ORCHIDEE)) against the combination of prior information upon the parameters and eddy covariance fluxes. The model calculates energy, water, and CO2 fluxes and their interactions on a half-hourly basis, and we carry out the inversion using measurements of CO2, latent heat, and sensible heat fluxes as well as of net radiation over a pine forest in southern France. The inversion method makes it possible to assess the reduction of uncertainties and error correlations of the parameters. We designed an ensemble of inversions with different set ups using flux data over different time periods, in order to (1) identify well-constrained parameters and loosely constrained ones, (2) highlight some model structural deficiencies, and (3) quantify the overall information gained from assimilating each type of CO2 or energy fluxes. The sensitivity of the optimal parameter values to the initial carbon pool sizes and prior parameter values is discussed and an analysis of the posterior uncertainties is performed. Assimilating 3 weeks of half-hourly flux data during the summer improves the fit to diurnal variations, but merely improves the fit to seasonal variations. Assimilating a full year of flux data also improves the fit to the diurnal cycle more than to the seasonal cycle. This points out to the key importance of timescales when inverting parameters from high-frequency eddy-covariance data. We show that photosynthetic parameters such as carboxylation rates are well-constrained by the carbon and water fluxes data and get increased from their prior values, a correction that is corroborated by independent measurements at leaf scale. In contrast, the parameters controlling maintenance, microbial and growth respirations, and their temperature dependencies cannot be robustly determined. The CO2 flux data could not discriminate between the different respiration terms. At face value, all the parameters controlling the surface energy budget can be safely determined, leading to a good model-data fit on different timescales.",2007,10.1029/2006GB002834,no
Estimating the greenhouse gas fluxes of European grasslands with a process-based model: 1. Model evaluation from in situ measurements,"[1] We improved a process-oriented biogeochemical model of carbon and nitrogen cycling in grasslands and tested it against in situ measurements of biomass and CO(2) and CH(4) fluxes at five European grassland sites. The new version of the model (PASIM) calculates the growth and senescence of aboveground vegetation biomass accounting for sporadic removals when the grassland is cut and for continuous removals when it is grazed. Limitations induced by high leaf area index (LAI), soil water deficits and aging of leaves are also included. We added to this a simple empirical formulation to account for the detrimental impact on vegetation of trampling and excreta by grazing animals. Finally, a more realistic methane emission module than is currently used was introduced on the basis of the quality of the animals' diet. Evaluation of this improved version of PASIM is performed at (1) Laqueuille, France, on grassland continuously grazed by cattle with two plots of intensive and extensive grazing intensities, (2) Oensingen, Switzerland, on cut grassland with two fertilized and nonfertilized plots, and (3) Carlow, Ireland, on grassland that is both cut and grazed by cattle during the growing season. In addition, we compared the modeled animal CH(4) emissions with in situ measurements on cattle for two grazing intensities at the grazed grassland site of Laqueuille. Altogether, when all improvements to the PASIM model are included, we found that the new parameterizations resulted into a better fit to the observed seasonal cycle of biomass and of measured CO(2) and CH(4) fluxes. However, the large uncertainties in measurements of biomass and LAI make simulation of biomass dynamics difficult to make. Also simulations for cut grassland are better than for grazed swards. This work paves the way for simulating greenhouse gas fluxes over grasslands in a spatially explicit manner, in order to quantify and understand the past, present and future role of grasslands in the greenhouse gas budget of the European continent.",2007,10.1029/2005GB002611,no
Combined population balance and thermodynamic modelling of the batch top-spray fluidised bed coating process. Part I - Model development and validation,"A combined population balance and thermodynamic model was developed for the batch top-spray fluidised bed coating process. The model is based on the one-dimensional discretisation of the fluidised bed into different control volumes, in which the dynamic heat and mass balances for air, water vapour, core particles and coating material were established. The calculation method involves a Monte Carlo technique for the simulation of the particle exchange in combination with the first-order Euler's method for solving the heat and mass balances. The model enables the prediction of both the dynamic coating mass distribution and the one-dimensional thermodynamic behaviour of the fluidised bed during batch operation. The simulation results were validated using the results from tests on a Glatt GPCG-1 fluidised bed unit. (c) 2005 Elsevier Ltd. All rights reserved.",2007,10.1016/j.jfoodeng.2005.09.030,no
Using extended activity-network diagram to design a process quality control model,"A complicated mechanical product to be manufactured always contains a series of procedures named processes or operations which form a complicated network diagram. In order to guarantee the process quality and the final quality of product, this paper presents a process quality control model by using the extended activity-net-work diagram technique. In this control model, task nodes correspond to processes or operations and arcs to relationship between two nodes. The functions of a task node are made up of event and status. The event refers to the activities of off-line or on-line quality control and fault diagnosis of a process or an operation while status to error status and its transmitting rule of a process or an operation. The proposed model can map and navigate the quality control procedure of multistage processes well.",2007,,no
Modeling of extremal fuzzy dynamic systems. Part VI. Problems of states estimation (filtration) of extremal fuzzy process,"A fuzzy-integral model of an extremal fuzzy process (EFP) is considered. The model describes the evolution of one class of weakly structurable fuzzy dynamic systems (Klir 1985), i.e. of the so-called extremal fuzzy dynamic systems, which have been constructed in Part III of this work. In the present paper, problems of an optimal filtering of continuous as well as of discrete EFP are solved by means of ""past"" evaluating information. Sufficient conditions are established for the existence of an optimal estimating fuzzy process. Using only one ""past"" evaluating fuzzy state of the considered system, variants of fuzzy observers (representations of an optimal estimating EFP) are constructed in terms of approximation of piecewise-constant and extremally measurable filtration functions for continuous and discrete extremal fuzzy dynamic systems. The results obtained are illustrated by a numerical example.",2007,10.1080/03081070600913601,no
A quality cost model for food processing plants,"A HACCP-based system is a recognized food safety management program aiming at the control of all the factors affecting food safety. It is also possible to add factors related to food quality. To evaluate the effectiveness of a quality system, a realistic estimate of quality costs is essential. The purpose of this work is to develop a mathematical model for the calculation of the costs associated with a specific quality level due to HACCP-based system implementation. Experimental results obtained at Argentinean hake freezing plants (Merluccius hubbsi) are presented and compared with those calculated with the proposed model. The proportion of variance explained by the model was 0.903 for total quality costs; proving its optimum performance. (c) 2007 Elsevier Ltd. All rights reserved.",2007,10.1016/j.jfoodeng.2007.03.029,no
A model of flight request collection and processing in the flight scheduling problem,"A method of analysis of service systems with relative priority, where physical restrictions of request queue generating process are used to derive design formulas is considered. This approach allows one to formalize the interference of partial input flows with description of their dynamic equilibrium and, consequently, to avoid branching of a transition and system state graph by reducing the system model to an ordinary composition of Markov chains for each component of the complete request flow. An example of analysis of priority collection and processing of requests for airspace use in a centralized flight scheduling service established for the united system of Russian air traffic control is given.",2007,10.1134/S1064230707030124,no
Sound-quality prediction for nonstationary vehicle interior noise based on wavelet pre-processing neural network model,"A new concept for sound-quality prediction, the so-called wavelet pre-processing neural network (WT-NN) model, is presented in this paper. Based on interior vehicle noise, the WT-NN sound-quality evaluation model was developed by combining the techniques of wavelet analysis and neural network (NN) classification. A wavelet-based 21-point model for vehicle noise feature extraction was established, as was a NN model. Verification results show that the trained WT-NN models are accurate and effective for sound-quality prediction of nonstationary vehicle noises. Due to its outstanding time-frequency characteristics, the proposed WT-NN model can be used to deal both with stationary and nonstationary signals, and even transient ones. In place of conventional psychoacoustical models, the WT-NN technique is suggested not only to predict, classify, and compare the sound quality (loudness and sharpness) of vehicle interiors, but also to apply to other sound-related fields in engineering. (c) 2006 Elsevier Ltd. All rights reserved.",2007,10.1016/j.jsv.2006.07.034,no
Statistical validation and an empirical model of hydrogen production enhancement found by utilizing passive flow disturbance in the steam-reformation process,"A passive flow disturbance has been proven to enhance the conversion of fuel in a methanol-steam reformer. This study presents a statistical validation of the experiment based on a standard 2(k) factorial experiment design and the resulting empirical model of the enhanced hydrogen producing process. A factorial experiment design was used to statistically analyze the effects and interactions of various input factors in the experiment. Three input factors, including the number of flow disturbers, catalyst size, and reactant flow rate were investigated for their effects on the fuel conversion in the steam-reformation process. Based on the experimental results, an empirical model was developed and further evaluated with an uncertainty analysis and interior point data. (c) 2007 Elsevier Inc. All rights reserved.",2007,10.1016/j.expthermflusci.2007.05.010,no
A decision-support model for evaluating changes in biopharmaceutical manufacturing processes,"A simulation is described that evaluates the impacts of altering bio-manufacturing processes. Modifications designed to improve production levels, times and costs were assessed, including increasing feed volumes/titres, replacing initial downstream stages with packed or expanded bed affinity steps and removing ion exchange steps. Options were evaluated for manufactured product mass, COG, batch times and development costs and timescales. Metrics were combined using multi-attribute-decision-making techniques generating a single assessment metric for each option. The utility of this approach was illustrated by application to an FDA-approved process manufacturing rattlesnake anti-venom (Protherics U.K.). Currently, ovine serum containing anti-venom IgG is purified by precipitation/centrifugation, prior to antibody proteolysis by papain. An ion exchanger removes F(C), before affinity chromatography yields the final anti-venom. An expanded bed affinity column operating with an 80% higher IgG titre, 66% higher feed volume and without the ion exchanger delivered the best multi-attribute-decision-making value, potentially providing the most desirable alternative.",2007,10.1007/s00449-006-0086-8,no
The principles of information processing based on the numerical solution of the nonlinear heat-conduction problem. Pt. 1. The three-dimensional discrete mathematical model,A three-dimensional solution of the direct problem of heat conduction when a linear pulsed heat source acts in the plane of contact of two semibounded bodies is presented. The solution is obtained by the method of finite differences. An example of the modeling is considered.,2007,10.1007/s11018-007-0201-5,no
Numerical modeling of water quality and sediment related processes,"A three-dimensional water quality model (CCHE3D_WQ) was developed for simulating temporal and spatial variations of water quality with respect to phytoplankton, nutrients, and dissolved oxygen. Four major interacting systems were simulated, including phytoplankton dynamics, nitrogen cycle, phosphorus cycle, and dissolved oxygen balance. The effects of suspended and bed sediment on the water quality processes were also considered. The model was verified using analytical solutions for the transport of non-conservative substances in open channel flow, and then calibrated and validated by applying it to the study of the water quality of a natural shallow oxbow lake. The simulated time serial concentration of phytoplankton (as chlorophyll) and nutrients were generally in good agreement with field observations. Sensitivity studies were then conducted to demonstrate the impacts on water quality due to varying nutrients and suspended sediment loads to the chlorophyll concentration. (c) 2006 Elsevier B.V. All rights reserved.",2007,10.1016/j.ecolmodel.2006.10.003,no
Study on calibration and validation of full-coupled activate sludge model No.1 for AAO process simulation,"According to the mechanism of microorganism growth, Full Coupled Activated Sludge Model No. 1 (FCASM) which was modified from ASM Bio-P model by adding inhibition switching function was introduced. Both steady-state numerical simulation and dependent-time numerical simulation of computer was carried out. The effluent value of NH4-N simulated by FCASM1 AND asm3 Bio-P are 1.90 g/m(3) and 0 g/m(3) respectively be sides the measure effluent value is 1.50 g/m(3). The result shows that FCASM1 correspond with the actual better than ASM3 Bio-P when the are employed in simulation to the process for NH3-N removal. Such conclusion also can be obtained from the results of dependent-time numerical simulation. There are littler differences between FCASM1 and ASM3 Bio-P when they are used for simulating the processes of COD and PO43--P removal.",2007,,no
Modelling non measurable processes by neural networks: Forecasting underground flow case study of the Ceze Basin (Gard-France),"After a presentation of the nonlinear properties of neural networks, their applications to hydrology are described. A neural predictor is satisfactorily used to estimate a flood peak. The main contribution of the paper concerns an original method for visualising a hidden underground flow Satisfactory experimental results were obtained that fitted well with the knowledge of local hydrogeology, opening up an interesting avenue for modelling using neural networks.",2007,10.1007/978-1-4020-6264-3_10,no
Measuring the inconsistencies between process model and process execution,"At present, using process models to simulate, validate and guide software development is an important approach to improve software development processes, and enhance the quality of software products, but in reality the actual executing process always deviates from the expected process model. How to detect and measure such inconsistencies is a challenging task. The highly dynamic and exceptional nature of software processes means that simple yes-no answers carry too little information about the significance of any given inconsistency. Managers need to understand where an inconsistency occurs and how severe that inconsistency might be before taking any corrective action. This paper uses event trees to represent the process model and process execution, puts forwards MinCost and Similarity metrics to measure the severity of inconsistencies, and designs an algorithm to assist in compare event trees and compute metrics.",2007,,no
Process evaluation for complex interventions in primary care: understanding trials using the normalization process model,"Background: The Normalization Process Model is a conceptual tool intended to assist in understanding the factors that affect implementation processes in clinical trials and other evaluations of complex interventions. It focuses on the ways that the implementation of complex interventions is shaped by problems of workability and integration. Method: In this paper the model is applied to two different complex trials: ( i) the delivery of problem solving therapies for psychosocial distress, and ( ii) the delivery of nurse- led clinics for heart failure treatment in primary care. Results: Application of the model shows how process evaluations need to focus on more than the immediate contexts in which trial outcomes are generated. Problems relating to intervention workability and integration also need to be understood. The model may be used effectively to explain the implementation process in trials of complex interventions. Conclusion: The model invites evaluators to attend equally to considering how a complex intervention interacts with existing patterns of service organization, professional practice, and professional- patient interaction. The justification for this may be found in the abundance of reports of clinical effectiveness for interventions that have little hope of being implemented in real healthcare settings.",2007,10.1186/1471-2296-8-42,no
Understanding the implementation of complex interventions in health care: the normalization process model,"Background: The Normalization Process Model is a theoretical model that assists in explaining the processes by which complex interventions become routinely embedded in health care practice. It offers a framework for process evaluation and also for comparative studies of complex interventions. It focuses on the factors that promote or inhibit the routine embedding of complex interventions in health care practice. Methods: A formal theory structure is used to define the model, and its internal causal relations and mechanisms. The model is broken down to show that it is consistent and adequate in generating accurate description, systematic explanation, and the production of rational knowledge claims about the workability and integration of complex interventions. Results: The model explains the normalization of complex interventions by reference to four factors demonstrated to promote or inhibit the operationalization and embedding of complex interventions (interactional workability, relational integration, skill-set workability, and contextual integration). Conclusion: The model is consistent and adequate. Repeated calls for theoretically sound process evaluations in randomized controlled trials of complex interventions, and policy-makers who call for a proper understanding of implementation processes, emphasize the value of conceptual tools like the Normalization Process Model.",2007,10.1186/1472-6963-7-148,no
"The model project ""newborn auditory screening"" in the Upper Palatinate. High process and result quality of an interdisciplinary concept","Background. In May 2003, a newborn auditory screening program was initiated in the Oberpfalz. Methods. Sequential OAE- and BERA-screening was conducted in all hospitals with obstetric facilities. The Screening Center at the Public Health Authority was responsible for the coordination of the screening process, completeness of participation, the follow-up of all subjects with a positive screening test and the quality of instrumental screening. Results. A total of 96% of 17,469 newborns were screened. The referral rate at discharge was 1.6% (0.4% for bilateral positive findings). For 97% of the positive screening results, a definite diagnosis to confirm or exclude hearing loss was achieved; for 43% only after intervention by the Screening Center. Fifteen children with profound bilateral hearing impairment were identified of whom eight were only detected by the intervention of the Screening Center. Conclusion. The effective structures established in the Upper Palatinate provide a standard for the quality of neonatal auditory screening achievable in Germany.",2007,10.1007/s00106-006-1383-x,no
The revised learning process questionnaire: A validation of a Western model of students' study approaches to the South Pacific context using confirmatory factor analysis,"Background. Research evidence seems to suggest that the social and cultural environments influence students' approaches to their study. This social and cultural contention has led to the rethinking and reconceptualization of theories (e.g. Biggs, 1987; Marton & Saljo, 1976) pertaining to student approaches to learning (SAL) in academic settings. Aims. The present research discusses two separate empirical studies on student learning approaches situated in the South Pacific region with two respective cohorts of secondary students. Study I involved the examination of secondary Pacific Islands students in their learning approaches using a modified version of Biggs' (1987) original Learning Process Questionnaire (LPQ). Study II involved the administration of a revised version of the LPQ (R-LPQ-2F; Kember, Biggs, & Leung, 2004) to another cohort of secondary Pacific Islands students. Sample. The first sample included 2,150 (1,285 girls, 865 boys) students and the second sample included 2,295 (1,363 girls, 932 boys) students. Methods. The factor structures of approaches to learning were examined by means of confirmatory factor analysis (CFA) using the LISREL program. Different a priori models were hypothesized and tested. Results. Results of Study I indicated a two-factor structure solution to Biggs' LPQ, supporting Richardson's (1994) theoretical model of learning and emphasized the factors of Reproducing and Meaning. Study II indicated a hierarchical organization of two main study approaches - deep and surface - that are structured as higher-order factors and a defined by four first-order factors. Conclusion. The results from the two studies accentuate the important argument for the rethinking and reconceptualization of learning approaches, as well as for the redevelopment and modification of learning inventories such as the LPQ. They also suggest the importance of situating the theoretical paradigm of learning approaches in a social and cultural environment.",2007,10.1348/000709906X158339,no
Consistency of business process models and object life cycles,"Business process models and object life cycles can provide two different views on behavior of the same system, requiring that these models are consistent with each other. However, it is difficult to reason about consistency of these two types of models since their relation is not well-understood. We clarify this relation and propose an approach to establishing the required consistency. Object state changes are first made explicit in a business process model and then the process model is used to generate life cycles for each object type used in the process. We define two consistency notions for a process model and an object life cycle and express these in terms of conditions that must hold between a given life cycle and a life cycle generated from the process model.",2007,,no
Evaluation of heterogeneous processes in the polar lower stratosphere in the Whole Atmosphere Community Climate Model,"Chemical ozone loss in the polar lower stratosphere is derived from an ensemble of three simulations from the Whole Atmosphere Community Climate Model (WACCM3) for the period 1960-2003, using the tracer-tracer correlation technique. We describe a detailed model evaluation of the polar region by applying diagnostics such as vortex temperature, sharpness of the vortex edge, and the potential of activated chlorine (PAC1). Meteorological and chemical information about the polar vortex, temperature, vortex size, and activation time, and level of equivalent effective stratospheric chlorine, are included in PAC1. Discrepancies of the relationship between chemical ozone loss and PAC1 between model and observations are discussed. Simulated PAC1 for Antarctica is in good agreement with observations, owing to slightly lower simulated temperatures and a larger vortex volume than observed. Observed chemical ozone loss of 140 +/- 30 DU in the Antarctic vortex core are reproduced by the WACCM3 simulations. However, WACCM3 with the horizontal resolution used here (4 x 5) is not able to simulate the observed sharp transport barrier at the polar vortex edge. Therefore the model does not produce an homogeneous cold polar vortex. Warmer temperatures in the outer region of the vortex result in less chemical ozone loss over the entire polar vortex than observed. For the Arctic, WACCM3 temperatures are biased high (by 2-3 degrees in the annual average) and the vortex volume and chlorine activation period is significantly smaller than observed. WACCM3 Arctic chemical ozone loss only reaches 20 DU for cold winters, where observations suggest approximate to 80-120 DU.",2007,10.1029/2006JD008334,no
"Modelling, screening, and solving of optimisation problems: Application to industrial metal forming processes","Coupling Finite Element (FEM) simulations to mathematical optimisation techniques provides a high potential to improve industrial metal forming processes. In order to optimise these processes, all kind of optimisation problems need to be mathematically modelled and subsequently solved using an appropriate optimisation algorithm. Although the modelling part greatly determines the final outcome of optimisation, the main focus in most publications until now was on the solving part of mathematical optimisation, i.e. algorithm development. Modelling is generally performed in an arbitrary way. In this paper, we propose an optimisation strategy for metal forming processes using FEM. It consists of three stages: a structured methodology for modelling optimisation problems, screening for design variable reduction, and a generally applicable optimisation algorithm. The strategy is applied to solve manufacturing problems for an industrial deep drawing process.",2007,,no
An optimization model of enterprise business processes based on house of quality,"Customer satisfaction index has become one of most important measurements of enterprise performance. It is an urgent problem that enterprise is reengineered abased customer satisfaction index. Quality function deployment (""QFD"" or House of Quality) can alliance between customer satisfaction index and business process optimization, which is an effective optimization approach. The paper constructs a business process optimization model abased House of Quality. The model is tested by a service enterprise data, showing its validity and rationality.",2007,,no
Validation and estimation of parameters for a general probabilistic model of the PCR process,"Earlier work rigorously derived a general probabilistic model for the PCR process that includes as a special case the Velikanov-Kapral model where all nucleotide reaction rates are the same. In this model, the probability of binding of deoxy-nucleoside triphosphate (dNTP) molecules with template strands is derived from the microscopic chemical kinetics. A recursive solution for the probability function of binding of dNTPs is developed for a single cycle and is used to calculate expected yield for a multicycle PCR. The model is able to reproduce important features of the PCR amplification process quantitatively. With a set of favorable reaction conditions, the amplification of the target sequence is fast enough to rapidly outnumber all side products. Furthermore, the final yield of the target sequence in a multicycle PCR run always approaches an asymptotic limit that is less than one. The amplification process itself is highly sensitive to initial concentrations and the reaction rates of addition to the template strand of each type of dNTP in the solution. This paper extends the earlier Saha model with a physics based model of the dependence of the reaction rates on temperature, and estimates parameters in this new model by nonlinear regression. The calibrated model is validated using RT-PCR data.",2007,10.1089/cmb.2006.0123,no
Application of Computational Fluid Dynamics modelling in the process of forensic fire investigation: Problems and solutions,"Fire modelling has been gaining more and more interest into the community of forensic fire investigation. Despite an attractiveness that is partially justified, the application of fire models in that field of investigation rises some difficulties. Therefore, the understanding of the basic principles of the two main categories of fire models, the knowledge of their effective potential and their limitations are crucial for a valid and reliable application in forensic science. The present article gives an overview of the principle and basics that characterise the two kinds of fire models: zone models and field models. Whereas the first ones are developed on the basis of mathematical relation from empirical observations, such as stratification of fluid zones, and give a relatively broad view of mass and energy exchanges in an enclosure, the latter are based on fundamentals of fluid mechanics and represent the application of Computational Fluid Dynamics (CFD) to fire scenarii. Consequently, the data that are obtained from these two categories of fire models differ in nature, quality and quantity. First used in a fire safety perspective, fire models are not easily applied to assess parts of forensic fire investigation. A suggestion is proposed for the role of fire modelling in this domain of competence: a new tool for the evaluation of alternative hypotheses of origin and cause by considering the dynamic development of the fire. An example of a real case where such an approach was followed is explained and the evaluation of the obtained results comparing to traces revealed during the on-site investigation is enlightened. (c) 2006 Elsevier Ireland Ltd. All rights reserved.",2007,10.1016/j.forscint.2006.06.053,no
A statistical model for estimating the effect of process variations on delay and slew metrics for VLSI interconnects,"For optimizations like placement, interconnect synthesis and static timing analysis, efficient interconnect delay computation is critical for RC networks. Because of its simple closed form and fast evaluation, the Elmore delay model has been widely used. The other delay metrics PRIMO and H-gamma match the first three circuit moments to the probability density function (PDF) of a Gamma statistical distribution. Although these methods demonstrate impressive accuracy compared to other delay metrics, their implementations tend to be challenging. In this paper simple and efficient two-parameter analytic expressions for both delay and slew, based on Erlang distribution (ERD) function, are presented under process variations. The effectiveness of the proposed metrics for RC trees is proved through experimental results.",2007,,no
Fluid-dynamic investigations in a scaled cold model for a dual fluidized bed biomass steam gasification process: Solid flux measurements and optimization of the cyclone,"Gasification of biomass is an attractive technology for combined heat and power (CHP) production. A dual fluidized bed steam gasifier is in commercial operation at the biomass CHP plant in Guessing/Austria since 2002. For circulating fluidized bed applications the bed material consumption is economically crucial. Thus, cyclones for circulating fluidized beds need to be designed properly. Some erosion and caking in the cyclone of the gasifier could be observed with increasing hours of operation. The influences of these effects as well as the influence of the solid circulation rate between the two units on the separation efficiency were investigated by fluid-dynamic investigations using a scaled cold model. The results show that due to erosion and caking elutriation rates are increased, especially for smaller particles. However, the cyclone achieves fractional separation efficiencies of more than 99.9%.",2007,,no
Impact velocity modelling and signal processing of spur gear vibration for the estimation of defect size,"Gears are one of the most common elements in any rotating machinery. If gear defect can be assessed, gearbox maintenance schedule can be optimally planned. This paper presents an impact velocity model relating measurable vibration signal to the defect size on the gear tooth flank. The analytical model was verified experimentally. The experimental results support the effectiveness of the analytical model in estimating defect size. In addition, experimental vibration signals were decomposed using empirical mode decomposition (EMD) technique. These decomposed oscillatory functions are called intrinsic mode functions (IMFs). Kurtosis value of selected IMF was calculated for early detection of fault. (c) 2006 Elsevier Ltd. All rights reserved.",2007,10.1016/j.ymssp.2005.12.011,no
A method of linking business process modeling with information system design using UML and its evaluation by prototyping,"In recent years, business modeling has attracted attention in the development of information systems for analyzing and designing the working procedures of the system under development. The paper concentrates upon the use of business modeling in the process of analysis from the point of view of early identification of procedural bottlenecks. It goes on to propose and evaluate a method of configuring information systems that links business modeling with UML. The proposed method builds a business process model from the requirement specifications and translates the model into UML. The proposed method was applied to the prototyping of a bus booking and control system, comparing the results obtained using the proposed method with those using only UML for the configuration. The proposed method proved effective in the early detection and removal of bottlenecks, in coping with changes in the specifications, and in accurate modeling.",2007,,no
Guidelines for reverse engineering process modeling of technical systems,"In spite of the process of conception generation being essential to promote product innovation, it has not been effectively carried out by designers, due to the abstract nature of this activity. In order facilitate this process, Reverse Engineering (RE) has been suggested as a way to formalize the processes of identifying, purchasing and modeling design information in terms of functions and solution principles, in a continuous and systematic way. This paper presents a literature review of technical systems conception processes and of RE. The results of a technical visit to a leading RE company, and a preliminary methodological proposal for RE process modeling for technical systems, are also reported. This proposal is intended to support the concept generation process of technical systems. Finally, guidelines are proposed for the final version of the methodology.",2007,,no
Development and experimental evaluation of a steady-state model for the step-feed biological nitrogen removal process,"In this article, a steady-state mathematical model was developed and experimentally evaluated to investigate the effect of influent flow distribution and volume ratios of anoxic and aerobic zones in each stage on the total nitrogen concentration of the effluent in the step-feed biological nitrogen removal process. Unlike the previous modeling methods, this model can be used to calculate the removal rates of ammonia and nitrate in each stage and thereby predict the concentrations of ammonia, nitrate, and total nitrogen in the effluent. To verify the simulation results, pilot-scale experimental studies were carried out in a four-stage step feed process. Good correlations were achieved between the measured data and the simulation results, which proved the validity of the developed model. The sensitivity of the model predictions was analyzed. After verification of the validity, the step feed process was optimally operated for five months using the model and the criteria developed for the design and operation. During the pilot-scale experimental period, the effluent total nitrogen concentrations were all below 5mg-L(-1), with more than 90% removal efficiency.",2007,10.1016/S1004-9541(07)60100-1,no
Modeling of pulp quality parameters from distribution curves extracted from process acoustic measurements on a thermo mechanical pulp (TMP) process,"In this paper the feasibility of modeling strength and optical Pulp properties from length distribution curves extracted from acoustic data using continuous wavelet transform-fiber length extraction, CWT-FLE (A Bjork and L-G Danielsson, 'Extraction of Distribution Curves from Process Acoustic Measurements on a TMP-Process', Pulp and Paper Canada 105 No. 11 (2004), T260-T264) by use of Partial Least Squares (PLS) have been tested. The curves used have earlier been validated against length distribution curves obtained by analyzing pulp samples with a commercial analyzer (FiberMaster). The curves were extracted from acoustic data without any ""calibration"" against fiber length analyses. The acoustic measurements were performed using an accelerometer affixed to the refiner blow-line during a full-scale trial with a Sunds Defibrator double disc refiner at SCA Ortviken, Sweden. Pulp samples were collected concurrently with the acoustic measurements and extensive physical testing has been made on these samples. For each trial point three pulp samples were collected. PLS1 and PLS2 models were successfully made linking the distribution curves obtained using CWT-FLE to pulp tensile strength properties as well as optical properties. The resulting Root Mean Square Error of Prediction (RMSEP) for all parameters is comparable to what can be obtained by pooling the standard deviations of reference measurements from the different trial points. The results obtained are compared to FiberMaster data modeled in the same fashion, yielding lower prediction errors than the CWT-FLE data. However, this can be partly due to the five-year storage of pulp samples between pulp sampling/acoustic measurement and FiberMaster analyses/sheet testing. The acoustic method is fast and produces results without dead time and could constitute a new tool for improving process control and optimizing the fiber characteristics in a specific process and for a specific purpose. The technique could be implemented in a PC-environrnent at a fairly low cost. (c) 2006 Elsevier B.V. All rights reserved.",2007,10.1016/j.chemolab.2006.04.007,no
An experimental evaluation of state estimation with fluid dynamical models in process tomography,"In this paper we perform an experimental evaluation of a state estimation approach in process tomography. In particular, we concentrate on the case where a system with rapidly moving target is imaged with electrical impedance tomography. We show experimental results which confirm that non-stationary estimation with proper fluid dynamical models works well even in cases where stationary estimates are completely useless. (c) 2006 Elsevier B.V. All rights reserved.",2007,10.1016/j.cej.2006.09.025,no
"Applying Problem Solving Methods for Process Knowledge Acquisition, Representation, and Reasoning","In this paper we present an approach towards knowledge acquisition of process knowledge for the natural sciences. The work has been conducted within Project Halo, which is creating advanced knowledge authoring and question answering systems for the natural sciences. An analysis of AP (R)-level questions for Biology, Chemistry and Physics uncovered that process knowledge is the single most frequent type of knowledge required. Thus, we developed means to acquire process knowledge, to formally represent it, and to reason about it in order to answer novel questions about the domains. All these tasks are supported by an abstract process meta-model. It provides the terminology for user-tailored process diagrams, which are automatically translated into executable FLogic code. The meta-model and the code generation are based on the notion of Problem Solving Methods (PSM) which represent an abstract formalization of the reasoning strategies needed for processes.",2007,,no
Mechanical stress and defect formation in device processing: Validity of the numerical models for mechanical stress calculation,"In this paper, we address the problem of estimating the risk of crystal defect generation in a complex device process. The validity of numerical calculations of the mechanical stress developed in the device process flow is assessed by comparing these calculations to the results of the electrical tests of structures designed to monitor the formation of dislocations. The results show that, based upon numerical calculations, it is possible to define the mechanical stress criteria for preventing defect generation. By using this sort of criteria, potentially dangerous process variations can be easily identified. This method is quite general and can be applied to any device process flow.",2007,10.1109/TED.2007.892948,no
A unified ontology-based process model for software maintenance and comprehension,"In this paper, we present a formal process model to support the comprehension and maintenance of software systems. The model provides a formal ontological representation that supports the use of reasoning services across different knowledge resources. In the presented approach, we employ our Description Logic knowledge base to support the maintenance process management, as well as detailed analyses among resources, e.g., the traceability between various software artifacts. The resulting unified process model provides users with active guidance in selecting and utilizing these resources that are context-sensitive to a particular comprehension task. We illustrate both, the technical foundation based on our existing SOUND environment, as well as the general objectives and goals of our process model.",2007,,no
Implementing spiking neural networks for real-time signal-processing and control applications: A model-validated FPGA approach,"In this paper, we present two versions of a hardware processing architecture for modeling large networks of leaky-integrate-and-fire (LIF) neurons; the second version provides performance enhancing features relative to the first. Both versions of the architecture use fixed-point arithmetic and have been implemented using a single field-programmable gate array (FPGA). They have successfully simulated networks of over 1000 neurons configured using biologically plausible models of mammalian neural systems. The neuroprocessor has been designed to be employed primarily for use on mobile robotic vehicles, allowing bio-inspired neural processing models to be integrated directly into real-world control environments. When a neuroprocessor has been designed to act as part of the closed-loop system of a feedback controller, it is imperative to maintain strict real-time performance at all times, in order to maintain integrity of the control system. This resulted in the reevaluation of some of the architectural features of existing hardware for biologically plausible neural networks (NNs). In addition, we describe a development system for rapidly porting an underlying model (based on floating-point arithmetic) to the fixed-point representation of the FPGA-based neuroprocessor, thereby allowing validation of the hardware architecture. The developmental system environment facilitates the cooperation of computational neuroscientists and engineers working on embodied (robotic) systems with neural controllers, as demonstrated by our own experience on the Whiskerbot project, in which we developed models of the rodent whisker sensory system.",2007,10.1109/TNN.2007.891203,no
Model based process design of the combined high pressure and mild heat treatment ensuring safety and quality of a carrot simulant system,"In this research the combined mild heat and high pressure (HP) treatment of two food processing targets is under study: one microbiological safety target (Escherichia coli K12) and one quality related target (carrot Pectin Methylesterase, PME). A polynomial non-monotonous model structure fulfilling a number of constraints was identified for describing the log-linear inactivation kinetics of E. coli (at T = 5-45 degrees C and P = 200-500 MPa). Similarly, a polynomial non-monotonous model structure is used in order to describe the evolution of the carrot PME inactivation kinetics at T = 10-65 degrees C and P = 0.1-825 MPa. Iso-rate contour plots are constructed integrating the microbial and enzymatic kinetics for the combined T and P treatments. Additionally, the effect of the pressure build-up time (specific to the experimental set-up at hand) on the processing targets was quantified based on the microbial and enzymatic activity load before the initiation of the experiment and after the stabilisation of the treatment conditions. When T-P kinetic diagrams were constructed with combinations of treatments (also at extrapolation regions) resulting in the same log reductions, i.e., iso-reduction contour plots, it was evident that carrot PME was more resistant than E. coli. According to the analysis of the T-P diagrams (incorporating the pressure build-up processing effects), a thermal process in a range of 55-80 degrees C, and a combined low temperature (30-50 degrees C)-high pressure (700-800 MPa) process revealed to be equivalent. (c) 2006 Elsevier Ltd. All rights reserved.",2007,10.1016/j.jfoodeng.2005.12.051,no
Three-component adsorption modeling to evaluate and improve integrated sorption-membrane processes,"Integrated sorption-membrane (ISM) processes combining low-pressure membranes with adsorbents are increasingly popular because they cost-effectively expand low-pressure membrane treatment to include dissolved contaminant removal. However, contemporary ISM processes often exhibit antagonistic tradeoffs between adsorption and membrane performance that were investigated using state-of-the-art adsorption models that include both of the predominant competitive effects of natural organic matter: direct site competition and pore blockage. Two currently used ISM process configurations, powdered activated carbon-ultrafiltration (PAC-UF) and adsorptive floc blanket reactor-ultrafiltration (FBR-UF) were compared with a novel configuration, upflow adsorption-ultrafiltration (UA-UF), which consists of a moving-bed of granular activated carbon upstream of a membrane. Model simulations quantitatively compared performance and evaluated potential improvements for each configuration. For instance, using contemporary PAC-UF practices and 90% atrazine removal as a baseline, alternative membrane backwashing procedures can lower carbon usage rates (CURs) by 75% but may also reduce membrane hydraulic performance. Using the same baseline, FBR-UF can reduce CURs by 92% while simultaneously improving membrane performance via pretreatment; however, process size increases 10-fold. The novel UA-UF configuration only increases process size modestly, but can still yield CURs 96% lower than the PAC-UF baseline while simultaneously providing beneficial membrane pretreatment and improving sustainability features by reducing residuals.",2007,10.1021/es070410o,no
Modelling exposure in flour processing sectors in The Netherlands: A baseline measurement in the context of an intervention program,"Introduction: Recent studies have shown that even low exposure levels to flour dust and related allergens can cause severe respiratory symptoms. In The Netherlands the Dutch government and responsible branch organizations [from bakeries (traditional & industrial), flour mills and bakery ingredient producers] signed a covenant to reduce exposure to Hour dust and decrease the prevalence of work-related occupational airway disease. This paper describes a sector wide survey to measure exposure to flour dust, wheat allergens and fungal a-amylase. The results are being used to underpin various elements of the covenant. Methods: A dataset containing 910 personal measurements was compiled from four field studies containing information on exposure and potential determinants. The dataset represents a baseline estimate of exposure for four major flour processing sectors in The Netherlands. Exposure models for all sectors and agents were generated, based on job, tasks and company size, taking into account worker and company as random effect components. Use of control measures and, where possible, their effect were evaluated. Results: Flour dust and enzyme exposures vary strongly between sectors. The job performed and specific tasks were identified as important determinants of exposure. The number of identified control measures during walk-through surveys, and their effectiveness in reduction of dust exposure was generally limited. The exposure models explained significant exposure variability between companies and workers but performed poorly in explaining day to day differences in exposure. Discussion: The dataset serves as a baseline estimate and will be compared with a post intervention survey in the near future. The information obtained on control measures can be used to optimize the intervention scenarios that will be implemented in the different sectors by external occupational hygienists. The predictive exposure models will provide a relevant measure of average personal exposure that will be used in the sector wide health surveillance system.",2007,10.1093/annhyg/mem008,no
A stage model of knowledge management: an empirical investigation of process and effectiveness,"Knowledge management (KM) is now widely recognized to be important to the success or failure of business management. Seeking to better understand the determinants of the evolution of KM, this study focuses on two main problems: (1) whether firms change their KM processes over time to improve KM effectiveness as well as develop their KM practices, and (2) whether socio-technical support results in more mature KM practices. This study draws on the previous literature to identify key dimensions of KM process (knowledge acquisition, knowledge conversion, knowledge application and knowledge protection), KM effectiveness (individual-level and organizational-level KM effectiveness) and socio-technical support (organizational support and information technology diffusion). The evolution of these dimensions is studied in the form of a stage model of KM that includes initiation, development, and mature stages. Data gathered from 141 senior executives in large Taiwanese organizations were employed to test the propositions. The results show that different stages of KM evolution can be distinguished across dimensions of KM process, KM effectiveness, and socio-technical support. Implications for organizations are also discussed.",2007,10.1177/0165551506076395,no
Quality prediction model of the sheet blanking process for thin phosphorous bronze,"ln this paper. the shearing experiments for the progressive die of thin phosphorous bronze sheet were first performed to investigate the material of shearing punch. punch-die clearance. punch shear angle, and effect of the shearing stroke towards the burnish band of sheared surface, as well as the burr width of sheared corner. Then a prediction model of burr width was established by adopting the least-square method, exponential smoothing method and grey prediction theory. respectively. The findings show that the best prediction results can be achieved based on the dynamic model of grey prediction and it can be referred in the shear product estimate and the tool life prediction. (C) 2007 Elsevier B.V. All rights reserved.",2007,10.1016/j.matprotec.2007.04.110,no
A model measurement of the relaxation process of ferromagnetic particles in living tissues,"Magnetopneumography is a noninvasive technique for measuring the ferromagnetic portion of retained dust in the lungs by means of its remanent magnetization. This paper deals with the relaxation process which occurs after magnetization of ferromagnetic microparticles located in the tissue, mainly in the lung's macrophages. As the remanent magnetic field is not steady, but decreases in time, we carried out model measurements of these processes. Especially we have studied the influences of the magnetization intensity and the concentration of the particles. The resulting time-courses of the relaxation, as well as their calculated and measured values are used in the applied magnetopneumography.",2007,,no
A metamodel for modeling and measuring Scrum development process,"Many organizations using agile processes would like to adopt a process measurement framework, e.g. for assessing their process maturity. In this paper we propose a meta-model supporting derivation of specific data models for agile development processes. Then, we show how our meta-model can be used to derive a model of the Scrum process.",2007,,no
Modeling as a teaching learning process for understanding materials: A case study in primary education,"Modeling is being used in teaching learning science in a number of ways. It will be considered here as a process whereby children of primary school age exercise their capacity of organizing recognizable and manageable forms during their understanding of complex phenomenologies. The aim of this work is to characterize this process in relation to the modeling of properties of and changes in materials. The data are discussed by establishing relationships between the modeling process with three different aspects: the specialized scientific knowledge, the physical manipulation of phenomena, and the interaction among those participating in the class. The results show how 7-8-year-old students generate a modeling process that leads them to explain the behavior of different materials by using a ""model of parts"" created ad hoc. This model, built up from some kind of a discrete vision of the material, proves to be coherent for children of this age and evolves by relating the visible continuum with an imagined discontinuum. (c) 2007 Wiley Periodicals, Inc. Sci Ed 91:398-418, 2007",2007,10.1002/sce.20196,no
Evaluation of neural networks for modelling ink transfer in the gravure printing process,"Modelling of the ink transfer in gravure printing presents significant difficulties due to the complexity of the process. This paper outlines the development and evaluation of Artificial Neural Networks for the modelling of the ink transfer. On the basis of experimental data, models were created for the estimation of the optical density of the print as result of engraved cell geometric characteristics (specifically screen ruling, screen angle, volume). The developed models have been shown to be accurate and reliable, being able to estimate the density within the normal human range of sensitivity to colours and outperforming more traditional modelling techniques such as polynomial regression fitting techniques.",2007,,no
Understanding information technology preadoption and postadoption: An integrated process model,"Much of the prior information technology (IT) research has done to explain users' acceptance of initial IT. The first time adoption does not mean the IT continued use, so recent research has focused on IT postadoption behavior or continuance. The technology acceptance model (TAM) is recognized as an important theoretical model to interpret initial IT adoption behavior. Expectation confirmation theory (ECT) from the marketing literature is currently introduced to explain the IT postadoption behavior. However, IT adoption should be regarded as a process model rather than a static model at. two separate time points. This paper examines and compares two models in explaining IT preadoption and postadoption stage, then suggests an integrated process model derived from them. The proposed process model was empirically tested in the context of Chinese students. The objective of the paper is to suggest future research directions for explaining IT adoption.",2007,,no
The analysis of seasonal activity of photosynthesis and efficiency of various vegetative communities on a basis NDVI for modeling of biosphere processes,"NDVI (Normalized Difference Vegetation Index) is proposed as an area-dependent climatic variable, which reflects climatically significant events and processes. NDVI is taken as a simple quantitative indicator of the amount of photosynthetically active biomass. Mean values of NDVI have been calculated for the period between 1996 and 2001. NDVI time series have been analyzed in conjunction with meaningful synoptic parameters that influence the behavior of plants in different plant communities of Eastern Siberia (tundra, taiga, and steppe). Based on GIS technologies, statistical tests have been carried out and correlations between the study parameters have been found. (c) 2007 Published by Elsevier Ltd on behalf of COSPAR.",2007,10.1016/j.asr.2006.02.028,no
Validation of a numerical model for the simulation of an electrostatic powder coating process,"Numerical modeling of a complete powder coating process is carried out to understand the gas-particle two-phase flow field inside a powder coating booth and results of the numerical simulations are compared with experimental data to validate the numerical results. The flow inside the coating booth is modeled as a three-dimensional turbulent continuous gas flow with solid powder particles as a discrete phase. The continuous gas flow is predicted by solving Navier-Stokes equations using a standard k-epsilon turbulence model with non-equilibrium wall functions. The discrete phase is modeled based on a Lagrangian approach. In the calculation of particle propagation, a particle size distribution obtained through experiments is applied. The electrostatic field, including the effect of space charge due to free ions, is calculated with the use of the user defined scalar transport equations and user defined scalar functions in the software package, FLUENT, for the electrostatic potential and charge density. (C) 2006 Elsevier Ltd. All rights reserved.",2007,10.1016/j.ijmultiphaseflow.2006.12.001,no
How does the geometry affect the internal biomechanics of a lumbar spine bi-segment finite element model? Consequences on the validation process,"Numerical modelling can provide a thorough understanding of the mechanical influence on the spinal tissues and may offer explanations to mechanically linked pathologies. Such objective might be achieved only if the models are carefully validated. Sensitivity study must be performed in order to evaluate the influence of the approximations inherent to modelling. In this study, a new geometrically accurate L3-L5 lumbar spine bi-segmental finite-element model was acquired by modifying a previously existing model. The effect of changes in bone geometry, ligament fibres distribution, nucleus position and disc height was investigated in flexion and extension by comparison of the results obtained from the model before and after the geometrical update. Additional calculations were performed in axial rotation and lateral bending in order to compare the computed ranges of motion (ROM) with experimental results. It was found that the geometrical parameters affected the stress distribution and strain energy in the zygapophysial joints, the ligaments, and the intervertebral disc, changing qualitatively and quantitatively their relative role in resisting the imposed loads. The predicted ROM were generally in good agreement with the experimental results, independently of the geometrical changes. Hence, although the model update affected its internal biomechanics, no conclusions could be drawn from the experimental data about the validation of a particular geometry. Hence the validation of the lumbar spine model should be based on the relative role of its structural components and not only on its global mobility. (c) 2007 Elsevier Ltd. All rights reserved.",2007,10.1016/j.jbiomech.2006.11.021,no
Understanding time-variable gravity due to core dynamical processes with numerical geodynamo modeling,"On decadal time scales, there are three major physical processes in the Earth's outer core that contribute to gravity variations: (i) the mass redistribution due to advection in the outer core, (ii) the mantle deformation in response to (i), and (iii) the (core) pressure loading on the core-mantle boundary. Except the last one, they cannot be evaluated from surface observations. In this paper we use MoSST core dynamics model and PREM model to understand the gravity anomalies from the three processes. Our numerical results show that, the gravity anomalies are comparable in magnitude, though that from the process (i) is in general the strongest. The gravity anomalies from the first two processes tend to offset each other (""mantle shielding""). Consequently the pressure loading effect contributes more to axisymmetric part of the net gravity variation, while the net effect from the first two processes is more important to non axisymmetric components.",2007,,no
Estimation of Faraday Rotation Measures of the Near Galactic Sky Using Gaussian Process Models,"Our primary goal is to obtain a smoothed summary estimate of the magnetic field generated in and near to the Milky Way by using Faraday rotation measures (RM's). Each RM in our data set provides an integrated measure of the effect of the magnetic field along the entire line of sight to an extragalactic radio source. The ability to estimate the magnetic field generated locally by our galaxy and its environs will help astronomers distinguish local versus distant properties of the universe. RM's can be considered analogous to geostatistical data on a sphere. In order to model such data, we employ a Bayesian process convolution approach which uses Markov chain Monte Carlo (MCMC) for estimation and prediction. Complications arise due to contamination in the RM measurements, and we resolve these by means of a mixture prior on the errors.",2007,,no
Applying process models: For example problem management at BMW and partners,Process models lay a foundation for optimized development of complex products. The benefit of process models increases even more when R&D is shared with external partners. BMW has experienced this for example in the process of coping with unexpected development results. This paper is an overview of the Problem Management Process at BMW and partners.,2007,,no
Combined use of Model based Data Validation and Data driven Techniques for Process Monitoring,Process monitoring has to consider the problem of measurement uncertainty. A model based approach (data validation) is compared to data driven techniques for an industrial application.,2007,,no
Process monitoring using a combination of data driven techniques and model based data validation,"Process monitoring is made difficult when measurements are subjected to errors, since pertinent information is hidden in the measurement noise. To address this issue, one can use model based data validation, or rely on statistical techniques to analyze large historical data sets (data mining). An industrial case study is presented here, where a model based approach (data validation) is compared to data driven techniques.",2007,,no
Process cost modeling: Strategic engineering and economic evaluation of materials technologies,"Production cost is a vital performance metric for engineering and management analysis. Despite its obvious relevance throughout the product development cycle, cost analysis has not been a focus of the design engineer. In part, this is because of some key misunderstandings of what cost is-engineers have not been trained in the techniques that tie manufacturing cost to the technical and design parameters with which they are more comfortable and familiar. While there have been many calls for a closer relationship between engineering and economic analysis, these key conceptual obstacles, in conjunction with the limits of the computational tools available, have limited the integration of cost analysis into product and process development. This paper summarizes the conceptual limitations that need to be overcome and presents a basis for revising the notion of process cost analysis. Moreover, it presents a series of cost analysis cases that demonstrate the way in which the notion of ""context"" lies at the heart of effective use of engineering cost estimates.",2007,10.1007/s11837-007-0126-0,no
Modelling ice melting processes: numerical and experimental validation,"Purpose - This work is devoted to the experimental analysis, numerical modelling and validation of ice melting processes. Design/methodology/approach - The thermally coupled incompressible Navier-Stokes equations including water density inversion and isothermal phase-change phenomena are assumed as the governing equations of the problem. A fixed-mesh finite element formulation is proposed for the numerical solution of such model. In particular, this formulation is applied to the analysis of two different transient problems. Findings - The numerical results computed with the finite element formulation have been found to be very similar to the corresponding predictions, also obtained in this study, provided by a finite volume enthalpy-based technique. Both numerical results, in turn, satisfactorily approached the available experimental measurements expressly conducted in the context of this work for validation purposes. Research limitations/implications - They are mainly due to some model simplifications (e.g. no volume changes are considered during the solid-liquid transformation) and to the inherent difficulties associated with the experimental measurements. Practical implications - This study may be relevant for a better understanding of the phenomena occurring in different engineering applications involving phase-change in water: food freezing, ice formation in pipes, freezing/melting processes in soils, ice growth in plane wings, etc. Originality/value - The study is mainly focused on the validation of the numerical predictions obtained with the finite element formulation mentioned above with other results provided by a well-known finite volume technique and, in addition, with available laboratory measurements carried out in the context of this work.",2007,10.1108/09615530710753008,no
A pattern-driven process model for quality-centered software architecture design - A case study on usability-centered design,"Quality requirements of software systems typically affect large portions of the system, and should be taken into account early in the design process. Patterns have become a mainstream technique to associate frequent quality-related design problems with proven solutions. We present a generic pattern-driven design process model, and apply this to usability, obtaining a usability-centered design process model. As a case study, we have applied the model to the usability-centered software architecture design of a stone crusher control system.",2007,10.1109/ASWEC.2007.8,no
The effect of rainfall measurement uncertainties on rainfall-runoff processes modelling,"Rainfall data are a crucial input for various tasks concerning the wet weather period. Nevertheless, their measurement is affected by random and systematic errors that cause an underestimation of the rainfall volume. Therefore, the general objective of the presented work was to assess the credibility of measured rainfall data and to evaluate the effect of measurement errors on urban drainage modelling tasks. Within the project, the methodology of the tipping bucket rain gauge (TBR) was defined and assessed in terms of uncertainty analysis. A set of 18 TBRs was calibrated and the results were compared to the previous calibration. This enables us to evaluate the ageing of TBRs. A propagation of calibration and other systematic errors through the rainfall-runoff model was performed on experimental catchment. It was found that the TBR calibration is important mainly for tasks connected with the assessment of peak values and high flow durations. The omission of calibration leads to up to 30% underestimation and the effect of other systematic errors can add a further 15%. The TBR calibration should be done every two years in order to catch up the ageing of TBR mechanics. Further, the authors recommend to adjust the dynamic test duration proportionally to generated rainfall intensity.",2007,10.2166/wst.2007.100,no
Modeling and processing measurement uncertainty within the theory of evidence: Mathematics of random-fuzzy variables,"Random-fuzzy variables (RFVs) are mathematical variables defined within the theory of evidence. Their importance in measurement activities is due to the fact that they can be employed for the representation of measurement results, together with the associated uncertainty, whether its nature is random effects, systematic effects, or unknown effects. Of course, their im- portance and usability also depend on the fact that they can be employed for processing measurement results. This paper proposes suitable mathematics and related calculus for processing RFVs, which consider the different nature and the different behavior of the uncertainty effects. The proposed approach yields to process measurement algorithms directly in terms of RFVs so that the final measurement result (and all associated available information) is provided as an RFV.",2007,10.1109/TIM.2007.894907,no
Evaluation of reward processes in an animal model of depression,"Rationale Anhedonia is a core symptom of major depression. Deficits in reward function, which underlie anhedonia, can be readily assessed in animals. Therefore, anhedonia may serve as an endophenotype for understanding the neural circuitry and molecular pathways underlying depression. Objective Surprisingly, there is scant knowledge regarding alterations in brain reward function after olfactory bulbectomy (OB), an animal model which results in a behavioural syndrome responsive to chronic antidepressant treatment. Therefore, the present studies aimed to assess reward function after bulbectomy. Materials and methods The present study utilized sucrose preference, cocaine-induced hyperlocomotion and intra-cranial self-stimulation (ICSS) responding to examine reward processes in the OB model. Results Bulbectomized animals showed a marked preference (> 90%) for 0.8% sucrose solution compared with water; similar to the preference exhibited by sham controls. Importantly, there were pronounced deficits in brain reward function, as assessed using ICSS, which lasted 8 days before returning to baseline levels. Furthermore, bulbectomized animals were hyper-responsive to the locomotor stimulating properties of an acute and a repeated cocaine regimen. However, no difference in ICSS facilitation was observed in response to an acute cocaine injection. Conclusions Taken together, these results suggest that bulbectomized rats display alterations in brain reward function, but these changes are not long-lasting and thus, not amenable to investigating the effects of pharmacological interventions. However, given that OB animals are hypersensitive to drugs of abuse, bulbectomy may be an appropriate inducing factor for the development of animal models of co-morbid depression and drug dependence.",2007,10.1007/s00213-006-0630-x,no
Crossing the divide: Representation of channels and processes in reduced-complexity river models at reach and landscape scales,"Reduced-complexity models have considerable potential as tools for elucidating river behaviour over periods of 1 00 104 years and, consequently, for addressing fundamental questions concerning the scale-dependent nature of explanation in geomorphology. This paper proposes a simple subdivision of reduced-complexity models of river behaviour into two categories that mirror methodological developments in fluvial geomorphology over the past 50 years. First, high-resolution cellular approaches that are implemented within a framework that resolves process-form feedbacks at small time and space scales. Second, models that incorporate section-averaged representations of channel geometry and processes, and that are typically underpinned by regime theory and equilibrium concepts. Examples of both model types are presented here, in the form of a cellular representation of stream braiding and a combined lattice-network model of alluvial fan evolution. Simulations conducted using these models demonstrate how small-scale process-form interactions determine the emergence of larger-scale channel and fan morphology and, in so doing, regulate system response to external forcing. In this sense, both models demonstrate that internal feedbacks play a critical role in controlling river responses to environmental change over historic and Holocene timescales. However, both classes of model are characterised by uncertainty in their parameterisation of geomorphic processes, such that internal feedbacks and thresholds for channel response to external forcing may vary substantially between competing models. Methods of refining both approaches are considered, and hybrid models based on lattice-network structures and mechanistic representations of channel process-form interactions are identified as a means of addressing the shortcomings of existing strategies. (c) 2007 Elsevier B.V. All rights reserved.",2007,10.1016/j.geomorph.2006.10.026,no
"Evaluating regional predictive capacity of a process-based mercury exposure model, regional-mercury cycling model, applied to 91 Vermont and New Hampshire lakes and ponds, USA","Regulatory agencies must develop fish consumption advisories for many lakes and rivers with limited resources. Process-based mathematical models are potentially valuable tools for developing regional fish advisories. The regional mercury cycling model (R-MCM) specifically was designed to model a series of lakes for a given region with site-specific data and parameterization for each application. In this paper, we explore the feasibility of R-MCM application to develop regional fish advisories from existing data by testing model performance across 91 Vermont ([VT], USA) and New Hampshire ([NH], USA) lakes. We use a progressive method of parameter refinement ranging from simple defaults specified by the model to site-specific parameterization to evaluate potential improvements in model prediction. Model applications and parameter refinement tiers are based on Regional Environmental Monitoring Assessment Program (REMAP) data. Results show that R-MCM generally underpredicts water column methylmercury and total mercury concentrations and overpredicts sediment methylmercury concentrations. Default level input parameterization produced the largest amount of random scatter in model forecasted values. Using site-specific values for the default level characteristics reduced this variability but did not improve overall model performance. By separating the observed and predicted data by lake characteristics, we identify some overall trends in bias and fit, but are unable to identify systematic biases in model performance by lake type. This analysis suggests that process-based models like R-MCM cannot be used for a priori predictive applications at the regional scale at this time. Further, this work reinforces the need for additional research on the transport and transformation of mercury to elucidate parameterization useable in a modeling framework to help refine predictive capabilities of process-based models.",2007,10.1897/06-317R.1,no
Requirements change management process models: An evaluation,"Requirements are one of the major reasons for software-intensive system failures. Most of the requirements related problems emerge as results of changing requirements. To manage changing requirements in a better way many requirements change management (RCM) process models have been proposed in the literature. Process models facilitate human understating and communication, supporting process improvement and process management. Coverage of process elements and representational perspectives has an affect on process model. This paper reports evaluation of current requirements change management process models present in the literature on the basis of process model elements and representations. The aim is to highlight deficiencies in the current RCM process models, and to propose process ontology and the use of business process modeling notion (BPMN) to model RCM process.",2007,,no
Development and evaluation of a simple model-based automated fault detection and diagnosis (FDD) method suitable for process faults of large chillers,"Research into the development of fault detection and diagnosis (FDD) methods as applied to heating, ventilating, air-conditioning, and refrigerating equipment has been ongoing for over a decade, and several papers have been published. However studies specifically related to large chillers have been few despite the importance of such equipment. The objective of this paper is to propose and illustrate a simple model-based FDD method for medium to large chillers that uses sensors available in most cooling plants, allows tuning of specific thresholds so as to attain the desired compromise between robustness and sensitivity, and has the potential to be automated and implemented online. Lacking actual field data of chillers operated under fault-free and faulty operation, the performance of the proposed FDD method is demonstrated with data generated from tests on a laboratory chiller in the framework of an earlier research study. This proposed FDD scheme is based on five important characteristic features (that have been identified from 15 variables evaluated) that allow six process faults to be identified (although two cannot be done uniquely). Since large chillers are not prepackaged as is unitary equipment, their design and assembly allow selecting different subsystems for a stipulated maximum cooling capacity. In such a case, selection of specific variables needed for FDD will be impacted by different faults, depending on several aspects unique to the chiller installation: the type of chiller load control (thermostatic expansion valve or inlet guide vane), specific choice of the relative heat exchanger size, and the load fraction at which the chiller is usually operated. Hence, it is likely that some customization would be needed to adapt the FDD thresholds and association rules for each chiller installation. Different options and challenges relevant to practical implementation of the proposed FDD method are discussed.",2007,,no
"Sintering of polymers: Comprehension, modeling and application to rotomolding process","Sintering process is a phenomenon that occurs during the heating phase of a rotomolding cycle. Experiments were done using ethylene-propene copolymer, poly(ethylene) and grafted poly(ethylene). The temperatures chosen are the typical transformation temperatures for rotomolded parts. Two substrates were used to investigate their effects on the sintering. It appears that poly(ethylene) coalesces faster than grafted poly(ethylene), despite the first one has the higher viscosity. Interfacial tension between the material and its substrate seems to take a part in the sintering process. This is not taken into account in the models.",2007,,no
Evaluating Markov decision process as a model for decision making under uncertainty environment,"Soccer simulation is a suitable environment for implementing and testing modern Multi-Agent artificial intelligence distributed algorithms. Decision making under uncertainty conditions is one of the main problems in Multi-Agent systems. Uncertain and stochastic factors affect the performance of the action and make the agent unable to take the appropriate task. A high complexity of such environment causes decision making on the basis of traditional method (Hard Code) to be absolutely difficult, so new approaches of decision making such as Markov Decision Process (MDP) and Neural Networks are used. In our research, the soccer 3D simulation is used as a test bed for Multi-Agent systems. In this paper, the Markov Decision Process is used as a strategy for decision making of soccer 3D agents, and also a novel policy is proposed in our MDP method. The simulation results are used for performance evaluation of our proposed approach in comparison with traditional method in situations that agents' sense/act abilities have a lot of noise involved, the simulation results show that in our MDP method, the agents make better decision and carry out better action which help to reach the goal of plan.",2007,,no
An ontological process model for software maintenance and comprehension,"Software maintenance involves the integration, abstraction and analysis of different knowledge resources and artifacts. Maintainers are typically left with no guidance on how to utilize these resources to complete a particular maintenance task. In this research, we present a novel approach that integrates relevant maintenance resources and an existing software maintenance process through a formal unified ontological model. Reasoning and query services are applied to this ontological representation to establish traceability between resources and the process model and to provide maintainers with context sensitive guidance.",2007,,no
Understanding the impact of rigorous mask effects in the presence of empirical process models used in optical proximity correction (OPC) - art. no. 65203M,"Some practical aspects of integrating a mask modeling solution into the Optical Proximity Correction (OPC) framework are discussed. Specifically, investigations were performed to understand to what degree empirical process models used in OPC can compensate for mask effects when a Kirchhoff mask model is used. It is shown that both Constant Threshold Resist (CTR) models as well as more complex variable threshold process models can both compensate for mask effects at a single plane of focus. However, when looking through process window, neither process model can predict the focal behavior of Electro-Magnetic Field (EMF) simulators. The impact of mask effects will therefore need to be modeled in OPC, since process models cannot fully compensate for their effects. Heuristic approaches to modeling mask effects, like a constant biasing of feature edges, are then investigated and compared to more complex mask modeling solutions like Domain Decomposition Methods (DDM). It is shown that these heuristic approaches can be effective at single planes of focus to partially mitigate mask effects, however, do not provide complete solutions to predict and compensate for mask effects. DDM stands in stark contrast to heuristic methods, correctly predicting the through focus behavior of EMF simulations for the tested pitches and CDs. The impact of optical diameter (periodic boundary conditions) is also investigated to understand how the introduction of mask periodicity effects from optical diameter degrades the benefits derived from mask modeling. It is shown that as much as a 33% reduction in CD predictability is observed from an optical model with a 1 um optical diameter compared to a 2.5 6um optical diameter. Finally, both a Kirchhoff mask model and a DDM mask model are compared to see which mask model more accurately explains experimental CD measurement data from a 65mn process. The DDM model generally reduces the edge placement error (EPE) on the calibrated focal data by 0.3-1.0nm.",2007,10.1117/12.712362,no
Student understanding of the physics and mathematics of process variables in P-V diagrams,Students in an upper-level thermal physics course were asked to compare quantities related to the First Law of Thermodynamics along with similar mathematical questions devoid of all physical context. We report on a comparison of student responses to physics questions involving interpretation of ideal gas processes on P-V diagrams and to analogous mathematical qualitative questions about the signs of and comparisons between the magnitudes of various integrals. Student performance on individual questions combined with performance on the paired questions shows evidence of isolated understanding of physics and mathematics. Some difficulties are addressed by instruction.,2007,,no
Chemical processes effect on ambient air quality: modelling and primary/secondary pollutants monitoring study,The aim of this paper is to show the monitoring results of airborne pollutants around an urban area surrounded with a number of industrial facilities in the state of Kuwait. Data collected (2004-2005) in two different spans included primary and secondary ambient pollutants as well as major metrological conditions. The adsorption effect of PM10 on methane gas and other atmospheric chemicals recorded was investigated. A series of concentration and wind roses were constructed to study the prevailing wind and dispersion manner of the airborne chemicals from the different sources. Seasonal analysis was carried out with respect to ground and ambient methane levels from the upstream and downstream neighbouring facilities. A discussion on future strategies for the reduction of the different emitted pollutants from the industrial sites is given.,2007,10.2495/AIR070281,no
The study of complex system neural network modeling and validating for biofilter organic exhaust gas processing,"The biofilter exhaust gas processing is a complex biochemical reaction system, whose flux of exhaust gas, flux of circle liquid, pH value and temperature in biofilter can affect the exhaust gas purifying efficiency badly, and the nonlinear parameter make the process simulation, forecasting and control need There is no paper delivered on the nerve network (NN) modeling for biofilter. The paper analyses and studies on the complex biofilter exhaust gas processing, adopt BP arithmetic to set up NN model, compare with the experiment data and improve the arithmetic. The NN model structure is 2-35-15-1 and Fixes the number of the hidden layer nodes, the training error is 0.001, and after 36 iterative we can get the best NN structure. The experiment and simulation indicate that the model could equal to the biofilter exhaust gas processing, though there are some errors, it could satisfy to the modeling, forecasting and control need. The model can support a good base for the future control.",2007,,no
System evaluation and model verification using integrated processes of facultative biochemical reactor & constructed wetland for municipal wastewater,"The combination of facultative biochemical & constructed wetland technology is a newly cost-effective technology treating low concentratic domestic sewage. The biochemical Grau second-order kinetic model and the constructed wetland first-order kinetic model were used in this study. Eac parameter of the model was gained by a 200m(3)/d pilot plant, and the correlation between each kinetic parameter was also analyzed using Matlab7.0. When if influent COD is below 200mg.L-1, the HRT of the facultative biochemical unit is 8-9h and the hydraulic loading of the constructed wetland unit is less than 0.8m(3)/m(2).d, the effluent COD of the integrated technology is less than 50mg.L-1, that is the first grade A of the GB18918-2002 standard. Comparing if forecasted COD by the kinetic model with the actual COD, the correlation coefficient is 0.9093 that shown the model is worthy of application.",2007,,no
An evaluation of the coping patterns of rape victims - Integration with a schema-based information-processing model,"The current study sought to provide an expansion of Resick and Schnicke's information-processing model of interpersonal violence response. Their model posits that interpersonal violence threatens victims' schematic beliefs and that victims can resolve this threat through assimilation, accommodation, or overaccommodation. In addition, it is hypothesized that how victims resolve schematic threat affects their coping strategies. To test this hypothesis, a cluster analysis of rape victims' coping patterns was conducted. Victims' coping patterns were related to distress, self-worth, and rape label in ways consistent with predictions. Thus, future research should focus on the implications of how victims integrate trauma with schemas.",2007,10.1177/1077801207304825,no
Computer model to simulate the injection process in a rotary injection pump: The inverse problem,"The design of conventional injection systems requires the use of trial and error processes (with a high experimental load). Mathematical models, such as the one described in a previous work [Palomar et al. Energy Fuels 2005, 19, 1526-1535], helps in the design process. However, these models do not allow us to approach the inverse problem; that is to say, given a target law, the model cannot provide the system's dimensional characteristics which allow the injection system to reproduce the objective law. In this way, this study confronts a first approach to the resolution of the inverse problem, by means of an optimization algorithm over the direct problem. For this purpose, a second-order Newton's method is proposed to solve this problem. In this problem, a small number of parameters are maintained as variables to be used in the optimization algorithm. Thus, the main goal of the procedure is to generate a prearranged needle lifting law. The defined laws have been geometrically designed. They are indicative, without relation to a reference parameter, such as velocity, load, or injected fuel. As an initial approach, an unconstrained optimization has been carried out. Herein, we have demonstrated the efficiency of the proposed methodology to optimize the design of the engine fuel injection system according to the appropriate working conditions of the engine related to parameters such as combustion, performance, exhaust emissions, and noise.",2007,10.1021/ef0602022,no
Understanding the effects of cognition in creative decision making: A creativity model for enhancing the design studio process,"The essential components of creativity-persons, processes and products-were investigated inside a creative environment by deeply focusing on the cognitive stages of the creative decision making process. Mental imagery and external representation were considered as the implicit parts of creativity for enhancing design studio process. An experiment was conducted with 15 subjects who designed the public area of a train as the task in design studio. Observation, protocol analysis, and rating scales were used as assessment tools. Considering the components of creativity, it was found that the highest correlation was between process and overall creativity. Person and product followed process, respectively. However, no significant relationship was observed between imagery and creativity in design process. Three-dimensional representations were found to lead to more creativity compared to 2-dimensional depictions.",2007,,no
A common weighted performance evaluation process by using data envelopment analysis models,"The finance literature searches for a link between production and performance, controlling for variables, such as sales, firm size, employee number, etc., that influence production packages. The performance indices are designed according to the resulting effects and determine whether the performance model is appropriate or not, and/or whether the performance of system is good or not. In this paper, a process, based on data envelopment analysis (DEA), is developed to evaluate and rank the relative importance of key performance indices (KPIs). The relative importance of each KPI is evaluated by performance loss measure, and each KPI is weighted according to the measure. Then, the relative performance of each unit is the ratio of weighted output to weighted input based on the common weights.",2007,10.1109/IEEM.2007.4419306,no
A Novel quality inspection management model based on six-element structure in the process of software development,"The focus of this research is IT Innovation Management, especially, Quality Inspection Management in the Process of Software Development (QIM-PSD). In this paper, firstly, the technology of software quality inspection is reviewed, and the defects of traditional QIM-PSD are analyzed; secondly, a Quality Inspection Management Model based on Six-element Structure in the Process of Software Development (QIMM-PSD6es) is proposed. The structure of six-element, Knowledge (K), Prediction (P), Inspection (I), Estimation & Standard (E S), Processing-Leaning & Track-Leaning (L) and Defects List (DL), is defined. Thirdly, the paper presents the work flow of model by figures. Last but not least, the novel model has been successfully implemented in the quality inspection management of CoDesign system development process. With this model, the QIM-PSD can be continuously monitored, predicted and corrected, while the track-learning, the capability and the efficiency of QIM-PSD are improved obviously.",2007,,no
GPS receivers timing data processing using neural networks: Optimal estimation and errors modeling,"The Global Positioning System (GPS) is a network of satellites, whose original purpose was to provide accurate navigation, guidance, and time transfer to military users. The past decade has also seen rapid concurrent growth in civilian GPS applications, including farming, mining, surveying, marine, and outdoor recreation. One of the most significant of these civilian applications is commercial aviation. A stand-alone civilian user enjoys an accuracy of 100 meters and 300 nanoseconds, 25 meters and 200 nanoseconds, before and after Selective Availability (SA) was turned off. In some applications, high accuracy is required. In this paper, five Neural Networks (NNs) are proposed for acceptable noise reduction of GPS receivers timing data. The paper uses from an actual data collection for evaluating the performance of the methods. An experimental test setup is designed and implemented for this purpose. The obtained experimental results from a Coarse Acquisition (C/A)-code single-frequency GPS receiver strongly support the potential of methods to give high accurate timing. Quality of the obtained results is very good, so that GPS timing RMS error reduce to less than 120 and 40 nanoseconds, with and without SA.",2007,10.1142/S0129065707001226,no
Evaluation of the impact of different human factor models on industrial and business processes,"The human factor is becoming a main topic in modelling and simulation; this paper proposes an approach for evaluating quantitatively the potential benefits for introducing these aspects versus the different possible modelling alternatives in order to support the simulation experts. In particular, this research is focused on micro-logistics in order to emphasise the impact of human behaviour in industrial and business processes, providing also real cases as support for validation of the proposed approach. (c) 2006 Elsevier B.V. All rights reserved.",2007,10.1016/j.simpat.2006.09.020,no
Chemical vapor deposition of thin films and coatings: Evaluation and process modeling,"The macroscopic modeling of CVD reactors is aimed at paving the way towards linking the film properties to process parameters. The models used are based on thermodynamic, kinetic and transport data bases and they involve thermodynamic, kinetic and heat and mass transfer calculations which can be interlinked. This presentation describes specific examples of the application of data bases and software packages to solve CVD modeling problems arising in the growth of apparently well-known materials, like SiC single crystals or SrTiO(3) thin layers with high dielectric permittivity. The benefits of this kind of approach will be emphasized. At the same time, the dangers related to the use of incorrect data (even in good data bases), inaccurate phase diagrams or MOCVD precursors of unknown thermal decomposition are illustrated, showing that without high-quality data even the most advanced model cannot be of any value. (C) 2007 Elsevier B.V. All rights reserved.",2007,10.1016/j.surfcoat.2007.04.131,no
A modified two-tuple FLC model for evaluating the performance of SCM: By the Six Sigma DMAIC process,"The objective of this study aims at proposing a modified 2-tuple fuzzy linguistic computing ( FLC) model to evaluate the performance of Supply chain management ( SCM). In this model, the management implication of high precision setting involving in the Six Sigma- Define, Measure, Analyze, Improve and Control ( DMAIC) processes is employed to construct the evaluation framework. The original 2-tuple fuzzy linguistic representation model is modified as the proposed model to provide the aggregation algorithm toward ensuring the consistent property. In this study, the Delphi method is used to precisely integrate the experts' opinions on criterion selection, weighting identification and performance appraisal that are realistically expressed by fuzzy linguistic variables. The modified 2-tuple FLC technique is formed by utilizing a geometric operator and a couple of new symbol translation functions to aggregate precisely the 2-tuple terms involved. Finally, in the case verification, two representatives of the mechanical firms in Taiwan are selected as the target firms to verify the capability of the proposed model. To show the superiority, a comparative analysis on the results obtained using the modified model and the original model is presented. (c) 2006 Elsevier B. V. All rights reserved.",2007,10.1016/j.asoc.2006.06.008,no
Development and process evaluation of the participatory and action-oriented empowerment model facilitated by occupational health nurses for workplace health promotion in small and medium-sized enterprises,"The objective of this study is to develop an available empowerment model for workplace health promotion (WHP) in small and medium-sized enterprises (SMEs) and to evaluate its applicability and feasibility. Semi-structured interviews with employers and workers in SMEs were conducted to assess their actual requirements for support. The structure of our new empowerment model was discussed and established through several rounds of focus group meetings with occupational safety and health researchers and practitioners on the basis of results of our interviews. We developed a new participatory and action-oriented empowerment model based on needs for support of employers and workers in SMEs. This new model consists of three originally developed tools: an action checklist, an information guidebook, and a book of good practices. As the facilitators, occupational health nurses (OHNs) from health insurance associations were trained to empower employers and workers using these tools. Approximately 80 SMEs (with less than 300 employees) were invited to participate in the model project. With these tools and continued empowerment by OHNs, employers and workers were able to smoothly work on WHP. This newly developed participatory and action-oriented empowerment model that was facilitated by trained OHNs appears to be both applicable and feasible for WHP in SMEs in Japan.",2007,10.2486/indhealth.45.62,no
Character process model for semen volume in AI rams: evaluation of correlation structures for long and short-term environmental effects,"The objective of this study was to build a character process model taking into account serial correlations for the analysis of repeated measurements of semen volume in AI rams. For each ram, measurements were repeated within and across years. Therefore, we considered a model including three environmental effects: the long-term environmental effect, which is a random year* subject effect, the short-term environmental effect, which is a random within year subject* collection effect, and the classical measurement error. We used a four-step approach to build the model. The first step explored graphically the serial correlations. The second step compared four models with different correlation structures for the short-term environmental effect. We selected fixed effects in the third step. In the fourth step, we compared four correlation structures for the long-term environmental effect. The model, which fitted best the data, used a spatial power correlation structure for the short-term environmental effect and a first order autoregressive process for the long-term environmental effect. The heritability estimate was 0.27 (0.04), the within year repeatability decreased from 0.56 to 0.44 and the repeatability across years decreased from 0.43 to 0.37.",2007,10.1051/gse:2006033,no
Impact of prior physico-chemical treatment on the clogging process of subsurface flow constructed wetlands: Model-based evaluation,"The objective of this study was to check the effect of the use of a physico-chemical treatment on the clogging process of horizontal subsurface flow constructed wetlands by means of dynamic modelling. The hydraulic submodel was based on series as well as parallel branched complete stirred tanks of equal volume. The model was validated with data obtained from 2 identical experimental wetlands, which had a surface area of 0.54 m(2) and a water depth of 0.30 m, and that were monitored over a period of 5 months. One of the wetlands was fed with settled urban wastewater, whereas the other with the same wastewater, but previously treated with a physico-chemical treatment. In the model, pore volume reduction depends on the growth of bacteria and on solids retained. The effluent concentrations of COD and ammonium in both experimental wetlands were very similar in all the conditions tested, and therefore the physico-chemical treatment did not improve the removal efficiency. The model indicated that after 120 days of operation in some regions of the wetland fed with settled wastewater the porosity decreased in a 17%, whereas in the other wetlands it only decreased as much as 6%. The use of a prior physico-chemical treatment is a good alternative for avoiding an anticipated clogging of subsurface flow constructed wetlands.",2007,10.1007/s11270-007-9434-9,no
Model based evaluation for the anaerobic treatment of corn processing wastewaters,"The objective of this study was to implement a process model to simulate the dynamic behavior of a full-scale anaerobic reactor treating corn processing wastewaters under mesophilic conditions. For this purpose, the IWA Anaerobic Digestion Model No.1 (ADMl) was applied to an expanded granular sludge bed reactor of an industrial wastewater treatment plant. The plant has a wastewater treatment capacity of 1594 m(3)/day. Sensitivity analysis showed that the influent characterization, based on disintegration into carbohydrates, proteins, lipids and inert components, had a large impact on the model outputs. An efficient correlation obtained between the measured and the simulated data, including effluent COD, pH and methane production, highlights good parameter optimization and wastewater characterization. This paper demonstrates a practical example for the application of the ADM1 to an agro-industrial full-scale anaerobic reactor.",2007,10.1002/clen.200700105,no
Modelling efficiency in insect olfactory information processing,"The olfactory system of insects is essential for the search of food and mates, and weak signals can be detected, amplified and discriminated in a fluctuating environment. The olfactory system also allows for learning and recall of odour memories. Based on anatomical, physiological, and behavioural data from the olfactory system of insects, we have developed a cross-scale dynamical neural network model to simulate the presentation, amplification and discrimination of host plant odours and sex pheromones. In particular, we model how the spatial and temporal patterns of the odour information emerging in the glomeruli of the antennal lobe (AL) rely on the glomerular morphology, the connectivity and the complex dynamics of the AL circuits. We study how weak signals can be amplified, how different odours can be discriminated, based on stochastic (resonance) dynamics and the connectivity of the network. We further investigate the spatial and temporal coding of sex pheromone components and plant volatile compounds, in relation to the glomerular structure, arborizing patterns of the projection neurons (PNs) and timing patterns of the neuronal spiking activity. (C) 2006 Elsevier Ireland Ltd. All rights reserved.",2007,10.1016/j.biosystems.2006.04.021,no
Methods for modal model quality improvement - Measurement data pre-processing for model identification process (part I),The paper concerns techniques of modal model quality improvement at the stage of data preprocessing for the purposes of identification process. Basic concepts of problem formulation ( matrix conditioning) improvement by the use of the Tikhonov regularization method are presented as well as examples of method application to solution estimation of the Fredholm integral equation of the first kind and to transfer function matrix noise reduction. In the second part of the paper the authors proposed the method for transfer function estimation on the basis of data measured by the use of impulse test on big industrial systems for which classical methods fail for the reason of low coherence between exciting force and system response. Two examples of method application are presented.,2007,10.1177/1077546307074574,no
Time model of data processing block in measurement system,The paper presents application of the Petri Nets to the modelling of the data processing in the distributed measurement system. The main advantage of the model is the consideration of the time dependencies both in server and client side of the system. Details of the model and tools used to its design are presented. Experiments using different computer configurations and integrated programming environments are described. Verification of the model and conclusions about its applications are included.,2007,,no
Modeling cooperative problem solving process with extended alternating-time temporal logic,"The paradigm example of social interaction is cooperative problem solving, in which a group of autonomous agents choose to work together to achieve a common goal. In order to improve existing work on developing formal computing model for cooperative problem solving in multi-agent systems, several efforts are made in this paper. Firstly, a new multi-agent cooperation logic called ATL-BDI designed in our previous work is enhanced so as to make it more suitable for modeling cooperative problem solving. Secondly, basing on ATL-BDI, the cooperative problem solving process is formally described. The corresponding new computing models of cooperation recognition, team formation, plan formation and execution are developed. The iteration of these steps is also well considered. Several important properties are gained and proved finally.",2007,10.1109/SNPD.2007.525,no
"Cortical representation of verb processing in sentence comprehension: Number of complements, subcategorization, and thematic frames","The processing of various attributes of verbs is crucial for sentence comprehension. Verb attributes include the number of complements the verb selects, the number of different syntactic phrase types (subcategorization options), and the number of different thematic roles (thematic options). Two functional magnetic resonance imaging experiments investigated the cerebral location and pattern of activation of these attributes. Experiment 1 tested the effect of number of complements. Experiment 2 tested the number of options of subcategorization and of thematic frames. A group of mismatch verbs with different number of options for subcategorization and thematic frames was included to distinguish between the effects of these attributes. Fourteen Hebrew speakers performed a semantic decision task on auditorily presented sentences. Parametric analysis revealed graded activations in the left superior temporal gyrus and the left inferior frontal gyrus in correlation with the number of options. By contrast, the areas that correlated with the number of complements, the right precuneus and the right cingulate, were not conventionally linguistic. This suggests that processing the number of options is more specifically linguistic than processing the number of complements. The mismatch verbs showed a pattern of activation similar to that of the subcategorization group but unlike that of the thematic frames group. By implication, and contrary to claims by some linguists, subcategorization seems indispensable in verb processing.",2007,10.1093/cercor/bhl105,no
An empirical study of the relationship of stability metrics and the QMOOD quality models over software developed using highly iterative or agile software processes,"The purpose of our study is to analyze whether the Bansiya and Davis quality models also reflect the ongoing stability of a software design in software developed using a highly iterative or agile process. We performed an empirical study over multiple iterations of various software packages developed using highly iterative or agile methods. We examined several Bansiya and Davis quality factor models (reusability, flexibility, understandability, functionality, extendibility, and effectiveness) over this data set and we compared them to stability metrics. We conclude that the Bansiva and Davis total quality index does indeed reflect stability over the data sets examined.",2007,10.1109/SCAM.2007.29,no
An inter organization communication architecture model for real time activity base online transmission to increase the work process efficiency of container terminal,"The purpose of this research is to analyze the impact of instituting automated container yard management on the operation of a prototype EBILCY. This paper studies a total yard operation inflow and outflow of containers and cargos where different stakeholders depend on the operation of off-dock terminal. The primary challenge is to efficiently operate the operations and acquire the real-time data to minimize the operation lag due to information passing delay. In particular, to automate the every process, an online activity base strategy is used. This strategy groups jointly co-operate with OLAP and data mining process to reduce the manual work. An approach has been introduced to automate the all operational activity of off-dock as well as container terminal in real time basis. The methodology is approaching a cost-effective and authenticated communication through different stake holders of container terminal i.e. Main Line Operator (MLO), Freight Forwarder, C&F Agents, Shipper, Consignee etc. The model is solved using message passing strategy. Due to required interaction and multiple hub access issues a standard structure is introduced with details specification. The communication architecture has implemented through the authenticated portable database tool, Extended Markup Language (XML). To establish the easy communication through the end point, email client is introduced. For ensuring the information security MD5 encryption method has also used. This approach helps to atomize the full industry segment to work in one umbrella. The standard message structure ensures data validity among the stakeholders. This approach also can be used to communicate within different application where this approach would be work as a communication agent.",2007,,no
A computational model of metaphor understanding consisting of two processes,"The purpose of this study is to construct a computational model of the metaphor understanding process. This study assumes that metaphor understanding consists of two processes. The first is a categorization process; a target is assigned to an ad hoc category of which the vehicle is a prototypical member. The second is a dynamic interaction process, the target assigned to the ad hoc category is influenced by dynamic interaction among features. Feature emergence is extracted through this dynamic interaction. In this study, a model of metaphor understanding is constructed based on this assumption by applying a statistical analysis of large-scale corpus. Further a psychological experiment is conducted in order to verify the psychological validity of the constructed model of metaphor understanding. Reflecting the fact that the constructed model represents more appropriate features of a metaphor than a model incorporating only the categorization process, the experimental results support its validity.",2007,,no
A process framework for customising software quality models,The quality objective of many software organisations is to deliver software products that meet and or exceed customer expectations. The key to achieving this is to capture these expectations at the beginning of the project by clearly defining all quality requirements. The characteristics particularly defined in ISO/IEC 9126-1 (2001) provide the framework for specifying quality requirements. The ISO/IEC 9126-1 quality model is intended to be applicable to any type of software product or intermediate product. Before application this model needs to be tailored to a specific software and specific need. Since these characteristics cannot be directly measured this makes it difficult to directly prioritise and choose the most relevant characteristics and sub-characteristics. Hence a process framework that will link these characteristics and subcharacteristics to user needs is required. This will in turn help customise software quality models like ISO/EEC 9126-1 (2001) and other general software quality models. A process framework for customising software quality models is proposed in the text and it is further shown how this framework was applied in a real working environment in an attempt to quantitatively validate it. The results collected in the study showed that the framework could be used reliably in customising a generic software quality model at characteristic level only. The deviations at sub-characteristic level were due to unclear questions in the generated Generic Quality Questionnaire that resulted in misunderstandings. And the metrics used to create these questions were not fully tested for validity and reliability due to time constraints. Enhancements are discussed in the study and it is further shown how reliability can also be achieved at sub-characteristic level.,2007,,no
Proposal of thinking process model based on putting a question to oneself for problem solving by skilled engineers,"The retirement of the skilled engineers and researchers, who have been working in the field of water chemistry of BWRs since the establishment of Japanese BWRs, has been accelerating so that it is necessary to preserve and share the knowledge and know-how of these skilled engineers and to effectively and securely inherit them to the following generations. Moreover, in order for other engineers to share knowledge, including the tacit knowledge of a skilled engineer, it is necessary not only to understand the knowledge obtained as the results of the thinking of the skilled engineers but also to externalize and experience their thinking processes as the processes of problem solving that produced the results. The target of this research is to build a new thinking process model that can externalize thinking processes effectively by analyzing the knowledge and thinking contents of the skilled engineers for investigating causes of changes in the quality of reactor water in boiling water reactor type power plants. As part of this research, we found out a new metacognitive activity, Putting a Question to Oneself, which plays a large role in the thinking process structure. In addition, we developed a new fundamental thinking process model, which can externalize thinking processes effectively by integrating the concept of Putting a Question to Oneself into the thinking process model.",2007,10.3327/jnst.44.997,no
Software process conversion rules in ImPProS - Quality models conversion for a software process implementation environment,"The software process conversion is a technique based on the mapping of the existing relationship between the content of the quality norms/models. The basic estimated of the conversion is to obtain making an adaptation of the software processes without the necessary effort to specify new models, guaranteeing the unicity and the consistency. For a company to reach a definitive market, its software process will have to be guided by patterns defined for a norm, and if it glimpses the penetration in other markets perhaps it is necessary the guide for other norms so different. This paper presents a process to convert software processes using quality models/norms, and a discussion of some rules used to support the execution of this process in a software development context. This process is part of a software process implementation environment, called ImPProS, developed at CIn/UFPE - Center of Informatics/Federal University of Pernambuco.",2007,,no
"Estimating water quality pollution impacts based on economic loss models in urbanization process in Xi'an, China","The study investigates water quality pollution impacts on urbanization by analyzing temporal and spatial characteristics of different water quality parameters, and simulating economic loss of water quality pollution in Xi'an, China from 1996 to 2003. Results show that organic pollutants were the greatest contributors of surface water quality pollution from 1996 to 2003. High values existed in petroleum concentration, chemical oxygen demand index (KMnO4), biochemical oxygen demand index, and phenol concentration, followed by nitrogen concentration (TN and NH3-N). From spatial analysis in different buffers from central urban area (inner buffers: 1-5 km; central buffers: 5-10 km; outer buffers: > 10 km), socioeconomic activities such as business activities, car transportation, industry factories, agriculture practices, and households were likely to lead to different behaviors of water quality parameters in nature. Results also reveal that both surface and ground water quality improved gradually after enforcement of control measures within the 7 years from 1996 to 2003. It shows the total economic loss, including cost of water use and supply, agriculture economic loss, ecosystem conservation costs, and economic loss of human health, reached $1.12 X 10(9) from 1996 to 2003, which increased $1.79 X 10(7) from $1.26 X 10(8) in 1996 to $1.46 X 10(8) in 2003. However, economic loss of water quality pollution increased while water quality pollution alleviated in the past years. This can be explained by more intensive social activities in broader regions, more populations were moved from rural area into urban area, and more costs were input in water quality pollution treatment.",2007,10.1061/(ASCE)0733-9488(2007)133:3(151),no
Problems of numerical modelling of processes determined by shear fracture,"The term 'processes determined by shear fracture' covers both material mechanical working processes and material wear processes. An interdisciplinary approach to the material strain-fracture relations is needed to model such processes. The problem is that there is still no adequate model of the relations. Attempts to solve the problem through numerical modelling raise many doubts, particularly as to the way in which the transition from modelling the development of uniform strains to modelling the onset of fracture is made. Another problem is the lack of adequate criteria for verifying the results of numerical simulations. It is shown that direct transition from uniform strains (described by a strain hardening curve) to fracture (determined on the basis of ductile fracture criteria) is a source of serious errors. It has been found that shear fracture is determined by the development of strains and fracture in the so far neglected zone of localization of nondilatational strains. The discussion is illustrated with the results of numerical simulations and experiments.",2007,,no
Process modeling and validation of resin infusion type manufacturing,"The use of composite materials in large primary structures such as wind turbine blades and boat hulls has dramatically increased in recent years. The manufacturing of these structures is shifting from hand lay-up to more modem large volume infusion processes. To perform these processes very expensive tooling is required, which may prohibit the trial and error approach that has been used for many years. The need for accurate process modeling in the design of tooling is becoming essential. Unfortunately, as the processes become more complex so do the models. The goal of this work was to develop a modeling technique that could accurately model these processes, yet not so complex as to loose its utility. The model correlated well with the experiments performed and revealed critical information about these types of processes. A major conclusion is that an accurate and straightforward model can be created for large scale processes, using the small scale bench tests performed in this study. Also, the governing equations developed here from Darcy flow and Stokes flow aid in understanding how the scaling of key parameters affects the process as a whole.",2007,,no
Evaluation of grout concentration during grouting process based on convective dispersive flow model,"The use of ultrafine cement-based grout has been gaining importance for soil grouting over the last few years. Laboratory experiments were conducted to examine the spread of grout injection through porous media under saturation conditions. A part of the complete breakthrough curves could be reasonably approximated using simple one-component models. In this paper, an evaluation of grout concentration using image processing technique and Beer-Lambert theory is presented. By differentiation and calibration for different soil densities, we can evaluate the reflectance of interstitial fluid in saturated soil column. This latter can directly be linked to concentration of cement particles in the grout at different times of grouting injection.",2007,,no
Comparison and evaluation of academic process designer for workflow modeling and analysis,"The workflow management technology provides a flexible and appropriate solution to facilitate the development of new business process and the analysis of existing ones. Process designer is such a functional component in workflow management system (WfMS). As an indication, in one hand, most of commercial WfMSs do not yet provide process designers with formal analysis, especially verification, tool. In the other hand, this lack has increased more and more development of process designer, according to different purposes and using different techniques. In this paper, we make an overview of some representative academic process designers; one of them is developed by our team, then compare and evaluate them by the criterions of workflow modeling and workflow analysis. Also some further issues of future process designer are proposed and discussed. The primary concern of this paper is to make the review job easier, and also provide a comparative assessment and aid in other people's decisions and selections.",2007,,no
Developing students' understanding and thinking process by model construction,"There is growing recognition that models play a fundamental role in the comprehension of science concepts. This paper aims at enhancing students' understanding and thinking by model construction. Seventh grade middle school students from an urban public school participated in this study as a part of their weekly science club that met after the regular school hours. During the course of the study, the students investigated several common environmental factors about animals by collecting data, made drawings, constructed objects, and wrote journals. The use of journals allowed them to reflect on their experience and the abstract concepts. It also allowed the researchers to have indirect access to the models underlying the meanings students were making every week. Students developed initial models, came up with analogies, and constructed newer models by improving the initial ones. The findings show that model criticism and modification processes are promising activities for science education.",2007,,no
Model based layout pattern dependent metal filling algorithm for improved chip surface uniformity in the copper process,"Thickness range, i.e. the difference between the highest point and the lowest point of the chip surface, is a key indicator of chip yield. This paper presents a novel metalfilling algorithm that seeks to minimize the thickness range of the chip surface during the copper damascene process. The proposed solution considers the physical mechanisms in the damascene process, namely ECP (which is the process used to deposit Cu in the trenches) and CMP (which is the process used to polish Cu after ECP), that affect thickness range. Key predictors for the final thickness range, which is the thickness range after ECP & CMP, that can be computed efficiently are identified and used to drive the metal filling process. To the best of our knowledge, this is the first metal filling algorithm that uses an ECP model among other things to guide metal filling. Experimental results are very promising and indicate that the proposed method can significantly reduce the thickness range after metal filling. This is in sharp contrast with the density-driven approaches which often increase the thickness range after metalfilling, thereby potentially adversely impacting yield. In addition, the proposed method inserts significantly smaller amount of fill when compared to the densitydriven approaches. This is desirable as it limits the impact Of metalfilling on timing.",2007,,no
Modeling Soft Error Effects Considering Process Variations,"This paper addresses the aggregated effects of two types of variations that contribute to the reliability degradation The first one is the increasing level of process variation; the second one is one particular type of environmental variation - the radiation-induced soft error. Their simultaneous presence can cause large negative performance impact. We present a statistical approach to model the generation and propagation of a transient soft error inside combinational circuits considering the existence of inter-die channel length variation in CMOS digital circuits. Experiment results have demonstrated that channel length variation can significantly aggravate the soft error effect, which can be accurately evaluated using the proposed methodology.",2007,10.1109/ICCD.2007.4601927,no
Methods for modal model quality improvement - Parameter estimation process (part II),"This paper concerns quality improvement of modal models estimated by the use of ""standard"" modal analysis methods. The authors proposed application of the Least Squares Frequency Domain method making use of the Gauss-Newton algorithm to the quality improvement of stabilization diagrams as well as application of the Balanced Realization method to identify and remove spurious poles from stabilization diagrams. Formulated algorithms were implemented in the Matlab environment and verified for data measured on real structures. Created software supports critical stages of modal analysis, which results in quality improvement of estimated modal models and higher credibility of drawn conclusions.",2007,10.1177/1077546307082984,no
An EPQ model with setup cost and process quality as functions of capital expenditure,"This paper considers an economic production quantity (EPQ) model with imperfect production processes, in which the setup cost and process quality are functions of capital expenditure. The mathematical model is derived to investigate the effects of an imperfect production process on the optimal production cycle time when capital investment strategies in setup reduction and process quality improvement are adopted. An efficient procedure is developed to find the optimal production run length, setup cost and process quality. Finally, a numerical example is provided to illustrate the theoretical results. Some managerial implications are also included. (c) 2006 Elsevier Inc. All rights reserved.",2007,10.1016/j.apm.2006.03.034,no
Contribution of primary carbonaceous aerosol to cloud condensation nuclei: processes and uncertainties evaluated with a global aerosol microphysics model,"This paper explores the impacts of primary carbonaceous aerosol on cloud condensation nuclei (CCN) concentrations in a global climate model with size-resolved aerosol microphysics. Organic matter (OM) and elemental carbon (EC) from two emissions inventories were incorporated into a preexisting model with sulfate and sea-salt aerosol. The addition of primary carbonaceous aerosol increased CCN(0.2%) concentrations by 65-90% in the globally averaged surface layer depending on the carbonaceous emissions inventory used. Sensitivity studies were performed to determine the relative importance of organic solubility/hygroscopicity in predicting CCN. In a sensitivity study where carbonaceous aerosol was assumed to be completely insoluble, concentrations of CCN(0.2%) still increased by 40-50% globally over the no carbonaceous simulation because primary carbonaceous emissions were able to become CCN via condensation of sulfuric acid. This shows that approximately half of the contribution of primary carbonaceous particles to CCN in our model comes from the addition of new particles (seeding effect) and half from the contribution of organic solute (solute effect). The solute effect tends to dominate more in areas where there is less inorganic aerosol than organic aerosol and the seeding effect tends to dominate in areas where there is more inorganic aerosol than organic aerosol. It was found that an accurate simulation of the number size distribution is necessary to predict the CCN concentration but assuming an average chemical composition will generally give a CCN concentration within a factor of 2. If a 'typical' size distribution is assumed for each species when calculating CCN, such as is done in bulk aerosol models, the mean error relative to a simulation with size resolved microphysics is on the order of 35%. Predicted values of carbonaceous aerosol mass and aerosol number were compared to observations and the model showed average errors of a factor of 3 for carbonaceous mass and a factor of 4 for total aerosol number; however, errors in the accumulation mode concentrations were found to be lower in comparisons with European and marine observations. The errors in CN and carbonaceous mass may be reduced by improving the emission size distributions of both primary sulfate and primary carbonaceous aerosol.",2007,,no
Error modeling for five-axis laser processing robot,"This paper introduces the research on error modeling of a five-axis laser processing robot which has three linear axes and two rotational axes. The error models are developed based on vector/matrix notion. Homogeneous coordinate transformations are employed to represent the ideal and real linear\rotation transformations of the five-axis laser processing robot by considering small angular approximation. Position error associated function which can reflect the influence of each error origin on the positioning error of the machine tool is given to describe the transmission error of the robot in detail. Based on this method, the paper puts forward the error model of the five-axis laser processing robot.",2007,10.1109/ROBIO.2007.4522462,no
A comprehensive study for real-time learning of wave-net models of a nonlinear time-varying experimental process,This paper presents a real time on-fine learning study of an experimental thermal process based on on-line wave-nets. Wave-nets are wavelet based neural networks with localized and hierarchical multi-resolution learning. The multi-resolution framework of wave-nets allows nonlinear modeling of any complicated systems. The recently developed on-line features to wave-net learning have enhanced their capability in learning and adaptation of any non-linear time varying systems (with low dimensions). A real experimental time varying thermal process is modeled and simulated to show the applicability of these algorithms. The platform used for the Real-Time implementation is MATLAB/REAL TIME TOOLBOX using a DAQ interface. The results show the effectiveness of the methods.,2007,,no
A quality prediction framework for multistage machining processes driven by an engineering model and variation propagation model,"This paper proposes a comprehensive quality prediction framework for multistage machining processes, connecting engineering design with the activities of quality modeling, variation propagation modeling and calculation, dimensional variation evaluation, dimensional variation analysis, and quality feedback. Presented is an integrated information model utilizing a hybrid (feature/point-based) dimensional accuracy and variation quality modeling approach that incorporates Monte Carlo simulation, variation propagation, and regression modeling algorithms. Two important variations (kinematic and static) for the workpiece, machine tool, fixture, and machining processes are considered. The objective of the framework is to support the development of a quality prediction and analysis software tool that is efficient in predicting part dimensional quality in a multistage machining system (serial, parallel, or hybrid) from station level to system level.",2007,10.1115/1.2752520,no
Validation of the awareness net model for the Australian security investment processes,"This study evaluates the appropriateness of an existing collaborative business process model for representation of the collaboration context within the Australian security investment market (ASIM). It is an initial step towards developing knowledge-base system of a management support system for investment decision processes within the ASIM. The three aspects of appropriateness evaluated correspond to the three quality categories namely, syntactic, semantic and pragmatic qualities using the conceptual model quality framework (CMQF) which is based on semiotic theory. An interpretivist case study methodology is adopted using a combination of theory building and sense-making case study strategies. Seven in-depth interviews were conducted in order to assess the above three appropriateness measures by the actors within the ASIM whose roles correspond to those in the awareness model. The study highlighted required modifications to the concept descriptions of the initial version of the model. A modified awareness net was then constructed accordingly. (c) 2006 Elsevier B.V. All rights reserved.",2007,10.1016/j.knosys.2006.10.005,no
"A family process model of marital hostility, parental depressive affect, and early adolescent problem behavior: The roles of triangulation and parental warmth","This study examined a family process model of early adolescent problem behavior in a community sample of 416 two-parent families. With family systems theory, a model was developed that suggests (a) marital hostility and parental depressive affect are conjoint familial stressors for youths, (b) youth triangulation mediates the association between marital hostility and adolescent problems, and (c) parental warmth buffers the negative effects of parental depressive affect and youth triangulation. With structural equation modeling, youth-perceived triangulation mediated the association between marital hostility and adolescent internalizing problems. Marital hostility was associated with externalizing problems. Mothers' depressive affect was associated with internalizing problems, and fathers' depressive affect was associated with internalizing and externalizing problems. Parental warmth was not a significant moderator.",2007,10.1037/0893-3200.21.4.614,no
Partially testing a process model for understanding victim responses to an anticipated worksite closure,"This study partially tested a recent process model for understanding victim responses to worksite/function closure (W/FC) proposed by Blau [Blau, G. (2006). A process model for understanding victim responses to worksite/function closure. Human Resource Management Review, 16, 12-28], in a pharmaceutical manufacturing site. Central to the model are the Kubler-Ross [Kubler-Ross, E. (1969). On death and dying. New York: Macmillan] grieving stages, which have not been formally measured and applied to downsizing research. Following Blau (2006), individual grieving stages were successfully measured and clustered into more general grieving categories, i.e., negative (denial, anger, bargaining depression) and positive (exploration, acceptance). Across four waves of data 53 respondents constituted the complete data sample. The Time I personal factors had minimal impact on any type of response. However, Time 1 situational factors did have an impact, paced by higher perceived contract violation leading to greater strain, work incivility, organizational deviance, and intent to sue employer, and lower transactional obligations and employer endorsement. Earlier Time 2 grieving stages were used as individual antecedents in regression analyses to explain Time 3 (N = 77) victim responses (general strain, work incivility, interpersonal deviance, organizational deviance, transactional obligations, relational obligations) and also Time 4 (N = 53) prior to closure responses (intent to sue employer, employer endorsement). Within negative grieving, results indicated that greater anger was the most influential grieving stage, since it led to greater strain, work incivility, organizational deviance, and intent to sue, as well as lower transactional obligations and lower endorsement. Within positive grieving acceptance was the most influential, since it led to lower strain, lower work incivility, lower organizational deviance, and lower intent to sue. Study limitations and future research issues are discussed. (c) 2007 Elsevier Inc. All rights reserved.",2007,10.1016/j.jvb.2007.08.005,no
A comprehensive kinetic model of the exocytotic process: Evaluation of the reaction mechanism,"This study presents a comprehensive, quantitative description of the exocytotic process that has both analytic and predictive powers. The model utilizes strict chemical formalism and is based on a set of equilibria between the various SNARE proteins, their complexes and the reaction which free Ca2+ ions. All these reactions are linked by first and second order rate constants. With the proper set of rate constants, which were selected by a systematic search in a multi dimensional parameter space, the model reconstructs the fusion dynamics as recorded under divergent experimental protocols: the effect of repeated depolarization, recovery after depletion of the vesicular pools of the cell, and over-expression or knockout of specific proteins. The model provides a detailed scenario of the maturation process, where the vesicles progress from the early steps of the SNARE complex formation up to the last event of the vesicles' fusion with the cells' plasma membrane. The dynamics of each intermediate enable us to describe the experimental result in terms of overall flow, where upstream intermediates are mobilized during the progression of the reaction.",2007,,no
Identification and evaluation of a dynamic model for a thin film deposition process,"This study proposes an algorithm for computing a dynamic model for a thin film deposition process. The proposed algorithm is used on high dimensional Kinetic Monte Carlo (KMC) simulations and consists of applying principal component analysis (PCA) for reducing the state dimension, self organizing map (SOM) for grouping similar surface configurations and simple cell mapping (SCM) for identifying the transitions between different surface configuration groups. The error associated with this model reduction approach is characterized by running 600 test simulations with highly dynamic and random input profiles. Global error, which is the normalized Euclidean distance between the real and predicted states, is found out to be 0.0058 on average for the test simulations. Our study shows that the proposed algorithm is useful for extracting dynamic models from high dimensional and noisy molecular simulation data.",2007,,no
Experimental evaluation of repair process of burn wound treated with aqueous extract of Achillea millefolium on animal model: Clinical and histopathological study,"This study was performed to evaluate the accelerating effect of aqueous extract of yarrow on the burn wound healing in an animal model. Ten male adult white Dutch rabbits, with mean weight of 2,000 +/- 250 g were studied. Burn wounds were created in dorsal region of each animal, according to Hoekstra model. The experiment wounds were treated with 5 mL of aqueous extracts of yarrow every day. The control wounds were only washed with the same amount of normal saline. The wounds were photographed and compared for rate of wound contraction with digital scanning software. Specimens were taken for histopathological examinations on day 21. Significant differences were seen between the experiment and control wounds for the rate of contraction. On histopathological evaluation granulation and epithelization were more pronounced and the collagen fibers were also more orderly arranged in the experiment group. It is concluded that topical application of aqueous yarrow extract, as used in this study could improve rate of burn wound healing in the rabbit.",2007,,no
Some methodologies in fitting and validation of mathematical models applied to anaerobic wastewater treatment processes,"This work presents the application and analysis of some fitting and validation methodologies of kinetic models used in anaerobic wastewater treatment systems, utilizing experimental results obtained by Rodrigues et al. (2003) in a reactor operating in batch and fed-batch modes with different feeding strategies. A mathematical model rimental results by linear and non-linear fits, having as objective function minimization of the sum of the square residuals. These fits were statistically evaluated by the correlation coefficient, F Test and Random Test.",2007,,no
Scale and process dependent model errors in seismic history matching,"Time-Lapse (4D) seismic data offers spatial and dynamic information about changes in reservoir fluid properties and can be used to constrain flow simulation models thereby improving confidence in the reservoir characterisation and its predicted behaviour. To address this, we have developed a method of quantitatively integrating 4D seismic data in an automated history matching workflow. Appropriately parameterised flow simulations are converted to predictions of 4D signatures by a petro-elastic transform and suitable resealing before a misfit is calculated by comparison to observed data. Model parameters are then updated using a quasi-global stochastic inversion method. This process is affected by scale and process dependent model errors. Flow simulations are often created such that computer resources are optimised and some level of accuracy is sacrificed. To speed up simulations, some form of upscaling is required to capture two-phase flow properties such as relative permeability but also to represent geological heterogeneity. The upscaling may be over-simplified or ignored. In addition, simplifications to the flow processes may be made, for example by using streamline methods. Finally, the petro-elastic transform contributes to the model errors due to assumptions about saturation distributions and cross-scaling is required because modelled and observed seismic are obtained for different volumes. We present an analysis of the above model errors that occur using a synthetic geo-model based on a North Sea reservoir. We show that the model error depends on the rock physics parameters as well as the underlying geo-model. When the 4D signature is dominated by pressure effects, the model error is negligible in our case. We describe how the model error affects the history matching process due to biasing. The latter results in a best set of model parameters which may be different from that obtained by upscaling while the uncertainty estimator is also changed. We compare the effect of the model error to other errors such as observed data errors. Finally, we describe how the model error is addressed in the misfit calculation to improve the history matching process and reduce the biasing effect.",2007,10.2516/ogst:2007011,no
Modelling the operator know-how to control sensory quality in traditional processes,"Traditional foods are generally manufactured in small factories where operators often play an important role: (1) to make on-line evaluations of the properties of foods and/or (2) to adjust the process variables to ensure a smooth running of the process and respect of the quality requirements. The paper presents the methodological guideline we have developed to manage the expert-operator knowledge for controlling the sensory quality of food products. It involved several steps: collection of sensory measurements, instrumental measurements and heuristics controlling rules; modelling of the operator know-how by using suitable mathematical tools such as fuzzy logic or expert systems; development of decision support systems, easy to use by the operators. The principles and the results of the method will be illustrated by examples of traditional processes: dry sausage processing and biscuits aeration. As a conclusion, the main interests of the approach are underlined: traceability of the practices, safer measurements and practices, formation of inexperienced operators, increase of the reliability in the decision of the operators and valorisation of their role. (C) 2007 Elsevier Ltd. All rights reserved.",2007,10.1016/j.jfoodeng.2007.02.016,no
Heat conduction process on community networks as a recommendation model,"Using heat conduction mechanism on a social network we develop a systematic method to predict missing values as recommendations. This method can treat very large matrices that are typical of internet communities. In particular, with an innovative, exact formulation that accommodates arbitrary boundary condition, our method is easy to use in real applications. The performance is assessed by comparing with traditional recommendation methods using real data.",2007,10.1103/PhysRevLett.99.154301,no
The ICT-supported Unified Process Model of Offshore Outsourcing of Software Production: Exploratory examination and validation,"Various important benefits can be achieved through the successful management of offshore outsourcing. Numerous studies exist on outsourcing in general, yet the large extant literature on offshore outsourcing has dealt with information technology (IT) outsourcing from the client's perspective. Several frameworks, focusing on guiding information systems managers, for IT outsourcing have been developed. However, none of these frameworks attempted to provide a holistic guideline to manage the entire process of offshore outsourcing of software production. There is a significant lack of studies dealing with the management of offshore outsourcing of software production from both the vendor's and client's perspectives. Thus, there is a great need for studying such a multifaceted and complex phenomenon more deeply from both scenarios to find out the best practices for managing the unified process. In this study, we utilize the conceptual framework of The ICT-supported Unified Process Model of Offshore Software Production Outsourcing as our research model. We then validate this model by reviewing the large extant literature, and conducting multiple case studies from both the vendor's and client's viewpoints, where professionals with extensive experience in managing offshore outsourcing of software production are interviewed. The implications of the findings are discussed for both practical and research purposes.",2007,10.1109/PICMET.2007.4349468,no
The impact of context-aware fisheye models on understanding business processes: An empirical study of data flow diagrams,"We investigated whether a ""context-aware"" fisheye view can more successfully communicate the information contained in a set of process models (data flow diagrams) than a traditional ""context-free"" presentation. We conducted two controlled experiments: the first included a simple set of DFDs and tasks that required a basic understanding of the system, while the second involved more detailed views of the same processes, and also a more complex task. Subjects who used the fisheye process models outperformed those using the traditional presentations. This difference was reflected in task performance for all subjects, and in task completion time for inexperienced subjects. (c) 2006 Elsevier B.V. All rights reserved.",2007,10.1016/j.im.2006.10.004,no
Sortal structures: Supporting representational flexibility for building domain processes,"We present a formal approach to representational flexibility, sorts, to support alternative representations of an entity. The approach is constructive, based on a part relation on elements within a sort, which enables the recognition of emergent information. The use of data functions as a sort provides for the embedding of data queries within a representational structure. We discuss the application of sorts to supporting alternative data views, illustrating this through a case study in building construction.",2007,10.1111/j.1467-8667.2006.00473.x,no
Dynamic factor process convolution models for multivariate space - time data with application to air quality assessment,"We propose a Bayesian dynamic factor process convolution model for multivariate spatial temporal processes and illustrate the utility of this approach in modeling large air quality monitoring data. Key advantages of this modeling framework are a descriptive parametrization of the cross-covariance structure of the space-time processes and dimension reduction features that allow full Bayesian inference procedures to remain computationally tractable for large data sets. These features result from modeling space-time data as realizations of linear combinations of underlying space-time fields. The underlying latent components are constructed by convolving temporally-evolving processes defined on a grid covering the spatial domain and include both trend and cyclical components. We argue that mixtures of such components can realistically describe a variety of space-time environmental processes and are especially applicable to air pollution processes that have complex space-time dependencies. In addition to the computational benefits that arise from the dimension reduction features of the model, the process convolution structure permits misaligned and missing data without the need for imputation when fitting the model. This advantage is especially useful when constructing models for data collected at monitoring stations that have misaligned sampling schedules and that are frequently out of service for long stretches of time. We illustrate the modeling approach using a multivariate pollution dataset taken from the EPA's CASTNet database.",2007,10.1007/s10651-007-0019-y,no
Analysis of membrane fusion as a two-state sequential process: Evaluation of the stalk model,"We propose a model that accounts for the time courses of PEG-induced fusion of membrane vesicles of varying lipid compositions and sizes. The model assumes that fusion proceeds from an initial, aggregated vesicle state ((A) membrane contact) through two sequential intermediate states (I-1 and I-2) and then on to a fusion pore state (FP). Using this model, we interpreted data on the fusion of seven different vesicle systems. We found that the initial aggregated state involved no lipid or content mixing but did produce leakage. The final state (FP) was not leaky. Lipid mixing normally dominated the first intermediate state (I-1), but content mixing signal was also observed in this state for most systems. The second intermediate state (I-2) exhibited both lipid and content mixing signals and leakage, and was sometimes the only leaky state. In some systems, the first and second intermediates were indistinguishable and converted directly to the FP state. Having also tested a parallel, two-intermediate model subject to different assumptions about the nature of the intermediates, we conclude that a sequential, two-intermediate model is the simplest model sufficient to describe PEG-mediated fusion in all vesicle systems studied. We conclude as well that a fusion intermediate ""state'' should not be thought of as a fixed structure (e.g., ""stalk'' or ""transmembrane contact'') of uniform properties. Rather, a fusion ""state'' describes an ensemble of similar structures that can have different mechanical properties. Thus, a ""state'' can have varying probabilities of having a given functional property such as content mixing, lipid mixing, or leakage. Our data show that the content mixing signal may occur through two processes, one correlated and one not correlated with leakage. Finally, we consider the implications of our results in terms of the ""modified stalk'' hypothesis for the mechanism of lipid pore formation. We conclude that our results not only support this hypothesis but also provide a means of analyzing fusion time courses so as to test it and gauge the mechanism of action of fusion proteins in the context of the lipidic hypothesis of fusion.",2007,10.1529/biophysj.106.090043,no
Service-oriented business process modeling and performance evaluation based on AHP and simulation,"With the evolution of Grid technologies and the application of Service-Oriented Architecture (SOA), more and more enterprises are integrated and collaborated with each other in a loosely coupled environment. A business process in that environment, i.e., the Service-Oriented Business Process (SOBP), shows highly flexibility for its free selection and composition of different services. The performance of the business process usually has to be evaluated and predicted before its being implemented And it has special features since it includes both business-level and IT-level attributes. However, the existing modeling and performance evaluation methods of business process are mainly concentrated on business-level performance. And the researches on service selection and composition are usually limited to the IT-level metrics. An extended activity-network-based SOBP Model, its three-level performance metrics, and the corresponding calculation algorithm are proposed to fulfill these requirements. The advantages of our method in SOBP modeling and performance evaluation are highlighted also.",2007,10.1109/ICEBE.2007.22,no
On the flexibility of M/G/infinity processes for modeling traffic correlations,"With the increasing popularity of multimedia applications, video data represents a large portion of the traffic in modern networks. Consequently, adequate models of video traffic, characterized by a high burstiness and a strong positive correlation, are very important for the performance evaluation of network architectures and protocols. This paper presents new models for traffic with persistent correlations based on the M/G/infinity process. We derive two new discrete distributions for the service time of the M/G/infinity queueing system, flexible enough to give rise to processes whose correlation structure is able to exhibit both Short-Range Dependence (SRD) and Long-Range Dependence (LRD). The corresponding simulation models are easy to initialize in steady state. Moreover, as the two distributions have subexponential decay, we can apply a highly efficient and flexible generator of synthetic traces of the resulting M/G/infinity processes.",2007,,no
Evaluating electronic service quality: a transaction process based evaluation model,"With the rapid growth of the Internet and the globalization of the market, most enterprises are trying to attract and win customers in the highly competitive electronic market. Best practice exemplars suggest that e-Service plays a critical role in e-marketing, which wins customers for enterprises through the Internet. This paper addresses the issue how to evaluate web service in the electronic marketplace. We draw on the e-Service quality perspective to suggest that enterprises that develop good content service and functional service in the transaction process can win more customers in the electronic market. In this study an e-Service quality evaluation model is conceptualized based on the literature review of the dimensions of e-Service quality. Our study suggests that the service quality of both content service and functional service in e-Service are important for enterprises to attract customers in the electronic market. In this study the evaluation process is based on customers' online purchasing process. It illustrates that different dimensions of e-Service quality are of different importance in different purchasing phrases. Furthermore it is implied in this study that good content service quality and function service quality can enable customers to get a good psychological satisfaction, which is vital for customers to make the decision to purchase products or services online, and to establish trust and loyalty to service providers.",2007,,no
Aqueous sol-gel synthesis route for the preparation of YAG: Evaluation of sol-gel process by mathematical regression model,"Yttrium aluminium garnet (Y3Al5O12, YAG) polycrystalline samples have been prepared by a simple aqueous sol-gel methodology. The influence of nineteen sol-gel processing variables on the formation of YAG has been investigated. Effects of different fabrication parameters on the phase purity and morphological properties of the compounds were studied by energy-dispersive spectrometry (EDS), X-ray powder diffraction (XRD) analysis and scanning electron microscopy (SEM). The parameters of the sol-gel processing such as pH of starting solution, concentration and nature of complexing ligand, temperature and duration of gelation, powder rehomogenization during annealing, duration and temperature of the final heat treatment were found to be the most significant. For the evaluation and verification of the experimental results the Brandon's model of a multiple regression was successfully used.",2007,10.1007/s10971-006-9002-6,no
A three-dimensional model of the wire-arc spray process and its experimental validation,"A CFD study of the twin wire-arc spray process is presented in this paper. A three-dimensional model was used to compute the flow inside and outside the spray gun and gas-particles interactions were computed on the basis of one-way and two-way coupling methods, the latter allowing taking the loading effect into account. The considered particle size distributions were measured experimentally on particles collected at the spray distance, so that no atomizing model was required. In a second step, the model validation was performed on the basis of comparisons between predictions and particle velocity measurements. (c) 2007 Elsevier B.V All rights reserved.",2008,10.1016/j.jmatprotec.2007.08.032,no
Complex-Valued Neural Network in Signal Processing: A Study on the Effectiveness of Complex Valued Generalized Mean Neuron Model,"A complex valued neural network is a neural network which consists of complex valued input and/or weights and/or thresholds and/or activation functions. Complex-valued neural networks have been widening the scope of applications not only in electronics and informatics, but also in social systems. One of the most important applications of the complex valued neural network is in signal processing. In Neural networks, generalized mean neuron model (GMN) is often discussed and studied. The GMN includes a new aggregation function based on the concept of generalized mean of all the inputs to the neuron. This paper aims to present exhaustive results of using Generalized Mean Neuron model in a complex-valued neural network model that uses the back-propagation algorithm (called 'Complex-BP') for learning. Our experiments results demonstrate the effectiveness of a Generalized Mean Neuron Model in a complex plane for signal processing over a real valued neural network. We have studied and stated various observations like effect of learning rates, ranges of the initial weights randomly selected, error functions used and number of iterations for the convergence of error required on a Generalized Mean neural network model. Some inherent properties of this complex back propagation algorithm are also studied and discussed.",2008,,no
Increasing process understanding through data mining and statistical modelling,A lean and simple approach to increasing process understanding that aligns scientific and engineering thinking with statistical principles is presented. This allows a larger community of scientific and engineering users to apply statistical methods to increase process understanding based on data and use this knowledge to increase product quality.,2008,,no
Selection of the structure of a model in processing the results of measurements in control systems,A method of structural identification of nonlinear dynamical systems with a single input and single output based on the methodology of synthesis information is set forth. Identification is realized with the use of statistical models and a set of auxiliary data obtained as a result of the application of analysis and decision algorithms based on processing of the results of measurements. Results of a simulation are presented.,2008,10.1007/s11018-008-9158-2,no
Gross Errors Detection Based on the Transferable Belief Model in Process Industries,"A new method is proposed to detect gross errors by combining several traditional algorithms. The judgment of locating gross errors by traditional algorithms based on the theory of statistics is only made by one criterion and the belief of the result can't be identified. And it's also a problem to confuse the results of different algorithms. In the paper, a new method is proposed to assign the belief of different sets of a collection of measurements (the frame of discernment) based on the results of different traditional algorithms. The transferable belief model (TBM) is used as a reasoning model. Serial compensation strategy is also applied in the proposed method. Simulation results show the new method possesses high performance to identify gross errors.",2008,,no
640 Gb/s RZ Eye-Diagram Evaluation by Optical Sampling Oscilloscope w/o Post-Processing and ms Refresh Time,A polarization insensitive quasi-asynchronous optical sampling oscilloscope with sub-ps-resolution is presented. The scheme is able to resolve tens of ns and provides eye-diagram functionality. Experimental results for a 640 Gb/s data frame are reported. (c) 2008 Optical Society of America,2008,,no
Learning from model improvement: On the contribution of complementary data to process understanding,"A priori determined model structures are common in catchment rainfall-runoff modeling. While this has resulted in many ready-to-use modeling tools, there are several shortcomings of a one-size-fits-all model structure. The uniqueness of catchments with respect to their hydrological behavior and the need to adapt model complexity to data availability challenge this status quo. We present a flexible approach to model development where the model structure is adapted progressively based on catchment characteristics and the data described by the experimentalist. We demonstrate this approach with the Maimai catchment in New Zealand, a location with a large availability of data, including stream discharge, groundwater levels, and stream isotope measurements. Different types of data are introduced progressively, and the architecture of the model is adjusted in a stepwise fashion to better describe the processes suggested by the new data sources. The revised models are developed in a way to strike a balance between model complexity and data availability, by keeping models as simple as possible, but complex enough to explain the dynamics of the data. Our work suggests that ( 1) discharge data provides information on the dynamics of storage ( represented by the '' free'' water in the reservoirs) subject to pressure wave propagation generated by rainfall into the catchment, ( 2) groundwater data provides information on thresholds and on the contribution of different portions of the catchment to stream discharge, and ( 3) isotope data provides information on particle transport and mixing of the rainfall with the storage present in the catchment. Moreover, while groundwater data appear to be correlated with discharge data, and only a marginal improvement could be obtained adding this information to the model development process, isotope data appear to provide an orthogonal view on catchment behavior. This result contributes to understanding the value of data for modeling, which may serve as a guidance in the process of gauging ungauged catchments.",2008,10.1029/2007WR006386,no
Computational understanding and modeling of filling-in process at the blind spot,"A visual model for filling-in at the blind spot is proposed. The general scheme of standard regularization theory is used to derive a visual model deductively. First, we indicate problems of the diffusion equation, which is frequently used for various kinds of perceptual completion. Then, we investigate the computational meaning of a neural property discovered by Matsumoto and Komatsu (J. Neurophysiology, vol. 93, pp. 2374-2387, 2005) and introduce second derivative quantities related to image geometry into a prion knowledge of missing images on the blind spot. Moreover, two different information pathways for filling-in (slow conductive paths of horizontal connections in V1, and fast feedforward/feedback paths via V2) are regarded as the neural embodiment of adiabatic approximation between V1 and V2 interaction. Numerical simulations show that the outputs of the proposed model for filling-in are consistent with a neurophysiological experimental result, and that the model is a powerful tool for digital image inpainting.",2008,,no
Understanding nanoparticle formation by a wire explosion process through experimental and modelling studies,"A wire explosion process (WEP) has been used to produce nano aluminium powder in nitrogen, argon and helium atmospheres. The impact of energy deposited into the exploding conductor on the size and shape of the particles was analysed using TEM analysis, which forms the first part of the study. It is observed that the higher the energy deposited, the smaller the particles formed. In the second part, modelling studies were carried out by solving the general dynamic equation through the nodal approach, and the particle size distributions were predicted. It is realized that, at the point of high saturation ratio and nucleation rate, the size of the critical nucleus formed is low. The particle size distribution predicted by the model correlates well with the experimental results. Time-series analysis of particle formation indicates that particles of lower dimensions form and, in the process of coagulation, larger particles are formed. It is realized that the plasma formed during the explosion plays a major role in the particle formation, and the modelling studies confirm that particle formation is not an instantaneous process but requires a certain time period to form stable sizes and shapes.",2008,10.1088/0957-4484/19/02/025703,no
Fractal Modeling Approach for Supporting Business Process Flexibility,"Ability to support various business models has been recognized as one of the essential competitive advantages of companies operating in global networked business environment. The use of several business models simultaneously, requires availability of flexible business process models. Flexibility of business process models. ill turn, depends oil appropriate information systems Support. One of the ways how to support business process flexibility is to use a fractal paradigm in information systems development. The fractal paradigm call be applied at two levels of abstraction: the level of business process system and the level of software system. Applications of the fractal paradigm at two abstraction levels correspond to two different opportunities of supporting flexible business processes.",2008,,no
Water analogue validation of numerical model for fluid flow during slow shot phase in die casting process,"Air that becomes entrapped in the molten metal during the die casting process has a major effect on the formation of porosity in horizontal cold chamber die castings. In this study, a three-dimensional computational fluid dynamics model was developed to study the flow patterns of liquid metal in the injection chamber, in which the moving boundary conditions of the plunger movement was considered in detail. According to the principle of die casting machines, a water analogue system was designed and built to investigate the slow shot process. A colour high speed camera was used to record the fluid flow patterns under different plunger movement profiles. The numerical simulation results agreed well with the water analogue experimental results, which validated the numerical model of shot processing in the cold chamber of the die casting process.",2008,10.1179/136404608X370198,no
Online Near-Infrared Measurements of an Epoxy Cure Process Compared to Mathematical Modeling Based on Differential Scanning Calorimetry Measurements,"An epoxy resin containing diglycidyl ether of bisphenol A, dicyandiamide, and an accelerator (diurone) was investigated under different cure cycles. The mathematical prediction of the degree of cure in a thermoset as a function of time and temperature was investigated and compared to measured data. Near-infrared analysis was used to measure the conversion of epoxy and primary amine and the production of hydroxyl. Modulated differential scanning calorimetry was used to measure the changes in the heat capacity during cure. The measurements revealed differences in the primary amine conversion and hydroxyl production, and close relations to the measurements of heat capacity were found. The measurements of the degree of cure revealed that cure cycles initiated at 80 degrees C produced a lower degree of cure than cure cycles initiated at 90 degrees C, although all cure cycles were post-cured at 110 degrees C. These findings were to some degree supported by measurements of the primary amine conversion and hydroxyl production. The characteristics found were attributed to differences in the cure mechanisms. The mathematical model did not incorporate these differences, and this may have led to discrepancies between the predicted and actual values of the degree of cure. (C) 2008 Wiley Periodicals, Inc. J Appl Polym Sci 110: 2184-2194, 2008",2008,10.1002/app.28407,no
Packet size process modeling of measured self-similar network traffic with defragmentation method,"Analysis and modeling of telecommunication networks by simulations has become one of the main tools in the process of telecommunication-networks' planning and upgrading. Knowledge regarding the statistical modeling of network traffic is very important. Here we tend towards modeled network traffic which would be the best possible approximation of the measured traffic. Throughout our research in the field of self-similar network traffic we have faced problem of statistically describing the packet-size process. We have noticed that small discrepancies between measured histograms and estimated probability density functions, as used in traffic generator models, lead to large discrepancy between measured and modeled network traffics. In this research we tried to estimate the probability density function of a measured histogram for process-packet size, in such way that would decrease these discrepancies. For this purpose, we have developed a novel method of modeling network traffic, which is based on the defragmentation of measured traffic. Using this defragmentation method, we can estimate parameters of files' size process, from captured packets and use these statistical parameters for traffic generation, via the OPNET simulation tool. From these simulations, we can show that this newly-developed method decreases discrepancy between packet size process histograms of measured and simulated network traffics. This consequently leads to a decrease in discrepancy between measured and simulated network traffics.",2008,10.1109/IWSSIP.2008.4604415,no
Evaluation of a process simulator for modelling and analysis of Rankine cycle steam power plants,"Applicability of the HYSYS process simulation package to thermodynamic modelling and analysis of Rankine cycle steam power plants, using first and second law analyses, has been evaluated. Theoretical Rankine cycles, and more realistic cycles based on power plants modelled in other studies, were simulated. Energy, exergy and efficiency values determined from the simulations were compared to those obtained from theory or reported in literature. Discrepancies of <1% were observed for theoretical cycles, while more realistic cycles were modelled to within 8% of reported values. Results suggest HYSYS is appropriate for use in thermodynamic assessments of steam power plant performance.",2008,10.1504/IJEX.2008.016674,no
A java-based fMRI processing pipeline evaluation system for assessment of univariate general linear model and multivariate canonical variate analysis-based pipelines,"As functional magnetic resonance imaging (fMRI) becomes widely used, the demands for evaluation of fMRI processing pipelines and validation of fMRI analysis results is increasing rapidly. The current NPAIRS package, an IDL-based fMRI processing pipeline evaluation framework, lacks system interoperability and the ability to evaluate general linear model (GLM)-based pipelines using prediction metrics. Thus, it can not fully evaluate fMRI analytical software modules such as FSL.FEAT and NPAIRS.GLM. In order to overcome these limitations, a Java-based fMRI processing pipeline evaluation system was developed. It integrated YALE (a machine learning environment) into Fiswidgets (a fMRI software environment) to obtain system interoperability and applied an algorithm to measure GLM prediction accuracy. The results demonstrated that the system can evaluate fMRI processing pipelines with univariate GLM and multivariate canonical variates analysis (CVA)-based models on real fMRI data based on prediction accuracy (classification accuracy) and statistical parametric image (SPI) reproducibility. In addition, a preliminary study was performed where four fMRI processing pipelines with GLM and CVA modules such as FSL.FEAT and NPAIRS.CVA were evaluated with the system. The results indicated that (1) the system can compare different fMRI processing pipelines with heterogeneous models (NPAIRS.GLM, NPAIRS.CVA and FSL.FEAT) and rank their performance by automatic performance scoring, and (2) the rank of pipeline performance is highly dependent on the preprocessing operations. These results suggest that the system will be of value for the comparison, validation, standardization and optimization of functional neuroimaging software packages and fMRI processing pipelines.",2008,10.1007/s12021-008-9014-1,no
A comprehensive model of process variability for statistical timing optimization - art. no. 69251G,"As technologies scale, the impact of process variations to circuit performance and power consumption is increasingly significant. In order to improve the efficiency of statistical circuit optimization, a better understanding of the relationship between circuit variability and process variation is needed. Our work proposes a hierarchical variability model, which addresses both systematic and random variations at wafer, field, die, and device level, and spatial correlation artifacts are captured implicitly. Finally, layout dependent effects are incorporated as an additive component. The model is verified by applying to 90nm ring oscillator measurement data and can be used for variability prediction and optimization.",2008,10.1117/12.772980,no
Behavior of host and graft cells in the early remodeling process of rotator cuff defects in a transgenic animal model,"Autologous tissue graft is one of the treatment options for a large rotator cuff defect. To develop appropriate strategies for enhanced solid graft integration at the bone-tendon interface and tendon-tendon interface, clarifying the fate of the graft and host cells that contribute to repair and remodeling is necessary. We have developed a new grafting model using green fluorescent protein-transgenic rats and wild-type rats to simulate autologous transplantation for examining the behavior of the host and graft cells in the remodeling process after tendon grafting. We found that the host cells commenced proliferation in the graft at 1 day after grafting. The host cells infiltrated into the graft from the subacromial synovium, proximal tendon, and bone-tendon insertion. The number of graft-derived cells decreased with time. Our result clearly demonstrated that host cells, rather than graft cells, were essential for rotator cuff remodeling after tendon grafting for rotator cuff defect.",2008,10.1016/j.jse.2007.07.008,no
Research on Process Control Model of Service Quality in Public Scientific Research Institutions,"Based on the non-profit nature and difficult control of service process in public scientific research institutions, three theoretical bases for effectively controlling of service quality in public scientific research activities are analyzed. The process control model of service quality in public scientific research institutions is put forward. Further modified Servqual Model which simulated enterprise management's model is provided. Finally the author brings forward three practical control strategies which will be used for public scientific research institutions to improve the process management of service quality.",2008,10.1109/SOLI.2008.4682973,no
Study on Evaluation Model of Construction Firm's Core Competence Based on Analytic Hierarchy Process and Information Entropy of Fuzzy Matter-element Theory,"Based on the theory of core competence, the paper applies intellectual capital theory to analyzing construction enterprise's core competence and constructs the index system from the point of intellectual capital. Further, drawing on the method of analytic hierarchy process and information entropy of fuzzy matter-element, this article establishes a fuzzy comprehensive evaluation model. Finally, it makes an empirical research with an example and gives its suggestions.",2008,,no
A comprehensive model to describe radiolytic processes in cement medium,"Basic mechanisms controlling the radiolysis in cementitious matrices are reviewed in the specific context of the gamma irradiation, in closed system without upper vapour space, at 25 degrees C, with a pore solution representative of a Portland cement paste. A general survey of data corresponding to each phenomenological area is given, particularly with a list of reactions for alkaline medium and a revised description of equilibria related to calcium hydroxide. Simulations as a function of dose rate, liquid saturation in the porosity or initial amount of H-2 are carried out with the CHEMSIMUL code. They show the existence of several ways in the radiolysis regulation involving calcium peroxide octahydrate precipitation, a law of mass action through the bulk general system, or the particular activation of an Allen's type chain reaction. This latter seems faster in alkaline medium (pH > 13) where radicals H-center dot and OH center dot are, respectively, replaced by e(aq)(-) and O center dot-. Excepted when a strong reducing agent is initially present, 02 is normally produced by radiolysis and CaO2 center dot 8H(2)O cannot be responsible of its disappearance. (c) 2006 Published by Elsevier B.V.",2008,10.1016/j.jnucmat.2006.10.004,no
Model visualization for evaluation of biocatalytic processes,"Biocatalysis offers great potential as an additional, and in some cases as an alternative, synthetic tool for organic chemists, especially as a route to introduce chirality. However, the implementation of scalable biocatalytic processes nearly always requires the introduction of process and/or biocatalyst enhancements to ensure effective scale-up. This paper describes a paradigm for the purpose of evaluating biocatalytic processes in order to provide guidance on process and biocatalyst modification. The paradigm is illustrated with the biocatalytic synthesis of S,S-ethylenediaminedisuccinic acid (S,S-EDDS), a biodegradable chelant, and is characterised by the use of model visualization using 'windows of operation'. (C) 2008 The Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.",2008,10.1016/j.fbp.2008.03.006,no
Assuring Consistency of Business Process Models and Web Services Using Visual Contracts,"Business process models describe workflows by a set, of actions together with their ordering. When implementing business processes within a service-oriented architecture, these actions are mapped to existing IT (web) services, which are then to be executed in the order specified by the business process. However, the. execution of a web service call require certain preconditions to be fulfilled. These might not hold at the time of execution specified in the business process model: it call be inconsistent with the well service specification. In this paper we propose a technique for checking consistency of process models with web service specifications. To this end, both are equipped with a far-mal semantics (in terms of graph transformations). We show how to use an existing model checker for graph transformation systems to carry out, the consistency check.",2008,,no
Development and validation of mathematical model for aerobic composting process,"By integrating the reaction kinetics with the mass and heat transfer between the three phases of the system, a new dynamic structured model for aerobic composting process was developed in this work. In order to evaluate kinetic parameters in mathematical model and to validate the model, experiments were performed with the reactor of volume 32 L, in controlled laboratory conditions. Different ratios of poultry manure to wheat straw were mixed and used as a substrate. Rosenbrock optimization method was used for parameter estimation. In order to solve the system of 12 non-linear differential (and corresponding algebraic) equations, Runge-Kutta-Fehlberg method was used, with approximation of fourth and fifth order and adjustment of step size. Both algorithms were implemented in FORTRAN programming language. In order to achieve as accurate description of the process dynamics as possible, the developed mathematical model was validated by the results of several experimentally. measured dynamic state variables. Comparisons of experimental and simulation results for temperature of substrate, organic matter conversion, carbon dioxide concentration and oxygen concentration, in general showed good agreement during the whole duration of the process in a reactor. In the case of ammonia, an agreement was achieved for the first 4 days and for the last 3 days of the process. A sensitivity analysis was performed to determine the key parameters,of the model. Analysis showed that two parameters had a great influence on the main characteristics of the process. With validated model for aerobic composting of mixture of poultry manure and wheat straw, optimal values were determined: initial moisture content (70%) and airflow (0.541 min(-1) kg(OM)(-1)). (C) 2007 Elsevier B.V. All rights reserved.",2008,10.1016/j.cej.2007.08.017,no
A Mechanistic Dissolved Oxygen Model of Corpus Christi Bay to Understand Critical Processes Causing Hypoxia,"Corpus Christi Bay (TX, USA) is a shallow wind-driven bay which experiences hypoxia (dissolved oxygen < 2 mg/L) during the summer months in the southeast region of the bay. We have developed and installed real-time monitoring systems in the bay to measure various water quality, meteorological and hydrodynamic parameters. These systems can aid in determining the extent and frequency of hypoxic events in this energetic bay. A three-dimensional mechanistic dissolved oxygen model has been developed in this study to investigate the key processes that induce hypoxia in Corpus Christi (CC) Bay. This model includes variable advection and dispersion coefficients so that it can be driven by real-time monitoring hydrodynamic data. The results from model simulations indicate that hypoxia may occur at the lower depths of the bay when both stratification and higher biological activity conditions exist. The water column in the south-east part of the bay becomes stratified during calm wind conditions when there is inflow of hyper-saline water from the neighboring Laguna Madre waterbody. This condition, when combined with higher biological activity during the summer months, induces hypoxia at the lower depths of the bay. The simulation results also point out that physical transport processes have more pronounced effect on the DO distribution within the water column than the effects of biological activity. Therefore, it is necessary to develop suitable sampling strategies that will measure hydrodynamic data at greater spatial and temporal resolution. The integration of this data with our developed model will provide a useful tool to the stakeholders to assess the water quality of the bay in real time.",2008,,no
An Information System Development Method Based on the Link of Business Process Modeling with Executable UML Modeling and its Evaluation by Prototyping,"Currently, Business Process Modeling (BPM) is used to create current analytical model (As-Is model) of the work processes, upon which an optimized model (the To-Be model) can be based, including the optimization of system configuration. Also, there is strong interest in the eXecutableUML (xUML), which makes it possible to validate system viability before it is implemented This paper proposes a methodology for configuring information systems that links BPM and xUML techniques, and evaluates its applicability. In the proposed method, bottlenecks are analyzed and evaluated using simulation of the work process flow in BPM and, after translating this model into UML model elements, and after the UML model has been created, expands it to xUML. Then, in xUML modeling, dynamic system evaluation is carried out with automatic generation of the code for actual implementation. The proposed method was applied to the development of an actual prototype information system and evaluated.",2008,10.1109/WAINA.2008.113,no
DEA models for two-stage processes: Game approach and efficiency decomposition,"Data envelopment analysis (DEA) is a method for measuring the efficiency of peer decision making units (DMUs). This tool has been utilized by a number of authors to examine two-stage processes, where all the outputs from the first stage are the only inputs to the second stage. The current article examines and extends these models using game theory concepts. The resulting models are linear, and imply an efficiency decomposition where the overall efficiency of the two-stage process is a product of the efficiencies of the two individual stages. When there is only one intermediate measure connecting the two stages, both the noncooperative and centralized models yield the same results as applying the standard DEA model to the two stages separately. As a result, the efficiency decomposition is unique. While the noncooperative approach yields a unique efficiency decomposition under multiple intermediate measures, the centralized approach is likely to yield multiple decompositions. Models are developed to test whether the efficiency decomposition arising from the centralized approach is unique. The relations among the noncooperative, centralized, and standard DEA approaches are investigated. Two real world data sets and a randomly generated data set are used to demonstrate the models and verify our findings. (C) 2008 Wiley Periodicals, Inc.",2008,10.1002/nav.20308,no
AN ECONOMIC PRODUCTION QUANTITY MODEL WITH RANDOM DEFECTIVE RATE IN IMPERFECT PRODUCTION PROCESSES,"Determining the optimal production quantity has been widely used by the classic economic production quantity (EPQ) model. However, the analysis for finding an EPQ model has many faults which led many researchers to make extensions in several aspects on the original EPQ model. The basic assumption of EPQ is that 100% of manufactured products are non-defective. This assumption is not valid for many production processes. In this paper a modified EPQ model has been developed. In this model, a portion of the defective items is considered to be scrap, rework and imperfect quality which are sold to a particular purchaser at a lower price. The wooden chipboard production process and high voltage transformers producing process give good examples for such situations. A mathematical model has been developed and for illustration of model a numerical examples are presented.",2008,,no
Neural network approach for robust and fast calculation of physical processes in numerical environmental models: Compound parameterization with a quality control of larger errors,"Development of: neural network (NN) emulations for fast calculations of physical processes in numerical climate and weather prediction models depends significantly on Our ability to generate a representative training set. Owing to the high dimensionality of the NN input vector which is of the order of several hundreds or more, it is rather difficult to cover the entire domain, especially its ""far corners"" associated with rare events, even when we use model simulated data for the NN training. Moreover the domain may evolve (e.g., due to climate change). In this situation the emulating NN may be forced to extrapolate beyond its generalization ability and may lead to larger errors in NN Outputs. A new technique, a compound parameterization, has been developed to address this problem and to make the NN emulation approach more suitable for long-term climate prediction and climate change projections and other numerical modeling applications. Two different designs of the compound parameterization are presented and discussed. (C) 2008 Elsevier Ltd. All rights reserved.",2008,10.1016/j.neunet.2007.12.019,no
A process-based diagnostic approach to model evaluation: Application to the NWS distributed hydrologic model,"Distributed hydrological models have the potential to provide improved streamflow forecasts along the entire channel network, while also simulating the spatial dynamics of evapotranspiration, soil moisture content, water quality, soil erosion, and land use change impacts. However, they are perceived as being difficult to parameterize and evaluate, thus translating into significant predictive uncertainty in the model results. Although a priori parameter estimates derived from observable watershed characteristics can help to minimize obstacles to model implementation, there exists a need for powerful automated parameter estimation strategies that incorporate diagnostic information regarding the causes of poor model performance. This paper investigates a diagnostic approach to model evaluation that exploits hydrological context and theory to aid in the detection and resolution of watershed model inadequacies, through consideration of three of the four major behavioral functions of any watershed system; overall water balance, vertical redistribution, and temporal redistribution (spatial redistribution was not addressed). Instead of using classical statistical measures (such as mean squared error), we use multiple hydrologically relevant ""signature measures'' to quantify the performance of the model at the watershed outlet in ways that correspond to the functions mentioned above and therefore help to guide model improvements in a meaningful way. We apply the approach to the Hydrology Laboratory Distributed Hydrologic Model (HL-DHM) of the National Weather Service and show that diagnostic evaluation has the potential to provide a powerful and intuitive basis for deriving consistent estimates of the parameters of watershed models.",2008,10.1029/2007WR006716,no
A meta-analysis of teamwork processes: Tests of a multidimensional model and relationships with team effectiveness criteria,"Drawing from Marks, Mathieu, and Zaccaro (2001), we proposed that narrowly focused teamwork processes load onto 3 higher-order teamwork process dimensions, which in turn load onto a general teamwork process factor. Results of model testing using meta-analyses of relationships among narrow teamwork processes provided support for the structure of this multidimensional theory of teamwork process. Meta-analytic results also indicated that teamwork processes have positive relationships with team performance and member satisfaction, and that the relationships are similar across the teamwork dimensions and levels of process specificity. Supplemental analyses revealed that the 3 intermediate-level teamwork processes are positively and strongly related to cohesion and potency. Results of moderator analyses suggested that relationships among teamwork processes and team performance are somewhat dependent on task interdependence and team size.",2008,10.1111/j.1744-6570.2008.00114.x,no
Modeling of Machining Error Propagation Network for Multistage Machining Processes,"Due to the complicated interactions among different stages in multistage machining processes (MMPs), it is important and necessary to explore the inherent mechanism of machining error propagation for improving product quality. As for this issue, a machining error propagation network (MEPN) of MMPs is proposed in this paper, in which machining form feature (MFF) of workpeice, machine tool, fixture and cutting tool (That is machining element, ME) are defined as different network nodes. The associated relationship among different nodes is mapped by graph theory, according to the results of CAPP of workpiece. Based on this, complex networks theory is introduced to analyze the characteristics of the established MEPN, and several measuring indices of error propagation at different nodes are proposed to quantify the interactions among different nodes. At last, a box part of missile launcher is used to illustrate the proposed method.",2008,,no
Evaluation of CO2 fluxes from an agricultural field using a process-based numerical model,"During 2004, soil CO2 fluxes, and meteorological and soil variables were measured at multiple locations in a 30-ha agricultural field in the Sacramento Valley, California, to evaluate the effects of different tillage practices on CO2 emissions at the field scale. Field scale CO2 fluxes were then evaluated using the one-dimensional process-based SOILCO2 module of the HYDRUS-1D software package. This model simulates dynamic interactions between soil water contents, temperature, and soil respiration by numerically solving partial-differential water flow (Richards) and heat and CO2 transport (convection-dispersion) equations using the finite element method. The model assumes that the overall CO2 production in the soil profile is the sum of soil and plant respiration, whose optimal values are affected by time, depth, water content, temperature, and CO2 concentration in the soil profile. The effect of each variable is introduced using various reduction functions that multiply the optimal Soil CO2 production. Our results show that the numerical model could predict CO2 fluxes across the soil surface reasonably well using soil hydraulic parameters determined from textural characteristics and the HYDRUS-1D software default values for heat transport, CO2 transport and production parameters without any additional calibration. An uncertainty analysis was performed to quantify the effects of input parameters and soil heterogeneity on predicted soil water contents and CO2 fluxes. Both simulated volumetric water contents and surface CO2 fluxes show a significant dependency on soil hydraulic properties. (C) 2008 Elsevier B.V. All rights reserved.",2008,10.1016/j.jhydrol.2008.07.035,no
Evaluation of pervaporation process for recovering a key orange juice flavour compound: modeling and simulation,"During fruit juice processing, characteristic flavour components are usually lost as consequence of the heating process. Membrane separation processes are considered as a promising alternative for this issue, e.g., in orange juice industry, an important agro industrial chain in Brazilian economy. Ethyl butyrate (EB) is one of the fresh orange flavour key contributors. Pervaporation is an attractive technology for processing thermal sensitive compounds. This membrane process is based on a selective transport of a liquid feed mixture through a selective polymeric or ceramic membrane. In this work, the pervaporation performance was simulated for recovering EB from a diluted binary aqueous mixture using a PDMS (poly(dimethylsiloxane)) hydrophobic membrane. Innovative preliminary process results obtained using predicted POMS (poly(octylmethylsiloxane)) properties are also presented. A FORTRAN simulator named PERVAP based on an essentially predictive mathematical model was applied in this work.",2008,,no
An evaluation of Arctic cloud and radiation processes during the SHEBA year: simulation results from eight Arctic regional climate models,"Eight atmospheric regional climate models (RCMs) were run for the period September 1997 to October 1998 over the western Arctic Ocean. This period was coincident with the observational campaign of the Surface Heat Budget of the Arctic Ocean (SHEBA) project. The RCMs shared common domains, centred on the SHEBA observation camp, along with a common model horizontal resolution, but differed in their vertical structure and physical parameterizations. All RCMs used the same lateral and surface boundary conditions. Surface downwelling solar and terrestrial radiation, surface albedo, vertically integrated water vapour, liquid water path and cloud cover from each model are evaluated against the SHEBA observation data. Downwelling surface radiation, vertically integrated water vapour and liquid water path are reasonably well simulated at monthly and daily timescales in the model ensemble mean, but with considerable differences among individual models. Simulated surface albedos are relatively accurate in the winter season, but become increasingly inaccurate and variable in the melt season, thereby compromising the net surface radiation budget. Simulated cloud cover is more or less uncorrelated with observed values at the daily timescale. Even for monthly averages, many models do not reproduce the annual cycle correctly. The inter-model spread of simulated cloud-cover is very large, with no model appearing systematically superior. Analysis of the co-variability of terms controlling the surface radiation budget reveal some of the key processes requiring improved treatment in Arctic RCMs. Improvements in the parameterization of cloud amounts and surface albedo are most urgently needed to improve the overall performance of RCMs in the Arctic.",2008,10.1007/s00382-007-0286-1,no
Evaluation of indirect competitive and double antibody sandwich ELISA tests to determine -lactoglobulin and ovomucoid in model processed foods,"Enzyme-linked immunosorbent assay (ELISA) kits (indirect competitive and sandwich formats) to determine either -lactoglobulin or ovomucoid were evaluated in model foods. A cut-off value was established for each kit to consider food samples as positive for milk or egg addition. Sausage and bread were positive at lower percentages of added milk using the sandwich format (0.005 and 0.05%) than the indirect competitive format (0.05 and 0.25%) and pate was positive at 0.25% milk addition for both formats. Sausage was positive at 0.005%, and bread at 0.05% added egg for indirect competitive and sandwich formats, whereas pate was positive at 0.25% egg only by the indirect competitive assay. The concentration of added milk and egg to give a positive result depends on heat treatment, being higher for pate (sterilised), followed by bread (baked) and sausage (pasteurised). The particularities of each format and the heat processing applied influenced the determination by ELISA of allergenic proteins in foods.",2008,10.1080/09540100802520755,no
The internal consistency and precedence of key process areas in the capability maturity model for software,"Evaluating the reliability of maturity level (ML) ratings is crucial for providing confidence in the results of software process assessments. This study investigates the dimensions underlying the maturity construct in the Capability Maturity Model (CMM) for Software (SW-CMM) and estimates the internal consistency of each dimension. The results suggest that SW-CMM maturity is a three-dimensional construct, with ""Project Implementation"" representing the ML 2 key process areas (KPAs), ""Organization Implementation"" representing the ML 3 KPAs, and ""Quantitative Process Implementation"" representing the KPAs at MLs 4 and 5. The internal consistency for each of the three dimensions as estimated by Cronbach's alpha exceeds the recommended value of 0.9. Based on those results, this study builds and tests a theoretical model which posits that the achievement of lower ML KPAs sustains the implementation of higher ML KPAs. Results of path analysis using partial least squares (PLS) support the theoretical model and provide detailed understanding of the process improvement path. The analysis is based on 676 CMM-Based Appraisal for Internal Process Improvement (CBA IPI) assessments.",2008,10.1007/s10664-007-9049-1,no
Beyond Soundness: On the Semantic Consistency of Executable Process Models,"Executable business process models build on the specification of process activities, their implemented business functions (e.g., Web services) and the control flow between these activities. Before deploying such a model, it is important to verify control-flow correctness. A process is sound if its control-flow guarantees proper completion and there are no deadlocks. However a sound control flow is not sufficient to ensure that an executable process model indeed behaves as expected This is due to business functions requiring certain preconditions to be fulfilled for execution and having an effect on the process (postconditions). Semantic annotations provide a means for taking such further aspects into account. Inspired by OWL-S and WSMO, we consider process models in which the individual activities are annotated with logical preconditions and postconditions specified relative to an ontology that axiomatizes the underlying business domain. Verification then means to determine whether the interaction of control flow and logical states of the process is correct. To this end, we formalize the semantics of annotated processes and point out which kinds of flaws may arise. We then identify a class of processes with restricted semantic annotations where correctness can be verified in polynomial time; and we prove that the semantic annotations cannot be generalized without losing computational efficiency. The paper is written at a semi-formal level using an illustrative example, details can be looked up in a longer technical report.",2008,10.1109/ECOWS.2008.32,no
Process Quality Optimization Model Based on ARM and Immune Principle,"Facing with. increasing international competition, how to reduce process variability is always one of the major concerns of manufacturing organization. The purpose of this paper is to investigate a data driven optimization model: Process Quality Optimization Model (PQOM) based on Association Rules Mining (ARM and immune principle to support both static and dynamic optimization of process quality. Realization of PQOM consists of two stages. First, ARM is used to analyze historical SPC data to explore the explicit, hidden process input-output mapping relations that affect final product quality. Then, based on excavated rules, negative selection algorithm inspired by natural immune system is introduced to analyze on-line monitoring data for dynamic quality control. Two types of rule are utilized, namely Rules For Process Optimization (RFPO) and Rules For Exception Detection (REED). RFED is used as detector to target process exception, while RFPO is used as immune antibody that reacts to process exception. The PQOM model is tested in a squeeze casting enterprise to verify its feasibility and correctness.",2008,10.1109/ISCID.2008.216,no
Mathematical Model for Development and Robustness Evaluation of Fluidised Bed Drying Processes,"Fluidised bell technology is commonly used in the pharmaceutical industry for drying of granulations. The objective of the following work is to derive a mathematical model, which is able to predict the product moisture content and the exhaust air temperature during fluidised bed drying processes. At low product moisture contents, this is often very challenging. To overcome this problem it is suggested to incorporate adsorption isotherm models. It was found that for the aqueous granulation studied the best results were achieved by using the Guggenheim-Anderson-de Boer (GAB) isotherm. The mathematical model can be used to aid process development and robustness testing. The science-based approach may potentially support design space filings of new pharmaceutical products.",2008,,no
On Measuring Process Model Similarity Based on High-Level Change Operations,"For various applications there is the need to compare the similarity between two process models. For example, given the as-is and to-be models of a particular business process, we would like to know how much they differ from each other and how we can efficiently transform the as-is to the to-be model; or given a running process instance and its original process schema., we might be interested in the deviations between them (e.g. due to ad-hoc changes at instance level). Respective considerations can be useful, for example, to minimize the efforts for propagating the schema changes to other process instances as well. All these scenarios require a. method to measure the similarity or distance between two process models based on the efforts for transforming the one into the other. In this paper, we provide an approach using digital logic to evaluate the distance and similarity between two process models based on high-level change operations (e.g. to add. delete or move activities). In this way, we can not only guarantee that, model transformation results in a sound process model, but also ensure that related efforts are minimized.",2008,,no
A new marked point process model for the federal funds rate target - Methodology and forecast evaluation,"Forecasts of key interest rates set by central banks are of paramount concern for investors and policy makers. Recently it has been shown that forecasts of the federal funds rate target, the most anticipated indicator of the Federal Reserve Bank's monetary policy stance, can be improved considerably when its evolution is modeled as a marked point process (MPP). This is due to the fact that target changes occur in discrete time with discrete increments, have an autoregressive nature and are usually in the same direction. We propose a model which is able to account for these dynamic features of the data. In particular, we combine Hamilton and Jorda's [2002. A model for the federal funds rate target. Journal of Political Economy 110(5), 1135-1167] autoregressive conditional hazard (ACH) and Russell and Engle's [2005. A discrete-state continuous-time model of financial transactions prices and times: the autoregressive conditional multinomial-autoregressive conditional duration model. Journal of Business and Economic Statistics 23(2), 166 - 180] autoregressive conditional multinomial (ACM) model. The paper also puts forth a methodology to evaluate probability function forecasts of MPP models. By improving goodness of fit and point forecasts of the target, the ACH-ACM qualifies as a sensible modeling framework. Furthermore, our results show that NIPP models deliver useful probability function forecasts at short and medium term horizons. (C) 2008 Elsevier B.V. All rights reserved.",2008,10.1016/j.jedc.2008.02.007,no
The Evaluation Model of the Grey Correlative Degree and Analytic Hierarchy Process Analysis in the Enterprise Competitive Power and Its Application,"Formulating a scientific, comprehensive and quantitative mathematical method that can be used to evaluate the enterprise competitive power through utilizing both the Grey Correlative Degree Analysis and AHP connection method to construct a model for the scientific, comprehensive and key enterprise competitive power indicator system that we had chosen. And thus make the enterprise competitive power evaluation more objective and scientific by using this model.",2008,,no
Modeling and Evaluation of Air pollution from a Gaseous Flare in an Oil and Gas Processing Area,"Gas flaring is the one the hottest environmental issues in developing countries. Flaring of gases causes serious air pollution in oil processing area and enters many air pollutants such as NOx, CO, CO(2), SOx and total hydrocarbon into atmosphere. In this study, modeling of a typical gas flare was carried out using MATLAB. The results were compared with experimental data obtained from a gas flare in Petroleum Company in an industrial city of Nigeria. Also the effects of some parameters i.e. flare height and atmospheric conditions were studied on the dispersion pattern of pollutants.",2008,,no
Prediction of pricing and hedging errors for equity linked warrants with Gaussian process models,"Gaussian process (GP) model is a Bayesian kernel-based learning machine. In this paper, we propose a GP model with a various mixed kernel for pricing and hedging ELWs (equity linked warrants) traded at KRX with predictive distribution. We experiment with daily market data relevant to KOSPI200 call ELWs from March 2006 to July 2006, comparing the performance of the GP model with those of various neural network (NN) models to show its effectiveness. The applied NN models contain early stopping, regularized NN, and bagging. The proposed GP model shows that its forecast capability outperforms those of the three NN models in terms of both pricing and hedging errors, thereby generating consistent results. (c) 2007 Elsevier Ltd. All rights reserved.",2008,10.1016/j.eswa.2007.07.041,no
Evaluating the viscoelastic properties of glass above transition temperature for numerical modeling of lens molding process - art. no. 662403,"Glass molding process has emerged as a promising way to produce complex optical elements with high precision. Glass material shows explicit viscoelasticity at molding temperature, therefore studying the viscoelastic properties of glass at elevated temperature is important for the molding process. In this paper, Young's modulus and viscosity of glass were tested by compressing cylindrical glass gobs above the transition temperature, and obtained by curve fitting using the Burgers model of viscoclastic deformation. Based on the viscoelastic parameters obtained from experiments, a numerical model was developed to simulate the glass molding process with the help of a commercial finite element method software package. The simulation results provide an easy way to analyze and understand the molding process in detail, such as temperature distribution, stress, strain and strain rate, which are difficult or impossible to measure in experiments. A good agreement between the calculated pressing loads and the experimental results verified the validity of the numerical model, which can be applied to predict the pressing load during complex-shape glass lens molding process.",2008,,no
Evaluating the accuracy of a calibrated Rigorous Physical resist Model under various process and illumination conditions - art. no. 692411,"If RET selection by simulation is to be successful for the deep sub-wavelength technologies of today, then the predictions of the simulator must be quantitatively accurate over the parameter space of interest. The Rigorous Physical resist Model (RPM) within PROLITH and Lithoware is separable from the illumination conditions and the reflection behavior of the wafer stack, and thus should be an excellent candidate for such projects. In this work, the RPM is calibrated for a commercially available ArF photoresist using top-down CD-SEM data, including focus-exposure matrices and CD vs. mask pitch data, under fixed process conditions. It will be shown that this RPM is able to predict the performance of line, trench and contact features, with quantitative accuracy, under different numerical aperture and illumination conditions, even when the wafer stack is altered significantly. The stack alterations include resist thickness change, the presence or absence of an immersion topcoat, substitution of different underlying substrate materials and the use of a single or double layer anti-reflection coating. The resist model accurately describes both the experimental calibration data and two separate experimental validation datasets. The RMS error seen in the extrapolative predictions is comparable to that observed between the model and the original calibration dataset.",2008,10.1117/12.772766,no
Using stochastic process algebra models to estimate the quality of information in military sensor networks,"In a typical military application, a wireless sensor network will operate in difficult and dynamic conditions. Communication will be affected by local conditions, platform characteristics and power consumption constraints, and sensors may be lost during an engagement. It is clearly of great importance to decision makers to know what quality of information they can expect from a network in battlefield situations. We propose the development of a supporting technology founded in formal modeling, using stochastic process algebras for the development of quality of information measures. A simple example illustrates the central themes of outcome probability distribution prediction, and time-dependency analysis.",2008,10.1117/12.777277,no
An approach based on process signature modeling for roundness evaluation of manufactured items,"In evaluating the geometrical characteristics of mechanical part, cleverness may be added with the definition of an empirical model representing the ""signature"" left by the manufacturing process used to make the part. This manufacturing signature is the systematic pattern that characterizes all the features machined with that process. If such a model is available, it may be exploited to enhance geometrical inspection accuracy. In this paper an approach for geometrical inspection of machined profiles is proposed. This approach consists in computing form deviations by reconstructing the actual profile using a frequency model of process signature. The method has been thoroughly investigated in different simulated scenarios and benefits in terms of improved accuracy are demonstrated. Within the paper a case study, related to roundness of mechanical parts obtained by turning, is used. The relationships between the number of sampled points and fitting algorithms are also pointed out.",2008,10.1115/1.2904923,no
Reliability Evaluation of Distribution Networks Considering Optimization Models in the Restoration Process,"In operation studies of distribution networks, the restoration problem is solved using optimization techniques. However, in planning studies, such as reliability evaluation, the restoration problem is solved considering engineering judgment. Therefore, there is a mismatch between the restoration modeling used in the operation of the network and in the planning studies. This paper presents a method for reliability evaluation of distribution networks including optimization in the restoration process, through local search algorithm. The best restoration strategy is obtained considering network constraints (voltage drop and feeder loading) and optimizing operational objectives associated with: the number of interrupted customers and restoration times. The models and techniques proposed in this paper were applied in a large scale distribution network, in the northeast region of Brazil. The results demonstrate that the optimization models in the restoration process improve the reliability indices of the distribution networks.",2008,,no
Cooperative Problem Solving Process Based on MAS(NP) Model,"In order to enable the individual agents to be aware of the both macro constraints from agent social and instructions from agent's owner; a norm and policy extended agent BDI model (called NPCD-Agent) has been proposed, by integrating norms and policies into traditional agent BDI model, On the basis of NPCD-Agent model, this paper proposes a semantic model of Multi-agent system (called MAS(NP)), where cooperative problem solving (CRY) process is formally described. On the one hand, at the social level, the social constraint of agents can be specified by means of norms defined by the social administrators. And on the other hand, agents are instructed by polices presented by their owners. A case study shows that macro social constraint and instruction from agent owner are both considered in CPS process, which makes the CPS process more reasonable.",2008,10.1109/EUC.2008.109,no
Integrated Evaluation Model for Software Process Modeling Methods,"In order to help developers choose suitable modeling method according to specific modeling environment and requirement for achieving the best modeling effect, it is important to make reasonable assessment for software process modeling methods. An evaluation system for software process modeling methods is presented and an evaluation model for evaluating modeling methods is established using method that combines uncertain analytic hierarchy process (AHP) with fuzzy integrated evaluation technology. Further, by an example it proves that using the model can evaluate modeling methods soundly, so it has practical value in software project development.",2008,,no
An Agent-Oriented Fuzzy Evaluation Model for Evaluating Software Process Modeling Methods,"In order to help developers choose suitable modeling method according to specific modeling environment and requirement for achieving the best modeling effect, it is significant to make reasonable evaluation for software process modeling methods. Agent-oriented fuzzy evaluation model for evaluating software process modeling methods is discussed in the paper. Firstly, the software architecture of fuzzy evaluation is presented and optimized by using the design pattern and multi-Agent technology. Secondly, the unit structure of Agent is put forward and the composition principle of the unit structure is analyzed in details. Finally, the design and implementation of Agent-oriented fuzzy evaluation model are discussed by giving a four-table combined implementation template and the implementation algorithms. The proposed Agent-oriented fuzzy evaluation model has feasibility and engineering practical value.",2008,10.1109/ICINFA.2008.4608009,no
Evaluation and selection of models for out-of-sample prediction when the sample size is small relative to the complexity of the data-generating process,"In regression with random design, we study the problem of selecting a model that performs well for out-of-sample prediction. We do not assume that any of the candidate models under consideration are correct. Our analysis is based on explicit finite-sample results. Our main findings differ from those of other analyses that are based on traditional large-sample limit approximations because we consider a situation where the sample size is small relative to the complexity of the data-generating process, in the sense that the number of parameters in a 'good' model is of the same order as sample size. Also, we allow for the case where the number of candidate models is (much) larger than sample size.",2008,10.3150/09-BEJ127,no
Dimensional errors of rollers in the stream of variation modeling in cold roll forming process of quadrate steel tube,"In the cold roll forming process, the sheet metal strip is gradually bent into a desired profile by successive roller stations. During this process the roller locating errors of each roller station are introduced, transformed and accumulated until the sheet metal is bent into the final desired profile, which does influence the product's dimension quality, affect the end-welding of quadrate steel tube, and elongate the costly error-and-trial phase in ramp-up. This paper introduces procedures for expressing the influence of roller locating errors in the forming process of quadrate steel tube, which is based on the formulation of the stream of variation (SOV) model of roller dimensional errors using the CAD/CAPP parameters of the cold roll forming process. The SOV model is utilized to reveal the variation propagation in the manufacturing process. The modeling process is experimentally validated in a two-station forming process.",2008,10.1007/s00170-007-1066-0,no
STUDY ON THE MODEL OF VALUE AT QUALITY RISK IN SUPPLY CHAIN BASED ON THE PROCESS CAPABILITY INDEX OF C(PM),"In the past 20 years, quality risk in supply chain has gained wide attention. But the previous studies on quality risk in supply chain are mainly qualitative descriptions and analyses, lacking quantitative measurements and evaluations. This paper puts forward a general frame of quantitatively analyzing and evaluating quality risk in complex supply chain. It firstly summarizes and analyzes the research status of quality risk management of supply chain since 1990's. Secondly, focusing on the quantitative analysis of the first-type quality technology risk in supply chain, it gives a calculating model of value at quality risk based on the quality loss function and process capability index, further extends the model to the level of product and supply chain, and presents a general formula of calculating value at quality risk Finally, it validates the reliability of the calculating model of value at quality risk in supply chain by a simulation case based on ARENA.",2008,,no
Research on fuzzy information modeling process of Quality Function Deployment,"In the process of products design, most input information for Quality Function Deployment always possess the characteristics of fuzziness and language, so, various input information of house of quality are regarded as language variable, glossary convene of fuzzy language and fuzzy numeral. The main idea of modeling of fuzzy information with house of quality is explained in detail, and fuzzy integrating of custom needs and fuzzy modeling of products planning with HOQ are proposed. Thus, the fuzzy or qualitative information can be dealt with efficiently, and the significative decision in fuzzy circumstance can be made.",2008,,no
Assessing post-event processing after social situations: a measure based on the cognitive model for social phobia,"In their cognitive model of social phobia, Clark and Wells (1995) described a process called post-event processing that is characterized by prolonged ruminative and negative thinking about a past social event. Referring to this concept, Rachman and colleagues (2000) developed a questionnaire that has been used in several studies subsequently (Post-Event Processing Questionnaire (PEPQ)). Our aim was to examine a German version of the PEPQ and, where necessary, modify this measure. In Study 1 (N=130 students), we inspected the psychometric properties of the German version of the PEPQ. According to the item analyses, problematic items were identified and eliminated or reformulated. To map aspects of post-event processing that were missing in the original questionnaire, new items were developed. In Study 2, the psychometric properties of the revised instrument were analyzed in a sample of students (N=268).The revised instrument showed excellent internal consistency and a meaningful pattern of correlations with anxiety, depression, and dysfunctional self-consciousness. With regard to the factorial structure of the construct, our data suggest that a four-factorial model may be more appropriate than the one-dimensional structure proposed by Rachman and colleagues.",2008,10.1080/10615800701424672,no
Model based substrate set point control of yeast cultivation processes based on FIA measurements,"In this contribution a model based substrate control system for Saccharomyces cerevisiae fed batch cultivations is presented. The intention is to keep the concentration of the substrate glucose at a fixed selected set point during the process run. Set points of 0.07 g L-1 and 0.5 g L-1 are chosen, as the cells change their metabolism from pure oxidative to oxidative-reductive depending on the glucose concentration. The precise control of glucose concentration during cultivations still poses a challenge as the analysis with available online measurement systems still has the problem of noise and a time delay of at least 6 min. To compensate these effects a control system based on an ordinary FIA system for glucose measurements complemented by an extended Kalman filter is employed. The Kalman filter could handle the dynamics of the process accurately. Based on the glucose measurement every 3 min it estimated the biomass and glucose concentration as well as the growth rate factor and the volume of the culture broth. Utilising the estimated values of the process variables a feed forward controller was complemented by a PI controller to adjust the glucose concentration at the desired set points. During the control phase the standard deviation of the measurements are 0.002 g L-1 and 0.022 g L-1 for the set points of 0.07 g L-1 and 0.5 g L-1, respectively. (C) 2008 Elsevier B.V. All rights reserved.",2008,10.1016/j.aca.2008.06.011,no
Process control problems in ammonia production plant and their solution using hybrid models,In this paper problems of process control of ammonia producing plant are presented. The origin of these problems is analyzed and possible ways of their solution are presented. A hybrid model of the process is chosen as a possible solution. The key variables for the hybrid model of the process are defined and presented. Further steps of investigation are outlined.,2008,,no
Process-based model designed for filling of large data gaps in tower-based measurements of net ecosystem productivity,"In this paper we present a simple hybrid gap-filling model (GFM) designed with a minimum number of parameters necessary to capture the ecological processes important for filling medium-to-large gaps in Flux data. As the model is process-based, the model has potential to be used in filling large gaps exhibiting a broad range of micro-meteorological and site conditions. The GFM performance was evaluated using ""Punch hole"" and extrapolation experiments based on data collected in west-central New Brunswick. These experiments indicated that the GFM is able to provide acceptable results (r(2) > 0.80) when >500 data points are used in model parameterization. The GFM was shown to address daytime evolution of NEP reasonably well for a wide range of weather and site conditions. An analysis of residuals indicated that for the most part no obvious trends were evident; although a slight bias was detected in NEP with soil temperature. To explore the portability of the GFM across ecosystem types, a transcontinental validation was conducted using NEP and ancillary data from seven ecosystems along a north-south transect (i.e., temperature-moisture gradient) from northern Europe (Finland) to the Middle East (Israel). The GFM was shown to explain over 75% of the variability in NEP measured at most ecosystems, which strongly suggests that the GFM maybe successfully applied to forest ecosystems outside Canada. (C) 2007 Elsevier B.V. All rights reserved.",2008,10.1016/j.ecolmodel.2007.11.018,no
Summertime stratospheric processes at northern mid-latitudes: comparisons between MANTRA balloon measurements and the Canadian Middle Atmosphere Model,"In this paper we report on a study conducted using the Middle Atmospheric Nitrogen TRend Assessment (MANTRA) balloon measurements of stratospheric constituents and temperature and the Canadian Middle Atmosphere Model (CMAM). Three different kinds of data are used to assess the inter-consistency of the combined dataset: single profiles of long-lived species from MANTRA 1998, sparse climatologies from the ozonesonde measurements during the four MANTRA campaigns and from HALOE satellite measurements, and the CMAM climatology. In doing so, we evaluate the ability of the model to reproduce the measured fields and to thereby test our ability to describe mid-latitude summertime stratospheric processes. The MANTRA campaigns were conducted at Vanscoy, Saskatchewan, Canada (52 degrees N, 107 degrees W) in late August and early September of 1998, 2000, 2002 and 2004. During late summer at mid-latitudes, the stratosphere is close to photochemical control, providing an ideal scenario for the study reported here. From this analysis we find that: (1) reducing the value for the vertical diffusion coefficient in CMAM to a more physically reasonable value results in the model better reproducing the measured profiles of long-lived species; (2) the existence of compact correlations among the constituents, as expected from independent measurements in the literature and from models, confirms the self-consistency of the MANTRA measurements; and (3) the 1998 measurements show structures in the chemical species profiles that can be associated with transport, adding to the growing evidence that the summertime stratosphere can be much more disturbed than anticipated. The mechanisms responsible for such disturbances need to be understood in order to assess the representativeness of the measurements and to isolate long-term trends.",2008,,no
An Analytical Model to Describe the Efficiency of an Immersion Rinsing Process,"In this paper, a simple model is presented, describing an immersion rinsing process for flat solid substrates (e.g., semiconductor wafers). It is assumed that together with the solid a layer of processing liquid is transferred into a tank filled with rinsing liquid. The model calculates the replacement of the processing liquid with rinsing liquid as a function of the rinse time and the position in the tank. For static rinsing systems an analytical expression is readily obtained. For dynamic systems (where liquid convection plays a role) the concept of a convective boundary layer is introduced. It is assumed that within this boundary layer diffusion is the dominant transport mechanism while outside the boundary layer convection is dominant. Under these assumptions an analytical expression can also be obtained for dynamic rinsing with the thickness of the convective boundary layer as a process parameter. The resulting analytical expressions can be readily used to calculate the concentration of remaining processing liquid in the vicinity of the solid substrate as a function of rinse time. This provides a good way to determine the efficiency of the rinsing process. Moreover, these expressions allow us to assess the effect of changes in boundary layer thickness and thus can help with the optimization of existing rinsing processes.",2008,10.1109/TSM.2008.2005399,no
Evaluation and comparison of models and modelling tools simulating nitrogen processes in treatment wetlands,"In this paper, two ecological models of nitrogen processes in treatment wetlands have been evaluated and compared. These models were implemented, simulated, and visualized using the Modelica modelling and simulation language [P. Fritzson, Principles of object-oriented modelling and simulation with modelica 2.1 Wiley-IEEE Press, USA, 2004.] and an associated tool. The differences and similarities between the MathModelica Model Editor and three other ecological modelling tools have also been evaluated. The results show that the models can well be modelled and simulated in the MathModelica Model Editor, and that nitrogen decrease in a constructed treatment wetland should be described and simulated using the Nitrification/Denitrification model as this model has the highest overall quality score and provides a more variable environment. (c) 2007 Elsevier B.V. All rights reserved.",2008,10.1016/j.simpat.2007.08.010,no
Modeling and Evaluating of Emotional Processes,"In this paper, we explain the emotional decision-making model. Firstly, we presented an emotional model. The model is designed to explore people's behavior in certain circumstances, while under specified emotional states. Special attention was given to the thought process and actions displayed in the hypothetical scenarios. Secondly, we characterized thoughts and actions associated with each scenario and emotional state. Each particular action or proof of steps taken in the thought process was given a percentage value directly proportional to answers given by the test population. Finally, we developed an experimental game program for the evaluation of our emotional decision making model.",2008,,no
A channel model for the bit error rate process in 802.15.4 LR-WPAN wireless channels,"In this work we analyze the nature of the error process in IEEE 802.15.4 Low Rate-Wireless Personal Area Network (LR-WPAN) channels experiencing varying levels of interference from nearby 802.11b/g networks co-habiting the unlicensed 2.4GHz Industrial-Scientific-Medical (ISM) band. The entire analysis is performed on a set of residual bit error traces, the data collection methodology for which is described in detail. These traces allow us to identify individual bit positions in a received packet that were in error. We use lagged phase space analysis, a technique commonly used in the analysis of chaotic systems. The analysis uncovers the stable nature of the BER process during short bursts. This simple behavior of the Bit Error Rate (BER) can be captured by three parameters; the burst length of consecutive good packets, bad packets, and the distribution with which different BERs occur. Using collected residual bit error traces, we model the BER process by estimating the probability distributions of its parameters using Maximum Likelihood Estimates (MLE).",2008,10.1109/ICC.2008.55,no
The use of a meta-model to support multi-project process measurement,"In today's environment, software companies are engaged in multiple projects delivered on heterogeneous platforms for a wide class of applications in disparate application domains. They are increasingly engaged in the co-development of software systems through joint software development projects including staff from partners and customers as well as their own. As a result, they must support multiple software development processes while hying to guarantee uniform levels of process enactment, and product quality across all projects. Our approach is capable of providing process measurement in a joint-project, multi-process model business environment. It is based on a simple meta-model for computing across-process, multiple-project metrics designed to permit monitoring of CMMI compliance. The open source tool Spago4Q has been developed to support our approach and is capable of producing the measurements needed for monitoring of a set of large-scale development projects using different process models, in a real industrial setting in Europe. The results support the view that that it will not always be possible to aggregate the same set of metrics across disparate process models.",2008,10.1109/APSEC.2008.55,no
A Measurement-Driven Process Model For Managing Inconsistent Software Requirements,"Inconsistency is a pervasive issue in software engineering. Both general rules of inconsistency management and special case-based approaches to handling inconsistency have recently been considered. In this paper we present a process model for handling requirements inconsistency within the Viewpoints framework. In this process model, when an inconsistency among viewpoints is detected, a set of candidate proposals for handling inconsistency will be generated using techniques from Multi-agent automated negotiations. The proposals are then prioritized using an integrated measurement of inconsistencies. The viewpoints involved in the inconsistency will then enter the negotiations by being presented with the candidate proposals and thus selecting an acceptable proposal based on the priorities associated with each candidate proposal. The facilitate usability, in our process, we assume that the natural language requirements statements are first translated into corresponding logical formulas using a translator software. Moreover, the candidate proposals for handling inconsistency are also translated back from formal logic into natural language before being presented for selection.",2008,10.1109/APSEC.2008.24,no
"Study on security risk evaluation model, process and tools of information system","Information security risk evaluation, an important process of information security assure system, plays the key role in security design, construction, operation and maintenance of information system. Based on the security architecture described by security dimension, security layer and security plane, a common information security risk evaluation model is put forward and the risk evaluation technique is presented in this paper. Moreover, the general process of risk evaluation is illustrated in detail.",2008,,no
Modeling efficiency and water balance in PEM fuel cell systems with liquid fuel processing and hydrogen membranes,"integrating PEM fuel cells effectively with liquid hydrocarbon reforming requires careful system analysis to assess trade-offs associated with H(2) production, purification, and overall water balance. To this end, a model of a PEM fuel cell system integrated with an autothermal reformer for liquid hydrocarbon fuels (modeled as C(12)H(23)) and with H(2) purification in a water-gas-shift/membrane reactor is developed to do iterative calculations for mass, species, and energy balances at a component and system level. The model evaluates system efficiency with parasitic loads (from compressors, pumps, and cooling fans), system water balance, and component operating temperatures/pressures. Model results for a 5-kW fuel cell generator show that with state-of-the-art PEM fuel cell polarization curves, thermal efficiencies > 30% can be achieved when power densities are low enough for operating voltages > 0.72 V per cell. Efficiency can be increased by operating the reformer at steam-to-carbon ratios as high as constraints related to stable reactor temperatures allow. Decreasing ambient temperature improves system water balance and increases efficiency through parasitic load reduction. The baseline configuration studied herein sustained water balance for ambient temperatures <= 35 degrees C at full power and <= 44 degrees C at half power with efficiencies approaching similar to 27 and similar to 30%, respectively. (c) 2008 Elsevier B.V. All rights reserved.",2008,10.1016/j.jpowsour.2008.08.057,no
Evaluation of the sorption process for imidacloprid and diuron in eight agricultural soils from southern Europe using various kinetic models,"Kinetic studies are of great concern for understanding the processes and parameters involved in the sorption of pollutants by soils. Sorption kinetics of imidacloprid and diuron in eight soils of different characteristics, with very low organic carbon content were investigated. Pseudosecond-order kinetic reactions closely correlate with the experimental kinetic (R-2 > 0.98) in all soils. The sorbed amount of diuron was higher than that for imidacloprid. The low OC content of these soils correlated neither with the sorbed amount nor with the kinetic parameters for both pesticides. Imidacloprid sorption was correlated with silt and sand content and cation exchange capacity (CEC); meanwhile for diuron, no correlation was found. Thus, sorption kinetics take place throughout different mechanisms related mainly to the chemical character of the pesticides. Sorption kinetic parameters determined using three of the four models selected (pseudosecond-order kinetic reactions, Elovich equation, and Weber-Morris models) have been shown to be worthy to distinguish the process controlling the sorption kinetic of both pesticides.",2008,10.1021/jf8004349,no
Measurement based analysis and modeling of the error process in IEEE 802.15.4 LR-WPANs,"Knowledge of the error process and related channel parameters in wireless networks is invaluable and highly instrumental in a broad range of applications. Under the IEEE 802.15.4 Low Rate-Wireless Personal Area Networks (LR-WPAN) standard, compliant devices are capable of providing two pieces of information about the channel conditions along with each received packet, the Link Quality Indication (LQI) and Received Signal Strength Indication (RSSI). Together they constitute a form of Channel State Information (CSI). This work is based on statistical and information theoretic analysis of a very extensive data set of wireless channel traffic between a transmitter and receiver, called packet traces. Data is collected in a variety of documented environments. To our knowledge, this is the first detailed trace collection effort for this type of network. The traces distinguish themselves from data sets of other studies in that they record individual bit errors as well as packets that are never detected by receivers. First, we provide a detailed analysis of the IEEE 802.15.4 wireless channel. More specifically, we provide a detailed analysis of the Bit Error Rate (BER) process at individual bit and on a packet-by-packet basis. We explore the relationship between the packet-level BER process and the LQI and RSSI processes (also observable on a packet-by-packet). The analysis shows that measurements of both LQI and RSSI provide information that allows us to reduce uncertainty about the BER. Secondly, we develop a model of the BER process that is driven by observable CSI parameters. Thirdly, we continue our analysis with measurements of channel memory at the packet and bit level. We determine that the wireless channel 2 bits memory. At the packet level we observe that the amount of channel memory is more varied.",2008,,no
Research on Fuzzy-Grey Comprehensive Evaluation of Software Process Modeling Methods,"Large numbers of practices show that software process modeling technique can be effectively used to describe and analyze software process and offer opportunity to discover process improvements. There are many kinds of software process modeling method, so it is significant to make reasonable evaluation of software process modeling methods which can help developers choose the most appropriate modeling method according to specific modeling environment and requirement for achieving the best modeling effect. On the basis of evaluation system of software process modeling methods, comprehensive evaluation method which combines fuzzy evaluation and grey theory is applied to evaluate some modeling methods. It can make full use of fuzziness and grayness of evaluation information by experts which makes the evaluation more objective and accurate. Further, it is proved that the method can estimate modeling methods soundly and have feasibility and engineering value to software development project practice by an example.",2008,10.1109/KAM.2008.139,no
Policy to tackle the social determinants of health: using conceptual models to understand the policy process,"Like health equity, the social determinants of health (SDH) are becoming a key focus for policy-makers in many low and middle income countries. Yet despite accumulating evidence on the causes and manifestations of SDH, there is relatively little understanding about how public policy can address such complex and intractable issues. This paper aims to raise awareness of the ways in which the policy processes addressing SDH may be better described, understood and explained. It does so in three main sections. First, it summarizes the typical account of the policy-making process and then adapts this to the specific character of SDH. Second, it examines alternative models of the policy-making process, with a specific application of the policy streams and networks models to the SDH policy process. Third, methodological considerations of the preceding two sections are assessed with a view to informing future research strategies. The paper concludes that conceptual models can help policy-makers understand and intervene better, despite significant obstacles.",2008,10.1093/heapol/czn022,no
A stable clock error model using coupled first- and second-order Gauss-Markov processes,"Long data outages may occur in applications of global navigation satellite system technology to orbit determination for missions that spend significant fractions of their orbits above the navigation satellite constellation(s). Current clock error models based on the random walk idealization may not be suitable in these circumstances, since the covariance of the clock errors may become large enough to overflow flight computer arithmetic. A model that is stable, but which approximates the existing models over short time horizons is desirable. A coupled first- and second-order Gauss-Markov process is such a model.",2008,,no
"Challenging the role of calibration, validation and sensitivity analysis in relation to models of health care processes","Many applications of Operational Research in the context of health care involve processes of calibration, validation and sensitivity analysis. Indeed these processes seem to have such an elevated status that their absence is often regarded as a marker that a study is somehow substandard. Undoubtedly this may be the case, however there may also be circumstances where it is perfectly reasonable not to use such methods. This paper concerns general principles underlying mathematical modelling, particularly in contexts where data for calibration are either poor quality or non-existent. The discussion challenges the view that modelling should necessarily be subject to formulaic calibration, validation and sensitivity analysis processes in an attempt to achieve or establish 'accuracy'. Some models are used purely to deduce the logical consequences of a set of beliefs and in this context, the need for validation is at best questionable. If calibration and sensitivity analysis are to be carried out, there is a need to be clear about what the objective is in such analyses.",2008,10.1007/s10729-008-9058-7,no
"Guideline models, process specification, and workflow","Many clinical practice guidelines use flowcharts to aid the description of recommendations specified in the guidelines. Similarly a number of computer-interpretable guideline formalisms use graphical task networks to organize knowledge formalized in these models. However, the precise meaning encoded in these graphical structures is often unclear. In this presentation, I will survey some of the computer-interpretable formalisms and analyzed the graphical representations that are used to express process information embodied in clinical guidelines and protocols. I will argue that we can distinguish a number of process types: (1) flowcharts for capturing problem-solving processes, (2) disease-state maps that link decision points in managing patient problems over time, (3) plans that specify sequences of activities that contribute toward a goal, (4) workflow specifications that model care processes in an organization, and (5) computational processes that generates recommendations for specific clinical situations. These process types may be related to each other. A plan of actions, for example, may be mapped to a workflow process when its actions are assigned to healthcare providers playing different roles. A flowchart may depict decisions and actions that are performed over time. Furthermore, a guideline formalism may not make a commitment to the nature of processes being modeled. Its process-specification language may be used to encode different types of processes. Nevertheless, understanding the nature of process being modeled is crucial when it comes to enacting the encoded guidelines and protocols to provide decision support in clinical workflow. A process description that models the problem-solving steps depicted in a narrative guideline, for example, may contain steps that are not appropriate as part of human-computer interactions in a busy clinic.",2008,,no
Fundamental experiments as benchmark problems for modeling ultrasonic micro-impact processes,"Many ultrasonic processes are based on the mechanical contact of oscillating parts. Within ultrasonic machining (drilling, milling, grinding) micro impacts lead to abrasion at the processed workpiece and hopefully do not damage the tool. In ultrasonic motors ideally neither part gets worn. Thus the appropriate design of contact partners as well as their kinematics is a substantial task during the development of such devices. A first step to optimize contact mechanics is to understand their behavior and dependencies on parameter variations, such as vibration amplitude and pre-stress of the impacting parts. For a detailed understanding models validated with convincing experimental data from measurements are absolutely essential. This paper focuses on simple vibro-impact experiments which can be used as benchmark data for future models. The setup of the experiment and first experimental investigations are described in detail.",2008,10.1007/s10832-007-9169-4,no
Modelling of the Lenzing SO2 recovery process and validation with plant data,"Mass balances were performed for the recovery absorption stages of a magnesium bisulphite pulping process in order to obtain accurate basic data for further process simulation. Data for the main components (liquid flows, sulphur and magnesium oxide) were obtained at three different power levels of the downstream recovery boiler. Based on these results, single absorption stages were simulated with a commercial modelling package. The results of the mass balance and the results of the simulation of single absorption stages are compared. Furthermore, results simulated with the new model of the whole absorption recovery system are shown and problems, which appeared during the calculation runs, are described. (C) 2006 Elsevier Ltd. All rights reserved.",2008,10.1016/j.jclepro.2006.08.027,no
Defect occurrence and modeling for the thermomechanical processing of aerospace alloys,"Mechanism-based models for the evolution of defects during the thermomechanical processing of aerospace titanium- and nickel-based alloys are reviewed. These defects include those comprising microstructural/metal-flow irregularities and those that are damage related (i.e., cracks and cavities). The development of undesirable/nonuniform microstructures and cavities during the mill processing of alpha/beta titanium alloys is addressed first. Relatively simple, diffusion-based models of spheroidization and coarsening are applied to quantify the propensity for microstructure nonuniformities. Similarly, first-order micromechanical models have been formulated to estimate the effect of local crystallographic texture on nonuniform flow, the generation of triaxial stresses, and cavity growth/closure in alpha/beta titanium alloys with a colony-alpha microstructure. The occurrence of nonuniform grain structures (and so-called ALA, or ""as large as, "" grains) in cast, wrought, and powder-metallurgy superalloys is also discussed. A physics-based model to treat the topology of recrystallization and the evolution of ALA grains in such materials is proposed.",2008,10.1115/1.2840958,no
Understanding mating systems: A mathematical model of the pair formation process,"Mechanisms generating inequalities among males in reproductive success are key to understanding the evolutionary significance of sexual selection. This paper develops a stochastic model to quantitatively describe and analyze mating systems on a continuous scale from strict monogamy to extreme polygyny. The variance in male mating success is shown to increase with increased differences among males, with decreased interdependence of mating events, with increased population size, and with an increased number of females per male. The latter condition decreases the opportunity for sexual selection. It is found that different combinations of mating system characteristics can lead to the same variance in male mating success, although the distribution differs. This emphasizes the importance of using a model of this type to study mating systems, rather than relying solely on the variance in reproductive success as a descriptor of different systems. (C) 2007 Elsevier Inc. All rights reserved.",2008,10.1016/j.tpb.2007.09.003,no
Using process-oriented holonic (PrOH) modelling to increase understanding of information systems,"Methodologies for understanding business processes and their information systems (IS) are often criticized, either for being too imprecise and philosophical (a criticism often levied at softer methodologies) or too hierarchical and mechanistic (levied at harder methodologies). The process-oriented holonic modelling methodology combines aspects of softer and harder approaches to aid modellers in designing business processes and associated IS. The methodology uses holistic thinking and a construct known as the holon to build process descriptions into a set of models known as a holarchy. This paper describes the methodology through an action research case study based in a large design and manufacturing organization. The scientific contribution is a methodology for analysing business processes in environments that are characterized by high complexity, low volume and high variety where there are minimal repeated learning opportunities, such as large IS development projects. The practical deliverables from the project gave IS and business process improvements for the case study company.",2008,10.1111/j.1365-2575.2008.00308.x,no
Estimating transpiration and the sensitivity of carbon uptake to water availability in a subalpine forest using a simple ecosystem process model informed by measured net CO2 and H2O fluxes,"Modeling how the role of forests in the carbon cycle will respond to predicted changes in water availability hinges on an understanding of the processes controlling water use in ecosystems. Recent studies in forest ecosystem modeling have employed data-assimilation techniques to generate parameter sets that conform to observations, and predict net ecosystem CO2 exchange (NEE) and its component processes. Since the carbon and water cycles are linked, there should be additional process information available from ecosystem H2O exchange. We coupled SIPNET (Simple Photosynthesis EvapoTranspiration), a simplified model of ecosystem function, with a data-assimilation system to estimate parameters leading to model predictions most closely matching the net CO2 and H2O fluxes measured by eddy covariance in a high-elevation, subalpine forest ecosystem. When optimized using measurements of CO2 exchange, the model matched observed NEE (RMSE = 0.49 g C m-2) but underestimated transpiration calculated independently from sap flow measurements by a factor of 4. Consequently, the carbon-only optimization was insensitive to imposed changes in water availability. Including eddy flux data from both CO2 and H2O exchange to the optimization reduced the model fit to the observed NEE fluxes only slightly (RME = 0.53 g C m(-2)), however this parameterization also reproduced transpiration calculated from independent sap flow measurements (r(2) = 0.67, slope = 0.6). A significant amount of information can be extracted from simultaneous analysis of CO2 and H2O exchange, which improved the accuracy of transpiration estimates from measured evapotranspiration. Conversely, failure to include both CO2 and H2O data streams can generate results that mask the responses of ecosystem carbon cycling to variation in the precipitation. In applying the model conditioned on both CO2 and H2O fluxes to the subalpine forest at the Niwot Ridge AmeriFlux site, we observed that the onset of transpiration is coincident with warm soil temperatures. However, after snow has covered the ground in the fall, we observed significant inter-annual variability in the fraction of evapotranspiration composed of transpiration; evapotranspiration was dominated by transpiration in years when late fall air temperatures were high enough to maintain photosynthesis, but by sublimation from the surface of the snowpack in years when late fall air temperatures were colder and forest photosynthetic activity had ceased. Data-assimilation techniques and simultaneous measurements of carbon and water exchange can be used to quantify the response of net carbon uptake to changes in water availability by using an ecosystem model where the carbon and water cycles are linked. (C) 2008 Elsevier B.V. All rights reserved.",2008,10.1016/j.agrformet.2008.04.013,no
Modeling Algorithm-induced Errors in Iterative Mono-plotting Process,"Mono-plotting from a single photo is on efficient method to 3D spatial information collection. The core of the mono-plotting process is an algorithm to determine the ground coordinates of pixel points in a single image by intersecting the view ray with the DEM-defined surface. A traditional algorithm used in photogrammetry is to iteratively calculate the coordinates based on the inverse collinearity equations. Being an iterative method, this algorithm may induce errors to the derived coordinates. When the iteration precision (the distance between the lost two iterated points) becomes less than a predefined precision threshold, the iteration is considered convergent and the ground coordinates are produced. However, the true error of the output coordinates may be still worse than the threshold. This paper theoretically investigates the error budget of the iterative algorithm, models the true error from the iteration precision, and estimates the algorithm-induced error in the ground coordinate output.",2008,,no
Investigation of process impact on soft error susceptibility of nanometric SRAMs using a compact critical charge model,"Nanometric SRAMs are more vulnerable to experiencing particle induced soft error due to lower operating voltages coupled with higher packing density and increased process variations. In this paper, we present a compact model for critical charge of a 6T SRAM cell for estimating the effects of process variations on its soft error susceptibility. The model is based on dynamic behavior of the cell and a simple decoupling technique for the non-linearly coupled storage nodes. The model describes the critical charge in terms of transistor parameters, cell supply voltage, and injected current parameters. Consequently, it enables investigating the spread of critical charge due to process induced variations in these parameters and to manufacturing defects, such as, resistive contacts or vias. In addition, the model can estimate the improvement in critical charge when MIM capacitors are added to the cell in order to improve the soft error robustness. The critical charge calculated by the model is in good agreement with SPICE simulations for a commercial 90nm CMOS process with a maximum discrepancy of less than 5%.",2008,,no
Clarifying quality of life assessment: do theoretical models capture the underlying cognitive processes?,"Objective To develop an analysis scheme capturing the cognitive processes underlying QoL assessment to increase our understanding on how to interpret responses to QoL items. Tourangeau et al.'s (The psychology of survey response, 2000) and Rapkin and Schwartz' (Health Qual Life Outcomes 2:14, 2004) cognitive process models form the basis for this analysis scheme. Methods We conducted think aloud interviews with six cancer patients prior to and following radiotherapy to elicit the cognitive processes underlying the assessment of 7 EORTC QLQ-C30 items. Content analysis was carried out by two to four researchers independently. Eighty text fragments were analyzed inductively and combined in an iterative process with deductive analyses based on both models. Results We have developed a comprehensive analysis scheme feasible for analyzing the cognitive processes underlying QoL assessment qualitatively. All cognitive components of both models could be distinguished in our data. The cognitive component 'reporting and response selection' needed extension to fully capture the cognitive processes used. Conclusion The two models combined are useful in describing the cognitive processes cancer patients use in answering QoL items, and as such facilitate insight into patients' self-reported QoL assessments. Interestingly, the content of the cognitive processes not only differed between patients but also between items within patients and over time.",2008,10.1007/s11136-008-9380-z,no
Effects of anger suppression on pain severity and pain behaviors among chronic pain patients: Evaluation of an ironic process model,"Objective: Evidence for links between anger inhibition or suppression and chronic pain severity is based mostly on studies with correlation designs. Following from ironic process theory, we proposed that attempts to suppress angry thoughts during provocation would increase subsequent pain intensity among chronic low back pain (CLBP) patients, and do so through paradoxically enhanced accessibility of anger. Design: CLBP patients (N = 58) were assigned to suppression and nonsuppression conditions while performing a computer maze task with a harassing confederate. A structured pain behavior task (SPBT) followed. Main outcome measures: Self-reported anger, anxiety, and sadness following maze task. Self-reported pain severity and number of observed pain behaviors during SPBT. Results: Patients told to suppress during provocation: (a) reported greater anger following the maze task, reported greater pain intensity during the SPBT, and exhibited more pain behaviors than patients not suppressing; (b) postmaze anger levels significantly mediated group differences on pain behaviors. Conclusion: Attempts by CLBP patients to suppress anger may aggravate pain related to their clinical condition through ironically increased feelings of anger.",2008,10.1037/a0013044,no
Highly Process-Focused Organizational Performance Measurement Model,"On the basis of analyzing the most popular performance measurement systems, the paper absorbs their thoughts and develops a new highly process-driven performance measurement system which measures the organizational performance from five perspectives: vision, customer, strategy, process and staff. Its main points are: the enterprise is controllable, and its future result is determined by the present state, so it can achieve the future performance through controlling the most important elements of today with focusing on the best goals. Besides, in the model people are valued deadly and regarded as the first thing. Thus, our model also can be a managerial tool. Additionally, because there is no financial indicators, it can appraise the performance of non-profit organization or functional units.",2008,,no
RESEARCH ON QUALITY SUPERVISION MODEL FOR COOPERATIVE MANUFACTURING PROCESS IN VIRTUAL ENTERPRISE,"On the basis of the analysis of internal and external research on current situation of quality supervision for cooperative manufacturing process in virtual enterprise, the quality supervision model based on the method of neural network for cooperative manufacturing process in. virtual enterprise is firstly constructed; and secondly, combined with the practice of quality supervision for cooperative manufacturing process of power products in virtual enterprise this paper mainly probes into such key problems as the extraction of characteristic vector rested on the measured data of key quality characteristics and the expert assessment data for controlling state of key process and activities and its determination of input membership functions, three stages on the learning and training of neural network and the determination of output membership function based on preferences. And then, the above model is trained by the training sample data and tested by the testing sample data so as to achieve the goal of classification-judgment of the state of quality supervision of manufacturing cooperative partners. Finally, the intelligent quality analysis technology with machine learning in the main is formed.",2008,,no
Research on quality supervision model for cooperative manufacturing preparing process in virtual enterprise,"On the basis of the analysis of internal and external research on current situation of quality supervision theory for cooperative manufacturing preparing process in virtual enterprise, the quality supervision model for cooperative manufacturing preparing process in virtual enterprise is firstly constructed; and secondly, this paper mainly probes into contents and quantitative method of the supervision and approve of process planning and trial manufacturing quality plans in cooperative manufacturing process by means of ISO 9000 family standards and the method of system reliability analysis; thirdly, the method of appraisal and approve of the stability and capability in initial process is exploited by the methods of data transfer in continuous process and SPC technology; finally, contents and quantitative method of the appraisal and approve of trial products are presented; Meanwhile, the applied research is proposed in combined with the practice of cooperative manufacturing preparing process of power products in a virtual enterprise.",2008,10.1109/ICRMEM.2008.115,no
Research on quality supervision model for cooperative manufacturing verification process and its reliability in virtual enterprise,"On the basis of the analysis of internal and external research on current situation of quality supervision theory for cooperative manufacturing verification process in virtual enterprise, the quality supervision model for cooperative manufacturing verification process in virtual enterprise is firstly constructed; and secondly, this paper mainly probes into contents and quantitative method of the exmination and approve of process monitoring and measuring plans in cooperative manufacturing verification process by means of ISO 9000 family standards and system reliability analysis; thirdly, the approve method of verification process and its result of product's component are exploited by SPC technology, and then, the approve method of verification process and its result of products' function are presented; finally, supervision program of product acceptance inspection process of the union leader are discussed. Meanwhile, the applied research is proposed in combined with the practice of cooperative manufacturing verification process of power product in a virtual enterprise.",2008,,no
Research on Quality Supervision Model for Cooperative Manufacturing Preparing Process in Virtual Enterprise,"On the basis of the analysis or internal and external research on current situation of quality supervision theory for cooperative manufacturing preparing process in virtual enterprise, the quality supervision model for cooperative manufacturing preparing process in virtual enterprise is firstly constructed; and secondly, this paper mainly probes into contents and quantitative method of the supervision and approve of process planning and trial manufacturing quality plans in cooperative manufacturing process by means of ISO 9000 family standards and the method of system reliability analysis; thirdly, the method of appraisal and approve of the stability and capability in initial process is exploited by the methods of data transfer in continuous process and SPC technology; finally, contents and quantitative method of the appraisal and approve of trial products are presented; Meanwhile, the applied research is proposed in combined with the practice of cooperative manufacturing preparing process of power products in a virtual enterprise.",2008,,no
A Simple Atmospheric Evaporation Device as a Useful Tool for Validation of Air Flow Models and for Process Control Applications,"On their way from the producer to the consumer perishable horticultural produce frequently lose much water as the result of unfavourable environmental conditions. An effective control of the mass transfer under practical conditions is almost impossible due to the large number of influencing parameters and the complex flow conditions against and around the produce respectively the packaging units. In the present study newly developed, simple evaporation devices are introduced. An account of constructive details and methodical aspects of the development and the application is given. An application for the determination of local air velocities on a motor truck during transport of apples is presented as an example.",2008,,no
Facilitative interactions of model- and experience-based processes: Implications for type and flexibility of representation,"People are often taught using a combination of instruction and practice. In prior research, we have distinguished between model-based knowledge (i.e., acquired from explicit instruction) and experience-based knowledge (i.e., acquired from practice), and have argued that the issue of how these types of knowledge (and associated learning processes) interact has been largely neglected. Two experiments explore this issue using a dynamic control task. Results demonstrate the utility of providing model-based knowledge before practice with the task, but more importantly, suggest how this information improves learning. Results also show that learning in this manner can lead to ""costs"" such as slowed retrieval, and that this knowledge may not always transfer to new task situations as well as experientially acquired knowledge. Our findings also question the assumption that participants always acquire a highly specific ""lookup"" table representation while learning this task. We provide an alternate view and discuss the implications for theories of learning.",2008,10.3758/MC.36.1.157,no
"On Correctness, Compliance, and Consistency of Process Models","Process management incorporates a plethora of models, which are expressed in different languages for different layers of abstraction. A holistic process modeling environment must provide means for dealing with three types of modeling constraints. First, it must ensure a model's correctness. Second, a modeling environment must account for compliance of models with respect to models on higher layers of abstraction. Third, consistency of different models of the same language layer should be supported. In this paper, we exemplify these three constraint types and discuss how they can be enforced in a holistic modeling environment.",2008,10.1109/WETICE.2008.9,no
Evaluation of two process-based models to estimate soil N(2)O emissions in Eastern Canada,"Process-based models play an important role in the estimation of soil N(2)O emissions from regions with contrasting soil and climatic conditions. A study was performed to evaluate the ability of two process-based models, DAYCENT and DNDC, to estimate N(2)O emissions, soil nitrate- and ammonium-N levels, as well as soil temperature and water content. The measurement sites included a maize crop fertilized with pig slurry (Quebec) and a wheat-maize-soybean rotation as part of a tillage-fertilizer experiment (Ontario). At the Quebec site, both models accurately simulated soil temperature with an average relative error (ARE) ranging from 0 to 2%. The models underpredicted soil temperature at the Ontario site with ARE from -5 to -7% for DNDC and from -5 to -13% for DAYCENT. Both models underestimated soil water content particularly during the growing season. The DNDC model accurately predicted average seasonal N(2)O emissions across treatments at both sites whereas the DAYCENT model underpredicted N(2)O emissions by 32 to 58% for all treatments excluding the fertilizer treatment at the Quebec site. Both models had difficulty in simulating the timing of individual emission events. The hydrology and nitrogen transformation routines need to be improved in both models before further enhancements are made to the trace gas routines.",2008,,no
16T: toward a dynamic vendor evaluation model in integrated SCM processes,"Purpose - The purpose of this paper is to provide a ""trait based approach"" for vendor selection and evaluation for expensive procurements in large businesses through a simple and easy-to-use mathematical model using safety, quality, delivery and cost criteria. Design/methodology/approach - We use 16 traits addressing safety (S), quality (Q), delivery (D) and cost (C) areas for evaluating performance of a vendor on a linear 10 point scale. We start with evaluating each ""supplier-supply item"" combination and compute gross averages for each of the SQDC areas and finally arrive at an ""Overall Performance Index"" for each supplier-supply item combination. These indices form a ""Vendor Performance Dashboard"" for decision making. Findings - The case study shows that the proposed method is quick and easy to adopt, and provides a logical framework for vendor selection and management, based on performance in the four critical (SQDC) areas. Research limitations/implications - A lot of monotonous computations are required for the proposed vendor evaluation process. Hence, the scope for software development warrants further investigation. There is also a need to develop a process for weighting the different SQDC elements for application in different industry/market contexts. Originality/value - This paper presents a unique and simple approach for generating dashboard data for decisions regarding evaluation of vendors and distribution of sub-contracts in a dynamic technology intensive market with practical examples. The proposed model for vendor selection and determination of order sizes is less dependent on complex algorithms and more practical and logically framed than existing models.",2008,10.1108/13598540810905642,no
Measuring similarity between business process models,"Quality aspects become increasingly important when business process modeling is used in a large-scale enterprise setting. In order to facilitate a storage without redundancy and an efficient retrieval of relevant process models in model databases it is required to develop a theoretical understanding of how a degree of behavioral similarity can be defined. In this paper we address this challenge in a novel way. We use causal footprints as an abstract representation of the behavior captured by a process model, since they allow us to compare models defined in both formal modeling languages like Petri nets and informal ones like EPCs. Based on the causal footprint derived from two models we calculate their similarity based on the established vector space model from information retrieval. We validate this concept with an experiment using the SAP Reference Model and an implementation in the ProM framework.",2008,,no
Analyzing process quality of early childhood education with many facet rash measurement model,"Quality of early childhood education institutions specifically, dimensions of process quality, should be evaluated. Purpose of this study is to analyze process quality of early childhood education by using many-facet Rasch measurement model (MFRM). In this study, data were collected from twelve early childhood education institutions by four independent judges. Early Childhood Environment Rating Scale (ECER) was used to evaluate the process quality of the institutions. MFRM was applied to analyze the data. The results indicated that early childhood education institutions were below the desirable level of process quality. It has been found that judges exhibited similar behaviors and when item statistics were examined they served the purpose Of the evaluation. Standardized measurement tools and measurement models were recommended to increase process quality of early childhood education.",2008,,no
Markov Decision Processes Models for the Returns Disposition Problem in Reverse Supply Chains,"Quick disposition of returns is the most important part in a successful reverse supply chain, if returns can be disposed in time and processed quickly, more profit we can get from them, and also service level can be increased. In this paper, the model is called returns disposition model without outsourcing. By theoretical proof, we give out optimal structures for this model which can help us with decision-making in the real world.",2008,,no
Performance Evaluation Model for R&D Department: An Integrated Balanced Scorecard and Analytical Hierarchy Process Approach,"R&D activities and the most common source of it, R&D departments, become more critical than ever for organizations to survive in this highly competitive environment. In order to gain competitive advantage in the market, organizations are obliged to generate their own technologies. As a result, the process of this technology generation, which manages the funnel from idea to the product in the market, should be monitored, assessed and managed effectively. Despite the difficulties originated from the nature of R&D activities, companies are forced by competitive external pressures to evaluate the process and corresponding results. This paper aims to integrate the Balanced Scorecard (BSC) and Analytical Hierarchy Process (AHP) in order to develop an analytical approach to evaluate the performance of R&D departments. The model proposed by this stud weighs and ranks main indicators affecting the R&D performance and is applied to a company with an R&D experience more than 15 years.",2008,10.1109/ISKE.2008.4731127,no
Validation of a Rapid Thermal Processing model in steady-state,"Rapid Thermal Processing (RTP) is widely used in advanced semiconductor manufacturing. The present work deals with the heat transfer from infrared lamps to the silicon wafer in a commercial RTP equipment. Both numerical and experimental approaches are considered. For numerical purposes, the RTP system is modelled in two (2D) and three dimensions (3D). Calculations are performed in steady-state. The computational fluid dynamics method (CFD) is used for solving the mass and heat conservation equations. The radiative heat transfer equation is solved with the Monte Carlo method. In order to validate these models, measurements of the wafer temperature are realized for five electric power values supplied to the infrared lamps. The experimental wafer temperature profiles are in good agreement with the numerically calculated ones. Moreover, a confrontation between the experimental temperature of the infrared lamp filaments evaluated from the Ohm law and the one used in the numerical calculations shows a good agreement with the 3D model. The slight difference observed with the 2D model is explained. So the numerical simulations are fully validated. Two relations are established in order to predict the power which has to be applied to infrared lamps to obtain the required wafer temperature. (C) 2008 Elsevier B.V. All rights reserved.",2008,10.1016/j.mee.2008.07.012,no
Identifying student difficulty in problem solving process via the framework of the House Model (HM),"Recently, many students have been losing their interest in physics. One of the essential reasons why students look away from physics is the fact that they face difficulty in solving physics problems. Since mechanics is a fundamental subject in physics, many researchers have studied how students' learn mechanics and solve problems related to mechanics. However. there is little research oil die students' specific difficulties in the process of problem solving This study investigated degree of students' difficulties in process and the core sources of these difficulties. 24 university students who majored in physics education participated in this study. We have developed a framework, House Model (HM), for helping and analyzing Students' problem solving. We found that students felt greater difficulty in planning and executing steps than in visualizing, knowing and finding steps. As die problems grew in difficulty, this pattern became more distinct. We also found the sources of the students' difficulties and discussed die educational implications of these results.",2008,,no
A dynamic cost model for the effect of improved process flexibility in steel plants,"Reduced setup times in the rolling mill generate flexibility which allows shorter leadtimes through continuous casting and hot rolling. Traditionally known as schedule-free rolling, this flexibility allows the rolling mill to handle variations without the need for buffering. Cost models based on system dynamics methodology are used to assess the economic potential. Effects on inventory, energy and work roll consumptions are analysed. The simulation results show that investments in flexible processes can be evaluated with dynamic cost models. There is an opportunity for significant cost reduction, but also lowered environmental impact due to reduced energy consumption.",2008,10.1007/978-1-84800-267-8_9,no
Correctness-preserving configuration of business process models,"Reference process models capture recurrent business operations in a given domain such as procurement or logistics. These models are intended to be configured to fit the requirements of specific organizations or projects, leading to individualized process models that are subsequently used for domain analysis or solution design. Although the advantages of reusing reference process models compared to designing process models from scratch are widely accepted, the methods employed to configure reference process models are manual and error-prone. In particular, analysts are left with the burden of ensuring the correctness of the individualized process models and to manually fix errors. This paper proposes a foundation for configuring reference process models incrementally and in a way that ensures the correctness of the individualized process models, both with respect to syntax and behavioral semantics. Specifically, assuming the reference process model is behaviorally sound, the individualized process models are guaranteed to be sound.",2008,,no
Errors of mathematical processing: The relationship of accuracy to neural regions associated with retrieval or representation of the problem state,"Regions in the prefrontal and parietal cortices contribute to mathematical problem-solving through their roles in retrieval and mental representation, respectively. This fMRI study examined whether activity in these regions tracked with subsequent errors in solving algebraic equations. Whereas previous studies have used recognition paradigms (e.g., decide whether 2+2=5 is correct) to assess the relationship of neural functioning with performance, participants in this study were required to generate an answer themselves. For the prefrontal region that in previous studies has exhibited activity modulated by retrieval demands, we found that activity was greater when equations were solved correctly than when errors were committed, Good solvers also tended to exhibit more activity in this region than poor problem-solvers. This was not true for the region in the parietal cortex that has been associated with representing the number of transformations to the equation. This suggests that, in our adult sample, successful performance was related to retrieval abilities rather than to difficulty in representing or updating changes in the equation as it is being solved. (C) 2008 Elsevier B.V. All rights reserved.",2008,10.1016/j.brainres.2008.08.030,no
siRNA screening: A process model to evaluate hit rate discovery,"RNA interference has been widely used to identify genes involved in the production of particular biological phenotypes. This type of gene silencing technology has been used in plants, invertebrates and mammalian systems [1]. The availability of the sequences of large numbers of genes has allowed large libraries of siRNAs to be produced. To effectively use these libraries in screens, high-throughput robotic screening methodologies have been devised. The identification of meaningful results from any screening system requires the analyst to identify and discount variances in output that arise from the devices used in the screen as well as variances that arise from biological components in the screen that are unrelated to gene silencing. In this developing technology, this analytical task is made difficult by variances in uptake of the siRNAs by the cells, variations in the magnitude of the silencing effect, and mechanical effects that can produce systematic alterations in cell delivery and cell growth during the experiment. To examine how the analysis can be optimized, models of the screening process have been built using estimates of the various noise and signal variances derived from available screen data. Synthetic data was then generated from this model and used to test the capability of a number of data normalization methods to reduce noise and allow signal detection.",2008,,no
"Waste Recycling in Romania, a Component of Waste Management Case Study - Economic Model for Evaluating a Recycling Process","Romanian waste management challenges various factors. One of its main problems is the increasing amount of plastic waste in concurrence with declining landfill space. The usage of PET is growing in Romania at a rate of 15% per year. Waste recycling: represents the breaking down of materials from waste streams into raw materials. An economic model for recycling cost centers is presented in this study. The model gives for each activity its respective costs and profits in the recycling of secondary raw materials by analyzing the collection and regeneration phases. The discussion refers only to plastic materials. With other materials, it is possible reutilization, following these model guidelines, to reach specific results, individualizing and characterizing the phases of analysis. From the application of the model it clearly emerges that collection and reuse represent an environmentally correct instrument and an optimum investment.",2008,,no
Modelling crystal growth between potash particles near contact points during drying processes. Part I: Problem formulation,"Salts and chemical fertilizers, such as potash, have a strong tendency to cake when exposed to humid air. This paper examines the recrystallization process between potash particles as a result of water evaporation after the particles have been previously wetted. A theoretical/numerical model is presented to simulate binary ion diffusion and crystallization near one contact point between two potash (KCl) particles during a typical drying process. The effects of three independent factors are investigated, initial moisture content, evaporation rate, and degree of supersaturation on the surface surrounding the contact point. In part I of this two-part series of papers, this problem is formulated subject to specific boundary conditions and initial conditions and the method of numerical analysis is presented.",2008,10.1002/cjce.20026,no
Modeling to improve the efficiency of product and process development,"Simulations are used in many areas of the food development chain, have major impact in process engineering and scale-up studies, and are used to guide chemical and physical understanding in food research. On the other hand, they have much less impact on the development of food recipes. Consumer pressure is driving for healthier yet tastier foods, which need to be built for the purpose. This requires an understanding of the functions of food and how its structure can be engineered for that purpose and will require integration of product and process development. This should increase the use and impact of simulations, because of the increased scientific content of product development and engineers becoming more accustomed to using simulation as one tool of the trade. However, to date there are too few validated models amenable to truly predictive simulations: more needs to be done to develop predictive models at the fundamental level, before industrial food development can use simulations in the daily routine. Another key factor is the education of more people in the proper understanding and use of modeling as one of the tools guiding food product development.",2008,10.1111/j.1541-4337.2007.00033.x,no
Evaluation of snow models in terrestrial biosphere models using ground observation and satellite data: impact on terrestrial ecosystem processes,"Snow is important for water management, and an important component of the terrestrial biosphere and climate system. In this study, the snow models included in the Biome-BGC and Terrestrial Observation and Prediction System (TOPS) terrestrial biosphere models are compared against ground and satellite observations over the Columbia River Basin in the US and Canada and the impacts of differences ill snow models oil simulated terrestrial ecosystem processes are analysed. First, a point-based comparison of ground observations against model and satellite estimates of snow dynamics are conducted. Next, model and satellite snow estimates for the entire Columbia River Basin are compared. Then, using two different TOPS simulations, the default TOPS model (TOPS with TOPS snow model) and the TOPS model with the Biome-BGC snow model, the impacts of snow model selection oil runoff and gross primary production (GPP) are investigated. TOPS snow model predictions were consistent with ground and satellite estimates of seasonal and interannual variations in snow cover, snow water equivalent, and snow season length; however, in the Biome-BGC snow model, the snow pack melted too early, leading to extensive Underpredictions of snow season length and snow covered area. These biases led to earlier simulated peak runoff and reductions ill summer GPP, underscoring the need for accurate snow models within terrestrial ecosystem models. Copyright (c) 2007 John Wiley & Sons, Ltd.",2008,10.1002/hyp.6616,no
A speech-act model for talking to management. Building a framework for evaluating communication within the SRI engagement process,"Socially Responsible Investment (SRI) has grown considerably over the past three decades. One form of SRI, engagement-SRI, is today by far the most practiced form of SRI (in assets managed) and has the potential to mainstream SRI even further. However, lack of formalized engagement procedures and evaluation tools leave the engagement practice too opaque for such a mainstreaming. This article can be considered as a first step in the development of a standard for the engagement practice. By developing an engagement heuristic, this article offers a more transparent engagement dialog. Drawing on Stevenson's and Austin's speech-act theories, this article develops a classification of management's responses to the signaling of allegations and controversies on two dimensions: a factual dimension concerning (dis)agreements on factual claims and an attitudinal dimension concerning (dis)agreements on responsibilities, values, and norms. On the basis of the distinctions this article develops, the authors provide for a synoptic table and offer a next-step heuristic for the engagement process that started with signaling a concern to management. The article uses an engagement logic that, while keeping the exit option for the investor open, allows management to address signaled concerns without having to let down or to opt out at the first setback in the dialog process between investor and investee corporation.",2008,10.1007/s10551-007-9563-5,no
GENSIM 2.0: A customizable process simulation model for software process evaluation,"Software process analysis and improvement relies heavily on empirical research. Empirical research requires measurement, experimentation, and modeling. Moreover, whatever evidence is gained via empirical research is strongly context dependent. Thus, it is hard to combine results and capitalize upon them in order to improve software development processes in evolving development environments. The process simulation model GENSIM 2.0 addresses these challenges. Compared to existing process simulation models in the literature, the novelty of GENSIM 2.0 is twofold: (1) The model structure is customizable to organization-specific processes. This is achieved by using a limited set of macro-patterns. (2) Model parameters can be easily calibrated to available empirical data and expert knowledge. This is achieved by making the internal model structures explicit and by providing guidance on how to calibrate model parameters. This paper outlines the structure of GENSIM 2.0, shows examples of how to calibrate the simulator to available empirical data, and demonstrates its usefulness through two application scenarios. In those scenarios, GENSIM 2.0 is used to rank feasible combinations of verification and validation (V&V) techniques with regards to their impact on project duration, product quality and resource consumption. Though results confirm the expectation that doing more V&V earlier is generally beneficial to all project performance dimensions, the exact rankings are sensitive to project context.",2008,,no
Software Test Process Measurement and Control Model based on SPC Technology,"Software test is an important method of software quality assurance. With the popularity of iterative lifecycle, Software testing process becomes a complex process covering the overall software development lifecycle. To insure the stabilization and effectiveness of test process, quantitative management pattern based on measurement must be adopted to monitor and control the test process. In this process, the effectiveness and reality of measuring data and the practicality and validity of analysis methods are two critical problems. In this paper, a measurement and management model of test process based on statistic process control technology is proposed, and the application of this model in real projects proved the good effect of this model.",2008,,no
Evaluation of insoluble bone gelatin as a carrier for enhancement of osteogenic protein-1-induced intertransverse process lumbar fusion in a rabbit model,"Study Design. Postero-lateral lumbar fusion in a rabbit model was performed to compare the bone induction potential of autograft, insoluble bone gelatin (ISBG), osteogenic protein-1 (OP-1), and the combination of ISBG and OP-1. Objective. To evaluate the efficiency of ISBG as a carrier/ enhancer for OP-1 in a rabbit spinal fusion model. Summary of Background Data. OP-1 or recombinant human BMP-7 has been shown to be effective in inducing new bone formation in surgical applications such as spinal arthrodesis. However, the lack of an ideal carrier contributes to its associated comorbidities (e. g., uncontrolled bone growth, local inflammatory over- response, nonfusion) and limits its use clinically. Methods. Adult New Zealand white rabbits ( n = 32) underwent bilateral lumbar intertransverse process fusion procedures at L5 to L6 and were randomized to receive: (1) autograft; ( 2) ISBG; ( 3) OP- 1; or ( 4) ISBG in combination with OP-1 (ISBG + OP- 1). Spinal fusion masses were evaluated by manual palpation, biomechanical testing, radiographic assessment, microcomputer tomography scanning and histologic examination at 6 weeks after surgery. Results. Treatment of ISBG + OP- 1 resulted in higher spinal fusion rates (7 of 7, 100%) than that of autograft ( 3 of 7, 43%), ISBG ( 2 of 8, 25%), and OP-1 (2 of 7, 29%) based on manual palpation ( P < 0.01). Greater fusion rates in the ISBG + OP- 1 group were also evidenced by radiographic examination (P < 0.01), microcomputer tomography bone volume analysis (P < 0.01), and biomechanical testing (P < 0.05). Histologic assessment demonstrated that treatment of ISBG + OP-1 induces new contiguous bone formation in the interval between the transverse processes which was absent in the other groups. Conclusion. In this study, ISBG + OP-1 resulted in more effective lumbar intertransverse process fusion than autograft, OP-1 putty or ISBG alone. ISBG is capable of enhancing OP-1-induced bone formation.",2008,10.1097/BRS.0b013e31817e1cf1,no
Inter-trials variability analysis by ARIMA models: An informational spring in understanding memory process,"The aim of this paper is to discuss the conceptual limitation of variability when simply conceived as stochastic error around the mean and to show that other forms of variability can represent an important informational source to understand human cognition.Recent papers which have studied behavioural fluctuations in time have pointed out the relevance of such modelling, which are proving properties of processes underlying behaviour (Spray & Newell, 1986). Nevertheless, researches focusing on variability remained rare for two reasons: the methodological constraints of variability modelling in time and the lack of knowledge concerning time series analysis tools (Arnau & Bono, 2001). In this methodological paper, we begin by outlining some limitations of the traditional approaches of variability analysis, before examining the rationale of discrete time series analysis and the use of ARIMA models. Then, an illustration of this approach is provided through the presentation of a research on memory. The aim of this research was to model the dynamics of memory processes according to memorisation and recall conditions (Delcor et aL, 2003). We demonstrated that analysis of recalls accuracy fluctuations through ARIMA models proffers clues as the nature and properties of processes underlying recalls. Stochastic modelling of successive recalls reveals the time-dependent and deterministic character of memory processes and offers relevant perspective of research.",2008,,no
Problems pertaining to macroeconomic risk assessment in the process of modelling of investment solutions of air carriers,"The article deals with the problems of investment development of air companies. The author examines the aspects of modelling of investment solutions under conditions of developing aviation business, giving methods of modelling of demand, including those based oil dynamics of macroeconomic indicators. The article contains the problems relating to assessment of macroeconomic risks, using models of assessment of discount rates. Variants of assessment of currency risks in the process of evaluation of efficiency of international investment solutions are analysed as well.",2008,,no
The supplier evaluation model based on the Analytic Hierarchy Process and Grey Relative Analysis Process,"The auto part suppliers are the source of the automotive supply chain and play an important role in the manufacture resources. Its behaviour is closely related with the market and the quality. Its achievement directly affects the product quality, the marketing ability of the auto company, and the customer satisfaction. The article discusses the auto part suppliers' evaluation criteria, which are put forward from the service ability, business enterprise ability, cooperation ability and sustainable development ability, according to the characteristics of auto companies, combining the trends of the automotive part industry. The article also presents the sub-essential factors. Taking the advantages of the grey characteristic based on the AHP for weight analysis, the integration method of the Analytic Hierarchy Process (AHP) and Grey Relative Analysis Process (GRAP) had been used in above evaluation model. The evaluation method can not only evaluate the sub-system of the complicated system, but also can be used in the comprehensive evaluation process based on the sub-system evaluation. An application example has been given at last.",2008,,no
A Mixed Integer Programming Model for the Multiperiod Scheduling Problem of Blending Zinc-Bearing Materials to Minimize Setups in the Hydrometallurgical Process,"The blending procedure, selecting from various kinds of zinc-bearing materials containing different contents of components and determining their mixing ratios, is the first step in the hydrometallurgical zinc process. Whenever a setup takes place to change the selection of materials or their ratios in the blending process, the setup itself and the adjustments required in the subsequent processes incur cost, time, and effort. Reducing the number of setups by keeping the blending ratios of the selected materials for the longest possible period is critical for consistent operation and processing and, thus, for assuring the quality of the final zinc product. In this article, we formulate a mixed integer programming model for the blending schedule that minimizes the number of setups over a planning horizon, given the constraints on the contents of components and the daily schedules of material supply and zinc production. Also, we propose an efficient heuristic, which provides a solution of good quality within reasonable time bounds. The applicability and efficiency of the model along with its heuristic are verified through simulation experiments as well as the model's application to a real zinc refinery.",2008,10.1007/s11663-008-9194-y,no
Interaction of the methane cycle and processes in wetland ecosystems in a climate model of intermediate complexity,"The climate model of the Institute of Atmospheric Physics of the Russian Academy of Sciences (IAP RAS CM) has been supplemented with a module of soil thermal physics and the methane cycle, which takes into account the response of methane emissions from wetland ecosystems to climate changes. Methane emissions are allowed only from unfrozen top layers of the soil, with an additional constraint in the depth of the simulated layer. All wetland ecosystems are assumed to be water-saturated. The molar amount of the methane oxidized in the atmosphere is added to the simulated atmospheric concentration of CO2. A control preindustrial experiment and a series of numerical experiments for the 17th-21st centuries were conducted with the model forced by greenhouse gases and tropospheric sulfate aerosols. It is shown that the IAP RAS CM generally reproduces preindustrial and current characteristics of both seasonal thawing/freezing of the soil and the methane cycle. During global warming in the 21st century, the permafrost area is reduced by four million square kilometers. By the end of the 21st century, methane emissions from wetland ecosystems amount to 130-140 Mt CH4/year for the preindustrial and current period increase to 170-200 MtCH(4)/year. In the aggressive anthropogenic forcing scenario A2, the atmospheric methane concentration grows steadily to approximate to 3900 ppb. In more moderate scenarios A1B and B1, the methane concentration increases until the mid-21st century, reaching approximate to 2100-2400 ppb, and then decreases. Methane oxidation in air results in a slight additional growth of the atmospheric concentration of carbon dioxide. Allowance for the interaction between processes in wetland ecosystems and the methane cycle in the IAP RAS CM leads to an additional atmospheric methane increase of 10-20% depending on the anthropogenic forcing scenario and the time. The causes of this additional increase are the temperature dependence of integral methane production and the longer duration of a warm period in the soil. However, the resulting enhancement of the instantaneous greenhouse radiative forcing of atmospheric methane and an increase in the mean surface air temperature are small (globally < 0.1 W/m(2) and 0.05 K, respectively).",2008,10.1134/S0001433808020011,no
Modeling procyanidin self-association processes and understanding their micellar organization: A study by diffusion NMR and molecular mechanics,"The colloidal behavior of eight synthetic procyanidins (three monomers, four dimers, and a trimer) has been investigated in water or in a winelike medium using DOSY NMR spectroscopy and molecular dynamics simulations. Different behavior was observed for monomers and oligomers. Monomers self-associate with a high affinity constant (37-53 M-1) to form micelles at low cmc (critical micelle concentration) values (1-5 g.L-1). These micelles undergo a time-dependent coalescence process to form hazes and precipitates. As for dimers and the trimer, self-association also occurs but with a lower affinity (similar to 6 M-1) and at higher cmc values (10-20 g.L-1) to form small micelles (<5 nm) that remain stable throughout the experiment. The presence of 10% ethanol does not significantly affect the self-association constant for monomers and oligomers but increases their cmc values by approximately 50% and decreases the micelle size by a factor 2. However, the presence of 20 mM NaCl appears to negate the effect of ethanol. This study helps to clarify the role of procyanidin monomers versus oligomers in wine turbidity and demonstrates that procyanidin oligomers are fully available to interact with saliva proteins.",2008,10.1021/la8015904,no
Integrating remote sensing and a process-based hydrological model to evaluate water use and productivity in a south Indian catchment,"The combined use of remote sensing and a distributed hydrological model have demonstrated the improved understanding of the entire water balance in an area where data are scarcely available. Water use and crop water productivity were assessed in the Upper Bhima catchment in southern India using an innovative integration of remotely sensed evapotranspiration and a process-based hydrological model. The remote sensing based Surface Energy Balance Algorithm for Land (SEBAL) was used to derive an 8 month time series of observed actual evapotranspiration from October 2004 to May 2005. This dataset was then used in the calibration of the Soil and Water Assessment Tool (SWAT). This hydrological model was calibrated by changing 34 parameters to minimize the difference between simulated and observed actual evapotranspiration. The calibration efficiency was assessed with four different performance indicators. The calibrated model was used to derive a monthly basin water balance and to assess crop water productivity and crop water use for the irrigation year 2004-2005. It was found that evapotranspiration is the largest water loss in the catchment and total evaporative depletion was 38,172 Mm(3) (835 mm). Of the total evaporative depletion 42% can be considered as non-beneficial and could be diverted to other beneficial utilization. Simulated crop water productivities for sugarcane, sorghum and winter wheat are relatively high at 2.9 kg/m(3), 1.3 kg/m(3) and 1.3 kg/m(3), respectively. The frequency distributions of crop water productivity are characterised by low coefficient of variation, yielding limited scope for improvement in the agricultural areas under the current cropping systems. Further improvements in water productivity may however be achieved by shifting the crop base from sugarcane to a dual crop and introducing a fallow period from March to May or by converting non-productive rangelands to bio, fuel production or other agricultural land uses. (c) 2007 Elsevier B.V. All rights reserved.",2008,10.1016/j.agwat.2007.08.006,no
Prospective scenarios for water quality and ecological status in Lake Sete Cidades (Portugal): The integration of mathematical modelling in decision processes,"The design of alternative strategies for water and ecological quality protection at the Lake Verde of Sete Cidades should be coupled with the assessment of future trophic states. Therefore, a mathematical model was developed to make prospective scenarios to reduce the risk of environmental degradation of the lake, and a modified Psenner scheme was used to characterize P distribution in the sediments. The model was able to describe thermal stratification, nutrient cycling (P, NH4 and NO3 dissolved O-2, and phytoplankton dynamics in the water column and adjacent sediment layers. Internal P recycling, resulting from thermal stratification and sediment anoxia, was identified as the main cause for the increase of P concentration in the hypolimnion followed by slow transfer to the epilimnion (about 20 mu g/L annual average). Cyanobacteria blooms during spring were explained by the availability of P and increased water temperature verified during this season. The most sensitive model parameter was sediment porosity. This parameter has a direct effect in dissolved 02 and P profiles and also in phyroplankton biomass. Finally, different water quality restoration scenarios were identified and their effectiveness assessed. Without the adoption of remediation measures (scenario control), Lake Verde water quality would deteriorate with annual average concentrations of total P and phytoplankton biomass (dry matter) reaching 34 mu g/L and 2 mg/L, respectively, after 10 years of simulation. The reduction of P loads (scenario PORAL) into the lake would improve water quality comparatively to the scenario control, reducing the annual average concentrations of total P from 34 mu g/L to 26 mu g/L and of phytoplankton from 2 mg/L down to 1.4 mg/L after 10 years of simulation. in scenario sediments, corresponding to a decrease in the organic content of the sediments, a reduction in the concentrations of total P and phytoplankton is expected in the first two years of simulation, but this effect, would be attenuated throughout the years due to organic matter sedimentation. The best strategy is obtained by combining external and internal measures for P remediation. Finally, it is recommended that the model be used to integrate the results of water quality monitoring and watershed management plans. (c) 2008 Elsevier Ltd. All rights reserved.",2008,10.1016/j.apgeochem.2008.03.001,no
Investigation of the effect of ultrasonic waves on the enhancement of efficiency of direct photolysis and photooxidation processes on the removal of a model contaminant from textile industry,"The effect of ultrasonic waves (US) was studied on the degradation of Malachite Green (MG) as a model contaminant from textile industry by direct photolysis with ultraviolet (UV) radiation and UV/H(2)O(2) processes. The US (35 kHz) and UV (253.7 nm) radiations were carried out with an ultrasonic bath and a 15 W low-pressure mercury lamp, respectively. Degradation of MG follows pseudo-first order kinetics in all cases. The apparent reaction rate constant (k(ap)) for UV/US process is greater than sum of the UV and US processes but it is relatively low for practical uses. However UV/H(2)O(2) treatment more efficiently decomposes this organic contaminant and combining this process with US can improve its efficiency. kap is influenced by variation of operational parameters such as power density, temperature, initial concentration of MG and H(2)O(2) for US/UV/H(2)O(2) process and activation energy was 9 kJ mol(-1) in the range of 294-307 K for this process. UV-vis spectral change of MG showed hypsochromic shift occurred with increasing sonication time, suggested N-demethylation process of MG.",2008,,no
Layout optimization based on a generalized process variability model - art. no. 69250F,"The growing impact of process variation on circuit performance requires statistical design approaches in which circuits are designed and optimized subject to an estimated variation. Previous work [1] has explicitly accounted for variation and spatial correlations by including extra margins in each of the gate delay and correlation factor between path delays. However, as it is recently shown, what is often referred to as ""spatial correlation"" is an artifact of un-modeled residuals after the decomposition of deterministic variation components across the wafer and across the die [2]. Consequently, a more accurate representation of process variability is to introduce these deterministic variability components in the model, and therefore generate any apparent spatial correlation as the artifact of those deterministic components, just like in the actual process. This approach is used to re-size an 8-bit Ladner-Fischer adder. The optimized circuit delay distribution is obtained from Monte Carlo simulations. A layout generation tool is also being constructed to incorporate the optimization procedure into the standard design flow. Custom circuit layouts are first subjected to design rules to extract constraints that specify the margins allowed for each transistor active area edge movement. Sizing optimization is then performed with design rule constraints taken into account. A new circuit layout is generated based on the optimization results and checked to ensure DRC cleanness. The optimized layout can be subjected to further verification such as hotspot detection to account for any additional layout dependant effects.",2008,10.1117/12.772882,no
Improving the hydraulic efficiency of water process tanks using CFD models,"The hydraulic efficiency of water process tanks can be significantly improved by performing simple modifications in the initial geometries of the tanks. This concept was applied in the main tanks of the water supply network of Athens, which are used for disinfection. Various modified geometries were formulated by ""inserting"" guiding walls in these tanks, a very common, simple and inexpensive modification. The hydraulic efficiency of the tanks was assessed with a CFD model, which calculated the flow fields and the flow through curves (FTCs) for the initial and the modified geometries. From the comparison of the results the following conclusions were drawn: (1) the flow for the initial geometries was characterized by high levels of short-circuiting, large re-circulation regions and high degrees of mixing, and (2) the use of guiding walls created flow fields in the tanks with significant volumes of ""plug flow"", reduced short-circuiting, smaller re-circulation regions and thus less mixing. The modified geometries showing the best hydraulic efficiency were proposed for construction. This procedure, which can be applied in the all types of water tanks, resulted in significant cost savings, because the modifications were examined with the CFD model, prior to their actual implementation in the real tanks. (c) 2007 Elsevier B.V. All rights reserved.",2008,10.1016/j.cep.2007.02.033,no
Diagnostic analyses of a regional air quality model: Changes in modeled processes affecting ozone and chemical-transport indicators from NOx point source emission reductions,"The impact of nitrogen oxide (NOx) emission reductions from major point sources on the key physical and chemical processes contributing to ozone formation and accumulation is studied, and the extent of change in the chemical regime is examined using selected photochemical indicators in the eastern United States. The Community Multiscale Air Quality (CMAQ) chemical-transport model, equipped with the process analysis technique, was applied in modeling scenarios involving 2002 base case emissions and an emissions scenario containing real-world point source NOx reductions implemented before the summer ozone season of 2004. Spatial patterns and temporal variations in process rates and changes in chemical-transport indicators are highlighted from results of summer 2002 days, representative of generally southwesterly wind flows across the Midwestern source region with ozone transport toward the northeastern states. Substantial decreases exceeding 50% in O-3 chemical production rates were associated with the largest NOx point source emission reductions, causing declines in ozone concentrations at the surface and aloft in downwind areas. The decreases in the various physical processes and their spatial difference patterns closely resembled the change in maximum O-3 concentrations. The net ozone production efficiency was found to increase, since the decline in O-3 concentrations was less than the decrease in reactive nitrogen products (NOz). The O-3/NOx ratio also increased between the base case and NOx reduction scenario results, indicating a noticeable shift in the chemical regime toward more NOx-limited conditions in plume-impacted areas downwind of the sources. The drop in surface NOx concentrations in modeled and observed results at a location just downwind of the Ohio River Valley source region is attributable to the point source NOx emission reductions.",2008,10.1029/2007JD009537,no
Intelligent Modeling and Optimization Method Based on Comprehensive Product Indices for Lead-Zinc Sintering Process,"The lead-zinc sintering process has the characters of complex mechanism, multi-parameter and strong interconnected coupling. This paper presents an intelligent modeling and optimization method based on comprehensive production Indices. First, the neural network prediction model of the comprehensive production indices is proposed, which is synthesizing some techniques, including gray correlation analysis, principal components analysis, and neural network and so on. And the target function is deduced and the model based on comprehensive production Indices is given by using the multi-objective optimization technique. At last, this paper proposes an improved multi-objective particle swarm optimization algorithm based on SPEA2 to calculate the optimization executing parameters. The applied results of actual runs show that the intelligent modeling and optimization method proposed in this paper attains a better industrial effect, and provides an effective and new way to implement the global optimization.",2008,10.1109/CHICC.2008.4605729,no
FLATNESS AND SHAPE DEFECTS CONTROL IN METAL STRIP USING THE 3-D FINITE ELEMENT MODELS OF THE TENSION-LEVELLING PROCESS,"The objective of the present work has been to develop knowledge of the complete three-dimensional material behaviour of steel products when subjected to bending and stretching operations. In order to fulfil those aims a 3-D tension and roller levelling finite element model has been built up to enable calculation of the effect of tension and roller positioning on material behaviour across the full width of the metal strip. Cylindrical and profiled rolls were modelled as analytical rigid surfaces and reduced integration shell elements with a number of integration points across the thickness were used to model the metal strip. The flat metal strip and a strip containing initial defects such as the edge wave have been modelled. The simulations have been made using the global model approach. Consequently, the metal strip was modelled as a number of slices across the width, incorporating submodelling techniques in order to obtain the results of the residual stresses and shape changes by analogy with slit bow tests. The 3-D ABAQUS/Standard model created in this work is able to represent a diverse range of levelling and similar processes.",2008,,no
Comprehensive Kinetic Model for the Degradation of Methyl tert-Butyl Ether by an Ozone/UV Process,"The paper presents a comprehensive kinetic model that describes the degradation of methyl tert-butyl ether (MTBE) in an ozone/UV process. First, the degradation pathways for MTBE were proposed on the basis of major reaction intermediates identified during the oxidation of MTBE. The yield for each major reaction intermediate was determined by fitting experimental data to the model prediction. Accordingly, 43, 20, 15, 11, and 6% of MTBE oxidized resulted in the generation of TBF, MMP, TBA, acetone, and methyl acetate, respectively. TBF oxidation resulted in the generation of HiBA, acetone, and TBA as primary intermediates at 47, 24, and 20%, respectively. During the oxidation of TBA, 66% resulted in the generation of HiBA and 34% in acetone. The kinetic model was verified using different sets of experimental data by varying the initial concentration of MTBE, influent ozone gas concentration, and incident UV light intensity. The model predicted well the degradation of MTBE by an ozone/UV process. In addition, the model predicted the accumulation and decay of primary intermediates (TBF, TBA, MMP, methyl acetate, and acetone) with slight variations.",2008,10.1021/ie800721t,no
"CAD FOR OPTIMAL SCALING OF THE 3D MODEL, TO COMPENSATE THE SLS POST-PROCESSING ERRORS","The paper presents a new, CAD method and a new software package to calculate better scaling factors for the Selective Laser Sintering (SLS) technology. The errors during the SLS process and during post processing are analysed FEA was used to estimate the 3D deformations during post processing in the oven the SLS metal parts, in order to infiltrate them to decrease their porosity and improve their mechanical properties.",2008,,no
Research on Cooperative Design Process Modeling and Evaluation Method Oriented MAS,"The paper proposed a new thought that a multi-agent organization model was designed, which based on cooperative design system network framework and workflow. The system workflow and system architecture were presented. The Ant Colony Optimization (ACO) method which was improved by Bayes was applied to the partner selection. And the Principal Component Analysis (PCA) method was used to evaluate the design method of design tasks. The collaborative process of negotiation mechanism was discussed. According to the theory analysis, software SPSS was used to deal with the sample data. On the basis of the theory a network of collaborative design prototype system was developed which confirms the system is reliability, practicality and efficiency.",2008,10.1109/ISCID.2008.102,no
A reduced efficiency approach-based process model for a circulating air classifier,"The performance of a circulating air classifier has been studied systematically by conducting experiments over a range of process and design variables. It is found that while the overall induced flow rate is proportional to the speed of rotation of the wheel, the circulation pattern inside the classifier depends on the configuration of stationary guide vanes. This is found to have a significant effect on the range of operability of the classifier. Results show that the cut size, sharpness of separation and the bottom and top size selectivity increments are influenced strongly by the stationary guide vane configuration. Using experimental data from a dense material (fly ash) and a light material (rice husk), a model based on the reduced efficiency curve approach, originally proposed for hydrocyclones, has been developed to predict the performance of the classifier. (c) 2007 Published by Elsevier B.V.",2008,10.1016/j.cep.2007.10.016,no
Evaluation of real-time PM2.5 forecasts and process analysis for PM2.5 formation over the eastern United States using the Eta-CMAQ forecast model during the 2004 ICARTT study,"The performance of the Eta-Community Multiscale Air Quality (CMAQ) modeling system in forecasting PM2.5 and chemical species is assessed over the eastern United States with the observations obtained by aircraft (NOAA P-3 and NASA DC-8) and four surface monitoring networks (AIRNOW, IMPROVE, CASTNet and STN) during the 2004 International Consortium for Atmospheric Research on Transport and Transformation (ICARTT) study. The results of the statistical analysis at the AIRNOW sites show that the model was able to reproduce the day-to-day and spatial variations of observed PM2.5 and captured a majority (73%) of PM2.5 observations within a factor of 2, with normalized mean bias of -21%. The consistent underestimations in regional PM2.5 forecast at other networks (IMPROVE and STN) were mainly due to the underestimation of total carbonaceous aerosols at both urban and rural sites. The significant underestimation of the ""other'' category, which predominantly is composed of primary emitted trace elements in the current model configuration, is also one of the reasons leading to the underestimation of PM2.5 at rural sites. The systematic overestimations of SO42- both at the surface sites and aloft, in part, suggest too much SO2 cloud oxidation due to the overestimation of SO2 and H2O2 in the model. The underestimation of NH4+ at the rural sites and aloft may be attributed to the exclusion of some sources of NH3 in the emission inventory. The systematic underestimations of NO3- may result from the general overestimations of SO42-. Note that there are compensating errors among the underestimation of PM2.5 species (such as total carbonaceous aerosols) and overestimation of PM2.5 species (such as SO42-), leading to generally better performance of PM2.5 mass. The systematic underestimation of biogenic isoprene (by similar to 30%) and terpene (by a factor of 4) suggests that their biogenic emissions may have been biased low, whereas the consistent overestimations of toluene by the model under the different conditions suggest that its anthropogenic emissions might be too high. The contributions of various physical and chemical processes governing the distribution of PM2.5 during this period are investigated through detailed analysis of model process budgets using the integrated process rate (IPR) analysis along back trajectories at five selected locations in Pennsylvania and Georgia. The results show that the dominant processes for PM2.5 formation and removal vary from the site to site, indicating significant spatial variability.",2008,10.1029/2007JD009226,no
Performance measurement and lumped parameter modeling of single server flow lines subject to blocking: An effective process time approach,"The present paper extends the so-called effective process time (EPT) approach to single server flow lines with finite buffers and blocking. The power of the EPT-approach is that it quantifies variability in workstation process times without the need to identify each of the contributing disturbances, and that it directly provides an algorithm for the actual computation of EPTs. It is shown that EPT-realizations can be simply obtained from arrival and departure times of lots, by using sample path equations. The measured EPTs can be used for bottleneck analysis and for lumped parameter modeling. Simulation experiments show that for lumped parameter modeling of flow lines with finite buffers, in addition to the mean and variance, offset is also a relevant parameter of the process time distribution. A case from the automotive industry illustrates the approach. (C) 2007 Elsevier Ltd. All rights reserved.",2008,10.1016/j.cie.2007.10.016,no
"Mathematical modelling of a composting process, and validation with experimental data","The present study aimed to develop a mathematical model of composting which, while not overlooking the fundamental principles of physical and microbiological chemistry, could be easily applied in practice and be validated by experimental data. The experimental results of the biological aerobic decomposition of a mixture consisting of rice and rice husks, could be explained in terms of the parameter aggregation model, assuming a set of pseudo-first-order reactions in series, in which a hydrolysis step is followed by a biochemical oxidative step with formation of compost, biomass and biological gases (CO(2), O(2)). The corresponding kinetic parameters and their temperature dependence were determined. These parameters indicated that the hydrolysis step was always the slowest one, and, therefore, the overall rate-determining step. This is in substantial agreement with our experimental observations of a non-dependency of the overall rate on the oxygen concentration, and suggests that rather than using mesophilic and thermophilic bacteria and fungi for seeding or accelerating the process, adequate hydrolytic enzymes (or related micro-organisms) should be added, instead.",2008,10.1177/0734242X07086514,no
Ten steps applied to development and evaluation of process-based biogeochemical models of estuaries,"The procedures involved in model development may be set out as a ten step process, beginning with defining the purpose of the model and ending with evaluation of the appropriateness and utility of the completed model. This process, recently outlined by Jakeman et al. [Jakeman, A.J., Letcher, R.A., Norton, J.P., 2006. Ten iterative steps in development and evaluation of environmental models. Environmental Modelling and Software 21, 602-614], is often iterative as model development is a continuous process that refines and improves the intended capacity of the model. Here, the ten steps of model development are critiqued and applied using a process-based biogeochemical model of aquatic systems, with examples from two case studies: a model of phytoplankton succession and nutrient concentrations in the Swan-Canning Estuary (Western Australia) and a model of sediment and nutrient transport and transformation in the Fitzroy Estuary and Keppel Bay (Queensland). Crown Copyright (C) 2007 Published by Elsevier Ltd. All rights reserved.",2008,10.1016/j.envsoft.2007.05.019,no
A comprehensive resist model for the prediction of line-edge roughness material and process dependencies in optical lithography - art. no. 69230R,"The reduction of semiconductor device dimensions necessitates, amongst others, a reduction of the line-edge roughness (LER) of the lithographically patterned device components. Experimentally, the impact of many process and material parameters on resist LER has been demonstrated. The impact of some parameters on LER has also been described quantitatively. This paper presents a mesoscopic (i.e., discrete and stochastic) modeling approach including all exposure, post-exposure bake (PEB), and development related parameters and their impact on LER. This allows a prediction of the resulting resist profiles including average dimensions as well as LER. The mesoscopic models are applied for simulating the impact of aerial image contrast, acid diffusion length, and quencher base concentration on LER. The results are compared to experimental data. After this validation of the models, they are applied for LER optimization. The optimum combination of acid and base diffusion length is identified for resist formulations with various levels of base concentration. While the impact of acid diffusion length is already known in principle, it is shown in this paper for the first time how the optimum acid diffusion length depends crucially on base diffusion length and initial base concentration of the resist.",2008,10.1117/12.772507,no
Semantically-Enhanced Model-Experiment-Evaluation Processes (SeMEEPs) within the Atmospheric Chemistry Community,"The scientific model development process is often documented in an ad-hoc unstructured manner leading to difficulty in attributing provenance to data products. This call cause issues when the data owner or other interested stakeholder seeks to interpret the data at a later date. In this paper we discuss the design, development and evaluation of a Semantically-enhanced Electronic Lab-Notebook to facilitate the capture of provenance for the model development process, within the atmospheric chemistry community. We then proceed to consider the value of semantically enhanced provenance within the wider community processes, Semantically-enhanced Model-Experiment Evaluation Processes (SeMEEPs), that leverage data generated by experiments and computational models to conduct evaluations.",2008,,no
Process Model for Composing High-quality Text Corpora,"The Teko corpus composing model offers a decentralized, dynamic way of collecting high-quality text corpora for linguistic research. The resulting corpus consists of independent text sets. The sets are composed in cooperation with linguistic research projects, so each of them responds to a specific research need. The corpora are morphologically annotated and XML-based, with in-built compatibilty with the Kaino user interface used in the corpus server of the Research Institute for the Languages of Finland. Furthermore, software for extracting standard quantitative reports from the text sets has been created during the project. The paper describes the project, and estimates its benefits and problems. It also gives an overview of the technical qualities of the corpora and corpus interface connected to the Teko project.",2008,,no
Regression models with process variables and parallel blocks of raw material measurements,"The topic of this paper is regression models based on designed experiments, where additional spectroscopic measurements are also available. This particular case describes a situation with two spectral blocks with no natural order: The blocks are parallel. Three methods are described, which combine least squares regression of the design variables with PCA or PLS on the spectra. The methods properties are explored in two simulation studies based on real experiments. The results show that the methods are equal when it comes to prediction, but interpretability varies. One of the methods, LS-ParPLS, is especially interesting when it comes to interpretability because it splits the spectral information into two parts; information that is common in both blocks and information that is unique for each block. Copyright (C) 2008 John Wiley & Sons, Ltd.",2008,10.1002/cem.1169,no
Development of a model for high precursor conversion efficiency pulsed-pressure chemical vapor deposition (PP-CVD) processing,"The unsteady pulsed-pressure chemical vapour deposition (PP-CVD) technique offers an increase in process intensification over conventional CVD processes due to the high precursor utilisation efficiency. A numerical model of the movement of precursor particles in the process is developed to study the high efficiencies observed experimentally in this process. The modelling procedures were verified via a study of velocity persistence in an equilibrium gas and through direct simulation Monte Carlo (DSMC) modelling of unsteady self-diffusion processes. The results demonstrate that in the PP-CVD process the arrival time for precursor particles at the deposition surface is much less than the reactor pump-down time, resulting in high precursor conversion efficiencies. Higher conversion efficiency was found to correlate with smaller size carrier gas molecules and moderate reactor peak pressure. (c) 2007 Elsevier B.V. All rights reserved.",2008,10.1016/j.cej.2007.03.027,no
Modeling and analysis of machinability evaluation in the wire electrical discharge machining (WEDM) process of aluminum oxide-based ceramic,"The wire electrical discharge machining (WEDM) allowed success in the manufacture of the hard, fragile, and materials difficult to cut, especially for electroconductive ceramic materials. In this study, the mathematical models of material removal rate (MRR) and surface roughness (SR) used for the machinability evaluation in the WEDM process of aluminum oxide-based ceramic material (Al2O3 + TiC) have been carried out. The experimental plan adopts the face centered central composite design (CCD). The mathematical models using the response surface methodology (RSM) are developed so as to investigate the influences of four machining parameters, including the peak current, pulse on time, duty factor, and wire speed, on the performance characteristics of MRR and SR. It has been proved that the proposed mathematical models in this study would fit and predict values of the performance characteristics, which would be close to the readings recorded in experiment with a 95% confidence level. The significant parameters that critically affect the performance characteristics are examined.",2008,10.1080/10426910701860616,no
A nonlinear programming model for refinery planning and optimisation with rigorous process models and product quality specifications,"The yield of products in large-scale plants such as oil refineries have a significant impact on overall profitability. Currently, many refineries apply Linear Programming (LP) techniques for their production planning models. However, this will often give inconsistent predictions of refinery productivity and operation. Moreover, the stringent environmental regulations, product qualities, and heavier feed stocks make it necessary to develop accurate models for refinery-production planning. In this work, an approach with more accurate representation of the refinery processes is presented. The resulting model is able to predict the operating variables such as the Crude Distillation Unit (CDU) cut-point temperatures and the conversion of the Fluid Catalytic Cracking unit (FCC). It can also evaluate properties of the final products to meet the market specification as well as the required product demands to achieve a maximum refinery profit. The model is illustrated on representative case studies, and the results are discussed. [Received: December 4, 2007; Accepted: January 17, 2008]",2008,10.1504/IJOGCT.2008.019846,no
Complexity Threshold in RNA-World: Computational Modeling of Criticality in Galton-Watson Process,"Theory of RNA-World describes the early stage of the evolution of Life and assumes the existence of RNA-species in which RNA constitutes genetic material and enzymes. The RNA enzymes, called rybozymes, are required for metabolism and for self-replication with low error-rate. In particular one hypothetical rybozyme called RNA replicase is crucial in the early phase of RNA-World, since it can reduce the mutation rate and thus allow for development of longer genomes. Yet the length of RNA replicase itself is limited, because in the phase of evolution proceeding the ""invention"" of this rybozyme the replication could not take the advantage of the low mutation rate. Therefore, RNA replicase would have never been able to evolve if its function appeared only in long RNA chains. In the paper there is considered the problem by application of Demetrius/Kimmel model of criticality in branching processes. The novelty of the paper is the introduction into the mentioned model the parameters which can be experimentally measured in a test tube. Therefore the estimations of the complexity threshold of the early RNA-based RNA replicase can be determined using data from biochemical experiments.",2008,,no
A model-guided process evaluation: Office-based prescribing and pharmacy dispensing of methadone,"This article presents an exemplar of a model-guided process evaluation that specifies the treatment model, assesses its implementation, monitors the fidelity of the model throughout the project, assesses model exposure and absorption, and helps understand the program's intermediate effects (proximal outcomes) as well as final effects (distal outcomes). The New Mexico study on office-based prescribing and community pharmacy dispensing of methadone is a research demonstration project that phases a small group of female methadone maintenance patients out of methadone clinics and into a program where they will obtain their scheduled doses of methadone at pharmacies that work in collaboration with physicians and a social worker. The patient's methadone treatment will in this way become part of their overall health care. Early detection of problems of implementation (e.g., the omission of program content or the delivery of inaccurate information) enables the researcher to make adjustments before the problems become unmanageable and the integrity of the original research design is compromised. A model-guided process evaluation can critically inform health services research demonstrations designed for enabling continuous, ongoing feedback and improvement of client-related services. (C) 2008 Elsevier Ltd. All rights reserved.",2008,10.1016/j.evalprogplan.2008.04.011,no
Evaluation of professional development: Deploying a process-focused model,"This evaluation used a change transition model to explore the processes of development of a three-phase professional programme devised by two teams of researchers to support teachers' expertise in six domains of science teaching. The full programme operated over two years. Interviews with developers at the end of each phase (21 interviews) and with teachers at the end of phases two and three (11 interviews) formed the main data set. The four features of the change transition model - trigger, vision, conversion, maintenance - were used as a framework for analysis of the qualitative data. Four themes emerged as contributing to the success of the process of development of the programme: establishing a shared vision of the goals of the programme and its outcomes; maintaining flexibility in implementing the phases and details of the programme; negotiating common understanding with participants; and ensuring fruitful collaboration in planning and implementation. The demands of attending to all of these features should not be underestimated in any successful developmental process. The evaluation therefore provides evidence for additional guidance in future collaborative professional development.",2008,10.1080/09500690701854899,no
On modeling post decryption error processes in UMTS air interface,"This paper analyzes the impact, of encryption over the UNITS air interface. Using a finite state Markov characterization of the UNITS decryption process, a stochastic model has been developed that quantifies the impact of bit errors in the ciphertext and cipher synchronization counter. The effects of residual bit errors, UNITS air interface power budget, interleaving span and channel coding rate on the decryption process are analyzed.",2008,,no
Combination of Analytic Network Process and Bayesian Network model for multi-criteria engineering decision problems,This paper deals with Multi Criteria Decision Analysis fit an engineering area (info-communication networks) be combining two effective mathematical modelling tools: Analytic Hierarchy Process and Bayesian Network. An extension of Bayesian Network is introduced. this Vector Bayesian Network is able to calculate the effects of the trade faults in info-communication networks on different errors occurred at end users. We have developed a method combining expert knowledge and probabilistic theory to analyse fault spreading problem. A numerical example of network problem is worked out in order to certify our theoretical conception.,2008,,no
Viscous pull force evaluation in the pultrusion process by a finite element thermo-chemical rheological model,"This paper deals with the development of computational model to investigate a conventional pultrusion process. The model takes into account several aspects related to heat and mass transfer, crystallization kinetics, and rheological behavior of the considered processing material and allows to evaluate the viscous pull force acting between the internal die walls and the processing material. A finite element scheme has been used to solve the formulated model using suitable boundary conditions.",2008,10.1007/s12289-008-0264-0,no
Towards a Software Process for Aspect-Oriented Modeling of Quality Attributes,"This paper defines a process for the aspect-oriented modeling of quality attributes, especially those with high functional implications. The goal of this process is to produce ""built-in"" reusable and parameterizable architectural solutions for each quality attribute. We propose using the AO-ADL Tool Suite to specify and store these solutions.",2008,,no
Large volume metrology process models: A framework for integrating measurement with assembly planning,"This paper describes a generic methodology dealing with the theoretical definition of metrology process models and their systematic integration with design evaluation and assembly planning. The research resulted in the specification of a novel, theoretical framework for the specification and generation of metrology process models, especially focusing on large volume, frameless metrology that is suitable for the verification of large and complex products. The process modelling framework has four generic sections that support early design evaluation, assembly process interface, set-up and deployment, and verification data management. Initial testing results, using aerospace products, demonstrated the effectiveness of the process modelling methods. (C) 2008 CIRP.",2008,10.1016/j.cirp.2008.03.017,no
COOPERATING MODELLING METHODS FOR PERFORMANCE EVALUATION OF INTERCONNECTED INFOCOMMUNICATION AND BUSINESS PROCESS SYSTEMS,"This paper describes how the rapid and detailed modelling and simulation methods can be used to increase the efficiency of mixed simulation projects initiated to support the design of interconnected ICT (information and Communication Technology) and BP (Business Process) systems. A system of cooperating rapid and detailed methods for critical and non-critical parts of systems is introduced in the phase of preliminary and detailed modelling. The method of translation of information of conceptual models that had been built prior to simulation is described, too. New methods using rapid models to model the ICT and BP systems functioning as environment for the detailed models are presented. A novel method for preliminary modelling based only on cooperating system of rapid models is described.",2008,,no
PSYCHIC - A process-based model of phosphorus and sediment transfers within agricultural catchments. Part 2. A preliminary evaluation,"This paper describes the preliminary evaluation of the PSYCHIC catchment scale (Tier 1) model for predicting the mobilisation and delivery of phosphorus (P) and suspended sediment (SS) in the Hampshire Avon (1715 km(2)) and Herefordshire Wye (4017 km(2)) drainage basins, in the UK, using empirical data. Phosphorus and SS transfers to watercourses in the Wye were predicted to be greater than corresponding delivery in the Avon; SS, 249 vs; 33 kg ha(-1) yr(-1); DID, 2.57 vs 1.26 kg ha(-1) yr(-1); PP, 2.20 vs 0.56 kg ha(-1) yr(-1). The spatial pattern of the predicted transfers was relatively uniform across the Wye drainage basin, whilst in the Avon, delivery to watercourses was largely confined to the river corridors and small areas of drained land. Statistical performance in relation to predicted exports of P and SS, using criteria for relative error (RE) and root mean square error (RMSE), reflected the potential shortcomings associated with using Longer-term climate data for predicting shorter-term (2002-2004) catchment response and the need to refine calculations of point source contributions and to incorporate additional river basin processes such as channel bank erosion and in-stream geochemical processing. PSYCHIC is therefore best suited to characterising Longer-term catchment response. (C) 2007 Elsevier B.V. All rights reserved.",2008,10.1016/j.jhydrol.2007.10.044,no
Automation Model to Increase Efficiency in the Supply Chain Management Processes,"This paper discloses and discusses the result of a poll conducted with thirty-eight (38) companies out of the five hundred (500) top revenue-earning businesses in Brazil - intended to demonstrate the benefits, satisfaction and degree of automation after implementation of the EDI within the sales, logistics and financial areas of such companies. Some information such as, e.g., the sales increase and decrease of product shortage on shelves (rack break), among other results, are also emphasized since, according to the research, such indicators improved through the utilization of the EDI. Relying on this inquiry and by introducing the appropriate adaptations and based on the considerations herein, one should be able to propose improvements to similar companies.",2008,,no
A geometric programming model of the lot-scheduling problem with investments in setup reductions and process improvements,"This paper discusses a general nonlinear optimization model for the problem of investing in reduced setup times and process improvements for a manufacturer of several products operating in a JIT environment. Demand for each product occurs at a constant rate. The problem is constrained by limitations on the process improvement and setup reduction budgets, and on the manufacturing and warehousing capacities. The objective is to determine optimal levels of setup time and process improvements along with the corresponding amounts of investments respectively, and the optimal production cycle time for each product. A convex geometric programming model of this problem is developed in order to solve it. In order to deal with numerical difficulties that may be encountered in practice, a dual reformulation of the model is discussed.",2008,,no
Error localization in an FE model in model updating process using supermodels,"This paper introduces a novel method for error localization of a design model using the information from a reference model or a supermodel in the model updating process. The concept of Equivalent Element Modal Strain Energy (EEMSE) and Equivalent Element Modal Kinetic Energy (EEMKE) are discussed and two indicators, i.e. the indicator of element errors in stiffness based on EEMSE and the indicator of element errors in mass based on EEMKE, are developed to identify element errors in the design model. Investigation shows that the indicators also provide information about how much the error in the elements should be corrected in the updating process. A beam structure is used as an example to demonstrate the effectiveness of the method. Various types of the beam model with meshing errors or combinations of meshing and parameter errors, together with the refined model which is treated as a supermodel to provide reference data, have been simulated and further compared with the distributions of element strain energy and kinetic energy. The Equivalent Element Modal Strain Energy (EEMSE) and Equivalent Element Modal Kinetic Energy (EEMKE) were used to identify clearly various errors in the beam model. In addition, the indicators of EEMKE and EEMSE were employed to localise the simulated errors in the mass and stiffness in some elements in the models. This method shows potential for the industrial application in the effective and efficient validation of a design model in the industrial design process.",2008,,no
On modeling of uncertainty measures and observed processes,"This paper is about a short survey of some basic uncertainty measures in systems analysis arising from coarse data, together with new modeling results on upper semi.continuous random processes, viewed as random fuzzy sets. Specifically, we present the most general mathematical framework for analyzing coarse data, such as random fuzzy data, which arise often in information systems. Our approach is based upon the theory of continuous lattices. This probabilistic analysis is also useful for investigating upper semicontinuous random functions in stochastic optimization problems.",2008,,no
Estimation of springback in double-curvature forming of plates: Experimental evaluation and process modeling,"This paper presents the results obtained from a series of experiments on double-curvature forming of 300 turn square and 15 turn thick plates of type 316L(N) stainless steel to evaluate the inherent springback and also to validate finite element method (FEM) based process model developed for forming of multiple-curvature sectors of large size vessels. The experimental results show that twisting of the plate occurs during pressing, which is unavoidable in an actual forming setup on the shop floor. Twisting increases with increase in slope of the die cavity. Springback in the plate changes in an ascending order towards the centerline of the plate from the edges. The final radius of curvature (ROC) on the pressed plate after springback does not remain constant along a particular axis although the die and the punch had constant ROC along that axis because of varying constraint to opening up of the plate from centerlines to the edges. Springback also increases with reduction in the stiffness of the die and punch. The simulated plate profiles obtained from the FEM process model for multiple-curvature plate forming compared well with the experiments, the maximum error being within 6%. The process model used a sequential dynamic explicit formulation for the plate pressing phase and a static implicit formulation for the unloading (springback) phase in the Lagrangian framework. Reduced integration shell elements were used for the plate and the die and the punch were considered rigid. Dynamic explicit FEM for pressing and static implicit FEM for the unloading phase are adequate and economic for modeling of plate forming process by using FEM. The necessary material and frictional property data needed for the FEM process model were generated in-house. This model can be applied to design of dies and punches for forming the petals of large pressure vessels. The FEM process model predicts the final shape of the product and the residual cold work level for a given die, punch and plate configuration and this information can be used to correct the die and punch shapes for springback to manufacture the petals to the desired accuracy. (c) 2007 Elsevier Ltd. All rights reserved.",2008,10.1016/j.ijmecsci.2007.12.002,no
"Guided team self-correction - Impacts on team mental models, processes, and effectiveness","This research investigated the effects of guided team self-correction using an empirically derived expert model of teamwork as the organizing framework. First, the authors describe the process used to define this model. Second, they report findings from two studies in which the expert model was used to structure the process of guided team self-correction. Participants were U.S. Navy command and control teams (25 in Study 1, 13 in Study 2). Results indicated that teams debriefed using the expert model-driven guided team self-correction approach developed more accurate mental models of teamwork (Study 1) and demonstrated greater teamwork processes and more effective outcomes (Study 2) than did teams debriefed using a less participative and chronologically organized approach that is more typical for these teams.",2008,10.1177/1046496408317794,no
RFID in mixed-model automotive assembly operations: Process and quality cost savings,"This research shows how to characterize the potential operational benefits of RFID in a complex automotive assembly system. Both process savings and quality (rework) savings are included in the presented model and it is demonstrated that process savings and quality cost savings are strongly interdependent. Indeed, a trade-off exists in which the decision maker has to decide whether to allocate time savings afforded by RFID to process time savings, or to allocate those time savings to potential quality cost savings. It is shown how this allocation decision can be solved such that the maximum expected cost savings are achieved. From the structural properties of this optimal solution, it is concluded that an RFID implementation will tend to yield the most benefit in a fast-paced complex assembly environment where: (i) total assembly times are substantial; (ii) tact times are low; (iii) there is little to no worker idle time; and (iv) rework is present and costly. How to evaluate the impact of an RFID implementation in terms of the net present value of cost savings is demonstrated. The model framework is applied in a numerical example to study a situation at a car manufacturer and the main results are reported.",2008,10.1080/07408170802167654,no
Assessing asymmetric response effect of behavioral intention to service quality in an integrated psychological decision-making process model of intercity bus passengers: a case of Taiwan,"This study introduces the concept of loss aversion to consumer behavioral intention at the personal psychological level to develop an integrative structural equation model for analyzing traveler psychological decision making. In this model, the relationship between behavioral intention and service quality is a non-smooth function based on the theory of loss aversion. The expectation service quality in the SERVQUAL model proposed by Parasuraman, Zeithaml, and Berry (PZB) serves as a reference point. This model can be applied to analyze the effect of non-smooth response of behavioral intention to service quality in a traveler psychological decision-making process model. Intercity travel among cities in Taiwan is used as an empirical example. Data were gathered in cities in Taiwan via a questionnaire survey, and the model was tested using path analysis performed by LISREL. The empirical result shows that all causal relationships are statistically significant. Service quality loss influences repurchase intention more than does Service quality gain. Finally, this study concludes by discussing managerial implications and suggesting directions for future research.",2008,10.1007/s11116-007-9139-3,no
A mathematical programming framework for optimal model selection/validation of process data,"This work considers the use of information indices for optimal model selection and validation of process data. The approach followed assumes the existence of a set of fundamental process models associated with possible, although distinct, operating regions. A 2-phase mathematical programming algorithm for the assessment of structural changes and optimal fitting of local models in data series is proposed. This approach is used to determine the kinetic parameters of the gelation reaction of chitosan with genipin, employing dynamical elastic modulus data.",2008,,no
Lyapunov-based model predictive control of particulate processes subject to asynchronous measurements,"This work focuses on state feedback model predictive control of particulate processes subject to asynchronous measurements. A population balance model of a typical continuous crystallizer is taken as an application example. Three controllers, a standard model predictive controller and two recently proposed Lyapunov-based model predictive controllers, are applied to stabilize the crystallizer at an open-loop unstable steady-state in the presence of asynchronous measurements. The stability and robustness properties of the closed-loop system under the three controllers are compared extensively under two different assumptions on how the measurements from the crystallizer are obtained.",2008,,no
Lyapunov-based Model Predictive Control of Particulate Processes Subject to Asynchronous Measurements,"This work focuses on state feedback, model predictive control of particulate processes subject to asynchronous measurements. A population balance model of a typical continuous crystallizer is taken as an application example. Three controllers, i.e., a standard model predictive controller and two recently proposed Lyapunov-based model predictive controllers, are applied to stabilize the crystallizer at an open-loop, unstable steady-state in the presence of asynchronous measurements. The stability and robustness properties of the closed-loop system under the three predictive controllers are compared extensively under three different assumptions on how the measurements from the crystallizer are obtained.",2008,10.1002/ppsc.200800030,no
Organization Application Oriented Software Process Measurement Model,"To meet the organization process measurement need, this paper presented an integrated measurement model for the software process management, which establishes the relationship between organization unit, role definition and the GDM analysis result of business goal. This model supports such process measurement activities as the data collection, data analysis and communication of analysis result. When the business goal is changed, the data collection and the data analysis could be changed accordingly. Based on the integrated process model, an integrated software process measurement platform has been developed, which supports the data management for quality, cost, delivery, process, people skill, etc. It provides the measurement view for a variety of role on different organization levels to facilitate the usage of analysis result.",2008,10.1109/ISCSCT.2008.280,no
An e-quality control model for multistage machining processes of workpieces,"To track and control the changes of process quality attributes in multistage machining processes (MMPs), an e-quality control (e-QC) model is proposed. The e-QC model is defined as a quality information service node with e-formalizing technology, whose input/output and intermediate process (that is IPO) are known to other nodes, and its implemention in MMPs is provided. In order to establish the e-QC model, a measuring network is constructed to acquire the original quality data, and the changes of process quality attributes are monitored and diagnosed by the integrated quality analysis tools attached to the e-QC, which can be tracked by information template network in real time. Furthermore, a hierarchical control method is adopted to coordinate e-QCs, in which the quality loss and adjusting cost are used to quantify the opportunities for e-QCs to improve process quality. At last, a prototype is developed to verify the proposed methods.",2008,10.1007/s11431-008-0240-4,no
"GEM-AQ, an on-line global multiscale chemical weather modelling system: model description and evaluation of gas phase chemistry processes","Tropospheric chemistry and air quality processes were implemented on-line in the Global Environmental Multiscale weather prediction model. The integrated model, GEM-AQ, was developed as a platform to investigate chemical weather at scales from global to urban. The current chemical mechanism is comprised of 50 gas-phase species, 116 chemical and 19 photolysis reactions, and is complemented by a sectional aerosol module with 5 aerosols types. All tracers are advected using the semi-Lagrangian scheme native to GEM. The vertical transport includes parameterized subgrid-scale turbulence and large scale deep convection. Dry deposition is included as a flux boundary condition in the vertical diffusion equation. Wet deposition of gas-phase species is treated in a simplified way, and only below-cloud scavenging is considered. The emissions used include yearly-averaged anthropogenic, and monthly-averaged biogenic, ocean, soil, and biomass burning emission fluxes, as well as NOx from lightning. In order to evaluate the ability to simulate seasonal variations and regional distributions of trace gases such as ozone, nitrogen dioxide and carbon monoxide, the model was run for a period of five years (2001-2005) on a global uniform 1.5 degrees x 1.5 degrees horizontal resolution domain and 28 hybrid levels extending up to 10 hPa. Model results were compared with observations from satellites, aircraft measurement campaigns and balloon sondes. We find that GEM-AQ is able to capture the spatial details of the chemical fields in the middle and lower troposphere. The modelled ozone consistently shows good agreement with observations, except over tropical oceans. The comparison of carbon monoxide and nitrogen dioxide with satellite measurements emphasizes the need for more accurate, year-specific emissions fluxes for biomass burning and anthropogenic sources. Other species also compare well with available observations.",2008,,no
A MODEL FOR DOCUMENT VALIDATION USING CONCURRENT AUTHENTICATION PROCESSES,"Under the trend toward c-business, the paper-based documents in enterprises are transformed into the electronic ones. In most real-world cases, the electronic documents are simply stored in an enterprise document repository. Owing to the high computation accuracy and efficiency, personal computers have been gradually used to assist domain experts in document authentication tasks (e.g., decision making or process control). Regarding document authentication process management, previous studies focus more on authentication processes without branches and the characteristic of concurrent engineering (CE) is almost ignored in the document authentication issues. In most enterprise knowledge management environment, knowledge validation and authentication is carried out through a collaboration network. Concerning the enterprise operation requirements, this research develops a heuristic model for concurrent authentication process determination based on the existing document authentication history. This research also develops a web-based document authentication information management system and performance of the prototype system is also evaluated via a demonstration case. In summary, by application of the proposed approach, the document authentication process can be automatically determined to support efficient knowledge validation.",2008,,no
Comprehensive dynamic model for BOF process: a glimpse into thermal efficiency mechanisms,"Understanding thermal transfer mechanisms in BOF converter is a key for higher energy efficiency, involving higher production capacity and lower CO(2) emissions through low hot metal ratio capability. The developed model achieves a coupled description of the major phenomena: refining reactions, mass transfers between phases, scrap melting, post-combustion, slag formation... The calculated exhaust fumes enthalpy is fitting well the measured thermal balance trials with industrial converter when implementing a detailed description of the post-combustion. This model, compatible with real-time use, is also a valuable tool for further improvement in dynamic control of the process.",2008,10.1051/metal:2008024,no
Evaluating Different Processes of Translating OCL to PVS Using Model Transformation,Various works have been done in verifying Object Constraint Language (OCL) by translating it to formal specification language. This paper presents the process of translating OCL to Prototype Verification System (PVS) using model transformation. PVS is a formal specification language based on Higher-Order Logic and comes with a theorem prover. Two processes have been identified to translate OCL to PVS. The first process translates OCL constraint to PVS specification directly using model-to-text transformation while the second process transforms OCL to PVS model. Both processes are evaluated to identify the advantages and disadvantages. Transforming OCL to PVS model allow the result to be use for other model management operation.,2008,,no
Computer completion of the process of modelling the layout of illumination of Wawel Hill in Cracow,Wawel has many features that render it extremely difficult to illuminate. Designing illumination with the use of field trials is practically impossible. Achieving an optimal effect required creation of a 3D model of the whole Hill and working out a computer program - supporting modelling of global luminance.,2008,,no
A Method for Verifiable and Validatable Business Process Modeling,"We define an extensible semantical framework for business process modeling notations. Since our definition starts from scratch, it helps to faithfully link the understanding of business processes by analysts and operators, on the process design and management side, by IT technologists and programmers; on the implementation side, and by users, on the application side. We illustrate the framework by a, high-level operational definition of the semantics of the BPMN standard of OMG The definition combines the visual appeal of the graph-based BPMN with the expressive power and simplicity of rule-based modeling and can be applied as well to other business process modeling notations, e.g. UML 2.0 activity diagrams.(1)",2008,,no
A credit evaluation and decision-making model based on analytic hierarchy process for banks in China,"We develop a credit evaluation and decision-making model based on analytic hierarchy process for the Chinese banks to measure a firm's credibility and a bank's profit expectation. The firm's credibility is evaluated by managerial analysis, financial analysis, credit record and guarantee analysis. We classify and place the relevant criteria, sub-criteria and measures in a hierarchical decision structure to calculate the overall credibility scores for the applicant firms. The bank's decision making score is obtained by synthesizing the firm's credibility score and the bank's profit expectation score. We also provide an example to illustrate our model.",2008,,no
Integration of process-based soil respiration models with whole-ecosystem CO2 measurements,"We integrated soil models with an established ecosystem process model (SIPNET, simplified photosynthesis and evapotranspiration model) to investigate the influence of soil processes on modelled values of soil CO2 fluxes (R (Soil)). Model parameters were determined from literature values and a data assimilation routine that used a 7-year record of the net ecosystem exchange of CO2 and environmental variables collected at a high-elevation subalpine forest (the Niwot Ridge AmeriFlux site). These soil models were subsequently evaluated in how they estimated the seasonal contribution of R (Soil) to total ecosystem respiration (TER) and the seasonal contribution of root respiration (R (Root)) to R (Soil). Additionally, these soil models were compared to data assimilation output of linear models of soil heterotrophic respiration. Explicit modelling of root dynamics led to better agreement with literature values of the contribution of R (Soil) to TER. Estimates of R (Soil)/TER when root dynamics were considered ranged from 0.3 to 0.6; without modelling root biomass dynamics these values were 0.1-0.3. Hence, we conclude that modelling of root biomass dynamics is critically important to model the R (Soil)/TER ratio correctly. When soil heterotrophic respiration was dependent on linear functions of temperature and moisture independent of soil carbon pool size, worse model-data fits were produced. Adding additional complexity to the soil pool marginally improved the model-data fit from the base model, but issues remained. The soil models were not successful in modelling R (Root)/R (Soil). This is partially attributable to estimated turnover parameters of soil carbon pools not agreeing with expected values from literature and being poorly constrained by the parameter estimation routine. We conclude that net ecosystem exchange of CO2 alone cannot constrain specific rhizospheric and microbial components of soil respiration. Reasons for this include inability of the data assimilation routine to constrain soil parameters using ecosystem CO2 flux measurements and not considering the effect of other resource limitations (for example, nitrogen) on the microbe biomass. Future data assimilation studies with these models should include ecosystem-scale measurements of R (Soil) in the parameter estimation routine and experimentally determine soil model parameters not constrained by the parameter estimation routine.",2008,10.1007/s10021-007-9120-1,no
Monitoring process quality in off-shore outsourcing: A model and findings from multi-country survey,"We investigate how recent advances in information technology and telecommunications have led to real-time monitoring of processes at the site of the provider by a buyer located across the globe. We construct a game-theoretic model of the dynamics of the buyer-supplier interaction in the presence of moral hazard and incomplete contracting. We derive the Minimum Quality Threshold (MQT) below which the provider's output will certainly be inspected. Our findings show that the buyer can pick a level of monitoring and thereby force the provider to exceed the quality level of the MQT in output quality and avoid costly and wasteful inspection. Finally, our model explains why the production of processes that are complex and more prone to errors are actually monitored less by the buyers. We furnish the results of a comprehensive, multi-year, multi-country survey of the efficacy of monitoring in off-shore outsourcing projects and demonstrate strong empirical support for the findings of the model. (C) 2007 Published by Elsevier B.V.",2008,10.1016/j.jom.2007.02.014,no
Analog VLSI layout design and the circuit board manufacturing of advanced image processing for artificial vision model,"We propose herein an artificial vision model for the motion detection which uses analog electronic circuits and design the analog VLSI layout. We also have manufactured the circuit board. The proposed model is comprised of four layers. The model was shown to be capable of detecting a movement object. The number of elements in the model, is reduced in its' realization using the integrated devices. The proposed model is robust with respect to fault tolerance. Moreover, the connection of this model is between adjacent elements, making hardware implementation easy.",2008,,no
On a free boundary problem modelling inductive-heating processes,"We study a free boundary problem describing a melting process by using induction heating. The mathematical model in one space dimension consists of a coupled parabolic system in each phase along with a nonequilibrium kinetic condition on the interface. By applying an energy estimate and Campanato type estimates, it is shown that the problem has a unique classical solution globally.",2008,,no
Evaluation of a method for fitting a semi-Markov process model in the presence of left-censored spells using the Cardiovascular Health Study,"We used a longitudinal data set covering 13 years from the Cardiovascular Health Study to evaluate the properties of a recently developed approach to deal with left censoring that fits a semi-Markov process (SMP) model by using an analog to the stochastic EM algorithm-the SMP-EM approach. It appears that the SMP-EM approach gives estimates of duration-dependent probabilities of health changes similar to those obtained by using SNIP models that have the advantage of actual duration data. SMP-EM estimates of duration-dependent transition probabilities also appear more accurate and less variable than multi-state life table estimates. Published in 2008 by John Wiley & Sons, Ltd.",2008,10.1002/sim.3382,no
Stimulus complexity and prospective timing: Clues for a parallel process model of time perception,"Whereas many studies have considered the role of attention in prospective timing, fewer have established relations between movement complexity and prospective timing. The present study aims at assessing to what extent motion complexity interferes with prospective timing and at delineating a neuropsychophysical plausible model. We have thus designed a visual paradigm presenting stimuli in sequential pairs (reference comparison interval). Stimuli are motionless or moving according to different complexities, and stimulus complexities are intermixed within each pair. To prevent a possible attention-sharing effect, no concurrent task was required. Our study suggests that movement complexity is a key component of duration perception, and that the relative judgement of durations depends on spatio-temporal features of stimuli. In particular, it shows that movement complexity can bias subjects' perception and performance, and that subjects detect that comparison intervals are longer than reference before their end. In the discussion, we advocate that the classical internal clock,model cannot easily account for our results. Consequently, we propose a model for time perception, based on a parallel processing between comparison interval perception and the reconstruction of the reference duration. (C) 2007 Elsevier B.V. All rights reserved.",2008,10.1016/j.actpsy.2007.09.011,no
A genetic model for understanding higher order visual processing: Functional interactions of the ventral visual stream in Williams syndrome,"Williams syndrome (WS) is a rare neurodevelopmental disorder caused by a 1.6 Mb microdeletion on chromosome 7q11.23 and characterized by hypersocial personality and prominent visuospatial construction impairments. Previous WS studies have identified functional and structural abnormalities in the hippocampal formation, prefrontal regions crucial for amygdala regulation and social cognition, and the dorsal visual stream, notably the intraparietal sulcus (IPS). Although aberrant ventral stream activation has not been found in WS, object-related visual information that is processed in the ventral stream is a critical source of input into these abnormal regions. The present study, therefore, examined neural interactions of ventral stream areas in WS. Using a passive face- and house-viewing paradigm, activation and functional connectivity of stimulus-selective regions in fusiform and parahippocampal gyri, respectively, were investigated. During house viewing, significant activation differences were observed between participants with WS and a matched control group in IPS. Abnormal functional connectivity was found between parahippocampal gyrus and parietal cortex and between fusiform gyrus and a network of brain regions including amygdala and portions of prefrontal cortex. These results indicate that abnormal upstream visual object processing may contribute to the complex cognitive/behavioral phenotype in WS and provide a systems-level characterization of genetically mediated abnormalities of neural interactions.",2008,10.1093/cercor/bhn004,no
Performance Evaluation of XPath Form-based ontology Storage Model Regarding Query Processing and Ontology Update,"With rapid growth of Internet, the amount of information in the Web is increasing exponentially. However, information on the current Web is understandable only for human, and thus it makes the exact information retrieval difficult. For solving this problem, the Semantic Web is suggested and we must use ontology languages that can endow data to semantics for implementing it. For effective management of information in Semantic Web environment, we already suggested the XPath form-based OWL Storage (XPOS) model including hierarchy information between classes or properties as XPath form and enabling intuitive and effective information retrieval. In this paper, we focus on the quantitative experiment on the performance of XPOS model, Sesame, and the XML file system-based storage (XFSS) model regarding query processing and ontology update for comparative evaluation.",2008,10.1109/ICCIT.2008.198,no
An Approach to the Process of Understanding Numerals by Children: Relationships between Mental Representations and Semiotic Representations,"With support in the works of Duval (2004) regarding the operation of conversion between semiotic registries, the studies of numerical ""transcodification"" are analyzed. It is stated that in the studies on the understanding process of the decimal system of numeration by children, it is convenient to relate the activity of conversion between the verbal and Hindu-Arabic registers to the operating activity of children, when they try to respond to the logical demands presented to them by the understanding of the syntax implied in the decimal system of numeration, as a form of revealing the mechanisms that govern the constructions built by the children in this field.",2008,,no
Evaluation of ICT Investments in Public Administrations Based on Business Process Models,"Within the public sector domain there is great potential for business process optimization through ICT. However, until today these possibilities remain largely unexploited. To measure the impact of ICT-investments all processes of a public administration have to be taken into account. The PICTURE modelling method has been proposed as a way to efficiently model the whole process landscape of a public administration. Based on the processes captured, the impact of certain ICT functionalities can be analyzed. ICT investment decisions become more transparent towards the political leadership which are the decision makers in the public sector. This paper has two research objectives: First, an architecture for an semi-automated evaluation of ICT investment decisions is introduced. Second, the practical feasibility of the architecture is shown based on an investment decision for a document management system.",2008,,no
VARIUS: A model of process variation and resulting tuning errors for icroarchitects,"Within-die parameter variation poses a major challenge to high-performance microprocessor design, negatively impacting a processor's frequency and leakage power. Addressing this problem, this paper proposes a microarchitecture-aware model for process variation-including both random and systematic effects. The model is specified using a small number of highly intuitive parameters. Using the variation model, this paper also proposes a framework to model timing errors caused by parameter variation. The model yields the failure rate of microarchitectural blocks as a function of clock frequency and the amount of variation. With the combination of the variation model and the error model, we have VARIUS, a comprehensive model that is capable of producing detailed statistics of timing errors as a function of different process parameters and operating conditions. We propose possible applications of VARIUS to microarchitectural research.",2008,10.1109/TSM.2007.913186,no
Evaluation of a novel 'semi-solid' Fenton process: Case study on a kinetic model of o-nitroaniline degradation in hazardous solid waste,"A 'semi-solid' Fenton process (SSFP), in which the Fenton process (FP) takes place in a matrix with a low liquid-to-solid ratio (<= 5:1), was conducted on the degradation of o-nitroaniline (ONA) in hazardous solid waste. Based on the assumption of a first order reaction between ONA and the hydroxyl radical (center dot OH), an SSFP kinetic model was developed to accurately describe the kinetics of ONA degradation and to quantitatively investigate the effect of operating conditions. Half-life prediction model was also proposed and compared with the half-life calculated from pseudo first order equation to prove the effect of operating conditions. It showed that the degradation of ONA proceeded rapidly at pH 3.5. The kinetic rate constant, k, for ONA degradation increased with the initial ferrous ion concentration, but decreased with the liquid-to-solid ratio. With respect to the lowest residual ONA concentration, the optimal ferrous ion concentration and liquid-to-solid ratio were 0.062 mol/kg dry weight and 1.8, respectively.",2009,10.1080/10934520903005152,no
Neural network processing of natural language: II. Towards a unified model of corticostriatal function in learning sentence comprehension and non-linguistic sequencing,"A central issue in cognitive neuroscience today concerns how distributed neural networks in the brain that are used in language learning and processing can be involved in non-linguistic cognitive sequence learning. This issue is informed by a wealth of functional neurophysiology studies of sentence comprehension, along with a number of recent studies that examined the brain processes involved in learning non-linguistic sequences, or artificial grammar learning (AGL). The current research attempts to reconcile these data with several current neurophysiologically based models of sentence processing, through the specification of a neural network model whose architecture is constrained by the known cortico-striato-thalamo-cortical (CSTC) neuroanatomy of the human language system. The challenge is to develop simulation models that take into account constraints both from neuranatomical connectivity, and from functional imaging data, and that can actually learn and perform the same kind of language and artificial syntax tasks. In our proposed model, structural cues encoded in a recurrent cortical network in BA47 activate a CSTC circuit to modulate the flow of lexical semantic information from BA45 to an integrated representation of meaning at the sentence level in BA44/6. During language acquisition, corticostriatal plasticity is employed to allow closed class structure to drive thematic role assignment. From the AGL perspective, repetitive internal structure in the AGL strings is encoded in BA47, and activates the CSTC circuit to predict the next element in the sequence. Simulation results from Caplan's [Caplan, D., Baker, C., & Dehaut, F. (1985). Syntactic determinants of sentence comprehension in aphasia. Cognition, 21, 117-175] test of syntactic comprehension, and from Gomez and Schvaneveldts' [Gomez, R. L, & Schvaneveldt, R. W. (1994). What is learned from artificial grammars?. Transfer tests of simple association. Journal of Experimental Psychology: Learning, Memory and Cognition, 20,396-410] artificial grammar learning experiments are presented. These results are discussed in the context of a brain architecture for learning grammatical structure for multiple natural languages, and non-linguistic sequences. (C) 2008 Elsevier Inc. All rights reserved.",2009,10.1016/j.bandl.2008.08.002,no
Contextual Analysis Processing Methods able to interpret Sentiments Evaluation Representations,"A contextual analysis processing technique, consisting in determining the context understanding together with coherences in sentences, of concepts and phenomena related to each others., must simultaneously be able to interpret accurately a sequence of multiple semantic representations. A word frequently carries different meanings according to the context For example, having a ""big heart"" is usually understood as a quality, whereas the expression ""big words"" has rather a pejorative connotation The ""Evaluation polarity"" of a word has a probability of changing considerably according to the context By applying a semantic analysis of co-occurrence expressions, based on the use of phrases having an absolute evaluation polarity, we developed a system achieving analysis capable of detecting ""the role relations between words"", the relationship of meaning in a sentence, identifying transitions in the topic, anaphora, endophora, and analyzing even idiomatic expressions and textual emoticons Our system evaluated correctly ""positive"" or ""negative"" nuance for 75 0 % of those.",2009,10.1109/ICSC.2009.98,no
Modeling coupled THM processes of geological porous media with multiphase flow: Theory and validation against laboratory and field scale experiments,"A FEM model for analysis of fully coupled multiphase flow, thermal transport and stress/deformation in geological porous media was developed based on the momentum, mass and energy conservation laws of the continuum mechanics and the averaging approach of the mixture theory over a three phase (solid-liquid-gas) system. Six processes (i.e. stress-strain, water flow, gas flow, vapor flow, heat transport and porosity evolution processes) and their coupling effects are considered, which not only makes the problem well-defined, but renders the governing PDEs closed, complete. compact and compatible. Displacements, pore water pressure, pore gas pressure, pore vapor pressure, temperature and porosity are selected as basic unknowns. The physical phenomena such as phase transition, gas solubility in liquid, thermo-osmosis, moisture transfer and moisture swelling are modeled. As a result, the relative humidity and other related variables in porous media can be evaluated on a sounder physical basis. A three dimensional computer code, THYME3D, was developed, with eight degrees of freedom at each node. The laboratory CEA Mock-up test and the field scale FEBEX benchmark test on bentonite performance assessment for underground nuclear waste repositories were used to validate the numerical model and the software. The coupled THM behaviors of the bentonite barriers were satisfactorily simulated, and the effects and impacts of the governing equations, constitutive relations and property parameters on the coupled THM processes were understood in terms of more straightforward interpretation of physical processes at microscopic scale of the porous media. The work developed enables further in-depth research on fully coupled THM or THMC processes in porous media. (C) 2009 Elsevier Ltd. All rights reserved.",2009,10.1016/j.compgeo.2009.06.001,no
Process-oriented tests for validation of baroclinic shallow water models: The lock-exchange problem,"A first step often taken to validate prognostic baroclinic codes is a series of process-oriented tests, as those suggested by Haidvogel and Beckmann [Haidwvogel, D., Beckmann, A., 1999. Numerical Ocean Circulation Modeling. Imperial College Press, London], among others. One of these tests is the so-called ""lock-exchange"" test or ""dam break"" problem, wherein water of different densities is separated by a vertical barrier, which is removed at time zero. Validation against these tests has primarily consisted of comparing the propagation speed of the wave front, as predicted by various theoretical and experimental results, to model output. In addition, inter-model comparisons of the lock-exchange test have been used to validate codes. Herein, we present a high resolution data set, taken from a laboratory-scale model, for direct and quantitative comparison of experimental and numerical results throughout the domain, not just the wave front. Data is captured every 0.2 s using high resolution digital photography, with salt concentration extracted by comparing pixel intensity of the dyed fluid against calibration standards. Two scenarios are discussed in this paper, symmetric and asymmetric mixing, depending on the proportion of dense/light water (17.5 ppt/0.0 ppt) in the experiment: the Boussinesq approximation applies to both. Front speeds, cast in terms of the dimensionless Froude number, show excellent agreement with literature-reported values. Data are also used to quantify the degree of mixing, as measured by the front thickness, which also provides an error band on the front speed. Finally, experimental results are used to validate baroclinic enhancements to the barotropic shallow water ADvanced CIRCulation (ADCIRC) model, including the effect of the vertical mixing scheme on simulation results. Based on salinity data, the model provides an average root-mean-square (rms) error of 3.43 ppt for the symmetric case and 3.74 ppt for the asymmetric case, most of which can be attributed to the shear instabilities along the density front, which are not resolved by this hydrostatic model. Front tracking and mixed layer thickness rms errors average 34.9 mm (15.7% relative to total fluid depth) and 10.2 mm (4.9%) for the symmetric test and 37.2 mm (16.8%) and 14.0 mm (6.2%) for the asymmetric test, respectively. All non-constant vertical closure schemes are able to capture the front speed, but the Mellor-Yamada 2.5 scheme most accurately captures the synoptic behavior of the mixing layer thickness. (C) 2009 Elsevier Ltd. All rights reserved.",2009,10.1016/j.ocemod.2009.01.003,no
Generalized model of the quantum measurement process in an environment,"A generalized model of the quantum measurement process is considered, in which the relaxation of the detector system interacting with an environment can be treated. When the effect of the environment on the detector system is described by the quantum Markovian process, the model can be solved exactly. The time evolution of the particle and detector is investigated in detail. It is shown that the decay of quantum coherence of the particle becomes negligible when the relaxation time of the detector system is sufficiently short. Furthermore the stochastic model is found, which is equivalent to the proposed model with respect to the particle state.",2009,10.1103/PhysRevA.79.032113,no
Fundamental understanding and modeling of spin coating process : A review,A mathematical model is derived to elucidate the dominant mechanism governing film formation. It leads to a relation between film thickness and film radius spreading with time. Inclusion of evaporation and shear stress was mad e with extension to non-Newtonian fluid. The advantages and disadvantages of this process with applications are reviewed.,2009,,no
Comprehensive Modeling Tool for Chemical Looping Based Processes,"A model library has been created to describe gas-solid reactors for chemical looping combustion (CLC). CLC allows oxidation of a hydrocarbon fuel avoiding direct mixing with combustion air. Metal oxides are used to transport oxygen from one fluidized bed reaction zone to another, undergoing repeated cycles of oxidation and reduction. The equation-oriented IPSEpro environment is used for implementation. The models are based on conservation of mass and energy and allow calculation of the thermodynamic equilibrium with the thermodynamic data of different metal oxide systems implemented. The model assumptions are made in accordance with operating experience from laboratory installations. The characteristics of the technology are discussed using the presented models for the two fluidized bed reactors. The modeling tool allows investigation of integrated power plant configurations based on CLC technology.",2009,10.1002/ceat.200800568,no
Model-based evaluation of control strategies for phosphorus removal in a full-scale advanced phase isolation ditch process,"A model-based evaluation of operation conditions and control strategies was conducted for phosphorus removal in a full-scale Advanced Phase Isolation Ditch (APID) process. The APID process is an alternating type and does not have a separated anaerobic reactor. We suggested that it would be a suitable operational option for robust phosphorus removal by having a different input point for the influent and return sludge flow at specific modes. For evaluation of control strategies, three cases of influent disturbance were assumed, and five manipulated variables were selected for controlling the cases of disturbance. In the case of an increased influent flow rate, a combination of four manipulated variables is proposed through our simulation results as the best control strategy. The optimal k(L)a value was found to be 250/d when pollutants loading kept increasing without variations in the flow rate. When both the pollutants loading and the influent flow rates were increased simultaneously, the robust control strategy is to combine the return sludge inflow point, the exclusive operation modes which have a relatively long hydraulic retention time (HRT), operation period of 30 minutes, and the increase of the return sludge flow rate in proportion to the influent flow rate added to 300/d of k(L)a value.",2009,10.2166/wst.2009.426,no
Novel modeling concept for evaluating the effects of cadmium and copper on heterotrophic growth and lysis rates in activated sludge process,"A new modeling concept to evaluate the effects of cadmium and copper on heterotrophic growth rate constant (mu H) and lysis rate constant (b(H)) in activated sludge was introduced. The oxygen uptake rate (OUR) was employed to measure the constants. The results indicated that the mu(H) value decreased from 4.52 to 3.26d(-1) or by 28% when 0.7 mg L(-1) of cadmium was added. Contrarily the b(H) value increased from 0.31 to 0.35 d(-1) or by 11%. When adding 0.7 mg L(-1) of copper, the mu(H) value decreased to 2.80d(-1) or by 38%. The b(H) value increased to 0.42 d(-1) or by 35%. After regression, the inhibitory effect was in a good agreement with non-competitive inhibition kinetic. The inhibition coefficient values for cadmium and copper were 1.82 and 1.21 mg L(-1), respectively. The relation between the b(H) values and heavy metal concentrations agreed with exponential type well. The heavy metal would enhance b(H) value. Using these data, a new kinetic model was established and used to simulate the degree of inhibition. It was evident that not only the inhibitory effect on mu(H) but also that the enhancement effect on b(H) should be considered when heavy metal presented. (C) 2008 Elsevier B.V. All rights reserved.",2009,10.1016/j.jhazmat.2008.11.009,no
EVALUATION OF NONSTATIONARY VEHICLE PASSING LOUDNESS BASED ON AN ANTINOISE WAVELET PRE-PROCESSING NEURAL NETWORK MODEL,"A new technique for sound loudness evaluation, the so-called antinoise wavelet preprocessing neural network (ANWT-NN) model, is presented in this paper. Based on passing vehicle noise, the ANWT-NN loudness model combines the techniques of wavelet analysis and neural network regression and classification. A wavelet-based, 21-point model for vehicle noise feature extraction is established. Veri. cation shows that the trained ANWT-NN models are more accurate and effective than the WT-NN models for sound quality evaluation of nonstationary vehicle noises. The newly proposed ANWT-NN model can be applied to both the stationary and nonstationary sound signals and even to the transient ones. The ANWT-NN technique is suggested not only for the prediction, classification, and comparison of the sound quality of passing vehicle noise, but also for applications in other sound-related engineering fields, in place of the conventional psychoacoustical models.",2009,10.1142/S0219691309003033,no
Multi-model based real-time final product quality control strategy for batch processes,"A novel real-time final product quality control strategy for batch operations is presented. Quality control is achieved by periodically predicting the final product quality and adjusting process variables at pre-specified decision points. This data-driven methodology employs multiple models, one for each decision point, to capture the time-varying relationships. These models combine real-time batch information from process variables and initial conditions with information from prior batches. Design of experiments is performed to generate informative data that reveal the relationship between process conditions and the final product quality at various times. Control action is also taken at pre-specified decision points; at these times, the manipulated variable values are calculated by solving an optimal control problem similar to model predictive control. A key benefit of this strategy is that missing data imputation is obviated. The proposed modeling and quality control strategy is illustrated using a batch reaction case study. (C) 2008 Elsevier Ltd. All rights reserved.",2009,10.1016/j.compchemeng.2008.10.022,no
Modeling Study of the Sorption-Enhanced Reaction Process for CO2 Capture. I. Model Development and Validation,"A one-dimensional reactor model has been developed to describe the performance of a sorption-enhanced steam-methane reforming and water-gas shift reactor. In part I of this paper, the model is verified using the analytical solution for the breakthrough curve and validated using the results of laboratory-scale CO2 sorption-only experiments. Langmuir and Freundlich isotherms are fitted to an experimentally derived adsorption isotherm, while a linear driving force model is used to describe the sorption kinetics. The breakthrough profile is accurately described using the Freundlich isotherm. This holds also when the purge flow or duration of the desorption step are decreased, provided the mass transfer coefficient is changed accordingly during the desorption step. A sensitivity analysis shows that the breakthrough profile is sensitive to the adopted isotherm model and its parameters. The molecular diffusion coefficient affects the slope of the breakthrough curve, while particle size and heat of adsorption show hardly any effect. In part II, the model will be applied to laboratory-scale sorption-enhanced steam-methane reforming experiments.",2009,10.1021/ie801319q,no
Risk Evaluation Process Modeling in Software Project Investment Based on Bayesian networks,"A risk evaluation model in software project investment based on Bayesian networks (BNs) is presented in this paper. The BNs parameter learning is applied to the modeling process based on sample data set, so that the BNs is more accordant with the project feature in the software project investment phase. In addition, the validity of the parameter learning is validated with algorithm precision and convergence. Practice proves that the risk evaluation model can provide the accurate risk information for decision-makers in the process of software project investment.",2009,,no
A process-based model to evaluate site quality for Eucalyptus nitens in the Bio-Bio Region of Chile,"A simplified, physiologically based (3-PG) model was used to evaluate the effects of spatial variation in climate and soils on the potential productivity of Eucalyptus nitens Deane and Maiden (Maiden). The model was parameterized based on the physiology of E. nitens to predict potential productivity and leaf area index as influenced by environmental factors. Data obtained from conventional weather stations were utilized by the 3-PG model to predict productivity site classes. A final plantation suitability grid was mapped to show areas of the region with productivity classes predicted to be very high (> 52 m(3) ha(-1) year(-1)), high (47.5-52 m(3) ha(-1) year(-1)), moderate (45-47.5 m(3) ha(-1) year(-1)), low (35-45 m(3) ha(-1) year(-1)) and very low (< 35 m(3) ha(-1) year(-1)). The lowest potential productivity was attributable to soil water and nutrients limitations. A process-based forest growth model that can be widely extrapolated using geographic information system is particularly useful to screen areas as prospective plantation sites.",2009,10.1093/forestry/cpn045,no
Understanding Riser and Downer Based Fluid Catalytic Cracking Processes by a Comprehensive Two-Dimensional Reactor Model,"A two-dimensional reactor model incorporating hydrodynamics, mass balance, energy balance, and a 4-lump/ 14-lump kinetic model was established to simulate the riser and downer based fluid catalytic cracking (FCC) processes. The kinetic parameters of the 4-lump kinetic model were re-evaluated from the originally published experimental data for a more reliable description of the FCC process. The 14-lump kinetic model based on a molecular description of cracking and hydrogen transfer reactions was to include more details about the feedstock composition, reaction mechanisms, and the products distribution for a better understanding on the reactor performance for FCC process. This comprehensive model captured the key characteristics of the gas-solid reacting flows in the riser and downer, i.e., the uniformity of flow structures, the distinct backmixing behavior in the riser and downer, and the momentum and energy balances during the complex FCC reactions. The model predictions were first validated against industrial data from several literature sources and found to agree with each other reasonably well. Then, the simulations were carried out to fully understand the different reactor performances of riser and downer in the application of FCC refining processes. It can be concluded that the downer benefits from its advantages of the plug-flow nature and uniform flow structures, tending to have more products in the middle distillates, e.g., gasoline and light olefins, especially under high-severity operations. Better control of the reaction extent for increased selectivity to desired intermediate products would allow the use of downer reactors for the larger-scale practical applications in the FCC process, together with the valuable by production of light olefins.",2009,10.1021/ie800168x,no
Modeling of Dimensional Errors in Slender Bar Turning Process Using Artificial Neural Networks,"Accurate predictive modeling is an essential prerequisite for optimization and control of production in modern manufacturing environments. For slender bar turning operations, dimensional deviation is one of the most important product quality characteristics due to the low stiffness of part. In this study, radial basis function neural network is employed to investigate dimensional errors in slender bar turning. The relationship between cutting parameters and dimensional errors is firstly described by the proposed model. Simulation is provided to investigate the effects of cutting parameters on dimensional errors. Further, real-time predictive model based on radial basis function neural network is developed to perform the dimensional error monitoring during slender bar turning process. Experiments verify that the proposed in-process predictive system has the ability to monitor efficiently dimensional errors within the range that they have been trained.",2009,10.4028/www.scientific.net/AMM.16-19.549,no
Integrated Estimation of Measurement Error with Empirical Process Modeling-A Hierarchical Bayes Approach,"Advanced empirical process modeling methods such as those used for process monitoring and data reconciliation rely on information about the nature of noise in the measured variables. Because this likelihood information is often unavailable for many practical problems, approaches based on repeated measurements or process constraints have been developed for their estimation. Such approaches are limited by data availability and often lack theoretical rigor. In this article, a novel Bayesian approach is proposed to tackle this problem. Uncertainty about the error variances is incorporated in the Bayesian framework by setting noninformative priors for the noise variances. This general strategy is used to modify the Sampling-based Bayesian Latent Variable Regression (Chen et al., J Chemom., 2007) approach, to make it more robust to inaccurate information about the likelihood functions. Different noninformative priors for the noise variables are discussed and unified in this work. The benefits of this new approach are illustrated via several case studies. (C) 2009 American Institute of Chemical Engineers AIChE J, 55: 2883-2895, 2009",2009,10.1002/aic.11918,no
Material model validation for laser shock peening process simulation,"Advanced mechanical surface enhancement techniques have been used successfully to increase the fatigue life of metallic components. These techniques impart deep compressive residual stresses into the component to counter potentially damage-inducing tensile stresses generated under service loading. Laser shock peening (LSP) is an advanced mechanical surface enhancement technique used predominantly in the aircraft industry. To reduce costs and make the technique available on a large-scale basis for industrial applications, simulation of the LSP process is required. Accurate simulation of the LSP process is a challenging task, because the process has many parameters such as laser spot size, pressure profile and material model that must be precisely determined. This work focuses on investigating the appropriate material model that could be used in simulation and design. In the LSP process material is subjected to strain rates of 10(6) s(-1), which is very high compared with conventional strain rates. The importance of an accurate material model increases because the material behaves significantly different at such high strain rates. This work investigates the effect of multiple nonlinear material models for representing the elastic-plastic behavior of materials. Elastic perfectly plastic, Johnson-Cook and Zerilli-Armstrong models are used, and the performance of each model is compared with available experimental results.",2009,10.1088/0965-0393/17/1/015010,no
Declarative versus Imperative Process Modeling Languages: The Issue of Understandability,"Advantages and shortcomings of different process modeling languages are heavily debated, both in academia and industry, but little evidence is presented to support judgements. With this paper we aim to contribute to a more rigorous, theoretical discussion of the topic by drawing a link to well-established research on program comprehension. In particular, we focus on imperative and declarative techniques of modeling a process. Cognitive research has demonstrated that imperative programs deliver sequential information much better while declarative programs offer clear insight into circumstantial information. In this paper we show that in principle this argument can be transferred to respective features of process modeling languages. Our contribution is a pair of propositions that are routed in the cognitive dimensions framework. In future research, we aim to challenge these propositions by an experiment.",2009,,no
Application of analytic hierarchy process-based model of Ratio of Comprehensive Cost to Comprehensive Profit (RCCCP) in pest management,"After investigation and analysis of the practice of pesticide application in the protected horticultural fields of the Agricultural Experimental Zone in Pudong district, Shanghai, China, analytic hierarchy process (AHP) was applied to evaluate 5 alternative pest-control strategies (including three gray insect-proof net rooms (with mesh) of different height, white insect-proof thin film rooms (without mesh) and normal control and prevention areas (without any covering nets or thin films)). Comprehensive Profit (CP) and Comprehensive Cost (CC) were used in this paper to analyze the superiority of different strategies in protected horticultural fields for pest management. CP and CC of the pest-control strategies were used as the targets of AHP, in which CP was classified into economic, social and ecological profit, while CC was split into economic, social and ecological cost. Ratio of Comprehensive Cost to Comprehensive Profit (RCCCP) was calculated by dividing Comprehensive Cost (CC) by Comprehensive Profits (CP). According to the analysis, the best strategy was to apply gray insect-proof net rooms of 2.2 m high and 360 (12 m x 30 m) square meters in protected horticultural fields for pest management. Published by Elsevier B.V.",2009,10.1016/j.ecolecon.2008.07.021,no
Measuring and Controlling Model of Pulp Kappa Number with Spectroscopy during Batch Sulfite Pulping Process,"Aiming at the difficulties and problems of online measurement of pulp kappa number during batch pulping, this paper proposes a new measuring idea, which is to predict the lignin content ill Pulp through the measurement of dissolved lignin in cooking liquor with spectral technology, The basic and modified mathematical model of kappa number during sulfite pulping as well as the end point controlling model are put forward in detail. The site running result shows that the introduction of modified factor epsilon, which represents the change of the total lignin amount added into the digester, is effective. Therefore, the online measurement of kappa number and the determination of the end point of pulping can be achieved.",2009,10.1021/ie900327a,no
Modelling and Evolutionary Multi-objective Evaluation of Interdependencies and Work Processes in Airport Operations,"An airport is a multi-stakeholders environment, with work processes and operations cutting across a number of organizations. Airport landside operations involve a variety of services and entities that interact and depend on each others. In this paper, we introduce the Landside Modelling and Analysis of Services (LAMAS) tool to simulate, analyze and evaluate the interdependencies of services in airport operations. A genetic algorithm is used to distribute resources among the different entities in an airport such that the level of service is maintained. The problem is modelled as a multi-objective constrained resource allocation problem with the objective functions being the maximization of quality of service while reducing the total cost.",2009,,no
Optimal iterative learning control for end-point product qualities in semi-batch process based on neural network model,"An optimal iterative learning control (ILC) strategy of improving endpoint products in semi-batch processes is presented by combining a neural network model. Control affine feed-forward neural network (CAFNN) is proposed to build a model of semi-batch process. The main advantage of CAFNN is to obtain analytically its gradient of endpoint products with respect to input. Therefore, an optimal ILC law with direct error feedback is obtained explicitly, and the convergence of tracking error can be analyzed theoretically. It has been proved that the tracking errors may converge to small values. The proposed modeling and control strategy is illustrated on a simulated isothermal semi-batch reactor, and the results show that the endpoint products can be improved gradually from batch to batch.",2009,10.1007/s11432-009-0123-8,no
Using non-equilibrium biosorption column modelling for improved process efficiency,"An unsteady-state biosorption column model in one space variable is considered in this paper. The theoretical study is motivated by the need for predicting the dependent variables of biosorption columns for removal of heavy metals from wastewater, which is potentially an important technology in cleaner production. Pollutant concentrations in the bulk phase, in the liquid filling the pores and in the solid biomass along the axial coordinate of the column were evaluated for a wide range of physical and chemical parameters of the global process. In particular, the assumption of instantaneous chemical reactions is replaced by a more realistic expression taking into account chemical kinetics and fluid advection time scales separately. The resulting system of partial differential equations was solved by means of a reliable numerical algorithm based on the method of lines. The data obtained using the model described in this paper are compared with published experimental data and an estimation of the increased efficiency is made. (C) 2009 Elsevier Ltd. All rights reserved.",2009,10.1016/j.jclepro.2009.02.010,no
INVESTIGATING REPRESENTATIONAL FLUENCY IN MODELING PROCESS: THE EXPERIENCE OF PRE-SERVICE TEACHERS WITH A CASSETTE PROBLEM,"As a part of larger research on pre-service teachers mathematical modeling abilities, this study investigates pre-service teachers' representational fluency, and its effect on modeling process in one modeling activity, that of cassette problem. The data is collected by students' individual and group written responses to the mathematical modeling activity, video-taped group discussions and classroom observation by the researcher. This study is conducted during the spring semester in 2007 and thirty three pre-service teachers were the participants of the study. The data showed that pre-service teachers have difficulty in transition between different modes of mathematical representations and this difficulty has an influence on modeling process.",2009,,no
Hydrogen quality for fuel cell vehicles - A modeling study of the sensitivity of impurity content in hydrogen to the process variables in the SMR-PSA pathway,"As fuel cell vehicles approach wide-scale deployment, the issue of the quality of hydrogen dispensed to the vehicles has become increasingly important. The various factors that must be considered include the effects of different contaminants on fuel cell performance and durability, the production and purification of hydrogen to meet fuel quality guidelines, and the associated costs of providing hydrogen of that quality to the fuel cell vehicles. In this paper, we describe the development of a model to track the formation and removal of several contaminants over the various steps of hydrogen production by steam-methane reforming (SMR) of natural gas, followed by purification by pressure-swing adsorption (PSA). We have used the model to evaluate the effects of setting varying levels of these contaminants in the product hydrogen on the production/purification efficiency, hydrogen recovery, and the cost of the hydrogen. The model can be used to track contaminants such as CO(2), CO, N(2), CH(4), and H(2)S in the process. The results indicate that a suggested specification of 0.2 ppm CO would limit the maximum hydrogen recovery from the PSA under typical design and operating conditions. The steam-to-carbon ratio and the process pressure are found to have a significant impact on the process efficiency. Varying the CO specification from 0.1 to 1 ppm is not expected to affect the cost of hydrogen significantly, although the cost of gas analysis to comply with such stringent requirements may add 210 cents/kg to the cost of hydrogen. (C) 2009 International Association for Hydrogen Energy. Published by Elsevier Ltd. All rights reserved.",2009,10.1016/j.ijhydene.2009.06.026,no
UNDERSTANDING AND ADDRESSING PSYCHOLOGICAL AND SOCIAL PROBLEMS: THE MEDIATING PSYCHOLOGICAL PROCESSES MODEL,"Background: Psychological and social problems such as mental disorder, unemployment, substance misuse and crime are personally distressing and absorb huge proportions of Government effort. Addressing these is a multi-agency, multidisciplinary exercise, but there is evidence of a marked policy shift toward the provision of psychological therapies and interventions. Aim: To offer a distinctively psychological perspective on these key social and mental health problems. Method: Scholarly review of the relevant literature. Results: This article presents a coherent model - the mediating psychological processes model - addressing the complex, interconnected, nature of these problems. The mediating psychological processes model suggests that disruption or dysfunction in psychological processes is a final common pathway in the development of mental disorder and social problems. The model proposes that biological, social and circumstantial factors lead to mental disorder, crime and other social problems through their conjoint effects in influencing or disrupting relevant psychological processes. Conclusions: The implications for policy, and implementation of policy, are discussed.",2009,10.1177/0020764008097757,no
Analysis on the Feedback Relation Diagram of Hospital Operation Efficiency Based on Process Reengineer,"Based on the feedback analysis theory of system dynamics, key variable reinforing and balancing feedback relation diagram is applied to the field of hospital management without precedent. This article analyized the reinforcing and balancing factors of hospital management successfully. Based on Peter Senge M's archetype method, the article posed that hospital process reengineer is the effective countermeasure for hospital running efficiently.",2009,,no
THERMOMECHANICAL MODELING OF SOLIDIFICATION PROCESS OF SQUEEZE CASTING II. Numerical Simulation and Experimental Validation,"Based on the thermomechanical model and solution methodology described in Part I, a finite element program for simulating the solidification process of squeeze casting was developed. By using the program and a constitutive relationship based on Gleeble test data, the solidification processes of A356 aluminum alloy under different process parameters were simulated. Squeeze casting experiments were carried out for validating the developed model and program. It is shown that the results of numerical simulation are in good agreement with the experimental results.",2009,,no
Evaluating and Selecting Flexible Manufacturing Systems by Integrating Data Envelopment Analysis and Analytical Hierarchy Process Model,"Because of continuous changes in the market and the today's competitiveness world, most of manufacturing firms are competing for meeting demand, increasing quality and decreasing costs. Today, manufacturing firms are required to select a suitable flexible manufacturing system (FMS) to keep their share of the market. We propose a procedure for evaluating the flexible manufacturing systems based on a model incorporating two decision models namely ""Data Envelopment Analysis (DEA)"" and ""Analytical Hierarchy Process (AHP)"". DEA helps us to categorize the Decision Making Units (DMUs) to only two classes of efficient and inefficient units, whereas by using an AHP, we can have a full rank of DMUs. Input and output factors considered for ranking FMSs are Capital and Operating Costs, Throughput Time, Work in Process, Labor Requirements, Required Floor Space, Product Mix Flexibility, Yield, and Volume Flexibility. We implement this approach for a Vehicle Manufacturing Company in Iran.",2009,10.1109/AMS.2009.68,no
Temperature measurements in the boron carbide manufacturing process - A hot model study,"Boron carbide is produced in a heat resistance furnace using boric oxide and petroleum coke as the raw materials. The product yield is very low. Heat transfer plays an important role in the formation of boron carbide. Temperature at the core reaches up to 2600 K. No experimental study is available in the open literature for this high temperature process particularly in terms of temperature measurement and heat transfer. Therefore, a laboratory scale hot model of the process has been setup to measure the temperatures in harsh conditions at different locations in the furnace using various temperature measurement devices such as pyrometer and various types of thermocouple. Particular attention was paid towards the accuracy and reliability of the measured data. The recorded data were analyzed to understand the heat transfer process inside the reactor and the effect of it on the formation of boron carbide. (C) 2008 Elsevier Ltd. All rights reserved.",2009,10.1016/j.ijrmhm.2008.10.004,no
Clinical cognition and diagnostic error: applications of a dual process model of reasoning,"Both systemic and individual factors contribute to missed or delayed diagnoses. Among the multiple factors that impact clinical performance of the individual, the caliber of cognition is perhaps the most relevant and deserves our attention and understanding. In the last few decades, cognitive psychologists have gained substantial insights into the processes that underlie cognition, and a new, universal model of reasoning and decision making has emerged, Dual Process Theory. The theory has immediate application to medical decision making and provides an overall schema for understanding the variety of theoretical approaches that have been taken in the past. The model has important practical applications for decision making across the multiple domains of healthcare, and may be used as a template for teaching decision theory, as well as a platform for future research. Importantly, specific operating characteristics of the model explain how diagnostic failure occurs.",2009,10.1007/s10459-009-9182-2,no
Dynamic Performance Evaluation Model of Collaborative Knowledge Chain in Collaborative Manufacturing Process,"Building a dynamic performance evaluation model is an important part in the management of collaborative manufacturing process. By analyzing the current evaluating method, this paper firstly put forward the mathematical model of dynamic performance evaluation of collaborative knowledge chain, and built a performance evaluation indices model. Then, the weight factors of performance evaluation model were calculated by using Delphi method. Thirdly, performance values of each knowledge unit and collaborative knowledge chain were calculated, and accordingly figures for performance values can be built. Finally, a case study was carried out.",2009,10.1109/ICMTMA.2009.264,no
Modeling and Optimizing for Flexibility in Business Processes,"Business process flexibility (BPF) is critical for enterprises in improving their competition abilities. In this paper, the business process flexible degree is introduced firstly. Then, the BPF is divided into three tiers, and each flexible degree is described. At last, an optimization model for business process flexibility is built for a process design.",2009,10.4028/www.scientific.net/AMM.16-19.436,no
Development of a Hybrid Model to Improve the Efficiency of Business Process Reengineering,Business process is a fundamental building block of organizational success. Business process reengineering (BPR) has been considered an important way to improve organizational performance. This paper develops a hybrid model for operationalizing a BPR project. This model for selecting and assessing the best practice items of BPR is proposed to address inter-relationships among best practice items using Decision Making Trial and Evaluation Laboratory (DEMATEL) and composite importance (CI) method. We compare the importance of the best practice items from a managerial perspective with the importance of the best practice from the perspective of employees to evaluate the optimal best practice items for improving BPR efficiency. A case is used to present the processes of this hybrid model.,2009,10.1109/IEEM.2009.5372929,no
Verifying the Consistency between Business Process Model and Data Model,"Business process model and data model play important roles in information system construction. They represent two different perspectives of business knowledge, and are closely related. A trouble to the model quality is the inconsistency between business process model and data model, which can often conduce to interaction errors. While finding such inconsistency is a meaningful problem, it receives little attention in available verification methods. We concentrate on this problem and identify some consistency anomalies between process model and data model. In our paper, a verification method PDGV is proposed to verify the consistency between process model and data model. Implemented prototype reveals that our scheme can detect consistency anomalies effectively.",2009,10.1109/JCAI.2009.122,no
An Information System Development Method Connecting Business Process Modeling and Its Experimental Evaluation,"Business process modeling (BPM) is gaining attention as a measure for analysis and improvement of the business process. BPM analyzes the Current business process as an As-Is model and solves problems to improve the current business, and also aims to create a business process that produces value as a To-Be model. However, research into techniques that connect the business process improvement acquired by BPM to the implementation of the information system seamlessly is rarely reported. If the business model obtained by BPM is converted to UML, and the implementation can be carried out by the technique of UML, one can expect an improvement in the efficiency of information system implementation. In this paper, we describe a method for system development that converts the process model obtained by BPM into UML. The method is evaluated by modeling a prototype of a parts procurement system. In the evaluation, a comparison with a case in which the system is implemented using the conventional UML techniques without BPM is performed. (C) 2009 Wiley Periodicals, Inc. Electr Eng Jpn, 169(4): 9-20, 2009; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/eej.20856",2009,10.1002/eej.20856,no
Understanding the effectiveness of Capability Maturity Model Integration by examining the knowledge management of software development processes,"Capability Maturity Model Integration (CMMI) is one of the well-known models that provide best practices for software quality improvement. Many articles praise the benefits of CMMI adoption, such as enhanced knowledge management of software development, improved software quality and increased efficiency of software development. However, these intangibles, especially those relating to knowledge management, have not been investigated yet. To build a deeper understanding of CMMI this paper intends to investigate the effectiveness of CMMI by examining the impact of quality management on the knowledge management of software development processes. A case study approach was employed to collect complete information of the impact of CMMI on software knowledge management from different aspects. The results reveal that the incongruence between managing the knowledge of software development and managing the process of software knowledge management has created blind points in the adoption of the CMMI programme. In addition to complying with the CMMI prescriptive guidelines CMMI-adoption organisations would need to integrate knowledge from both internal and external customers.",2009,10.1080/14783360902863671,no
Comprehensive Evaluation of CDIO model teachers' classroom teaching quality based on Fuzzy Analytic Hierarchy Process,"CDIO model is the latest achievement in the international engineering education reform, it is centralized summarization and abstract expression of learning by doing and Project based education and learning. AHP and Fuzzy judgement was used the evaluation system of CDIO model teachers' classroom teaching quality. By analyzing constituent elements of teaching quality, teaching attitude, teaching method, teaching content and teaching effectiveness was proposed as index system, on the basis of experts' knowledge and subjective experience, the subjective element was removed as much as possible by using marigorous logic mathematical methods, index weights was determined by using AHP, the model was established by using Fuzzy comprehensive evaluation. The evaluation results can reflect the management of teaching quality more objectively, so as to provide a new way for evaluation of CDIO model teachers' classroom teaching quality, thus it has theoretical and practical value.",2009,10.1109/ICIM.2009.38,no
Evaluation of inter-organizational business process solutions: A conceptual model-based approach,"Collaboration and coordination between organizations are necessary in today's business environment, and are enabled by inter-organizational processes. Many approaches for the construction of such processes have been proposed in recent years. However, due to the lack of standard terminology it is hard to evaluate and select a solution that fits a specific business scenario. The paper proposes a conceptual model which depicts the nature of interaction between organizations through business processes under specific business requirements that emphasize the privacy and autonomy of the participating organizations. The model is generic, and relies on the generic process model (GPM) framework and on Bunge's ontology. Being generic and theory-based, we propose to use the model as a basis for comparing and evaluating design and implementation-level approaches for inter-organizational processes. We demonstrate the evaluation procedure by applying it to three existing approaches.",2009,10.1007/s10796-008-9090-7,no
Mapping Enterprise Process Measure into Information Model,"Collecting measure data play a key role in practicing enterprise process measure. For mapping measure to enterprise information system and collecting measure data automatically, it is necessary to provide a common method to define semantics of process measure and information model. The semantics structure of basic measure is an analyze at first. Based on domain ontology, a structure called semantic tree is defined to describe the semantics relationships among measured entity, measurable attribute and constrains. The structure is suitable for enterprise information model too. Arithmetic to computer the similarity of semantics tree is presented and a method to map measure and enterprise information model is put forward. These methods are the base of automatic process measure data collection and enterprise information integration.",2009,10.1109/ETCS.2009.144,no
"Measuring the Impact of Product Lifecycle Management: Process Plan, Waste Reduction and Innovations Conceptual Frameworks, and Logic Model for Developing Metrics","Companies implement Product Lifecycle Management (PLM) as a means to reduce costs associated with wasted time, energy, and material with the expectation that captured resources can be reallocated to product and process improvements and innovations, potentially resulting in new revenue streams. As organizations prepare to implement PLM, it is critical that they understand their expectations of PLM and that they track the impact of PLM relative to achieving their organizational goals. A PLM Process Plan, PLM Waste Reduction and Innovation Conceptual Frameworks, and PLM Logic Model for Developing Metrics are suggested as a guide to organizations tasked with determining the impact of their PLM initiative.",2009,10.1007/978-0-387-09482-3_4,no
Achieving Flexibility in Business Process Modeling Using an Algebraic Language,"Companies nowadays are under increasing pressure to adapt their business processes to the ever-changing business environment. However, most traditional business process modeling (BPM) methods lack enough flexibility. It is hard to extract and reuse information and knowledge embedded in the existing business process models to build new models. We propose an algebraic framework based on a many-sorted algebra, Algebra of System (AoS), to achieve the flexibility in business process modeling. We decompose business process models into different modules of knowledge and encode them as different algebraic domains of AoS. Within each algebraic domain, knowledge is encoded as operands. Based on operands, each algebraic domain is equipped with operators to provide computation capabilities. The operands and operators ensure the flexibility that the framework can achieve and the analysis capabilities that the framework can provide.",2009,10.1109/MBSE.2009.5031720,no
Complementarity of interferometers interacting with the dynamical model of the quantum measurement process,"Complementarity of wave-like and particle-like properties in the Mach-Zehnder and Michelson interferometers is investigated by means of the exactly solvable model for a quantum measurement process. When the error-minimum measurement and unambiguous measurement are used for extracting the which-path information, the fringe visibility and which-path information are derived and their complementary relation is examined. It is shown that there is a possibility of the recovery of the quantum coherence in the Michelson interferometer since the which-path information can be erased by the propagating particle itself.",2009,10.1080/09500340802517694,no
Berry-Esseen type bounds of estimators in a semiparametric model with linear process errors,"Consider the semiparametric regression model y(i) = x(i)beta + g(t(i)) + V-i, 1 <= i <= n, where beta is an unknown parameter of interest, (x(i), t(i)) are nonrandom design points, y(i) are the response variables, g(.) is an unknown function defined on the closed interval [0, 1], and the correlated errors V-i = Sigma(infinity)(j=-infinity) psi(j)e(i-j), with Sigma(infinity)(j=-infinity) vertical bar psi(j)vertical bar < infinity, and ei are negatively associated random variables. Under appropriate conditions, in this paper, we derive Berry-Esseen type bounds for estimators of beta and g(.). As a corollary, by making a certain choice of the weights, we give the Berry-Esseen type bounds for estimators of beta and g(.); they are O(n(-1/4)(log n)(3/4)) and O(n(-3/28) (log n)(9/28)), respectively, and under further restriction for the weights, the Berry-Esseen type bound for estimator of g(.) can also attain O(n(-1/4) (log n)(3/4)). (C) 2008 Elsevier Inc. All rights reserved.",2009,10.1016/j.jmva.2008.03.006,no
Controlled measurement processes: Simple spin-chain model of controlled quantum-state amplification,"Controlled measurement processes with high fidelity and robustness will be of substantial interest to fundamental quantum studies as well as quantum-information processing. This work proposes a simple scheme to realize controlled measurements of the state of a single spin, via controlled quantum signal amplification in a one-dimensional spin-chain model. It is shown that, by adiabatically moving an external field applied to a spin chain that is also irradiated with a transverse driving field, a robust spin amplifier may be realized with little dispersion. Our results may find applications in spin-based quantum-information processing and in controlled growth of ""Schrodinger cat"" states.",2009,10.1103/PhysRevA.79.012317,no
Boosting efficiency and accurate modeling of parametric down-conversion processes,"Conversion efficiency is boosted by using tandem crystal optical parametric amplifier geometry. We have undertaken comprehensive numerical modeling of the optical systems, where the output beam for the first stage are calculated and then propagated to the second stage. As a concrete example periodically poled lithium niobate crystals are experimentally examined. The phase and amplitude information about the pump and signal beams were retrieved using the Fresnel phase retrieval method and used as input in the theory. This enabled real laboratory conditions to be modeled. Subsequently, the amplitude and phase of the complex pump and signal beam from the crystal was calculated and the results were validated by experiments. Non-collinear interactions in the second stage also yielded an efficient idler output over a broad range of temperatures. ZnGeP(2) crystals were used in the numerical simulations with first and second stage amplification. We predicted an idler energy increase by a more than a factor of 3.",2009,10.1117/12.816582,no
An Abstract Processing Model for the Quality of Context Data,"Data quality can be relevant to many applications. Especially applications coping with sensor data cannot take a single sensor value for granted. Because of technical and physical restrictions each sensor reading is associated with an uncertainty. To improve quality, an application can combine data values from different sensors or, more generally, data providers. But as different data providers may have diverse opinions about a certain real world phenomenon, another issue arises: inconsistency. When handling data from different data providers, the application needs to consider their trustworthiness. This naturally introduces a third aspect, of quality: trust,. In this paper we propose a novel processing model integrating the three aspects of quality: uncertainty, inconsistency and trust.",2009,,no
A Dynamic Model to Handle Defect-Oriented Processes,"Defect prediction and estimation techniques play an important role in the quantitative management. This paper proposes a defect-oriented model based on System Dynamics, incorporating the Defect Removal Efficiency model to improve the quantitative defect management. Using the dynamic model, we can simulate defect introduction and removal processes, and estimate the residual defect density in different phases of the software lifecycle. We provide the model initialization, and validate simulation results with the empirical and real project data. These experiments show that the model could access the variance of the actual situation.",2009,,no
Modelling and experimental validation of emulsification process in continuous rotor-stator units,"Despite the wide range of industrial applications of structured emulsions, current approaches towards process design and scale-up are commonly based on trial-and-error experimentation. As this design approach is foreseen to deliver most likely suboptimal process solutions, we propose in this contribution a model-based approach as the way forward to designing manufacturing processes of structured emulsions. In this context, process modelling and simulation techniques are applied to predict production rates and equipment sizing. Moreover, sensitivity analysis of the process model provides insight about potential bottlenecks in the process.",2009,,no
A Comprehensive Model of the Electroslag Remelting Process: Description and Validation,"Electroslag remelting (ESR) is widely used for the production of high-value-added alloys such as special steels or nickel-based superalloys. Because of high trial costs and the complexity of the mechanisms involved, trial-and-error-based approaches are not well suited for fundamental studies or for optimization of the process. Consequently, a transient-state numerical model has been developed that accounts for electromagnetic phenomena and coupled heat and momentum transfers in an axisymmetrical geometry. The model simulates the continuous growth of the electroslag-remelted ingot through a mesh-splitting method. In addition, solidification of the metal is modeled by an enthalpy-based technique. A turbulence model is implemented to compute the motion of liquid phases (slag and metal), while the mushy zone is described as a porous medium the permeability of which varies with the liquid fraction, thus enabling accurate calculation of solid/liquid interaction. The coupled partial differential equations (PDEs) are solved using a finite-volume technique. The computed results are compared to the experimental observation of an industrial remelted ingot; the melt pool depth and shape, in particular, are investigated, in order to validate the model. These results provide valuable information about the process performance and the influence of the operating parameters. In this way, we present an example of a model used as a support in analyzing the influence of the electrode fill ratio.",2009,10.1007/s11663-008-9208-9,no
COMPARISON OF RELATIVE CONTIBUTIONS OF PROCESS AND GEOMETRIC ERRORS FOR MICRO AND MACRO SCALE MILLING USING AN INTEGRATED ERROR MODEL,"Error models are available for individual error sources for specific components in a machine tool. However, proper error budgeting requires an understanding of the relative importance of various error sources. This paper presents an analysis of the relative contribution of process and geometric errors on the dimensional error produced in micro scale milling. The error contributions at the micro scale are compared and contrasted with those in macro scale milling. The analysis makes use of an error model that uses homogeneous transformation matrices (HTMs) to model the effect of each error source in each component of the machine tool on the final position of the tool with respect to the workpiece. The model is developed for a generic milling machine and used to compare the relative importance of errors due to misalignment, thermal growth, tool deflection and wear of the tool on the size of the machined feature at the micro and macro scale for slot milling using standard length tools.",2009,,no
Evaluation Model for Computer Network Information Security Based on Analytic Hierarchy Process,"Evaluation for computer network information security is helpful for taking corresponding preventive measures. In order to obtain a comprehensive assessment of network security, analytic hierarchy process (AHP) model is proposed to assess the computer network information security. As the criteria and the relevant factors are decomposed hierarchically corresponding to evaluation and judgment of the problem, all kinds of factors of influencing network security are researched and the evaluation indexes for computer network information security are constructed, analytic hierarchy process (AHP) evaluation model for computer network information security is constructed on the basis of the evaluation indexes. The experimental results indicate that the evaluation of computer network information security by analytic hierarchy process is effective.",2009,10.1109/IITA.2009.95,no
Statistical Methods for Quality Control of Steel Coils Manufacturing Process using Generalized Linear Models,"Fault detection and diagnosis is an important problem in process engineering. Process equipments are subject to malfunctions during operation. Galvanized steel is a value added product, furnishing effective performance by combining the corrosion resistance of zinc with the strength and formability of steel. Fault detection and diagnosis is an important problem in continuous hot dip galvanizing and the increasingly stringent quality requirements in automotive industry has also demanded ongoing efforts in process control to make the process more robust. When faults occur, they change the relationship among these observed variables. This work compares different statistical regression models proposed in the literature for estimating the quality of galvanized steel coils on the basis of short time histories. Data for 26 batches were available. Five variables were selected for monitoring the process: the steel strip velocity, four bath temperatures and bath level. The entire data consisting of 48 galvanized steel coils was divided into sets. The first training data set was 25 conforming coils and the second data set was 23 nonconforming coils. Logistic regression is a modeling tool in which the dependent variable is categorical. In most applications, the dependent variable is binary. The results show that the logistic generalized linear models do provide good estimates of quality coils and can be useful for quality control in manufacturing process.",2009,,no
Fixed-links models for investigating experimental effects combined with processing strategies in repeated measures designs: A cognitive task as example,Fixed-links models enable the investigation of experimental effects in combination with processing strategies in the framework of repeated measures designs. The consideration of different processing strategies is reasonable whenever the task requirements can be met in different ways. Effects due to specific processing strategies become obvious in comparing the models representing these processing strategies and also by investigating the variances of the corresponding latent variables. The usefulness of such models is demonstrated by the reaction time data of a cognitive task including three treatment levels. Two different processing strategies were considered: analytic and holistic. The results indicated that the participants applied both processing strategies. The analytic strategy was primarily applied for performing with respect to the second and third treatment levels and the holistic strategy with respect to the first treatment level.,2009,10.1348/000711007X268558,no
Network Processing Performability Evaluation on Heterogeneous Reliability Multicore Processors using SRN Model,"Future network systems and embedded infrastructure devices in ubiquitous environments will need to consume low power and process large amounts of network packet traffic. In order to meet necessary high processing efficiency requirements, future processors will have many heterogeneous cores with reduced reliability due to low voltage, small transistor sizes, semiconductor wearout, and environmental factors such as noise and interference. It will be necessary for multi-core network infrastructure software to mitigate transient hardware faults to maintain acceptable system reliability. Applications such as packet processing can benefit from the reliability versus performance tradeoff. We propose a model based on Stochastic Reward Nets to evaluate the performance vs. reliability tradeoff of unreliable embedded multi-core network processors, and apply this model to a multi-core packet processing application.",2009,,no
Modelling and Simulation of the Fiber Optic Gyroscope (FOG) in Measurement-While-Drilling (MWD) Processes,"Gyroscopes are sensors that are used to determine angular velocity and position. Normally, magnetometers are used in horizontal drilling processes in oil industry to determine the azimuth of bottom hole assembly (BHA). Using magnetometers has some shortcomings in measuring earth's magnetic field due to downhole ore deposits; drill string-induced interference and geomagnetic influences. To overcome these problems, we propose using Fiber Optic Gyroscope (FOGs) as a better alternative in measurement-while-drilling (MWD) processes for determining the azimuth of the BHA. Computer modelling and simulation confirm that the FOG could result in a better accuracy and performance considering the severe downhole conditions.",2009,,no
Measuring the impact of option market activity on the stock market: Bivariate point process models of stock and option transactions,"I apply the bivariate Autoregressive Conditional Duration model of Engle and Lunde [2003. Trade and quotes: a bivariate point process. Journal of Financial Econometrics 1, 159-188] to stock and option market transactions. The first model uses option trades and stock trades. Shocks to option trade/option trade durations have a significant impact on option trade/stock trade durations. Higher implied volatility, larger stock and option market order imbalances, larger stock trades, larger spreads, smaller depths in the stock market and faster trading in the stock and option markets are all associated with faster trading in both markets. In the second model, option trade/option trade timing leads option trade/stock quote timing and several information-related stock and option market covariates impact the expected inter-market event durations. (c) 2008 Elsevier B.V. All rights reserved.",2009,10.1016/j.finmar.2008.01.002,no
On Measuring Semantic Similarity of Business Process models,"Identifying and integrating similar business processes within different organizations is an important task in construction of virtual enterprise. However, the same business process may be represented in different ways by different modelers even when they using the same modeling language. This is because that different organization using different terminologies and the problem of semantic heterogeneity makes it a tedious job to compare business processes. Therefore, the technologies of semantic similarity computing are employed to resolve ambiguity issues caused by the use of synonyms or homonyms. In particular, the idea of similarity propagation is introduced to pick out a mapping between corresponding activities and data, and Hungarian algorithm is expanded to reduce its time complexity. Then the similarity of whole models is measured based on Jaccard coefficient. Finally, an experiment is given to evaluate the method.",2009,10.1109/I-ESA.2009.50,no
A consistency based approach to deal with modeling errors and process failures in DES,"In classical consistency based approaches used for diagnosis of discrete event systems (DES), an inconsistency between the behaviors described in the reference model and the behavior observed through the sensors detects the occurrence of a failure in the system. If the model is not considered as error free, this traditional interpretation of inconsistencies must be enhanced to include the model errors as a potential responsible for the inconsistency. This paper has two objectives. First, it proposes a mechanism able to discriminate among inconsistencies the ones due to modeling errors an those caused by failures. Second, it describes a method to restore the consistency between the models and the observations.",2009,10.1109/ICSMC.2009.5346222,no
Modeling and Evaluating Energy-Performance Efficiency of Parallel Processing on Multicore Based Power Aware Systems,"In energy efficient high end computing, a typical problem is to find an energy-performance efficient resource allocation for computing a given workload. An analytical solution to this problem includes two steps: first estimating the performances and energy costs for the workload running with various resource allocations, and second searching the allocation space to identify the optimal allocation according to an energy-performance efficiency measure. In this paper, we develop analytical models to approximate performance and energy cost for scientific workloads on multicore based power aware systems. The performance models extend Amdahl's law and power-aware speedup model to the context of multicore-based power aware computing. The power and energy models describe the power effects of resource allocation and workload characteristics. As a proof of concept, we show model parameter derivation and model validation using performance, power, and energy profiles collected on a prototype multicore based power aware cluster.",2009,,no
On the Use of Dirichlet Process Mixtures for the Modelling of Pseudorange Errors in Multi-constellation Based Localisation,"In Global Navigation Satellite Systems (GNSS) positioning, the receiver measures the pseudoranges with respect to each observable navigation satellite and determines the user position. The use of many constellations should lead to highly available, highly accurate navigation anywhere. However, it is important to notice that even if modern receivers achieve high position accuracy in line-of-sight (LOS) conditions, multipath propagation highly degrades positioning performances even in multi-constellation based localisation (""GPS + Galileo"" for instance). In urban area, some obstacles (cars, pedestrians, etc) can appear suddenly and thus can induce a random error in the pseudorange measure. We address here the case where the noise probability density functions are of unknown functional form. A flexible Bayesian nonparametric noise model based on Dirichlet process mixtures (DPM) is introduced. Hence, the paper will contain two main parts. The first part focuses on the modelling of the pseudorange noises using DPMs and its suitability in the estimation problem handled by an efficient particle filter. The other part contains interesting validation schemes.",2009,10.1109/ITST.2009.5399308,no
Using Dirichlet Process Mixtures for the Modeling of GNSS Pseudorange Errors in Urban Canyon,"In Global Navigation Satellite Systems (GNSS) positioning, when the signal is received from the satellite in Line of Sight (LOS), the pseudorange error distribution is considered Gaussian. Otherwise, when the signal arrives to the antenna only after one or more reflections, the pseudorange error distribution can be represented by a Gaussian Mixture. In real urban environment, such assumptions are in some way restrictive. In fact, a random error in the pseudorange measure with an unknown distribution form is always induced in constrained environments. In order to ensure high accuracy positioning a good estimation of the observation error in such cases is required. We address here the case where the noise probability density functions are of unknown functional form. A flexible Bayesian nonparametric noise model based on Dirichlet process mixtures (DPM) is introduced. This novel approach will be compared to a Jump Markov System (JMS) based on a finite gaussian mixture modeling. Hence, the paper will contain three main parts. The first part presents a JMS based on a finite gaussian mixture modeling. The second part focuses on the modeling of the pseudorange noises using DPMs and its suitability in the estimation problem handled by an efficient particle filter. The last part contains interesting validation schemes.",2009,,no
A Goal-driven Measurement Model for Software Testing Process,"In order to enhance the software quality, control and improve software testing process, the software testing process needs to be effectively measured. This paper presents a goal-driven measurement model for software testing process (MM4STP). Based on MM4STP, software organizations can deduce the appropriate measurement process according to the process's goals they determine. At the same time, they also can get the direction of testing process, which will provide an effective support for making the correct decision to better the testing process.",2009,10.1109/WCSE.2009.27,no
A Goal-driven Measurement Model for Software Testing Process,"In order to enhance the software quality, control and improve software testing process, the software testing process needs to be effectively measured. This paper presents a goal-driven measurement model for software testing process (MMSTP). Based on MMSTP, software organizations can deduce the appropriate measurement process according to the process goals they determine. At the same time, they also can get the direction of testing process, which will provide an effective support for make the correct decision to better the testing process.",2009,10.1109/IFITA.2009.565,no
Dependability and error manifestation for a web service based on the doubly stochastic model and renewal processes,"In order to success the design and the implementation of web services, both of the reliability and the fault tolerance must be studied. Such studies can be conducted through a formal mathematical model. Many reliability models assume that the failure or the error arrival times are exponentially distributed. This is inappropriate for web services as the error arrival times will be dependent on the operating state including workload of the server where the web service resides. In this manuscript, the reliability modelling based on different environment parameters is studied, considering the case of a web service. This study can be easily extended to deal with the case of appropriate real and complex system. The reliability model is developed based on the Doubly Stochastic model and the Renewal processes. Finally, the Mean Time To Error MTTE is evaluated and the effect of the different parameters on the MTTE evolution are studied.",2009,10.1109/PRDC.2009.37,no
Aerosol size distribution modeling with the Community Multiscale Air Quality modeling system in the Pacific Northwest: 2. Parameterizations for ternary nucleation and nucleation mode processes,"In order to test Community Multiscale Air Quality (CMAQ) model performance for ultrafine particle concentrations in the Pacific Northwest, CMAQ v4.4 was modified for ternary NH3-H2SO4-H2O nucleation and for atmospheric processing of ultrafine particles. Sulfuric acid from sulfur dioxide oxidation is iteratively partitioned into gaseous sulfuric acid, newly condensed aerosol sulfate, and aerosol sulfuric acid contained in new 1 nm particles. Freshly nucleated particles are either coagulated to larger particles or grown by sulfuric acid condensation to 10 nm at which point they are included in CMAQ's existing Aitken mode. Multiple nucleation parameterizations were implemented into CMAQ, and one other was investigated in a sensitivity analysis. For a case study in the Pacific Northwest where aerosol number concentration and size distributions were measured, standard binary nucleation in CMAQ produces nearly no particles for this case study. Ternary nucleation can produce millions of 1 nm particles per cm 3, but few of these particles survive coagulation loss and grow to 10 nm and into the Aitken mode. There are occasions when the additions to CMAQ increase the number of particles to within an order of magnitude of observations, but it is more common for number concentrations to remain underpredicted by, on average, one order of magnitude. Significant particle nucleation in CMAQ successfully produces a distinct Aitken and accumulation mode and an Aitken mode that is more prominent than the accumulation mode, although errors in the size distribution remain. A more recent ternary nucleation scheme including ammonium bisulfate clusters does not nucleate an appreciable number of particles.",2009,10.1029/2009JD012187,no
Material selection diagram: contribution to engineering project quality in process industries,"In recent years, the material selection diagram (MSD) is increasingly becoming one of the important engineering documents generated during development and execution of capital projects in the process industries. The intent of the paper is to provide a brief overview of the role of the MSD on such projects. The paper expands on the MSD development and its relation to other project documents and explores its effect on the project quality. Specific details are discussed on the material selection philosophy as well as corrosion engineering aspects of the selected equipment or equipment components in terms of the scope of information that usually finds its way onto the MSD drawing. Strategies to maintain information consistency between some of the project documents are discussed in greater detail. The paper concludes with an outline of MSD checklists that can contribute to improving client satisfaction, reducing work process cycle time, errors, waste and surprises.",2009,10.1179/174327808X286293,no
MODELING OF ULTRAFILTRATION PROCESS EFFICIENCY IN TREATMENT COKE PLANT WASTEWATER WITH USE INDUSTRIAL MEMBRANES,"In the article there have been presented the results of research whose aim was to determine the effectiveness of treating the coke plant wastewater in the integrated ultrafiltration - reverse osmosis system. In the process pressure membrane filtration was carried out on using the polymer membranes of an American Osmonics brand. Determining ultrafiltration initial permeat fluxes (J(0)) and equilibrium (saturations - J(infinity)) on the experimental road and into the graphic way of the temporary constant (t(0)) enabled based on the relaxation model established, that describes, define the possibility of predicting the polysulfone ultrafiltration membranes efficiency in the process of coke industry wastewater treatment.",2009,,no
New Quality Metrics for Evaluating Process Models,"In the context of business process intelligence, along with the need to extract a process model from a log, there is also the need to measure the quality of the extracted process model. Hence, process model quality notions and metrics are required. We present a systematic approach for developing quality metrics for block structured process models, which offer less expressive power than Petri-nets but have easier semantics. The metrics are based on tagging an initial block structured process model with self-loop and optional markings in order to explain all the instances in the given log. Then we transform the marked model to an equivalent maximal model by rewriting the self-loop and optional markings for consistency, and determine a badness score for it, which determines quality. Our approach is compared with related work, and a plan for testing and validation on noise-free and noisy data is discussed.",2009,,no
Prediction Model of Quality Indices Based-on RBF Neural Network in the Raw Slurry Blending Process,"in the raw slurry blending process, red mud, alkali powder, blending ore and limestone are translated into ball mills to produce the raw slurry whose quality indices include calcium ratio, alkali ratio and water content. The operation control objective of the blending process is to control the quality indices into their targeted ranges. However, in the raw slurry blending process, quality indices can not be measured on-line using the instrument, and it is also difficult to obtain the accuracy model of quality indices. Therefore, the only way to obtain the actual quality indices is the manual chemical examination. For the disadvantage of manual method such as long cycle and low accuracy, it is difficult to realize the operation control objective. To solve this problem, with the integration of the subtractive clustering, RBF neural network and operator's experience, an intelligent prediction model of quality indices of raw slurry is proposed. The application results in some alumina factory have proven the effectiveness of the proposed method.",2009,,no
A THERMOELASTIC UNILATERAL CONTACT PROBLEM WITH DAMAGE AND WEAR IN SOLIDIFICATION PROCESSES MODELING,"In this paper is presents a coupled thermo-elastic contact problem with tribological processes on the contact interface (friction, wear and damage). The unilateral contact between the cilindrical rolls system and a deformable foudation (slab, bloom, etc) is modeled by the Kuhn-Tucker (normal compliance) conditions, involving damage and wear effect of contact surfaces. The continuum tribological model is based on gradient theory of the damage variable for studying crack initiation in fretting fatigue [18], [24], [25], and the wear is described by Archard's law. The friction law that we consider is a regularization of the Coulomb law. The weak formulation of the quasistatic boundary value problem is described by using the variational principle of virtual power, the principles of thermodynamics and variational inequalities theory. Thus, the main results of existence for weak solution are established using a discretization method and a fixed-point strategy [7].",2009,,no
The Model of Software Process Measurement and Improvement Driven by Project Performance,"In this paper on the basis of the existing model, Proposed project performance-driven process improvement framework and P-GQM model. The performance of products and process for process fragments are the metrics for the process objectives. It formalizes description of the achievement, goals, problems and the interrelationship between measurements, to measure results more accurately. Comprehensive reflects the software process or process fragment of the true state of operation, Software process improvement projects and provide a reliable basis, To facilitate timely project manager developed strategy to avoid risks, The model improve the success rate of projects.",2009,,no
Analyzing the Impact of Process Variations on Parametric Measurements: Novel Models and Applications,"In this paper we propose a novel statistical framework to model the impact of process variations on semiconductor circuits through the use of process sensitive test structures. Based on multivariate statistical assumptions, we propose the use of the expectation-maximization algorithm to estimate any missing test measurements and to calculate accurately the statistical parameters of the underlying multivariate distribution. We also propose novel techniques to validate our statistical assumptions and to identify any outliers in the measurements. Using the proposed model, we analyze the impact of the systematic and random sources of process variations to reveal their spatial structures. We utilize the proposed model to develop a novel application that significantly reduces the volume, time, and costs of the parametric test measurements procedure without compromising its accuracy. We extensively verify our models and results on measurements collected from more than 300 wafers and over 25 thousand die fabricated at a state-of-the-art facility. We prove the accuracy of our proposed statistical model and demonstrate its applicability towards reducing the volume and time of parametric test measurements by about 2.5 - 6.1x at absolutely no impact to test quality.",2009,,no
The Unascertained Comprehensive Appraisement Model on Mechanical Process Plans,"In this paper, index distinguishable height, effective value and comparable value of index relative to samples are defined. The conversion from single-index measure to multi-index comprehensive measure is also proposed. Unascertained measure model of multi-hierarchy comprehensive appraisement is built up and used in mechanical process plans.",2009,,no
A Comparison and Analysis of Genetic Algorithm and Particle Swarm Optimization Using Neural Network Models for High Efficiency Solar Cell Fabrication Processes,"In this paper, statistical experimental design is used to characterize the surface texturing and emitter diffusion formation processes for high-performance silicon solar cells. The output characteristics considered are reflectance, sheet resistance, diffusion depth, and cell efficiency. The influence of each parameters affected to efficiency is investigated through the main effect and interaction analysis. Sequential neural network process models are constructed to characterize the entire 3-step process. In the sequential scheme, each work cell sub-process is modeled individually, and each sub-process model is linked to previous sub-process outputs and subsequent sub-process inputs. These neural network models are used for process optimization using both genetic algorithms and particle swarm optimization to maximize cell efficiency. The optimized efficiency found via particle swarm optimization showed better performance than optimized efficiency found via genetic algorithms.",2009,10.1109/FUZZY.2009.5277392,no
Business Process Modelling with Continuous Validation,"In this paper, we demonstrate the prototype of a modelling toot that applies graph-based rules for identifying problems in business process models. The advantages of our approach are twofold. Firstly, it is not necessary to compute the complete state space of the model in order to find errors. Secondly, our technique can even be applied to incomplete business process models. Thus, the modeller can be Supported by direct feedback during the model construction. This feedback does not only report problems, but it also identifies their reasons and makes suggestions for improvements.",2009,,no
Modeling and Evaluating of Emotional Processes,"In this paper, we explain the emotional decision-making model. Firstly, we present an emotional model. The model is designed to explore people's behavior in certain circumstances, while under specified emotional states. Special attention was given to the thought process and actions displayed in the hypothetical scenarios. Secondly, we characterized thoughts and actions associated with each scenario and emotional state. Each particular action or proof of steps taken in the thought process was given a percentage value directly proportional to answers given by the test population. Finally, we developed an experimental game program for the evaluation of our emotional decision making model.",2009,,no
A Markov Process-based Model for the B2B E-commerce Trust Evaluation,"In this paper, we explore the trust evaluation system of the enterprise in B2B e-commerce business and establish a trust evaluation model based on the Markov process, which includes time, the heavy-weight of transactions and the reputation of buying agents as the restraining variables. They control the five effect factors for building a stably credit evaluation model. It's confirmed that the Markov process-based model for the B2B e-commerce trust evaluation has better stability and the ability for anti-jamming through the simulation. The result of the experiment indicates we calculate the trust of selling agents factually by the trust evaluation model of this paper and reduce the risk of transaction.",2009,10.1109/ICIE.2009.65,no
A Heuristic Method for Business Process Model Evaluation,"In this paper, we present a heuristic approach for finding errors and possible improvements in business process models. First, we translate the information that is included in a model into a set of Prolog facts. We then search for patterns which are related to a violation of the soundness property, bad modeling style or otherwise give raise to the assumption that the model should be improved. By testing our approach on a large repository of real-world models, we found that the heuristic approach identifies violations of the soundness property almost as accurate as model-checkers that explore the state space of all possible executions of the model. Other than these tools, our approach never ran into state-space explosion problems. Furthermore, our pattern system can also detect patterns for bad modeling style which can help to improve the quality of the models.",2009,,no
Error control process in function interpolation using statistical spline model,"In this paper, we propose an algebraic method based on solving some in equalities of polynomial type to control the error value of interpolation formulas whose residue depends on amonic polynomial. This method then leads to construct some piece wise approximations (splines) of statistical type, which are based on a specific partition of the main interval. In other words, in this model of spline, approximate criteria are considered fixed and sub-intervals corresponding to criteria are derived as accurately as possible. In this sense, some statistical concepts such as expected value, variance measure, skewness and kurtosis coefficients are also inserted into the definition of statistical splines. Finally, a numerical results section is separately given to confirm all results in the paper. (C) 2009 Elsevier Ltd. All rights reserved",2009,10.1016/j.mcm.2008.11.012,no
Model-based process analysis of partial nitrification efficiency under dynamic nitrogen loading,"In this study, the ammonia removal efficiency for high ammonia-containing wastewaters was evaluated via partial nitrification. A nitrifier biocommunity was first enriched in a fill-and-draw batch reactor with a specific ammonium oxidation rate of 0.1 mg NH(4) (-)-N/mg VSS.h. Partial nitrification was established in a chemostat at a hydraulic retention time (HRT) of 1.15 days, which was equal to the sludge retention time (SRT). The results showed that the critical HRT (SRT) was 1.0 day for the system. A maximum specific ammonium oxidation rate was achieved as 0.280 mg NH(4) (-)-N/mg VSS.h, which is 2.8-fold higher than that obtained in the fill-and-draw reactor, indicating that more adaptive and highly active ammonium oxidizers were enriched in the chemostat. Dynamic modeling of partial nitrification showed that the maximum growth rate for ammonium oxidizers was found to be 1.22 day(-1). Modeling studies also validated the recovery period as 10 days.",2009,10.1007/s00449-008-0289-2,no
VIRTUAL MODELING OF CONCEPT GENERATION PROCESS FOR UNDERSTANDING AND ENHANCING THE NATURE OF DESIGN CREATIVITY,"In this study, we attempt to capture the nature of the concept generation process by finding an effective thinking pattern for creativity. We consider a space composed of chain processes of concepts that are both explicitly evoked in the concept generation process and inexplicitly imaged as a thinking space, and we focus on two viewpoints-structure and latent sensitivities. The former refers to the structure of the thinking space; the latter, the latent concepts that are inexplicitly imaged in the thinking space. From these viewpoints, we propose a method for modeling a virtual thinking space using a semantic network, and we quantitatively analyze its pattern. As results, we found that there is a significant correlation between the structure and the creativity and that the model could clarify the nature of design creativity in the concept generation process. These findings suggest that factors that affect the evaluated creativity score for design idea are closely related to the structure of the thinking space. This indicates that there might exist an effective thinking pattern for creativity; thus, this model could be used for enhancing design creativity.",2009,,no
A Process Model for Group Decision Making with Quality Evaluation,"In this work it is addressed the problem of information evaluation and decision making process in Group Decision Support Systems (GDSS). A Multi-valued Extended Logic Programming language is used for imperfect information representation and reasoning. A model embodying the quality evaluation of the information, along the several stages of the decision making process, is presented. This way we give the decision makers a measure of the value of the information that supports the decision itself. This model is presented in the context of a GDSS for VirtualECare, a system aimed at sustaining online healthcare services. Reasoning with incomplete and uncertain knowledge has to be dealt with in this kind of environment, due to the particular nature of the healthcare services, where the awful consequences of bad decisions, or lack of timely ones, demand for a responsible answer.",2009,,no
Atomistic Modeling of Junction Formation: Tools for Physics Understanding and Process Optimization,In this work we describe the basis of different atomistic simulation techniques and their application for process modeling. Using a Kinetic Monte Carlo simulator we analyze the influence of implant flux on the position of the amorphous/crystalline interface and the amount of residual damage. We also illustrate the challenges for doping narrow finFET devices. Atomistic modeling provides insight into the physical mechanisms and clues for process optimization.,2009,10.1149/1.3204432,no
"Measurement and Modeling Process Partitioning of Cephalexin Antibiotic in Aqueous Two-Phase Systems Containing Poly(ethylene glycol) 4000, 10000 and K2HPO4, Na(3)Citrate","In this work, the partition coefficients of Cephalexin in aqueous two-phase systems containing PEG [poly(ethylene glycol)] 4000, 10000 and K2HPO4, Na(3)Citrate (C6H5Na3O7 5, 5H(2)O) have been measured. The experimental data were obtained in a wide range of temperatures, (28.2 to 37.2) degrees C. The effects of temperature, pH, polymer concentration, polymer molecular weight, and salt concentration on the partitioning of Ceohalexin were also studied. The results showed that salt concentration has a large effect on the partition coefficient, and temperature has an almost negligible effect. The Chen-NRTL Gibbs energy model was used to Correlate the experimental results.",2009,10.1021/je800996j,no
Evaluating a Dual-Process Model of Risk: Affect and Cognition as Determinants of Risky Choice,"In three studies we addressed the impact of perceived risk and negative affect oil risky choice. In Study 1, we tested a model that included both perceived risk and negative affect as predictors of risky choice. Study 2 and Study 3 replicated these findings and examined the impact of affective versus cognitive processing modes. In all the three studies, both perceived risk and negative affect were shown to be significant predictors of risky choice. Furthermore, Study 2 and Study 3 showed that an affective processing, mode strengthened the relation between negative affect and risky choice and that a cognitive processing mode strengthened the relation between perceived risk and risky choice. Together. these findings show support for the idea of a dual-process model of risky choice. Copyright (C) 2008 John Wiley & Sons, Ltd.",2009,10.1002/bdm.610,no
Evaluating Modeling Sessions Using the Analytic Hierarchy Process,"In tins paper, which is methodological in nature, we propose to use an established method from the field of Operations Research, the Analytic Hierarchy Process (AHP), in the integrated, stakeholder-oriented evaluation of enterprise modeling sessions: their language, process, tool (medium), and products. We introduce the AHP and briefly explain its mechanics. We describe the factors we take into consideration, and demonstrate the approach at the hand of a case example we devised based on a semi-realistic collaborative modeling session. The method proposed is to be a key part of a larger setup: a ""laboratory"" for the study of operational (i.e. real) modeling sessions and related study and development of methods and tools deployed in them.",2009,,no
A Perspective-based Model of Quality for Software Engineering Processes,"Increasingly, software engineering organisations are defining and implementing processes as a means to support, guide and control project execution. An assumption underlying this process-centric approach to business improvement is that the quality of the process will influence the quality, cost and time-to-release of the software produced. Given this presumed relationship, a critical question arises of what constitutes quality for software engineering processes. This paper describes the results of research undertaken to investigate this question and presents a perspective-based model of quality for software engineering processes that is derived from the stated experiences of software engineering practitioners. The model proposes that practitioners perceive the overall quality of a process with respect to the four quality attributes of suitability, usability, manageability and evolvability and that these judgements are influenced by key process properties and environmental factors. The paper also suggests how, knowledge of these quality attributes, properties, environmental factors and their relationships can be practically applied to support software process engineering activities.",2009,10.1109/ASWEC.2009.40,no
Cognitive abilities and superior decision making under risk: A protocol analysis and process model evaluation,"Individual differences in cognitive abilities and skills can predict normatively superior and logically consistent judgments and decisions. The current experiment investigates the processes that mediate individual differences in risky choices. We assessed working memory span, numeracy, and cognitive impulsivity and conducted a protocol analysis to trace variations in conscious deliberative processes. People higher in cognitive abilities made more choices consistent with expected values; however, expected-value choices rarely resulted from expected-value calculations. Instead, the cognitive ability and choice relationship was mediated by the number of simple considerations made during decision making - e. g., transforming probabilities and considering the relative size of gains. Results imply that, even in simple lotteries, superior risky decisions associated with cognitive abilities and controlled cognition can reflect metacognitive dynamics and elaborative heuristic search processes, rather than normative calculations. Modes of cognitive control (e. g., dual process dynamics) and implications for process models of risky decision-making (e. g., priority heuristic) are discussed.",2009,,no
Validating Circuit Board Interconnect Stress Test Preconditioning Processes Using Statistical Model Comparisons of Accelerated Test Data,"Interconnect stress testing (IST) is used for acceptance of product by testing coupons fabricated along with the production panels to determine the long-term reliability of the plated thru-holes. Prior to in-service use, the actual production circuit boards are subjected to Circuit Card Assembly processes like soldering components, but simulated on coupons using preconditioning cycles (PCCs). In this paper, the Circuit Card Assembly preconditioning process is compared with IST PCCs. Accelerated test data for this study use IST preconditioning and Re-Flow Oven preconditioning at an independent company laboratory from coupons supplied by three fabrication suppliers. The data indicate that a Weibull distribution be applied for statistical model comparisons using the likelihood-ratio test on both types of preconditioning processes with coupons that are then subjected to subsequent in-service use IST cycles until failure occurs. Results show that using six IST PCCs are statistically equivalent to six Re-Flow Oven PCCs and thus five IST PCCs currently in use should be reconsidered, but the advantages of effective IST preconditioning lower acceptance costs significantly. In addition, the statistical results are validated by root mean square power calculations. Copyright (C) 2009 John Wiley & Sons, Ltd.",2009,10.1002/qre.1008,no
Evaluating shared workspace performance using human information processing models,"Introduction. Shared workspace evaluation can be expensive and time consuming. It is also usually oriented towards high-level qualitative perspectives of the collaboration among users. A quantitative method is presented, giving emphasis to the low-level details of critical scenarios of shared workspace interaction, and allowing for comparisons of predicted execution time. Method. Models of human information processing are used to approximate human behaviour while working through a shared workspace. Generic groupware input/output devices and information flows are categorised. Analysis. Three cases of shared workspace activity are analysed. For each case, two or more design scenarios are evaluated and their performance compared. Results. The method contributes to formative evaluation regarding the manipulation of coupling mechanisms and the timing and availability of group awareness information, and it offers indications about the potential performance of users working with shared workspaces. Conclusions. The proposed method is aligned with the century-old need to measure before improving. It is aimed at providing the groupware designer with a tool to make quick calculations, enabling several design iterations, without requiring users or functional prototypes.",2009,,no
Using Security Metrics Coupled with Predictive Modeling and Simulation to Assess Security Processes,"It is hard for security practitioners and decision-makers to know what level of protection they are getting from their investments in security, especially when they have invested in a number of technologies and processes which interact and combine together. It is even harder to estimate how well these investments can be expected to protect their organizations in the future as security policies, regulations and the threat environment are constantly changing. In this paper we propose that for measuring the effectiveness of security processes in large organizations, a greater emphasis needs to be put on process-based metrics, in contrast to the more commonly used symptomatic lagging indicators. We show, by means of two case studies, how these process-based metrics can be combined with executable, predictive models, based on a sound mathematical foundation, to both assess organizations' security processes under current conditions and predict how well they are likely to perform in potential future scenarios, which may include changes in working practices, policies or threat levels, or new investments in security. We present two case studies, in the areas of vulnerability threat management, and identity and access management, as significant examples to illustrate how this modeling and simulation-based approach can be used to provide a rich picture of how well existing security processes are protecting the organization and to answer ""what-if"" questions, such as exploring the effects of a change in security policy or an investment in new security technology. Our approach enables the organization to apply the metrics that are most relevant to its business, and provide a comprehensive view that shows the benefits and losses to the different stakeholders.",2009,,no
Research on Efficiency of Information Transfer in Cardiac Emergency Process Based on Markov Hybrid Model,"It is the base of a successive cardiac emergency (CE) treatment to architect a rapid cardiac emergency information transfer process with high efficiency and high quality. Aiming at the shortage of quantitative method to diagnose the process of CE, a hybrid method was presented, in which time Petri nets (TPN) was used to model the process of CE and ABC-Markov process was adopted to diagnose quantitatively. As a result, the bottlenecks of the efficiency of information transfer of CE were found. And on this base, the bottlenecks were optimized based on the theory of BPR. As a result, the time of CE treatment was shortened and the success rate was raised.",2009,10.1109/ICIEEM.2009.5344323,no
Improving the quality of process reference models: A quality function deployment-based approach,"Little academic work exists on managing reference model development and measuring reference model quality, yet there is a clear need for higher quality reference models. We address this gap by developing a quality management and measurement instrument. The foundation for the instrument is the well-known Quality Function Deployment (QFD) approach. The QFD-based approach incorporates prior research on reference model requirements and development approaches. Initial evaluation of the instrument is carried out with a case study of a logistic reference process. The case study reveals that the instrument is a valuable tool for the management and estimation of reference model quality. (C) 2009 Elsevier B.V. All rights reserved.",2009,10.1016/j.dss.2008.12.006,no
Process model-free analysis for thermodynamic efficiencies of sulfur-iodine processes for thermochemical water decomposition,"Material, energy, and entropy balances, which depend only on stream conditions and flows entering and leaving a system, have been used to evaluate different scenarios for thermochemical decomposition of water to manufacture hydrogen using the Sulfur-Iodine cycle. Energy efficiencies have been found for idealized systems with variable stream amounts, as well as for a common flowsheet, to locate the greatest effects on energy requirements and inefficiencies. Aspen Plus(R), OLI Engine, and ProSimPlus property models have been used on the sections of a General Atomics process to reveal the effects of differences in computed energies and entropy generation. While the calculated efficiencies are generally consistent with those of the literature, differences in stream properties and phase behaviors suggest that optimal process configurations from simulations may have significant uncertainties. (C) 2008 International Association for Hydrogen Energy. Published by Elsevier Ltd. All rights reserved.",2009,10.1016/j.ijhydene.2008.08.024,no
Development and evaluation of two ELISA formats for the detection of beta-lactoglobulin in model processed and commercial foods,"Milk proteins have many functional properties, making them valuable food ingredients. However, they are a frequent cause of food allergy, especially in children. The aim of the present work was to develop and evaluate two immunoassays (indirect competitive and double antibody sandwich formats) for the detection of undeclared milk proteins in foods. Antisera raised to P-lactoglobulin was used in the competitive assay and antibodies isolated by immunoadsorption in the sandwich ELISA. Results obtained indicated that the sandwich format could detect lower percentages of powdered milk added to processed model foods than the competitive format. Likewise, the sandwich assay could discriminate better than the competitive assay between commercial foods with declared or non-milk ingredients. The antibody population used to recognize the target protein as well as the particularities of each ELISA format greatly influence the determination of immunoreactive P-lactoglobulin present in processed food. (c) 2008 Elsevier Ltd. All rights reserved.",2009,10.1016/j.foodcont.2008.09.017,no
"Empirical Investigations of Model Size, Complexity and Effort in a Large Scale, Distributed Model Driven Development Process","Model driven development (MDD) is a software engineering practice that is gaining in popularity We aim to investigate to what extend it is effective. There is a lack of empirical data to verify the pay-offs of employing MDD tools and techniques. In order to increase the knowledge we have of the impact of MDD in large scale industrial projects, we investigate the project characteristics of a large software development project in which MDD is used in a pure form. This study focuses on analyzing model size and complexity and metrics related to model quality and effort. Furthermore, project team members were asked to elaborate on their views on the impact of using MDD. Our findings include that larger models are more complex, contain more diagrams, are changed more often and worked on longer but do not necessarily contain more defects. However, models that are changed often do contain more defects. Benefits mentioned by team members were an increase in productivity, benefits from a consistent implementation and their perception of improvement of overall quality. Also, a reduction in complexity was attributed to the use of MDD techniques. We could confirm the perceived increase in the quality of the product in that the average amount of defects found is significantly lower than in similar size projects in which MDD was not employed.",2009,10.1109/SEAA.2009.70,no
Process model development and validation using artificial neural networks,"Modelling and simulation are important analysis and presynthesis tools in engineering. Flat end milling processes are accurately modelled using Artificial Neural Networks (ANNs) and real experimentation. ANNs are expensive techniques, as they require enormous experiments over ranges of input parameters. This is crucial to any realistic modelling. Orthogonal Arrays (OAs) and Design of Experiments (DOEs) are used to remedy the modelling expense using Neural Networks (NNs). The high cost of ANNs is offset by combining DOEs with ANNs to model part of the domain instead of the full domain. The process variables include depth of cut (a), spindle speed (n), feed rate (f) and tool diameter (d). Our interest is in measuring the dynamic variations of the cutting forces and their time behaviour. Several experimental models are developed, including two-level, three-level, four-level and five-level OA-based models. The result is a valid neural model for flat end milling over realistic domains of process parameters. [Submitted 21 February 2008; Revised 07 May 2008; Accepted 23 June 2008]",2009,10.1504/EJIE.2009.021586,no
Checking Correctness and Compliance of Integrated Process Models,"Models of different kinds are used in the area of business process management. Abstract process knowledge as well as executable process definitions call be visualized and edited in a graphical manner The same holds true for models of process instances in some process-aware information systems (PAIS), which allow for dynamic modifications in a process instance during process runtime. An appropriate modeling tool must not only provide means to graphically edit different kinds of models oil different abstraction layers but also detect or prevent violations of certain model constraints. These constraints comprise a model's internal correctness as well as a model's compliance with general process knowledge expressed in another more abstract model. In this paper we contribute an approach that allows for uniformly specifying correctness as well as compliance checks for diverse graphical process models. This is achieved by means of all integrated meta-model for models of different kinds. The integrated meta-model is referenced by a fixed set of Object Constraint Language (OCL) expressions, which specify, correctness and compliance checks. We exemplify, some OCL expressions and their application within a prototypical modeling tool.",2009,10.1109/SYNASC.2008.10,no
Modeling Process Integrated Quality Management System in Manufacturing Enterprises,"Modern quality management focuses on all processes that affect product quality level instead of some single process. It is necessary to develop a process integrated quality management system for manufacturing enterprises. In this paper the architecture of process integrated quality management system is put forward which embodies the whole supply chain quality. The enterprise integration information system is modeled based on ERP/MES/Control framework, and a prototype for a steel manufacturing company is developed.",2009,10.1109/FITME.2009.25,no
Agreement attraction in comprehension: Representations and processes,"Much work has demonstrated so-called attraction errors in the production of subject-verb agreement (e.g.,'The key to the cabinets are on the table', [Bock, J. K., & Miller, C. A. (1991). Broken agreement. Cognitive Psychology, 23, 45-93]), in which a verb erroneously agrees with an intervening noun. Six self-paced reading experiments examined the online mechanisms underlying the analogous attraction effects that have been shown in comprehension; namely reduced disruption for subject-verb agreement violations when these nouns intervene. One class of theories suggests that these effects are rooted in faulty representation of the number of the subject, while another class of theories suggests instead that such effects arise in the process of re-accessing subject number at the verb. Two main findings provide evidence against the first class of theories. First attraction also, occurs in relative clause configurations in which the attractor noun does not intervene between subject and verb and is not in a direct structural relationship with the subject head (e.g., 'The drivers who the runner wave to each morning'). Second, we observe a. grammatical asymmetry': attraction effects are limited to ungrammatical sentences, which would be unexpected if the representation of subject number were inherently prone to error. We argue that agreement attraction in comprehension instead reflects a cue-based retrieval mechanism that is subject to retrieval errors. The grammatical asymmetry can be accounted for under one implementation that we propose, or if the mechanism is only called upon when the predicted agreement features fail to be instantiated on the verb. (C) 2009 Elsevier Inc. All rights reserved.",2009,10.1016/j.jml.2009.04.002,no
GAUSSIAN PROCESS MODELLING OF DEPENDENCIES IN MULTI-ARMED BANDIT PROBLEMS,"Multi-armed bandit problems, in analogy with slot machines in casinos, are problems in which one has to choose actions sequentially (pull arms) in order to maximise a cumulated reward (gain), with no initial knowledge on the distribution of actions/arms' rewards. We propose a general framework for handling dependencies across arms, based on a new assumption on the mean-reward function which is that it is drawn from a Gaussian Process (GP), with a given arm covariance matrix. We show on a toy problem that this allows to perform better than the popular UCB bandit algorithm, which considers arms to be independent.",2009,,no
Observation of Asian dust and air-pollution aerosols using a network of ground-based lidars (ADNet): Realtime data processing for validation/assimilation of chemical transport models,"Network observations of Asian dust and air-pollution aerosols are being conducted with automatic two-wavelength (1064nm, 532nm) polarization (532nm) lidars. Currently, the lidars are operated at 20 locations including cooperative stations. Aiming at real-time validation/assimilation of chemical transport models, an automatic data processing system was developed. A method using the lidar depolarization ratio is employed to estimate the extinction coefficient profiles of Asian dust and spherical aerosols. At the same time, the attenuated backscattering coefficients at 1064nm and 532nm, and the total depolarization ratio at 532nm are derived. A data assimilation system based on 4-dimensional variational (4DVAR) method was developed for the lidar-network Asian dust data. In the assimilation, the dust emission factor was used as the control parameter. The results show the data assimilation is effective not only for improving the model results but also for estimating the emission in the dust source region.",2009,10.1088/1755-1307/7/1/012003,no
"Release Engineering Processes, Models, and Metrics","No matter the development process or methodology, a software product must ultimately be released to a user in a readily consumable form. Different software products, from desktop applications to web services, may require different release processes, but each process must produce an artifact which meets an expected level of quality, and is relatively bug-free. We describe current research to model and quantify existing release processes, and an effort to prescribe improvements to those processes.",2009,,no
Integrating peatlands and permafrost into a dynamic global vegetation model: 1. Evaluation and sensitivity of physical land surface processes,"Northern peatlands and permafrost soils are associated with large carbon stocks. Rising temperatures are likely to affect the carbon balance in high-latitude ecosystems, but to what degree is uncertain. We have enhanced the Lund-Potsdam-Jena (LPJ) dynamic global vegetation model by introducing processes necessary to simulate permafrost dynamics, peatland hydrology, and peatland vegetation. The new version, LPJ-WHy v1.2, was used to study soil temperature, active layer depth, permafrost distribution, and water table position. Modeled soil temperatures agreed well with observations, apart from a Siberian site where the soil is insulated by an extensive shrub layer. Water table positions were generally in the range of observations, with some exceptions. Simulated active layer depth showed a mean absolute error of 44 cm when compared to observations, but the error was reduced to 25 cm when the soil type for seven sites was manually corrected to mirror local conditions. A sensitivity test, in which temperature and precipitation were varied independently, showed that soil temperatures and active layer depths increased more under higher temperatures when precipitation was increased at the same time. The sensitivity experiment suggested persisting wet conditions in peatlands even under temperature increases of up to 9 degrees C as long as annual precipitation is allowed to increase with temperature to the extent indicated by climate model experiments.",2009,10.1029/2008GB003412,no
Mental Models in Process Visualization - Could They Indicate the Effectiveness of an Operator's Training?,"Nowadays process plant visualizations and operations take place without the operator's physical presence at the technical device. As a conesquence a lot of complex systems must be visualized simultaneously on one or more monitors. Conventional two-dimensional mail machine interfaces hardly meet the requirements of those increasing complexity Of production processes. One approach to deal with the increasing, number of faults during process plant monitoring is the creation and implementation of 3D visualizations. We examined the development of mental models with 2D and 3D visualizations and different forms of training (freeze image vs. slider vs. slider with interaction) regarding completeness and structure as well as the relation of the quality of problem solving and the accurate recognizing of critical Situations. Additionally, we investigated the mental demand in different groups Of Visualization and training.",2009,,no
BAYESIAN MODELS AND STOCHASTIC PROCESSES APPLIED TO CSP SAMPLING PLANS FOR QUALITY CONTROL IN PRODUCTION IN SERIES AND BY LOTS,"Nowadays, businesses consider that their methods are perfect, this means, that by having available a department of analysis and statistic control of the process, everything that the inspector or the inspection tools decides are considered to be correct, with not even a minimum of error involved. Yet, if they considered the principles of uncertainty of Heisenberg, in which he believes that the uncertainty associated to the observation, does not contradict the existence of laws that govern the behavior of the particles in the universe, not even the capacity of the scientists to discover those laws, which will be seen as precise predictions, which can be substituted by the calculations of probabilities. This investigation focuses on the study of CSP sampling plans for acceptance with Bayesian and Markovian revisions, in the processes of production in series and by lots, that support the quality activities and reduction of costs by inspection.",2009,,no
Relative Efficiency of Single-Outlier Discordancy Tests for Processing Geochemical Data on Reference Materials and Application to Instrumental Calibrations by a Weighted Least-Squares Linear Regression Model,"Numerous studies report geochemical data on reference materials (RMs) processed by outlier-based methods that use univariate discordancy tests. However, the relative efficiency of the discordancy tests is not precisely known. We used an extensive geochemical database for thirty-five RMs from four countries (Canada, Japan, South Africa and USA) to empirically evaluate the performance of nine single-outlier tests with thirteen test variants. It appears that the kurtosis test (N15) is the most powerful test for detecting discordant outliers in such geochemical RM databases and is closely followed by the Grubbs type tests (N1 and N4) and the skewness test (N14). The Dixon-type tests (N7, N8, N9 and N10) as well as the Grubbs type test (N2) depicted smaller global relative efficiency criterion values for the detection of outlying observations in this extensive database. Upper discordant outliers were more common than the lower discordant outliers, implying that positively skewed inter-laboratory geochemical datasets are more frequent than negatively skewed ones and that the median, a robust central tendency indicator, is likely to be biased especially for small-sized samples. Our outlier-based procedure should be useful for objectively identifying discordant outliers in many fields of science and engineering and for interpreting them accordingly. After processing these databases by single-outlier discordancy tests and obtaining reliable estimates of central tendency and dispersion parameters of the geochemical data for the RMs in our database, we used these statistical data to apply a weighted least-squares linear regression (WLR) model for the major element determinations by X-ray fluorescence spectrometry and compared the WLR results with an ordinary least-squares linear regression model. An advantage in using our outlier procedure and the new concentration values and uncertainty estimates for these RMs was clearly established.",2009,10.1111/j.1751-908X.2009.00885.x,no
Validation of Knowledge Acquisition for Surgical Process Models,"Objective: Surgical Process Models (SPMs) are models of surgical interventions. The objectives of this study are to validate acquisition methods for Surgical Process Models and to assess the performance of different observer populations. Design: The study examined 180 SPM of simulated Functional Endoscopic Sinus Surgeries (FESS), recorded with observation software. About 150,000 single measurements in total were analyzed. Measurements: Validation metrics were used for assessing the granularity, content accuracy, and temporal accuracy of structures of SPMs. Results: Differences between live observations and video observations are not statistically significant. Observations performed by subjects with medical backgrounds gave better results than observations performed by subjects with technical backgrounds. Granularity was reconstructed correctly by 90%, content by 91%, and the mean temporal accuracy was 1.8 s. Conclusion: The study shows the validity of video as well as live observations for modeling Surgical Process Models. For routine use, the authors recommend live observations due to their flexibility and effectiveness. If high precision is needed or the SPM parameters are altered during the study, video observations are the preferable approach.",2009,10.1197/jamia.M2748,no
"A MODULAR DIFFERENTIAL DIELECTRIC SENSOR (DDS) FOR USE IN MULTIPHASE SEPARATION, PROCESS MEASUREMENT AND CONTROL - PART I: ANALYTICAL MODELING","Oil industry increasingly demands accurate and stable continuous measurement of the percent water in crude oil production streams (watercut) over the entire 0 to 100% range. High accuracy and stability are also required for surface measurement to support niche applications such as control of processes which remove trace amounts of oil and particulates from produced water prior to disposal. Differential Dielectric Sensors (DDS) have been developed by Chevron as independent tools connected with multiphase meters for process management and composition measurement. This paper is a two-part paper - the first part (current paper) deals with analytical modeling of the DDS (configured in a single ended mode) and the second part discusses the results of key experimental investigations obtained in a differential mode. The main objective of this paper is to develop appropriate mathematical models for the DDS which characterize the microwave attenuation and phase shift as a function of fluid properties, sensor geometry and operational conditions. Forward models based on the analysis of microwave propagation have been developed for sensors configured as circular waveguides. Results of this project will be useful for optimization and refinement of multiphase meters.",2009,,no
Intelligent Measurement System Based on Soft-sensor Model and Its Applications in Fermentation Process,"On-line measurement of the biomass parameters has become an urgent need for the optimal control for the fermentation process. An intelligent measurement system based on soft-sensor model is proposed. The system structure and concrete realization are presented in detail. The system has integrated the soft-sensor technology and the key biomass parameters can be obtained using the soft-sensor models, which are established based on the measurable variables. This intelligent system has been used in the L-lactic acid fermentation process measurement system and the key biomass parameter can be estimated on-line. In addition, it makes the real-time fermentation process closed loop optimal control become possible.",2009,,no
Modeling nutrient in-stream processes at the watershed scale using Nutrient Spiralling metrics,"One of the fundamental problems of using large-scale biogeochemical models is the uncertainty involved in aggregating the components of fine-scale deterministic models in watershed applications, and in extrapolating the results of field-scale measurements to larger spatial scales. Although spatial or temporal lumping may reduce the problem, information obtained during fine-scale research may not apply to lumped categories. Thus, the use of knowledge gained through fine-scale studies to predict coarse-scale phenomena is not straightforward. In this study, we used the nutrient uptake metrics defined in the Nutrient Spiralling concept to formulate the equations governing total phosphorus in-stream fate in a deterministic, watershed-scale biogeochemical model. Once the model was calibrated, fitted phosphorus retention metrics where put in context of global patterns of phosphorus retention variability. For this purpose, we calculated power regressions between phosphorus retention metrics, streamflow, and phosphorus concentration in water using published data from 66 streams worldwide, including both pristine and nutrient enriched streams. Performance of the calibrated model confirmed that the Nutrient Spiralling formulation is a convenient simplification of the biogeochemical transformations involved in total phosphorus in-stream fate. Thus, this approach may be helpful even for customary deterministic applications working at short time steps. The calibrated phosphorus retention metrics were comparable to field estimates from the study watershed, and showed high coherence with global patterns of retention metrics from streams of the world. In this sense, the fitted phosphorus retention metrics were similar to field values measured in other nutrient enriched streams. Analysis of the bibliographical data supports the view that nutrient enriched streams have lower phosphorus retention efficiency than pristine streams, and that this efficiency loss is maintained in a wide discharge range. This implies that both small and larger streams may be impacted by human activities in terms of nutrient retention capacity, suggesting that larger rivers located in human populated areas can exert considerable influence on phosphorus exports from watersheds. The role of biological activity in this efficiency loss showed by nutrient enriched streams remained uncertain, because the phosphorus mass transfer coefficient did not show consistent relationships with streamflow and phosphorus concentration in water. The heterogeneity of the compiled data and the possible role of additional inorganic processes on phosphorus in-stream dynamics may explain this. We suggest that more research on phosphorus dynamics at the reach scale is needed, specially in large, human impacted watercourses.",2009,,no
A two-warehouse inventory model with imperfect quality production processes,"One of the weaknesses of some production-inventory models is the unrealistic assumption that all items produced are of good quality. But production of defective units is a natural phenomenon in a production process. Defective items should be treated as a result of imperfect quality production. On the other hand, the classical inventory models usually assume the available warehouse has unlimited capacity. In many practical situations, there exist many factors like temporary price discounts making retailers buy a capacity of goods exceeding their own warehouse. In this case, retailers may rent other warehouses for the need of business. A lot of researchers studied inventory models with two warehouses and inventory models with imperfect quality separately. Compared with previous models, based on Salameh and Jaber [Salameh, M. K., Jaber. M. Y. (2000). Economic production quantity model for items with imperfect quality. International journal of Production Economics, 64, 59-64.], this paper tries to incorporate the above concepts to establish a new inventory model with two warehouses and imperfect quality simultaneously. The mathematical model by maximizing the annual total profit and the solution procedure are developed and numerical examples are provided to illustrate them. (C) 2008 Elsevier Ltd. All rights reserved.",2009,10.1016/j.cie.2008.05.005,no
Eliminating the Effect of Multivariate Outliers in PLS-Based Models for Inferring Process Quality,"Outliers in multivariate data demand special attention in data-driven process modeling. Their extremeness usually gives them an excessively high influence in the calculation, which may result in a less precise model. It is challenging to detect them using existing univariate approaches. A novel robust modeling method is presented; this PLS based modeling procedure not only alleviates the harmful effect of multivariate outliers, but also retains the information necessary for building a robust model from the training data. The performance of the proposed approach is compared with conventional strategies using an actual industrial case study.",2009,,no
A PART ERROR MODEL CONSIDERING THE MACHINE TOOL POSITIONING ERRORS AND PROCESS-INDUCED ERRORS,"Parts geometrical and dimensional error for a machining process can be attributed to several factors, including tool wear, thermal deformation, the machine tool positioning error and force-induced process error. Although the latter two factors are often more significant, their effect on the parts accuracy is more elusive and difficult to predict due to their inherent statistical dispersion property. It is therefore the subject of this investigation to quantitatively relate the parts error to machine tool spatial error and process-induced errors. Through root mean square calculation, a part error model is established by combining the machine tool positioning error, work vibration and tool vibration. The part error model considers two ranges of surface error consisting of surface roughness and cutting depth error of a machined plate. Using milling process as an example, the part error is predicted and compared with measurement result. The validity of this model is verified through a series of milling experiments under various cutting conditions.",2009,,no
"Effects of peat compaction on delta evolution: A review on processes, responses, measuring and modeling","Peat is most compressible of all natural soils. Compaction of peat layers potentially leads to substantial amounts of land subsidence. Peat is common in many distal parts of Holocene deltas, which are often densely populated. It is known that land subsidence due to peat compaction may have serious societal implications in such areas, as it may cause damage to construction works and lead to land inundation. Effects of peat compaction on the natural evolution of deltas are however poorly Understood, whereas this might be all important control oil delta evolution at both local and regional scales. The main objective of this paper is to review current knowledge concerning the peat compaction process and its effect oil delta evolution in Holocene settings, and to identify gaps in this knowledge. An overview is given regarding: 1) the compaction process, 2) presumed and potential effects of peat compaction oil delta evolution. 3) field Methods to quantify peat compaction and 4) numerical models to calculate the amount and rate of peat compaction. Peat compaction and formation influence! channel belt elevation. channel belt geometry and channel belt configuration. Last-mentioned aspect mostly concerns the influence of peat compaction oil avulsion, Which is one of the most important processes controlling delta evolution. Interactions between peat compaction, peat formation and avulsion have seldom been Studied and remain unclear, partly because factors Such as peat type, organic matter content, sediment sequence composition and groundwater table fluctuation are so far not taken into account. Peat compaction and formation Potentially influence avulsion as 1) a decrease in accommodation space created by peat compaction Underneath a channel Causes superelevation and/or in increase in lateral migration. 2) the high cohesiveness of peat banks inhibits lateral migration, which increases bed aggradation, decreases sediment transport capacity and hence increases crevassing frequencies, which possibly evolve into ail avulsion, although the low regional gradient in peatlands will hinder this, and 3) peat compaction and oxidation in flood basins following groundwater table lowering leads to relief amplification of channel belts. At delta scale, variations ill compaction tales might stimulate the Occurrence of nodal avulsions. To quantify effects of peat compaction on delta evolution, and to determine the relative importance of different factors involved, held research Should be combined with numerical models describing peat compaction and formation. The model should be validated and calibrated with field data. (c) 2008 Elsevier B.V. All rights reserved.",2009,10.1016/j.earscirev.2008.11.001,no
Integrating peatlands and permafrost into a dynamic global vegetation model: 2. Evaluation and sensitivity of vegetation and carbon cycle processes,"Peatlands and permafrost are important components of the carbon cycle in the northern high latitudes. The inclusion of these components into a dynamic global vegetation model required changes to physical land surface routines, the addition of two new peatland-specific plant functional types, incorporation of an inundation stress mechanism, and deceleration of decomposition under inundation. The new model, LPJ-WHy v1.2, was used to simulate net ecosystem production (NEP), net primary production (NPP), heterotrophic respiration (HR), and soil carbon content. Annual peatland NEP matches observations even though the seasonal amplitude is overestimated. This overestimation is caused by excessive NPP values, probably due to the lack of nitrogen or phosphorus limitation in LPJ-WHy. Introduction of permafrost reduces circumpolar (45-90 degrees N) NEP from 1.65 to 0.96 Pg C a(-1) and leads to an increase in soil carbon content of almost 40 Pg C; adding peatlands doubles this soil carbon increase. Peatland soil carbon content and hence HR depend on model spin-up duration and are crucial for simulating NEP. These results highlight the need for a regional peatland age map to help determine spin-up times. A sensitivity experiment revealed that under future climate conditions, NPP may rise more rapidly than HR resulting in increases in NEP.",2009,10.1029/2008GB003413,no
A Study on Comprehensive Evaluation Model of Product Producing Process Capability,"Process Capability Index is a widely used tool for quality evaluation. Available methods of Process Capability evaluation are limited to machining process. There are no suitable methods for comprehensive evaluation of total producing process capability. In this paper, a comprehensive evaluation model of product producing process capability is proposed, so that data of all stages can be fully used. Methods for machining capability evaluation are extended to purchasing, assembling and delivery inspection and an index system of producing process capability evaluation is presented. In addition, analytic hierarchy process is used to determine the weight of each stage and then producing process capability index is achieved by summing. Finally, an application example is analyzed to show that the entire producing capability can be evaluated objectively. It should play an important part in monitoring and improving producing process capability.",2009,,no
Analyzing a Software Process Model Repository for Understanding Model Evolution,"Process models play a central role in the process improvement cycle. Often, large process models evolve in an ad-hoc manner, a fact that may easily have critical implications such as increased maintenance effort. This highlights the need for supporting the control and management of process model evolution, a kind of support that is currently widely missing. Analyzing existing model repositories in order to better understand model evolution can be seen as a first step towards identifying requirements for process model evolution support. This article presents a study that analyzes the evolution history of a large process model with the purpose of understanding model changes and their consequences. Besides the study description, the article provides an overview of related work, and suggests open questions for future work.",2009,,no
A band-type network model for the time-series problem used for IC leadframe dam-bar shearing process,"Progressive shearing process of a thin metal sheet is regarded as one of the most important fabrication processes in the 3C industry. In this process, the problem of punch wear is a time-series issue that would be changed along with the increase of time and deciding the size of the punched hole. This study proposes a neural network to modeling the time-series problem involving three kinds of presentations. First, the time-characteristic diagram is plotted based on the characteristic values that the network inferred. Second, the characteristic value of the un-examined time could be derived by network inferring. Third, a band-type diagram is built to present the possible variation range of the characteristic analyzed. Two cases of punch wearing band and punched hole bands for IC (integrated circuit) leadframe dam-bar shearing process are discussed to demonstrate the construction of the network model. The verification experiments show that the network inferring results are in agreement with the actual process, and derive more accurate predictions than that of the curve-fitting methods commonly used in engineering.",2009,10.1007/s00170-008-1432-6,no
Developing a measurement model of institutional processes in policing,"Purpose - Institutional theory shows promise as a viable framework for understanding police organization structures and activities but difficulties in measuring its core concepts make testing problematic. In order to advance the application of institutional theory in policing, this paper's aim is to develop a measurement model of institutional pressures derived from DiMaggio and Powell's discussion of institutional isomorphism. Design/methodology/approach - First and second-order confirmatory factor analyses are performed on secondary data originally collected in a 1997 national survey of law enforcement agencies about their approach to community policing. Findings - The results showed a refined model of institutional processes including three constructs - professionalization, publications, and mimesis. A construct indicated by funding measures does not seem to be consistent with other institutional pressures. Research limitations/implications - The research made use of available data and existing measures not explicitly constructed for the purpose of theory testing. Nevertheless, the results appear to be consistent with institutional theory. Originality/value - The model provides a framework for future testing of institutional theory in policing and avenues for the development of additional indicators.",2009,10.1108/13639510910958226,no
Modelling of laser processing cut quality by an adaptive network-based fuzzy inference system,"Real-world problems in precision machining now require intelligent systems that integrate knowledge, techniques, and methodologies. Intelligent systems possess human-like expertise within a specific domain to adapt themselves and to learn to do better in making decisions for an intelligent manufacturing system. An intelligent tool called adaptive network-based fuzzy inference system (ANFIS) was used to model and predict the laser cut quality of a 2.5 mm manganese-molybdenum (Mn-Mo) alloy pressure vessel plate in this article. A 3 kW CO2 laser machine with seven selected design parameters was used to carry out 128 experiments based on 2(k) factorial design with single replication. Because surface roughness (Ra) was the response parameter, it was targeted to be < 15 mu m to meet the requirement and benchmark of the pressure vessel manufacturer who sponsored this project. The DIN 2310-5 German laser cutting of metallic materials standard and procedure was referred to for evaluating surface roughness, where experimentally obtained results were used for Ra predictive modelling. Predictions of non-linear laser processing by ANFIS were found to be extremely promising in supplying the desired output, where Ra was predicted to an excellent degree of accuracy, reaching almost 70 per cent with the experimental pure error below 30 per cent.",2009,10.1243/09544062JMES1319,no
"Steady-state model-based evaluation of sulfate reduction, autotrophic denitrification and nitrification integrated (SANI) process","Recently we developed a process for wastewater treatment in places where part of the fresh water usage is replaced by seawater usage. The treatment of this saline sewage consists of sulfate reduction, autotrophic denitrification and nitrification integrated (SANI) process. The process consists of an up-flow anaerobic sludge bed (UASB) for sulfate reduction, an anoxic filter for autotrophic denitrification using dissolved sulfide produced in the UASB and an aerobic filter for nitrification. The system was operated for 500 days with 97% COD removal and 74% total nitrogen removal without withdrawal of sludge. To verify these results and to understand this novel process, a steady-state model was developed from the COD, nitrogen and sulfur mass and charge balances based on the stoichiometries of the sulfate reduction, the autotrophic denitrification and the autotrophic nitrification. The model predictions agreed well with measured data on COD, nitrate and sulfate removal, sulfide production, effluent TSS, and mass balances of COD, sulfur and nitrogen in the three reactors. The model explains why withdrawal of sludge from the SANI system is not needed through comparisons of the predictions and measurements of effluent TSS and phosphorus concentrations. (C) 2009 Elsevier Ltd. All rights reserved.",2009,10.1016/j.watres.2009.05.013,no
Measuring the Compliance of Processes with Reference Models,"Reference models provide a set of generally accepted best practices to create efficient processes to be deployed inside organizations. However, a central challenge is to determine how these best practices are implemented in practice. One limitation of existing approaches for measuring compliance is the assumption that the compliance can be determined using the notion of process equivalence. Nonetheless, the use of equivalence algorithms is not adequate since two models can have different structures but one process can still be compliant with the other. This paper presents a new approach and algorithm which allow to measure the compliance of process models with reference models. We evaluate our approach by measuring the compliance of a model currently used by a German passenger airline with the IT Infrastructure Library (ITIL) reference model and by comparing our results with existing approaches.",2009,,no
Construction of a treated wastewater reuse system for renewal of a water cycle mechanism in urban areas - modeling analysis of reclamation treatment processes corresponding to target water quality by use,"Regarding the target water quality for reuse, it is important to pay attention to the COD taking into account countermeasures against pathogens and trace matters in public waters. The main types of secondary wastewater treatment used until now were conventional activated sludge processes. More recently, however, advance wastewater treatment processes have also been developed aiming at the removal of nitrogen and phosphorus, and so on, and an analysis of the water quality characteristics of treated wastewater obtained using these processes was carried out. In addition, reclamation treatment processes that meet the target water quality for each application were selected and also their design parameters were studied as well as a cost analysis based on those results. It was found that the treatment cost was greatly affected by the COD of treated wastewater, which is a given condition. Thus, the key to a biological reaction treatment system is the attainment of treated wastewater containing as low a concentration of COD as possible.",2009,,no
Sliding-Window Recursive PLS Based Soft Sensing Model and Its Application to the Quality Control of Rubber Mixing Process,"Rubber industry requires highly the quality control of rubber mixture. In order to overcome the shortage of PLS algorithm in rubber mixing process with complex nonlinearity and time-variance, an adaptive soft sensing model based on sliding-widow recursive PLS (RPLS) is presented to build a prediction model for the Mooney-viscosity of rubber mixture. The improved RPLS model can adaptively adjust the structures and parameters of PLS model according to on-line monitoring data and take characteristics of batch-wise data updated with the process changes and time-variant tracking capabilities. The application results show that the adaptive model has stronger tracking ability and higher precision than the traditional PLS model. The results also verify its effectiveness.",2009,10.1007/978-3-642-04962-0_3,no
Measuring and modeling hydrological processes of sand-storage dams on different spatial scales,"Sand-storage dams are a successful water harvesting technology in Kenya and a promising solution to ensure water and food security in other semi-arid regions. Assessing the suitability of sand-storage dams for other semi-arid regions requires both a good understanding of the hydrological factors for success of a single dam and the regional effects of a network of dams. Results from a measurement campaign on hydrological processes in the surroundings of a single dam in the Kitui District in Kenya indicated that groundwater levels increase quickly after precipitation. Recession of groundwater levels during the dry season following the rains was more gradual. Based on these results, a groundwater model for a single sand-storage storage dam was developed. As the river banks are important recharge areas for the groundwater stored upstream of the dam, the model showed high sensitivity for parameters like thickness and hydraulic conductivity of the shallow aquifer on the riverbanks and thickness of the sand layer in the riverbed. Parallel to the single dam model, a model for a series of dams was developed. This second model indicated that the inter-darn distance is an important parameter. The distance between dams determined whether influence areas did overlap or that dams behaved as individual structures. When the influence areas did overlap, stored water volume per dam decreased. The results from measurements and modeling confirm that sand-storage darns can effectively increase water availability throughout the dry season. Since measurements and models explain how sand-storage dams successfully modify hydrological systems in semi-arid Kenya, the results can assist in planning introduction of the technology in other regions. (C) 2008 Elsevier Ltd. All rights reserved.",2009,10.1016/j.pce.2008.06.057,no
Quality-assured setup planning based on the stream-of-variation model for multi-stage machining processes,"Setup planning is a set of activities used to arrange manufacturing features into an appropriate sequence for processing. It has significant impact on the product quality, which is often measured in terms of dimensional variation in key product characteristics. Current approaches to setup planning are experience-based and tend to be conservative due to the selection of unnecessarily precise machines and fixtures to ensure final product quality. This is especially true in multi-stage machining processes (MMPs) since it is difficult to predict variation propagation and its impact on the quality of the final product. In this paper, a methodology is proposed to realize cost-effective, quality-assured setup planning for MMPs. Setup planning is formulated as an optimization problem based on quantitative evaluation of variation propagations. The optimal setup plan minimizes the cost related to process precision and satisfies the quality specifications. The proposed approach can significantly improve the effectiveness as well as the efficiency of the setup planning for MMPs.",2009,10.1080/07408170802108526,no
Models and performance evaluation for multiple-input multiple-output space-time adaptive processing radar,"Signal and clutter modelling and optimum performance evaluation for multiple-input multiple-output (MIMO)-based space-time adaptive processing radar is addressed. A signal model is developed to account for both code diverse MIMO (c-MIMO) and frequency diverse MIMO (f-MIMO), and a general framework on performance evaluation is presented to take into account various waveform configurations including phased array (PA), partially correlated MIMO and ideally orthogonal MIMO. The proposed framework evaluates the system performance through optimum processing (OP) gain and transmit array (TA) gain. The OP gain is in turn evaluated by the number of available space-time measurements (ASMs) that depends on the number of clutter degrees of freedom (clutter NDoF) relative to the system degrees of freedom (system NDoF). The waveform diversity introduced by MIMO, especially f-MIMO, could significantly enhance the OP gain by increasing the number of ASMs. Hence, in OP-gain-limited scenarios, where the overall performance significantly degrades despite the TA gains, the preferable configuration in terms of optimum performance would be ordered as f-MIMO, c-MIMO, and last, PA, that is, no MIMO.",2009,10.1049/iet-rsn.2008.0025,no
Quality improvement by using grey prediction tool compensation model for uncoated and TiAlCN-coated tungsten carbide tools in depanel process of memory modules,"Small outline dual in-line memory modules (SO-DIMM) are the memory modules used in notebook computers. This study investigates the SO-DIMM accuracy and tool life using an uncoated and TiAlCN-coated tungsten carbide tool in the depanel process. During the cutting process, the tool operating time of a TiAlCN-coated tool can be extended and the surface quality of SO-DIMM at the cutting edge is better than uncoated tools based on EDS (energy dispersive spectrometer) observation. In the milling process, tool wear is a serious problem for new tools and to solve the problem of dimension variation, this study proposes the use of grey prediction tool compensation to offset the variation and enhance the quality of SO-DIMM. According to the process capability indices within the experimental scope, the quality of memory modules using uncoated and TiAlCN-coated tools using grey prediction tool compensation were shown to improve both its Ca and Cpk values. Thus, the grey prediction tool compensation method has really proven to be workable and can improve the quality of SO-DIMM.",2009,10.1007/s00170-008-1410-z,no
Unity Criteria for Business Process Modelling A theoretical argumentation for a Software Engineering recurrent problem,"Software Engineering has a recurrent problem in relation to Business Process Modelling (BPM): there is no agreement with regards to business process modularity. We claim that this results from a lack of theoretical underpinnings on the matter. This paper goes deeply into this issue by unfolding the notion of modularity: modularisation has an engineering intention that depends on the field where it is applied, and it relies on information hiding and encapsulation mechanisms. Unity criteria provide guidance for encapsulation. An important contribution of the paper is to provide unity criteria for BPM. These criteria are mainly underpinned by systemic principles and Communication Theory. The resulting unity criteria allow to clearly differentiate between problem space and solution space in BPM. The argumentations are illustrated with explanatory examples and figures. Also, a historical review of unity criteria in Software Engineering and Requirements Engineering is offered.",2009,10.1109/RCIS.2009.5089279,no
An Architecture for Modeling and Applying Quality Processes on Evolving Software,"Software process and product views should be closely linked in order to better manage quality improvement. However until now the two views have not been effectively synchronized. Current approaches to Software Configuration Management (SCM) are strongly based on files and lacking in logical and semantic understanding. Some impediments faced when modeling and analyzing software evolution include additional effort for dealing with language dependent source code analysis and continuous mining of the evolving system. By leveraging features offered by modem VMs and other enabling technologies, we have developed a language neutral architecture with extensibility mechanisms to support continuous Software Evolution Management (SEM). Our research aims to contribute to an SEM infrastructure where semantic artifacts can be consistently accessed, tracked and managed for performing software evolution analytics beyond the file-based model. This paper presents compelling factors for our infrastructure, the architecture we have developed, and then sketches a case study to demonstrate its application.",2009,,no
FlexSPMF: A Framework for Modelling and Learning Flexibility in Software Processes,"Software processes are dynamic entities that are often changed and evolved by skillful knowledge workers such as software development team members. Consequently, flexibility is one of the most important features within software process representations and related tools. However, in the everyday practice, team members do not wish for total flexibility. They rather prefer to learn about and follow previously defined advices on which, where and how they can change/adapt process representations. In this paper we present FlexSPMF: a framework for modelling controlled flexibility in software processes. It comprises three main contributions: 1) identifying a core set of flexibility concepts; 2) extending a Process Modelling Language (PML)'s metamodel with these concepts; and 3) providing modelling resources to this extended PML. This enables process engineers to define and publish software process models with additional (textual/graphical) flexibility information. Other team members can then visualise and learn about this information, and change processes accordingly.",2009,,no
Analysis of System and Process based Quality Management System and Evaluation Model,"Some people often think about ISO 9000 as a group of documents, or a set of interrelated ideas, principles and rules that only concerns with the conformity to the documents. The true aim of 9000 system to enhance customer satisfaction and enterprise performance is compromised, so its effectiveness is often being questioned. Therefore, this paper proposes a quality system evaluation methods based on the system and the process method. Quantitative method based on modern information technology is used to improve the effectiveness of the management system itself, and monitor the conformity of processes. The research indicates that quality management system (QMS) can be broken down into three-layered structure consists of process network, process and activity. A model for evaluation of effectiveness of QMS is established based on the three-layered structure of QMS. The comprehensive evaluation method based on GAHP is adopted for evaluation of QMS.",2009,,no
Research on Model of Fuzzy Petri Net Evaluation on Implementation Process of PFI Project,"SPC is the actual implementer of PFI project, but SPC itself usually has no ability of development. It will make widely use of various agents in the process of project development, which has greatly increased the complexity of implementation of project. To evaluate the implementation process of project by phase and real-time can become effective tool of SPC to help it realize the project well. From the prospect of SPC, the text has set up the system of index assessment and put forward an arithmetic and assessment model based on the fuzzy Petri net. This model can effectively achieve the dynamic assessment of PFI project.",2009,10.1109/ICIII.2009.120,no
Stress at work: Using a process model to assist employers to understand the trajectory,"Successful management of stress at the workplace has become a topic of great interest over the last decade. Motivated by escalating costs, associated workplace injuries and the increasing demands placed on workers in the work context, the need to effectively manage stress within acceptable timeframes and at minimal cost is paramount. According to contemporary models of rehabilitation, the maintenance of a strong and trusting 'bond' between the injured worker and the employer is essential in promoting an efficacious outcome for both parties. In an attempt to provide a greater understanding of the importance of this bond, and to highlight the factors that can impact on the experience of stress at work, a process model is discussed. This model enables the trajectory of stress injuries to be tracked and its implications explored more fully by employers.",2009,10.3233/WOR-2009-0812,no
COMPREHENSIVE PROCESS-DRIVEN BOUNDARY MAKING MODEL:A CASE STUDY OF THE JORDAN-ISRAEL BOUNDARY,"Surveyors herd a central role it? boundary-making processes from at least the 19(th) century. The latest technological developments in this field have further widened their involvement in the process and changed the concepts and practices implemented. The process Model presented here incorporates the new technical means available for modern surveyors in accomplishing a more stable and sustainable boundary through a structured procedure. The roots of the traditional theory of boundary making were put down a century ago by Curzon[6]. Holdich [9] and others referring mainly, to the three-stage process of allocation, delimitation and demarcation. This article renews the existing model of boundary making process adding to it two additional stages: boundary documentation and boundary maintenance to build up a comprehensive approach. The direct involvement of the authors in boundary making processes between Israel and its neighboring countries facilitated assessment of the model elements and their full implementation in the Israel-Jordan Boundary making front negotiations, through demarcation in the field up to continuing bi-lateral maintenance operations.",2009,10.1179/003962609X390085,no
TOWARDS EMPIRICALLY-DERIVED GUIDELINES FOR PROCESS MODELLING INTERVENTIONS IN ENGINEERING DESIGN,"Task-network modelling approaches are widely used to understand and improve product development (PD) processes. The best-practice application of these tools is often presented prescriptively. This paper proposes that best-practice, and academic understanding, of PD process modelling can be further developed through an empirical approach, in which modelling tools are taken as given and practice is analysed in terms of how industry stakeholders use these tools. We argue this inductive approach can result in process modelling guidelines which explicitly recognise the challenges of modelling in industry. We thus analyse four cases of process modelling at Rolls-Royce plc and find that: 1) modelling is a social process of knowledge-creation; 2) effective modelling requires selection of the appropriate methods and tools at the right time; and 3) understanding the purpose of each interaction between participants in the modelling process helps to choose the best approach. To conclude we present guidelines to suggest which modelling approach should be used in which context, where context is described in terms of the purpose of a given modelling interaction.",2009,,no
Updated Activated Sludge Model n degrees 1 Parameter Values for Improved Prediction of Nitrogen Removal in Activated Sludge Processes: Validation at 13 Full-scale Plants,"The Activated Sludge Model n degrees 1 (ASM1) is the main model used in simulation projects focusing on nitrogen removal. Recent laboratory-scale studies have found that the default values given 20 years ago for the decay rate of nitrifiers and for the heterotrophic biomass yield in anoxic conditions were inadequate. To verify the relevance of the revised parameter values at full scale, a series of simulations were carried out with ASM1 using the original and updated set of parameters at 20 degrees C and 10 degrees C. The simulation results were compared with data collected at 13 full-scale nitrifying-denitrifying municipal treatment plants. This work shows that simulations using the original ASM1 default parameters tend to overpredict the nitrification rate and underpredict the denitrification rate. The updated set of parameters allows more realistic predictions over a wide range of operating conditions. Water Environ. Res., 81, 858 (2009).",2009,10.2175/106143009X407393,no
Business Process Measurement Model Based On the Fuzzy Multi Agent Systems,"The aim of this article is to introduce a new model for measuring Business process based on the Fuzzy Multi Agent Systems. Some of features providing by this model are: 1.Simplicity of use 2.Enterprise look vision 3.Accuracy of measurement result 4.Focus on quantitative criteria to measure the performance of business, first a composite model called BSC-GQM is issued and then the result of performance measurement is analyzed by multi agent fuzzy systems. The use of this model will increase noticeably the accuracy of performance measurement results done on business process.",2009,,no
"Modeling the forced-air cooling process of fresh strawberry packages, Part III: Experimental validation of the energy model","The aim of this study was to validate a mathematical model previously developed for predicting the cooling rate of individual packages of strawberries (clamshells) during an industrial forced-air cooling application. The differences between the predicted and experimental profiles of the average-fruit temperature per clamshell were less than 0.7 degrees C (within the limits of the experimental uncertainty). The 7/8th cooling time of individual clamshells was predicted within less than 3% of the experimental value. In addition, the local performance of the model and its capability to predict the strawberry moisture loss were qualitatively analyzed. The moisture loss was predicted between 73% and 88% of the experimental value. The predicted temperature profile of individual fruits and airflow within clamshells followed the general trends experimentally deter-mined. Finally, the results corroborated that the transport phenomena during force-air cooling applications can be modeled by decoupling the momentum transport from the transport of energy and mass. (C) 2008 Elsevier Ltd and IIR. All rights reserved.",2009,10.1016/j.ijrefrig.2008.04.011,no
"Modeling the forced-air cooling process of fresh strawberry packages, Part II: Experimental validation of the flow model","The aim of this study was to validate a previously developed mathematical model for predicting the airflow behavior within individual packages of strawberries (clamshells) during forced-air cooling applications. The model was validated by using a non-intrusive flow measurement technique (PIV). The use of PIV required the development of a simplified transparent system that reproduces the packaging structure of typical retail clamshells. The validation was achieved by comparing the velocity field predicted by the model within this system against experimental data. The model not only predicted the main flow features, but also the location of steep acceleration within the packed structure voids. This work shows that, assuming that the momentum transport can be decoupled from the transport of energy and mass during forced-air cooling applications, the steady-state Navier-Stokes equations can accurately predict the airflow within individual clamshells of strawberries. (C) 2008 Elsevier Ltd and IIR. All rights reserved.",2009,10.1016/j.ijrefrig.2008.04.009,no
Use of a spatial process-based model to quantify forest plantation productivity and water use efficiency under climate change scenarios,"The area of commercial Eucalyptus plantations has expanded dramatically in many countries during the last three decades, and continues to do so. This is causing concerns about the potential impacts of these plantations on water resources and uncertainty about productivity under different environmental conditions. We present a study where the effects of climate change on the spatial variation in climate are taken into account when predicting potential productivity in terms of dry mass (DM) of wood or mean annual volume of wood increment and water use efficiency (WUE) of hybrids of Eucalyptus grandis and Eucalyptus urophylla plantations across more than 32 million ha located near the Atlantic coast of Brazil. Our main objective was to estimate the effects of future climate and increasing CO(2) concentration on planted forests in this region. Predictions of mean annual increment in wood production and of water use efficiency were generated with an updated spatial version of the process-based growth model 3-PG (Landsberg and Waring, 1997). The model has been modified to include the direct effects of increasing levels of atmospheric CO(2) on the vegetation. We assume that light saturated assimilation rate and light use efficiency increase as atmospheric CO(2) concentration increases, while maximum stomatal conductance declines. The study considered three climatic periods with different CO(2) concentrations: the historical scenario has 350 ppm, while the 2030 and 2050 scenarios correspond to 450 ppm and 520 ppm, respectively. Sensitivity analyses quantified the effects of the parameters used in the model to account for the effects of atmospheric CO(2) on the predicted forest productivity. Stem mass is strongly sensitive to changes in canopy quantum efficiency, and hence to the effect of CO(2) on light use efficiency, but less so to changes in stomatal conductance, and hence to the effect of CO(2) on conductance. WUE, defined here as DM of wood per mass of water evapotranspired, is also sensitive to these parameters. Analysis of the climatic data for the 2030 and 2050 scenarios in the study area suggests a reduction of 2% and 3% in annual precipitation and an increase of 8% and 15% in vapour pressure deficit in 2030 and 2050, respectively, compared with the period 1971 to 2000. Application of 3-PG with the 2030 and 2050 climates suggests that, averaged over the study area, forest productivity may increase by of the order of 6 m(3) ha(-1) year(-1) by 2030, and 10 m(3) ha(-1)year(-1) by 2050, corresponding to 17% and 26% increments compared with the historical period. WUE increases by an average of 1.0g DM kg(-1) H(2)O in 2030 and 1.7 g DM kg(-1) H(2)O in 2050 compared with the historical scenario, which is equivalent to increases of 29% and 51% in WUE, respectively. This shows that with increasing CO(2) the trees are more efficient in using water. If these changes do occur it will increase the amount of land with higher potential productivity for Eucalyptus plantations in the study area. From the total area of 32 million hectares (Mha), 8.5 Mha currently have potential productivity above 40 m(2) ha(-1) year(-1). With increasing CO(2) this area increases to 20.5 Mha in 2030 and 26.0 Mha in 2050.",2009,,no
Models and Metrics for the Technology Transfer Process from Federal Labs to Application and the Market,"The author and his colleagues* have worked with over a dozen federal agencies (as well as many industrial firms) on the process of getting new technology out of their labs and into their own innovation programs and/or into the broader markets of industry and other agencies. The focus of this paper is on metrics and flow models for the outputs, at each stage of the process, and the barriers and facilitators that impede or enhance the flow. It deals with the notorious ""Valley of Death"" that slows or sinks the flow of items of technology at various stages of the R&D/Innovation (R&D/I) process. It suggests a systematic methodology for identifying and measuring the impacts, outputs, barriers, and facilitators encountered in the flow. Criteria trees are suggested for connecting stage outputs to the Critical Success Factors (CSFs) of the operating units, parent organizations, and other sponsors and clients served by the labs. Some examples of common barriers and facilitators are given, including: the over-focus of many Tech Transfer Offices on ""paper"" Intellectual Property (IP), such as patents and licenses vs. ""real"" outputs and impacts such as new products and applications of technology that are transferred to and adopted by the various types of potential users of the technology. Specific examples are also drawn from studies by the author and his colleagues in the fields of: aerospace and automotive research; agriculture; transportation; healthcare; military R & D; and environment, energy, and materials R&D.",2009,,no
Two-particle coherence in an exactly solvable model of the quantum measurement process,"The boson-detector model is generalized to investigate the decay of two-particle coherence. It is found from the visibility of the two-particle interferometer that the detector system not only destructs the coherence but also creates the correlation between the particles. Under certain conditions, the two-particle coherence can survive though the single particle coherence is completely destroyed. (C) 2009 Elsevier B.V. All rights reserved.",2009,10.1016/j.physleta.2009.05.003,no
A business process activity model and performance measurement using a time series ARIMA intervention analysis,"The degree of performance excellence that an enterprise can achieve greatly depends on the business process flow that the enterprise adopts, where the more efficient and effective the business process flow, the greater the degree of performance excellence the enterprise can achieve. Most conventional business process analyses focus on qualitative methodologies, but these lack solid measurement for supporting the business process improvement. Therefore, a quantitative methodology using an activity model that is described in this paper is proposed. This model involves the use of an adjacent matrix to empirically identify inefficient and ineffective activity looping, after which the business process flow can then be improved. With the proposed quantitative methodology, a time series intervention ARIMA model is used to measure the intervention effects and the asymptotic change in the simulation results of the business process reengineering that is based on the activity model analysis. The approach is illustrated by a case study of a purchasing process of a household appliance manufacturing enterprise that involves 20 purchasing activities. The results indicate that the changes can be explicitly quantified and the effects of BPR can be measured. (c) 2008 Elsevier Ltd. All rights reserved.",2009,10.1016/j.eswa.2008.08.027,no
DEVELOPMENT OF A MATHEMATICAL PROCEDURE FOR MODELLING AND INSPECTING COMPLEX SURFACES FOR MEASUREMENT PROCESS,"The design and the manufacture of free form surfaces are being a current practice in the industry. Thus the problem of the parts conformity of complex geometry is felt more and more. By this work, a mathematical procedure for modelling and inspecting complex surfaces in measurement process is presented including the correction of geometrical deviations within manufacturing process. The method is based on the Iterative Closest Point (I.C.P.) algorithm for alignment stage between nominal Surface and measured points. The finite elements method for geometric compensation STL model generally obtained by a triangulation of nominal model using Computer Aided Design (CAD) software; is used in our proposal. An industrial application concerning a rapid prototyping technology process is presented. This last is used for manufacturing of real parts obtained from a STL file format.",2009,,no
The 9-step problem design process for problem-based learning: Application of the 3C3R model,"The design of problems is crucial for the effectiveness of problem-based learning (PBL). Research has shown that PBL problems have not always been effective. Ineffective PBL problems could affect whether students acquire sufficient domain knowledge, activate appropriate prior knowledge, and properly direct their own learning. This paper builds on the 3C3R problem design model, which is a systematic conceptual framework for guiding the design of effective and reliable problems for PBL. To help practitioners apply the 3C3R model, this paper introduces a 9-step problem design process. The initial steps guide an instructional designer through analyses on learning goal, content, and context to help select problems. Later steps ensure that the problem appropriately affords the specifications identified in the analyses. The last two steps incorporate a reflection component, as well as ensure the integrity of the 3C3R components in the problem. (c) 2008 Elsevier Ltd. All rights reserved.",2009,10.1016/j.edurev.2008.12.001,no
Modeling of Network Education Effectiveness Evaluation in Fuzzy Analytic Hierarchy Process,"The effectiveness of Network education in China has become the key to the development of e-learning; to build an effective system to assess the effectiveness of network education is an important means of improving the network education effectiveness. According to the primary factors that influence the network education effectiveness, this paper establishes a relatively comprehensive system of evaluated guideline. The index weight factors are achieved with AHP (Analytic Hierarchy Process) method. The sub-effectiveness value is combined into the network education effectiveness system with Fuzzy Mathematic method. Finally, an example is given.",2009,10.1109/ICNDS.2009.129,no
Business Process Models supporting participatory layout planning,The following paper specifies an enterprise application for participatory layout planning. Based on a business process model for the process of factory planning the goal is to accelerate the planning process and to generate higher quality planning results while reducing the planning effort. In addition to that all employees are enabled to participate in the layout planning process by using an integrated planning platform - the participatory planning table. The BPM and the resulting participatory planning table were developed at the Technical University of Braunschweig in closely cooperation with enterprises of the automotive industry.,2009,,no
Review of ANN Technique for Modeling Surface Roughness Performance Measure in Machining Process,"The former, which is defined as modeling of machining processes, is essential to provide the basic mathematical models for formulation of the certain process objective functions. With conventional approaches such as Statistical Regression technique, explicit models are developed that required complex physical understanding of the modeling process. With non conventional approaches or Artificial Intelligence techniques such as Artificial Neural Network, Fuzzy Logic and Genetic Algorithm based modeling, implicit model are created within the weight matrices of the net, rules and genes that is easier to be implemented. With the focus on surface roughness performance measure, this paper outlines and discusses the concept, application, abilities and limitations of Artificial Neural Network in the machining process modeling. Subsequently the future trend of Artificial Neural Network in modeling machining process is reported.",2009,10.1109/AMS.2009.78,no
Evaluation of Methodologies for Mathematical Modeling of Packaged Conductive Foods Heat Process,"The heat process is a safe method of food preservation. The use of mathematical modeling for heat transfer by finite elements analysis (FEA) makes it possible to determine the cold spot of conventional and non conventional packages, evaluate their thermal history, microbial and enzyme inactivation and nutrients retention for an optimum process design. Several works use simplifications during mathematical modeling, such as adiabatic headspace, considering the thermal resistance of package negligible. The impact of these simplifications is rarely evaluated. The aim of the present work was to evaluate the effect of these simplifications on sterilization value (Fp) for a conductive food. Two commercial glass bottles (G1 and G2) were selected for the assays. FEA model was built using the bottles' real geometries. Three methodologies were evaluated, considering (i) the four components of the system, i. e., product, glass wall, headspace and metal cap, and uniform heating (PGHM); (ii) adiabatic headspace, i. e., a model considering product and glass wall, with its upper side adiabatic (PG); and (iii) only product, with adiabatic upper side (P). A tomato concentrate industrial pasteurizator profile was used as boundary condition. The Fp was determined by using two values of thermal coefficient (z), 5.5(sic)C and 12.5(sic)C, representing a possible range of contaminant's z-value. The cold spot of the two packages was located at 32% (G1) and 46% (G2) of the product height. For the same process, the differences of Fp for the two packages ranged between 62 and 320%. Comparing the Fp by PGHM and PG models, differences were observed between 4 and 13%. These differences were over 45% when comparing PGHM with P models, even with similar thermal history. The results indicated the importance of the previous evaluation of the impact of each simplification on the accuracy of the model. Due to exponential relationship between temperature and reactions during the heat process, the need for Fp evaluation instead of thermal history in conductive food was confirmed.",2009,10.2202/1556-3758.1458,no
An Agent-Based Model for the Adaptation of Processing Efficiency for Prioritized Traffic,The increasing number and variety of telecommunication services being offered by networks have emphasized the demand for optimized load management strategies. Obtainable processing efficiency is one of the key elements for all services and has to be dominantly supervised. Different priority levels and classifications can co-exist to support services. such fine regulation of precedence mixed with hard and destructive priority for critical service classes. Intelligent processing efficiency management with adaptive protection of predefined processing power can support such requests. This paper deals with the regulation of packets moving towards service-processing resources. Regulation is based on service priorities and processing load in the agent-controlled part of the network.,2009,,no
Evaluating Self-similar Processes for Modeling Graphical Remote Desktop Systems' Network Traffic,"The increasing speed of internet access links motivated the shift in applications, used to remotely access the computers, from the text-based terminal connections to graphical remote desktop systems (GRDS). Because these services are highly interactive, user-perceived latency is more important than in other different types of internet applications (such as Web, Video streaming, and Peer-to-Peer File transfer), which usually attract the researchers' attention. In previous studies, the traffic performance is shown to be mainly affected by the statistical nature of applications' packet arrival process at network level. This paper describes characterization and modeling of graphical remote desktop system's network traffic at the IP level. Following the past researches, we have evaluated self-similar processes and noticed a very good fit at timescales ranging from 100 ms to 5 s and from 10 to 200 s. We provide the reasons for this self-similar traffic behavior and discuss its consequences at the development of synthetic workloads and performance studies of GRDS systems.",2009,,no
Understanding and modeling the physical processes that govern the melting of snow cover in a tropical mountain environment in Ecuador,"The ISBA/CROCUS coupled ground-snow model developed for the Alps and subsequently adapted to the outer tropical conditions of Bolivia has been applied to a full set of meteorological data recorded at 4860 m above sea level on a moraine area in Ecuador (Antizana 15 glacier, 0 degrees 28'S; 78 degrees 09'W) between 16 June 2005 and 30 June 2006 to determine the physical processes involved in the melting and disappearance of transient snow cover in nonglaciated areas of the inner tropics. Although less accurate than in Bolivia, the model is still able to simulate snow behavior over nonglaciated natural surfaces, as long as the modeled turbulent fluxes over bare ground are reduced and a suitable function is included to represent the partitioning of the surface between bare soil and snow cover. The main difference between the two tropical sites is the wind velocity, which is more than 3 times higher at the Antizana site than at the Bolivian site, leading to a nonuniform spatial distribution of snow over nonglaciated areas that is hard to describe with a simple snow partitioning function. Net solar radiation dominates the surface energy balance and is responsible for the energy stored in snow-free areas (albedo = 0.05) and transferred horizontally to adjacent snow patches by conduction within the upper soil layers and by turbulent advection. These processes can prevent the snow cover from lasting more than a few hours or a few days. Sporadically, and at any time of the year, this inner tropical site, much wetter than the outer tropics, experiences heavy snowfalls, covering all the moraine area, and thus limiting horizontal transfers and inducing a significant time lag between precipitation events and runoff.",2009,10.1029/2009JD012292,no
Process Modeling of Comprehensive Integrated Forest Biorefinery-An Integrated Approach,"The key to expanding the energy supply, increasing energy security, and reducing the dependency on foreign oil is to develop advanced technologies to efficiently transform our renewable bioresources into domestically produced bioenergy and bioproducts. Conventional biorefineries, i. e., forest products industry's pulp and paper mills with long history of sustainable utilization of lignocellulose ( wood), offer a suitable platform for being expanded into future integrated forest biorefineries. Due to the preexisting infrastructure in current forest products operations, this could present a very cost-effective approach to future biorefineries. In order to better understand the overall process, technical, economic, and environmental impacts, a detailed process modeling of the whole integrated forest biorefinery is presented here. This approach uses a combination of Aspen Plus (R), WinGEMS (R), and Microsoft Excel (R) to simulate the entire biorefinery in detail with sophisticated communication interface between the three simulations. Preliminary results for a simple case study of an integrated biorefinery show the feasibility of this approach. Further investigations, including additional details, more process options, and complete integration, are currently underway.",2009,10.1007/s12010-008-8478-7,no
Integration Enterprise Process Metrics Model and Information Model Based On Semantics,"The key to practice enterprise process measure is to integrate metrics model with information model, and collect metrics data from information system. The semantics structure of basic metric is analyzed, and guide lines are given to describe the semantics of metric for collecting metric data automatically. Based on domain ontology, a structure called semantic tree is defined to describe the semantics relationships among measured entity, measurable attribute and constrains, that provides the same semantic definition method for metrics and data elements in enterprise information model. Arithmetic to computer the similarity of semantics tree is presented and a method to map metrics and enterprise information model is put forward.",2009,10.1109/WCSE.2009.273,no
Evaluation of electrical energy per order (E-EO) with kinetic modeling on the removal of Malachite Green by US/UV/H2O2 process,"The kinetics of decolorization of Malachite Green (MG) as a model cationic dye from textile industry, by US/UV/H2O2 process, was investigated with nonlinear regression analysis. The experimental results indicated that the decolorization kinetics of MG in this process fit well by pseudo-first order kinetics. With nonlinear regression analysis a model was developed for pseudo-first order constant (k(ap)) as a function of operational parameters such as initial concentrations of H2O2 (25-600 mg l(-1)) and MG (1.82-9.87 mg l(-1)), temperature (294-307 K) and power density (0.049-0.16 W ml(-1)) as following: k(ap) = 283 exp(-9000/RT)(0.0205 + (1.76 x 10(-3)[H2O2](o)/1 + 4.86 x 10(-4)[H2O2](o) + 9.5 x 10(-6)[H2O2](o)(2))[MG](-0.668)(P/V)(0.313) For electrical energy cost evaluation, the figure-of-merit electrical energy per order (E-EO) was estimated from experimental and theoretical data. The results show that EEO is very sensitive to the mentioned operational parameters. (C) 2009 Elsevier B.V. All rights reserved.",2009,10.1016/j.desal.2008.07.025,no
Modelling of machining error flow based on form features for multistage processes,"The machining quality of a workpiece is determined by many machining process elements, such as the machine tool, fixturing, and process planning, etc. It is especially observed that there is a propagation chain of quality attributes in the multistage machining processes (MMPs). Owing to the propagation chain of quality attributes, the machining quality of the workpiece at a single stage is determined not only by the machining elements of the current stage, but also by the quality attributes of its previous stages. In order to describe the propagation chain, a form-feature-based quality control model for MMPs is proposed. Based on this, a linear error propagation model is established by the use of a rigid-body kinematics method, which can explicitly describe the mapping relationship between machining errors of quality attributes and machining process elements. In order to have some comparison with the error propagation model, the above linear regression model has been built to enable correlated quality attributes to be determined with this error propagation model. A workpiece has been used to show that the error propagation model can effectively describe the propagation chain of the quality attributes.",2009,10.1080/09511920802623985,no
Techniques of Imitation Modeling by the Numerical Methods of Measuring Processes of the Multifunctional Force Sensor,The main provisions of the techniques of imitation modeling by the numerical methods of measuring processes of the multifunctional force sensor are analyzed. Its physical model is described. One of the options for elastic element design is provided.,2009,10.1109/EDM.2009.5173947,no
Strategy for Validating a Population Balance Model of a Batch Crystallization Process Using Particle Size Distribution from Image-based Sensor,"The modeling of the transient behavior of the crystal size distribution is essential to predict and effectively control the quality of the end product. The population balance approach provides an appropriate mathematical framework for the modeling of crystal size. For most batch crystallization processes, it is often difficult to obtain on-line relevant information about the crystal size distribution and the dissolved solid concentration in the liquid phase. We have recently developed an automated image analysis strategy that yields real-time measurements of both particle length and width with acceptable accuracy. In this contribution, we present an approach for validation of a population balance model for batch crystallization of monosodium glutamate by using on-line information about the state of the crystallization process provided by our image analysis strategy and other in-situ process analytical tools such ATR-FTIR.",2009,,no
A process model for explaining genotypic and environmental variation in growth and yield of rice based on measured plant N accumulation,"The objective of this study was to develop a mechanistic model for simulating the genotypic and environmental variation in rice growth and yield based on measured plant N accumulation. The model calibrations and evaluations were conducted for rice growth and yield data obtained from a cross-locational experiment on 9 genotypes at 7 climatically different locations in Asia. The rough dry grain yield measured in the experiment ranged from 71 to 1044 g m(-2) over the genotypes and locations. An entire process model was developed by integrating sub-models for simulating the processes of leaf-area index development, partitioning of nitrogen within plant organs, vegetative biomass growth, spikelet number determination, and yield. The entire process model considered down-regulation of photosynthesis caused by limited capacity for end-product utilization in growing sink organs by representing canopy photosynthetic rate as a function of sugar content per unit leaf nitrogen content. The model well explained the observed genotypic and environmental variation in the dynamics of above-ground biomass growth (for validation dataset, R(2) = 95), leaf area index development (R(2) = 0.82) and leaf N content (R(2) = 0.85), and spikelet number per unit area (R(2) = 0.67) and rough grain yield (R(2) = 0.66), simultaneously. The model calibrations for each sub-model and the entire process model against observed data identified 10 genotype-specific model parameters as important traits for determining genotypic differences in the growth attributes. Out of the 10 parameters, 5 were related to the processes of phenological development and spikelet sterility, considered to be major determinants of genotypic adaptability to climate. The other 5 parameters of stomatal conductance, radiation extinction coefficient, nitrogen use efficiency in spikelet differentiation, critical leaf N causing senescence, and potential single grain mass had significant influence on the yield potential of genotypes under given climate conditions. (C) 2009 Elsevier B.V. All rights reserved.",2009,10.1016/j.fcr.2009.05.010,no
Analysis of the heat and mass transfer processes in solar stills - The validation of a model,"The outcome of the earlier systematic research work on the theoretical modeling of the complex transport phenomena occurring in solar stills was the development of the fundamental Dunkle's model, already known almost four decades ago. Although it has been based on several simplified assumptions, this model has extensively been employed over the years as a convenient and sufficiently accurate predictive tool for solar stills working under ordinary operating conditions. However, it has occasionally been reported that it fails under unusual operating conditions, mainly corresponding to higher average temperatures, usually leading to higher distillate yields. The aim of the present investigation was to relax the initially established simplified assumptions of the fundamental Dunkle's model and to evaluate the comparative accuracy of both, the refined and the earlier fundamental models against an extensive body of previously reported measurements from the literature, both field and laboratory. The comparative presentation of results indicates that although both models are impressively correct for ordinary low temperature operating conditions where the humid air thermophysical properties are close to those of dry air, the saturation vapor pressure at the brine and condensing plate temperatures are negligible compared to barometric pressure and the familiar Jakob's dimensionless Nusselt-Rayleigh correlation for natural convection heat transfer appears to be valid, they both fail at higher operational temperatures. It appears that as far as Dunkle's simplified model is concerned, this occurs not only owing to the first two counteracting effects but also to the effect of the dimensionless convective heat transfer correlation affecting also the accuracy of the refined model, which fails to predict precisely the natural convection conditions at higher Rayleigh numbers representing conditions of strong turbulence in the solar still cavity. Assuming a constant asymptotic value of the exponent n = 1/3 which persists over a broad region of high Rayleigh numbers relevant to solar still operation, an improved value of the proportionality constant C around the value of 0.05 was estimated for the accurate prediction of measurements, at least as far as the available data from the literature is concerned. (c) 2008 Elsevier Ltd. All rights reserved.",2009,10.1016/j.solener.2008.09.007,no
Development and Modelling of High-Efficiency Computing Structure for Digital Signal Processing,"The paper is devoted to problem of spline approximation. A new method of nodes location for curves and surfaces computer construction by means of B-splines and results of simulink-modeling is presented. The advantages of this paper is that we comprise the basic spline with classical polynomials both on accuracy, as well as degree of paralleling calculations are also shown.",2009,10.1109/MSPCT.2009.5164207,no
High Accuracy Correction of Critical Dimension Errors Appearing in Large Scale Integrated Circuits Fabrication Processes: Pattern-Based Model,"The pattern density model has been used for correcting critical dimension (CD) errors appearing in LSI fabrication processes. However, this method has an inherent uncertainty in the figure position and a correction error caused by this uncertainty. In this paper, we introduce a pattern-based model for discussing CD errors in order to avoid this uncertainty. Based on the pattern-based model, a new nonlinear equation for correction is derived, and a formula for obtaining a high-accuracy numerical solution for the correction equation is also derived. The correction accuracy of the solution is evaluated with several approximate solutions by both analytical and numerical methods under the condition that (1) the point spread function g(x) of the position shift of a point on a figure edge is the Gaussian function, and (2) a one-dimensional cross section of lines and spaces is used for evaluation. The proposed method can provide a high-accuracy correction both in the case in which the interaction distance sigma >> figure size and also when sigma << figure size. Furthermore, even in the case in which sigma is comparable with figure size, the method provides higher-accuracy corrections than other methods. For example, when sigma is 10nm, the figure size is 20 nm, and the maximum deviation gamma(d)(= 2 gamma(d)*) from the designed size is 20 nm, our correction method supresses the dimensional error of lines and speces pattern less than 0.9 nm. These results suggest that the method proposed in this paper has a wide range of applications for future LSI fabrication. (c) 2009 The Japan Society of Applied Physics",2009,10.1143/JJAP.48.046508,no
Spatial point process models for location-allocation problems,"The problem of finding an optimal location frequently occurs in geomarketing, economics and other fields: positioning a new branch of a bank, a supermarket, a fire station, a plant, designing a traffic network, etc. The optimal location of the source facility is the argument-minimum of an optimization problem parametrized by some characteristics of the clients. The random nature of some of these characteristics has already been recognized, but few stochastic models for location-allocation problems address the issue of uncertainty of the locations of the clients, and even then they do it with very naive tools. It is proposed to recognize uncertainty in the spatial positions of the clients, and possible spatial autocorrelation as well, by considering the random inputs of the optimization as one realization of a spatial marked point process. The method, called SPP location-allocation, involves fitting a point process model, simulating from the adjusted process, and solving a family of optimization problems for each simulated set of observations. The advantage of this approach over the deterministic one is twofold: it gives an indication of the spatial variability of the optimal solution, and it allows one to solve larger problems. Finally an application to the optimal positioning of a new fire station in the Toulouse area (France) is presented with some heuristic algorithms. (c) 2008 Elsevier B.V. All rights reserved.",2009,10.1016/j.csda.2008.10.016,no
A process targeting model for a product with two dependent quality characteristics using 100% inspection,"The purpose of this paper is to develop a process targeting model for a product with two quality characteristics produced by two processes in series. The first quality characteristic is determined by the setting of the first process, whereas the second quality characteristic depends on the setting of the two processes. The quality of the product is controlled by a 100% inspection plan, and inspection is assumed to be error free. The objective of the model is to determine the optimal target for both processes that maximizes the profit. A realistic case study has been used to demonstrate the utility of the model. The results have shown that the model results improve the profit per lot and aid the factory in conducting cost/benefit analysis for reducing the variances of the two processes. In addition, sensitivity analysis has been performed to study the effect of different parameters on the expected profit and optimal processes means. It has been shown that the results of the model are sensitive to the variances of the two processes, selling prices, and rework costs.",2009,10.1080/00207540600705111,no
Process-targeting model for a product with two dependent quality characteristics using acceptance sampling plans,"The purpose of this paper is to develop a process-targeting model for a product with two quality characteristics produced by two processes in series. The first quality characteristic is determined by the setting of the first process, whereas the second quality characteristic depends on the setting of the two processes. The quality of the product is controlled by a single sample inspection plan where inspection is assumed to be error free. The objective of the model is to determine the optimal target for both processes that maximises the profit. A realistic case study is used to demonstrate the utilisation of the model. The results of applying the model indicated that the plant could improve its profit by using optimal process setting and better inspection plans parameters. In addition, sensitivity analysis was performed to study the effect of different parameters on the expected profit and optimal processes means.",2009,10.1080/00207540701644243,no
Consistency between hydrological models and field observations: linking processes at the hillslope scale to hydrological responses at the watershed scale,"The purpose of this paper is to identify simple connections between observations of hydrological processes at the hillslope scale and observations of the response of watersheds following rainfall, with a view to building a parsimonious model of catchment processes. The focus is on the well-studied Panola Mountain Research Watershed (PMRW), Georgia, USA. Recession analysis of discharoe Q shows that while the relationship between dQ/dt and Q is approximately consistent with a linear reservoir for the hillslope, there is a deviation from linearity that becomes progressively larger with increasing spatial scale. To account for these scale differences conceptual models of streamflow recession are defined at both the hillslope scale and the watershed scale, and an assessment made as to whether models at the hillslope scale can be aggregated to be consistent with models at the watershed scale. Results from this Study show that a model with parallel linear reservoirs provides the most plausible explanation (of those tested) for both the linear hillslope response to rainfall and non-linear recession behaviour observed at the watershed outlet. In this model each linear reservoir is associated with a landscape type. The parallel reservoir model is consistent with both geochemical analyses of hydrological flow paths and water balance estimates of bedrock recharge. Overall, this study demonstrates that standard approaches of using recession analysis to identify the functional form of storage-discharge relationships identify model structures that are inconsistent with field evidence, and that recession analysis at multiple spatial scales can provide useful insights into catchment behaviour. Copyright 0 2008 John Wiley & Sons, Ltd.",2009,10.1002/hyp.7154,no
Using the Process-based Stand Model ANAFORE Including Bayesian Optimisation to Predict Wood Quality and Quantity and Their Uncertainty in Slovenian Beech,"The purpose of this study was to expand an existing semi-mechanistic forest model, ANAFORE (ANAlysing Forest Ecosystems), to allow for the prediction of log quality and the accompanying uncertainty as influenced by climate and management. The forest stand is described as consisting of trees of different cohorts, either of the same or of different species (deciduous or coniferous). In addition to photosynthesis, transpiration, total growth and yield, the model simulates the daily evolution in vessel biomass and radius, parenchyma and branch development. From these data early and latewood biomass, wood tissue composition, knot formation and density are calculated. The new version presented here, includes the description of log quality, including red heart formation of beeches. A Bayesian optimisation routine for the species parameters was added to the stand model. From a given range of input parameters (prior), the model calculates an optimised range for the parameters (posterior) based on given output data, as well as an uncertainty on the predicted values. A case study was performed for Slovenian beech forests to illustrate the main model functioning and more in particular the simulation of the wood quality. The results indicate that the ANAFORE model is a useful too] for analyzing wood quality development and forest ecosystem functioning in response to management, climate and stand characteristics. However, the Bayesian optimization showed that the remaining uncertainty on the input parameters for the chosen stand was very large, due to the large number of input parameters in comparison to the limited stand data.",2009,10.14214/sf.204,no
"Measuring CRM effectiveness: Construct development, validation and application of a process-oriented model","The quality of customer relationship management (CRM) is usually evaluated by outcome indicators such as customer loyalty and business performance. To maintain or improve these indicators, CRM managers should regularly evaluate the progress of CRM practices. In this paper, we propose and develop a construct, called CRM effectiveness (CRME), comprising three dimensions: relationship marketing (RM), customer-focused information technology (CFIT) and customer-focused organisational climate (CFOC). The development of CRME followed the three-stage methodology of Churchill (1979). A survey was conducted at 523 financial services institutions and 407 manufacturing companies in Taiwan with 221 usable returns. The survey evaluated reliability, convergent validity, discriminant validity and nomological validity of the construct. The results support the proposed construct and its three dimensions. The three dimensions and their measures offer a parsimonious and practical approach for evaluating CRME and identifying its strengths and weaknesses. They reflect the process perspective of CRME and provide a better operationalisation of CRME for businesses and researchers to apply in practices.",2009,10.1080/14783360902719451,no
INVESTIGATION OF THE STATISTICAL MODEL BASED OPTIMIZATION ACCURACY UNDER EXPERIMENTAL ERRORS. CASE STUDY: OPTIMIZATION OF NUTRIENT MEDIA FOR MICROORGANISMS' CULTIVATION PROCESS,"The response surface methodology based process optimization procedure, including design of factorial experiments, development of statistical model and estimation of the optimum response point is simulated in order to investigate influence of experimental errors and experimental design area on the optimization accuracy. The investigated practical problem is optimization of nutrient media composition for cultivation of microorganisms' culture. The optimization results under a priori estimated experimental errors and various factor variation ranges in factorial experiments are investigated and the relationships between the factors variation range and the confidence interval of the optimum point estimations are determined.",2009,,no
Risk Evaluation Process Model of Information Security,"The risk assessment applied in information technology is the information security risk evaluation. A modeling method of security system that is suitable to description results and restriction of information and to modeling form and control of distributed system is put forward based on the analysis of the concept of security system risk evaluation in the field of information security. A model of information security evaluation process has been built based on Petri net. The process is divided into several objects, such as assets identification, threats identification, vulnerability identification and existing security control measures identification etc. and make a detailed description to each object on Petri net theory.",2009,10.1109/ICMTMA.2009.143,no
A Comprehensive Network Security Risk Model for Process Control Networks,"The risk of cyber attacks on process control networks (PCN) is receiving significant attention due to the potentially catastrophic extent to which PCN failures can damage the infrastructures and commodity flows that they support. Risk management addresses the coupled problems of (1) reducing the likelihood that cyber attacks would succeed in disrupting PCN operation and (2) reducing the severity of consequences in the event of PCN failure or manipulation. The Network Security Risk Model (NSRM) developed in this article provides a means of evaluating the efficacy of candidate risk management policies by modeling the baseline risk and assessing expectations of risk after the implementation of candidate measures. Where existing risk models fall short of providing adequate insight into the efficacy of candidate risk management policies due to shortcomings in their structure or formulation, the NSRM provides model structure and an associated modeling methodology that captures the relevant dynamics of cyber attacks on PCN for risk analysis. This article develops the NSRM in detail in the context of an illustrative example.",2009,10.1111/j.1539-6924.2008.01151.x,no
QUALITY OF ORANGE JUICE DRINK SUBJECTED TO A PREDICTIVE MODEL-BASED PASTEURIZATION PROCESS,"The utility of a previously developed predictive model for the decimal reduction time (D-72C) of Salmonella Typhimurium (ATCC 13311) in citrus systems was validated by establishing a pasteurization process (P-72C) for Philippine native orange juice drink (POD). Quality characteristics of the pasteurized POD were evaluated and used as bases for assessing the utility of the developed model. The (p) P-72C for POD was calculated by multiplying the model-generated D-72C of the reference strain with the recommended order of lethality (m) of 5.00 log(10) cycles. A (a) P-72C was likewise calculated based on D-72C that resulted from actual thermal inactivation study of the reference strain on POD. Pasteurization resulted to PODs with significantly (P < 0.05) altered color, microbial quality and sensory profile. Except for the overall quality, differences in the consumer acceptance of various unpasteurized and pasteurized PODs quality characteristics were found to be not significant. PRACTICAL APPLICATIONS A mathematical model was developed to estimate the thermal death times of the pathogen Salmonella Typhimurium (ATCC 13311) from the pH, titratable acidity (% citric acid) and soluble solid (degrees Brix) values of suspending citrus systems. In this study, the utility of this model in establishing a pasteurization process for a test orange juice drink was determined. As the establishment of specific thermal process schedule for individual food products can be difficult, utilization of the validated model shall be a convenient alternative to the traditional but rigorous process.",2009,10.1111/j.1745-4557.2009.00260.x,no
Mathematical Modeling and Experimental Validation of the Warm Spray (Two-Stage HVOF) Process,"The warm spray (WS) gun was developed to make an oxidation-free coating of temperature-sensitive material, such as titanium and copper, on a substrate. The gun has a combustion chamber followed by a mixing chamber, in which the combustion gas is mixed with the nitrogen gas at room temperature. The temperature of the mixed gas can be controlled in the range of about 1000-2500 K by adjusting the mass flow rate of nitrogen gas. The gas in the mixing chamber is accelerated to supersonic speed through a converging-diverging nozzle followed by a straight barrel. This paper shows how to construct the mathematical model of the gas flow and particle velocity/temperature of the WS process. The model consists of four parts: (a) thermodynamic and gas-dynamic calculations of combustion and mixing chambers, (b) quasi-one-dimensional calculation of the internal gas flow of the gun, (c) semiempirical calculation of the jet flow from the gun exit, and (d) calculation of particle velocity and temperature traveling in the gas flow. The validity of the mathematical model is confirmed by the experimental results of the aluminum particle sprayed by the WS gun.",2009,10.1007/s11666-009-9299-0,no
Nonlinear Dynamical Mathematical Models for Plates and Numerical Solution Cauchy Problems by Gauss-Hermite Processes,"There are construct now 2D respect to spatial coordinates nonlinear dynamical mathematical models von Karman-Mindlin-Reissner type systems of PDE for anisotropic poro, piezo, viscous elastic plates. Truesdell-Ciarlet unsolved (even in case of isotropic elastic plates) problem about physical soundness respect to von Karman system is decided. New two-dimensional with respect to spatial coordinates mathematical models of KMR type had created and justified for poro-viscous-elastic binary mixtures when it represents a thin-walled structure. There is find also new dynamical summand partial derivative(n)Delta Phi in the another equation of von Karman type systems too. Thus the corresponding systems in this case contains Rayleigh-Lamb wave processes not only in the vertical, but also in the horizontal direction This of KMR type dynamical system represents evolutionary equations for which the methods of Harmonic Analyses nonapplicable. In this connection for Cauchy problem suggests new schemes having arbitrary order of accuracy and based on Gauss-Hermite processes.",2009,,no
"Evaluating Geography Textbook Questions from a Spatial Perspective: Using Concepts of Space, Tools of Representation, and Cognitive Processes to Evaluate Spatiality","This article examines whether questions embedded in geography textbooks address three components of spatial thinking: concepts of space, tools of representation, and processes of reasoning. A three-dimensional taxonomy of spatial thinking was developed and used to evaluate questions in four high school level geography textbooks. The results indicate that textbook questions focus on low-level spatial concepts more frequently than high-level spatial concepts; few questions require students to create various kinds of spatial representations; and textbook questions only rarely encourage higher-order cognitive skills. The study provides insights on the design and use of textbook questions to foster learning to think spatially.",2009,10.1080/00221340902758401,no
"Scientific Knowledge and Solving Problems Modelling, Representation and Processing","This article presents several important topics that show the importance of knowledge and solving problems in the development of scientific knowledge. At present the scientific and technical development, solving problems in a different field (math, science, physics, chemistry etc.) is a creative activity, by building a reasoning, generation, describing the following activities: demonstration process (deduction and reasoning) to show the existence of a solution or several solutions and / or to determine the exact effective solutions; computational process (algorithm) to codify a demonstration, a method or technique to solve in order to determine (possibly approximate) exact solutions. In the problem-solving processes require demonstrative thinking, a algorithms thinking. From the methodological point of view, we need to recast usual problems explicitly and properly resolve their mathematical. If the computer should use to develop algorithmic methods. In both cases you must know the limits of thinking demonstration. You should also know the limits of performance computing and algorithmic thinking. Every science is based on the theories, theorems (laws) and hypotheses that have been identified, studied and demonstrated by the strengthening, development and evolution in time of sciences. The article presents the problem of Gauss and Green theorem used to calculate the area of any polygon. Finally, we propose the following meta-model: problem solution = modelling + processing; modelling = knowledge + representation; processing = language + interpretation.",2009,,no
"Non-homogeneous Poisson Process (NHPP), stochastic model applied to evaluate the economic impact of the failure in the Life Cycle Cost Analysis (LCCA)","This paper aims to explore different aspects related with the failure costs (non reliability costs) within the Life Cycle Cost Analysis (LCCA) of a production asset. Life cycle costing is a well-established method used to evaluate alternative asset options. This methodology takes into account all costs arising during the life cycle of the asset. These costs can be classified as the 'capital expenditure' (CAPEX) incurred when the asset is purchased and the 'operating expenditure' (OPEX) incurred throughout the asset's life. In this paper we explore different aspects related with the ""failure costs"" within the life cycle cost analysis, and we describe the most important aspects of the stochastic model called: Non-homogeneous Poisson Process (NHPP). This model will be used to estimate the frequency failures and the impact that could cause the diverse failures in the total costs of a production asset. The paper also contains a case study where we applied the above mentioned concepts. Finally, the model presented provides maintenance managers with a decision tool that optimizes the life cycle cost analysis of an asset and will increase the efficiency of the decision-making process related with the control of failures.",2009,,no
Process modeling for quality in order-handled manufacturing system,"This paper deals with mechanical products process modeling for quality in order-handling manufacturing system. It investigates an influence of the product and process qualitatives-quantitatives indications to the manufacturing quality. The research is based on product design features peculiarities as material, geometrical form, dimensions, tolerances, and roughness of surfaces and so on. The graph theory for transformation of mechanical part drawings' data into digital codes has been applied. The developed model can forecast product manufacturing and quality cost at the early stage of a product design and also it is useful at the new business consideration phase in order-handling manufacturing systems. Modeling of quality cost can decrease the risk of production errors in high variety and low volume manufacturing system rightly estimating its possibilities and suggesting decision making for creation of a right process structure and fabrication facility and tooling aiming the minimal production and quality costs. The created model is tested and validated in two Lithuanian manufacturing companies and it is able to use in mechanical engineering. industry and also for study process in technical universities.",2009,,no
MODELLING OF DECISION-MAKING PROCESS RELATING TO DESIGN FOR MAINTAINABILITY OF COMPLEX TECHNICAL ITEMS,"This paper deals with modelling issues of the decision-making process relating to design for maintainability of complex technical items. Especially, we presented the formal notation of a decision-making system, its elements and functions, and general description as well. Moreover, some application examples of design for the maintainability of such complex technical items as the ship power plants are presented. For development of the decision-making system, we applied a concept of the Mestrovic's general system. Based on this system, we have worked out the computer-aided system called Maintainability Design System for ship power plants.",2009,,no
"Modeling, Estimating and Predicting the Packet-level Bit Error Rate Process in IEEE 802.15.4 LR-WPANs Using Hidden Markov Models","This paper describes a stochastic wireless channel model that captures the behavior of the packet-level Bit Error Rate (BER) and the Link Quality Indication (LQI) processes. The model is based on a discrete-time Hidden Markov Model (HMM) whose hidden states correspond to different BERs, and whose observable states correspond to different LQI values. We use the Baum-Welch algorithm to train the HAIM. The data required as input to the training phase is captured experimentally using IEEE 802.15.4 compliant Crossbow MICAz motes. We demonstrate the RAM-based Channel Model's (HCM) utility and versatility by three applications. In the first we use it to synthesize traces whose properties closely resemble those of the training data. This application simultaneously demonstrates the HCM's correctness as well. In the second application we demonstrate the HCM's ability to estimate a received packet's BER based on the LQI with which it was received. In the third application we demonstrate the HCM's ability to predict the BER to which future packets will be subjected. For evaluation purpose we restrict our prediction to the next packet.",2009,10.1109/CISS.2009.5054724,no
"Real-Time, Non-Invasive Measurement of Medical Aerosol Charge and Size Distribution Signal processing strategy, system modelling and optimization","This paper describes and compares two particle excitation methods, square and sine wave, used in conjunction with novel application of Phase Doppler Anemometry (PDA) to obtain particle size and charge simultaneously in real time. Different signal processing strategies for particle velocity and size estimation are investigated, including Quadrature Demodulation (QD), spectral analysis and correlation technique. The system performance has been tested using Monte Carlo simulations obtained from the synthesized Doppler burst signals. Finally a mathematical model has been developed in order to estimate the percentage of particles captured in the measurement volume and successfully analyzed by the signal processing system leading to the optimization of the system parameters. It can be concluded that the signal processing system working in the square-wave excitation field and based on the spectral analysis performs better both in terms of the measurement range and the SNR requirement.",2009,,no
Error-free scheduling for batch processes using symbolic model verifier,"This paper focuses on the development of a new approach for the synthesis of error-free operating schedules in batch processes. The synthesis of error-free operating procedures for batch processes becomes an important issue in the safe operation of industrial plant. It spends considerable amount of time and effort in scheduling and verifying operating procedures for correctness and completeness In this study, we adopted SMV (Symbolic Model Verifier), an automatic error finding system, which is applied to various batch processes to test their safety and feasibility The strength of this method is to minimize safety hazard and operability errors, and adjust process and recipe changes during the planning of operating procedure. The proposed approach identifies embedded errors and finds a minimum makespan and synthesizes an error-free operating sequence at the same time Several examples are presented to illustrate the effectiveness of the proposed approaches. (C) 2009 Elsevier Ltd All rights reserved.",2009,10.1016/j.jlp.2009.01.001,no
Understanding and modelling of interactions in bioleach processes,"This paper gives a brief introduction to the modelling of bioleach processes developed from a careful analysis of the fundamental process steps at the gas-liquid, biological and mineral interfaces and how these interact in a given reactor environment (tanks and heaps). The insights gained from modelling work can guide both engineers in the optimisation of processes and scientist in directing their research at areas not yet well understood. Some perspectives Of future directions of the bioleaching field are offered.",2009,,no
Understanding the effects of requirements volatility in software engineering by using analytical modeling and software process simulation,"This paper introduces an executable system dynamics simulation model developed to help project managers comprehend the complex impacts related to requirements volatility on a software development project. The simulator extends previous research and adds research results from an empirical survey. including over 50 new parameters derived from the associated survey data, to a base model. The paper discusses detailed results from two cases that show significant cost, schedule, and quality impacts as a result of requirements volatility. The simulator can be used as an effective tool to demonstrate the complex set of factor relationships and effects related to requirements volatility. (C) 2009 Elsevier Inc. All rights reserved.",2009,10.1016/j.jss.2009.03.014,no
Sensitivity analysis of stamping processes using various friction models appropriate for non-stationary contact problems,"This paper looks at the sensitivity of thickness to variation of friction. The models of friction used are: the classic Amontons-Coulomb; a nonlinear pressure-dependent model proposed by Wriggers, vu Van and Stein; and a velocity-dependent model proposed by Molinari, Estrin and Mercier. They are coded in FORTRAN for use with finite element program ABAQUS. The contact problem is then formulated in the total Lagrangian formulation for contact between an elastic - plastic body and rigid tools. The variational (weak) form of the formulation is given and this is discretized by finite element method. To test and compare the models, one common metal forming processes is simulated: deep drawing of a square-cup. The sensitivity graphs showing each of the three friction models together is given at the end. One other conclusion although not major part of this work is that Amonton-Coulomb is not the best model suited for contact conditions in metal forming processes, because Wriggers et al. model and Molinari et al. model provide better results for modelling bends and corners. (C) 2009 The Franklin Institut. Published by Elsevier Ltd. All rights reserved.",2009,10.1016/j.jfranklin.2009.05.002,no
"Model-Based Fault-Tolerant Control of Particulate Processes: Handling Uncertainty, Constraints and Measurement Limitations","This paper presents a methodology for the integrated synthesis of a fault detection and isolation (FDI) and fault-tolerant control (FTC) system for particulate processes described by population balance models. The approach is based on a suitable low-order model of the process and accounts explicitly for model uncertainty, control constraints and measurement errors. The main idea is to shape the fault-free closed-loop dynamics via robust control in a way that enables the derivation of FDI and reconfiguration criteria that are less sensitive to the uncertainties and measurement errors. The results are illustrated through an application to a continuous crystallizer with a fines trap.",2009,,no
MODELING AND EXPERIMENTAL VALIDATION OF THE LEARNING PROCESS DURING CLOSED-LOOP BMI OPERATION,"This paper presents a model and experimental validation of the learning process during operation of a closed-loop brain-machine interface. The model consists of a population of simulated cortical neurons, a decoder that transforms neural activity into motor output, a feedback controller whose role is to reduce the error based on an error-descent algorithm, and an open-loop controller whose parameters are updated based on the corrections made by the feedback controller. Using this approach, we show that the population of neurons can learn the inverse model of the decoder. Then, we validate the model by comparing its predictions with real experimental data recorded from a macaque monkey. Such a simulation tool will be useful to predict the behavior of a closed-loop BMI and in the design of optimal decoders.",2009,10.1109/ICMLC.2009.5212798,no
A Model for Measurement and Analysis of the Workflow Processes,"This paper presents all approach based on 3-layered model that may be useful for analysis and measurement of workflow processes. The workflow system can evaluate a state of the products or services ill each point of processing and is divided into three layers (sources, workflow management system and workflow processes), whereas each layer covers the processing of its specific tasks.",2009,,no
Evaluating Generalized Semi Markov Process Model of SoC Bus Architectures using HCFG,"This paper presents an efficient approach based on Hierarchical Concurrent Flow Graph (HCFG) for performance evaluation of single shared bus architecture and hierarchical bus bridge architecture. The formulation is based on generalized semi Markov process model of these architectures. In particular, we focus on building model for a single shared bus architecture and extend the approach to model architecture consisting of hierarchical buses connected through bus bridge. Our modeling approach provides early estimation of performance parameters viz, memory bandwidth, processor utilization, average queue length and average waiting time. We validate the proposed modeling and evaluation approach by comparing the results of evaluation against those that are obtained by SystemC simulation of the same communication architectures under consideration. The HCFG approach is not only time efficient but also provides much detailed evaluation of stochastic properties of performance parameters as compared to SystemC simulation. To illustrate the efficacy of the approach, we compare the results with the results available in the literature for some more examples.",2009,,no
WORKFLOW BASED PROCESS MODELING FOR OPTICAL COORDINATE MEASUREMENT,This paper presents in detail the relevance and the potential of a novel approach for process modeling in creation of fixed sequences of various measurement steps using optical coordinate measurement devices.,2009,,no
Target Based Software Process Evaluation Model and Application,"This paper presents the target-based software process evaluation model (TSPEM) with the decomposition and analysis of the orgnization goals and application of tipical technology of statistical prosess control (SPC). This paper also achieves assesment of each subprocess by analyzing the mappin relationship among orgnization goals, metric indicators and subprocesses. At the end of this paper, an example is provided as the application of this model.",2009,10.1109/ICIC.2009.34,no
Machining Process Language Understanding in 3D Process Modeling,"This paper proposed to get 3D process model and final part through by process language understanding and graphics understanding. Based on the characteristics and structure of the machining process language, machining process language could be represented by ""four elements"". Then constructed the knowledge base based on ontology. Through dividing the process statements into five ""blocks"" and extracting the process semantic information, proposed a process language understanding method. At last, combined with the graphics understanding, we developed a prototype system and gave a case. The result showed that the method was effective.",2009,10.1109/CADCG.2009.5246847,no
Discrete State Change Model of Manufacturing Quality to Aid Assembly Process Design,"This paper proposes a representation model of the quality state change in an assembly process that can be used in a computer-aided process design system. In order to formalize the state change of the manufacturing quality in the assembly process, the functions, operations, and quality changes in the assembly process are represented as a network model that can simulate discrete events. This paper also develops a design method for the assembly process. The design method calculates the space of quality state change and outputs a better assembly process (better operations and better sequences) that can be used to obtain the intended quality state of the final product. A computational redesigning algorithm of the assembly process that considers the manufacturing quality is developed. The proposed method can be used to design an improved manufacturing process by simulating the quality state change. A prototype system for planning an assembly process is implemented and applied to the design of an auto-breaker assembly process. The result of the design example indicates that the proposed assembly process planning method outputs a better manufacturing scenario based on the simulation of the quality state change.",2009,10.1299/jamdsm.3.378,no
"Intelligent Laboratory Resource Supply Chain Conceptual Network Model with Process and Information Integration, Visibility and Flexibility","This paper proposes a web-based laboratory resource supply chain conceptual model for an educational institution to increase the process and information integration, visibility and flexibility. The proposed model utilizes a reasoning engine with fuzzy, parallel fuzzy rules, and de-fuzzy processes to decide the optimal purchase ordering quantity and the best constant stocks in the laboratory. The fuzzy takes the crisp input data through the characteristic function and maps the input data into its corresponding membership degree. The fuzzy rules are processed with different degree of membership and all rules in the system are processed before triggering an action. The de-fuzzy takes each item's purchase ordering degree of membership through the singleton output membership function and generates corresponding crisp data. The model allows users keying in their required experimental materials via the Web Site, uses the database management system to integrate all related information, and applies the fuzzy reasoning engine to generate the final purchase order reports to support the executor making the optimal decisions.",2009,,no
"Enhanced AP-PE-CVD Process Understanding and Control by Application of Integrated Optical, Electrical and Modelling Diagnostics","This paper reports the use of a multiple thrust methodology to monitor the atmospheric pressure plasma-enhanced chemical vapour deposition (PE-CVD) of silica films on glass. Simultaneous spectroscopic analysis: optical emission spectroscopy, FT-IR spectroscopy and in situ near-infrared laser diode spectroscopy have been combined with electrical measurements to investigate the plasma during the deposition process. The experimentation has been carried out as a chemometric design so as to derive statistical models tying the affected variables to the measured results. Initial results confirm the viability of this approach for improved process understanding, process control and enhanced design capabilities.",2009,10.1002/ppap.200931608,no
Callous-Unemotional Traits and Social Information Processing: Multiple Risk-Factor Models for Understanding Aggressive Behavior in Antisocial Youth,"This study examined multiple risk factor models of links among callous-unemotional traits, aggression beliefs, social information processing, impulsivity, and aggressive behavior in a sample of 150 antisocial adolescents. Consistent with past research, results indicated that beliefs legitimizing aggression predicted social information processing biases and that social information processing biases mediated the effect of beliefs on aggressive behavior. Callous-unemotional traits accounted for unique variance in aggression above and beyond effects of more established risk factors of early onset of antisocial behavior, social information processing, and impulsivity. These findings add to recent research showing that callous-unemotional traits are a unique risk factor associated with aggression and criminal offending and suggest that targeting both affective and cognitive vulnerabilities may enhance clinical intervention with antisocial youth.",2009,10.1007/s10979-008-9171-7,no
Activity and Information Modeling of Comprehensive Assessment for Sustainable Process Design,"This study presents an activity model for sustainable process design with comprehensive assessment integrating risk assessment (RA) and life cycle assessment (LCA). The type-zero method of Integrated DEFinition language, or IDEF0 and the Unified Modeling Language for information system, or UML, were applied for enabling systematic and effective development of procedure and information infrastructure of such comprehensive assessment in process design. Integration of different assessment can be fulfilled by these modeling. In this study, to demonstrate the proposed model, a case study is performed on metal cleaning process where various chemical substances have been utilized.",2009,,no
An interpretability-guided modeling process for learning comprehensible fuzzy rule-based classifiers,"This work presents a new process for building comprehensible fuzzy systems for classification problems. Firstly, a feature selection procedure based on crisp decision trees is carried out. Secondly, strong fuzzy partitions are generated for all the selected inputs. Thirdly, a set of linguistic rules are defined combining the previously generated linguistic variables. Then, a linguistic simplification procedure guided by a novel interpretability index is applied to get a more compact and general set of rules without losing accuracy. Finally, an efficient and simple local search strategy increases the system accuracy while preserving the high interpretability. Results obtained in several benchmark classification problems are encouraging because they show the ability of the new methodology for generating highly interpretable fuzzy rule-based classifiers while yielding accuracy comparable to that achieved by other methods like neural networks and C4.5.",2009,10.1109/ISDA.2009.24,no
Voyage without constellation: evaluating the performance of three uncalibrated process-oriented models,"Three process-oriented dynamic acidification models were applied to a long-term monitoring site without calibration to evaluate the influence of model structural differences on simulation. The models were simplified to share as many commonalities as possible so that the main structural differences could be investigated. The models differed in sub-models for cation exchange, organic acids and acid anion speciation. All models were populated with 'equivalent' parameters by systematic input mapping. The influence of input variability was addressed through Monte Carlo parameter sampling. The three models behaved exactly the same for tracers (e. g. sulphate and chloride), indicating successful cross-parameterization of the models. Differences in model structure had an impact on some of the simulated chemical parameters. In particular, models using Gapon cation exchange simulated higher base saturation levels in the long run than their Gaines-Thomas counterparts, but simulated lower base cation concentration and acid neutralizing capacity in soil solution when acid deposition levels were high. Multiple-model evaluation frameworks as presented here allow for greater certainty in model predictions; ultimately, this type of framework should be employed when evaluating the impacts of future climate and environmental changes on soil and surface water hydrogeochemistry.",2009,10.2166/nh.2009.085,no
A comprehensive model of the process of telephone nursing,"Title. A comprehensive model of the process of telephone nursing. Aim. This paper is a report of a study conducted to develop a theoretical model of the process nurses use to deliver care over the telephone. Background. Telephone nursing is practised internationally in diverse settings, and research has shown it to be an effective service. Although studies have identified important variables that influence the practice, the telephone nursing process in its entirety has not been described. Method. In this grounded theory study, data were collected from ten experienced telephone nurses from four different sites using semi-structured interviews. Concurrent data collection and analysis took place in 2005. A cumulative process of theoretical sampling and constant comparison was used to identify initial concepts, and then expand, validate, and clarify them until the concepts and relationships were fully developed. The findings were validated through peer and participant review. Findings. Telephone nursing is a dynamic and goal-oriented process consisting of three phases: gathering information, cognitive processing, and output. While generally sequential, the phases can be simultaneous or recurring in response to caller needs. Interpreting takes place throughout the call; that is, the nurse translates data from the caller into healthcare information and healthcare information into caller language. Factors shaping the process are call prioritization and level of complexity, resource availability, and the nurse's need for validation. Conclusion. Telephone nursing training and practice should emphasize gathering information, using implicit and explicit information to identify client needs, and translating healthcare information back into language comprehensible to clients.",2009,10.1111/j.1365-2648.2009.05132.x,no
Modelling of the Flow Distribution on a Porous Material in a PEMFC and Image Processing Validation,"To determine the adequateness of the material, a model based on the analogy between fluidic and electric flows is proposed to analyse the flow distribution on a porous-media bipolar plate. The model shows higher flow in the periphery than in the central area. It also shows that this difference depends on the ratio between the hydraulic resistance of the porous media and that of the collectors/distributors. The proposed model and the results of the experimental work performed can be used for the design of porous material for PEMFC bipolar plates. The theoretical results are validated by image analysis techniques, which process the patterns obtained when passing a coloured fluid through the porous-media. The acquired images were processed, to calculate approximate flow speeds at different locations on the bipolar plate, which are compared to the model predictions. The images were also processed to obtain a visual representation of the flow distribution.",2009,,no
Modeling and Simulating the Quality of Sequential Iterative Development Processes,"To effectively manage and improve the product developing process quality, a quantitative quality model is proposed for sequential iterative processes. To estimate and manage the expected process quality, a quality estimate simulation model and a quality improvement simulation model are established. Experiment results for process quality estimate and improvement are presented by an example of software test process.",2009,10.1109/ITCS.2009.289,no
Structural modeling of mutant alpha-glucosidases resulting in a processing/transport defect in Pompe disease,"To elucidate the mechanism underlying transport and processing defects from the viewpoint of enzyme folding, we constructed three-dimensional models of human acid a-glucosidase encompassing 27 relevant amino acid substitutions by means of homology modeling. Then, we determined in each separate case the number of affected atoms, the root-mean-square distance value and the solvent-accessible surface area value. The analysis revealed that the amino acid substitutions causing a processing or transport defect responsible for Pompe disease were widely spread over all of the five domains comprising the acid a-glucosidase. They were distributed from the core to the surface of the enzyme molecule, and the predicted structural changes varied from large to very small. Among the structural changes, we paid particular attention to G377R and G483R. These two substitutions are predicted to cause electrostatic changes in neighboring small regions on the molecular surface. The quality control system of the endoplasmic reticulum apparently detects these very small structural changes and degrades the mutant enzyme precursor (G377R), but also the cellular sorting system might be misled by these minor changes whereby the precursor is secreted instead of being transported to lysosomes (G483R). Journal of Human Genetics (2009) 54, 324-330; doi:10.1038/jhg.2009.32; published online 3 April 2009",2009,10.1038/jhg.2009.32,no
Modeling Efficiency at the Process Level: An Examination of the Care Planning Process in Nursing Homes,"To examine the efficiency of the care planning process in nursing homes. We collected detailed primary data about the care planning process for a stratified random sample of 107 nursing homes from Kansas and Missouri. We used these data to calculate the average direct cost per care plan and used data on selected deficiencies from the Online Survey Certification and Reporting System to measure the quality of care planning. We then analyzed the efficiency of the assessment process using corrected ordinary least squares (COLS) and data envelopment analysis (DEA). Both approaches suggested that there was considerable inefficiency in the care planning process. The average COLS score was 0.43; the average DEA score was 0.48. The correlation between the two sets of scores was quite high, and there was no indication that lower costs resulted in lower quality. For-profit facilities were significantly more efficient than not-for-profit facilities. Multiple studies of nursing homes have found evidence of inefficiency, but virtually all have had measurement problems that raise questions about the results. This analysis, which focuses on a process with much simpler measurement issues, finds evidence of inefficiency that is largely consistent with earlier studies. Making nursing homes more efficient merits closer attention as a strategy for improving care. Increasing efficiency by adopting well-designed, reliable processes can simultaneously reduce costs and improve quality.",2009,10.1111/j.1475-6773.2008.00895.x,no
Mission Reliability Model and Indexes Evaluation of Discrete Processing System,"To improve the productivity and mission availability of discrete processing system, a mission reliability model is presented and the modeling method is introduced. During the modeling, producing unqualified products is defined as the failure of processing system. Both equipment failures and processing failures including handlers, methods, materials, etc, which will result in unqualified products, are considered synchronously. The connections between each failure unit are also built. The reliability indexes are proposed based on the mission reliability model and the calculation method are researched. Finally, the mission reliability model of an actual manufacturing process of a certain product is built, and the relevant indexes are calculated and evaluated.",2009,,no
"4K-cells Resistive and Charge-Base-Capacitive Measurement Test Structure Array (R-CBCM-TSA) for CMOS Logic Process Development, Monitor and Model","To maximize the design efficiency of the test chip area and maintain the high accuracy measurement requirement of resistors and capacitors, a 4K-cells resistive and charge-base capacitive test structure array is designed for CMOS logic process development, monitor and model. The test chip utilizes 4-terminal (one of 4 is strongly grounded) Kelvin force/sense measurement for resistive-type and charge-base capacitance measurement (CBCM) for capacitive-type test structures. With the aid of memory-addressing design scheme, any one of the device-under-test in an array can be randomly or sequentially selected for testing with all of them sharing a common probe pad group. To accelerate the testing speed, the address control signals of 8 test structure array are connected in parallel for synchronized parallel testing. A 32x16x8 test structure array has been implemented by utilizing a state-of-the-art logic process to demonstrate design feasibility. The results confirm the excellence of this architecture in measurement with 0.1fF for capacitive and 0.1ohm for resistive systematic errors, and 7 times testing speed improvement.",2009,10.1109/ICMTS.2009.4814645,no
Modelling dynamic value streams in support of process design and evaluation,"To remain competitive, most manufacturing enterprises (MEs) need cost-effective and responsive business processes with capability to realise multiple value streams specified by changes in customer needs. Models of MEs can play a critical role in enabling enhanced enterprise process and systems design and change based on analysis of their performance, and ongoing management and control of their operation. Typical models of MEs can provide reusable computational representations of organisational structures, processes, information, resources and related value flows in an enterprise. This paper presents a dynamic modelling approach to value stream mapping which enhances current best practice when reasoning about changing process and resource systems requirements. Here, coherent use of enterprise and simulation modelling techniques were deployed to develop value streams of a case study enterprise which is a make-to-order furniture manufacturing SME. The paper explains how models created during the modelling stages were validated and reused as a basis for informed SME decision making in relation to product realisation strategies and related organisation design and change decisions and actions.",2009,10.1080/09511920802527574,no
Fuzzy intelligent quality monitoring model for X-ray image processing,"Today's imaging diagnosis needs to adapt modern techniques of quality engineering to maintain and improve its accuracy and reliability in health care system. One of the main factors that influences diagnostic accuracy of plain film X-ray on detecting pathology is the level of film exposure. If the level of film exposure is not adequate, a normal body structure may be interpretated as pathology and vice versa. This not only influences the patient management but also has an impact on health care cost and patient's quality of life. Therefore, providing an accurate and high quality image is the first step toward an excellent patient management in any health care system. In this paper, we study these techniques and also present a fuzzy intelligent quality monitoring model, which can be used to keep variables from degrading the image quality. The variables derived from chemical activity, cleaning procedures, maintenance, and monitoring may not be sensed, measured, or calculated precisely due to uncertain situations. Therefore, the gamma-level fuzzy Bayesian model for quality monitoring of an image processing is proposed. In order to apply the Bayesian concept, the fuzzy quality characteristics are assumed as fuzzy random variables. Using the fuzzy quality characteristics, the newly developed model calculates the degradation risk for image processing. A numerical example is also presented to demonstrate the application of the model.",2009,10.3233/XST-2009-0228,no
A Total Quality Management Model for the Construction Process under ISO 9000,"Total Quality Management (TQM) means ""business excellence"". It is defined as both a quality management philosophy and a set of guiding principles that foster an organizational culture and participation of all members of the organization aiming at long-term success through customer satisfaction and continuous improvement of products or services, and benefits to all the members and society. In the paper, a TQM model and the quality management system have been developed in an attempt to integrate the eight principles of ISO 9000, the five standard modules of ISO 9001, the TQM techniques, and the six performances indicators (i.e., quality, cost, time, safety, communication and morale) into construction practice. Under the standards with respect to the model developed in the paper, good relationship between contractors and clients is established by having well-defined and mutually agreed requirements for the product and service. Thus, the system will result in better quality of works, more efficient allocation of resources, less of materials, and, the most important of all, better site safety.",2009,,no
Trajectory Processing Under Incomplete Measurement Situation Using a Sparse Representation Model,"Under the incomplete measurement situation, how to get a smoothing trajectory with high precision is an important problem. In this article, a fusion method based on the Error Model Best Estimate of Trajectory (EMBET) algorithm is proposed to handle the incomplete measurement problem by using a sparse representation model for trajectory of reentry target. And the sparse representation model of trajectory is built by analyzing the dynamic characteristics of launch vehicles. Simulations show the effectiveness of the proposed method.",2009,,no
QUALITY MANAGEMENT IN KNOWLEDGE INTENSIVE BUSINESS PROCESSES Development of a Maturity Model to Measure the Quality of Knowledge Intensive Business Processes in Small and Medium Enterprises,"Up to now the isolated tools for quality, business process, and knowledge management can be integrated to develop a suitable structure for SMEs to measure and gradually build up the competence for knowledge processing. A maturity model is being developed for SMEs to measure and assess the quality of their business processes. This enables the companies to determine their existing status, and to take the necessary actions for the competence development of their business processes, which should contribute to the attainment of their knowledge management goals.",2009,,no
Impact assessment of climate change on rice production in Asia in comprehensive consideration of process/parameter uncertainty in general circulation models,"We assessed the impact of climate change oil rice production in Asia in comprehensive consideration of the process/parameter uncertainty ill general circulation models (GCMs). After inputting future Climate scenarios based oil the projections of GCMs for three Special Report oil Emissions Scenarios (SRES) (18 GCMs for A1B, 14 GCMs for A2, and 17 GCMs for B1) into a crop model, we calculated the average change in production (A(CP)), the standard deviation of the change in production (SDCP), and the probability of a production decrease (P-PD) for each SIRES scenario, taking into account the effect of CO2 fertilization. In the 2020s, P-PD, values were high for all SRES scenarios because the negative impacts of climate change were larger than the positive effects of CO2 fertilization in almost all climate scenarios in the near future. This Suggests that it will he necessary to take immediate adaptive actions, regardless of the emission scenario, ill the near future. In the 2080s, there were large differences in A(CP), SDCP, and P-PD among the SIRES scenarios. The scenario with the highest atmospheric CO2 concentration, A2, showed a notable decrease in production and a high P-PD in the 2080s compared with the other scenarios, despite having the largest CO2 fertilization effect. In addition, A2 had the largest SDCP, among the SIRES scenarios. On the other hand, the scenario with the lowest atmospheric CO2 concentration, B1, showed a small decrease in production, and a Much smaller SDCP and a much lower P-PD, than in the case of A2. These results for the 2080s Suggest that a reduction ill CO2 emissions in the long term has great potential not only to mitigate decreases in rice production, but also to reduce the uncertainty in these changes. (C) 2009 Elsevier B.V. All rights reserved.",2009,10.1016/j.agee.2009.02.004,no
Model selection for Levy measures in diffusion processes with jumps from discrete observations,"We deal with parametric inference and selection problems for jump components in discretely observed diffusion processes with jumps. We prepare several competing parametric models for the Levy measure that might be misspecified. and select the best model from the aspect of information criteria. We construct quasi-information criteria (QIC), which are approximations of the information criteria based on continuous observations. (C) 2008 Elsevier B.V. All rights reserved.",2009,10.1016/j.jspi.2008.05.009,no
A semi-Markov multistate model for estimation of the mean quality-adjusted survival for non-progressive processes,"We discuss the estimation of the expected value of the quality-adjusted survival, based on multistate models. We generalize an earlier work, considering the sojourn times in health states are not identically distributed, for a given vector of covariates. Approaches based on semiparametric and parametric (exponential and Weibull distributions) methodologies are considered. A simulation study is conducted to evaluate the performance of the proposed estimator and the jackknife resampling method is used to estimate the variance of such estimator. An application to a real data set is also included.",2009,10.1007/s10985-008-9106-0,no
A BRANCHING PROCESS MODEL FOR FLOW CYTOMETRY AND BUDDING INDEX MEASUREMENTS IN CELL SYNCHRONY EXPERIMENTS,"We present a flexible branching process model for cell population dynamics in synchrony/time-series experiments used to study important cellular processes. Its formulation is constructive, based on an accounting of the unique cohorts in the population as they arise and evolve over time, allowing it to be written in closed form. The model can attribute effects to subsets of the population, providing flexibility not available using the models historically applied to these populations. It provides a tool for in silico synchronization of the population and can be used to deconvolve population-level experimental measurements, such as temporal expression profiles. It also allows for the direct comparison of assay measurements made from multiple experiments. The model can be fit either to budding index or DNA content measurements, or both, and is easily adaptable to new forms of data The ability to use DNA content data makes the model applicable to almost any organism. We describe the model and illustrate its utility and flexibility in a study of cell cycle progression in the yeast Saccharomyces cerevisiae.",2009,10.1214/09-AOAS264,no
Process Data: a Means to Measure Operational Performance and Implement Advanced Analytical Models,"We present the case of an ambulatory clinic in which an operational review was conducted to identify opportunities for efficiency in appointment scheduling and capacity allocation. We required process data to compare that which was planned to that which actually happened and to develop advanced analytical models. Similar to other health care studies, these data proved to be limited or non-existent. Consequently we had to conduct a time-consuming collection of operational metrics. We make recommendations for the perpetual collection of process data for modeling and simulation.",2009,10.3233/978-1-58603-979-0-17,no
A Dynamically Customizable Process-Centered Evaluation Model,"We present the graph operational semantics approach used for defining NiMo nets execution, which mix lazy, data-driven and a weak form of eager evaluation, all in parallel. NiMo is a totally graphic language from the family of higher order typed languages but with a strong data-flow inspiration. Programs are process networks that evolve showing the full state at each execution step and can be dynamically changed or completed. In NiMo parallelization is implicit. The conjunction of two kinds of tags determine which processes are able to act in the same execution step. According to their tag, processes can behave in five modes that can be globally or locally set for each one, and can be also dynamically changed. Combining modes gives a very flexible way to experiment different strategies to increase processor usage, decrease channel population, and achieve subnet synchronization. Together with symbolic execution it also provides the means for generative and multi-stage programming.",2009,10.1145/1599410.1599416,no
The Relation between Students' Epistemological Understanding of Computer Models and their Cognitive Processing on a Modelling Task,"While many researchers in science education have argued that students' epistemological understanding of models and of modelling processes would influence their cognitive processing oil a modelling task, there has been little direct evidence for such ail effect. Therefore, this studs, aimed to investigate the relation between students' epistemological understanding of models and modelling and their cognitive processing (i.e., deep versus surface processing) on a modelling task. Twenty-six students, working in dyads, were observed while working on a computer-based modelling task in the domain of physics. Students' epistemological understanding was assessed on four dimensions (i.e., nature of models, purposes of models, process of modelling, and evaluation of models). Students' cognitive processes were assessed based on their verbal protocols, using a coding scheme to classify their types of reasoning. The outcomes confirmed the expected positive correlation between students' level of epistemological understanding and their deep processing (r = 0.40, p = .04), and the negative correlation between level of epistemological understanding and surface processing (r = -0.51, p = .008). From these results, we emphasise the necessity of considering epistemological understanding in research as well as in educational practice.",2009,10.1080/09500690802192181,no
Modelling and integration of customer flexibility in the order commitment process for high mix low volume production,"With increasing product variety and dynamic demand fluctuation, manufacturing industry is moving towards a high product mix and low order volume production environment. Consequently, the order commitment process is becoming one of the most important processes for manufacturing firms to meet individual customer's needs with limited resources. However, demands for shortened delivery lead time, diverse customer requirements and more frequent customer orders have made the order commitment task more challenging. This paper attempts to tackle these new challenges by incorporating not only manufacturing flexibility but also flexibility from the demand side. Customer flexibility is characterised by customer indifference to certain product attributes and/or delivery schedules. Intuitively, with the consideration of customer flexibility, both manufacturers' and customers' interests can be better served since the solution space of matching demand and supply can be extended beyond the traditional domain purely from a manufacturing perspective. To this end, a systematic approach is developed to characterise and model customer flexibility. A mixed-integer-programming model is formulated to provide optimal order commitment decisions.",2009,10.1080/00207540802266474,no
A Quality Optimization Method for Service Process Model,"With the popularity of Web services, a set of Web services with similar functions but different qualities can be found. Since services are often be composed through a service process model, the way to compose them affects the whole quality of the process model itself. In this paper, a quality optimization method for the service process model is proposed, which takes the structure of service process into account. Based on the proposed quality model, a genetic algorithm is proposed to optimize the service process model.",2009,10.1109/ISPA.2009.39,no
Preserving correctness during business process model configuration,"A configurable process model captures a family of related process models in a single artifact. Such models are intended to be configured to fit the requirements of specific organizations or projects, leading to individualized process models that are subsequently used for domain analysis or solution design. This article proposes a formal foundation for individualizing configurable process models incrementally, while preserving correctness, both with respect to syntax and behavioral semantics. Specifically, assuming the configurable process model is behaviorally sound, the individualized process models are guaranteed to be sound. The theory is first developed in the context of Petri nets and then extended to a process modeling notation widely used in practice, namely Event-driven Process Chains.",2010,10.1007/s00165-009-0112-0,no
Modelling and experimental validation of the hot-gas defrost process of an air-cooled evaporator,"A detailed transient simulation model has been developed to predict and evaluate the performance of the hot-gas defrost process of an air-coil evaporator. In the model, the defrost process is subdivided into six stages: preheating, tube frost melting start, fin frost melting start, air presence, tube-fin water film and dry-heating. In each stage, the control volume is subdivided into systems represented by a single node, which has the representative properties of the system. A finite difference approach was used to solve the model equations. The results include the time required to defrost, the distribution of the energy during defrost process, the instantaneous refrigerant properties and the instantaneous fin and tube temperature distribution. The results are compared with experimental data obtained in a local storage facility under actual operating conditions and also using data available in the literature. The model results substantially agree with the experimental data in both cases. (C) 2010 Elsevier Ltd and IIR. All rights reserved.",2010,10.1016/j.ijrefrig.2009.12.027,no
A Trust Evaluation Model using Controlled Markov Process for MANET,"A distributed trust evaluation model is presented for MANETs by which uncertainties of trust are transformed into probability vectors giving the probability distribution of trust levels. The system evolves over time as a finite-state Markov process with variant transition matrixes. We attempt to predict the trustworthiness values of entities that are determined by their inherent error patterns. The Markov process is associated to a Bonus-Malus System and controlled by the estimated error patterns of individual entities involved. As well, an iteration algorithm is designed to prevent inaccurate predictions for trust values because of the properties of Markov process. The simulation results demonstrate that our model is able to predict local trust successfully for entities in MANETs by estimating their actual error patterns accurately.",2010,,no
Control of Process Operations and Monitoring of Product Qualities through Generic Model-based in Batch Cooling Crystallization,"A generic model-based framework has been developed for crystallization processes, with applications aiming at the control of process operations and the monitoring of product quality. This generic model-based framework allows the systematic development of a wide range of crystallization models for different operational scenarios. This enables the design and control engineers to analyze various crystallization operations and conditions, thus facilitating the development of process control and monitoring systems (PAT systems) for crystallization processes. The generic framework has been implemented in the ICAS-PAT software which allows the user to design and validate PAT systems through a systematic computer-aided framework. The application of the framework is highlighted for batch cooling crystallization of paracetamol where the framework was applied for design of a process monitoring and control system to obtain a desired crystal size distribution (CSD).",2010,,no
High efficiency chemical energy conversion system based on a methane catalytic decomposition reaction and two fuel cells: Part I. Process modeling and validation,"A highly efficient integrated energy conversion system is built based on a methane catalytic decomposition reactor (MCDR) together with a direct carbon fuel cell (DCFC) and an internal reforming solid oxide fuel cell (IRSOFC). In the MCDR, methane is decomposed to pure carbon and hydrogen. Carbon is used as the fuel of DCFC to generate power and produce pure carbon dioxide. The hydrogen and unconverted methane are used as the fuel in the IRSOFC. A gas turbine cycle is also used to produce more power output from the thermal energy generated in the IRSOFC. The output performance and efficiency of both the DCFC and IRSOFC are investigated and compared by development of exact models of them. It is found that this system has a unique loading flexibility due to the good high-loading property of DCFC and the good low loading property of IRSOFC. The effects of temperature, pressure, current densities, and methane conversion on the performance of the fuel cells and the system are discussed. The CO(2) emission reduction is effective, up to 80%, can be reduced with the proposed system. (C) 2010 Elsevier B.V. All rights reserved,",2010,10.1016/j.jpowsour.2010.04.047,no
Research on Knowledge Transfer Process and Performance Evaluation Model among Supply Chain Members,"A knowledge transfer process model among supply chain members is proposed. The relationship between factors of influencing knowledge transfer and knowledge transfer performance is discussed with the methods of theoretical and quantitative analysis. The results show that the knowledge transfer among supply chain members can be divided into three levels, including between supply chain and external environment, between members of supply chain, and organization's internal knowledge transfer. Knowledge transfer performance among supply chain members is affected by knowledge characteristics, communication and trust degree among supply chain members, supply chain members' knowledge absorptive capacity, knowledge transfer channels' richness and knowledge transfer cost. An undirected weighted small-world network model can be used to evaluate the knowledge transfer performance among supply chain members.",2010,,no
A Kernel-Based DfM Model for Process from Layout to Wafer,"A layout design that passes the design rule check (DRC) may still have manufacturing problems today, especially around areas of critical patterns. Thus a design-for-manufacturability (DfM) model, which can simulate the process from designed layout to wafer and predict the final contours, is necessary. A new kind of DfM model called free-element-model (FEM) is proposed in this paper. The framework of FEM is borrowed from the forward process model, which is basically a set of convolution kernels in matrix form, yet the unknown variables are the kernel elements instead of process parameters. The modeling process is transformed into a non-linear optimization problem, with equality constraints which involve norm-2 regulation of kernels and inner production of any two kernels to keep the normalization and orthogonality of optimized kernels. Gradient-based method with Lagrange penalty function is explored to solve the optimization problem to minimize the difference between simulated contours and real contours. The dimension of kernels in FEM is determined by the cutoff frequency and the ambit. Since kernels are calculated by optimization method instead of decomposition of transmission cross coefficient (TCC), every element of kernels becomes a factor to describe the process. FEM is more flexible, and in it all effects that can be integrated into convolution kernels join naturally, such as the resist deviation and asymmetry of the process. No confidential process parameters, for example NA and defocus, appear in FEM explicitly, and thus the encapsulated FEM is suitable for IC manufacturers to publish. Moreover, enhancements and supplements to FEM are discussed in this paper, including the sufficiency of test patterns. In our experiments, DfM models for 2 process lines are generated based on test patterns, and the results show that the simulated shapes have an area error less than 2% compared to the real shapes of test patterns and an area error less than 3% compared to the shapes in typical blocks chosen from chip for verification purpose. The root mean square error of contour deviation between the 2 simulation results from FEM and conventional lithographic model is 10nm in a 65nm process.",2010,10.1117/12.844671,no
Modeling of the Precipitation Process in a Rotating Packed Bed and Its Experimental Validation,"A mixing-precipitation model based on the modified coalescence-redispersion model was presented to describe the flow, mixing, nucleation and growth in a rotating packed bed (RPB). The model was coupled with population balance, mass balance and crystallization kinetics. It predicted well the influence of coalescence probability, which represents the mixing intensity among droplets, on the particle number density, supersaturation and mean particle size of the produced precipitates. The effects of the radial thickness of packing, liquid flow rate and rotating speed on the product particle size were also investigated. The results indicate that the needed radial length of packing is short for sparingly soluble substance precipitation (about 40-50 mm in this work), and the mean particle size of precipitates decreases with the increase of rotating speed and liquid flow rate, respectively. The validity of this model was verified by experiment on BaSO(4) precipitation in APB.",2010,,no
Model-Eliciting Activities: Assessing Engineering Student Problem Solving and Skill Integration Processes,"A Model-Eliciting Activity (MEA) presents student teams with a thought-revealing, model-eliciting, open-ended, realistic, client-driven problem for resolution. Here we extend the original MEA construct developed by mathematics education researchers to upper-level engineering coursework and introduce an ethical component. Our extensions also require students to integrate previously learned concepts as they gain new understanding. We propose that MEAs offer engineering educators at least two potential benefits: improved conceptual understanding and a means for assessing the problem solving process. However, these benefits will only accrue if the MEAs are properly implemented. Consequently, relative to the first we propose certain strategies, learned through experience, for successful MEA implementation, recognizing that this is not a simple task. In addition, we suggest using MEAs as assessment tools and illustrate how they can help analyze students' problem solving processes. Our findings are based on experiments conducted in different learning environments over a two year period at the University of Pittsburgh's Swanson School of Engineering. This paper should serve as a resource for those engineering educators and researchers interested in implementing MEAs.",2010,,no
Modeling of Defect Formation Processes in Dislocation-Free Silicon Single Crystals,A new alternative model of the formation and transformation of grown-in microdefects in dislocation-free silicon single crystals is proposed. The basic concepts of the alternative mathematical model of the formation of secondary grown-in microdefects are considered. It is shown that vacancy micropores are formed in a narrow temperature range of 1130-1070 degrees C. This is caused by a sharp decrease in the concentration of background impurity which does not form agglomerates (they arise in the temperature range of 1420-1150 degrees C upon crystal cooling).,2010,10.1134/S1063774510040164,no
Compact Process and Layout Aware Model for Variability Optimization of Circuit in Nanoscale CMOS,"A predictive MOSFET model is very critical for early circuit design in nanoscale CMOS technologies. In this work, we developed a new compact MOSFET model which can dramatically improve the predictability of BSIM4 for the major 3 process and 2 layout variations by applying the simple physics-based equations to model these parameters. The accuracy of the model is verified using numerical TCAD simulation results and measurements under full range of temperature and bias conditions. The compact model for the circuit simulation can be efficiently used to predict the effects of process and layout variations on the circuit characteristics.",2010,,no
Time-interval process model discovery and validation-a genetic process mining approach,"A process management technique, called process mining, received much attention recently. Process mining can extract organizational or social structures from event logs recorded in an information system. However, when constructing process models, most process mining searches consider only the topology information among events, but do not include the time information. To overcome the drawbacks, a time-interval genetic process mining framework is proposed. First, time-intervals between events are derived for all event sequences. A discretization procedure is then developed to transform time-interval data from continues type to categorical type. Second, the genetic process mining method which is based on global search strategy is applied to generate time-interval process models. Finally, a precision measure is defined to evaluate the quality of the generated models. With the measure, managers can select the best process model among a set of candidate models without human involvement.",2010,10.1007/s10489-010-0240-5,no
A Cox model for radioactive counting measure: Inference on the intensity process,"A solution to the problem of calibrating a counting device from observed data, is developed in this paper by means of a Cox process model. The stochastic intensity of the process for counting emitted particles is estimated by functional principal components analysis and confidence bands are provided for two radioactive isotopes, (226)Ra and (137)Cs. A hypothesis test to assess the coherence of the new observed data with the estimated model is also included. (C) 2010 Elsevier B.V. All rights reserved.",2010,10.1016/j.chemolab.2010.06.002,no
"Substrate Uptake, Phosphorus Repression, and Effect of Seed Culture on Glycopeptide Antibiotic Production: Process Model Development and Experimental Validation","Actinomycetes, the soil borne bacteria which exhibit filamentous growth, are known for their ability to produce a variety of secondary metabolites including antibiotics. Industrial scale production of such antibiotics is typically carried Out in a multi-substrate medium where the product formation may experience catabolite repression by one or more of the substrates. Availability of reliable process models is a key bottleneck in optimization of such processes. Here we present a structured kinetic model to describe the growth, substrate uptake and product formation for the glycopeptide antibiotic producer strain Amycolatopsis balhimycina DSM5908. The model is based on the premise that the organism is an optimal strategist and that the various metabolic pathways are regulated via key rate limiting enzymes. Further, the model accounts for substrate inhibition and catabolite repression. The model is also able to predict key phenomena such as simultaneous uptake of glucose and glycerol but with different specific uptake rates, and inhibition of glycopeptide production by high intracellular phosphate levels. The model is Successfully applied to both production and seed medium with varying compositions and hence has good predictive ability over a variety of operating conditions. The model parameters are estimated via a well-designed experimental plan. Adequacy of the proposed model was established via checking the model sensitivity to its parameters and confidence interval calculations. The model may have applications in optimizing seed transfer, medium composition, and feeding strategy for maximizing production. Biotechnol. Bioeng. 2010;105: 109-120. (C) 2009 Wiley Periodicals, Inc.",2010,10.1002/bit.22505,no
Aircraft measurements and model simulations of stratospheric ozone and N2O: implications for chemistry and transport processes in the models,"Airborne measurements of stratospheric ozone and N2O from the SCIAMACHY (Scanning Imaging Absorption Spectrometer) Validation and Utilization Experiment (SCIA-VALUE) are presented. The campaign was conducted in September 2002 and February-March 2003. The Airborne Submillimeter Radiometer (ASUR) observed stratospheric constituents like O-3 and N2O, among others, spanning a latitude from 5A degrees S to 80A degrees N during the survey. The tropical ozone source regions show high ozone volume mixing ratios (VMRs) of around 11 ppmv at 33 km altitude, and the altitude of the maximum VMR increases from the tropics to the Arctic. The N2O VMRs show the largest value of 325 ppbv in the lower stratosphere, indicating their tropospheric origin, and they decrease with increasing altitude and latitude due to photolysis. The sub-tropical and polar mixing barriers are well represented in the N2O measurements. The most striking seasonal difference found in the measurements is the large polar descent in February-March. The observed features are interpreted with the help of SLIMCAT and Bremen Chemical Transport Model (CTMB) simulations. The SLIMCAT simulations are in good agreement with the measured O-3 and N2O values, where the differences are within 1 ppmv for O-3 and 15 ppbv for N2O. However, the CTMB simulations underestimate the tropical middle stratospheric O-3 (1-1.5 ppmv) and the tropical lower stratospheric N2O (15-30 ppbv) measurements. A detailed analysis with various measurements and model simulations suggests that the biases in the CTMB simulations are related to its parameterised chemistry schemes.",2010,10.1007/s10874-011-9191-4,no
Large Volume Metrology Process Model: Measurability Analysis with Integration of Metrology Classification Model and Feature-Based Selection Model,"Aircraft manufacturing industries are looking for solutions in order to increase their productivity. One of the solutions is to apply the metrology systems during the production and assembly processes. Metrology Process Model (MPM) (Maropoulos et al, 2007) has been introduced which emphasises metrology applications with assembly planning, manufacturing processes and product designing. Measurability analysis is part of the MPM and the aim of this analysis is to check the feasibility for measuring the designed large scale components. Measurability Analysis has been integrated in order to provide an efficient matching system. Metrology database is structured by developing the Metrology Classification Model. Furthermore, the feature-based selection model is also explained. By combining two classification models, a novel approach and selection processes for integrated measurability analysis system (MAS) are introduced and such integrated MAS could provide much more meaningful matching results for the operators.",2010,,no
Comparative Evaluation of the Efficiency of a Series of Commercial Antioxidants Studied by Kinetic Modeling in a Liquid Phase and During the Melt Processing of Different Polyethylenes,"An antioxidant response in condensed polymeric environments is often ambiguous and may vary strongly depending on the nature of the polymer and the conditions of polymer storage, processing, and use. The impact of polymeric environments during melt processing on the intrinsic efficiency of a set of commercial antioxidants was studied. The antioxidative activity of primary antioxidants Lowinox CPL, Lowinox 22IB46, Naugard 445, hydroxylamine Genox EP, and secondary phosphite Weston TNPP were determined by using two versions of the model reaction of cumene initiated (2,2'-azobisisobutyronitrile, AIBN, and cumyl hydroperoxide, ROOH) oxidation. The melt stabilizing efficiency of the antioxidants was also studied during multipass extrusion testing in HDPE (Phillipstype), metallocene LLDPE, and (Ziegler-Natta) LLDPE. The kinetic measurements showed that each of the three functional hydroxyl groups of Lowinox CPL is consumed in the model reaction (version 1) with the same high inhibition rate constant (k(7)) whereas the two functional groups of Lowinox 22IB46 have different activity stipulated by hydrogen bonding between the hydroxyls. All the primary stabilizers involved afforded transformation products with additional antioxidative activity. For phenolic Lowinox CPL and amine Naugard 445, these products exhibited lower inhibition rate constants than that of the main functionality, but for Lowinox 22IB46, the discrepancy was not observed. Genox EP revealed three inhibition centers with different rate constants which, however, have low values of the inhibition coefficients (f). This effect is presumably due to the versatility of the inhibition pathways for the antioxidant and its intermediates, including the path of active interception of cumylalkyl (R(center dot)) radicals. The secondary stabilizer Weston TNPP, tested by means of the second version of the model system, along with the expected decomposition of hydroperoxide appeared to be an effective radical scavenger. Kinetic parameters of the antioxidizing activity of the stabilizers - inhibition rate constants, coefficients of the oxidation chain termination, and total antioxidative activity {A = Sigma[k(7)((i)) (fn[InH])((i))]} - were determined for each functional group and for the whole antioxidant molecule. The phenolic stabilizers manifested powerful antioxidative activity. Their strongest functional groups have very high inhibition rate constant values: (log k(7)) = 5.4 +/- 0.15 (Lowinox 22IB46) and 5.2 +/- 0.1 M(-1)s(-1) (Lowinox CPL). In terms of the total inhibiting activity in the liquid system the antioxidants can be ordered as: Lowinox CPL > Lowinox 22IB46 > Naugard 445 > Genox EP > Weston TNPP. The effect of stabilizers during multipass extrusion experiments was assessed via melt flow rate and yellowness index measurements conducted as a function of the number of passes. Phenolic antioxidants and Genox EP significantly improved the melt stability of the polyethylenes in terms of melt viscosity retention and in partial compliance with the data from kinetic modeling measurements. According to the melt stabilizing efficiency data, the antioxidants can be arranged as: Lowinox 22IB46 > Lowinox CPL > Genox EP > Naugard 445 > Weston TNPP. The Lowinox 22IB46 with relatively lower molecular weight exhibited the best results among the primary stabilizers because of the unrestricted molecular dynamics in the viscous-flow state of the polymer. Yellowness index measurements made after multiple extruder passes indicated that Weston TNPP ffectively decreased the color development caused by the phenolic antioxidants. Genox EP displayed high efficiency as an antioxidant and melt-processing stabilizer and additionally provided good color protection. Generally, we received a good correlation between the activity of the antioxidants in the model system and their melt stabilization performance in HDPE, metallocene LLDPE, and LLDPE. The model reaction of cumene-initiated oxidation has demonstrated excellent applicability as an effective tool for preliminary quantitative assessment of antioxidant radical-scavenging efficiency. J. VINYL ADDIT. TECHNOL., 16:1-14, 2010. (C) 2009 Society of Plastics Engineers",2010,10.1002/vnl.20208,no
An engineering and economic evaluation of quick germ-quick fiber process for dry-grind ethanol facilities: Model description and documentation,"An engineering economic model, which is mass balanced and compositionally driven, was developed to compare the conventional corn dry-grind process and the pre-fractionation process called ""Quick germ/Quick fiber"". For the purposes of this model, the distillers dried grains with solubles price was correlated to its protein and fiber composition and the long-term average relationship with the corn price. This paper has been prepared to describe the development of the model and provide documentation for its use. This model can be used to provide decision support for ethanol producers considering the new emerging technologies that may provide sustainability to the business of ethanol production from corn. (C) 2010 Elsevier Ltd. All rights reserved.",2010,10.1016/j.biortech.2010.01.139,no
A decision model for evaluating third-party logistics providers using fuzzy analytic hierarchy process,"As a consequence of an increasing trend toward the outsourcing of logistics activities, shippers have been faced with the inevitability of selecting an appropriate third-party logistics (3PL) provider. The selection process for the identification of a 3PL provider that best fits user requirements involves multiple criteria and alternatives and may be one of the most complex decisions facing logistics users. In this regard, this study proposes an evaluation framework and methodology for selecting a suitable 3PL provider and illustrates the process of evaluation and selection through a case study. It is expected that the results of this study will provide a practical reference for logistics managers who want to engage the best 3PL provider. Future research using different datasets is warranted to verify the generalizability of the findings.",2010,,no
Mathematical models for job-shop scheduling problems with routing and process plan flexibility,"As a result of rapid developments in production technologies in recent years, flexible job-shop scheduling problems have become increasingly significant. This paper deals with two NP-hard optimization problems: flexible job-shop scheduling problems (FJSPs) that encompass routing and sequencing sub-problems, and the FJSPs with process plan flexibility (FJSP-PPFs) that additionally include the process plan selection sub-problem. The study is carried out in two steps. In the first step, a mixed-integer linear programming model (MILP-1) is developed for FJSPs and compared to an alternative model in the literature (Model F) in terms of computational efficiency. In the second step, one other mixed-integer linear programming model, a modification of MILP-1, for the FJSP-PPFs is presented along with its computational results on hypothetically generated test problems. (C) 2009 Elsevier Inc. All rights reserved.",2010,10.1016/j.apm.2009.09.002,no
Improving the energy efficiency of industrial processes using mathematical modelling,"As energy costs are generally on the increase, industrial processes have to be improved in terms of more efficiency to compensate for the increasing energy costs. Besides, it is generally aimed and politically forced by industrialized countries to reduce their energy demand. The aim of this paper is to present a method to analyze energy flows in industrial processes and to show up the effect of various process optimizations which finally lead to an increase of energy efficiency. The basis for this method is the use and design of dynamic process models that help to reflect complex and dynamic industrial processes in cases where steady-state calculations are not sufficient to exhibit the impact of process variations on the overall energy demand. In order to improve process efficiency, primarily variations of insulation devices and dimensions are investigated. Secondly, improvements in process control are examined with the intention to show up the potential for further heat recovery. The final aim of the presented method is to prove the best optimization procedure in terms of ecological and economical aspects and to visualize the potential of energy reduction and thereby of CO(2) emissions.",2010,10.3303/CET1021017,no
Full-Chip Layout Optimization for Process Margin Enhancement Using Model-Based Hotspot Fixing System,"As the design rule of integrated circuits is shrinking rapidly, it is necessary to use low-k(1) lithography technologies. With low-k(1) lithography, even if aggressive optical proximity correction is adopted, many sites become marginless spots, known as ""hotspots"". For this problem, hotspot fixer (HSF) in design-for-manufacturability flow has been studied. In our previous work, we indicated the feasibility of layout modification using a simple line/space sizing rule for metal layers in 65-nm-node logic devices. However, in view of the continuous design-rule shrinkage and design complication, a more flexible modification method has become necessary to fix various types of hotspots. In this work, we have developed a brute-force model-based HSF. To further reduce the processing time, the hybrid flow of rule- and model-based HSFs is studied. The feasibility of such hybrid flow is studied by applying it to the full-chip layout modification of a logic test chip. (C) 2010 The Japan Society of Applied Physics",2010,10.1143/JJAP.49.06GB02,no
An Evaluation Model of Network Security Risk in Commercial Bank-Based on Analytic Network Process,"At present, the correlation among indicators involving in computer network security evaluation will not be analyzing thoroughly. And there is difficult to understand the security status of computer networks in the whole. In this paper, therefore, an evaluation model based on analytic network process will be designed. The lack of the traditional evaluation model of network security, which the neglects the correlation and dependence among indicators, will be improved. An illustrative example is given to demonstrate that the feasibility and validity of the proposed method is suit to evaluating network security in Commercial Bank.",2010,,no
Process of healing of mucosal defects in the esophagus after endoscopic mucosal resection: histological evaluation in a dog model,"Background and study aims: Resection of a large amount of the esophageal mucosa often causes esophageal ulcer and postoperative stricture. The aim of this study was to evaluate the process of healing of defects in the esophageal mucosa after endoscopic mucosal resection (EMR). Materials and methods: Cap-assisted EMR was performed in the thoracic esophagus of six beagle dogs to prepare mucosal defects with a diameter ranging from 15 to 18 mm. The process of mucosal healing was assessed histologically immediately after EMR, and on postoperative day (POD) 2, 4, 7, 14, and 28. Results: Immediately after EMR, a thin layer of the submucosa remained in the mucosal defect, and no damage to the muscularis propria was evident. Ulcer formation and inflammatory cell invasion were observed in the remaining submucosa on POD 2 and 4. Angiogenesis and collagen fiber hyperplasia were observed after POD 7. Complete epithelialization of the ulcer was observed on POD 28. In the muscularis propria, further destruction and atrophy were evident after POD 7. Fibrosis of the muscularis propria was observed on POD 28. Conclusion: In the esophageal wall after epithelial loss resulting from EMR, atrophy and fibrosis of the muscularis propria remain even after epithelialization.",2010,10.1055/s-0030-1255741,no
Extracellular ascorbic acid fluctuation during the protective process of ischemic preconditioning in rabbit renal ischemia-reperfusion model measured,"Background Ascorbic acid has important antioxidant properties, and may play a role in the protective effects of ischemic preconditioning on later ischemia-reperfusion. Herein, we examined the role of endogenous extracellular ascorbic acid in ischemic preconditioning in the kidney. Methods We developed a solitary rabbit kidney model where animals received ischemia-reperfusion only (ischemia-reperfusion group, n=15) or ischemic preconditioning followed by ischemia-reperfusion (ischemic preconditioning group, n=15). Ischemia-reperfusion was induced by occluding and loosening of the renal pedicle. The process of ischemic preconditioning included 15-minute brief ischemia and 10-minute reperfusion. In vivo microdialysis coupled with online electrochemical detection was used to determine levels of endogenous extracellular ascorbic acid in both groups. The extent of tissue damage was determined in kidney sections stained with hematoxylin and eosin. Serum creatinine and urea nitrogen were also detected to assess renal function. Results During ischemia-reperfusion, the extracellular ascorbic acid concentration during ischemia increased rapidly to the peak level ((130.01+/-9.98)%), and then decreased slowly to near basal levels. Similar changes were observed during reperfusion (peak level, (126.78+/-18.24)%). In the ischemic preconditioning group there was a similar pattern of extracellular ascorbic acid concentration during ischemic preconditioning. However, the ascorbic acid level was significantly lower during the ischemia and early reperfusion stage compared to the ischemia-reperfusion group. Additionally, the extent of glomerular ischemic collapse, tubular dilation, tubular denudation, and loss of brush border were markedly attenuated in the ischemic preconditioning group. Levels of serum creatinine and urea nitrogen were also decreased significantly in the ischemic preconditioning group. Conclusions Ischemic preconditioning may protect renal tissue against ischemia-reperfusion injury via use of extracellular ascorbic acid. In vivo microdialysis coupled with online electrochemical detection is effective for continuous monitoring extracellular ascorbic acid in the renal cortex. Chin Med J 2010;123(11):1441-1446",2010,10.3760/cma.j.issn.0366-6999.2010.11.017,no
Does the process map influence the outcome of quality improvement work? A comparison of a sequential flow diagram and a hierarchical task analysis diagram,"Background: Many quality and safety improvement methods in healthcare rely on a complete and accurate map of the process. Process mapping in healthcare is often achieved using a sequential flow diagram, but there is little guidance available in the literature about the most effective type of process map to use. Moreover there is evidence that the organisation of information in an external representation affects reasoning and decision making. This exploratory study examined whether the type of process map - sequential or hierarchical - affects healthcare practitioners' judgments. Methods: A sequential and a hierarchical process map of a community-based anti coagulation clinic were produced based on data obtained from interviews, talk-throughs, attendance at a training session and examination of protocols and policies. Clinic practitioners were asked to specify the parts of the process that they judged to contain quality and safety concerns. The process maps were then shown to them in counter-balanced order and they were asked to circle on the diagrams the parts of the process where they had the greatest quality and safety concerns. A structured interview was then conducted, in which they were asked about various aspects of the diagrams. Results: Quality and safety concerns cited by practitioners differed depending on whether they were or were not looking at a process map, and whether they were looking at a sequential diagram or a hierarchical diagram. More concerns were identified using the hierarchical diagram compared with the sequential diagram and more concerns were identified in relation to clinical work than administrative work. Participants' preference for the sequential or hierarchical diagram depended on the context in which they would be using it. The difficulties of determining the boundaries for the analysis and the granularity required were highlighted. Conclusions: The results indicated that the layout of a process map does influence perceptions of quality and safety problems in a process. In quality improvement work it is important to carefully consider the type of process map to be used and to consider using more than one map to ensure that different aspects of the process are captured.",2010,10.1186/1472-6963-10-7,no
What work has to be done to implement collaborative care for depression? Process evaluation of a trial utilizing the Normalization Process Model,"Background: There is a considerable evidence base for 'collaborative care' as a method to improve quality of care for depression, but an acknowledged gap between efficacy and implementation. This study utilises the Normalisation Process Model (NPM) to inform the process of implementation of collaborative care in both a future full-scale trial, and the wider health economy. Methods: Application of the NPM to qualitative data collected in both focus groups and one-to-one interviews before and after an exploratory randomised controlled trial of a collaborative model of care for depression. Results: Findings are presented as they relate to the four factors of the NPM (interactional workability, relational integration, skill-set workability, and contextual integration) and a number of necessary tasks are identified. Using the model, it was possible to observe that predictions about necessary work to implement collaborative care that could be made from analysis of the pre-trial data relating to the four different factors of the NPM were indeed borne out in the post-trial data. However, additional insights were gained from the post-trial interview participants who, unlike those interviewed before the trial, had direct experience of a novel intervention. The professional freedom enjoyed by more senior mental health workers may work both for and against normalisation of collaborative care as those who wish to adopt new ways of working have the freedom to change their practice but are not obliged to do so. Conclusions: The NPM provides a useful structure for both guiding and analysing the process by which an intervention is optimized for testing in a larger scale trial or for subsequent full-scale implementation.",2010,10.1186/1748-5908-5-15,no
Evaluation of usability of Cassie and hybrid Cassie-Mayr models to simulate processes in AC arc circuits,"Based on experimental studies of welding transporters systems, supplying the electric arc, were determinate the courses of voltage and current. Using of Amsnick and Zuckler method were calculated the parameters of Cassie and hybrid Cassie-Mayr models. The macro models of corresponding arcs were created and then used for a simulation runs in a MATLAB-Simulink program. A large discrepancy of dynamic characteristics of real arc in comparison with the Cassie model was found. A satisfactory compliance was obtained for the characteristics of a hybrid Cassie-Mayr model. The functions of the parameters depending on the discharge current were calculated.",2010,,no
Quantifying wetland methane emissions with process-based models of different complexities,"Bubbling is an important pathway of methane emissions from wetland ecosystems. However the concentration-based threshold function approach in current biogeochemistry models of methane is not sufficient to represent the complex ebullition process. Here we revise an extant process-based biogeochemistry model, the Terrestrial Ecosystem Model into a multi-substance model (CH(4), O(2), CO(2) and N(2)) to simulate methane production, oxidation, and transport (particularly ebullition) with different model complexities. When ebullition is modeled with a concentration-based threshold function and if the inhibition effect of oxygen on methane production and the competition for oxygen between methanotrophy and heterotrophic respiration are retained, the model becomes a two-substance system. Ignoring the role of oxygen, while still modeling ebullition with a concentration-based threshold function, reduces the model to a one-substance system. These models were tested through a group of sensitivity analyses using data from two temperate peatland sites in Michigan. We demonstrate that only the four-substance model with a pressure-based ebullition algorithm is able to capture the episodic emissions induced by a sudden decrease in atmospheric pressure or by a sudden drop in water table. All models captured the retardation effect on methane efflux from an increase in surface standing water which results from the inhibition of diffusion and the increase in rhizospheric oxidation. We conclude that to more accurately account for the effects of atmospheric pressure dynamics and standing water on methane effluxes, the multi-substance model with a pressure-based ebullition algorithm should be used in the future to quantify global wetland CH(4) emissions. Further, to more accurately simulate the pore water gas concentrations and different pathways of methane transport, an exponential root distribution function should be used and the phase-related parameters should be treated as temperature dependent.",2010,10.5194/bg-7-3817-2010,no
The ontological deficiencies of process modeling in practice,"Business process modeling is widely regarded as one of the most popular forms of conceptual modeling. However, little is known about the capabilities and deficiencies of process modeling grammars and how existing deficiencies impact actual process modeling practice. This paper is a first contribution towards a theory-driven, exploratory empirical investigation of the ontological deficiencies of process modeling with the industry standard Business Process Modeling Notation (BPMN). We perform an analysis of BPMN using a theory of ontological expressiveness. Through a series of semi-structured interviews with BPMN adopters we explore empirically the actual use of this grammar. Nine ontological deficiencies related to the practice of modeling with BPMN are identified, for example, the capture of business rules and the specification of process decompositions. We also uncover five contextual factors that impact on the use of process modeling grammars, such as tool support and modeling conventions. We discuss implications for research and practice, highlighting the need for consideration of representational issues and contextual factors in decisions relating to BPMN adoption in organizations. European Journal of Information Systems (2010) 19, 501-525. doi:10.1057/ejis.2010.38; published online 15 June 2010",2010,10.1057/ejis.2010.38,no
Using CMP Model for 45nm/32nm BEOL Process and Design Evaluations,"Chemical Mechanical Polishing (CMP) is an essential process in semiconductor manufacturing. It is known to suffer from pattern dependencies such as dishing and erosion, which can lead to potential yield and performance problems, including copper pooling and excessive copper loss. These problems become very significant as lateral and vertical dimensions continue to shrink. They can therefore no longer be neglected in advanced node designs for optimal yield and performance. In this paper, we demonstrate a physics based CMP model in predicting metal line heights in macros with pattern factors from 10% to 90%. Model based checking as opposed to rule based checking can identify more accurately the weak points in a design and enable designers to provide improved layout for areas that will cause under polish or excessive erosion for a given CMP process. Thus CMP modeling can provide information on interlevel effects such as copper puddling from underlying topography that cannot be captured in recommended design rules.",2010,10.1149/1.3489040,no
Towards supporting expert evaluation of clustering results using a data mining process model,"Clustering is a popular non-directed learning data mining technique for partitioning a dataset into a set of clusters (i.e. a segmentation). Although there are many clustering algorithms, none is superior on all datasets, and so it is never clear which algorithm and which parameter settings are the most appropriate for a given dataset. This suggests that an appropriate approach to clustering should involve the application of multiple clustering algorithms with different parameter settings and a non-taxing approach for comparing the various segmentations that would be generated by these algorithms. In this paper we are concerned with the situation where a domain expert has to evaluate several segmentations in order to determine the most appropriate segmentation (set of clusters) based on his/her specified objective(s). We illustrate how a data mining process model could be applied to address this problem. (C) 2009 Elsevier Inc. All rights reserved.",2010,10.1016/j.ins.2009.09.019,no
Application of Analytic Hierarchy Process-Grey Target Theory Systematic Model in Comprehensive Evaluation of Water Environmental Quality,"Comprehensive evaluation of the water environment for effective water quality management is complicated by a considerable number of factors and uncertainties. It is difficult to combine micro-evaluation with the macro-evaluation process. To effectively eliminate the subjective errors of the traditional analytic hierarchy process (AHP), a new modeling approach-the analytic hierarchy process and grey target theory (AHP-GTT) systematic model-is presented in this study to evaluate water quality in a certain watershed. A case study of applying the AHP-GTT systematic model to the evaluation and analysis of the water environment was conducted in the Yibin section of the Yangtze River, China. The micro-evaluation is based on defining the weights of indices of the water quality (IWQ) of each water cross-section, while the macro-evaluation is based on calculating the comprehensive indices of water environmental quality and analyzing the tendency of the water environment of each cross-section. The results indicated that the Baixi and Shuidongmen sections are seriously polluted areas, with the tendencies of becoming worse. Also, the key IWQs of these two cross-sections are 5-day biochemical oxygen demand and chemical oxygen demand of permanganate, respectively. Water Environ. Res., 82, 633 (2010).",2010,10.2175/106143009X12529484816231,no
Computational modelling of spoken-word recognition processes Design choices and evaluation,"Computational modelling has proven to be a valuable approach in developing theories of spoken-word processing. In this paper, we focus on a particular class of theories in which it is assumed that the spoken-word recognition process consists of two consecutive stages, with an 'abstract' discrete symbolic representation at the interface between the stages. In evaluating computational models, it is important to bring in independent arguments for the cognitive plausibility of the algorithms that are selected to compute the processes in a theory. This paper discusses the relation between behavioural studies, theories, and computational models of spoken-word recognition. We explain how computational models can be assessed in terms of the goodness of fit with the behavioural data and the cognitive plausibility of the algorithms. An in-depth analysis of several models provides insights into how computational modelling has led to improved theories and to a better understanding of the human spoken-word recognition process.",2010,10.1075/pc.18.1.06sch,no
A parallel process growth mixture model of conduct problems and substance use with risky sexual behavior,Conduct problems substance use and risky sexual behavior have been shown to coexist among adolescents which may lead to significant health problems The current study was designed to examine relations among these problem behaviors in a community sample of children at high risk for conduct disorder A latent growth model of childhood conduct problems showed a decreasing trend from grades K to 5 During adolescence four concurrent conduct problem and substance use trajectory classes were identified (high conduct problems and high substance use increasing conduct problems and increasing substance use minimal conduct problems and increasing substance use and minimal conduct problems and minimal substance use) using a parallel process growth mixture model Across all substances (tobacco binge drinking and manjuana use) higher levels of childhood conduct problems during kindergarten predicted a greater probability of classification into more problematic adolescent trajectory classes relative to less problematic classes For tobacco and binge drinking models Increases in childhood conduct problems over time also predicted a greater probability of classification into more problematic classes For all models individuals classified into more problematic classes showed higher proportions of early sexual intercourse infrequent condom use receiving money for sexual services and ever contracting an STD Specifically tobacco use and binge drinking during early adolescence predicted higher levels of sexual risk taking into late adolescence Results highlight the importance of studying the conjoint relations among conduct problems substance use and risky sexual behavior in a unified model (C) 2010 Elsevier Ireland Ltd All rights reserved,2010,10.1016/j.drugalcdep.2010.04.013,no
MATHEMATICAL MODELING OF THE PROCESS OF MEASUREMENT CONTROL OF THE DEGRADATION OF CONSTRUCTION MATERIALS,Construction of a mathematical model of the process of measurement control of the degradation of a construction materials used for an elastic element is analyzed using as an example a linear oscillator with variable coefficient of internal viscous friction. Control of the parameters is realized in real time by means of measurements of the deviation of the period of the oscillator's free oscillations.,2010,10.1007/s11018-010-9607-6,no
Modelling and experimental validation of emulsification processes in continuous rotor-stator units,"Despite the wide range of industrial applications of structured emulsions, current approaches toward process design and scale-up are commonly based on trial-and-error experimentation. As this design approach is foreseen to deliver most likely suboptimal process solutions, we propose in this contribution a model-based approach as the way forward to designing manufacturing processes of structured emulsions. In this context, process modelling and simulation techniques are applied to predict production rates and equipment sizing. Moreover, sensitivity analysis of the process model provides insight about potential bottlenecks in the process. (C) 2010 Elsevier Ltd. All rights reserved.",2010,10.1016/j.compchemeng.2010.01.017,no
The Disclosure Processes Model: Understanding Disclosure Decision Making and Postdisclosure Outcomes Among People Living With a Concealable Stigmatized Identity,"Disclosure is a critical aspect of the experience of people who live with concealable stigmatized identities. This article presents the disclosure processes model (DPM)-a framework with which to examine when and why interpersonal disclosure may be beneficial. The DPM suggests that antecedent goals representing approach and avoidance motivational systems moderate the effect of disclosure on numerous individual, dyadic, and social contextual outcomes and that these effects are mediated by three distinct processes: (a) alleviation of inhibition, (b) social support, and (c) changes in social information. Ultimately, the DPM provides a framework that advances disclosure theory and identifies strategies that can assist disclosers in maximizing the likelihood that disclosure will benefit well-being.",2010,10.1037/a0018193,no
Modeling the Effects of Information Quality on Hospital Process Performance,"Due to rising costs and risks, it is necessary to optimize performance of the hospital processes. High quality information has a significant effect on improving process performance and patient satisfaction, as well as resolving patient disputes. Based on the analysis of the hospital process, information quality (IQ) is considered as an important contributory factor in improving patient throughput. In this paper, a process oriented theoretical framework and main content are presented. Through the establishment of quantitative information, quality indicators such as Timeliness, Completeness and Reputation, and the effect on process performance (registered queue length, waiting time, utilization of hospital resources), together with the cost of the hospital process, are analyzed from the theoretical aspect and then verified by simulation technology. Finally, the results of our studies provide evidence that simulation can provide prediction support to drive information quality toward the performance management.",2010,,no
A theorical model design for ERP software selection process under the constraints of cost and quality: A fuzzy approach,"Enterprise Resource Planning (ERP) software selection is one of the most important decision making issues covering both qualitative and quantitative factors for organizations. Multiple criteria decision making (MCDM) has been found to be a useful approach to analyze these conflicting factors. Qualitative criteria are often accompanied by ambiguities and vagueness. This makes fuzzy logic a more natural approach to this kind of problems. This study presents a beneficial structure to the managers for use in ERP software vendor selection process. In order to evaluate ERP vendors methodologically, a hierarchical framework is also proposed. As a MCDM tool, we used analytic hierarchy process (AHP) and its fuzzy extension to obtain more decisive judgments by prioritizing criteria and assigning weights to the alternatives. The objective of this paper is to select the most appropriate alternative that meets the customer's requirements with respect to cost and quality constraints. In the end of this study, a real-world case study from Turkey is also presented to illustrate efficiency of the methodology and its applicability in practice.",2010,10.3233/IFS-2010-0457,no
Void Formation Model and Measuring Method of Void Formation Condition During Hot Pressing Process,"For resin matrix composites, voids are common defects that can seriously deteriorate the properties of the composite parts. Thus, the elimination of voids is a crucial element in controlling the manufacturing process of composite parts. This article focuses on void formation originating from hygroscopic water for resin matrix composite laminates prepared with hot pressing process. The Kardos void formation model was developed to analyze the critical resin pressure for the initiation of voids, and the influencing factors were investigated experimentally to validate the modified model. It is found that resin pressure and gel temperature are the two key parameters to control void defects and that entrapped air in prepreg stacks must be considered in the void formation model. Furthermore, a simple method was established to measure the relationship between porosity and the processing parameters, and the void formation conditions of the resin and the prepreg stack were also studied. The theoretically predicted void formation conditions and the experimental results were compatible for the studied cases. These results are valuable for eliminating void defects, optimizing processing parameters, and enhancing the performance of composite parts. POLYM. COMPOS., 31:1562-1571,2010. (C) 2009 Society of Plastics Engineers",2010,10.1002/pc.20944,no
Decoupling MSW settlement into mechanical and biochemical processes - Modelling and validation on large-scale setups,"Forecasting settlements of non-hazardous waste is essential to ensure the integrity and durability of landfill covers over time. Over a short time span, the survey of settlements may also contribute to the investigation of the biodegradation processes. This paper addresses secondary settlements of Municipal Solid Waste (MSW), a heterogeneous and time-evolving material. An analysis of available experimental data from different pilots and the literature was conducted to quantify the influence of biodegradation on MSW secondary settlements. After making assumptions about the various features of the waste and their constitutive relationships, a one-dimensional biomechanical model to predict the secondary settlement has been developed. The determination of the total secondary settlement was obtained by the addition of two separate parts, the mechanical settlement, due to creep, and the biochemical settlement, due to the degradation of the organic matter. The latter has been evaluated based on the observed biogas production. Using the data from different recent large-scale experiments that provide a monitoring of biogas production, a method for predicting the biochemically-induced settlements is proposed and validated on these tests. The relative contributions of mechanical and biochemical settlements are also calculated and discussed as a function of waste pre-treatment and operation conditions (biological pre-treatment, shredding, leachate injection). Finally, settlement may be considered as a relevant indicator for the state of biodegradation. (C) 2010 Elsevier Ltd. All rights reserved.",2010,10.1016/j.wasman.2010.03.004,no
An Approach for Constructing Evaluation Model of Suitability Assessment of Agile Methods using Analytic Hierarchy Process,"G. Mikulenas, R. Butleris. An Approach for Constructing Evaluation Model of Suitability Assessment of Agile Methods using Analytic Hierarchy Process // Electronics and Electrical Engineering. - Kaunas: Technologija, 2010. - No. 10(106). - P. 99-104. Agile methods are rapidly growing in popularity software development methods that are not suitable everywhere due to their specifics. The results of previous researches including agility criteria and weights construction are used in this research. The approach for the construction of evaluation models of the suitability assessment of agile methods at IT companies is presented. The terms and concepts used for the approach are defined. The process of the construction the evaluation model is presented. A widely used Analytic Hierarchy Process (AHP) method is used for the construction of the relative weights of agility criteria. The case study of the creation concrete evaluation model is presented. The example of the usage of the constructed evaluation model at some IT company is presented. The accuracy check of the correctness of evaluations is performed. The conclusions of the research are presented, stating that proposed approach increases the effectiveness of the use of agile methods and facilitates the process of suitability evaluation. III. 1, bibl. 10, tabl. 16 (in English; abstracts in English and Lithuanian).",2010,,no
A family of experiments to validate measures for UML activity diagrams of ETL processes in data warehouses,"In data warehousing, Extract, Transform, and Load (ETL) processes are in charge of extracting the data from the data sources that will be contained in the data warehouse. Their design and maintenance is thus a cornerstone in any data warehouse development project. Due to their relevance, the quality of these processes should be formally assessed early in the development in order to avoid populating the data warehouse with incorrect data. To this end, this paper presents a set of measures with which to evaluate the structural complexity of ETL process models at the conceptual level. This study is, moreover, accompanied by the application of formal frameworks and a family of experiments whose aim is to theoretical and empirically validate the proposed measures, respectively. Our experiments show that the use of these measures can aid designers to predict the effort associated with the maintenance tasks of ETL processes and to make ETL process models more usable. Our work is based on Unified Modeling Language (UML) activity diagrams for modeling ETL processes, and on the Framework for the Modeling and Evaluation of Software Processes (FMESP) framework for the definition and validation of the measures. (C) 2010 Elsevier B.V. All rights reserved.",2010,10.1016/j.infsof.2010.06.003,no
"A Study of the Quality Review Process for an Australian Overseas Project under the AQTF 2007 Model-a Case Study on Quality Review of ""SDJU-BHI"" Australian Overseas projects","In line with the requirements of the Australian Qualifications Framework (AQF), and the Standards in the 2007 edition of the Australian Quality Training Framework (AQTF) Box Hill Institute supports partners to continuously improve the quality of academic processes through cyclical review. The Quality Review of the SDJU-BHI Australian Overseas Project was conducted under AQTF 2007. Its purpose is the implementation of continuous improvement and maintenance of standards. As part of their cyclical Review program, the Quality Review of the BHI-SDJU Australian overseas project not only examined the results of the cooperative education project, but also identified the problems of integrating the requirements of the Australia overseas project review with local requirements.",2010,,no
"A process-based model of N2O emission from a rice-winter wheat rotation agro-ecosystem: Structure, validation and sensitivity","In order to numerically simulate daily nitrous oxide (N2O) emission from a rice-winter wheat rotation cropping system, a process-based site model was developed (referred to as IAP-N-GAS) to track the movement and transformation of several forms of nitrogen in the agro-ecosystem, which is affected by climate, soil, crop growth and management practices. The simulation of daily N2O fluxes, along with key daily environmental variables, was validated with three-year observations conducted in East China. The validation demonstrated that the model simulated well daily solar radiation, soil temperature and moisture, and also captured the dynamics and magnitude of accumulated rice aboveground biomass and mineral nitrogen in the soil. The simulated daily N2O emissions over all three years investigated were generally in good agreement with field observations. Particularly well simulated were the peak N2O emissions induced by fertilizations, rainfall events or mid-season drainages. The model simulation also represented closely the inter-annual variation in N2O emission. These validations imply that the model has the capability to capture the general characteristics of N2O emission from a typical rice-wheat rotation agro-ecosystem. Sensitivity analyses revealed that the simulated N2O emission is most sensitive to the fertilizer application rate and the soil organic matter content, but it is much less sensitive to variations in soil pH and texture, temperature, precipitation and crop residue incorporation rate under local conditions.",2010,10.1007/s00376-009-8191-7,no
Air Quality: Meteorology Interaction Processes in the ICLAMS Modeling System,"In order to study processes and feedbacks between air pollution and climate a new Integrated Community Limited Area Modeling System - ICLAMS has been developed. ICLAMS is an enhanced version of the Regional Atmospheric Modeling SystemRAMS.6.0. It includes submodels for the dust and sea salt cycles, gas and aqueous phase chemistry and gas to particle conversion. All these processes are directly coupled with meteorology. The system has been developed to study air pollution transport and transformation processes in the Greater Euro-Mediterranean Region and East Atlantic. This area is well known for its regional characteristics where the mixture of different age of anthropogenic air pollutants with Saharan dust and sea salt may lead to the formation of new particles with different characteristics. In this presentation, we demonstrate the transport and transformation processes at various spatiotemporal scales and discuss implications related to aerosol composition and their impacts on cloud formation and on radiation. The new modeling tool is applied in studies related to air quality and climate in the Mediterranean, North Africa and Atlantic Ocean.",2010,,no
Development and Validation of a Steady-State Mathematical Model for the Physicochemical Processing of Biopolymers by Reactive Extrusion,"In reactive extrusion, the extruder is used as a solvent-free continuous chemical reactor able to process highly viscous materials. The chemical transformation of biopolymers by reactive extrusion appears as a very promising technology. Although punctual applications in this field have already been achieved on a laboratory or pilot scale, the amount of work to carry out is still considerable. A wide range of reactions and raw materials may be explored, and the reactions achieved on a laboratory scale have to be optimized and transposed to an industrial scale. Process modelling and simulation constitute useful tools for process understanding, development, optimization and scale-up. Although reactive extrusion modelling has interested many authors, it still remains a challenge because of the complex geometry and the strong coupling between operating parameters, flow conditions, material rheological behavior and reaction kinetics. A steady-state mathematical model for a biopolymer oxidation process by reactive extrusion is here proposed. The model is based on a hybrid approach combining chemical engineering methods and simplified continuum mechanics laws. The combination of these two approaches enables to simplify the calculations related to chemical reactions while ensuring a predictive character. The flexible structure of the model enabled its implementation within a global process simulator. A method to minimize the amount of experimental data required for model parameter adjustment is also presented. The model was validated by experiments conducted on a semi-pilot corotating twin-screw extruder. Even if it may be refined, the model proposed already constitutes a useful tool for later research work dealing with the development, modelling and simulation of chemical reactions in corotating twin-screw extruders.",2010,,no
Evaluation Model for Military Communication Network Effectiveness Based on Analytic Network Process and Cloud Model,"In the evaluation process of military communication network effectiveness (MCNE), there may be relations and dependences among the evaluation criteria; and fuzziness and uncertainty in expert's subjective judgment may exist. This research proposed an evaluation model of MCNE based on Analytic Network Process (ANP) and Cloud model to solve these problems. It involves the construction of the networked evaluation criteria system, the formation of the Cloud supermatrix that specifies the relationships between criteria within the evaluation process, and the generation of limit Cloud supermatrix that prioritizes the relative Cloud weights for the criteria and alternatives. Using the MCNE evaluation of a Naval Vessels Fleet as an example, we demonstrate that the evluation model proposed is credible and effective.",2010,,no
Term Structure Models Driven by Wiener Processes and Poisson Measures: Existence and Positivity,"In the spirit of [T. Bjork et al., Finance Stoch., 1 (1997), pp. 141-174], we investigate term structure models driven by Wiener processes and Poisson measures with forward curve dependent volatilities. This includes a full existence and uniqueness proof for the corresponding Heath-Jarrow-Morton-type term structure equation. Furthermore, we characterize positivity preserving models by means of the characteristic coefficients. A key role in our investigation is played by the method of the moving frame, which allows us to transform term structure equations to time-dependent SDEs.",2010,10.1137/090758593,no
Business process modeling with continuous validation,"In this article, we present the prototype of a modeling tool that applies graph-based rules for identifying problems in business process models. The advantage of our approach is twofold. First, it is unnecessary to compute the complete state space of the model to find errors. Second, our technique can even be applied to incomplete business process models. Thus, the modeler can be supported by direct feedback during the model construction. This feedback not only reports problems, but also identifies their reasons and gives suggestions for improvement. Copyright (C) 2010 John Wiley & Sons, Ltd.",2010,10.1002/smr.517,no
Model-Based Inference of Cognitive Processes from Unobtrusive Gait Velocity Measurements,"In this paper we describe a preliminary modeling and analysis of a unique data set comprising unobtrusive and continuous measurements of gait velocity in the elder participants' residences. The data have been collected as a part of a longitudinal study aimed at early detection of cognitive decline. We motivate these analyses by first presenting evidence that suggests significant relationship between gait parameters and cognitive functions. We then describe a simple, model-based approach to the analysis of gait velocity using a weighted correlation function estimates. One of the main challenges is due to the fact that the daily estimates of the gait parameters vary with the number of walks. We illustrate the importance of using weighted as opposed to unweighted estimates on a sample of different houses. The correlation functions appear to capture behavioral differences that can be related to the cognitive functioning of the participants.",2010,10.1109/IEMBS.2010.5626276,no
"Efficient OCR Post-Processing Combining Language, Hypothesis and Error Models","In this paper, an OCR post-processing method that combines a language model, OCR hypothesis information and an error model is proposed. The approach can be seen as a flexible and efficient way to perform Stochastic Error-Correcting Language Modeling. We use Weighted Finite-State Transducers (WFSTs) to represent the language model, the complete set of OCR hypotheses interpreted as a sequence of vectors of a posteriori class probabilities, and an error model with symbol substitutions, insertions and deletions. This approach combines the practical advantages of a de-coupled (OCR + post-processor) model with the error-recovery power of a integrated model.",2010,,no
Modelling of a thermomechanically coupled forming process based on functional outputs from a finite element analysis and from experimental measurements,"In this paper, measurements from experiments and results of a finite element analysis (FEA) are combined in order to compute accurate empirical models for the temperature distribution before a thermomechanically coupled forming process. To accomplish this, Design and Analysis of Computer Experiments (DACE) is used to separately compute models for the measurements and the functional output of the FEA. Based on a hierarchical approach, a combined model of the process is computed. In this combined modelling approach, the model for the FEA is corrected by taking into account the systematic deviations from the experimental measurements. The large number of observations based on the functional output hinders the direct computation of the DACE models due to the internal inversion of the correlation matrix. Thus, different techniques for identifying a relevant subset of the observations are proposed. The application of the resulting procedure is presented, and a statistical validation of the empirical models is performed.",2010,10.1007/s10182-010-0149-7,no
"Process analysis and sensitivity study of regional ozone formation over the Pearl River Delta, China, during the PRIDE-PRD2004 campaign using the Community Multiscale Air Quality modeling system","In this study, the Community Multiscale Air Quality (CMAQ) modeling system is used to simulate the ozone (O-3) episodes during the Program of Regional Integrated Experiments of Air Quality over the Pearl River Delta, China, in October 2004 (PRIDE-PRD2004). The simulation suggests that O-3 pollution is a regional phenomenon in the Pearl River Delta (PRD). Elevated O-3 levels often occurred in the southwestern inland PRD, Pearl River estuary (PRE), and southern coastal areas during the 1-month field campaign. Three evolution patterns of simulated surface O-3 are summarized based on different near-ground flow conditions. More than 75% of days featured interactions between weak synoptic forcing and local sea-land circulation. Integrated process rate (IPR) analysis shows that photochemical production is a dominant contributor to O-3 enhancement from 09:00 to 15:00 local standard time in the atmospheric boundary layer over most areas with elevated O-3 occurrence in the mid-afternoon. The simulated ozone production efficiency is 2-8 O-3 molecules per NOx molecule oxidized in areas with high O-3 chemical production. Precursors of O-3 originating from different source regions in the central PRD are mixed during the course of transport to downwind rural areas during nighttime and early morning, where they then contribute to the daytime O-3 photochemical production. The sea-land circulation plays an important role on the regional O-3 formation and distribution over PRD. Sensitivity studies suggest that O-3 formation is volatile-organic-compound-limited in the central inland PRD, PRE, and surrounding coastal areas with less chemical aging (NOx/NOy > 0.6), but is NOx-limited in the rural southwestern PRD with aged air (NOx/NOy < 0.3).",2010,10.5194/acp-10-4423-2010,no
STRATEGIES TO INCREASE THE EFFICIENCY OF THE LEARNING PROCESS IN HIGHLY PRACTICAL SUBJECTS WITH A HIGH NUMBER OF STUDENTS: IN VITRO CULTURE IN BIOTECHNOLOGY DEGREES AS A MODEL CASE,"In vitro culture in plants involves a plethora of biotechnological techniques, which include, among others: i) production of virus-free stock plants, ii) micropropagation, iii) development of haploid plants, iv) embryo rescue from interspecific crosses, v) selection of mutants, and vi) genetic transformation. For a suitable learning process of these abilities, students are required to acquire both theoretical and, particularly, practical knowledge. In addition, student work is one of the key points in the adaptation of education activities to the European Space for Higher Education. Thus, academic staff should innovate in order to facilitate and evaluate self-learning processes. In this respect, practical sessions must be planned in order to allow students initiating a process that has to be monitored by themselves during several weeks until the end of experiment. In addition, practical lessons must encompass as many theoretical items as possible. However, carrying out an accurate protocol of practice lessons in this subject is highly difficult because of the wide theoretical syllabus. Having a high number of students in the courses is an additional difficulty. This requires an adequate infrastructure, educational tools, and an accurate planning of in vivo growing material. In this work two different approaches for practical sessions have been compared in the 2007/08 and 2008/09 academic years in the subject ""In vitro culture and plant genetic transformation"" of the Biotechnology degree at the Universidad Politecnica de Valencia (UPV, Spain). In the academic year 2007/08 a total of 72 students were divided into three large groups of practical sessions (24 students each), they were provided with an outline of the corresponding session at the beginning of every session including a questionnaire which they were asked to accomplish and to give to lecturers for revision, and lab sessions were 2h/week. On the other hand, in academic year 2008/09 a total of 90 students were divided into seven smaller groups (12 students each), which were provided with a book containing all the practical sessions, and practices were divided into 1h of lab session and 1 h of homework questions. Advantages and disadvantages were found in both approaches, and they were evaluated not only from the point of view of lecturers, but also by taking into account the students' opinions in anonymous polls carried out at the end of the semester. The main conclusion was that the learning process was more efficient in approach used in 2008/09, although it was more time-consuming for lecturers. Furthermore, students preferred this approach as it provides them a better understanding of the applications of in vitro culture. Students also appreciate very much the integration between theoretical and applied topics of our practical lessons. They also found highly motivating interaction with lecturers.",2010,,no
Cost of Goods Modeling and Quality by Design for Developing Cost-Effective Processes,"Increases in recombinant protein titers expressed in mammalian cells and cell culture process volumes have shifted the economic paradigm from upstream processing to downstream processing for the manufacturing of recombinant proteins, especially monoclonal antibodies (MAbs). Moreover, the pressure exerted by authorities on the healthcare industry to decrease the costs associated with treatments, and the recent entry of biosimilars into the market, have forced the biopharmaceutical community to find new solutions and strategies to reduce production costs. MAb process development currently is focused on the optimization of yield and throughput to deliver cost-effective processes. In this article, we describe the use of a cost of goods model in combination with a Design of Experiment approach for optimizing the capture step by affinity chromatography. The most economical operating conditions identified through this approach imply sacrificing some protein without affecting the quality of the final product.",2010,,no
A dynamic process-based cost modeling approach to understand learning effects in manufacturing,"Informed technology decision-making requires a structured understanding of cost evolution over time. A dynamic approach integrating learning curves and process-based cost modeling is introduced to examine learning in manufacturing. The approach is applied to the case of a hydroforming process, and quantifies the cost impacts of learning improvements in cycle time, downtime, and reject rates. A comparison with cases of automotive assembly and wire drawing illustrates that variation in learning is tied to the individual process cost structure. The results show aggregate cost evolution is strongly dependent on cost structure and that major cost elements may not align with major cost improvement-through-learning opportunities. The analyses can be used to focus intentional learning activities on primary learning operational drivers. (C) 2010 Published by Elsevier B.V.",2010,10.1016/j.ijpe.2010.07.016,no
Groundwater-surface water interactions: New methods and models to improve understanding of processes and dynamics,"Interest in groundwater (GW)-surface water (SW) interactions has grown steadily over the last two decades. New regulations such as the EU Water Framework Directive (WFD) now call for a sustainable management of coupled ground- and surface water resources and linked ecosystems. Embracing this mandate requires new interdisciplinary research on GW-SW systems that addresses the linkages between hydrology, biogeochemistry and ecology at nested scales and specifically accounts for small-scale spatial and temporal patterns of GW-SW exchange. Methods to assess these patterns such as the use of natural tracers (e.g. heat) and integrated surface-subsurface numerical models have been refined and enhanced significantly in recent years and have improved our understanding of processes and dynamics. Numerical models are increasingly used to explore hypotheses and to develop new conceptual models of GW-SW interactions. New technologies like distributed temperature sensing (DTS) allow an assessment of process dynamics at unprecedented spatial and temporal resolution. These developments are reflected in the contributions to this Special Issue on GW-SW interactions. However, challenges remain in transferring process understanding across scales. (C) 2010 Elsevier Ltd. All rights reserved.",2010,10.1016/j.advwatres.2010.09.011,no
Implementing a whole school physical activity and healthy eating model in rural and remote First Nations schools: a process evaluation of Action Schools! BC,"Introduction: Aboriginal people who reside in rural and remote areas of Canada often have poorer health than other Canadians. For instance, the prevalence rate of type 2 diabetes is 3 to 5 times higher than for the general population. Chronic disease risk factors such as obesity are also more prevalent. Overweight and obesity have become major health challenges for all Canadian children, but for Aboriginal children, the numbers are 2 to 3 times higher. 'Action Schools! BC' (AS! BC) is a whole-school framework designed as a positive approach to addressing childhood inactivity and unhealthy eating patterns during the school day that was effective for children in a large urban center. The purpose of this study was to explore the feasibility and implementation of AS! BC in 3 remote Aboriginal communities in northern British Columbia. Methods: The AS! BC model provided tools for schools and teachers to create individualized 'action plans' to increase the opportunities for physical activity (PA) and healthy eating (HE) across 6 'action zones'. These zones included: (1) school environment; (2) scheduled physical education; (3) classroom action; (4) family and community; (5) extra-curricular; and (6) school spirit. Teachers (primarily generalists) were provided with the training and resources necessary to implement their action plan for their class. Schools had three visits from the AS! BC support team. Teachers received specialized training and support, a 'planning guide' and classroom-based resources. Gender- and skill-level-inclusive activities were prioritized. Although the model emphasized choice using a whole-school framework, 'classroom action' was a flagship component. Teachers were asked to provide students with a minimum of 15 additional minutes of PA each school day and at least one HE activity per month in the 'classroom action zone'. Information about implementation was gathered from weekly 'classroom logs' completed by teachers and focus groups with school staff. Results: The logs showed that all 3 schools implemented physical activities (mean = 140 min/week, range = 7-360 min/week) and HE activities (mean = 2.3 times/week, range = 0-10 times/week) but this varied by school and teacher. Adherence to logging was low (34% of eligible weeks). Focus group data showed that the program was well received and that support from the AS! BC master trainer and support team was crucial to delivery of the program. Staff highlighted challenges (eg time, high staff turnover at the schools and lack of financial resources), but felt that with continued support and cultural adaptations they would continue to implement AS! BC in their schools. Conclusions: The evaluation demonstrated that AS! BC was appropriate and feasible for use in the First Nations schools in these rural and remote communities with some cultural adaptations and ongoing support. Rural and remote locations have very specific challenges that need to be considered in broader dissemination strategies.",2010,,no
Analysis of intra-arch and interarch measurements from digital models with 2 impression materials and a modeling process based on cone-beam computed tomography,"Introduction: Study models are an essential part of an orthodontic record. Digital models are now available. One option for generating a digital model is cone-beam computed tomography (CBCT) scanning of orthodontic impressions and bite registrations. However, the accuracy of digital measurements from models generated by this method has yet to be thoroughly evaluated. Methods: A plastic typodont was modified with reference points for standardized intra-arch and interarch measurements, and 16 sets of maxillary and mandibular vinylpolysiloxane and alginate impressions were made. A copper wax-bite registration was made with the typodont in maximum intercuspal position to accompany each set of impressions. The impressions were shipped to OrthoProofUSA (Albuquerque, NM), where digital orthodontic models were generated via CBCT. Intra-arch and interarch measurements were made directly on the typodont with electronic digital calipers and on the digital models by using OrthoProofUSA's proprietary DigiModel software. Results: Percentage differences from the typodont of all intra-arch measurements in the alginate and vinylpolysiloxane groups were low, from 0.1% to 0.7%. Statistical analysis of the intra-arch percentage differences from the typodont of the alginate and vinylpolysiloxane groups had a statistically significant difference between the groups only for maxillary intermolar width. However, because of the small percentage differences, this was not considered clinically significant for orthodontic measurements. Percentage differences from the typodont of all interarch measurements in the alginate and vinylpolysiloxane groups were much higher, from 3.3% to 10.7%. Statistical analysis of the interarch percentage differences from the typodont of the alginate and vinylpolysiloxane groups showed statistically significant differences between the groups in both the maxillary right canine to mandibular right canine (alginate with a lower percentage difference than vinylpolysiloxane) and the maxillary left second molar to mandibular left second molar (alginate with a greater percentage difference than vinylpolysiloxane) segments. This difference, ranging from 0.24 to 0.72 mm, is clinically significant. Conclusions: In this study, digital orthodontic models from CBCT scans of alginate and vinylpolysiloxane impressions provided a dimensionally accurate representation of intra-arch relationships for orthodontic evaluation. However, the use of copper wax-bite registrations in this CBCT-based process did not result in an accurate digital representation of interarch relationships. (Am J Orthod Dentofacial Orthop 2010; 137: 456.e1-456.e9)",2010,10.1016/j.ajodo.2009.09.019,no
An evaluation of a cultured human corneal epithelial tissue model for the determination of the ocular irritation potential of pharmaceutical process materials,"Irritation and other forms of local toxicity following contact with eyes is a potentially serious problem arising from occupational exposure to chemicals. Traditionally, evaluation of the irritant potential of novel chemicals has relied on the use of in vivo studies with rabbits. Concerns about the predictive potential of in vivo methods for human hazard and demand for economical and rapid screening of chemicals has stimulated a great deal of work to investigate in vitro alternatives for evaluating ocular irritation potential. This publication describes a screening study to assess a reconstituted corneal epithelial culture system, as an alternative for testing for ocular irritation with pharmaceutical process materials, extending the chemical domain with which this system has been tested. A total of 21 test chemicals were applied to commercially supplied reconstituted human corneal epithelial (HCE) cultures and effects on tissue viability (MTT reduction assay), tissue histology and IL-alpha expression were assessed. Positive controls (0.5% and 1% SDS) showed dose- and time-related adverse effects on tissues, consistent with known irritant effects. Negative controls showed no histological changes and retained high viability throughout the time-course of the experiment. Concordance was excellent with accuracy at each sampling time point of over 80% when viability (MTT reduction) was compared with existing EU classification of the test articles for ocular irritation (classification based on results of in vivo evaluation). Tissue viability as estimated by KIT reduction appears most useful as the primary means of assessing the irritation potential of the chemicals. Histopathological examination generally agreed with the results of the MTT assay. However, the use of cytokine analysis will need further consideration as results for this parameter showed no relationship with known irritation potential. These results infer that HCE cultures, alone or as a part of a tiered hazard screening programme, have promise for use in reducing reliance on live subject tests and contribute to generation of an appropriate hazard classification and label advice. (C) 2010 Published by Elsevier Ltd.",2010,10.1016/j.tiv.2010.03.004,no
EVALUATING THE IMPACTS OF ITERATION ON PD PROCESSES BY TRANSFORMING TASK NETWORK MODELS INTO SYSTEM DYNAMICS MODELS,"Iteration is unavoidable in the design process and should be incorporated when planning and managing projects in order to minimize surprises and reduce schedule distortions. However, planning and managing iteration is challenging because the relationships between its causes and effects are complex. Most approaches which use mathematical models to analyze the impact of iteration on the design process focus on a relatively small number of its causes and effects. Therefore, insights derived from these analytical models may not be robust under a broader consideration of potential influencing factors. In this article, we synthesize an explanatory framework which describes the network of causes and effects of iteration identified from the literature, and introduce an analytic approach which combines a task network modeling approach with System Dynamics simulation. Our approach models the network of causes and effects of iteration alongside the process architecture which is required to analyze the impact of iteration on design process performance. We show how this allows managers to assess the impact of changes to process architecture and to management levers which influence iterative behavior, accounting for the fact that these changes can occur simultaneously and can accumulate in non-linear ways. We also discuss how the insights resulting from this analysis can be visualized for easier consumption by project participants not familiar with simulation methods.",2010,,no
Mask cleaning process evaluation and modeling,"Large error bars in cleaning experiments are commonly accepted in mask making but such errors restrict potential improvements in cleaning and restrict the uniform delivery of megasonic (MS) energy. Hence, large error limits in particle removals have an impact to operational costs based on contamination and breakage. New data handling methods are developed here, which exceed the current capability scatterometric particle measurement methods and which create a better statistical basis for interpretation. These improved data treatment methods employ subdivisions of the mask into regions as small as mm(2). The effective number of runs becomes many thousands of time greater which can compensate for the small number of blanks available for tests due to restricted costs. This new technology is combined with a precise modeling of the MS tracking patterns on a plate and allows better comparisons between theoretical modeling and experimentally observed cleans. The combination of these two methods yields an improved determination of rate kinetics for particle removal. Collectively, these methods provide the basis for better interpretation of the spatial non-uniformities seen in MS spin cleaning methods with obvious consequences to manufacturing costs.",2010,10.1117/12.864335,no
"Hydrogen Production from Steam Reforming of m-Cresol, a Model Compound Derived from Bio-oil: Green Process Evaluation Based on Liquid Condensate Recycling","Liquid pollutants and coke formation can cause many problems in steam reforming of bio-oil for hydrogen production. From an environmental and economic point of view, an operation of liquid condensate recycling aiming at eliminating secondary pollution as well as carbon deposition was applied in this work. Under the optimal reaction conditions, m-cresol (a heavy organic compound present in bio-oil) was steam-reformed on a highly efficient commercial Ni-based catalyst for 6 h time-on-stream. Gas product distribution, liquid pollutant formation, and carbon deposition behavior were investigated, respectively. On the basis of one-time liquid condensate recycling, a green and efficient steam-reforming process can be achieved. Under different reaction conditions, the possibility of achieving this green process was evaluated. The results indicated that under a much higher temperature (900 degrees C), m-cresol becomes easier to steam reform but it is still impossible to achieve a green process just by a single steam reforming. Under a low temperature (800 degrees C), low steam/carbon ratio (2.5), or high weight hourly space velocity of bio-oil (1.0 h(-1)), it is difficult to eliminate the liquid pollutants completely by one-time liquid condensate recycling. However, for every single test, the operation of liquid condensate recycling can make a contribution to the increase of the hydrogen yield and the reduction of secondary pollution and coke formation. It provides an alternative route for steam reforming of bio-oil, especially for some heavy components in bio-oil.",2010,10.1021/ef100369g,no
ADAPTIVE CALIBRATION AND CONTROL OF CASCADE PROCESSES WITH UNKNOWN MEASUREMENT MODEL AND ACTUATOR DYNAMICS AND ITS APPLICATION TO PAPER MANUFACTURING,"Many industrial processes are in cascade configuration in which the material being processed goes through a sequence of processing units. The output of an upstream processing unit is fed into the input of the unit downstream. In many cases, the variables of interests are only available via related uncertain surrogate measurements in the all the upstream stages, and directly measured at the output of the last stage. The problem of controlling such systems by adaptively calibrating the surrogate measurement model and the use of a pre-emptive control strategy was studied in [9]. In this paper, the unknown actuator dynamics is taken into account in the adaptive calibration and pre-emptive control as well. A Lyapunov based nonlinear control algorithm is derived. The proposed control algorithm is applied to a vacuum dewatering process in paper manufacturing industry. Simulation results show that the proposed control strategy can regulate the exiting moisture content of each unit at the desired value.",2010,,no
Modeling Hospital Process System to Determine Information Quality,"Many of the concepts and procedures of product quality control can be applied to the problem of producing better information quality (IQ). Based on the analysis of the hospital process, information quality is considered as an important contributory factor in improving hospital process performance. This paper presents a set of ideas, concepts, models, and procedures appropriate to hospital business processes that can be used to determine the information quality delivered, or transferred, to information customers. These processes produce information on a regular or as-requested basis. The model systematically tracks relevant attributes of the information quality such as timeliness, accuracy and cost. Measures of these attributes can then be used to analyze potential improvements to the hospital process under consideration. Finally, a simulation study is given to demonstrate the various features of the perioperative process system and show how it can be used to analyze and reduce delays and cancellations in surgical care.",2010,,no
Benefits of modeling of melting for the understanding of solidification processes,"Melting and solidification are both phase transformations involving a liquid and a solid phase. In a simplifying procedure melting could be treated as the inverse process of solidification. However, there are substantial differences in the thermodynamics and kinetics of melting and solidification. The elaboration of a model for melting of binary alloys has lead to the possibility to also describe solidification processes more consistently. Input parameters in the model are the Gibbs Free Energy curves and the diffusion coefficients in the liquid and solid phase, respectively. Assumptions about the thermodynamic state of the interface like local equilibrium are not necessary, recently developed interface thermodynamics is coupled with the kinetic equations. Simulations results for steady-state melting and solidification are compared. The treatment of both solidification and melting yields some insight in the properties of the liquid/solid interface and its role during the phase transformation.",2010,10.4028/www.scientific.net/MSF.649.53,no
Membrane fractionation technologies for high quality mill sugar and value-added by-products - an integrated sugar production process concept model,"Membrane filtration technology has been proven to be a technically sound process to improve the quality of clarified cane Juice and subsequently to increase the productivity of crystallisation and the quality of sugar production However, commercial applications have been hindered because the benefits to crystallisation and sugar quality have not outweighed the increased processing costs associated with membrane applications An 'Integrated Sugar Production Process (ISPP) Concept Model' is proposed to recover more value from the non-sucrose streams generated by membrane processing Pilot scale membrane fractionation trials confirmed the technical feasibility of separating high-molecular weight, antioxidant and reducing sugar fractions from cane Juice in forms suitable for value recovery It was also found that up to 40% of potassium salts from the Juice can be removed by membrane application while removing the similar amount of water with potential energy saving in subsequent evaporation Application of ISPP would allow sugar industry to co-produce multiple products and high quality mill sugar while eliminating energy intensive refining processes",2010,,no
Micro Patterning Processes for Thin Film Nitinol Endografts and Evaluation of Endothelialization in Swine Model,"Micro features were created in thin film nitinol using a novel lift-off process to create an endovascular biomedical device. This manuscript describes fabrication problems with wet etching and introduces an effective way, named ""Lift-off"" process to solve undercut and non-uniform pattern issues. Two lift-off processes (i.e., lift-off I and II) are discussed. Lift-off I process has fracture issues and the film peels off the substrate due to high aspect ratio post structures. Lift-off II process use the film on the top of the Si substrate to fabricate various shape patterns (i.e., ellipse, diamond, circle, square, etc.) in the range of 5 similar to 60 mu m. The lift-off II process shows smooth and well aligned micro patterns in thin film nitinol. In-vivo tests in swine were performed to evaluate the endothelial tissue growth through fabricated micro patterns. Angiography and SEM images show patency of the artery and a uniform endothelial layer covering the device without thromobosis.",2010,10.1117/12.847588,no
Complexity in microbial metabolic processes in soil nitrogen modeling: a case for model averaging,"Model uncertainty is rarely considered in the field of biogeochemical modeling. The standard biogeochemical modeling approach is to proceed based on one selected model with the ""right"" complexity level based on data availability. However, other plausible models can result in dissimilar answer to the scientific question in hand using the same set of data. Relying on a single model can lead to underestimation of uncertainty associated with the results and therefore lead to unreliable conclusions. Multi-model ensemble strategy is a means to exploit the diversity of skillful predictions from different models with multiple levels of complexity. The aim of this paper is two fold, first to explore the impact of a model's complexity level on the accuracy of the end results and second to introduce a probabilistic multi-model strategy in the context of a process-based biogeochemical model. We developed three different versions of a biogeochemical model, TOUGHREACT-N, with various complexity levels. Each one of these models was calibrated against the observed data from a tomato field in Western Sacramento County, California, and considered two different weighting sets on the objective function. This way we created a set of six ensemble members. The Bayesian Model Averaging (BMA) approach was then used to combine these ensemble members by the likelihood that an individual model is correct given the observations. Our results demonstrated that none of the models regardless of their complexity level under both weighting schemes were capable of representing all the different processes within our study field. Later we found that it is also valuable to explore BMA to assess the structural inadequacy inherent in each model. The performance of BMA expected prediction is generally superior to the individual models included in the ensemble especially when it comes to predicting gas emissions. The BMA assessed 95% uncertainty bounds bracket 90-100% of the observations. The results clearly indicate the need to consider a multi-model ensemble strategy over a single model selection in biogeochemical modeling study.",2010,10.1007/s00477-010-0381-4,no
Model-Based Evaluation of Avionics Maintenance and Logistics Processes,"Model-based design using executable models has the potential to increase system design efficiency and accuracy significantly. In the paper we present an industrial application which covers system design issues in the area of avionics and airline operation, especially taking into account maintenance and logistics operations and their impact on fleet availability. Such a complex system needs to be modeled on a very abstract level of detail in the early phases of design, and undergoes a stepwise refinement throughout the further development. Executability of a model ensures that the model is a behavioral specification, and allows simulation runs for a performance evaluation of design alternatives. Decisions can thus be made with more confidence. We use the software tool MLDesigner in the paper, which is capable of modeling an evaluating hierarchical multi-domain models of complex systems. Different models of computation for submodels can be mixed, and an extensive library of predefined modules is available. We present parts of an abstract airline model from an ongoing project, which is used to evaluate, among others, resource availabilities.",2010,,no
The Impact of Modeling in the Teaching-Learning Process: a symbiosis between problem solving and modeling everyday life,"Modeling has great potential in the teaching-learning process, although it is not used in class as much as would be expected. As teachers, one of our roles is (should be) to prepare tasks involving real problems for the students' daily life. This will allow them to be aware of the possible symbioses between problem solving and modeling of those kinds of situations. In this paper, I present and discuss some possible situations to introduce graphs theory from modeling daily life situations in different grades. The tasks where prepared with the goal of allowing students to build their own knowledge and the ability to make connections between different contents and contexts. This may lead them to recognize the mobility of those concepts thorough a conceptual network. I will also discuss some aspects related to the teachers' practice and professional knowledge as well as the implications of this kind of approach for the teaching-learning process.",2010,,no
Generalizing on Best Practices in Image Processing: A Model for Promoting Research Integrity Commentary on: Avoiding Twisted Pixels: Ethical Guidelines for the Appropriate Use and Manipulation of Scientific Digital Images,"Modifying images for scientific publication is now quick and easy due to changes in technology. This has created a need for new image processing guidelines and attitudes, such as those offered to the research community by Doug Cromey (Cromey 2010). We suggest that related changes in technology have simplified the task of detecting misconduct for journal editors as well as researchers, and that this simplification has caused a shift in the responsibility for reporting misconduct. We also argue that the concept of best practices in image processing can serve as a general model for education in best practices in research.",2010,10.1007/s11948-010-9226-2,no
On Measuring the Understandability of Process Models,"Much efforts are aimed at unveiling the factors that influence a person's comprehension of a business process model. While various potential factors have been proposed and studied in an experimental setting, little attention is being paid to reliability and validity requirements on measuring a person's structural understanding of a process model. This paper proposes the concepts to meaningfully argue about these notions, for the sake of improving future measurement instruments. The findings from an experiment, involving 178 students from three different universities, underline the importance of this topic. In particular, it is shown that the coverage of model-related questions is important. This paper provides various recommendations to properly measure structural model comprehension.",2010,,no
Use of simulated data from a process-based habitat model to evaluate methods for predicting species occurrence,"Much research has centered on determining which habitat model best predicts species occurrence. However, previous work typically used data sets that are inherently biased for evaluation. The use of simulated data provides a way of testing model performance using un-biased data where the true relationships between species occurrence and population processes are predefined using sound ecological theory. We used a process-based habitat model to generate simulated occurrence data to evaluate presence-absence and presence-only methods: generalized linear and generalized additive models (GLM, GAM), maximum entropy model (Maxent), and discrete choice models (DCM). This is the first study to use a DCM for predicting species distributions. We varied the effect that habitat quality had on fecundity and reported the model responses to these changes. When the effect of habitat quality on fecundity was weak, model performance was no better than random for all methods, however, performance increased as the habitat/fecundity relationship became stronger. For each level of habitat quality effect, there was little variation in performance between the presence-absence and presence-only methods. The use of a process-based habitat model to generate occurrence data for evaluating model performance has a distinct advantage over other testing methods, because no errors are made during sampling and the true ecological relationships between population process and species occurrence are known. This leads to un-biased results and increased confidence in assessing model performance and making management recommendations.",2010,10.1111/j.1600-0587.2009.05495.x,no
On the minimum description length complexity of multinomial processing tree models,"Multinomial processing tree (MPT) modeling is a statistical methodology that has been widely and successfully applied for measuring hypothesized latent cognitive processes in selected experimental paradigms. This paper concerns model complexity of MPT models. Complexity is a key and necessary concept to consider in the evaluation and selection of quantitative models. A complex model with many parameters often overfits data beyond and above the underlying regularities, and therefore, should be appropriately penalized. It has been well established and demonstrated in multiple studies that in addition to the number of parameters, a model's functional form, which refers to the way by which parameters are combined in the model equation, can also have significant effects on complexity. Given that MPT models vary greatly in their functional forms (tree structures and parameter/category assignments), it would be of interest to evaluate their effects on complexity. Addressing this issue from the minimum description length (MDL) viewpoint, we prove a series of propositions concerning various ways in which functional form contributes to the complexity of MPT models. Computational issues of complexity are also discussed. (C) 2010 Elsevier Inc. All rights reserved.",2010,10.1016/j.jmp.2010.02.001,no
Integrating utilization-focused evaluation with business process modeling for clinical research improvement,"New discoveries in basic science are creating extraordinary opportunities to design novel biomedical preventions and therapeutics for human disease. But the clinical evaluation of these new interventions is, in many instances, being hindered by a variety of legal, regulatory, policy and operational factors, few of which enhance research quality, the safety of study participants or research ethics. With the goal of helping increase the efficiency and effectiveness of clinical research, we have examined how the integration of utilization-focused evaluation with elements of business process modeling can reveal opportunities for systematic improvements in clinical research. Using data from the NW global HIV/AIDS clinical trials networks, we analyzed the absolute and relative times required to traverse defined phases associated with specific activities within the clinical protocol lifecycle. Using simple median duration and Kaplan-Meyer survival analysis, we show how such time-based analyses can provide a rationale for the prioritization of research process analysis and re-engineering, as well as a means for statistically assessing the impact of policy modifications, resource utilization, re-engineered processes and best practices. Successfully applied, this approach can help researchers be more efficient in capitalizing on new science to speed the development of improved interventions for human disease.",2010,10.3152/095820210X12827366906607,no
Understanding variation in roller compaction through finite element-based process modeling,"One of primary goals of the Quality by Design initiative in the pharmaceutical industry is to reduce variation in the product quality through increased understanding and control of the manufacturing process. In the case of roller compaction, in which mixtures of active and inert powders are fed via a screw to counter-rotating rolls, drawn into the nip region and compacted under hydrostatic and shear stresses, variation in density of the roller compacted material has been commonly observed. In the experimental part of this work we report measurements of pressure and shear under the rolls which show variation of the local stress conditions along the width of the roll which evolves with time. Also roll pressure and shear stress appear to persist past the minimum roll separation. To further investigate the potential causes of these variations, 2D and 3D explicit finite element-based models with adaptive meshing and arbitrary Eulerian-Lagrangian capabilities were developed A Drucker-Prager/cap constitutive model was used to describe the mechanical behavior of the powder Microcrystalline cellulose was used as the model powder The 2D model was used to evaluate the effects of feed stress, roll friction on roll force, profiles of roll pressure and roll shear stress, nip angle and relative density of the compacted powder. The results indicated increasing feed stress, and/or increasing roll friction lead to higher maximum roll surface pressure and attendant relative density at the exit. The results may be explained by the location of the nip angle and the amount of pre-densification in the feed zone Simulations with pressure-dependent frictional coefficients indicated significant differences in densification In addition, oscillating feed stress conditions revealed periodic variations in roll pressures and relative densities The 3D model predicted lower roll pressure and densities near the edges due to presence of side seal friction. Variable inflow of material along the roll width was related to variation in roll pressure Overall, the model predictions followed experimental trends The process modeling provided greater insight into the potential causes of the variation in density of the roller compacted material and highlighted the significance of the design of the feed system, which may be used to evaluate potential design improvements (C) 2010 Elsevier Ltd All rights reserved",2010,10.1016/j.compchemeng.2010.04.008,no
MODELING THE FORMATION OF GIANT PLANET CORES. I. EVALUATING KEY PROCESSES,"One of the most challenging problems we face in our understanding of planet formation is how Jupiter and Saturn could have formed before the solar nebula dispersed. The most popular model of giant planet formation is the so-called core accretion model. In this model a large planetary embryo formed first, mainly by two-body accretion. This is then followed by a period of inflow of nebular gas directly onto the growing planet. The core accretion model has an Achilles heel, namely the very first step. We have undertaken the most comprehensive study of this process to date. In this study, we numerically integrate the orbits of a number of planetary embryos embedded in a swarm of planetesimals. In these experiments, we have included a large number of physical processes that might enhance accretion. In particular, we have included (1) aerodynamic gas drag, (2) collisional damping between planetesimals, (3) enhanced embryo cross sections due to their atmospheres, (4) planetesimal fragmentation, and (5) planetesimal-driven migration. We find that the gravitational interaction between the embryos and the planetesimals leads to the wholesale redistribution of material-regions are cleared of material and gaps open near the embryos. Indeed, in 90% of our simulations without fragmentation, the region near those embryos is cleared of planetesimals before much growth can occur. Thus, the widely used assumption that the surface density distribution of planetesimals is smooth can lead to misleading results. In the remaining 10% of our simulations, the embryos undergo a burst of outward migration that significantly increases growth. On timescales of similar to 10(5) years, the outer embryo can migrate similar to 6 AU and grow to roughly 30 M(circle dot). This represents a largely unexplored mode of core formation. We also find that the inclusion of planetesimal fragmentation tends to inhibit growth except for a narrow range of fragment migration rates.",2010,10.1088/0004-6256/139/4/1297,no
Modeling the Effects of Information Quality on Process Performance in Operating Rooms,"Operating rooms are regarded as the most costly hospital facilities. Due to rising costs and risks, it is necessary to optimize performance of the operating rooms. High quality information has a significant effect on improving process performance and patient satisfaction, as well as resolving patient disputes. Based on the analysis of the operation process, information quality (IQ) is considered as an important contributory factor in improving patient throughput. In this paper, a theoretical framework and main content are presented. Through the establishment of quantitative information, quality indicators such as Trace-ability, Believability and Reputation, and the effect on process performance (registered queue length, waiting time, utilization of hospital facilities), together with the cost of the operating process, are analyzed from the theoretical aspect and then verified by simulation technology. Finally, the results of our studies provide evidence that simulation can provide effective decision support to drive performance in operating rooms in several phases of the IQ improvement.",2010,10.1109/UKSIM.2010.39,no
Production system with process quality control: modelling and application,"Over the past decade, there has been a great deal of research dedicated to the study of quality and the economics of production. In this article, we develop a dynamic model which is based on the hypothesis of a traditional economic production quantity model. Taguchi's cost of poor quality is used to evaluate the cost of poor quality in the dynamic production system. A practical case from the automotive industry, which uses the Six-sigma DMAIC methodology, is discussed to verify the proposed model. This study shows that there is an optimal value of quality investment to make the production system reach a reasonable quality level and minimise the production cost. Based on our model, the management can adjust its investment in quality improvement to generate considerable financial return.",2010,10.1080/00207720903470130,no
The Working Process and Time Efficiency of Patient Transportation in Cardiovascular Hospital Using Time Process Modeling,"Patient transportation is one of the daily and frequent jobs in the hospital, however, it requires much strain and time of nurses. We carried out continuous-observation time and motion study (TMS) on the second time scale with recording by the other recorder in four wards of a cardiovascular disease hospital. Based on the recorded data, we carried out time processes modeling (TPM), that visualize the each transportation process sketchy and we could investigate the workflows of transportation as event instance.",2010,,no
"POISSON POINT PROCESS MODELS SOLVE THE ""PSEUDO-ABSENCE PROBLEM"" FOR PRESENCE-ONLY DATA IN ECOLOGY","Presence-only data, point locations where a species has been recorded as being present, are often used in modeling the distribution of a species as a function of a set of explanatory variables-whether to map species occurrence, to understand its association with the environment, or to predict its response to environmental change. Currently, ecologists most commonly analyze presence-only data by adding randomly chosen ""pseudo-absences"" to the data such that it can be analyzed using logistic regression, an approach which has weaknesses in model specification, in interpretation, and in implementation. To address these issues, we propose Poisson point process modeling of the intensity of presences. We also derive a link between the proposed approach and logistic regression-specifically, we show that as the number of pseudo-absences increases (in a regular or uniform random arrangement), logistic regression slope parameters and their standard errors converge to those of the corresponding Poisson point process model. We discuss the practical implications of these results. In particular, point process modeling offers a framework for choice of the number and location of pseudo-absences, both of which are currently chosen by ad hoc and sometimes ineffective methods in ecology, a point which we illustrate by example.",2010,10.1214/10-AOAS331,no
On the Cognitive Effectiveness of Routing Symbols in Process Modeling Languages,"Process models provide visual support for analyzing and improving complex organizational processes. In this paper, we discuss differences of process modeling languages using cognitive effectiveness considerations, to make statements about the ease of use and quality of user experience. Aspects of cognitive effectiveness are of importance for learning a modeling language, creating models, and understanding models. We identify the criteria representational clarity, perceptual discriminability, perceptual immediacy, visual expressiveness, and graphic parsimony to compare and assess the cognitive effectiveness of different modeling languages. We apply these criteria in an analysis of the routing elements of UML Activity Diagrams, YAWL, BPMN, and EPCs, to uncover their relative strengths and weaknesses from a quality of user experience perspective. We draw conclusions that are relevant to the usability of these languages in business process modeling projects.",2010,,no
Problem-structuring methods and project management: an example of stakeholder involvement using Hierarchical Process Modelling methodology,"Project Management has gained in importance over the last few decades and it is increasingly common in many types of organisations. Today there is a concern over the relevance of the more conventional project management approaches to problems that are increasingly complex and constrained and involve large numbers of interested parties or stakeholders. This paper examines the relevance of problem-structuring methods to project management, focusing on the front-end of managing complex projects and discusses stakeholder involvement using Hierarchical Process Modelling methodology.",2010,10.1057/jors.2010.12,no
Evaluation of electronic nursing documentation-Nursing process model and standardized terminologies as keys to visible and transparent nursing,"Purpose: The purpose of this study was to describe and evaluate whether nurses have documented patient care in compliance with the national nursing documentation model in electronic health records, which means the use of the nursing process and the use of standardized terminology in different phases of the nursing process. Methods: The data were collected from a central hospital in 2003-2006. The data consist of the electronic nursing care plans of 67 neurological patients and 422 surgical patients. The data were analyzed using statistical methods and content analysis. Results: Standardized electronic nursing documentation is based on the nursing process, although the use of the nursing process varies across patients. There is a lack of progress notes relating to needs assessment, the identification of nursing diagnoses and care aims, and the nursing interventions planned in the documentation. The standardized terminology is used in the documentation but inconsistencies emerge in the use of the different classifications. Conclusion: The national model for electronic nursing documentation is suitable for the documentation of patient care in nursing care plans. However, health care professionals need further training in documenting patient care according to the nursing process, and in using the terminology in order to increase patient safety and improve documentation. (C) 2010 Elsevier Ireland Ltd. All rights reserved.",2010,10.1016/j.ijmedinf.2010.05.002,no
"SCIRehab: a model for rehabilitation research using comprehensive person, process and outcome data","Purpose. This article aims to present a comprehensive conceptual model of the SCIRehab project, which merges the International Classification of Functioning, Disability, and Health (ICF) focus on outcomes with the practice-based evidence (PBE) research design, which focusses on process and also quantifies person and outcomes details. The SCIRehab methodology operationalised this conceptual model to implement the most data-intensive study of spinal cord injury to date. We discuss the conceptual and methodological contributions of SCIRehab and how this comprehensive research approach may complement randomised controlled trials. Methods. PBE methodology applied to the SCIRehab study used extensive clinician input to develop taxonomies of each discipline's interventions and an electronic point-of-care documentation system to capture extensive details of the rehabilitation process. Traditional medical record abstracting and follow-up surveys were used to capture details on patient characteristics and outcomes achieved by 12 months post-injury. Results. Not applicable. Conclusions. Although data collection is not complete, the SCIRehab project has made major contributions to rehabilitation research, including a comprehensive conceptual model of person, process and outcome domains; discipline-specific taxonomies of rehabilitation interventions; and an electronic documentation system to capture details of the rehabilitation process.",2010,10.3109/09638281003775584,no
An advanced quality function deployment model using fuzzy analytic network process,"Quality function deployment (QFD) is a customer-oriented design tool used to ensure that the voice of customers is employed throughout the product planning and design stages. QFD uses the house of quality (HOQ), which is a matrix that provides a conceptual map for inter-functional planning and communication. In this paper, an advanced QFD model, based on fuzzy analytic network process (ANP) approach, is proposed to systematically take into account the interrelationship between and within the QFD components. The proposed method is aimed at expanding the current research scope from the product planning phase to the part deployment phase to provide product developers with more valuable information (ex. the importance and bottleneck level of part characteristics). Both customer requirements and the company's production demands will be used as the inputs for the QFD process to enhance the completeness and accuracy of the QFD analysis results. A case study is presented to illustrate the application of the proposed method. (C) 2010 Elsevier Inc. All rights reserved.",2010,10.1016/j.apm.2010.02.024,no
Modeling the Impact of Process Variation on Resistive Bridge Defects,"Recent research has shown that tests generated without taking process variation into account may lead to loss of test quality. At present there is no efficient device-level modeling technique that models the effect of process variation on resistive bridges. This paper presents a fast and accurate technique to model the effect of process variation on resistive bridge defects. The proposed model is implemented in two stages: firstly, it employs an accurate transistor model (BSIM4) to calculate the critical resistance of a bridge; secondly, the effect of process variation is incorporated in this model by using three transistor parameters: gate length (L), threshold voltage (V(th)) and effective mobility (mu(eff)), where each follow Gaussian distribution. Experiments are conducted on a 65-nm gate library (for illustration purposes), and results show that on average the proposed modeling technique is more than 7 times faster and in the worst case, error in bridge critical resistance is 0.8% when compared with HSPICE.",2010,,no
Calibration of stormwater quality regression models: a random process?,"Regression models are among the most frequently used models to estimate pollutants event mean concentrations (EMC) in wet weather discharges in urban catchments Two main questions dealing with the calibration of EMC regression models are investigated i) the sensitivity of models to the size and the content of data sets used for their calibration, ii) the change of modelling results when models are re-calibrated when data sets grow and change with time when new experimental data are collected Based on an experimental data set of 64 rain events monitored in a densely urbanised catchment, four TSS EMC regression models (two log-linear and two linear models) with two or three explanatory variables have been derived and analysed Model calibration with the iterative re-weighted least squares method is less sensitive and leads to more robust results than the ordinary least squares method Three calibration options have been investigated two options accounting for the chronological order of the observations, one option using random samples of events from the whole available data set Results obtained with the best performing non linear model clearly indicate that the model is highly sensitive to the size and the content of the data set used for its calibration",2010,10.2166/wst.2010.324,no
Evaluation of soil nitrogen emissions from riparian zones coupling simple process-oriented models with remote sensing data,"Riparian ecosystems have critical impacts on controlling the non-point source pollution and maintaining the health of aquatic ecosystems. In this study, a process oriented soil denitrification model was extended with algorithms from a simple nitrogen (N) cycle model and coupled to land surface remote sensing data to enhance its performance in spatial and temporal prediction of gaseous N emissions from soils in the riparian buffer zone surrounding the Guanting reservoir (China). The N emission model is based on chemical and physical relationships that govern the heat budget, soil moisture variations and nitrogen movement in soils. Besides soil water and heat processes, it includes nitrification. denitrification and ammonia (NH(3)) volatilization. SPOT-5 and Landsat-5 TM satellite data were used to derive spatial land surface information and the temporal variation in land cover parameters was also used to drive the model. A laboratory-scale anaerobic incubation experiment was used to estimate the soil denitrification model parameters for the different soil types. An in situ field-scale experiment was conducted to calibrate and validate the soil temperature, moisture and nitrogen sub-models. An indirect method was used to verify simulated N emissions, resulting in a coefficient of determination of R(2) = 0.83 between simulated and observed values. Then the model was applied to the whole riparian buffer zone catchment, using the spatial resolution (10 m) of the SPOT-5 image. Model sensitivity analysis showed that soil moisture was the most sensitive parameter for gaseous N emissions and soil denitrification was the main process affecting N losses to the atmosphere in the riparian area. From the aspect of land use management around the Guanting reservoir, the spatial structure and distribution of land cover and land use types in the riparian area should be adapted, to enhance faster ecological restoration of the wetland ecological system surrounding this strategically important water resource. (C) 2010 Elsevier B.V. All rights reserved.",2010,10.1016/j.scitotenv.2010.03.026,no
Concept Maps for the Modelling of Controlled Flexibility in Software Processes,"Software processes and corresponding models are dynamic entities that are often changed and evolved by skillful knowledge workers such as the members of a software development team. Consequently, process flexibility has been identified as one of the most important features that should be supported by both Process Modelling Languages (PMLs) and software tools that manage the processes. However, in the everyday practice, most software team members do not want total flexibility. They rather prefer to have controlled flexibility, i.e., to learn and follow advices previously modelled by a process engineer on which and how they can change the elements that compose a software process. Since process models constitute a preferred vehicle for sharing and communicating knowledge on software processes, the process engineer needs a PML that can express this controlled flexibility, along with other process perspectives. To achieve this enhanced PML, we first need a sound core set of concepts and relationships that defines the knowledge domain associated with the modelling of controlled flexibility. In this paper we capture and represent this domain by using Concept Maps (Cmaps). These include diagrams and descriptions that elicit the relationships between the concepts involved. The proposed Cmaps can then be used as input to extend a PML with modelling constructs to express controlled flexibility within software processes. Process engineers can use these constructs to define, in a process model, advices on changes that can be made to the model itself or to related instances. Software team members can then consult this controlled flexibility information within the process models and perform changes accordingly.",2010,10.1587/transinf.E93.D.2190,no
Using the semantic web to define a language for modelling controlled flexibility in software processes,"Software processes and corresponding models are dynamic entities that must evolve to cope with changes occurred in the enacting process, the software development organisation, the market and the methodologies used to produce software. However, in the everyday practice, software team members do not want total flexibility. They rather prefer to learn about and follow previously defined controlled flexibility, that is, advices on which, where, how and by whom process models and related instances can change/adapt. Process engineers can express these advices within a process model with a domain-specific language (DSL), which complements the core process modelling language with additional controlled flexibility information. Then, software team members can browse and learn on this information in process models and instances, and be guided when performing changes. In this study, the authors propose the use of the semantic web and associated ontology-based technologies to develop and evolve their controlled flexibility DSL for software processes. They use an ontology-based format to define the controlled flexibility-related concepts, descriptions and axioms that specify the formal semantics of their DSL. In addition, the authors provide concrete mappings between these ontology concepts and a unified modelling language class-based DSL metamodel and describe how it supports changes made in the ontology.",2010,10.1049/iet-sen.2010.0045,no
The Best Span Range of Control for an Efficient Organization Structure A Model Based on the Efficiency Analysis of Information Processing,"Span of control refers to the number of subordinates a supervisor has, which is commonly used in human resource management, particularly in the organization structure design. Modern telecommunication and information technology has been flattening more and more organizations. But not all the flattened organizations have widened the span of control in their structures according to the traditional theory that fewer structure levels means wider span of control. So a new explanation for this phenomenon is needed. This paper proposes a model based on the efficiency analysis of information processing, and it shows: a) there're different best span range of control for different types of organizations and positions; b) every position has a best range, but not a best numerical span value, in which good performance can be achieved; c) even if the best range supports good performance, supervisors need to adjust the value in the best range to achieve more appropriate performance by keeping balance between the perfectibility of information processing and the high gross of output. In this paper, we use ""the best range"" instead of the traditional ""the best span"" to study the span of control, and it might be helpful for further empirical researches on this topic.",2010,,no
Validation of an Adaptation of the Stress Process Model for Predicting Low Back Pain Related Long-term Disability Outcomes A Cohort Study,"Study Design. Twelve-month cohort study. Objective. The aim of the study was to examine the ability of an adaptation of the stress process model to predict different outcomes among low back pain (LBP) sufferers. Summary of Background Data. Recently, the stress process model was adapted and was shown to be useful to partially explain long-term disability related to low back pain, an important occupational health problem. Methods. French-speaking compensated workers on sick leave because of subacute common LBP (N = 439) completed a questionnaire including the adapted stress process model's factors: LIFE EVENTS AND APPRAISAL, COGNITIVE APPRAISAL OF LBP, EMOTIONAL DISTRESS, AVOIDANCE COPING STRATEGIES, and FUNCTIONAL DISABILITY. Six and 12 months later, participants gave information about their work status, number of days of absence, and functional disability. Regression analyses were performed to identify significant predictive factors of these outcomes. Pain intensity, fear of work, gender, and presence of pain radiating below the knee were used as control variables. Results. Number of days of absence, functional disability, and absence from work were predicted at 6 and 12 months by COGNITIVE APPRAISAL OF LBP and EMOTIONAL DISTRESS. Functional disability was predicted in addition by FUNCTIONAL DISABILITY at study entry (T1). When the control variables were considered, number of days of absence was predicted at 6 months by COGNITIVE APPRAISAL, fear of work, and being a male, and, in addition, by EMOTIONAL DISTRESS at 12 months. Functional disability was predicted by FUNCTIONAL DISABILITY T1, EMOTIONAL DISTRESS, COGNITIVE APPRAISAL OF LBP, and fear of work at 6 months, and by the same factors and variables at 12 months, except for FUNCTIONAL DISABILITY T1. Regarding absence from work, it was predicted at 6 months by fear of work and being a male, and at 12 months by COGNITIVE APPRAISAL of LBP and fear of work. Conclusion. In association with fear of work, 2 factors from the adapted stress process model are significantly useful for predicting LBP related long-term disability outcomes and could be targeted by preventive interventions.",2010,10.1097/BRS.0b013e3181c03d06,no
SYNCHROTRON-BASED MICROANALYSIS OF IRON DISTRIBUTION AFTER THERMAL PROCESSING AND PREDICTIVE MODELING OF RESULTING SOLAR CELL EFFICIENCY,"Synchrotron-based X-ray fluorescence microscopy is applied to study the evolution of iron silicide precipitates during phosphorus diffusion gettering and low-temperature annealing. Heavily Fe-contaminated ingot border material contains FeSi2 precipitates after rapid in-line P-diffusion firing, suggesting kinetically limited gettering in these regions. An impurity-to-efficiency (I2E) gettering model is developed to explain the results. The model demonstrates the efficacy of high-and medium-temperature processing on reducing the interstitial iron population over a range of process parameters available to industry.",2010,10.1109/PVSC.2010.5616767,no
Multiperspective-Modelling in the Process of Constructing and Understanding Physical Theories Using the Example of the Plane Mirror Image,"Teaching physics goes along with explaining natural phenomena. The modelling process during the acquisition of physical knowledge plays an important role in developing understanding and deeper insight. Novices, however, have problems with this modelling process, in particular because they do not understand that teachers are talking about models of reality and not about reality itself. Physical theories are described with linguistic and mathematical symbols; hence there exist at least two perspectives of modelling, physical and mathematical modelling. According to Greca and Moreira (2001) [2] understanding of physics in school is achieved if it is possible to predict a physical phenomenon from its physical models. Yet, apart from the physical and the mathematical perspective of modelling other perspectives of modelling are necessary for understanding complex physical phenomena. To prevent confusion for the learner it is essential to differentiate between these different perspectives of modelling. This process of differentiation between various perspectives of modelling will be referred to as 'Multiperspective-Modelling'. Prior studies (F. Goldberg and L. McDermott, (1986), Wiesner 1992) [1, 5] on how individual students think about images in plane mirrors revealed that the learners have misconceptions. Based on the idea of 'Multiperspective-Modelling' we developed and evaluated a special training for the learner. This training differentiates physical, mathematical and 'human' perspectives of modelling of the plane mirror phenomenon. The purposes of this study were to investigate the understanding of the plane mirror phenomenon of novices, before and after the special training.",2010,,no
Experimental validation of Numerical simulations of Air drawing Mathematical model of Polymers in Spunbonding Nonwoven process,"The air drawing model of polyethylene terephthalate(PET) polymer and the model of the air jet flow field in spunbonding process are founded. The air jet flow field model is simulated by means of the finite difference method. The numerical simulation computation results of distributions of the air velocity and air temperature match quite well with the experimental data. The air drawing model of polymer is solved with the help of the distributions of the air velocity measured by a Particle Image Velocimetry. The predicted filament fiber diameters agree with the experimental data well. It can be concluded that the higher initial air temperature can yield finer filament fiber diameter, and the higher initial air velocity can produce the finer fiber diameter as well. The experimental results show that the agreement between the results and experimental data is very better, which verifies the reliability of these models. Also, they reveal great prospects for this work in the field of computer assisted design (CAD) of spunbonding process.",2010,,no
QUALITY CONTROL OF THE REFINING PROCESS AT ELECTRON BEAM MELTING AND DEVELOPMENT AND IMPLEMENTATION OF ENGINEERING SUPPORT SYSTEM FOR PROCESS MODELING AND CONTROL,"The behavior of various impurities in the crystallized ingots and the evaporated base metal losses at Electron Beam Melting and Refining (EBMR) of Ti, Cu and some refractory metals are analyzed. Experimental kinetic dependencies of impurity concentrations are estimated. Simulation of the thermal processes is used for some parameter estimation at EBMR of the considered metals. Robust engineering design methods are applied for the quality improvement of the obtained ingots. Optimization of the EBMR regime conditions is performed aiming improving the quality of the obtained ingots (including the variations and the repeatability of the results) and minimization of base metal evaporation losses. The developed Engineering Support System (ESS) for EBMR plant is briefly described. The system is intended to serve modeling, control system design and simulation of EBMR and complementary production processes (either continuous or discrete) connected to the industrial needs for efficiency, adaptability, flexibility and re-configurability.",2010,,no
An Assessment of a Model for Error Processing in the CMS Data Acquisition System,"The CMS Data Acquisition System consists of O(20000) interdependent services. A system providing exception and application-specific monitoring data is essential for the operation of such a cluster. Due to the number of involved services the amount of monitoring data is higher than a human operator can handle efficiently. Thus moving the expert-knowledge for error analysis from the operator to a dedicated system is a natural choice. This reduces the number of notifications to the operator for simpler visualization and provides meaningful error cause descriptions and suggestions for possible countermeasures. This paper discusses an architecture of a workflow-based hierarchical error analysis system based on Guardians for the CMS Data Acquisition System. Guardians provide a common interface for error analysis of a specific service or subsystem. To provide effective and complete error analysis, the requirements regarding information sources, monitoring and configuration, are analyzed. Formats for common notification types are defined and a generic Guardian based on Event-Condition-Action rules is presented as a proof-of-concept.",2010,10.1088/1742-6596/219/2/022039,no
SmartCast: Mapping complexity of an industrial process into simplicity of a representative system with high modeling fidelity,"The continuous casting process in the steelmaking chain is a complex and critical process composed of many subsystems and development of an integral monitoring and control system without mapping functional complexity into architectural complexity is a design challenge in the coupled domains of steelmaking and systems engineering. This paper describes certain novel features of the design and engineering of such a system SmartCast that captures process functional complexity under stringent requirements on evolvability, reliability and performance. This is achieved using minimally-coupled modularity in a distributed framework where isolated synchronous modules communicate through database transactions. The developed system has been established in plant production mode and is running fully in accordance with its functional and performance requirements.",2010,10.1109/WCICA.2010.5554226,no
A Comprehensive Model of Simultaneous Denitrification and Methanogenic Fermentation Processes,"The denitrification process was incorporated into the IWA Anaerobic Digestion Model No. 1 (ADM1) in order to account for the effect of denitrification on the methanogenic fermentation process. The model was calibrated and optimized using previously published experimental data and kinetic parameter values obtained with a mixed, mesophilic (35 degrees C) methanogenic culture. Model simulations were used to predict the effect of nitrate reduction on the methanogenic fermentation process in batch, semi-continuous, and continuous flow reactors experiencing operational changes and/or system disturbances. The extended model clearly revealed the importance of substrate competition between denitrifiers and non-denitrifiers as well as the impact of N-oxide inhibition on process interactions between fermentation, methanogenesis, and denitrification Biotechnol. Bioeng. 2010; 105: 98-108.",2010,10.1002/bit.22443,no
Development of a Model and Measure of Process-Oriented Quality of Care for Substance Abuse Treatment,"The development of a detailed model of substance-abuse treatment (SAT) staff performance is described. The model describes the key behaviors of SAT staff. Specifically, researchers used the critical incident technique to develop the model, which includes a total of 15 dimensions, nested under four meta-dimensions: providing clinical services, employee citizenship behaviors, providing clinical support, and managerial behavior. Development and validation of a measure based on the model are also described. More than 600 SAT staff members in 51 SAT agencies completed the new measure. Factor analyses supported the measure's hypothesized dimensional structure; high internal consistency reliabilities were observed for all scales; and interrater agreement metrics indicated an acceptable level of within-agency agreement. Moreover, the measure correlated in expected and theoretically consistent ways with measures of job satisfaction and other job-related opinions.",2010,10.1007/s11414-009-9180-4,no
Application of simplified models for anaerobic biodegradability tests. Evaluation of pre-treatment processes,"The effect of thermal and sonication pre-treatment on the anaerobic degradation of sewage sludge was evaluated through the calculation of performance parameters by using three simplified mathematical models and one kinetic model. The Modified Gompertz equation, the Logistic function, Reaction Curve and First-Order models were all used with experimental data from the anaerobic biodegradability tests fed with primary and secondary thermal pre-treated sludge, and secondary sonicated sludge. All the models fit well with the experimental data, but the Reaction Curve model presented the best agreement in the fitting process. From the first-order equation no significant changes were observed in the hydrolysis constant under all conditions. Thermal pre-treatment (175 degrees C and 30 min) showed an important effect on the secondary sludge reaching an improvement of around 90% and 80% in the maximum production rate and the total biogas produced respectively. With regards to the sonication experiment, the best result was obtained when 12,400 kJ/kgTS were used, reaching an improvement of 40% in the total biogas production. (c) 2010 Elsevier B.V. All rights reserved.",2010,10.1016/j.cej.2010.03.082,no
On the Use of a Dual-Scale Model to Improve Understanding of a Pharmaceutical Freeze-Drying Process,"The evolution of product temperature and of residual ice content in the various vials of a batch during a freeze-drying process can be significantly affected by local conditions around each vial. In fact, vapor fluid dynamics in the drying chamber determines the local pressure that, taking into account the heat flow from the shelf and, eventually, radiation from chamber surfaces, is responsible for the sublimation rate and product temperature. These issues have to be taken into account when using mathematical simulation to predict the evolution of the product as a consequence of the operating conditions (recipe design), as well as during the scale-up of a recipe obtained in a small-scale equipment to a large-scale unit. In this framework, a dual-scale model can significantly improve the understanding for pharmaceuticals freeze-drying processes: it couples a three-dimensional model, describing the fluid dynamics in the chamber, and a second mathematical model, either mono- or bi-dimensional, describing the drying of the product in each vial. Thus, it can be profitably used to gain knowledge about process dynamics, and to improve the design of the equipment, as well as the performance of the control system of the process. (C) 2010 Wiley-Liss, Inc. and the American Pharmacists Association J Pharm Sci 99:4337-4350, 2010",2010,10.1002/jps.22127,no
A Hilbert Space Representation of Generalized Observables and Measurement Processes in the ESR Model,"The extended semantic realism (ESR) model recently worked out by one of the authors embodies the mathematical formalism of standard (Hilbert space) quantum mechanics in a noncontextual framework, reinterpreting quantum probabilities as conditional instead of absolute We provide here a Hilbert space representation of the generalized observables introduced by the ESR model that satisfy a simple physical condition, propose a generalization of the projection postulate, and suggest a possible mathematical descnption of the measurement process in terms of evolution of the compound system made up of the measured system and the measuring apparatus",2010,10.1007/s10773-010-0264-y,no
An evaluation of solution algorithms and numerical approximation methods for modeling an ion exchange process,"The focus of this work is on the modeling of an ion exchange process that occurs in drinking water treatment applications. The model formulation consists of a two-scale model in which a set of microscale diffusion equations representing ion exchange resin particles that vary in size and age are coupled through a boundary condition with a macroscopic ordinary differential equation (ODE), which represents the concentration of a species in a well-mixed reactor. We introduce a new age-averaged model (AAM) that averages all ion exchange particle ages for a given size particle to avoid the expensive Monte-Carlo simulation associated with previous modeling applications. We discuss two different numerical schemes to approximate both the original Monte-Carlo algorithm and the new AAM for this two-scale problem. The first scheme is based on the finite element formulation in space coupled with an existing backward difference formula-based ODE solver in time. The second scheme uses an integral equation based Krylov deferred correction (KDC) method and a fast elliptic solver (FES) for the resulting elliptic equations. Numerical results are presented to validate the new AAM algorithm, which is also shown to be more computationally efficient than the original Monte-Carlo algorithm. We also demonstrate that the higher order KDC scheme is more efficient than the traditional finite element solution approach and this advantage becomes increasingly important as the desired accuracy of the solution increases. We also discuss issues of smoothness, which affect the efficiency of the KDC-FES approach, and outline additional algorithmic changes that would further improve the efficiency of these developing methods for a wide range of applications. (c) 2010 Elsevier Inc. All rights reserved.",2010,10.1016/j.jcp.2010.03.021,no
A relaxation process for bifunctionals of displacement-Young measure state variables: A model of multi-material with micro-structured strong interface,The gradient displacement field of a micro-structured strong interface of a three-dimensional multi-material is regarded as a gradient-Young measure so that the stored strain energy of the material is defined as a bifunctional of displacement-Young measure state variables. We propose a new model by computing a suitable variational limit of this bifunctional when the thickness and the stiffness of the strong material are of order F and respectively. The stored strain energy functional associated with the model in pure displacements living in a Sobolev space is obtained as the marginal map of the limit bifunctional. We also obtain a new asymptotic formulation in terms of Young measure state variable when considering the other marginal map. (C) 2010 Elsevier Masson SAS. All rights reserved.,2010,10.1016/j.anihpc.2010.01.007,no
A New Method for F0 Tracking Errors Fix and Generation in HMM-based Mandarin Speech Synthesis using Generation Process Model,"The HIMM-based Text-to-Speech System can produce high quality synthetic speech with flexible modeling of spectral and prosodic parameters. However the quality of synthetic speech degrades when feature vectors used in training are noisy. Among all noisy features, pitch tracking errors and corresponding flawed voiced/unvoiced (VU) decisions are the two key factors in voice quality problems. Also these errors will enlarge the RMSE of phoneme duration. In HMM-based TTS durations are typically modeled statistically using state duration probability distributions and duration prediction for unseen contexts. Use of rich context features enables synthesis without high-level linguistic knowledge. In this paper, an F-0 generation process model is used to re-estimate F-0 values in the regions of pitch tracking errors, as well as in unvoiced regions. A prior knowledge of VU is imposed in each Mandarin phoneme and they are used for VU decision. Also we design two sets of syntax features to improve Mandarin phone and pause duration prediction respectively.",2010,,no
Measurements of the interfacial tension as a fundamental component of mass transfer modelling in process engineering,"The interfacial tension is a material parameter describing the interface between two fluid phases. It is also a changing physical value in mass transfer processes beyond equilibrium. Thus, the measurement of change in interfacial tension characterizes adsorption and mass transfer processes at liquid-liquid interfaces. The units of a pendant drop tensiometer can represent a mini mass transfer cell.",2010,10.1002/cite.201000025,no
Evaluating climate impacts on carbon balance of the terrestrial ecosystems in the Midwest of the United States with a process-based ecosystem model,"The Midwest of the United States includes 12 states and accounts for about a quarter of the total United State land area. In recent years, there is an increasing interest in knowing the biomass potential and carbon balance over this region for the past and the future. In this study, we use the Terrestrial Ecosystem Model (TEM) to evaluate these quantities in the region from 1948 to 2099. We first parameterize the model with field data of major crops, including corn (Zea mays), soybean (Glycine max), and wheat (Triticum spp); then the model is applied to the region for the historical period (1948-2000). Next, we evaluate the simulated forestry biomass with forest inventory data, the agricultural net primary production (NPP) with agricultural statistics data, and the regional NPP with a satellite-based product at the regional scale. Our results show that the simulated annual NPP for the Midwest increased by 1.75% per year and the whole Midwest terrestrial ecosystem acted as a carbon sink during 1948-2005. During the 21st century, vegetation and soil carbon fluxes and pools show an increase trend with a great inter-annual variability. The ecosystems serve as a carbon sink under future climate scenarios. NPP in the Midwest will increase and net ecosystem production (NEP) will also increase and show an even larger interannual variability. This study provides the information of the biomass and NEP at a state- level in the Midwest, which will be valuable for the region stakeholders to better manage their land for the purpose of increasing carbon sequestration on the one hand and meeting the increasing demand of biomass on the other.",2010,10.1007/s11027-010-9228-z,no
Cognitive Writing Processes Modelled on Shaping the Text and Quality of Writing the Text Among Students,"The model of cognitive writing processes tries to explain cognitive processes and components which are included in writing a text The quality of the written text depends on the use of strategies and techniques of writing, with a special emphasis on the strategies of planning and correcting the text The aim of the research was to examine the relation between the quality of the text, use of the two key writing strategies and the educational level of students, attitudes to writing and writing experience, attitudes to reading and reading experience and attitudes concerning the value of individual writing strategies We measured the quality of the text using the estimated cognitive level of processing information in written texts We measured the use of writing strategies by the protocol-based self-evaluation We measured the attitude to writing and writing experience, attitude to reading and reading experience, attitude on value of certain writing strategies using the Writing and reading questionnaire The results show us that participants with a higher level of education have better quality texts We established no difference in the quality of text regarding attitudes to writing and writing experience, attitudes to reading and reading experience and attitudes on value of certain writing strategies We established no difference in the use of strategic planning and correcting the text between the participants of lower and higher level of education and regarding the attitude to writing and writing experience We established difference in the use of strategic planning regarding the attitudes to reading and reading experience and attitudes on value of certain writing strategies We established no correlation between the quality of the text and the use of writing strategies",2010,,no
Bioprocesses: Modeling needs for process evaluation and sustainability assessment,"The next generation of process engineers will face a new set of challenges, with the need to devise new bioprocesses, with high selectivity for pharmaceutical manufacture, and for lower value chemicals manufacture based on renewable feedstocks In this paper the current and predicted future roles of process system engineering and life cycle inventory and assessment in the design, development and improvement of sustainable bioprocesses are explored The existing process systems engineering software tools will prove essential to assist this work However, the existing tools will also require further development such that they can also be used to evaluate processes against sustainability metrics, as well as economics as an integral part of assessments Finally, property models will also be required based on compounds not currently present in existing databases. It is clear that many new opportunities for process systems engineering will be forthcoming in the area of integrated bioprocesses. (C) 2010 Elsevier Ltd. All rights reserved",2010,10.1016/j.compchemeng.2010.03.010,no
MODEL SOLUTIONS (CMC-SUCROSE) STABILITY EVALUATION AFTER SEVERAL FREEZING PROCESS,"The objective of this paper was to evaluate the stability of the model solutions determining initial freezing temperature after repeated freezing process. The solutions were composed by water, sucrose and Carboxymethylcellulose (CMC). Were evaluated the concentrations of sucrose from 15% to 31.1% (w/w) and the concentration of CMC from 0.5%, 1.0% to 1.5% (w/w). It was also studied the possibility of reutilization of the model solutions for freezing tests. The results showed that at the concentration of 0.5% there was no alteration at the water binding capacity. The variation of sucrose concentration did not interfered in the stability of the solution during the study",2010,,no
Microbiological quality in the flour and starch cassava processing in traditional and model unit,"The objective of this research was to evaluate microbiological contamination in the flour and starch during cassava processing in traditional and model units. The total and fiscal conforms indexes, Bacillus cereus, Salmonella, bacteria, yeast and fungi were determined. Bacillus cereus and Salmonella were not detected in any sample. The incidence of microorganisms decreased along the processing to obtain cassava flour, and is lower in model unit. After the roasting, process, the microbial load was below the values established by the Brazilian legislation, and can be regarded as a critical step in obtaining cassava, flour Concerning starch production. the microbial load in the traditional units was higher than in the model units. and the increase of the extraction steps has promoted the growth of microorganisms. It's recommended the used of only 4 extractions.",2010,,no
Application of General Linear Model to the Reduction of Defectives in Packaging Process of Soap Industry,"The objective of this study is to improve the efficiency of the flow-wrap packaging process in soap industry through the reduction of defectives. At the 95% confidence level, with the regression analysis, the sealing temperature, temperatures of upper and lower crimper are found to be the significant factors for the flow-wrap process with respect to the number/percentage of defectives. Twenty seven experiments have been designed and performed according to three levels of each controllable factor. With the general linear model (GLM), the suggested values for the sealing temperature, temperatures of upper and lower crimpers are 185, 85 and 85 degrees C, respectively. Under the suggested process condition, the percentage of defectives is reduced from 12.47% to 5.51% and at the significant level of 5%, the percentage of defectives is between 5.05% and 5.98%.",2010,,no
A Study of CFD Models for Investigating the Water Beam Assisted Form Error In-Process Optical Measurement,"The opaque coolant used in the removal machining processes has an inaccessibility problem for the form error in-process optical measurement. The water beam assisted approach should be one of the possible solutions for the problem. In this project, we propose to study the measurement process using the computational approach for better understanding of the process and for validating the models and settings. The experimental results and the Computational Fluid Dynamics (CFD) results are found close to each other. This gives us confidence in using the proposed CFD models and settings. The proposed new transparent window definition is suitable. The models and settings established include geometric model, mesh adaption model, domain settings, coolant concentration representation, expressions, boundary conditions, and CFX solver settings. The developed CFD models and settings will be useful in our further studies of the form error measurement method.",2010,10.4028/www.scientific.net/KEM.437.472,no
Optimization models in agriculture and natural resources: the validation and verification process,"The optimization tools have been extensively used for developing the agriculture production planning decision support tools all over the world during several past decades. Although many different models based on different assumptions and mathematical methods have been developed, very few deal with the verification and validation procedures for these optimization models. This stems mainly from the fact that each such optimization problem is unique, hence no general validation and verification process can be designed. This paper aims to show the possibilities of developing a verification and validation process for the particular problem of optimizing the sowing plan when the randomness of harvests and the crop succession requirements are considered. A stochastic programming model is applied, the verification of the model and the resowing constraints are discussed and the validation procedure based on Monte Carlo simulations is suggested and performed for the particular case of a South Moravian farm.",2010,,no
Compensating the Overlay Modeling Errors in Lithography Process of Wafer Stepper,"The overlay modeling errors are commonly modeled as the sum of inter-field and intra-field errors in lithography process of wafer stepper. The inter-field errors characterize the global effect while the intra-field errors represent the local effect. To have a better resolution and alignment accuracy, it is important to model the overlay errors and compensate them into tolerances. This paper proposes a weighted least squares (WLS) estimator for two general overlay error models such that more accurate linear term parameters of the overlay error can be obtained. First, the least squares (LS) estimator is applied to obtain the parameters of linear and nonlinear terms. We intend to estimate the parameters of linear term while taking the nonlinear term as our modeling residual errors. Next, we use the WLS estimator to obtain more accurate parameters of linear term and thus reduced the modeling errors by choosing appropriate stepper control parameters. The WLS estimator is applied to a real data set collecting 537 wafers from a wafer fabrication facility. The test results demonstrate that the estimated linear term parameters of the WLS estimator are much more close to the assumed ones than the LS estimator.",2010,,no
ATTRIBUTE-BASED KNOWLEDGE REPRESENTATION IN THE PROCESS OF DEFECT DIAGNOSIS,"The problem of casting defects diagnosis includes several distinct areas in which only the relevant global perspective allows for a satisfactory solution of the diagnostic task. One of these areas relates to knowledge about the parameters of the possible occurrence of defects. This knowledge is the more valuable, the more extensive it is, and the more numerous are the sources it has been acquired from. It should be combined into a coherent whole and given the form allowing for component processing. The paper proposes a solution in which the methods of knowledge engineering and artificial intelligence have been used. In this solution, a very important role is played by formalisms and inference algorithm design as they have a decisive impact on the effectiveness of the diagnostic process. A new solution proposed in this paper is to use attribute table as, a tool supporting the identification of defects. To this purpose the table has been developed (as on open system) in which descriptions of defects from various sources (international literature, expert's knowledge, production data) was collected. The entire system creates a consistent methodological approach, enabling more comprehensive treatment of the diagnostic process, what should be noted as a new solution of the problem. All this results in increased efficiency and reliability of the diagnostic process.",2010,,no
Empirical likelihood inference for semiparametric model with linear process errors,The purpose of this article is to use the empirical likelihood method to study the confidence regions construction for the parameters of interest in semiparametric model with linear process errors under martingale difference. It is shown that the adjusted empirical log-likelihood ratio at the true parameters is asymptotically chi-squared. A simulation study indicates that the adjusted empirical likelihood works better than a normal approximation-based approach. (C) 2009 The Korean Statistical Society. Published by Elsevier B.V. All rights reserved.,2010,10.1016/j.jkss.2009.04.001,no
A fuzzy analytic network process (ANP) model for measurement of the sectoral competititon level (SCL),"The purpose of this study is to measure the sectoral competition level (SCL) of an organization within the framework of Porter's five forces analysis by using fuzzy analytic network process (ANP) technique The framework of the study was based on two main thoughts: First is the necessity to approach to the problem on fuzzy logic basis because of complexity and vagueness. which are inherent in the nature of the competition concept. Second is the necessity to take into consideration the mutual Interactions between factors as five forces analysis of Porter suggests so in order to determine SCL. In the scope of the study, firstly the factors and sub-factors which have an impact oil the competition level were determined and then inner dependencies between models and factors were examined The Weights Of sub-factors were determined oil the basis of these examined dependencies and these weights were used for a case Study to determine SCL (C) 2009 Elsevier Ltd All rights reserved",2010,10.1016/j.eswa.2009.05.074,no
Prediction of Business Process Model Quality Based on Structural Metrics,"The quality of business process models is an increasing concern as enterprise-wide modelling initiatives have to rely heavily on non-expert modellers. Quality in this context can be directly related to the actual usage of these process models, in particular to their understandability and modifiability. Since these attributes of a model can only be assessed a posteriori, it is of central importance for quality management to identify significant predictors for them. A variety of structural metrics have recently been proposed, which are tailored to approximate these usage characteristics. In this paper, we address a gap in terms of validation for metrics regarding understandability and modifiability. Our results demonstrate the predictive power of these metrics. These findings have strong implications for the design of modelling guidelines.",2010,,no
"Predicting the effects of cycle time variability on the efficiency of electronics assembly mixed-model, zero-buffer flow processing lines","The research literature emphasises the need to use flow processing lines to undertake processing and assembly within low demand volume, high product variety electronics manufacturing environments that have significant levels of product, process and demand variability to contend with. Currently, the presence of such high levels of product, process and demand variability prevents the design of efficient flow processing lines by significantly disrupting the synchronisation of materials movement between work stations, resulting in under-utilisation of manufacturing resources, long lead times and poor delivery reliability. In order to ensure efficient flow processing under such conditions, a range of methods has been developed for both reducing levels of variability and for managing the effects of variability. However, ensuring the effective use of each of these methods requires detailed knowledge of the effects this variability has on the resource requirements of individual workstations. The current research is concerned with the development of predictive models that can quantitatively estimate the amounts of blocking and waiting, on individual workstations along a flow line, arising from differences in cycle times between these workstations. Information derived from such models are able to enable more precise and effective use of the methods used to off-set the effects of cycle time variability.",2010,10.1080/0951192X.2010.500679,no
Declarative versus Imperative Process Modeling Languages: The Issue of Maintainability,"The rise of interest in declarative languages for process modeling both justifies and demands empirical investigations into their presumed advantages over more traditional, imperative alternatives. Our concern in this paper is with the ease of maintaining business process models, for example clue to changing performance or conformance demands. We aim to contribute to a rigorous, theoretical discussion of this topic by drawing a link to well-established research on maintainability of information artifacts.",2010,,no
Use of sophisticated heat exchanger simulation models for investigation of possible design and operational pitfalls in LNG processes,"The simulation rating programs S-FIN for PFHE and S-PLATE for PHE have been developed at SINTEF Energy Research. These tools can be incorporated in process simulation environments like PRO/II and Aspen HYSYS (R), and thus be used as an integrated part when doing process energy simulation and optimization. Static flow instabilities that can occur in heat exchangers used in cryogenic services are discussed. Examples on how to perform, and how to interpret, a Ledinegg instability analysis, are shown using the developed programs. With the well-known single mixed refrigerant process as a case study, a thermally valid plate-fin heat exchanger was designed that was subjected to Ledinegg instability. Remedies to avoid this and the effect on the process energy consumption are discussed. For the selected case, the compressor power increased by 14% going from an unstable to a stable design/operation. The examples show that detailed heat exchanger simulations should be performed as a part of process optimization. (C) 2010 Elsevier B.V. All rights reserved.",2010,10.1016/j.jngse.2010.10.003,no
Neutron Diffraction Residual Stress Evaluation and Numerical Modeling of Coating Obtained by PTA Process,"The Stellite 6 hardfacing alloys have been deposited on steel substrate using a Plasma Transferred Arc (PTA) with complex geometry. The residual strain of the PTA technology at the surface of coating layer and the interface were determined by neutron diffraction method. In the present work, a numerical model for the residual stresses formed during the PTA process with physical conditions and mechanical properties using the Abaqus code is analysed. The result reveals that the residual stresses obtained by the numerical simulation are in very good agreement with experimental results by the neutron diffraction.",2010,10.4028/www.scientific.net/MSF.638-642.594,no
Understanding the effect of non-conventional laser beam geometry on material processing by finite-element modelling,"The temperature distribution inside the material is of prime importance in laser material processing. Optimization of laser processes requires control over this temperature distribution in order to achieve the desired outcomes by manipulating heating/cooling rates and thermal gradients. So far, most of the laser material processing has been carried out by using circular and rectangular beam geometries with variations in laser power, spot size, and scanning speed. However, variations in these parameters are often limited by other processing conditions and it is not always possible to change, e. g. the scanning speed or laser power. One possible method of varying the temperature distribution, and hence the heating/cooling rates and thermal gradients, is to modify the geometry of laser beams. It has been shown that non-conventional laser beam geometries can be effectively employed for laser processes such as surface heating, transformation hardening, forming, melting of metallic materials, and laser cutting. This article presents a review of non-conventional laser beam geometries that can be utilized to improve and/or optimize many laser material processes. Some examples of laser material processing that have been previously studied in detail are briefly discussed.",2010,10.1243/09544062JMES1745,no
Using ARM Observations to Evaluate Cloud and Clear-Sky Radiation Processes as Simulated by the Canadian Regional Climate Model GEM,"The total downwelling shortwave (SWID) and longwave (LWD) radiation and its components are assessed for the limited-area version of the Global Environmental Multiscale Model (GEM-LAM) against Atmospheric Radiation Measurements (ARM) at two sites: the southern Great Plains (SGP) and the North Slope of Alaska (NSA) for the 1998-2005 period. The model and observed SWD and LWD are evaluated as a function of the cloud fraction (CF), that is, for overcast and clear-sky conditions separately, to isolate and analyze different interactions between radiation and 1) atmospheric aerosols and water vapor and 2) cloud liquid water. Through analysis of the mean diurnal cycle and normalized frequency distributions of surface radiation fluxes, the primary radiation error in GEM-LAM is seen to be an excess in SWD in the middle of the day. The SWD bias results from a combination of underestimated CF and clouds, when present, possessing a too-high solar transmissivity, which is particularly the case for optically thin clouds. Concurrent with the SWD bias, a near-surface warm bias develops in GEM-LAM, particularly at the SGP site in the summer. The ultimate cause of this warm bias is difficult to uniquely determine because of the range of complex interactions between the surface, atmospheric, and radiation processes that are involved. Possible feedback loops influencing this warm bias are discussed. The near-surface warm bias is the primary cause of an excess clear-sky LWD. This excess is partially balanced with respect to the all-sky LWD by an underestimated CF, which causes a negative bias in simulated all-sky emissivity. It is shown that there is a strong interaction between all the components influencing the simulated surface radiation fluxes with frequent error compensation, emphasizing the need to evaluate the individual radiation components at high time frequency.",2010,10.1175/2009MWR2745.1,no
The Process Reengineering about Public Human Resources Training Based on the Quality Model,"The training based on the quality model can overcome the problems that it is difficult to solve for the traditional training model, so the integration between quality model and the public human resource training is inevitable. Different from the training process based on position, the training based on the quality model will have a strategic and global nature and can better highlight the targeted and personalized feature, so it has a distinct role to upgrade the performance of public sector human resource development.",2010,,no
"Volumetric error modelling, measurement, and compensation for an integrated measurement-processing machine tool","The volumetric error of a measurement-processing integrated machine tool was studied by using a complicated surface workpiece grinding machine as a special example. The model of volumetric error was established by using homogeneous transformation matrices, and the effect of volumetric error on coordinate transformation between the measurement and the processing work station was analysed. Various error components of the machine tool were measured with a laser interferometer and an electronic level, and the volumetric error was compensated by external software. With a ball bar system and grinding experiments, the volumetric position accuracy was tested after compensation. The experiment results illustrated that both the volumetric position accuracy and machining precision were improved dramatically after compensation.",2010,10.1243/09544062JMES2200,no
Calibration and validation of a simplified process-based model for the prediction of the carbon balance of Scottish Sitka spruce (Picea sitchensis) plantations,"There is increasing recognition that forestry provides a low cost and robust means of climate change abatement through carbon sequestration and substitution. However, current understanding of forest ecosystem carbon exchange and forest-atmosphere interactions are often inadequately characterized by existing empirical growth models with resulting poor representation for regional extrapolations. In this paper, we describe the parameterisation and independent validation, against both eddy covariance and forest growth experimental data, of a process-oriented model 3PGN to provide assessments of carbon sequestration of Sitka spruce (Picea sitchensis (Bong.) Carriere) plantations across Scotland. In comparison with eddy covariance measurements, the model predicted all of the major annual carbon fluxes, i.e., gross primary production (P(G)), net ecosystem production (P(E)), and ecosystem respiration (R(E)), with biases lower than 10%. At a monthly time step, only P(G) and P(E) were accurately estimated, whereas R(E) was not. At longer time scales (i.e., several decades), the model reliably represented the major patterns of the carbon balance. Soil type was identified as the important factor influencing site productivity; fertilization practices did not alter long-term site nutritional status. The analyses also highlighted the potential impact of carbon loss from carbon-rich soils, which can result in differences between optimal rotation length for carbon sequestration and for timber production.",2010,10.1139/X10-181,no
"Gendered and Social Hierarchies in Problem Representation and Policy Processes: ""Domestic Violence"" in Finland and Scotland","This article identifies and critiques presumptions about gender and violence that continue to frame and inform the processes of policy formation and implementation on domestic violence. It also deconstructs the agendered nature of policy as gendered, multilevel individual and collective action. Drawing on comparative illustrative material from Finland and Scotland, we discuss how national policies and discourses emphasize physical forms of violence, place the onus on the agency of women, and encourage a narrow conceptualization of violence in relationships. The two countries do this in somewhat comparable, though different ways operating within distinct national gender contexts. The complex interweaving of masculinities, violence, and cultures, although recognized in many debates, is seemingly marginalized from dominant discourses, policy, and legal processes. Despite growth in critical studies on men, there is little attempt made to problematize the gendered nature of violence. Rather, policy and service outcomes reflect processes through which individualized and masculine discourses frame ideas, discourses, and policy work. Women experiencing violence are constructed as victims and potential survivors of violence, although the social and gendered hierarchies evident in policies and services result in longer-term inequities and suffering for women and their dependents.",2010,10.1177/1077801209355185,no
Understanding Scientific Study via Process Modeling,"This paper argues that scientific studies distinguish themselves from other studies by a combination of their processes, their (knowledge) elements and the roles of these elements. This is supported by constructing a process model. An illustrative example based on Newtonian mechanics shows how scientific knowledge is structured according to the process model. To distinguish scientific studies from research and scientific research, two additional process models are built for such processes. We apply these process models: (1) to argue that scientific progress should emphasize both the process of change and the content of change; (2) to chart the major stages of scientific study development; and (3) to define ""science"".",2010,10.1007/s10699-009-9168-9,no
Advanced modeling of electron avalanche process in polymeric dielectric voids: Simulations and experimental validation,"This paper deals with aging phenomena in polymers under electric stress. In particular, we focus our efforts on the development of a novel theoretical method accounting for the discharge process partial discharge) in well known defects present in polymers, which are essentially tiny air gaps embedded in a polymeric matrix. Such defects are believed to act as trigger points for the partial discharges and their induced aging process. The model accounts for the amplitude as well as the energy distribution of the electrons during their motion, particularly at the time in which they impact on the polymer surface. The knowledge of the number of generated electrons and of their energy distributions is fundamental to evaluate the amount of damage caused by an avalanche on the polymer-void interface and get novel insights of the basic phenomena underlying the relevant aging processes. The calculation of such quantities would require generally the combined solution of the Boltzmann equation in the energy and space/time domains. The proposed method simplifies the problem, taking into account only the main phenomena involved in the process and provides a partial discharge (PD) model virtually free of adjustable parameters. This model is validated by an accurate experimental procedure aimed at reproducing the same conditions of the simulations and regarding air gaps embedded in polymeric dielectrics. The experimental results confirm the validity and accuracy of the proposed approach. (C) 2010 American Institute of Physics. [doi:10.1063/1.3359713]",2010,10.1063/1.3359713,no
MODELING AND EXPERIMENTAL VALIDATION OF THE EFFECT OF SAND FILLING ON AVOIDING WRINKLING PHENOMENON IN THIN-WALLED TUBE BENDING PROCESS,"This paper investigates a method to avoid the wrinkling in thin-walled tubes in bending process. In the tube bending process there are several effective parameters such as wall thickness, outer diameter-to-wall thickness ratio, centerline bending radius-to-outer diameter ratio. Any mismatch in the selection of the process parameters would cause defects like wrinkling, serve changes in wall thickness, and cross section distortion. For example, the depth of wrinkling increases with reduction in wall thickness and outer diameter-to-wall thickness ratio for a certain bending angle and radius. In this research, to avoid wrinkle initiation, tube is filled by sand and then bended. This sandy core is supported the tube from inner, and tube is prepared to bending. After bending process, sand is removed. In this work, to study the process numerically, a 3D finite element model of the horizontal bending process is built using ANSYS software. Then, experimental tests have been carried out to verify the simulation results and are developed to provide additional insight. A comparison between numerical and experimental results shows a reasonable agreement. It shows that wrinkle initiation can be avoided with filler material like sand.",2010,,no
USING THE WEIGHTED CAUSE - EFFECT DIAGRAM IN THE PROCESS OF QUALITY ASSURANCE IN HIGHER EDUCATION,"This paper presents, first, a classification of cause-effect diagrams, as some of the main quality instruments. It is also presented the general methodology for developing the cause-effect diagrams. The weighted cause-effect diagrams are defined and their general characteristics are presented. This paper shows the utility of the weighted cause - effect diagram, where, in fact, each case has a certain weight, being possible to track down the case with the major influence on the analyzed effect. There are presented various methods to weight the cases. It is shown that this approach is part of the contemporary scientific trend to use quantifications, hierarchy, ordering. This trend is to eliminate, as possible, the uncertainty in scientific approach of reality. The paper presents a classification of the effects of the quality assurance process. These effects are, in fact, standards and performance indicators used in the quality assessment process in universities. It is shown the cases that influence these effects, at different hierarchic levels and in different organizational structures of university. It is made a weighting of these cases, obtaining, by default, a hierarchy of these cases.",2010,,no
A Model of Information Systems Operation and Maintenance Process Complexity,"This paper proposes a model of information systems operation and maintenance process complexity. It defines three metrics which include execution complexity, information complexity and flow complexity; presents formal definition and calculation method of these metrics; describes the process of evaluation; and analyzes the influence of the automation level, information sharing and workflow structure of the process on complexity, thereby proposing some methods for optimizing the process, including complexity hot-spot analysis, public activity extraction, common information convergence, and branch rearrangement. This model measures not only the operational complexity but also the structural complexity of the non-serial operation and maintenance process. Measurement indicators are simple and universal, and can reflect the characteristics inherent in the operation and maintenance process. Therefore, this model can be used to analyze, evaluate and optimize the complexity of the information systems operation and maintenance process, thus improving efficiency and quality of operation and maintenance.",2010,,no
EVALUATION OF THE SENSITIVITY OF AN IN VITRO HIGH FREQUENCY ULTRASOUND DEVICE TO MONITOR THE COAGULATION PROCESS: STUDY OF THE EFFECTS OF HEPARIN TREATMENT IN A MURINE MODEL,"This study evaluates the sensitivity of a new in vitro high frequency ultrasound test of the whole blood coagulation process. A rat model of anticoagulant treatment is reported. Many recent studies of the role of red blood cells in the whole blood coagulation process have revealed an increasing demand for global tests of the coagulation process performed on whole blood instead of plasma samples. In contrast to existing optical tests, high frequency ultrasound presents the advantages of characterizing the mechanical properties of whole blood clotting. Ultrasound longitudinal wave velocity and integrated attenuation coefficient (IAC) were simultaneously assessed in a 10 to 30 MHz frequency range during the whole blood coagulation process in vitro in rats under anticoagulant therapy. Differences between humans and rats were also clearly emphasized in non-clotting blood and in clotting blood using specific criteria deduced from acoustic parameters (ultrasound velocity for non-clotting blood: = 1574 +/- 2 m/s for rats and 1583 +/- 3 m/s for humans and IAC = 2.25 +/- 0.14 dB/cm for rats and 1.5 +/- 0.23 dB/cm for humans). We also measured the coagulation time t(0) from the acoustic velocity (t(0) = 11.15 +/- 7 min for control rat blood and 43.3 +/- 11.4 min for human blood). Different doses of heparin were administered to rats. The sensitivity of the ultrasound device to the effects of heparin was evaluated. Differences between non-treated rats and chronically and acutely treated rats were recorded and quantified. We particularly noted that the slope S and the amplitude I of the variations in acoustic velocity were linked to clot retraction, which is a good indicator of the platelet function. The amplitude of the variations in S was between (20 +/- 8) x 10(-3) m/s(2) for control group rats, and (0.92 +/- 0.35) x 10(-3) m/s(2) for chronic heparin-treated group rats. The values of I were 15 times higher for control group rats than for chronic heparin-treated group rats. (E-mail: rachel.libgot@univ-tours.fr) (C) 2010 World Federation for Ultrasound in Medicine & Biology.",2010,10.1016/j.ultrasmedbio.2009.10.010,no
Analytic Network Process-Based Approach to Evaluate the Preferable Bancassurance Alliance Model from Supervisory Authorities' Perspectives,"This study explores the most preferable model of financial alliance between banks and insurance companies from the point of supervisory authorities. As a decision support system, we incorporate the opinions of industry experts and apply the Analytic Network Process (ANP) technique. Based on the evaluations of the expert panel, the financial holding company option was preferred by Taiwan's Financial Supervisory Commission. The results find the best bancassurance alliance structure is Financial holding company and the order importance of four criteria according to priority are Sufficiency of capital, Risks, Government policies, and Alliance fruitages. This study provides an evaluation criterion for determining the optimal alliance structure for Taiwan's emerging bancassurance sector, and the proposed evaluation criterion provides high-level management of financial institutions, government supervisors, and academicians with recommendations for future development.",2010,10.1520/JTE102720,no
A Study of Undergraduate Physics Students' Understanding of Heat Conduction Based on Mental Model Theory and an Ontology-Process Analysis,"This study first used a new approach, combining students' ontological beliefs and process explanations, to represent students' mental models of heat conduction and then examined the relationships between their mental models and their predictions. Clinical interviews were conducted to probe 30 undergraduate physics students' mental models and their predictions about heat conduction. This study adopted a constant comparative method to discover patterns of the participants' responses across the various sources of data, such as verbal utterances, writings, and drawings. The results indicate that, based on the identified five process analogies for how heat is conducted and three ontological beliefs about the material basis for heat conduction, the combinations of these two aspects can better represent their mental models in terms of both the underlying mechanisms and emergent processes of heat conduction than using either alone as has sometimes been done in prior research. In addition, while a scientifically accepted mental model had a better chance to be accompanied by a correct prediction, a correct prediction might not result from a scientifically accepted mental model. However, as suggested by some cognitive psychologists, regardless of which mental models the participants possessed, they tended to automatically retrieve their learned rules or past experience, instead of manipulating their mental models, to generate predictions for the encountered problems. (c) 2009 Wiley Periodicals, Inc. Sci Ed 94:825-854, 2010",2010,10.1002/sce.20385,no
Developing and Assessing Commonality Metrics for Product Families: A Process-Based Cost-Modeling Approach,"To be competitive in today's global economy, firms must deliver more products that are viable in the marketplace for shorter times. The use of product families allows firms to meet these needs in a cost-competitive manner. The determination of which components to share and which should be unique is very important to the development of product families. Commonality metrics are presented with the goal of assessing (at the early stages of development) the ability of the product family to reduce costs. The methodology of process-based cost modeling is used to project product development, fabrication, and assembly costs in both the standalone and shared situations. A case study of two automotive instrument panel beams is analyzed. Linear-regression analysis shows that when compared to total cost savings, a simple piece commonality metric and a fabrication-investment-weighted metric have higher R(2) than a mass-or piece-cost-weighted metric. When correlated to fixed cost savings, the fabrication-investment-weighted metric has the highest R(2) (0.62) and is significant at the 0.025 level. Fixed cost savings are proposed as the desired quantity when assessing product family efficiency.",2010,10.1109/TEM.2009.2034642,no
Error correction of theory model of process stress accelerated test,"To correct the error in theoretical model of process stress accelerated test, a new calculation method is proposed The new method, based on computer-aided calculation, can significantly reduce the error of the model Theoretical data is calculated using both the novel model algorithm, which is the root test method, and the old model algorithm The results shows that the old model algorithm can generate errors of more than 13% m the activation energy, and of errors more than 150% in the extrapolated lifetimes (Q <= 1.0 ev), while the novel model algorithm generates errors in less than 1% in activation energy, and errors less than -4.1% in the extrapolated lifetime",2010,,no
Model-based Image Processing in Eddy Current Nondestructive Evaluation,"To improve the ability of eddy current nondestructive evaluation to characterize fatigue cracks in multilayer structures, a model-based image processing approach is presented that fits models based on first-principles to image data. Model refinements are presented that address edge responses, error associated with adjacent holes and poor hole centering due to cracks. Using this approach, improvements are demonstrated in the visual presentation of image data for crack detection and the potential for characterization.",2010,10.3233/978-1-60750-554-9-254,no
Optical metrology for process control: Modeling and simulation of sensors for a comparison of different measurement principles,"To increase the quality of future products and decrease the manufacturing cost at the same time a systematic control of the fabricated objects is necessary. A promising approach for inline quality control of surface and form parameters is the use of optical measurement systems. This is due to the non-destructive nature of the optical measurement techniques. But in the production environment there are many challenges to overcome for optical sensors. Examples are temperature fluctuation, vibrations, fluids on the object surface and rough surfaces. Therefore, it is likely that not all optical measurement methods are suitable for that task. Hence, a classification of the different principles is necessary with the objective to identify the most appropriate measurement approach for a particular inspection task. In this contribution we start with a systematic approach for a review of sensors within production systems. Then we concentrate on the most robust class of optical sensors, the point sensors. In order to minimize the effect of mechanical vibrations it is desirable to employ measurement techniques that are able to measure the height of an object point in a very short time. Therefore, we focus in this work on chromatic-confocal microscopy and spectral interferometry. The aim is to compare these measurement methods for their ability to cope with the challenges given by the production environment in general. To this end we will develop simulation models for the mentioned techniques and compare two exemplarily sensors for their capability to be used for process control.",2010,10.1117/12.855053,no
Process Design as Basis for Comprehensive Process Modeling,"Today, with process management generally established as a management tool, there is a stringed interest in process modeling Although there is a multitude of standard process modeling techniques available, often the modeling results are not satisfying, One of the reasons could be that the so called design phases including a requirement analysis and the implementation in an appropriate modeling language and tool is neglected Thus, we want to offer a framework which focus more on the design and gives the process modeler the option to design an adequate process modeling language and tool",2010,,no
Evaluation of the impacts of defoliation by tropical cyclones on a Japanese forest's carbon budget using flux data and a process-based model,"Tropical cyclones (""typhoons"") affect ecosystem processes by disturbing the ecosystem's biogeochemistry, population dynamics, and services. The impact of 10 typhoons that struck the Japanese Islands in 2004 was studied using a process-based ecosystem model (Vegetation Integrative SImulator for Trace gases) and by interpreting deviations in the carbon budget observed at the Takayama forest site in central Japan. The site-calibrated model appropriately simulated gross and net CO2 fluxes in most years but could not capture the clear depression of CO2 uptake in 2004, probably because it neglected the impact of defoliation on the canopy's carbon gain due to strong typhoon winds. The defoliation intensity caused by each typhoon event was inversely estimated from the flux measurement data and using a Monte Carlo approach. Accounting for the repeated 10%-20% defoliation that occurred in 2004 lowered the canopy carbon gain by nearly 200 g C m(-2) yr(-1), resulting in better agreement between the estimated and observed flux values. Comparison of the estimated defoliation pattern with satellite-based estimates of leaf area index indicated that such moderate defoliation from midsummer to autumn was plausible. The influence of CO2 emissions from typhoon-generated debris on the carbon budget in subsequent years was estimated to be tiny in this case. These results have implications for regional carbon accounting studies, because both devastating and moderate defoliation caused by tropical cyclones can have a marked effect on the regional carbon budget, especially when defoliation occurs repeatedly within a single growing season.",2010,10.1029/2010JG001314,no
Toward understanding the large-scale land-atmosphere coupling in the models: Roles of different processes,"Two different Atmospheric General Circulation Models (AGCMs), each coupled to three different land surface schemes (LSSs) (six different model configurations in total), are used to study the roles of different model components and different action processes in land-atmosphere coupling. Experiments show that, for the six model configurations, the choice of AGCMs is the main reason for the substantially different precipitation variability, predictability, and land-atmosphere coupling strength among the configurations. The impact of different LSSs is secondary. Intraseasonal precipitation variability, which is mainly a property of the AGCM, can impact land-atmosphere coupling both directly in the atmosphere and indirectly through soil moisture response to precipitation. These results lead to a common conceptual decomposition of the land-atmosphere coupling strength and increases the understanding on large-scale land-atmosphere coupling. Citation: Wei, J., and P. A. Dirmeyer (2010), Toward understanding the large-scale land-atmosphere coupling in the models: Roles of different processes, Geophys. Res. Lett., 37, L19707, doi:10.1029/2010GL044769.",2010,10.1029/2010GL044769,no
Analytical models approximating individual processes: A validation method,"Upscaling population models from fine to coarse resolutions, in space, time and/or level of description, allows the derivation of fast and tractable models based on a thorough knowledge of individual processes. The validity of such approximations is generally tested only on a limited range of parameter sets. A more general validation test, over a range of parameters, is proposed; this would estimate the error induced by the approximation, using the original model's stochastic variability as a reference. This method is illustrated by three examples taken from the field of epidemics transmitted by vectors that bite in a temporally cyclical pattern, that illustrate the use of the method: to estimate if an approximation over- or under-fits the original model; to invalidate an approximation; to rank possible approximations for their qualities. As a result, the application of the validation method to this field emphasizes the need to account for the vectors' biology in epidemic prediction models and to validate these against finer scale models. (C) 2010 Elsevier Inc. All rights reserved.",2010,10.1016/j.mbs.2010.08.014,no
Validation of VRRM Process Model,Value Based Requirements' Risk Management (VRRM) Process Model is first of its kind that provides a risk management process for requirements based upon the concept of value. VRRM is never been implemented in commercial environment. This case study aims to validate the VRRM Process Model by implementing it on two separate commercial projects. The first project was executed by a CMMI Level-2 company and other by the company having no standard practices. The implementation of VRRM on two different projects is done in order to validate its compliance with the CMMI and highlight the differences in its execution for both projects. The case study validates the claims possessed by the VRRM Process Model and highlights some problems faced by practitioners during its implementation. Its practicalities are presented in discrete manners to help generalize its use to treat the software requirements in value manners during the risk management process and outcomes are documented to provide a feedback that may be used for making necessary improvements in the process model.,2010,,no
Mathematical Model of the Occurrence of Human Error in Manufacturing Processes,"Various types of phenomena contribute to the variability of process results. Their common feature is randomness. Some of them can be described by continuous probability distributions, for example, the performance of machines or the properties of processed material. There are also discretely distributed contributions, such as human errors or machine failures. Six sigma methodology encompasses both continuous and discrete phenomena by expressing measures of variability by the so-called 'sigma measure'. However, this methodology cannot be used directly to assess the individual impact of a specific class of factors, such as human errors in a continuously distributed production process. This paper describes the development of a probabilistic model of human error. The model makes use of classical reliability concepts, such as a failure rate function, to represent substantial phenomena of various types (continuous and discrete) that play a significant role in the creation of errors in human work. The model includes a mechanism that is inherently associated with human work (i.e. the 'bathtub curve' that represents the processes of learning and fatiguing) and mechanisms introduced by the work environment (accumulation of tasks). The hypothesis is formulated that, in industrial processes, special causes of errors are closely related to the assignment of inadequate amounts of time for properly performing the operations. Graphs of error rate functions enable intuitive graphical interpretation of the causes of problems, and they can be used to support some considerations regarding the organization and measurement of workflow during a work shift. Thus, an intuitive graph can be useful for figuring out the potential impact on the risk of errors that will result from certain system events. Such graphs can be applied in a general capability study of a process to assess the variability measures associated with the individual impacts of particular classes of factors, for example, the sigma measure used in the six sigma methodology. It can be used to identify mechanisms of potential failures associated with human error in risk analysis, such as FMEA (Failure Mode and Effect Analysis). Copyright (C) 2010 John Wiley & Sons, Ltd.",2010,10.1002/qre.1162,no
Evaluation of a process-based ecosystem model for long-term biomass and stand development of Eucalyptus globulus plantations,"Versatile process-oriented ecosystem models are discussed as promising tools for the analyses of ecosystem services beyond wood yield, such as catchment water yield, sequestration of carbon and greenhouse gas balances. However, long-term yield simulation is often regarded as a weakness of such versatile models. In this context, we present a multiple response evaluation of the modular, process-based forest growth model MoBiLE-PDT based on mensurational data from 38 permanent sample plots in commercial Eucalyptus globulus plantations in Australia followed from establishment to 8 years of stand age. MoBiLE-PDT is based on the PnET-N-DNDC model and considers nitrogen availability and drought stress dynamically in dependence on tree and stand properties as well as on climate and deposition. New tree dimensions are calculated directly from carbon allocated to sapwood and mortality is derived from stand density. Towards the end of the rotation, model efficiency E was 0.58 for stand volume (m(3) ha(-1)) and 0.54 for aboveground biomass (t C ha(-1)). In a comparison with similar forest growth models evaluated against the same data only one had a better model efficiency, whereas MoBiLE-PDT was the most versatile model for the analyses of ecosystem services. Due to its modular structure, further model extensions for more ecological applications are easily possible.",2010,10.1007/s10342-009-0343-x,no
A Typology for Modeling Processes in Clinical Guidelines and Protocols,"We analyzed the graphical representations that are used by various guideline-modeling methods to express process information embodied in clinical guidelines and protocols. From this analysis, we distilled four modeling formalisms and the processes they typically model: (1) flowcharts for capturing problem-solving processes, (2) disease state maps that link decision points in managing patient problems over time, (3) plans that specify sequences of activities that contribute toward a goal, (4) workflow specifications that model care processes in an organization. We characterized the four approaches and showed that each captures some aspect of what a guideline may specify. We believe that a general guideline-modeling system must provide explicit representation for each type of process.",2010,,no
Further comments on the representation problem for stationary processes,"We comment on some points about the coding of stochastic processes by sequences of independent random variables. The most interesting question has to do with the standardness property of the filtration generated by the process, in the framework of Vershik's theory of filtrations. Non-standardness indicates the presence of long memory in a purely probabilistic sense. We aim to provide a short, non-technical presentation of Vershik's theory of filtrations. (C) 2009 Elsevier B.V. All rights reserved.",2010,10.1016/j.spl.2009.12.015,no
"Prediction of deep reservoir quality using early diagenetic process models in the Jurassic Norphlet Formation, Gulf of Mexico","We have developed process-based models for early grain coats and their impact on deep reservoir quality in the Jurassic eolian Norphlet Formation, Alabama, with implications for exploration and development in other conventional and tight-gas continental reservoirs. The Norphlet, a major gas reservoir to depths of 21,800 ft (6645 m) and temperatures of 419 degrees F (215 degrees C), displays contrasting intervals of high and low reservoir quality within compositionally similar cross-bedded eolian sands. Study results show that grain coats formed soon after deposition are responsible for differences in deep Norphlet porosity of up to 20% and permeability up to 200 md. Three types of grain coats were identified in Norphlet dune sands, each formed in a different part of a shallow groundwater system, and each with distinctive impact on deep reservoir quality. Diagenetic chlorite coats, formed where dunes subsided into shallow hypersaline groundwater, preserve good deep porosity (to 20%) and permeability (to 200 md). Continuous tangential illitic coats, formed in the vadose zone of stabilized dunes exposed to periodic fresh-water influx, preserve good deep porosity (to 15%) associated with poor permeability (<1 md) due to linked formation of later high-temperature diagenetic illite. Discontinuous grain coats, formed in active dunes where grains were abraded by eolian transport, are associated at depth with tight zones of pervasive quartz cement, low porosity (<8%), and low permeability (<1 md). These concepts plus data from 60 wells were used to derive bay-wide predictive tight and porous-zone isopachs that can be used for well placement, geologic models, and field development.",2010,10.1306/04211009152,no
The Process of Probability Problem Solving: Use of External Visual Representations,"We investigate the role of external inscriptions, particularly those of a spatial or visual nature, in the solution of probability word problems. We define a taxonomy of external visual representations used in probability problem solving that includes pictures, spatial reorganization of the given information, outcome listings, contingency tables, Venn diagrams, trees, and novel graphical representations. We also propose a process model for probability problem solving (PPS) and use it as a framework to better understand how and why external visual representations are used. In a study of 34 novice probability problem solvers, participants worked to solve six probability word problems covering six probability subtopics. Both written and verbal structured interview protocols were analyzed to investigate when and how external visual representations are spontaneously used by problem solvers. Analyses of the coded transcripts showed that participants' probability problem-solving efforts move through the stages of PPS in a sequential but not always linear manner, sometimes exhibiting iterated attempts to represent the problem mathematically and to find a solution strategy. Results showed that use of specific external visual representations was associated with specific probability topics, and that certain choices of representation are associated with higher rates of solution success. These findings suggest that an external visual representation can facilitate probability problem solving, but only when an appropriate representation is chosen. Finally, we present evidence to show that external visual representations are usually created and first used during the stages of representing the problem mathematically and finding a solution strategy. However, pictures are often created during the initial stage of problem text understanding, and tables are sometimes created during computation of the solution.",2010,10.1080/10986061003654240,no
Goal Selection in Argumentation Processes A Formal Model of Abduction in Argument Evaluation Structures,"When argumentation is conceived as a kind of process, typically a dialogue, for reasoning rationally with limited resources under conditions of incomplete and inconsistent information, arguers need heuristics for controlling the search for arguments to put foward, so as to move from stage to stage in the process in an efficient, goal-directed way. For this purpose, we have developed a formal model of abduction in argument evalution structures. An argument evaluation structure consists of the arguments of a stage, assumptions about audience and an assignment of proof standards to issues. A derivability relation is defined over argument evaluation structures for the literals 'in' a stage. Literals which are not derivable in a stage are 'out'. Abduction is defined as a relation between an argument evaluation structure and sets of literals, called 'positions', which, when the assumptions are revised to include the literals of the position, would make a goal literal in or out, depending of the standpoint of the agent. Soundness, minimiality, consistency and completeness properties of the abduction relation are proven. A heuristic cost function estimating how difficult it is to find or construct arguments pro a literal in the domain can be used to order positions and literals within positions. We compare our work to abduction in propositional logic, in particular the Assumption-Based Truth Maintenance System (ATMS).",2010,10.3233/978-1-60750-619-5-51,no
"Exploring the population dynamics of winter skate (Leucoraja ocellata) in the Georges Bank region using a statistical catch-at-age model incorporating length, migration, and recruitment process errors","Winter skate (Leucoraja ocellata) of all length classes increased dramatically in abundance on Georges Bank in the 1980s following the decline of many groundfish species. We present a full population model of winter skate to better understand the population dynamics of the species and elucidate the mechanisms underlying their increase in abundance in the 1980s. Specifically, we developed four statistical catch-at-age models incorporating length-frequency data with the following model structures: (i) observation error only (base model R1); (ii) observation and recruitment process errors (model R2); (iii) adult migration modeled as a random walk in adult mortality (model R3); and (iv) observation and recruitment process errors and adult migration (model R4). Akaike's information criterion values indicated that models R3 and R4, which both included adult migration, were the most parsimonious models. This finding strongly suggests that the winter skate population increase on Georges Bank in the 1980s was not solely a result of increases in recruitment but likely involved adult migration (i. e., it is an open population). Finally, recent predicted fishing mortalities exceeded FMSY for all models.",2010,10.1139/F10-008,no
An Amorphous Model for Morphological Processing in Visual Comprehension Based on Naive Discriminative Learning,"A 2-layer symbolic network model based on the equilibrium equations of the Rescorla-Wagner model (Danks, 2003) is proposed. The study first presents 2 experiments in Serbian, which reveal for sentential reading the inflectional paradigmatic effects previously observed by Milin, Filipovic Durdevic, and Moscoso del Prado Martin (2009) for unprimed lexical decision. The empirical results are successfully modeled without having to assume separate representations for inflections or data structures such as inflectional paradigms. In the next step, the same naive discriminative learning approach is pitted against a wide range of effects documented in the morphological processing literature. Frequency effects for complex words as well as for phrases (Arnon & Snider, 2010) emerge in the model without the presence of whole-word or whole-phrase representations. Family size effects (Moscoso del Prado Martin, Bertram, Haikio, Schreuder, & Baayen, 2004; Schreuder & Baayen, 1997) emerge in the simulations across simple words, derived words, and compounds, without derived words or compounds being represented as such. It is shown that for pseudo-derived words no special morpho-orthographic segmentation mechanism, as posited by Rastle, Davis, and New (2004), is required. The model also replicates the finding of Plag and Baayen (2009) that, on average, words with more productive affixes elicit longer response latencies; at the same time, it predicts that productive affixes afford faster response latencies for new words. English phrasal paradigmatic effects modulating isolated word reading are reported and modeled, showing that the paradigmatic effects characterizing Serbian case inflection have crosslinguistic scope.",2011,10.1037/a0023851,no
The Application of Life Cycle Assessment in a Typical Litopenaeus Vannamei Comprehensive Utilization Process Model,"A case study of typical Litopenaeus vannamei comprehensive utilization system in a factory has been performed. Life cycle assessment (LCA) provides a quantitative basis for assessment environmental and economic impact of a food production system. According to LCA methodology, the system investigated was divided into four subsystems: raw material acquisition, production, packaging, and transportation. The results showed that the comprehensive utilization patterns could create greater added value, and the enviromnental impact could be less than the previous production system. The most impact category is greenhouse effect (1.89x105). Thus, the utilization model of I,. vannamei production belongs to clean production mode. To processing 1,000 kg L. vannamei, the total cost is (sic) 38,966. The total products value of frozen headless shrimp, seasoning matmial and feed additive is (sic) 53,355. (C) 2011 Published by Elsevier Ltd. Selection and/or peer-review under responsibility of Conference ESIAT2011 Organization Committee.",2011,10.1016/j.proenv.2011.09.251,no
Kinetic Modeling of Semi-Interpenetrating Polymer Network (SIPN) Process - A Comprehensive Study on the Case of Polyethylene/Polystyrene Semi-I IPN,"A comprehensive kinetic model is developed for a semi-interpenetrating polymer network (SIPN) process, which involves simultaneous crosslinking, grafting, and degradation. Computational expense has been reduced considerably through a new component decomposition strategy, where a continuous variable approximation and a fixed pivot technique are applied for modeling each component. The inter-polymer formulation is then reconstructed by a statistical approach. Based on the kinetic parameters obtained from the literature and a series of experiments, the model provides consistent agreement for gel fraction, joint molecular weight distribution (MWD) and polymer composition predicted in the studied cases, showing promising capability for the SIPN industrial application as well as for other polymer composite systems.",2011,10.1002/mats.201000058,no
Coupling of Important Physical Processes in the Planetary Boundary Layer between Meteorological and Chemistry Models for Regional to Continental Scale Air Quality Forecasting: An Overview,"A consensus among many Air Quality (AQ) modelers is that planetary boundary layer processes are the most influential processes for surface concentrations of air pollutants. Due to the many uncertainties intrinsically embedded in the parameterization of these processes, parameter optimization is often employed to determine an optimal set or range of values of the sensitive parameters. In this review study, we focus on the two of the most important physical processes: turbulent mixing and dry deposition. An emphasis was put on surveying AQ models that have been proven to resolve meso-scale features and cover a large geographical area, such as large regional, continental, or trans-continental boundary extents. Five AQ models were selected. Four of the models were run in real-time operational forecasting settings for continental scale AQ. The models use various forms of level 2.5 closure algorithms to calculate turbulent mixing. Tuning and parameter optimization has been used to tailor these algorithms to better suit their AQ models which are typically comprised of a coupled chemistry and meteorology model. Longer forecasts and long lead-times are inevitably under increasing demand for these models. Land Surface Models that have the capability for soil moisture and temperature data assimilation will have an advantage to constrain the key variables that govern the partitioning of surface sensible and latent heat fluxes and thus attain the potential to perform better in longer forecasts than those models that do not have this capability. Dry deposition velocity is a very significant model parameter that governs a major surface exchange activity. An exploratory study has been conducted to see the upper bound of roughness length in the similarity equation for aerodynamic resistance.",2011,10.3390/atmos2030464,no
Development of analysis model for geometric error in turning processes,"A finite element model was established for analyzing the geometric errors in turning operations and a two-step analyzing process was proposed. In the first analyzing step, the cutting force and the cutting heat for the cutting conditions were obtained using the AdvantEdge. Also, the deformation of a workpiece was estimated in the second step using the ANSYS. The deformation was analyzed for a 150 mm-long workpiece at three different measuring points, such as 10, 70 and 130 mm from a reference point, and the amounts of the deformation were compared through experiments. In the results of the comparison and analysis, the values obtained from these comparison and analysis represent similar tendencies. Also, it is verified that their geometric errors increase with the increase in temperature. In addition, regarding the factors that affect the deformation of a workpiece, it can be seen that the geometric error in the lathe is about 15%, the error caused by the cutting force is about 10%, and the deformation caused by the heat is about 75%.",2011,10.1007/s11771-011-0752-0,no
Fundamental Mathematical Model for AOD Process. Part II: Model validation,"A fundamental mathematical model for AOD process has been developed and proposed in ""Fundamental Mathematical Model for AOD Process. Part I: Derivation of the Model'' [1]. Validation of the model with process data, measured from full scale AOD process, is presented in this paper. A broad selection of input data for the model was exported from various types of full scale industrial AOD heats. In this study 6 different types of heats were studied and simulated. Process data was measured from two AOD converters (95 t, 150 t). Validation of the model was then done by comparing simulated and measured values for carbon and chromium content, carbon release rate, melt composition, slag composition and bath temperature during final stages of carbon removal. The validation results showed that the model was in good agreement with the measured process data, and same model parameters were valid in all of the simulated heats.",2011,10.1002/srin.201000266,no
Migrating an incident reporting system to a CCPS process safety metrics model,"A major chemical company established a formal incident investigation and reporting system several years ago. The original system focused heavily on worker-related injuries, illnesses, and near-misses and was used primarily to track statistics reportable to the Occupational Safety and Health Administration (OSHA). This Occupational Injury and Illness (OII) approach has been recognized to be an ineffective tool for measuring, predicting, and preventing process safety incidents. The Center for Chemical Process Safety (CCPS) recently published guidelines on how to establish safety metrics for the measurement and reduction of process safety incidents. The process safety metrics approach relies upon leading and lagging metrics to improve organization process safety. This paper is a case study of the analysis of one organization's incident database. which represents approximately five years of data from over a dozen facilities. The aim of this investigation was to extract useful process safety metrics from the incident investigation and reporting system, which would be pertinent to the types of process units and process functions at these facilities. This paper will discuss the approach taken to extract process incident information from an OII-based database and present the difficulties of performing an analysis on such a database. This paper provides guidance on how to migrate an existing OII-based reporting system to a program that includes process safety metrics in accordance with industry best practices. (C) 2011 Elsevier Ltd. All rights reserved.",2011,10.1016/j.jlp.2011.06.008,no
Modeling of nonlinear boundary value problems in enzyme-catalyzed reaction diffusion processes,A mathematical model of steady state mono-layer potentiometric biosensor is developed. The model is based on non stationary diffusion equations containing a non linear term related to Michaelis-Menten kinetics of the enzymatic reaction. This paper presents a complex numerical method (He's variational iteration method) to solve the non-linear differential equations that describe the diffusion coupled with a Michaelis-Menten kinetics law. Approximate analytical expressions for substrate concentration and corresponding current response have been derived for all values of saturation parameter alpha and reaction diffusion parameter K using variational iteration method. These results are compared with available limiting case results and are found to be in good agreement. The obtained results are valid for the whole solution domain.,2011,10.1007/s10910-010-9752-9,no
Research on Five Sections Measuring Model in Process Pneumatic Measurement System for Inner Hole Honing,"A novel concept to estimate the shape of inner hole in course of honing is presented in this paper. Based on analysis of machining condition, five sections measuring model is established in process pneumatic measurement system for inner hole honing. It used matrix algorithms to simplify judging process about inner hole shape. And it improves the accuracy of machining area in inner hole. The method and principle are elaborated in this article. Program module was designed and written for experiment. And the experimental data show that the measuring model is feasible.",2011,10.4028/www.scientific.net/AMR.317-319.1342,no
A Comprehensive Multi-Scale Modeling of Heterogeneities in Mammalian Cell Culture Processes,A population balance model that describes the inter cellular variations in metabolism with respect to cell cycle apportioning is combined with an unstructured average cell model and an intrinsic culture state model to quantify various intra and inter cellular phenomena in Chinese Hamster Ovary (CHO) cell cultures at laboratory scale. Model validation is performed on three sets of batch cultures of CHO cells in spinner flasks.,2011,,no
Typical Accuracy and Quality Control of a Process for Creating CT-Based Virtual Bone Models,"A pragmatic method for assessing the accuracy and precision of a given processing pipeline required for converting computed tomography (CT) image data of bones into representative three dimensional (3D) models of bone shapes is proposed. The method is based on coprocessing a control object with known geometry which enables the assessment of the quality of resulting 3D models. At three stages of the conversion process, distance measurements were obtained and statistically evaluated. For this study, 31 CT datasets were processed. The final 3D model of the control object contained an average deviation from reference values of -1.07 +/- 0.52 mm standard deviation (SD) for edge distances and -0.647 +/- 0.43 mm SD for parallel side distances of the control object. Coprocessing a reference object enables the assessment of the accuracy and precision of a given processing pipeline for creating CT-based 3D bone models and is suitable for detecting most systematic or human errors when processing a CT-scan. Typical errors have about the same size as the scan resolution.",2011,10.1007/s10278-010-9287-4,no
Data processing path from multimodal 3D measurement to realistic virtual model,"A set of calculation methods has been developed and tested to provide means of creating virtual copies of three dimensional (3D) historical objects with minimal user input. We present a step by step data processing path along with algorithm description required to reconstruct a realistic 3D model of a culturally significant object. The important feature for archiving historical objects is the ability to include both information about its shape and texture, allowing visualization using arbitrary conditions of illumination. Data samples used as input for the processing method chain were collected using an integrated device consisting of shape, multispectral color and simplified BRDF measurements. To confirm the usability of presented methods, it has been tested by example of real life object - statue of an ancient Greek goddess Kybele. Additional visualization methods have also been examined to render a realistic virtual representation satisfying intrinsic surface properties of the investigated specimen.",2011,10.1117/12.872200,no
Quality Control Model for Manufacturing Process based on GQMM,"A structured quality control model was proposed. It was a multilevel and knowledge-based quality control model which took Goal, Question, Metric, Measure and other fundamental elements as the main line, structured principle as guidance, manufacturing process as carrier, and improve quality of manufacturing system as destination. Firstly, a structured analysis principle was introduced. And then a structured quality control model was proposed combining bearing manufacturing process. A formal description of mappings between Goal-Question-Metric-Measure (GQMM) was also discussed. Finally; the architecture of the model was presented.",2011,10.4028/www.scientific.net/AMR.214.612,no
"Theoretical modeling of growth processes, extended defects, and electronic properties of III-nitride semiconductor nanostructures","Ab initio based simulations have been proven in the past to be and still are a valuable and indispensable tool in the field of III-nitride semiconductors. They have been successfully used to explain, describe and guide growth and characterization experiments and to address a large variety of material problems at different length scales. In the present report we review on five selected topics which span different length scales, various method developments, and diverse material properties that have been theoretically addressed within the research group ""Physics of nitride-based, nanostructured, light emitting devices."" [GRAPHICS] (C) 2011 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim",2011,10.1002/pssb.201046511,no
Perceiving pain in others: Validation of a dual processing model,"Accurate perception of another person's painful distress would appear to be accomplished through sensitivity to both automatic (unintentional, reflexive) and controlled (intentional, purposive) behavioural expression. We examined whether observers would construe diverse behavioural cues as falling within these domains, consistent with cognitive neuroscience findings describing activation of both automatic and controlled neuroregulatory processes. Using online survey methodology, 308 research participants rated behavioural cues as ""goal directed vs. non-goal directed,"" ""conscious vs. unconscious,"" ""uncontrolled vs. controlled,"" ""fast vs. slow,"" ""intentional (deliberate) vs. unintentional,"" ""stimulus driven (obligatory) vs. self driven,"" and ""requiring contemplation vs. not requiring contemplation."" The behavioural cues were the 39 items provided by the PROMIS pain behaviour bank [27], constructed to be representative of the diverse possibilities for pain expression. Inter-item correlations among rating scales provided evidence of sufficient internal consistency justifying a single score on an automatic/controlled dimension (excluding the inconsistent fast vs. slow scale). An initial exploratory factor analysis on 151 participant data sets yielded factors consistent with ""controlled"" and ""automatic"" actions, as well as behaviours characterized as ""ambiguous."" A confirmatory factor analysis using the remaining 151 data sets replicated EFA findings, supporting theoretical predictions that observers would distinguish immediate, reflexive, and spontaneous reactions (primarily facial expression and paralinguistic features of speech) from purposeful and controlled expression (verbal behaviour, instrumental behaviour requiring ongoing, integrated responses). There are implicit dispositions to organize cues signaling pain in others into the well-defined categories predicted by dual process theory. (C) 2011 International Association for the Study of Pain. Published by Elsevier B.V. All rights reserved.",2011,10.1016/j.pain.2011.01.025,no
Mental Health Courts and Their Selection Processes: Modeling Variation for Consistency,"Admission into mental health courts is based on a complicated and often variable decision-making process that involves multiple parties representing different expertise and interests. To the extent that eligibility criteria of mental health courts are more suggestive than deterministic, selection bias can be expected. Very little research has focused on the selection processes underpinning problem-solving courts even though such processes may dominate the performance of these interventions. This article describes a qualitative study designed to deconstruct the selection and admission processes of mental health courts. In this article, we describe a multi-stage, complex process for screening and admitting clients into mental health courts. The selection filtering model that is described has three eligibility screening stages: initial, assessment, and evaluation. The results of this study suggest that clients selected by mental health courts are shaped by the formal and informal selection criteria, as well as by the local treatment system.",2011,10.1007/s10979-010-9250-4,no
Quality Prediction Model Based on PCA-BP Neural Network for Tobacco Leaves Redrying Process,"Aiming to the problem that is very difficult to establish the mechanism model of quality for the process of tobacco leaves redrying, this paper proposes a quality prediction model based on principal component analysis (PCA) and improved back propagation (BP)neural network for tobacco leaves redrying process. Firstly, 12 input variables are confirmed by analyzing the factors on quality of tobacco leaves redrying process. Second, the methods of PCA is used to eliminate the correlation of original input layer data, in which 12 input variables are transformed into 6 uncorrelated indicators. Then, the quality prediction model based on improved BP neural network is established. Finally, a simulation experiment is conducted and the average prediction error is as low as 1.03%, the absolute error for forecasting is fluctuated in the range of 0.16% - 2.49%. The result indicates that the model is simpler and has higher stability for prediction, which can completely meet the actual requirements of the tobacco leaves redrying process.",2011,10.4028/www.scientific.net/AMR.201-203.1627,no
Comparison of Lagrangian Process Analysis tools for Eulerian air quality models,"Air quality models (AQM) are used to understand the complex relationships between sources of air pollutants and ambient concentrations. Two new AQM diagnostic tools, the Lagrangian Process Analysis (LPA) tool and the Python-based Process Analysis (pyPA), have recently been created that allow users to track a plume within the AQM, and then calculate the chemical and physical process rates that occur within it. These two new process analysis tools perform their functions differently. The LPA in-model algorithm operates at the computational timestep of the AQM, and pyPA is a post-processor tool dependent on the temporal resolution of the AQM output, typically 1 h. This work compares process rates calculated by these tools, using as a case study the simulation of a rapidly evolving plume that resulted from an industrial hydrocarbon release. Releases from industrial sources are of regulatory significance in Houston and their accurate simulation of great importance. Results show that the largest differences in the outputs of the tools occur early in the life of the plume when it is rapidly expanding. During this time, the plume encounters NO(x) sources that significantly impact chemical and physical process rates that are not seen in the pyPA post-processing of hourly AQM output. Published by Elsevier Ltd.",2011,10.1016/j.atmosenv.2011.06.005,no
Modelling the dynamics of the tilt-casting process and the effect of the mould design on the casting quality,"All titanium alloys are highly reactive in the molten condition and so are usually melted in a water-cooled copper crucible to avoid contamination using processes such as Induction Skull Melting (ISM). These provide only limited superheat which, coupled with the surface turbulence inherent in most conventional mould filling processes, results in entrainment defects such as bubbles in the castings. To overcome these problems, a novel tilt-casting process has been developed in which the mould is attached directly to the ISM crucible holding the melt and the two are then rotated together to achieve a tranquil transfer of the metal into the mould. From the modelling point of view, this process involves complex three-phase flow, heat transfer and solidification. In this paper, the development of a numerical model of the tilt-casting process is presented featuring several novel algorithm developments introduced into a general CFD package (PHYSICA) to model the complex dynamic interaction of the liquid metal and melting atmosphere. These developments relate to the front tracking and heat transfer representations and to a casting-specific adaptation of the turbulence model to account for an advancing solid front. Calculations have been performed for a 0.4 m long turbine blade cast in a titanium aluminide alloy using different mould designs. It is shown that the feeder/basin configuration has a crucial influence on the casting quality. The computational results are validated against actual castings and are used to support an experimental programme. Although fluid flow and heat transfer are inseparable in a casting, the emphasis in this paper will be on the fluid dynamics of mould filling and its influence on cast quality rather than heat transfer and solidification which has been reported elsewhere. (C) 2010 Elsevier Ltd. All rights reserved.",2011,10.1016/j.compfluid.2010.11.010,no
Development of an emission processing system for the Pearl River Delta Regional air quality modeling using the SMOKE model: Methodology and evaluation,"An emission pre-processing tool is generally needed to transform a bulk annual emission inventory into gridded, temporal (monthly, weekly, and hourly), and speciated emissions in order to use complex air quality models such as the Community Multi-scale Air Quality (CMAQ) model to assess control strategies, forecast air quality, and investigate pollution formation and transport processes. To support the regional air quality modeling in the Pearl River Delta (PRO), we developed an emission pre-processing system, Sparse Matrix Operator Kernel Emissions-PRO (SMOKE-PRO), based on the U.S. Environmental Protection Agency (USEPA)/SMOKE model. This paper introduces the methods and procedures for adapting the SMOKE model to the PRD regional bulk emissions. These include the compilation of the PRO local source-class clarification codes, the incorporation of the PRO local emission inventory, and the updating of spatial, temporal, and chemical speciation information. The SMOKE-PRO system was evaluated, and a case study on ozone simulation was conducted to demonstrate the applicability of the SMOKE-PRO system. Results show that the model can properly simulate temporal variations, spatial patterns, and peak values of 03 concentrations in the PRO region, suggesting the SMOKE-PRO system was successfully localized and can be used to provide model-ready emissions for regional air quality modeling in the PRO region. This work can be extended for adapting the SMOKE model to process emissions for modeling use in other regions of China. (C) 2011 Elsevier Ltd. All rights reserved.",2011,10.1016/j.atmosenv.2011.06.037,no
Experimental Measurements and Mass Transfer/Reaction Modeling for an Industrial NOx Absorption Process,"An industrial reactive separation process for NOx absorption into water and nitric acid in a world-kale mononitrobenzene (MNB) plant in Redcar, U.K., was investigated. Gas- and liquid-phase concentrations were measured in situ, while operating conditions such as temperature, pressure, and absorbent flow were varied. Furthermore, a comprehensive mass transfer/reaction model was developed in the process simulator Aspen Plus to simulate the absorption process, with the field process data used for validation. The model accurately predicted NOx removal from the gas (i.e., typically within 3.5% of the plant data). Other key findings are that the model successfully predicted changes in performance when key parameters such as temperature and pressure were varied. Furthermore, addition of a bleaching section was investigated, resulting in a significant improvement in NOx removal. Currently, the completed model is used to simulate existing and proposed NOx absorption systems and to develop new and innovative designs for NOx capture.",2011,10.1021/ie100436p,no
Understanding Interaction Effects of Climate Change and Fire Management on Bird Distributions through Combined Process and Habitat Models,"Avian conservation efforts must account for changes in vegetation composition and structure associated with climate change. We modeled vegetation change and the probability of occurrence of birds to project changes in winter bird distributions associated with climate change and fire management in the northern Chihuahuan Desert (southwestern U.S.A.). We simulated vegetation change in a process-based model (Landscape and Fire Simulator) in which anticipated climate change was associated with doubling of current atmospheric carbon dioxide over the next 50 years. We estimated the relative probability of bird occurrence on the basis of statistical models derived from field observations of birds and data on vegetation type, topography, and roads. We selected 3 focal species, Scaled Quail (Callipepla squamata), Loggerhead Shrike (Lanius ludovicianus), and Rock Wren (Salpinctes obsoletus), that had a range of probabilities of occurrence for our study area. Our simulations projected increases in relative probability of bird occurrence in shrubland and decreases in grassland and Yucca spp. and ocotillo (Fouquieria splendens) vegetation. Generally, the relative probability of occurrence of all 3 species was highest in shrubland because leaf-area index values were lower in shrubland. This high probability of occurrence likely is related to the species' use of open vegetation for foraging. Fire suppression had little effect on projected vegetation composition because as climate changed there was less fuel and burned area. Our results show that if future water limits on plant type are considered, models that incorporate spatial data may suggest how and where different species of birds may respond to vegetation changes.",2011,10.1111/j.1523-1739.2011.01684.x,no
The Comprehensive Process Model of Engagement,"Background: Engagement refers to the act of being occupied or involved with an external stimulus. In dementia, engagement is the antithesis of apathy. Objective: The Comprehensive Process Model of Engagement was examined, in which environmental, personal, and stimulus characteristics impact the level of engagement. Methods: Participants were 193 residents of 7 Maryland nursing with a diagnosis of dementia. Stimulus engagement was assessed via the Observational Measure of Engagement, measuring duration, attention, and attitude to the stimulus. Twenty-five stimuli were presented, which were categorized as live human social stimuli, simulated social stimuli, inanimate social stimuli, a reading stimulus, manipulative stimuli, a music stimulus, task and work-related stimuli, and two different self-identity stimuli. Results: All stimuli elicited significantly greater engagement in comparison to the control stimulus. In the multivariate model, music significantly increased engagement duration, whereas all other stimuli significantly increased duration, attention, and attitude. Significant environmental variables in the multivariate model that increased engagement were: use of the long introduction with modeling (relative to minimal introduction), any level of sound (especially moderate sound), and the presence of between 2 and 24 people in the room. Significant personal attributes included Mini-Mental State Examination scores, activities of daily living performance and clarity of speech, which were positively associated with higher engagement scores. Conclusions: Results are consistent with the Comprehensive Process Model of Engagement. Personal attributes, environmental factors, and stimulus characteristics all contribute to the level and nature of engagement, with a secondary finding being that exposure to any stimulus elicits engagement in persons with dementia. (Am J Geriatr Psychiatry 2011; 19: 859-870)",2011,10.1097/JGP.0b013e318202bf5b,no
Patient perception of nursing service quality; an applied model of Donabedian's structure-process-outcome approach theory,"Background: Nursing is a labour-intensive field, and an extensive amount of latent information exists to aid in evaluating the quality of nursing service, with patients' experiences, the primary focus of such evaluations. To effect further improvement in nursing as well as medical care, Donabedian's structure-process-outcome approach has been applied. Aims: To classify and confirm patients' specific experiences with regard to nursing service based on Donabedian's structure-process-outcomes model for improving the quality of nursing care. Methods: Items were compiled from existing scales and assigned to structure, process or outcomes in Donabedian's model through discussion among expert nurses and pilot data collection. With regard to comfort, surroundings were classified as structure (e. g. accessibility to nurses, disturbance); with regard to patient-practitioner interaction, patient participation was classified as a process (e. g. expertise and skill, patient decision-making); and with regard to changes in patients, satisfaction was classified as an outcome (e. g. information support, overall satisfaction). Patient inquiry was carried out using the finalized questionnaire at general wards in Japanese hospitals in 2005-2006. Reliability and validity were tested using psychometric methods. Results: Data from 1,810 patients (mean age: 59.7 years; mean length of stay: 23.7 days) were analysed. Internal consistency reliability was supported (alpha = 0.69-0.96), with factor analysis items of structure aggregated to one factor and overall satisfaction under outcome aggregated to one. The remaining items of outcome and process were distributed together in two factors. Inter-scale correlation (r = 0.442-0.807) supported the construct validity of each structure-process-outcome approach. All structure items were represented as negative-worded examples, as they dealt with basic conditions under Japanese universal health care system, and were regarded as representative related to concepts of dissatisfaction and no dissatisfaction. Conclusion: Patients' experiences with nursing service were confirmed using Donabedian's approach and can therefore be applied to improve quality of nursing practice by practitioners, managers and policy makers.",2011,10.1111/j.1471-6712.2010.00836.x,no
Computerized prediction of intensive care unit discharge after cardiac surgery: development and validation of a Gaussian processes model,"Background: The intensive care unit (ICU) length of stay (LOS) of patients undergoing cardiac surgery may vary considerably, and is often difficult to predict within the first hours after admission. The early clinical evolution of a cardiac surgery patient might be predictive for his LOS. The purpose of the present study was to develop a predictive model for ICU discharge after non-emergency cardiac surgery, by analyzing the first 4 hours of data in the computerized medical record of these patients with Gaussian processes (GP), a machine learning technique. Methods: Non-interventional study. Predictive modeling, separate development (n = 461) and validation (n = 499) cohort. GP models were developed to predict the probability of ICU discharge the day after surgery (classification task), and to predict the day of ICU discharge as a discrete variable (regression task). GP predictions were compared with predictions by EuroSCORE, nurses and physicians. The classification task was evaluated using aROC for discrimination, and Brier Score, Brier Score Scaled, and Hosmer-Lemeshow test for calibration. The regression task was evaluated by comparing median actual and predicted discharge, loss penalty function (LPF) ((actual-predicted)/actual) and calculating root mean squared relative errors (RMSRE). Results: Median (P25-P75) ICU length of stay was 3 (2-5) days. For classification, the GP model showed an aROC of 0.758 which was significantly higher than the predictions by nurses, but not better than EuroSCORE and physicians. The GP had the best calibration, with a Brier Score of 0.179 and Hosmer-Lemeshow p-value of 0.382. For regression, GP had the highest proportion of patients with a correctly predicted day of discharge (40%), which was significantly better than the EuroSCORE (p < 0.001) and nurses (p = 0.044) but equivalent to physicians. GP had the lowest RMSRE (0.408) of all predictive models. Conclusions: A GP model that uses PDMS data of the first 4 hours after admission in the ICU of scheduled adult cardiac surgery patients was able to predict discharge from the ICU as a classification as well as a regression task. The GP model demonstrated a significantly better discriminative power than the EuroSCORE and the ICU nurses, and at least as good as predictions done by ICU physicians. The GP model was the only well calibrated model.",2011,10.1186/1472-6947-11-64,no
A proposed adaptation of the European Foundation for Quality Management Excellence Model to physical activity programmes for the elderly - development of a quality self-assessment tool using a modified Delphi process,"Background: There has been a growing concern in designing physical activity (PA) programmes for elderly people, since evidence suggests that such health promotion interventions may reduce the deleterious effects of the ageing process. Complete programme evaluations are a necessary prerequisite to continuous quality improvements. Being able to refine, adapt and create tools that are suited to the realities and contexts of PA programmes for the elderly in order to support its continuous improvement is, therefore, crucial. Thus, the aim of this study was to develop a self-assessment tool for PA programmes for the elderly. Methods: A 3-round Delphi process was conducted via the Internet with 43 national experts in PA for the elderly, management and delivery of PA programmes for the elderly, sports management, quality management and gerontology, asking experts to identify the propositions that they considered relevant for inclusion in the self-assessment tool. Experts reviewed a list of proposed statements, based on the criteria and sub-criteria from the European Foundation for Quality Management Excellence Model (EFQM) and PA guidelines for older adults and rated each proposition from 1 to 8 (disagree to agree) and modified and/or added propositions. Propositions receiving either bottom or top scores of greater than 70% were considered to have achieved consensus to drop or retain, respectively. Results: In round 1, of the 196 originally-proposed statements (best practice principles), the experts modified 41, added 1 and achieved consensus on 93. In round 2, a total of 104 propositions were presented, of which experts modified 39 and achieved consensus on 53. In the last round, of 51 proposed statements, the experts achieved consensus on 19. After 3 rounds of rating, experts had not achieved consensus on 32 propositions. The resulting tool consisted of 165 statements that assess nine management areas involved in the development of PA programmes for the elderly. Conclusion: Based on experts' opinions, a self-assessment tool was found in order to access quality of PA programmes for the elderly. Information obtained with evaluations would be useful to organizations seeking to improve their services, customer satisfaction and, consequently, adherence to PA programmes, targeting the ageing population.",2011,10.1186/1479-5868-8-104,no
A New Integer Programming Model about Counterfeit Coin Problem Based on Information Processing Method and Its General Solution,"Based on the Counterfeit Coin Problem that is often met in ordinary life, this paper looks weighting coins as an information processing process, introduce some interrelated information processing knowledge in Information Theory and use it to establish a new Integer Programming model based on information processing method. Its detailed logic inference process, general solution is discussed, its universal laws and specific academic explainer are also presented, so this problem is solved, these can expand the application scope of information processing method.",2011,,no
Coordination Analysis of Human Movements With Body Sensor Networks: A Signal Processing Model to Evaluate Baseball Swings,"Becoming proficient in a sport requires significant investment in training. Traditional training approaches such as training with a partner or an expert, and training with the help of videotaping can significantly increase progress. These techniques, however, do not provide fine grain detail about movements of the player, are time consuming, or are limited to specific locations. In contrast, wearable sensor devices can improve training due to the high level of mobility, ubiquity and intelligent feedback offered. In this paper, we present a wearable platform that provides baseball players with corrective feedback based on multidimensional physiological data collected from a body sensor network. We employ a swing model that specifies actions that must be performed properly, in the correct order, and with precise timing between limbs. The system evaluates a baseball swing using motion transcripts. Transcripts simplify interpretation of complex movements and can be used to reduce the size of data that need to be transmitted across the network. Using transcripts, we measure coordination among limb segments and joints of the body. The starting times of key events are found in the transcripts, and the coordination between these times is analyzed. The swing quality is then assessed by comparing the intersegment coordination of a test swing to that of a template swing.",2011,10.1109/JSEN.2010.2048205,no
EVALUATION OF NERIDRONATE ON THE OSSEOINTEGRATION PROCESS OF ENDEOUS TITANIUM IMPLANTS IN ANIMAL MODELS,Bisphosphonates are compounds that inhibit bone reabsorption mediated by osteoclasts. The use of bisphosphonates in oral implantology is still in the experimental stage. The aim of this study is to evaluate the efficacy of an aminobisphosphonate to increase the ability of the drug to act on the implant and bone surfaces in the development of the osseointegration in sheep. Forty SLA titanium implants were used on sheep iliac crests. Neridronate added to connective gel (test 1) or to physiological solution (test 2) was used in order to increase the bone and implant adhesiveness. Physiological solution (control 1) or connective gel (control 2) alone was given to the control groups. A topical administration of Neridronate was made on the implant surface and in the implant site. Four Bergamasca sheep were used and were sacrificed by intravenous injection of 10 cc Tanax after 8 weeks from implantation. Histologic and histomorphometric analyses were carried out. The results did not show significant differences between the test group and control group. Our data are different from other similar studies obtaining statistically significant differences. These differences could depend on the procedure of application of the drug on the implant. This study demonstrates the poor efficacy of neridronate applied topically to the implant and implant site during surgery. Further studies using different fixation techniques of the drug may be necessary to confirm the present data.,2011,,no
Using the Extended Parallel Process Model to create and evaluate the effectiveness of brochures to reduce the risk for noise-induced hearing loss in college students,"Brochures containing messages developed according to the Extended Parallel Process Model were deployed to increase intentions to use hearing protection for college students. These brochures were presented to one-half of a college student sample, after which a questionnaire was administered to assess perceptions of threat, efficacy, and behavioral intentions. The other half of the sample completed the questionnaire and then received brochures. Results indicated that people receiving the brochure before the questionnaire reported greater perceptions of hearing loss threat and efficacy to use ear plugs when in loud environments, however, intentions to use ear plugs were unchanged. Distribution of the brochure also resulted in greater perceptions of hearing loss threat and efficacy to use over-the-ear headphones when using devices such as MP3 players. In this case, however, intentions to use over-the-ear headphones increased. Results are discussed in terms of future research and practical applications.",2011,10.4103/1463-1741.82958,no
Delta Analysis: A Hybrid Quantitative Approach for Measuring Discrepancies between Business Process Models,"Business process management (BPM) continues to play a significant role in today's highly globalized world. In order to detect and prevent the gap between reference process model and the actual operation, process mining techniques discover operational model on the basis of the process logs. An important issue at BPM is to measure the similarity between the reference process model and discovered process model so that it can be possible to pinpoint where process participants deviate from the intended process description. In this paper, a hybrid quantitative approach is presented to measure the similarity between the process models. The proposed similarity metric is based on a hybrid process mining technique that makes use of genetic algorithms. The proposed approach itself is also a hybrid model that considers process activity dependencies and process structure.",2011,,no
A Quality-Oriented Business Process Meta-Model,"Business process modeling is an important part of information systems design. Business process modeling techniques provide a standard way of presentation and communication of business processes. One important problem in this context is to assess the quality of business processes as well as their ontological constructs, which raises several issues, such as the identification of business process constructs as well as the quality dimensions and factors relevant to these constructs. In this paper, our goal is twofold. First, we present a business process meta-model containing all possible constructs of a business process. Second, after discussing a set of quality factors suitable for business processes constructs, we present a quality oriented meta-model resulting from the enrichment of the business process meta-model with quality related information.",2011,,no
Towards Understanding Process Modeling - The Case of the BPM Academic Initiative,"Business process models are typically graphs that communicate knowledge about the work performed in organizations. Collections of these models are gathered to analyze and improve the way an organization operates. From a research perspective, these collections tell about modeling styles, the relevance of modeling constructs, and common formal modeling mistakes. With this paper, we outline a research agenda for investigating the act of process modeling using models of the BPM Academic Initiative. This collection comprises 1903 models, the majority captured in BPMN. The models were created by students from various universities as part of their process modeling education. As such, the collection is particularly suited to investigate modeling practice since it; is probably unique in terms of modeling heterogeneity. As a first step, we characterize EPC and BPMN models of the collection using established process model metrics. Further, we investigate the usage of language constructs for these models. Our findings largely confirm the results obtained in prior, smaller studies on modeling in a professional context.",2011,,no
Computing approach of cathodic process within solid oxide electrolysis cell: Experiments and continuum model validation,"Classical solid oxide fuel cell anode (Ni-cermet) could be employed as solid oxide electrolysis cell cathode. Ni-cermet has been synthesized and tested as solid oxide electrolyzer cathode using three-electrode techniques between 70 degrees C and 900 degrees C. yttria stabilized zirconia was used as the electrolyte and Pt as the counter electrode. Polarization curves and impedance spectra have been analyzed for two gas compositions. The presented results demonstrated an influence of Ni-cermet electrode behavior upon gas composition and temperature. The present results highlight a mechanism changing on Ni-cermet electrode upon gas composition. In a second part, a one-dimensional steady state model is developed to predict the cathodic behavior of Ni-cermet. This model takes into account mass and charge conservation, transport of species and reaction kinetics. It considers the porous electrode to be a homogeneous medium characterized. The influence of varying chemical and electrochemical steps kinetic on the shape of polarization curves is discussed. At high overpotential values the model with two rate-limiting steps has been validated using numerical optimization method. (C) 2011 Elsevier B.V. All rights reserved.",2011,10.1016/j.jpowsour.2011.07.033,no
An Evaluation of Arctic Cloud and Radiation Processes Simulated by the Limited-Area Version of the Global Multiscale Environmental Model (GEM-LAM),"Cloud and radiation processes simulated by the limited area version of the Global Environmental Multiscale Model (GEM-LAM) are evaluated for the period September 1997 to October 1998 over the western Arctic Ocean. This period coincides with the Surface Heat Budget of the Arctic Ocean (SHEBA) field experiment. Surface downwelling solar and terrestrial radiation, surface albedo, vertically integrated water vapour, liquid water path, precipitation, cloud cover and cloud radiative forcing simulated by GEM-LAM are evaluated against the SHEBA observation dataset. GEM-LAM simulates the annual cycle of the downwelling shortwave (SWD) and longwave (LWD) radiation at the surface reasonably well, as well as precipitable water at monthly and daily time scales. Cloud fraction at daily and monthly time scales is not captured well by the model. During winter, GEM-LAM produces a large negative bias for the vertically integrated liquid water path and a positive bias for cloud fraction. As a result, cloud radiative forcing at the surface and LWD radiation are well reproduced but for the wrong reasons because these two biases have an opposing effect on their magnitudes. During summer, the model underestimates the surface albedo, thus resulting in a substantial overestimation of the cloud radiative forcing at the surface. Precipitation is underestimated during winter and overestimated during summer and spring. The sensitivity of the results to the effective radius of ice crystals and the parameterization of cloud phase is also discussed.",2011,10.1080/07055900.2011.604266,no
Fractional Modeling Method of Cognition Process in Teaching Evaluation,Cognition process has been translated into other quantitative indicators in some assessment decision systems. In teaching evaluation system a fractional cognition process model is proposed in this paper. The fractional model is built on fractional calculus theory combining with classroom teaching features. The fractional coefficient is determined by the actual course information. Student self-parameter is decided by the actual situation potential of each individual student. The detailed descriptions are displayed through building block diagram. The objective quantitative description can be given in the fractional cognition process model. And the teaching quality assessments will be more objective and accurate based on the above quantitative description.,2011,,no
A modelling system for wastewater treatment process evaluation and screening: methodology and case study,"Considering nutrient reduction, China's wastewater treatment plants are facing the challenge to upgrade. A modelling system has been developed to screen suitable treatment processes, while statistical methods are not feasible due to data shortage. The system comprises a database to manage different treatment unit technologies, a module to generate candidate processes, a module to simulate effluent quality with material transfer functions, and a tool to identify technologies with better performance. Application of the system showed that TP is the major restrictive control index for effluent compliance. Reverse osmosis technology is preponderant under the strictest requirement.",2011,,no
The Construction and Application of Capability Evaluation Models for Larger-the-better Type process on the Assembly and Packaging of passive components Industry,"Consumer electronic products have become an indispensable part of living for people with the development of information technologies. Crystal oscillator, CXO, a frequency controlling component, which is needed by communication-related industries, is widely used in expensive electronic products like mobile phones, notebook computers and GPS for its outstanding features of stability in temperature and low wearing. In CXO products, wires of ill quality are very likely to lead to ill contact between signal connectors and gold wires on IC or broken wires, which would cause the whole IC-packaged product unable to work normally. This article, thus, will develop a model of assessing and testing process capabilities with process capability index, Cpl, specifically for the larger-the-better quality characteristic of wire process. The article will also provide the assessment procedure, whereby the industry can effectively evaluate whether the process capabilities of their products meet the benchmarks they are supposed to.",2011,10.4028/www.scientific.net/AMM.58-60.1618,no
"Extracting Insights from Electronic Health Records: Case Studies, a Visual Analytics Process Model, and Design Recommendations","Current electronic health record (EHR) systems facilitate the storage, retrieval, persistence, and sharing of patient data. However, the way physicians interact with EHRs has not changed much. More specifically, support for temporal analysis of a large number of EHRs has been lacking. A number of information visualization techniques have been proposed to alleviate this problem. Unfortunately, due to their limited application to a single case study, the results are often difficult to generalize across medical scenarios. We present the usage data of Lifelines2 (Wang et al. 2008), our information visualization system, and user comments, both collected over eight different medical case studies. We generalize our experience into a visual analytics process model for multiple EHRs. Based on our analysis, we make seven design recommendations to information visualization tools to explore EHR systems.",2011,10.1007/s10916-011-9718-x,no
"Design, application and evaluation of a model for process improvement in the context of mature industrial sectors. Case study","Current organizations must work in changing environments, being one of their major challenges to improve competitiveness through the continuous improvement of product quality and the efficiency of their production processes. To do this, companies use to launch programs of Continuous Improvement (CI) and/or further initiatives based on isolated improvement projects applying specific methods between which Six Sigma ( SS) is frequently emphasized. The benefits of both CI programs, just like the specific applications of the SS methodology are well known: for that reason, they are used by many industries. The implementation is not easy; there is no a magic wand for their instantaneous and successful implementation. This paper presents the development and application of a Process Improvement (PI) model, in which have joined the basics of CI programs and the SS methodology. The model developed is used as a management program, which will run CI projects using the methodology SS. The methodology designed for this research study has been tested through its application in four manufacturing companies, suppliers of components for the automotive and home appliances sectors. The model has been applied during the 2008-2009 and 8 projects have been addressed through its application: thus, important improvements in various production areas of these organizations have been achieved. Specifically, from the 8 projects undertaken in four of them the objectives have been fully achieved, in another one only at a medium level and in the remaining three, the results were not successful. The causes that have most affected the implementations have been the involvement of management, the project types undertaken, the surveillance of the operative process and the communication of lessons learned. Also one of the organizations where the model has been applied will be used as a basis for implementing an ongoing program of PI.",2011,,no
A study on process evaluation and selection model for business process management,"Currently, BPM is considered as the suitable framework for today's process-centric trends and BPM may result in considerable rewards for companies adopting it. For successful BPM initiative, the selection of suitable processes for BPM is very important. However, it is difficult to evaluate systematically and reasonably business processes for enterprises that plan to introduce BPM. This paper describes a web-based business process evaluation model based on BSC and fuzzy AHP for BPM. A web-based business process evaluation system was implemented and it provides impartial and reasonable results to enterprises or concerned persons that have insufficient experience and knowledge about BPM. Thus, this paper demonstrates the applicability of fuzzy AHP and BSC concepts in business process evaluation and selection for BPM, and provides a systemic guidance in the decision-making process. (c) 2010 Elsevier Ltd. All rights reserved.",2011,10.1016/j.eswa.2010.11.105,no
The Impact of Testcases on the Maintainability of Declarative Process Models,"Declarative approaches to process modeling are regarded well suited for highly volatile environments as they provide a high degree of flexibility. However, problems in understanding and maintaining declarative process models impede their usage. To compensate for these shortcomings Test Driven Modeling has been proposed. This paper reports from a controlled experiment evaluating the impact of Test Driven Modeling, in particular the adoption of testcases, on process model maintenance. Thereby, students modified declarative process models, one model with the support of testcases and one model without the support of testcases. Data gathered in this experiment shows that the adoption of testcases significantly lowers cognitive load and increases perceived quality of changes. In addition, modelers who had testcases at hand performed significantly more change operations, while at the same time the quality of process models did not decrease.",2011,,no
Redesigning combustion modeling algorithms for the Graphics Processing Unit (GPU): Chemical kinetic rate evaluation and ordinary differential equation integration,"Detailed modeling of complex combustion kinetics remains challenging and often intractable, due to prohibitive computational costs incurred when solving the associated large kinetic mechanisms. The Graphics Processing Unit (GPU), originally designed for graphics rendering on computer and gaming systems, has recently emerged as a powerful, cost-effective supplement to the Central Processing Unit (CPU) for dramatically accelerating scientific computations. Complex scientific computations are now being performed on the GPU in several research fields, such as quantum chemistry, molecular dynamics, and atmospheric modeling. Here, we present methods for exploiting the highly parallel structure of GPUs for combustion modeling. This paper outlines simple algorithm revisions that can be applied to the majority of existing combustion modeling algorithms for GPU computations. Significant simulation acceleration and predictive capability enhancements were obtained by using these GPU-enhanced algorithms for reaction rate evaluation and in ODE integration. For the demonstrations, we implemented the rate evaluation revisions in CHEMKIN and the ODE integration revisions in DASAC and DVODE and we tested the performance for simulating constant-volume ignition using SENKIN. The simulations using the revised algorithms are more than an order of magnitude faster than the corresponding CPU-only simulations, even for a low-end (double-precision) graphics card. Additionally, the computational time scales less than quadratically with the number of chemical species in the kinetic mechanism when using the GPU, as compared to the super-quadratic scaling normally seen with CPU-only chemical kinetics computations: and the GPU-based revisions do not involve approximations to the detailed kinetics. An analysis of the growth rates of combustion mechanism sizes versus computational capabilities of CPUs and GPUs further reveals the important role that GPUs are expected to play in the future of combustion modeling. Finally, we briefly outline practical steps for effectively transitioning from CPU-only to GPU-enhanced combustion modeling. (C) 2011 The Combustion Institute. Published by Elsevier Inc. All rights reserved.",2011,10.1016/j.combustflame.2011.01.024,no
Comprehensive predictive modeling and parametric analysis of multitrack direct laser deposition processes,"Direct laser deposition is a very useful method of fabricating complex parts directly from CAD drawings. Despite its usefulness, many parameters that affect the final quality of the fabricated part make the selection of optimal operating conditions difficult. In this paper, new comprehensive numerical modeling is presented to describe the underlying physics in multitrack direct laser deposition processes including laser powder interaction, mass addition, fluid motion, melting, and solidification. The model rigorously considers the continuous mass addition effects in the set of governing equations. An improved level-set equation is adopted for complex evolution of interfaces for multitrack deposition. Distinct thermal-fluidic behavior and complex three-dimensional part geometry are revealed by accurately modeling the molten pool shape, liquid melt flow, heat affected zone and final deposition track height and width during multitrack direct laser deposition. In addition, an experimental parametric study on various operating parameters on the final quality is presented. The model presented in the paper can be used to design graded or/and desired material structures and properties for multitrack laser deposition processes. (C) 2011 Laser Institute of America.",2011,10.2351/1.3567962,no
The complexity of developmental predictions from dual process models,"Drawing developmental predictions from dual-process theories is more complex than is commonly realized. Overly simplified predictions drawn from such models may lead to premature rejection of the dual process approach as one of many tools for understanding cognitive development. Misleading predictions can be avoided by paying attention to several cautions about the complexity of developmental extrapolations. The complexity of developmental predictions follows from the fact that overall normative responding at a given age derives from several different mental characteristics: (1) the developmental course of Type 1 processing, (2) the developmental course of Type 2 processing, (3) the acquisition of mindware usable by Type 1 processing, (4) the acquisition of mindware usable by Type 2 processing, and (5) the practicing of the mindware available to Type 2 processing to the extent that it is available to be processed in an autonomous manner. The complexity of all these interacting processes and sources of information can sometimes result in U-shaped developmental functions on some heuristics and biases tasks, making younger children look like they are responding more optimally than older children. This is particularly true when the youngest groups are ill-equipped to even understand the task and thus respond randomly. A final caution concerns terminology: The terms normative or rational should be reserved for responses and not attributed to subpersonal processes. (C) 2011 Elsevier Inc. All rights reserved.",2011,10.1016/j.dr.2011.07.003,no
Validation of Finite Element Modeling of Drilling Processes with Solid Tooling in Metals,"Drilling is the source of major cost and time elements in airframe assembly due to hole quality, burr formation, and tool life problems plaguing the industry. Aerospace applications focus on holes for rivets loaded in shear in aluminum, titanium and composite stack-ups. Optimal chip flow and tool life are often in competition with burr formation, general hole quality, and cycle time. Physics-based modeling of drilling processes can provide insight and information not readily available or easily obtained from experiments, and in a much faster time frame. A three-dimensional finite element-based model of drilling is presented which includes fully adaptive unstructured meshing, tight thermo-mechanical coupling, deformable tool-chip-workpiece contact, interfacial heat transfer across the tool-chip boundary, and constitutive models appropriate for high strain-rate, large strain and high temperature deformation.",2011,10.4028/www.scientific.net/AMR.223.182,no
Metric Trees for Efficient Similarity Search in Large Process Model Repositories,"Due to the increasing adoption of business process management and the key role of process models, companies are setting up and maintaining large process model repositories. Repositories containing hundreds or thousands of process models are not uncommon, whereas only simplistic search functionality, such as text based search or folder navigation, is provided, today. On the other hand, advanced methods have recently been proposed in the literature to ascertain the similarity of process models. However, due to performance reasons, an exhaustive similarity search by pairwise comparison is not feasible in large process model repositories. This paper presents an indexing approach based on metric trees, a hierarchical search structure that saves comparison operations during search with nothing but a distance function at hand. A detailed investigation of this approach is provided along with a quantitative evaluation thereof, showing its suitability and scalability in large process model repositories.",2011,,no
Time representations and mathematical models for process scheduling problems,"During the last 15 years, many mathematical models have been developed in order to solve process operation scheduling problems, using discrete or continuous-time representations. In this paper, we present a unified representation and modeling approach for process scheduling problems. Four different time representations are presented with corresponding strengthened formulations that rely on exploiting the non-overlapping graph structure of these problems through maximum cliques and bicliques. These formulations are compared, and applied to single-stage and multi-stage batch scheduling problems, as well as crude-oil operations scheduling problems. We introduce three solution methods that can be used to achieve global optimality or obtain near-optimal solutions depending on the stopping criterion used. Computational results show that the multi-operation sequencing time representation is superior to the others as it allows efficient symmetry-breaking and requires fewer priority-slots, thus leading to smaller model sizes. (C) 2010 Elsevier Ltd. All rights reserved.",2011,10.1016/j.compchemeng.2010.07.007,no
ANALYSIS OF CREATED REPRESENTATIONS OF THE DESIGN OBJECT DURING THE PROBLEM SOLVING PROCESS,"During the process of developing new solutions, the designer creates different representations of the design object, which have a high variation in their level of abstraction. These representations have great impact on reaching the project targets. Therefore, it is important to understand how the designers create these representations for the problem solving, in order to derive general ways of proceeding. This paper describes the assessment, classification and analysis of designer's proceedings during the process of problem-solving, based on two examples from the industry. The evaluation contains the created representations, as well as the proceedings for the problem solving. The results show, what level of detail and what scope is to be preferred by the designer, for representing the design object during the different stages of the problem solving. Furthermore, it is shown, in what order problems are solved during the design process.",2011,,no
Using logic programming for modeling the one-carbon metabolism network to study the impact of folate deficiency on methylation processes,"Dynamical modeling is an accurate tool for describing the dynamic regulation of one-carbon metabolism (1CM) with emphasis on the alteration of DNA methylation and/or dUMP methylation into dTMP. Using logic programming we present a comprehensive and adaptative mathematical model to study the impact of folate deficiency, including folate transport and enzymes activities. 5-Methyltetrahydrofolate (5mTHF) uptake and DNA and dUMP methylation were studied by simulating nutritional 5mTHF deficiency and methylenetetrahydrofolate reductase (MTHFR) gene defects. Both conditions had distinct effects on 1CM metabolite synthesis. Simulating severe 5mTHF deficiency (25% of normal levels) modulated 11 metabolites. However, simulating a severe decrease in MTHFR activity (25% of normal activity) modulated another set of metabolites. Two oscillations of varying amplitude were observed at the steady state for DNA methylation with severe 5mTHF deficiency, and the dUMP/dTMP ratio reached a steady state after 2 h, compared to 2.5 h for 100% 5mTHF. MTHFR activity with 25% of V(max) resulted in an increased methylated DNA pool after half an hour. We observed a deviation earlier in the profile compared to 50% and 100% V(max). For dUMP methylation, the highest level was observed with 25%, suggesting a low rate of dUMP methylation into dTMP with 25% of MTHFR activity. In conclusion, using logic programming we were able to construct the 1CM for analyzing the dynamic system behavior. This model may be used to refine biological interpretations of data or as a tool that can provide new hypotheses for pathogenesis.",2011,10.1039/c1mb05102d,no
Development and Experimental Evaluation of a 1D Distributed Model of Transport Phenomena in a Continuous Biodrying Process for Pulp and Paper Mixed Sludge,"Effective sludge management is increasingly critical for pulp and paper mills due to high landfill costs and complex regulatory frameworks for disposal options such as sludge landspreading and composting. A novel continuous biodrying process has been developed to dry mixed sludge so that it can be combusted efficiently in a biomass boiler for energy recovery. Modeling this process is important in order to better understand the transport phenomena in the biodrying reactor and for design and scale-up of the process. A one-dimensional (1D) distributed model for heat transfer coupled with mass and biological transfer phenomena is introduced in this article that shows that the temperature of the sludge matrix is a critical parameter. The model assumes lumped parameters in the gas flow direction and distributed parameters in the (vertical) solids flow direction. Bioheat as a source term and evaporative heat as a sink term are critical issues. In order to evaluate the parameters and assess the model accuracy, a series of experiments was performed. The matrix temperatures predicted by the model were found to be in reasonable agreement with the experimental results, showing that the main transport phenomena were reflected in the model. Larger discrepancies between the water removal rates predicted by the model and the experimental values were indentified at higher aerobic exothermicity, which can be attributed to the complex mechanisms governing the growth cycle of mesophilic and thermophilic bacteria. A dimensionless analysis was performed to identify key dimensionless groups as well as the most dominant transport phenomena in the biodrying process. The results confirmed that convection processes dominated heat transfer at the top of the reactor, and the exothermic aerobic bioenergy dominated at its bottom.",2011,10.1080/07373937.2010.482723,no
Efficient Consistency Measurement Based on Behavioral Profiles of Process Models,"Engineering of process-driven business applications can be supported by process modeling efforts in order to bridge the gap between business requirements and system specifications. However, diverging purposes of business process modeling initiatives have led to significant problems in aligning related models at different abstract levels and different perspectives. Checking the consistency of such corresponding models is a major challenge for process modeling theory and practice. In this paper, we take the inappropriateness of existing strict notions of behavioral equivalence as a starting point. Our contribution is a concept called behavioral profile that captures the essential behavioral constraints of a process model. We show that these profiles can be computed efficiently, i.e., in cubic time for sound free-choice Petri nets w.r.t. their number of places and transitions. We use behavioral profiles for the definition of a formal notion of consistency which is less sensitive to model projections than common criteria of behavioral equivalence and allows for quantifying deviation in a metric way. The derivation of behavioral profiles and the calculation of a degree of consistency have been implemented to demonstrate the applicability of our approach. We also report the findings from checking consistency between partially overlapping models of the SAP reference model.",2011,10.1109/TSE.2010.96,no
A Repository-Based Enterprise Strategy Management Process Maturity Evaluation Model,"Enterprise strategy management can be regarded essentially as a process of strategic planning, implementing, evaluating and controlling. Additionally, there are lots of detailed methods of operation in this process. How to evaluate the process of enterprise strategy management which applies various operating methods with one unified evaluating standard and measurement quantitatively and qualitatively is always the emphasis of the research of enterprise strategy management. Therefore, this paper uses the thought of Capability Maturity Model (CMM), which is applied in the process of software development for reference, and puts forward a Repository-Based Enterprise Strategy Management Process Maturity, Evaluation Model. In this model, the enterprise strategy management process is evaluated comprehensively and classified as a whole, according to the knowledge maturity of the methods of strategic planning, implementation, evaluation and control used in the process (i.e. score based on the repository). This model contains a 4-level enterprise strategy management process maturity framework and a maturity evaluating repository. The Repository-Based Enterprise Strategy Management Process Maturity Evaluation Model measures the scientificity and quality of the process of enterprise strategy management. We hope, to the enterprise strategy management, the process maturity should be evaluated firstly, and then the scientific methods and operating techniques which can make enterprise strategy management process ascend in the maturity stairs can be introduced.",2011,,no
Modeling the fate of atmospheric reduced nitrogen during the Rocky Mountain Atmospheric Nitrogen and Sulfur Study (RoMANS): Performance evaluation and diagnosis using integrated processes rate analysis,"Excess wet and dry deposition of nitrogen-containing compounds is a concern at a number of national parks. The Rocky Mountain Atmospheric Nitrogen and Sulfur Study (RoMANS) was conducted during the spring and summer of 2006 to identify the overall mix of ambient and deposited sulfur and nitrogen at Rocky Mountain National Park (RMNP), in north-central Colorado. The Comprehensive Air Quality Model with extensions (CAMx) was used to simulate the fate of gaseous and particulate species subjected to multiple chemical and physical processes during RoMANS. This study presents an operational evaluation with a special emphasis on the model performance of reduced nitrogen species. The evaluation showed large negative biases and errors at RMNP and the entire domain for ammonia; therefore the model was considered inadequate for future source apportionment applications. The CAMx Integrated Processes Rate (IPR) analysis tool was used to elucidate the potential causes behind the poor model performance. IPR served as a tool to diagnose the relative contributions of individual physical and chemical processes to the final concentrations of reduced nitrogen species. The IPR analysis revealed that dry deposition is the largest sink of ammonia in the model, with some cells losing almost 100% of the available mass. Closer examination of the ammonia dry deposition velocities in CAMx found that they were up to a factor of 10 larger than those reported in the literature. A series of sensitivity simulations were then performed by changing the original deposition velocities with a simple multiplicative scaling factor. These simulations showed that even when the dry deposition values were altered to reduce their influence, the model was still unable to replicate the observed time series; i.e., it fixed the average bias, but it did not improve the precision. (C) 2010 Elsevier Ltd. All rights reserved.",2011,10.1016/j.atmosenv.2010.09.011,no
Activities and Problems in New Product Development Process in the Networking Industry - A Case of Different Business Models,"Facing intensive changes in the external environment in a globally competitive market, as well as rapid technological changes and varied consumer demands, enterprise survival has become an important topic. Firms should focus on developing products that are higher quality and more reasonably priced than those of competitors, to increase their profits and establish a competitive advantage to satisfy and fulfill customer needs, making new product development an important topic for discussion. This study classified business models into two types, OBM and ODM, and classified four stages of new product development, Proposal and Planning stage, Product Development stage, Trial Run Planning stage, and Mass Production stage. This study examines networking firms in Taiwan using six case studies, to identify the representative activities during each NPD stage, such as product feasibility evaluation and product proposal establishment, as well as project team member determination/project milestone establishment and development budget estimation during the proposal and planning and product development stages, EVT proceeding in the product development stage, DVT proceeding and PVT proceeding in the trial run planning stage, and customer PO acceptance, shipment delivery and project final result survey in the mass production stage, and each activity will undoubtedly encounter different marketing, product development and process development problems.",2011,,no
Bounding reward measures of Markov models using the Markov decision processes,"For a Markov reward process, where upper and lower bounds for the transition rates and rewards are known, a new approach to bound the expected reward is presented. Based on a previous paper where sharp bounds have been defined for the problem, but only an inefficient and unstable algorithm is proposed, this paper presents algorithms to compute the bounds by interpreting the problem as a Markov Decision Process. In this way, the well known value and policy iteration algorithms can be adopted to compute reward bounds in a stable and fairly efficient way. Different numerical techniques are presented for computing the reward bounds. Copyright (C) 2011 John Wiley & Sons, Ltd.",2011,10.1002/nla.792,no
Forecasting the Diffusion of Innovation: A Stochastic Bass Model With Log-Normal and Mean-Reverting Error Process,"Forecasting the diffusion of innovations plays a major role in managing technology development and in engineering management overall. In this paper, we extend the conventional Bass model stochastically by specifying the error process of sales as log-normal and mean-reverting. Our model satisfies the following reasonable properties, which are generally ignored in the existing literature: sales cannot be negative, the error process can have a memory, and sales fluctuate more when they are high and less when they are low. The conventional and widely used model that assumes normally distributed error term does not have these properties. We address how to forecast properly under the log-normal and mean-reverting error process, and show analytically and numerically that in our extended model sales forecasts can substantially alter conventional Bass forecasts. We also analyze the model empirically, showing that our extension can improve the accuracy of future sales forecasts.",2011,10.1109/TEM.2010.2048912,no
Studies on profile error and extruding aperture for the RP parts using the fused deposition modeling process,"Fused deposition modeling (FDM) is one of the leading rapid prototyping processes for producing acrylonitrile butadiene styrene prototypes. However, the extruding accuracy in part fabrication is subject to transmission machinery and filament diameter. The parts cannot be filled up completely. Therefore, profile error and extruding apertures are two substantial quality characteristics to be considered. This study proposed an original image measurement method for examining profile error through a series of standard cylinders laid on the contour of the part. Also, images captured can be used to identify non-filled regions on part appearance to calculate aperture area on the surface layer. Besides, this study investigated the effects of extruding parameters, including contour width, contour depth, part raster width, and raster angle, on quality characteristics by Taguchi's method. A thin solid model based on a 2-D spiral was designed to demonstrate the proposed approach. Results of ANOVA and confirmation experiments showed that the parametric criteria concluded in this study could obtain satisfactory performances on profile error and extruding apertures in the FDM process.",2011,10.1007/s00170-010-2882-1,no
STATISTICAL MODELS OF AN OBJECT AND PROCESS OF MEASUREMENTS IN HYGROMETRY OF SOLID SUBSTANCES,Generalized statistical models of an object and the process of measurements in the hygrometry of solid substances are proposed. Statistical criteria of the homogeneity of a solid substance with respect to mass fraction of moisture and a method of estimating the mass fraction of the moisture of a solid substance of given volume are presented.,2011,,no
Evaluation of global continental hydrology as simulated by the Land-surface Processes and eXchanges Dynamic Global Vegetation Model,"Global freshwater resources are sensitive to changes in climate, land cover and population density and distribution. The Land-surface Processes and eXchanges Dynamic Global Vegetation Model is a recent development of the Lund-Potsdam-Jena model with improved representation of fire-vegetation interactions. It allows simultaneous consideration of the effects of changes in climate, CO2 concentration, natural vegetation and fire regime shifts on the continental hydrological cycle. Here the model is assessed for its ability to simulate large-scale spatial and temporal runoff patterns, in order to test its suitability for modelling future global water resources. Comparisons are made against observations of streamflow and a composite dataset of modelled and observed runoff (1986-1995) and are also evaluated against soil moisture data and the Palmer Drought Severity Index. The model captures the main features of the geographical distribution of global runoff, but tends to overestimate runoff in much of the Northern Hemisphere (where this can be somewhat accounted for by freshwater consumption and the unrealistic accumulation of the simulated winter snowpack in permafrost regions) and the southern tropics. Interannual variability is represented reasonably well at the large catchment scale, as are seasonal flow timings and monthly high and low flow events. Further improvements to the simulation of intra-annual runoff might be achieved via the addition of river flow routing. Overestimates of runoff in some basins could likely be corrected by the inclusion of transmission losses and direct-channel evaporation.",2011,10.5194/hess-15-91-2011,no
Nonlinear Dynamic Modeling and Validation of Heavy Machine's Processing,"Heavy machine plays an important role in the manufacturing field such as air and navigation, the research on heavy machine has a very important national defense and civilian value. Modeling of heavy machining'processing is the based and key part of many further study, the rationality of model directly influence the accuracy of further study,so it's very important to establish a good model. This paper establishes a nonlinear dynamic model by nonlinear dynamics theory for heavy machine's process, and verifies the rationality of the model on a heavy machine, finally,we find that this model can effectively simulates the actual process of heavy machine.",2011,,no
Validation of model virus removal and inactivation capacity of an erythropoietin purification process,"Human erythropoietin (hEpo) production requires mammalian cells able to make complex post-translational modifications to guaranty its biological activity. As mammalian cell can be reservoir of pathogenic viruses and several animal origin components are usually used in the cultivation of mammalian cells, hEpo contamination with viruses is something of great concern. As consequence, this study investigated the viral removal and inactivation capacity of a recombinant-hEpo (rec-hEpo) purification process. Canine parvovirus, Human poliovirus type-2, Bovine viral diarrhea virus and Human immunodeficiency virus type-1 were used for measuring process viral removal and inactivation capacities. In conclusion, this study corroborated that the assessed rec-hEpo purification process has enough capacity (5.0-19.4 Logs) for removing and inactivating these model viruses and sodium hydroxide demonstrated to be a robust sanitization solution for chromatography columns (5.0 (PV-2)-6.7 (CPV) Logs). (C) 2011 Published by Elsevier Ltd on behalf of The International Alliance for Biological Standardization.",2011,10.1016/j.biologicals.2011.09.006,no
Cognitive processes in implicit attitude tasks: An experimental validation of the Trip Model,"Implicit attitude tasks have become very popular in various areas of psychology. However, little is known about the cognitive processes they involve. To address this issue, we investigated the underlying processes of the Go/No-go Association Task (GNAT), a go/no-go variant of the well-known Implicit Association Test (IAT). More precisely, we tested two alternative multinomial processing tree (MPT) models of GNAT performance, the Trip Model and the generalized Quad Model. Both models assume that GNAT performance is influenced not only by automatic associations but also by response biases and a controlled discrimination process. However, the two models differ with respect to an additional overcoming bias process. In contrast to the Quad Model, the Trip Model assumes that overcoming bias does not play a major role in GNAT performance. Instead, the Trip Model emphasizes the role of response biases. We report three experiments that demonstrate the validity of the Trip Model for GNAT data. Copyright (C) 2010 John Wiley & Sons, Ltd.",2011,10.1002/ejsp.776,no
Behavioral and Emotional Regulation and Adolescent Substance Use Problems: A Test of Moderation Effects in a Dual-Process Model,"In a structural model, we tested how relations of predictors to level of adolescent substance use (tobacco, alcohol, marijuana), and to substance-related impaired-control and behavior problems, are moderated by good self-control and poor regulation in behavioral and emotional domains. The participants were a sample of 1,116 public high-school students. In a multiple-group analysis for good self-control, the paths from negative life events to substance use level and from level to behavior problems were lower among persons scoring higher on good behavioral self-control. In a multiple-group analysis for poor regulation, the paths from negative life events and peer use to level of substance use were greater among persons scoring higher on poor behavioral (but not emotional) regulation; an inverse path from academic competence to level was greater among persons scoring higher on both aspects of poor regulation. Paths from level to impaired-control and behavior problems were greater among persons scoring higher on both poor behavioral and poor emotional regulation. Theoretical implications concerning the role of behavioral and emotional regulation in moderation effects are discussed.",2011,10.1037/a0022870,no
The Caring-Disability Creation Process model: a new way of combining 'Care' in nursing and 'Rehabilitation' for better quality of services and patient safety,"In an effort to continuously improve the quality of the services provided to persons in rehabilitation and patient safety, this article presents a model for the development of a Caring-Disability Creation Process that combines two elements that, in the past, have often been separated: care in nursing and rehabilitation. This new model and its underlying concepts introduce to a comprehensive approach in the recognition of the added value of caring by the contribution of rehabilitation nurses making an optimal development of the individual's potential and achieving successful social participation.",2011,10.3109/09638288.2011.560330,no
Quality Relevant Data-Driven Modeling and Monitoring of Multivariate Dynamic Processes: The Dynamic T-PLS Approach,"In data-based monitoring field, the nonlinear iterative partial least squares procedure has been a useful tool for process data modeling, which is also the foundation of projection to latent structures (PLS) models. To describe the dynamic processes properly, a dynamic PLS algorithm is proposed in this paper for dynamic process modeling, which captures the dynamic correlation between the measurement block and quality data block. For the purpose of process monitoring, a dynamic total PLS (T-PLS) model is presented to decompose the measurement block into four subspaces. The new model is the dynamic extension of the T-PLS model, which is efficient for detecting quality-related abnormal situation. Several examples are given to show the effectiveness of dynamic T-PLS models and the corresponding fault detection methods.",2011,10.1109/TNN.2011.2165853,no
Comparison of Two Quality Control Models for Short Run Process Based on Bayesian Analysis,"In dealing with the problem of establishing control limits in short run production, Bayesian approach provides a effective way for are short run process control and are particularly attractive. In this paper, two quality control models for short run process are presented based on Bayesian analysis and the two models are compared. Models are focused on normally distributed data. The first way to establish model is through the posterior density of mean and variance of normally distributed data respectively and the second way is through the posterior predictive density. And the results deduced from two different ways are compared.",2011,,no
Subspace-Modeling-Based Nonlinear Measurement for Process Design,"In industry, many nonlinear processes can be approximated well by a linear model under a suitable design parameter, upon which a linear controller can effectively control these processes. The key problem is how to find this suitable design parameter, which is never considered in process design. In this paper, a novel design for control approach is proposed to design the process to have a satisfactory linear approximation. First, a subspace-modeling-based nonlinear measurement is proposed to avoid the infinite dimensional optimization problem in the traditional nonlinearity measurement. Then a particle-swarm-optimization-based design approach is developed to obtain the optimal design parameter through solving the nonconvex and nondifferential measurement problem. Finally, the proposed design is applied to design a practical curing oven and compared with the existing design.",2011,10.1021/ie2008554,no
An entropy-based uncertainty measure of process models,"In managing business processes, the process uncertainty and variability are significant factors causing difficulties in prediction and decision making, which evokes and augments the importance and need of process measures for systematic analysis. We propose an entropy-based process measure to quantify the uncertainty of business process models. The proposed measure enables capturing the dynamic behavior of processes, in contrast to previous work which focused on providing measures for the static aspect of process models. (C) 2010 Elsevier B.V. All rights reserved.",2011,10.1016/j.ipl.2010.10.022,no
Physical modeling and implementation scheme of native defect diffusion and interdiffusion in SiGe heterostructures for atomistic process simulation,"In order to simulate the diffusion kinetics during thermal treatments in SiGe heterostructures, a physically-based atomistic model including chemical and strain effects has been developed and implemented into a nonlattice atomistic kinetic monte carlo (KMC) framework. This model is based on the description of transport capacities of native point defects (interstitials and vacancies) with different charge states in SiGe alloys in the whole composition range. Lattice atom diffusivities have been formulated in terms of point defect transport, taking into account the different probability to move Si and Ge atoms. Strain effects have been assessed for biaxial geometries including strain-induced anisotropic diffusion, as well as charge effects due to strain-induced modifications of the electronic properties. Si-Ge interdiffusion in heterostructures has been analyzed from an atomistic perspective. A limited set of physical parameters have been defined, being consistent with previously reported ab initio calculations and experiments. The model has been implemented into a nonlattice KMC simulator and the relevant implementation details and algorithms are described. In particular, an efficient point defect mediated Si-Ge exchange algorithm for interdiffusion is reported. A representative set of simulated interdiffusion profiles are shown, exhibiting good agreement with experiments. (C) 2011 American Institute of Physics. [doi:10.1063/1.3581113]",2011,10.1063/1.3581113,no
"De-risking Pharmaceutical Tablet Manufacture Through Process Understanding, Latent Variable Modeling, and Optimization Technologies","In pharmaceutical tablet manufacturing processes, a major source of disturbance affecting drug product quality is the (lot-to-lot) variability of the incoming raw materials. A novel modeling and process optimization strategy that compensates for raw material variability is presented. The approach involves building partial least squares models that combine raw material attributes and tablet process parameters and relate these to final tablet attributes. The resulting models are used in an optimization framework to then find optimal process parameters which can satisfy all the desired requirements for the final tablet attributes, subject to the incoming raw material lots. In order to de-risk the potential (lot-to-lot) variability of raw materials on the drug product quality, the effect of raw material lot variability on the final tablet attributes was investigated using a raw material database containing a large number of lots. In this way, the raw material variability, optimal process parameter space and tablet attributes are correlated with each other and offer the opportunity of simulating a variety of changes in silico without actually performing experiments. The connectivity obtained between the three sources of variability (materials, parameters, attributes) can be considered a design space consistent with Quality by Design principles, which is defined by the ICH-Q8 guidance (USDA 2006). The effectiveness of the methodologies is illustrated through a common industrial tablet manufacturing case study.",2011,10.1208/s12249-011-9700-4,no
An Evaluation Model of Machining Process for Green Manufacturing,"In recent years going green has become a strategic priority in machining process aiming to enhance environmental performance and productivity simultaneously. The development of appropriate evaluation model is one of the keys to the success of sustainable development strategy for machining process. For the diversity and complexity of resources and environmental attributes generated by machining process, an evaluation model of machining process for green manufacturing is presented based on back-propagation (BP) neural network. The structure and solving of the model are studied as well as the quantification method of network input indexes. A case study on valve body machining process for green manufacturing is performed to illustrate and examine the validity of model.",2011,10.1166/asl.2011.1275,no
A Novel Generic Clinical Reference Process Model for Event-Based Process Times Measurement,"In recent years, performance measurement has become an important element of efficiency improvement projects in many organizations. Thereby. process-based measures are used to evaluate the process efficiency and quality. In health care, such measures are often neither commonly defined nor standardized. Therefore we present a novel approach for standardized clinical quality metrics measurement by means of a newly developed clinical reference process model and generic Key Performance Indicators (KPIs). We use both for a comprehensive description of the complete clinical patient-centered process in hospital. Our approach fulfils performance measurement requirements particularly in the field of time-critical diseases like heart attack and stroke.",2011,,no
Chance Programming Models for Time-Energy Trade-Off Problem of Product Disassembly Process with Multiple Stochastic Variables,"In the actual disassembly decision-making, decision-maker may comprehensively consider two or more evaluation parameters. In order to deal with this kind of issues, trade-off problems have attracted a lot of research attention. Recently, more and more people may concern the minimum energy consumption issue when completing the disassembly task. Therefore, a class of novel stochastic trade-off problem is introduced and proposed in this paper, namely stochastic time-energy trade-off problem. According to the related concepts of dependent-chance programming and chance-constrained programming, combined with the uncertainty characteristics of disassembly process, typical optimization models of time-energy trade-off problem are established. Simultaneously, the quasi-optimal solutions of optimization models of time-energy trade-off problem are solved by the hybrid intelligent algorithm integrating stochastic simulation and simplex. Some numerical examples are also presented to illustrate the modelling idea and the effectiveness of the proposed algorithm. The proposed models and method provide an insight into the optimal balance of time and energy under different decision-making conditions.",2011,10.1166/asl.2011.1464,no
Inverse thermal problems in computational modelling of the paper vacuum drying process,"In this article, a mathematical model of the vacuum paper drying process was developed utilizing Fluent, a commercial Computational Fluid Dynamic software. Paper is treated as a porous medium, which is in thermodynamic equilibrium with humid air. Water distribution within paper is resolved by solving a so-called lumped diffusion equation. A developed model requires numerous experimental data and unknown/uncertain parameters. The latter ones are subjects of inverse thermal analysis. This analysis is based on the Levenberg-Marquardt method.",2011,10.1080/17415977.2010.531476,no
A CONSUMPTION-INVESTMENT PROBLEM MODELLED AS A DISCOUNTED MARKOV DECISION PROCESS,"In this paper a problem of consumption and investment is presented as a model of a discounted Markov decision process with discrete-time. In this problem, it is assumed that the wealth is affected by a production function. This assumption gives the investor a chance to increase his wealth before the investment. For the solution of the problem there is established a suitable version of the Euler Equation (EE) which characterizes its optimal policy completely, that is, there are provided conditions which guarantee that a policy is optimal for the problem if and only if it satisfies the EE. The problem is exemplified in two particular cases: for a logarithmic utility and for a Cobb-Douglas utility. In both cases explicit formulas for the optimal policy and the optimal value function are supplied.",2011,,no
Modelling Joint Decision Making Processes Involving Emotion-Related Valuing and Empathic Understanding,"In this paper a social agent model for joint decision making is presented addressing the role of mutually acknowledged empathic understanding in the decision making. The model is based on principles from recent neurological theories on mirror neurons, internal simulation, and emotion-related valuing. Emotion-related valuing of decision options and mutual contagion of intentions and emotions between agents are used as a basis for mutual empathic understanding and convergence of decisions and their associated emotions.",2011,,no
A capital budgeting problem for preventing workplace mobbing by using analytic hierarchy process and fuzzy 0-1 bidimensional knapsack model,"In this paper, a capital budgeting problem for preventive measures of workplace mobbing based on fuzzy 0-1 bidimensional knapsack model with non-financial and financial budget limits is proposed. The weights to be used as the objective function coefficients of the model are obtained from analytic hierarchy process (AHP) methodology that incorporates possible causes of workplace mobbing on criteria level, and possible preventive measures on alternatives level of AHP hierarchy. The quantification of non-financial and financial budgets as well as non-financial and financial costs is developed based on their relative weights. In deterministic model, the relative weights are directly used, whereas in fuzzy model, they are quantified. The defuzzification of the fuzzy model is proposed to be made by using t-norm and t-conorm fuzzy relations which are expected to give the most optimistic, and the most pessimistic results, respectively. The results of the hypothetical example verify the expectations. (C) 2011 Elsevier Ltd. All rights reserved.",2011,10.1016/j.eswa.2011.04.022,no
Using a symbolic process model as input for model-based fMRI analysis: Locating the neural correlates of problem state replacements,"In this paper, a model-based analysis method for fMRI is used with a high-level symbolic process model. Participants performed a triple-task in which intermediate task information needs to be updated frequently. Previous work has shown that the associated resource - the problem state resource - acts as a bottleneck in multitasking. The model-based method was used to locate the neural correlates of 'problem state replacements'. To analyze the fMRI data, we fit the computational process model to the behavioral data and regressed the model's activity against the fMRI data. The brain region responsible for the temporary representation of problem states, the inferior parietal lobule, and the brain region responsible for long-term storage of problem states, the inferior frontal gyrus were thus identified. These results show that model-based fMRI analyses can be performed using high-level symbolic cognitive models, enabling fine-grained exploratory fMRI research. (C) 2011 Elsevier Inc. All rights reserved.",2011,10.1016/j.neuroimage.2011.05.084,no
Modelling generation mechanism of defects during permanent mould centrifugal casting process of TiAl alloy exhaust valve,"In this paper, the TiAl liquid filling process during vertical centrifugal casting into a permanent mould has been described analytically. A model has been established to simulate the forward filling and backward filling process, and two parameters were used to provide a quantitative description of the defect generation that is associated with the forward filling of the mould cavity. One of the parameters used was the forward filling cross-sectional area, and the other was the inclined angle of the free surface of forward filling flow. The cross-sectional area decreases, and the inclined angle increases when the rotational speed increases, the tendency of which becomes more obvious near the mould cavity entrance. The residual volume of the mould cavity after the forward filling is related to the volume of trapped pores. The filling process has also been investigated using numerical simulation. Based on the filling and solidification characteristic of a TiAl valve, the off-centre porosity distribution is also discussed.",2011,10.1179/026708310X12635619988023,no
Berry-Esseen bounds for wavelet estimator in a regression model with linear process errors,"In this paper, we derive the Berry-Esseen bounds of the wavelet estimator for a nonparametric regression model with linear process errors generated by phi-mixing sequences. As application, by the suitable choice of some constants, the convergence rate O(n(-1/6)) of uniformly asymptotic normality of the wavelet estimator is obtained. Our results generalize some known results in the literature. (C) 2010 Elsevier B.V. All rights reserved.",2011,10.1016/j.spl.2010.09.024,no
Measuring the Understandability of Business Process Models - Are We Asking the Right Questions?,"In this paper, we show how experiments on the understandability of business process models can depend on the exact wording used in the experiments' questionnaires. For this purpose, we partially replicated a published experiment. We asked a group of students a number of questions on relations between tasks in a business process model. Alternatively, we used a set of modified questions which were aimed to ask for exactly the same relations. The result was that there was a significant difference in the number of correct answers between the two systems to construct a question. We argue that a non-negligible part of the wrong answers given in the experiment did not result from problems to understand the model, but rather from problems to understand the question. It follows that it is dangerous to draw conclusions from such an experiment until enough effort has been taken to select appropriate questions.",2011,,no
Dynamic model-based evaluation of process configurations for integrated operation of hydrolysis and co-fermentation for bioethanol production from lignocellulose,"In this study a number of different process flowsheets were generated and their feasibility evaluated using simulations of dynamic models. A dynamic modeling framework was used for the assessment of operational scenarios such as, fed-batch, continuous and continuous with recycle configurations. Each configuration was evaluated against the following benchmark criteria, yield (kg ethanol/kg dry-biomass), final product concentration and number of unit operations required in the different process configurations. The results show that simultaneous saccharification and co-fermentation (SSCF) operating in continuous mode with a recycle of the SSCF reactor effluent, results in the best productivity of bioethanol among the proposed process configurations, with a yield of 0.18 kg ethanol/kg dry-biomass. (C) 2010 Elsevier Ltd. All rights reserved.",2011,10.1016/j.biortech.2010.09.045,no
A numerical study of a two property catalyst/sorbent pellet design for the sorption-enhanced steam-methane reforming process: Modeling complexity and parameter sensitivity study,"In this study the performance of a combined catalyst/sorbent pellet design for the sorption-enhanced steam-methane reforming process has been investigated. Different mathematical model complexities have been studied and parameter sensitivity analyses have been performed. The mathematical pellet model formulated for the process describes the evolution of species mole fractions, pressure, total concentration, temperature, fluxes, and convection within the voids of the porous pellet. The effective diffusivites are described according to the parallel pore and random pore models, and the mass diffusion fluxes are described according to the Maxwell-Stefan and dusty gas models. Moreover, models proposed in the literature for void fraction changes, product layer diffusion resistance, and degeneration clue to multiple carbonation/calcination cycles are employed. The simulated pellet effectiveness factor is a convenient parameter frequently used in modeling and simulations of chemical reactors indicating the relative importance of diffusion and reaction limitations. Thus, in this study, the effectiveness factor behavior clue to different mathematical modeling assumptions and model parameter values is elucidated. The combined pellet performance is promising compared to the conventional two-pellet design. For further improved modeling and simulations of the pellet, the characterizing of the pore size distribution is important because of the Knudsen diffusion mechanism. Moreover, the reduction in void fraction with the CaO conversion, product layer growth, and degeneration due to multiple cycles are all important effects in the SE-SMR process because these mechanisms influence on the cycle life time, reaction rate, and capture capacity. For industrial applications, the pellet is only of interest in the capture kinetic controlling step, i.e. before the process becomes controlled by the product layer diffusion resistance. (C) 2011 Elsevier B.V. All rights reserved.",2011,10.1016/j.cej.2011.10.045,no
An Empirical Modeling Platform to Evaluate the Relative Control Discrete CHO Cell Synthetic Processes Exert Over Recombinant Monoclonal Antibody Production Process Titer,"In this study we have combined empirically derived mathematical models of intracellular Mab synthesis to quantitatively compare the degree to which individual cellular processes limit recombinant IgG(4) monoclonal antibody production by GS-CHO cells throughout a state-of-the-art industrial fed-batch culture process. Based on the calculation of a production process control coefficient for each stage of the intracellular Mab synthesis and secretion pathway, we identified the major cellular restrictions on Mab production throughout the entire culture process to be recombinant heavy chain gene transcription and heavy chain mRNA translation. Surprisingly, despite a substantial decline in the rate of cellular biomass synthesis during culture, with a concomitant decline in the calculated rate constants for energy-intensive Mab synthetic processes (Mab folding/assembly and secretion), these did not exert significant control of Mab synthesis at any stage of production. Instead, cell-specific Mab production was maintained by increased Mab gene transcription which offset the decline in cellular biosynthetic rates. Importantly, this study shows that application of this whole-process predictive modeling strategy should rationally precede and inform cell engineering approaches to increase production of a recombinant protein by a mammalian host cell-where control of productivity is inherently protein product and cell line specific. Biotechnol. Bioeng. 2011; 108: 2193-2204. (C) 2011 Wiley Periodicals, Inc.",2011,10.1002/bit.23146,no
Experimental model validation of an integrated process for the removal of carbon dioxide from aqueous ammonia solutions,"In this work modelling and experimental validation of an integrated process for the removal of carbon dioxide from ammonia solutions - the so called decarbonisation - is presented. In this process, carbon dioxide and small amount of ammonia is stripped out from the solution at ambient pressure in a packed column. Recovery of the stripped ammonia can be reached by combining absorption of ammonia and condensation of stripping steam. The integration of stripping, absorption and direct-contact condensation (DCC) can be achieved in one compact unit in which stripping takes place in the lower part of the packed column, and the DCC and ammonia absorption in its upper part. This unit has been modelled in a rigorous way considering heat and mass transfer as well as reaction rates in multicomponent reactive stripping, absorption and direct-contact condensation in packed columns (Maekowiak et al., 2009). Extensive experimental investigations in a pilot scale packed column with diameters of 0.15 and 0.32 m have been performed for both, the stripping and for DCC. Relevant operation parameters as well as column dimensions were varied during the experiments in order to investigate their influence on the selectivity of the decarbonisation and to achieve a broad data base for the validation. Experimental validation of the two sub-processes and the entire decarbonisation shows good agreement between calculated and experimental values. Based on the validated model a successful optimisation of the decarbonisation process in industrial scale has been performed, leading to increased carbon dioxide removal and reduction of ammonia losses. (C) 2011 The Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.",2011,10.1016/j.cherd.2011.01.022,no
Modeling batch crystallization processes: Assumption verification and improvement of the parameter estimation quality through empirical experiment design,"In this work, experimental data of different batches was used for estimation of the kinetic parameters for the secondary nucleation framework of Gahn and Mersmann [Gahn, C. and Mersmann, A., 1999. Brittle fracture in crystallization processes. Chem. Eng. Sci. 54, 1273-1292]. An empirical experiment design procedure was used to design an informative batch experiment through optimization of the seed quality, size and mass and process conditions at seeding. The parameters estimated using the data of the designed experiment showed smaller magnitudes of the confidence ellipsoids and standard deviations as compared to those obtained by using the data of conventional (un) seeded batch experiments. It was shown that the designed experiment allowed reducing uncertainty in the initial conditions, namely, the mass and crystal size distribution of the initial population of crystals and the initial supersaturation. It was also demonstrated that the main reason for the model/process mismatch was the origin of nuclei. Dynamic experimental data could be described better if the state of the crystals forming the crystallization system corresponded to the assumptions of the used kinetic model. Differences in the crystal surface properties, shape, and strain content could be responsible for a divergent nucleation and growth behavior in batches that were initiated either by primary nucleation, seeding with small ground seeds or seeding with coarse crystals from the product of the previous batch. (c) 2011 Elsevier Ltd. All rights reserved.",2011,10.1016/j.ces.2011.06.049,no
Model-based Measurement of Particle Size Distributions in Layering Granulation Processes,"In this work, we present a method for the online estimation of particle size distributions in layering granulation processes using a methodology known as model-based measurement or state estimation. After presenting the necessary model equations for two practically relevant processes it is investigated which quantities of the processes have to be measured to estimate the size distribution. For the different processes square-root unscented Kalman filters (SR-UKF) are designed and tested in simulations to demonstrate the feasibility of this approach. (C) 2010 American Institute of Chemical Engineers AIChE J, 57: 929-941, 2011",2011,10.1002/aic.12314,no
MEASURING SOFTWARE FUNCTIONAL SIZE FROM BUSINESS PROCESS MODELS,"ISO 14143-1 specifies that a functional size measurement (FSM) method must provide measurement procedures to quantify the functional user requirements (FURs) of software. Such quantitative information, functional size, is typically used, for instance, in software estimation. One of the international standards for FSM is the COSMIC FSM method - ISO 19761 - which was designed to be applied both to the business application (BA) software domain and to the real-time software domain. A recurrent problem in FSM is the availability and quality of the inputs required for measurement purposes; that is, well documented FURs. Business process (BP) models, as they are commonly used to gather requirements from the early stages of a project, could be a valuable source of information for FSM. In a previous article, the feasibility of such an approach for the BA domain was analyzed using the Qualigram BP modeling notation. This paper complements that work by: (1) analyzing the use of BPMN for FSM in the BA domain; (2) presenting notation-independent guidelines for the BA domain; and (3) analyzing the possibility of using BP models to perform FSM in the real-time domain. The measurement results obtained from BP models are compared with those of previous FSM case studies.",2011,10.1142/S0218194011005359,no
Measuring the Weight of Egg with Image Processing and ANFIS Model,"It is clear that the egg is very important in human food basket. But a problem in food processing manufactures is measuring the weight of eggs as real time and it's difficult. One solution can be by using the camera. In this research we tried to measure the width and length of egg by real time image processing and then design and optimize an ANFIS model to find best relation between image processing outputs and the weight of egg. The correlation coefficient of experimental value for weight of egg and predicted value by ANFIS model is 0.9942. The result is very interesting and this idea is cheap, novel and practical.",2011,,no
Similarity of business process models: Metrics and evaluation,"It is common for large organizations to maintain repositories of business process models in order to document and to continuously improve their operations. Given such a repository, this paper deals with the problem of retrieving those models in the repository that most closely resemble a given process model or fragment thereof. Up to now, there is a notable research gap on comparing different approaches to this problem and on evaluating them in the same setting. Therefore, this paper presents three similarity metrics that can be used to answer queries on process repositories: (i) node matching similarity that compares the labels and attributes attached to process model elements; (ii) structural similarity that compares element labels as well as the topology of process models; and (iii) behavioral similarity that compares element labels as well as causal relations captured in the process model. These metrics are experimentally evaluated in terms of precision and recall. The results show that all three metrics yield comparable results, with structural similarity slightly outperforming the other two metrics. Also, all three metrics outperform text-based search engines when it comes to searching through a repository for similar business process models. (C) 2010 Elsevier B.V. All rights reserved.",2011,10.1016/j.is.2010.09.006,no
Mathematical models for process commonality under quality and resources breakdown in multistage production,"It is essential to manage customers' diverse desires and to keep manufacturing costs as low as possible for survival in competition and eventually in production. Sharing resources in manufacturing for different products is a vital method of accomplishing this goal. The advantages of using a common process in production are stated in the literature. However, the mathematical models as well as simulation or conceptual models are not sufficient. The main objective of this paper is to develop mathematical models for multiproduct and multistage production under quality and breakdown uncertainties. The idea of the process commonality is incorporated in the proposed models. The models are validated by primary data collected from a Malaysian company and comparison of the timely requirement schedules of earlier MRP II and the proposed models under stable and perfect production environments. An appreciable convergence of the outcomes is observed. However, the proposed models are carrying additional information about the available locations of the parts in a time frame. After validation, the effects of process commonality on cost, capacity and the requirement schedule under uncertainties are examined. It is observed that the use of common processes in manufacturing is always better than the non-commonality scenario in terms of production cost. However, the increase in capacity requirement for commonality designs is higher for an ideal system, while it is less when the system suffers from breakdowns and a quality problem.",2011,10.1631/jzus.A1000477,no
"NOx removal efficiency and ammonia selectivity during the NOx storage-reduction process over Pt/BaO(Fe, Mn, Ce)/Al2O3 model catalysts. Part II: Influence of Ce and Mn-Ce addition","It was previously demonstrated in the first part of this work that NOx storage-reduction process over Pt/BaO/Al2O3 model catalyst is limited by the reduction step, with ammonia emission since H-2 is not fully consumed. The stored NOx reacts preferentially with the introduced H-2 giving NH3, than with NH3 in order to produce N-2. Mn addition favors the NOx reduction with ammonia leading to better conversion and selectivity, but only at 400 degrees C. In Part II, a special attention was focused on the role of Ce and Mn-Ce addition in regard to the NOx conversion and the ammonia emission in the 200-400 degrees C temperature range. With ceria modified Pt/20Ba/Al catalyst, significant improvements are obtained from 300 degrees C. In addition to the enhancement of the NOx + NH3 reaction, the ammonia selectivity is maintained at a lower level compared with Pt/Ba(Mn)/Al catalysts, even in the case of a large H-2 excess. It is attributed to the ammonia oxidation into N-2 via the available oxygen at the catalyst surface. A synergetic effect is observed between Mn and Ce when they are added simultaneously in Pt/Ba/Al catalyst. (C) 2011 Elsevier B.V. All rights reserved.",2011,10.1016/j.apcatb.2010.12.043,no
Role of extracellular protease in nitrogen substrate management during antibiotic fermentation: a process model and experimental validation,"Kinetics of extracellular protease (ECP) production has typically been studied for processes that involve protease as a product. We argue that ECP is equally important in fermentations where protease is not a product of interest. Industrial fermentations typically use complex nitrogen substrates, which are proteolytically hydrolyzed to amino acids (AA) by ECP before assimilation. However, high AA concentrations may lead to nitrogen catabolite repression (NCR) of the products such as antibiotics. Thus, ECP plays a crucial role in managing the nitrogen substrate supply thereby affecting the antibiotic productivity. Here, we have studied the induction of ECP and its effect on the antibiotic productivity for a rifamycin B overproducer strain Amycolatopsis meditterranei S699. This organism produces ECP at the level of 14 U mL(-1) in complex media, which is sufficient for hydrolysis of proteins in the media but low compared to other ECP overproducers. We find ECP secretion to be repressed by ammonia, AA, and under conditions that support high growth rate. We propose a structured kinetic model which accounts for the kinetics of ECP secretion, amino acid availability, growth, and antibiotic production. In addition to the quantity, the timing of ECP induction was critical in achieving higher rifamycin productivity. We artificially created conditions that led to delayed protease secretion, which in turn led to premature termination of batch and lower productivity. The predictive value of the model can be useful in better management of the available nitrogen supply, minimization of NCR, and in the monitoring of fermentation batches.",2011,10.1007/s00253-011-3318-z,no
Biological removal of nitrate by an oil reservoir culture capable of autotrophic and heterotrophic activities: Kinetic evaluation and modeling of heterotrophic process,"Kinetics of heterotrophic denitrification was investigated using an oil reservoir culture with the ability to function under both autotrophic and heterotrophic conditions. In the batch system nitrate at concentrations up to 30 mM did not influence the kinetics but with 50 mM slower growth and removal rates were observed. A kinetic model, representing the denitrification as reduction of nitrate to nitrite, and subsequent reduction of nitrite to nitrous oxides and nitrogen gas was developed. The value of various kinetic coefficients, including maximum specific growth rate, saturation constant, yield and activation energy for nitrate and nitrite reductions were determined by fitting the experimental data into the developed model. In continuous bioreactors operated with 10 or 30 mM nitrate, complete removal of nitrate (no residual nitrite) and linear dependency between nitrate loading and removal rates were observed for loading rates up to 0.21 and 0.58 mM h(-1), respectively. The highest removal rates of 0.31 and 0.94 mM h(-1) observed at loading rates of 0.42 mM h(-1) and 1.26 mM h(-1), with corresponding removal percentages of nitrate and total nitrogen being 75.4, 54.4%, and 74.4 and 17.9%, respectively. Developed kinetic model predicted the performance of the continuous bioreactors with accuracy. (C) 2011 Elsevier B.V. All rights reserved.",2011,10.1016/j.jhazmat.2011.03.102,no
Quantifying eco-efficiency within life cycle management using a process model of strip coal mining,"Life cycle management can assist the mining industry in meeting its stated sustainability commitments. This paper demonstrates how sustainability metrics formulated from a life cycle perspective can be quantified at the operational level using a process model of open cut coal strip mining. An understanding of the material and energy flows of a working pit enables the identification of key areas to target for improvement. The effect of operational changes can also be modelled to determine changes in overall mine sustainability metrics, whether they are more local in nature, such as respirable particulates, or of global concern, such as greenhouse gas emissions. This can facilitate the adoption of more eco-efficient methods of operation, particularly where the focus is on maximising the output of the mine's ultimate utility; namely energy. An example is presented of a more eco-efficient method of blasting which reduces in-pit coal losses.",2011,10.1080/17480930.2011.553476,no
Modeling and Evaluating Non-shared Memory CELL/BE Type Multi-core Architectures for Local Image and Video Processing,"Local processing, which is a dominant type of processing in image and video applications, requires a huge computational power to be performed in real-time. However, processing locality, in space and/or in time, allows to exploit data parallelism and data reusing. Although it is possible to exploit these properties to achieve high performance image and video processing in multi-core processors, it is necessary to develop suitable models and parallel algorithms, in particular for non-shared memory architectures. This paper proposes an efficient and simple model for local image and video processing on non-shared memory multi-core architectures. This model adopts a single program multiple data approach, where data is distributed, processed and reused in an optimal way, regarding the data size, the number of cores and the local memory capacity. The model was experimentally evaluated by developing video local processing algorithms and programming the Cell Broadband Engine multi-core processor, namely for advanced video motion estimation and in-loop deblocking filtering. Furthermore, based on these experiences it is also addressed the main challenges of vectorization, and the reduction of branch mispredictions and computational load imbalances. The limits and advantages of the regular and adaptive algorithms are also discussed. Experimental results show the adequacy of the proposed model to perform local video processing, and that real-time is achieved even to process the most demanding parts of advanced video coding. Full-pixel motion estimation is performed over high resolution video (720x576 pixels) at a rate of 30 frames per second, by considering large search areas and five reference frames.",2011,10.1007/s11265-010-0463-z,no
Robust Data-Driven Modeling Approach for Real-Time Final Product Quality Prediction in Batch Process Operation,"Making on-specification products is a primary goal, and also a challenge in chemical batch process operation. Due to the uncertainty of raw materials and instability of operating conditions, it may not produce the desired on-spec final product. It would be helpful if one can predict the product quality during each operation, so that one can make adjustments to process conditions in order to make on-spec product. This paper addresses the issue of real-time prediction of final product quality during a batch operation. First, a data-driven modeling approach is presented. This multimodel approach uses available process information up to the current points to capture their time-varying relationships with the final product quality during the course of operation, so that the prognosis of product quality can be obtained in real-time. Then, due to its data-driven nature, the focus is given on how to make the models robust in order to eliminate the effect of noise, especially, outliers in the data. A model-based outlier detection method is presented. The proposed approach is applied to a generic chemical batch case study, with its prediction performance being evaluated.",2011,10.1109/TII.2010.2103401,no
The Research on Building up the Hierarchy Process and Fuzzy Evaluation Model and Evaluation System about the Cement Concrete Pavement on the Basis of the Fuzzy Mathematics Theory,"Making reference to the process of cement concrete pavement evaluation about current design, construction quality examination, highway maintenance management code, this paper builds up a hierarchy process and fuzzy evaluation model about the cement concrete pavement on the basis of the fuzzy mathematics theory, and confirms the weight of every process by the analytic hierarchy process, trying to elicit the import of each index, and sets up an evaluation system of the cement concrete pavement.",2011,10.4028/www.scientific.net/AMR.255-260.3228,no
A Conceptual Model for Assessing Quality of Care for Patients Boarding in the Emergency Department: Structure-Process-Outcome,"Many believe that the ""boarding'' of emergency department (ED) patients awaiting inpatient beds compromises quality of care. To better study the quality of care of boarded patients, one should identify and understand the mechanisms accounting for any potential differences in care. This paper presents a conceptual boarding ""structure-process-outcome'' model to help assess quality of care provided to boarded patients and to aid in recognizing potential solutions to improve that quality, if it is deficient. The goal of the conceptual model is to create a practical framework on which a research and policy agenda can be based to measure and improve quality of care for boarded patients.",2011,10.1111/j.1553-2712.2011.01033.x,no
Understanding hydrological processes and estimating model parameter values in large basins: the case of the Congo River basin,"Many large basins of the world are located in developing countries where the hydrometric networks are limited and where hydrological models have the potential to contribute to water resources management. However, it is difficult to ensure that models adequately represent the dominant hydrological processes, a problem further exacerbated by spatial scale issues and the typically large size of the modelling units. If models do not satisfactorily represent the hydrological processes, they may not be representing the runoff responses from ungauged areas and may not be useful for investigating the impacts of future water or land use developments. This paper reports on a study of the Congo River basin where the available stream flow data have been identified for 16 gauging stations within the total basin area of 3 680 000 km(2). The initial application of the model (Pitman monthly time-step model) involved manual calibration, which was followed by an exploration of the behavioural parameter sets in the context of the available basin physical property data (topography, drainage patterns, geology, soils, vegetation, etc.) in an attempt to constrain the plausible parameter sets to those that are conceptually realistic and consistent with real hydrological processes.",2011,,no
Security Validation of Business Processes via Model-Checking,"More and more industrial activities are captured through Business Processes (BPs). To evaluate whether a BP under-design enjoys certain security desiderata is hardly manageable by business analysts without tool support, as the BP runtime environment is highly dynamic (e.g., task delegation). Automated reasoning techniques such as model checking can provide the required level of assurance but suffer of well-known obstacles for the adoption in industrial systems, e.g. they require a strong logical and mathematical background. In this paper, we present a novel security validation approach for BPs that employs state-of-the-art. model checking techniques for evaluating security-relevant aspects of BPs in dynamic environments and offers accessible user interfaces and apprehensive feedback for business analysts so to be suitable for industry.",2011,,no
Carbon budget of tropical forests in Southeast Asia and the effects of deforestation: an approach using a process-based model and field measurements,"More reliable estimates of the carbon (C) stock within forest ecosystems and C emission induced by deforestation are urgently needed to mitigate the effects of emissions on climate change. A process-based terrestrial biogeochemical model (VISIT) was applied to tropical primary forests of two types (a seasonal dry forest in Thailand and a rainforest in Malaysia) and one agro-forest (an oil palm plantation in Malaysia) to estimate the C budget of tropical ecosystems in Southeast Asia, including the impacts of land-use conversion. The observed aboveground biomass in the seasonal dry tropical forest in Thailand (226.3 t C ha(-1)) and the rainforest in Malaysia (201.5 t C ha(-1)) indicate that tropical forests of Southeast Asia are among the most C-abundant ecosystems in the world. The model simulation results in rainforests were consistent with field data, except for the NEP, however, the VISIT model tended to underestimate C budget and stock in the seasonal dry tropical forest. The gross primary production (GPP) based on field observations ranged from 32.0 to 39.6 t C ha(-1) yr(-1) in the two primary forests, whereas the model slightly underestimated GPP (26.5-34.5 tC ha(-1) yr(-1)). The VISIT model appropriately captured the impacts of disturbances such as deforestation and land-use conversions on the C budget. Results of sensitivity analysis showed that the proportion of remaining residual debris was a key parameter determining the soil C budget after the deforestation event. According to the model simulation, the total C stock (total biomass and soil C) of the oil palm plantation was about 35% of the rainforest's C stock at 30 yr following initiation of the plantation. However, there were few field data of C budget and stock, especially in oil palm plantation. The C budget of each ecosystem must be evaluated over the long term using both the model simulations and observations to understand the effects of climate and land-use conversion on C budgets in tropical forest ecosystems.",2011,10.5194/bg-8-2635-2011,no
The First Passage Time Problem for Gauss-Diffusion Processes: Algorithmic Approaches and Applications to LIF Neuronal Model,"Motivated by some unsolved problems of biological interest, such as the description of firing probability densities for Leaky Integrate-and-Fire neuronal models, we consider the first-passage-time problem for Gauss-diffusion processes along the line of Mehr and McFadden (J R Stat Soc B 27:505-522, 1965). This is essentially based on a space-time transformation, originally due to Doob (Ann Math Stat 20:393-403, 1949), by which any Gauss-Markov process can expressed in terms of the standard Wiener process. Starting with an analysis that pinpoints certain properties of mean and autocovariance of a Gauss-Markov process, we are led to the formulation of some numerical and time-asymptotically analytical methods for evaluating first-passage-time probability density functions for Gauss-diffusion processes. Implementations for neuronal models under various parameter choices of biological significance confirm the expected excellent accuracy of our methods.",2011,10.1007/s11009-009-9132-8,no
A Bias-Dependent Model for the Impact of Process Variations on the SRAM Soft Error Immunity,"Nanometer SRAM cells are more susceptible to the particle strike soft errors and the increased statistical process variations, in advanced nanometer CMOS technologies. In this paper, an analytical model for the critical charge variations accounting for both die-to-die (D2D) and within-die (WID) variations, over a wide range of bias conditions, is proposed. The derived model is verified and compared to Monte Carlo simulations by using industrial hardware-calibrated 65-nm CMOS technology. This paper shows the impact of the coupling capacitor, one of the most common soft error mitigation techniques, on the critical charge variability. It demonstrates that the adoption of the coupling capacitor reduces the critical charge variability. The derived analytical model accounts for the impact of the supply voltage, from 0.1 to 1.2 V, on the critical charge and its variability.",2011,10.1109/TVLSI.2010.2068317,no
VIRTUAL QUALITY MANAGEMENT: A HOLISTIC APPROACH FOR THE SET-UP OF TRUE-TO-LIFE PROCESS MODELS AND ACCORDING VIRTUAL QUALITY MANAGEMENT TOOLS,"Nowadays innovative processes and process chains are optimized for the maximum output during the planning stage. But very often quality management techniques to control and monitor these optimized processes are not scheduled at that stage. Before the production line was not set up, quality management techniques are getting realized in the ramp-up phase or in the beginning of the series production. For this reason the production start-up is delayed seriously and high additional efforts are necessary, because many operational determinations have already been made so far and many basic conditions have to be accepted as unchangeable. Virtual Quality Management contains the capability for eliminating these severe deficits. Within this paper an overview will be given over the road map for the efficient set up quality-orientated process models and the reference model for developing virtual quality management techniques. Within the research work at the Chair Quality Management and Manufacturing Metrology in Erlangen ( Germany), different models have been created with an accuracy of more than 98 % in specific terms of condition. According to that, tools have been developed for simulation-based measurement system-, machine- and process-capability studies and on top, a tool for the simulation-based set-up and automated optimization of different types of quality control charts by the use of artificial intelligence techniques, has been provided.",2011,,no
"NOx removal efficiency and ammonia selectivity during the NOx storage-reduction process over Pt/BaO(Fe, Mn, Ce)/Al2O3 model catalysts. Part I: Influence of Fe and Mn addition","NOx storage-reduction process was studied using lean/rich cycling condition over Pt/BaO/Al2O3 model catalyst, with a special attention to the ammonia emission. The NOx reduction selectivity strongly depends on the hydrogen conversion introduced during the rich pulses. NH3 is emitted while hydrogen is not fully converted. It was concluded that the NH3 formation rate via the NOx reduction by H-2, is higher than the NH3 reaction rate with NOx to form N-2. The effect of H2O and CO2 on the reduction step was also examined and results were explained mainly taking into account the reverse water gas shift reaction. Fe addition in Pt/BaO/Al2O3 leads to a strong deactivation of the catalyst, probably due to interaction between iron and platinum. Mn is a poison for the reduction step at 200 and 300 degrees C, but it significantly enhances the NOx reduction at 400 degrees C (conversion and selectivity). Mn favors the NOx reduction with ammonia, even if the introduced hydrogen is not fully converted. (C) 2011 Elsevier B.V. All rights reserved.",2011,10.1016/j.apcatb.2010.12.040,no
Development and validation of a CFD model predicting the backfill process of a nuclear waste gallery,"Nuclear waste material may be stored in underground tunnels for long term storage. The example treated in this article is based on the current Belgian disposal concept for High-Level Waste (HLW), in which the nuclear waste material is packed in concrete shielded packages, called Supercontainers, which are inserted into these tunnels. After placement of the packages in the underground tunnels, the remaining voids between the packages and the tunnel lining is filled-up with a cement-based material called grout in order to encase the stored containers into the underground spacing. This encasement of the stored containers inside the tunnels is known as the backfill process. A good backfill process is necessary to stabilize the waste gallery against ground settlements. A numerical model to simulate the backfill process can help to improve and optimize the process by ensuring a homogeneous filling with no air voids and also optimization of the injection positions to achieve a homogeneous filling. The objective of the present work is to develop such a numerical code that can predict the backfill process well and validate the model against the available experiments and analytical solutions. In the present work the rheology of Grout is modelled as a Bingham fluid which is implemented in OpenFOAM - a finite volume-based open source computational fluid dynamics (CFD) tool box. Volume of fluid method (VOF) is used to track the interface between grout and air. The CFD model is validated and tested in three steps. First, the numerical implementation of the Bingham model is verified against an analytical solution for a channel flow. Second, the capability of the model for the prediction of the flow of grout is tested by means of a comparison of the simulations with experimental results from two standard flowability tests for concrete: the V-funnel flow time and slump flow tests. As a third step, the CFD model is compared with experiments in a transparent Plexiglas experimental test setup performed at Delft University of Technology, to test the model under more practical and realistic conditions. This experimental setup is a 1:12.5 scaled version of the setup of the full-scale mock-up test for backfilling of a waste gallery with emplaced canisters used in the European 6th framework project ESDRED (Bock et al., 2008). Furthermore, the plexiglas setup is used to study the influence of different backfill parameters. The CFD results for a channel flow shows good comparison against the analytical solution, demonstrating the correct implementation of the Bingham model in OpenFOAM. Also, the CFD results for the flowability tests show very good comparison with the experimental results, thereby ensuring a good prediction of the flow of grout. The simulations of the backfill process show good qualitative comparison with the plexiglas experiment. However, occurrence of segregation and also varying theological properties of the grout in the plexiglas experiment results in significant differences between the simulation and the experiment. (C) 2011 Elsevier B.V. All rights reserved.",2011,10.1016/j.nucengdes.2011.04.021,no
EVALUATION OF THE TRA ECETOC MODEL FOR INHALATION WORKPLACE EXPOSURE TO DIFFERENT ORGANIC SOLVENTS FOR SELECTED PROCESS CATEGORIES,"Objective: The aim of this work is to describe the operation principle of the TRA ECETOC model developed using the descriptor system, and the utilization of that model for assessment of inhalation exposures to different organic solvents for selected process categories identifying a given application. Method: Measurement results were available for toluene, ethyl acetate and acetone in workplace atmosphere in Poland. The following process categories have been postulated: (1) Paints and lacquers factory: use in closed, continuous process with occasional controlled exposure; (2) Shoe factory: roller or brush application of glues; (3) Refinery: use in closed process, no likelihood of exposure. The next step was to calculate the workplace concentration at chosen process categories by applying the TRA ECETOC model. Results: The selected categories do not precisely describe the studied applications. Very high concentration values of acetone were measured in the shoe factory, mean 443 ppm. The concentration obtained with the aid of the model is underestimated, ranging from 25.47 to 254.7 ppm, for the case with and without activation of the local exhaust ventilation (LEV), respectively. Estimated concentration at a level corresponding to that of the measured concentration would be possible if the process category involving spraying, e. g., PROC 7 was considered. For toluene and ethyl acetate, the measured concentrations are within the predicted ranges determined with the use of the model when we assume the concentration predicted with active ventilation for the beginning, and the concentration predicted with inactive ventilation for the end of the range. Conclusions: Model TRA ECETOC can be easily used to assess inhalation exposure at workplace. It has numerous advantages, its structure is clear, requires few data, is available free of charge. Selection of appropriate process categories related to the uses identified is guarantee of successful exposure assessment.",2011,10.2478/s13382-011-0021-3,no
A PSYCHOSOCIAL EVALUATION PROCESS FOR LIVING LIVER DONORS: THE UNIVERSITY OF ROCHESTER MODEL,"Objective: The donation of livers by living donors entails complex processes, both surgically and psychosocially, potentially involving risks in both domains. Thorough psychosocial evaluation is necessary to minimize those risks, yet little has been written about the donor assessment process. This article describes one such process, utilized by a transplant program in upstate New York. Method: Donor candidates undergo multiple psychosocial interviews early in the overall transplant evaluation process. Evaluators subsequently meet as a group, along with an independent ethicist, to determine psychosocial candidacy prior to final medical/surgical clearance. Results: Between 2003 and 2007, 416 donor candidates initiated and/or underwent full evaluation, resulting in a 17.5% surgery and 55.5% exclusion rate among those individuals. Of those ruled out, 20.8% were for (medical or psychosocial) reasons associated with the recipient, and 8.7% were for donor-related psychosocial issues. Conclusion: Given the primacy of psychosocial and ethical issues in living liver donor candidate evaluation, the multiple interview process, followed by team discussion and overseen by an ethicist removed from other transplant program functions, has advantages as a donor assessment model. (Int'l. J. Psychiatry in Medicine 2011;41:295-308)",2011,10.2190/PM.41.4.a,no
The Transtheoretical Model in Weight Management: Validation of the Processes of Change Questionnaire,"Objective: The processes of change implied in weight management remain unclear. The present study aimed to identify these processes by validating a questionnaire designed to assess processes of change (the P-Weight) in line with the transtheoretical model. The relationship of processes of change with stages of change and other external variables is also examined. Methods: Participants were 723 people from community and clinical settings in Barcelona. Their mean age was 32.07 (SD = 14.55) years; most of them were women (75.0%), and their mean BMI was 26.47 (SD = 8.52) kg/m(2). They all completed the P-Weight and the stages of change questionnaire (S-Weight), both applied to weight management, as well as two subscales from the Eating Disorders Inventory-2 and Eating Attitudes Test-40 questionnaires about the concern with dieting. Results: A 34-item version of the P-Weight was obtained by means of a refinement process. The principal components analysis applied to half of the sample identified four processes of change. A confirmatory factor analysis was then carried out with the other half of the sample, revealing that the model of four freely correlated first-order factors showed the best fit (GFI = 0.988, AGFI = 0.986, NFI = 0.986, and SRMR = 0.0559). Corrected item-total correlations (0.322-0.865) and Cronbach's alpha coefficients (0.781-0.960) were adequate. The relationship between the P-Weight and the S-Weight and the concern with dieting measures from other questionnaires supported the validity of the scale. Conclusion: The study identified processes of change involved in weight management and reports the adequate psychometric properties of the P-Weight. It also reveals the relationship between processes and stages of change and other external variables.",2011,10.1159/000335135,no
Validation of a theoretical model: knowing the interactive processes within the support network for people with tuberculosis,"Objective: To validate a theoretical model based on a study of interactive processes in the support network for people with tuberculosis. Methods: We used a Grounded Theory method; we opted for communicative validation, completed with six people with tuberculosis and three health professionals. Results: This validation was based on the presentation of a synthesis of the model to the participants, the analysis was performed from the perspective of application of the following criteria: adjust the theory to express the lived experience of reality people with tuberculosis - the model represented by the six components in the diagram, - theoretical generalization - for its conceptual interpretation and applicability to other realities. Conclusion: The experience of validating a theoretical model is challenging, however, the researcher arranged the components and categories that expressed the",2011,,no
Evaluating Translational Research: A Process Marker Model,"Objective: We examine the concept of translational research from the perspective of evaluators charged with assessing translational efforts. One of the major tasks for evaluators involved in translational research is to help assess efforts that aim to reduce the time it takes to move research to practice and health impacts. Another is to assess efforts that are intended to increase the rate and volume of translation. Methods: We offer an alternative to the dominant contemporary tendency to define translational research in terms of a series of discrete ""phases."" Results: We contend that this phased approach has been confusing and that it is insufficient as a basis for evaluation. Instead, we argue for the identification of key operational and measurable markers along a generalized process pathway from research to practice. Conclusions: This model provides a foundation for the evaluation of interventions designed to improve translational research and the integration of these findings into a field of translational studies. Clin Trans Sci 2011; Volume 4: 153-162",2011,10.1111/j.1752-8062.2011.00291.x,no
"A Modular Differential Dielectric Sensor for Use in Multiphase Separation, Process Measurement, and Control-Part I: Analytical Modeling","Oil industry increasingly demands accurate and stable continuous measurement of the percent water in crude oil production streams (watercut) over the entire 0 to 100% range. High accuracy and stability are also required for surface measurement to support niche applications such as control of processes which remove trace amounts of oil and particulates from produced water prior to disposal. Differential dielectric sensors (DDS) have been developed by Chevron as independent tools connected with multiphase meters for process management and composition measurement. This paper is a two-part paper-the first part (current paper) deals with analytical modeling of the DDS (configured in a single ended mode) and the second part (accompanying paper) discusses the results of key experimental investigations obtained in a differential mode. The main objective of this paper is to develop appropriate mathematical models for the DDS which characterize the microwave attenuation and phase shift as functions of fluid properties, sensor geometry and operational conditions. Forward models based on the analysis of microwave propagation have been developed for sensors configured as circular waveguides. Results of this project will be useful for optimization and refinement of multiphase meters. [DOI: 10.1115/1.4004978]",2011,10.1115/1.4004978,no
"A cokriging based approach to reconstruct air pollution maps, processing measurement station concentrations and deterministic model simulations","One of the aims of regional Environmental Authorities is to provide citizens information about the quality of the atmosphere over a certain region. To reach this objective Environmental Authorities need suitable tools to interpolate the data coming from monitoring networks to domain locations where no measures are available. In this work a spatial interpolation system has been developed to estimate 8-h mean daily maximum ozone concentrations and daily mean PM10 concentrations over a domain, starting from measured concentration values. The presented approach is based on a cokriging technique, using the results of a deterministic Chemical Transport Model (CTM) simulation as secondary variable. The developed methodology has been tested over a 60 x 60 km(2) domain located in Northern Italy, including Milan metropolitan area, one of the most polluted areas in Europe. (C) 2010 Elsevier Ltd. All rights reserved.",2011,10.1016/j.envsoft.2010.11.014,no
USING THE CULTURAL ADAPTATION PROCESS (CAP) MODEL TO EVALUATE ONLINE COURSES FOR GRADES K-12 FOR SPANISH-SPEAKERS IN LATIN AMERICA,"Online learning is increasingly becoming part of the mainstream in grades K-12. Globalization efforts are forcing more and more organizations to reach diverse learners from around the world. As a result, there is a need to understand the relationship between culture and online learning. This research examines Hofstede's framework of cultural dimensions within Edmundson's cultural adaptation process (CAP) model to assess the need to translate, localize, modularize or originate K-12 online courses originally created in English for Spanish-speakers in Latin America. It will draw implications for the process of instructional design when cultural differences are taken into consideration. The authors will propose modifications to the cultural adaptation process (CAP) model to adapt it specifically for K-12 online courses by replacing the content types with the knowledge dimension of the revised Bloom's Taxonomy. The goal is to ensure that e-learning courses targeted for Latin America are culturally adapted and optimized to help learners meet their learning outcomes.",2011,,no
Quality prediction for polypropylene production process based on CLGPR model,"Online measurement of the melt index is typically unavailable in industrial polypropylene production processes, soft sensing models are therefore required for estimation and prediction of this important quality variable. Polymerization is a highly nonlinear process, which usually produces products with multiple quality grades. In the present paper, an effective soft sensor, named combined local Gaussian process regression (CLGPR), is developed for prediction of the melt index. While the introduced Gaussian process regression model can well address the high nonlinearity of the process data in each operation mode, the local modeling structure can be effectively extended to processes with multiple operation modes. Feasibility and efficiency of the proposed soft sensor are demonstrated through the application to an industrial polypropylene production process. (C) 2011 Elsevier Ltd. All rights reserved.",2011,10.1016/j.conengprac.2011.01.002,no
Iterative optimization of the economic efficiency of an industrial process within the validity area of the static plant model and its application to a Pulp Mill,"Optimization of the steady state economic efficiency of an industrial process is a specific task because the decision variables of the optimization (setpoints of the control system) affect the process through the control strategy. Thus, the effects of saturation of a control system must be taken into account when the gradient of the objective function is estimated and the necessary optimality conditions are checked. In particular, because the optimality conditions cannot be checked directly in the presence of active constraints on the manipulated variables, approximations of the steady state values of the manipulated variables as functions of the setpoints (static plant model) are needed in order to be able to evaluate the optimality conditions. In this paper an iterative method for optimization of the plant profit rate is proposed avoiding the control saturation and is applied to the Pulp Mill benchmark model optimization. Three different static models describing the steady state values of the manipulated variables are constructed and used in the optimization. The results of the optimization are presented and compared against the straightforward single-step optimization of the plant economic efficiency. (c) 2010 Elsevier Ltd. All rights reserved.",2011,10.1016/j.compchemeng.2010.10.010,no
Applying Analytic Hierarchy Process-Technique for Order Preference by Similarity to Ideal Solution (AHP-TOPSIS) model to evaluate individual investment performance of retirement planning policy,"Owing to aged people become more and more and serious economy depression in the present society in Taiwan, how retirees will keep their past consumption level after their retirement is an important issue. The purpose of this research will appraise the performance of individual investment planning policy for retirement. Because the investment performance, risks, taxation etc must be considered in making investment policy choices, the methodology of this article will apply the analytic hierarchy process (AHP) (Satty, 1980), which is a multi-criteria decision making technique and use the technique for order preference by similarity to ideal solution (TOPSIS) to select the optimal individual retirement investment planning policy to determine the effectiveness of the proposed evaluation model. The analytic hierarchy process is to be used to establish quantitative and qualitative criteria and design a framework of an assessment method for evaluating individual investment policy performance in this research. These research results will provide some suggestions for investors retirement planning.",2011,,no
Modification of Petri nets modeling production processes when quality control is introduced,"Petri nets can be used as a model of production processes, as far as such processes often contain the steps which can be executed in parallel. Such modeling can serve for optimization and verification of production processes, because a wide range of existing methods of Petri net analysis can be applied. Adding to a production process the steps in which quality control is performed requires changes in the modeling net to keep it well-formed. Such modifications are the topic of the article. The production processes are modeled by s-nets and alpha-nets.",2011,,no
"Modeling food process, quality and safety: Frameworks and practical aspects","Physics-based models provide increased understanding and predictive capabilities that can increase efficiency in food product, process, and equipment design, and improve quality and safety. However, certain key food specific developments are needed to enable widespread use of simulation technology in the food sector. First and foremost need is the development of concise modeling frameworks (how to formulate various food processing situations into mathematical models) for various classes of processes, as opposed to a custom model for each process, as it mostly exists today. Deformable porous media with multiphase transport can provide such framework, as will be discussed through examples of various processes that have been modeled by many researchers. The next critical piece is to have easy access to properties needed to model. State of property prediction, starting from simple correlations to multiscale modeling, thermodynamics-based, and molecular dynamics, as being pursued by researchers around the world will be shared. Prediction beyond process, into quality and safety is the third topic where various approaches to modeling quality in diffusion-reaction modeling framework would be presented. For safety, a practical approach that groups various food products and thus provide an avenue to simulate safety for great many situations, will be shared. Finally, efforts to integrate the modeling components into a novel user-friendly software for increased use of modeling will be described. (c) 2011 Published by Elsevier B.V. Selection and/or peer-review under responsibility of 11th International Congress on Engineering and Food (ICEF 11) Executive Committee.",2011,10.1016/j.profoo.2011.09.179,no
Challenges for evaluating process-based models of gas exchange at forest sites with fetches of various species,"Physiologically-based (or process-based) models are commonly applied to describe plant responses mechanistically in dependence on environmental conditions. They are increasingly evaluated with eddy-covariance measurements that integrate carbon and water exchange of an area of several hectares (called the fetch). However, almost all models applied to date in such exercises have considered only the dominant tree species and neglected other species that contributed to the measured gas exchange rates-either in separate patches or in mixture. This decreases the transferability of the model from one site to another because the contributions from other species might be different. It is therefore a major challenge in modeling today to separate the measured gas exchanges by sources. In this study, a detailed physiologically-based biosphere model is applied that allows distinguishing between tree species in mixed forests, considering them as vegetation cohorts that interact with each other. The sensitivity of the model to different assumptions about how different tree species contribute to an integrated measurement of stand-scale gas exchange is investigated. The model exercise is carried out for a forest site in Finland with dominant Scots pine but presence of significant amounts of Norway spruce and birch. The results demonstrate that forest structure affects simulated gas exchange rates indicating a possible importance of considering differences in physiological properties at the species level. It is argued that the variation of stand structure within the range of eddy-covariance measurements should be better accounted for in models and that inventory measurements need to consider this variation.",2011,10.5424/fs/20112003-11084,no
Study on Construction Order of Logistics Park Based on Combined Model of Analytic Hierarchy Process and Gray Comprehensive Evaluation,"Priority order decision of logistics parks construction project is an important issue in the process of logistics park planning. It needs to synthesize various factors, such as, politics, economy, society and environment, etc. Because there are some grey relationships among these factors, construction of the order of questions can be look on as a grey multi-objective evaluation. This paper brings forward a new priority decision method named hierarchy -gray comprehensive evaluation model to evaluate the building order of the logistics parks. An example is used to elaborate how to use this model; meanwhile, the method is proved more scientific and reliable.",2011,10.4028/www.scientific.net/AMM.97-98.644,no
Identification and Classification of Human Error in Process Model Development,"Process models capture important corporate know-how for an effective knowledge management (KM). However, many process models do not match with corporate reality and therefore cannot serve the intended purpose. Human Error is a major source for these inconsistencies that might hinder process implementation and maintenance as well as continuous improvement efforts. The approach presented in this paper accounts for a more active participation of employees in order to further increase the economical benefit of KM. To this purpose, a human error analysis in process modeling was conducted. The results derived from data of 64 subjects show that errors of omission and erroneous execution on an activity level are considerably higher for novices than for subjects with theoretical knowledge and experienced modelers. However, it can be concluded that even for experienced modelers complex modeling scenarios are prone to reasoning fallacies and thus represent a possible source for model inconsistencies in corporate practice.",2011,,no
Evaluation of Process Variations on OPC Model Predictions,"Process models have been in use for performing proximity corrections to designs for placement on lithography masks for a number of years. In order for these models to be used they must provide an adequate representation of the process while also allowing the corrections themselves to be performed in a reasonable computational time. In what is becoming standard Optical Proximity Correction (OPC), the models used have a largely physical optical model combined with a largely empirical resist model. Normally, wafer data is collected and fit to a model form that is found to be suitable through experience. Certain process variables are considered carefully in the calibration process-such as exposure dose and defocus - while other variables - such film thickness and optical parameter variations are often not considered. As the semiconductor industry continues to march toward smaller and smaller dimensions - with smaller tolerance to error-we must consider the importance of those process variations. In the present work we describe the results of experiments performed in simulations to examine the importance of many of those process variables which are often regarded as fixed. We show examples of the relative importance of the different variables.",2011,10.1117/12.897531,no
Quality Control via Model-Free Optimization for a Type of Batch Process with a Short Cycle Time and Low Operational Cost,"Proper settings of key process variables are critical to the product quality control of mass-producing batch processes. The most widely used method in searching for the optimal process condition is the model-based optimization method (MBO). However, model development could be a challenging task in many cases. The accuracy of the model may deteriorate when the process conditions are changed. A systematic, model-free optimization method (MFO) for a type of batch process with a short cycle time and low operational cost is proposed to improve the efficiency of quality control. Instead of building a quality model, a direct search for the optimum process condition using experimental measurements is applied. Optimization algorithm is implemented as well to improve the search efficiency; both the gradient-based and the gradient-free optimization methods are discussed. The simultaneous perturbation stochastic approximation (SPSA) and the simplex search algorithm are incorporated in the MFO. The NEED method was applied to the quality control of injection molding process for demonstration using the part weight, part dimension, and focal length of molded products as quality measurements. A comparison with the Kriging modeling and optimization technique is also presented. The experimental results proved the effectiveness of the MFO technique.",2011,10.1021/ie1016927,no
Fuzzy Nonlinear Models for New Product Development Using Four-Phase Quality Function Deployment Processes,"Quality function deployment (QFD) frameworks are useful tools for constructing a new product development (NPD) plan that enables the clear itemization of customer needs and the systematic evaluation of each solution to maximize customer satisfaction. A complete QFD process includes four sequential phases in which four important decision outcomes are determined for NPD, namely, the fulfillment levels of design requirements (DRs), part characteristics, process parameters, and production requirements. Unlike prior studies which have focused only on determining DRs, this paper extends Chen and Ko's models to consider the close link between the four phases in NPD using the means-end chain concept to build up a series of fuzzy nonlinear programming models for determining the fulfillment levels of each decision outcome for customer satisfaction. In addition, this paper incorporates risk analysis, which is treated as the constraint in the models, into the QFD process. To deal with the vague nature of product development processes, fuzzy sets are applied for both QFD and risk analysis. A numerical example is used to demonstrate the applicability of the proposed model.",2011,10.1109/TSMCA.2010.2104140,no
Application of 6-sigma design system to developing an improvement model for multi-process multi-characteristic product quality,"Rapid developments in advanced manufacturing techniques and information technology have resulted in significant increases in the functionality, precision, and complexity of products. Most products have multiple quality characteristics, which may include multiple unilateral and bilateral specifications. The process capability for any quality characteristic must be satisfied if customers are to accept a product. The production process of complex products generally consists of several stages and processes. The qualities of these processes can be independent or interdependent. This paper develops an integrated assessment model for the manufacture of multi-process, multi-characteristic products based on the process capability index and process yield rate. The proposed method assess six-sigma improvement using the 'define, measure, analyse, design, verify' model. This study investigates the manufacturing processes of 12.7 mm ordinary-grade machine gun cartridges in order to increase their shooting precision. The proposed assessment model identifies the critical-to-quality processes that affect the precision, and then applies redesign and experimental design to obtain the optimal combination of process parameters. Finally, a mathematical programming method verifies the results, confirming an improvement in shooting precision. The resulting cartridges meet the customer demand for match quality.",2011,10.1177/2041297510393464,no
Understanding ignition processes in spray-guided gasoline engines using high-speed imaging and the extended spark-ignition model SparkCIMM Part B: Importance of molecular fuel properties in early flame front propagation,"Recent high-speed imaging of ignition processes in spray-guided gasoline engines has motivated the development of the physically-based spark channel ignition monitoring model SparkCIMM, which bridges the gap between a detailed spray and vaporization model and a model for fully developed turbulent combustion. Previously, both SparkCIMM and high-speed optical imaging data have shown that, in spray-guided engines, large variations in turbulence intensity, equivalence ratio, and enthalpy along the stretched and wrinkled spark plasma channel favor localized ignition spot formations in rich-mixture regions. In combination with strong local flow velocity, multiple successful ignition events along the re-striking spark lead to early non-spherical turbulent flame fronts. In this paper, SparkCIMM is enhanced by: (1) criteria to capture localized flame extinction phenomena, (2) a formulation of early flame kernel propagation based on the G-equation theory that includes effects of non-unity Lewis numbers, and (3) an extended equation to compute turbulent burning velocities of stretched flames in stratified mixtures. Localized rich ignition along the spark leads to early flames, whose propagation is, due to initially small turbulent Damkohler numbers, significantly influenced by molecular fuel properties. The analysis reveals that non-unity Lewis number curvature effects, intensified by heavy dilution by exhaust gas recirculation, strongly affect the early flame-kernel development in spray-guided gasoline engines. In particular, these effects significantly bias the flammability limit of flame kernels towards rich-mixtures while inhibiting their propagation in lean regions. Favorable initial conditions for combustion are found in rich-mixture regions, albeit in the presence of substantial equivalence ratio fluctuations and scalar dissipation rates. This paper demonstrates that the full complexity of the model equations developed here is required to reproduce the characteristic experimental features (spark channel stretching, multiple re-strikes, localized flame kernel formation, and early turbulent flame front corrugation) of spray-guided ignition phenomena. Published by Elsevier Inc. on behalf of The Combustion Institute.",2011,10.1016/j.combustflame.2011.04.003,no
Understanding ignition processes in spray-guided gasoline engines using high-speed imaging and the extended spark-ignition model SparkCIMM. Part A: Spark channel processes and the turbulent flame front propagation,"Recent high-speed imaging of ignition processes in spray-guided gasoline engines has motivated the development of the physically-based spark channel ignition monitoring model SparkCIMM, which bridges the gap between a detailed spray/vaporization model and a model for fully developed turbulent flame front propagation. Previously, both SparkCIMM and high-speed optical imaging data have shown that, in spray-guided engines, the spark plasma channel is stretched and wrinkled by the local turbulence, excessive stretching results in spark re-strikes, large variations occur in turbulence intensity and local equivalence ratio along the spark channel, and ignition occurs in localized regions along the spark channel (based upon a Karlovitz-number criteria). In this paper, SparkCIMM is enhanced by: (1) an extended flamelet model to predict localized ignition spots along the spark plasma channel, (2) a detailed chemical mechanism for gasoline surrogate oxidation, and (3) a formulation of early flame kernel propagation based on the G-equation theory that includes detailed chemistry and a local enthalpy flamelet model to consider turbulent enthalpy fluctuations. In agreement with new experimental data from broadband spark and hot soot luminosity imaging, the model establishes that ignition prefers to occur in fuel-rich regions along the spark channel. In this highly-turbulent highly-stratified environment, these ignition spots burn as quasi-laminar flame kernels. In this paper, the laminar burning velocities and flame thicknesses of these kernels are calculated along the mean turbulent flame front, using tabulated detailed chemistry flamelets over a wide range of stoichiometry and exhaust gas dilution. The criteria for flame propagation include chemical (cross-over temperature based) and turbulence (Karlovitz-number based) effects. Numerical simulations using ignition models of different physical complexity demonstrate the significance of turbulent mixture fraction and enthalpy fluctuations in the prediction of early flame front propagation. A third paper on SparkCIMM (companion paper to this one) focuses on the importance of molecular fuel properties and flame curvature on early flame propagation and compares computed flame propagation with high speed combustion imaging and computed heat release rates with cylinder pressure analysis. The goals of SparkCIMM development are to (a) enhance our fundamental understanding of ignition and combustion processes in highly-turbulent highly-stratified engine conditions, (b) incorporate that understanding into a physically-based submodel for RANS engine calculations that can be reliably used without modification for a wide range of conditions (i.e., homogeneous or stratified, low or high turbulence, low or high dilution), and (c) provide a submodel that can be incorporated into a future LES model for physically-based modeling of cycle-to-cycle variability in engines. Published by Elsevier Inc. on behalf of The Combustion Institute.",2011,10.1016/j.combustflame.2011.03.012,no
Computational Exploration of Metaphor Comprehension Processes Using a Semantic Space Model,"Recent metaphor research has revealed that metaphor comprehension involves both categorization and comparison processes. This finding has triggered the following central question: Which property determines the choice between these two processes for metaphor comprehension? Three competing views have been proposed to answer this question: the conventionality view (<link rid=""b7"">Bowdle & Gentner, 2005), aptness view (<link rid=""b31"">Glucksberg & Haught, 2006b), and interpretive diversity view (<link rid=""b92"">Utsumi, 2007); these views, respectively, argue that vehicle conventionality, metaphor aptness, and interpretive diversity determine the choice between the categorization and comparison processes. This article attempts to answer the question regarding which views are plausible by using cognitive modeling and computer simulation based on a semantic space model. In the simulation experiment, categorization and comparison processes are modeled in a semantic space constructed by latent semantic analysis. These two models receive word vectors for the constituent words of a metaphor and compute a vector for the metaphorical meaning. The resulting vectors can be evaluated according to the degree to which they mimic the human interpretation of the same metaphor; the maximum likelihood estimation determines which of the two models better explains the human interpretation. The result of the model selection is then predicted by three metaphor properties (i.e., vehicle conventionality, aptness, and interpretive diversity) to test the three views. The simulation experiment for Japanese metaphors demonstrates that both interpretive diversity and vehicle conventionality affect the choice between the two processes. On the other hand, it is found that metaphor aptness does not affect this choice. This result can be treated as computational evidence supporting the interpretive diversity and conventionality views.",2011,10.1111/j.1551-6709.2010.01144.x,no
A Fast and Accurate Process Variation-Aware Modeling Technique for Resistive Bridge Defects,"Recent research has shown that tests generated without taking process variation into account may lead to loss of test quality. At present, there is no efficient device-level modeling technique that models the effect of process variation on resistive bridge defects. This paper presents a fast and accurate technique to achieve this, including modeling the effect of voltage and temperature variation using the BSIM4 transistor model. To speed up the computation time and without compromising simulation accuracy (achieved through BSIM4), two efficient voltage approximation algorithms are proposed for calculating logic threshold of driven gates and voltages on bridged lines of a fault-site to calculate bridge critical resistance. Experiments are conducted on a 65 nm gate library (for illustration purposes), and results show that on average the proposed modeling technique is more than 53 times faster and in the worst case, error in bridge critical resistance is 2.64% when compared with HSPICE.",2011,10.1109/TCAD.2011.2162065,no
Petri-net models for comprehensive hazard analysis of MOCVD processes,"Reducing contamination level is of primary importance for the safety and efficiency of a MOCVD process. Off-line fault identification is one of the basic tasks that must be performed in hazard analysis to identify potential operational problems. For illustration convenience, the scope of present study is limited to the purge-gas purifier of the process. A systematic step-by-step procedure is proposed in this paper to construct Petri nets for modeling the purification system. Efficient hazard assessment studies have been performed by simulating the fault propagation behaviors on the basis of the system model. A comprehensive list of possible fault origins has been thoroughly examined to demonstrate the effectiveness of the proposed approach. (C) 2010 Elsevier Ltd. All rights reserved.",2011,10.1016/j.compchemeng.2010.05.006,no
Use of a process analysis tool for diagnostic study on fine particulate matter predictions in the U.S. - Part I: Model evaluation,"Regional ozone (O-3) and fine particles (PM2.5) modeling for both research-grade and regulation applications is important due to their known impacts on human health, air quality, and climate change. In this study, the fifth-generation Penn State/NCAR Mesoscale Model (MM5) and the U. S. EPA Models-3/Community Multiscale Air Quality (CMAQ) modeling system are applied to simulate the major air pollutants during the 1999 Southern Oxidants Study episode for the period of 12-28 June 1999. As Part I of two companion papers describing CMAQ performance, process analysis, and sensitivity simulations, this paper presents results from an operational evaluation for meteorological and chemical predictions using the available surface, aircraft, and satellite data. Both MM5 and CMAQ show reasonable performance for major meteorological variables (i.e., temperature, relative humidity, wind direction, planetary boundary layer height) with normalized mean biases (NMBs) of 0.4-24.2%, surface concentrations O-3, PM2.5, SO42-, and NH4+ with NMBs of -39% to 24.2%, and vertical profiles of temperature and sulfur dioxide. Relatively poor performance is found in the simulated precipitation (NMBs of -16.3% to 37.4%), the concentrations of NO3-, EC, and OC (NMBs of -77.8% to -22%) and total O-3 column mass. The evaluation identifies several research areas that are needed to improve model performance for nitrate, organic carbon, and black carbon at surface, vertical profiles of relative humidity, carbon monoxide, and nitrogen oxides, and tropospheric O-3 column abundance. (C) Author(s) 2011. This work is distributed under the Creative Commons Attribution 3.0 License.",2011,10.5094/APR.2011.007,no
Comprehensive flood-hydrological assessment of ungauged catchments in the Swiss part of the River Rhine basin by means of process-oriented modelling,"Reliable estimates of floods in ungauged catchments are indispensable prerequisites for modern flood protection. The stochastic and empiric estimation methods used today, however, exhibit considerable uncertainties and provide only peak-flow estimates. We introduce a procedure that aims at improving flood estimation through continuous simulations of long-term hydrographs with the hydrological modelling system PREVAH. The resulting simulated hydrographs are available for flood estimation by means of conventional extreme-value statistics and, moreover, they provide a basis for the comprehensive plausibility assessment of the results. We present results from 450 meso-scale and mostly ungauged catchments in the Swiss part of the River Rhine catchment. This is one of the first comprehensive applications of the continuous simulation approach at hourly time step. We show that the procedure achieves good results and is suitable for extending considerably the variability of the current estimation procedures. Thanks to the process orientation of the modelling system, additional promising applications become feasible, such as studies in climate and land-use changes or worst-case studies.",2011,,no
POISSON REPRESENTATIONS OF BRANCHING MARKOV AND MEASURE-VALUED BRANCHING PROCESSES,"Representations of branching Markov processes and their measure-valued limits in terms of countable systems of particles are constructed for models with spatially varying birth and death rates. Each particle has a location and a ""level,"" but unlike earlier constructions, the levels change with time. In fact, death of a particle occurs only when the level of the particle crosses a specified level r, or for the limiting models, hits infinity. For branching Markov processes, at each time t conditioned on the state of the process, the levels are independent and uniformly distributed on [0, r]. For the limiting measure-valued process, at each time t "" the joint distribution of locations and levels is conditionally Poisson distributed with mean measure K(t) x Lambda. where Lambda denotes Lebesgue measure, and K is the desired measure-valued process. The representation simplifies or gives alternative proofs for a variety of calculations and results including conditioning on extinction or nonextinction, Harris's convergence theorem for supercritical branching processes, and diffusion approximations for processes in random environments.",2011,10.1214/10-AOP574,no
QUALITY MANAGEMENT SYSTEM IN TOURIST COMMUNITY: METHODOLOGY OF PROCESS MODELLING,"Requirements of interested parties, amongst which the customer has the central position, are starting points of a quality management system that complies with the requirements of the ISO 9001:2008 international standards. Regardless of the fact whether we warn to arrange a management system to meet the requirements of this standard, one should be able to recognize exact requests of interested parties. Classic structural model of organization must be upgraded through developed, documented and implemented processes. That is very important for all organizations, and tourist communities of places, communes, regions, counties and cities, too. That is the central topic of hypothesis. Especially in Croatia as a tourist country and this detail can provide competitiveness for Croatian tourism on the global tourist market. Processes are one of structural elements of each management system. Therefore, the third chapter is devoted to the definition of the process, the typology of business processes and the process approach. Scientists and experts do not agree about generally accepted methodology of process modelling. In this paper author offers IDEF0 diagram as a method for business process display. The chapter four brings the methodology of business processes modelling. This method can be used in process modelling as a part of quality management system project in tourist community organizations, too. The fifth chapter explains the economic impact of tourist boards' quality management system to the economy of destination.",2011,,no
Reverse electrodialysis: A validated process model for design and optimization,Reverse electrodialysis (RED) is a technology to generate electricity using the entropy of the mixing of sea and river water. A model is made of the RED process and validated experimentally. The model is used to design and optimize the RED process. It predicts very small differences between counter- and co-current operation. It was decided to focus on co-current design because co-current operation causes smaller local pressure differences between the river and seawater compartments-hence smaller risk of leakages and the possibility to use very thin membranes with high fluxes and very open spacer structures with low hydrodynamic resistance. Segmentation of the electrodes proved to increase the power density by about 15% under realistic operational conditions. The model shows that with smaller systems - in terms of length of the flow path - higher power densities are possible. This effect is rather dramatical and poses a challenge for designing improved RED stacks on large commercial scale. It is suggested to reduce the flow path length by applying a fractal structure design of the spacers. Such structures can be made by profiling the membrane. (C) 2010 Elsevier B.V. All rights reserved.,2011,10.1016/j.cej.2010.10.071,no
Life cycle VV&A simulation modeling process based on validation,"Simulation modeling VV&A is verification, validation and accreditation. In the field of petrochemical industry, focus of VV&A still lays on the model validation, and people always do appropriate model verification, validation work after model development. It does not yet formed a complete hierarchy or framework for effective implementation of life cycle VV&A. This paper referenced the research achievement of military simulation modeling and test-driven software development technology, introduced the modeling VV&A technology into the whole life of the model development. Simulation modeling process based on validation discussed in this paper is totally based on validation, and it is carried out closely around VV&A from appearance of the first idea to the final application. As a result, people can completely control the whole process of model development. At the same time of improving model accuracy, reliability and credibility, such process could greatly improve the efficiency of model development and effectively reduce the model development period. And this process can be also applied to system-level, complex object.",2011,10.4028/www.scientific.net/AMM.80-81.511,no
Critical Problems in Validation Process of Simulation Models,"Simulation models are increasingly being used to solve more and more complex problems and to aid in decision-making. To provide a realistic and confident simulation environment for users, simulation models have become key components in military simulations. This paper discusses the modeling nature of simulation models, and then the modified validation criteria for measuring the agreements between Subject Matter Experts and simulation models are presented. Furthermore, validation methods such as graphical comparison, feature analysis, face validation, confidence interval and hypothesis tests of three types errors, are discussed according to the validation metrics of simulation models. Simulation models could be validated based on the proposed validation process effectively. The proposed process could be applied to the simulation systems and solve many VV&A difficulties. Example of the mass moment missile, illustrates the validity of the proposed process.",2011,10.4028/www.scientific.net/AMR.187.422,no
Use of a Computer-Mediated Delphi Process to Validate a Mass Casualty Conceptual Model,"Since the original work on the Delphi technique, multiple versions have been developed and used in research and industry; however, very little empirical research has been conducted that evaluates the efficacy of using online computer, Internet, and e-mail applications to facilitate a Delphi method that can be used to validate theoretical models. The purpose of this research was to develop computer, Internet, and e-mail applications to facilitate a modified Delphi technique through which experts provide validation for a proposed conceptual model that describes the information needs for a mass-casualty continuum of care. Extant literature and existing theoretical models provided the basis for model development. Two rounds of the Delphi process were needed to satisfy the criteria for consensus and/or stability related to the constructs, relationships, and indicators in the model. The majority of experts rated the online processes favorably (mean of 6.1 on a seven-point scale). Using online Internet and computer applications to facilitate a modified Delphi process offers much promise for future research involving model building or validation. The online Delphi process provided an effective methodology for identifying and describing the complex series of events and contextual factors that influence the way we respond to disasters.",2011,10.1097/NCN.0b013e3181fc3e59,no
An Evaluation Model for Software Reuse Processes,"Software reuse is a major concern in many software development companies. It is one of the main strategies used to reduce the cost of software product development. Studies show that the reuse strategy is the most significant strategy in terms of effort and quality. That it could save the half of the software development effort and increase the quality of the software product. Different ways of software reuse are proposed and discussed. In this study, an evaluation model for software reuse is proposed. The model is developed in order to consider the new methods of software reuse. That developed based on the framework of develop a reusable software components through software development processes. The model is proposed in order to present the applicable methods of software reuse and to evaluate their cost.",2011,,no
Multicriteria Optimization Model for Supply Process Problem under Provision and Demand Uncertainty,"Supply processes play an important role in customer satisfaction and company costs. The main characteristics of this problem are given by several decisions that follow a hierarchical structure and a very uncertain context, conditioning the success of the solutions proposed. Two significant sources of uncertainty are considered in this work, namely, provision and demand, both modeled as exogenous variables with random behavior. An optimization model is formulated to reduce the effects of the uncertainty in the company supply process. Because of the problem complexity, a multicriteria model is required to bring a comprehensive, solution. Several Pareto-optimal solutions are obtained through application of the epsilon-constraint technique. The original formulation is a nonconvex one that is then transformed to obtain a disjunctive linear model that guarantees a global result.",2011,10.1021/ie2005548,no
Structural-Model Approach of Causal reasoning in problem solving processes,"Taking into account the experience feedback on complex problems solving in industrial organizations is a way to improve the quality of products and processes. However, few academic works deal with representation and instrumentation of experience feedback systems. We propose in this paper a model of experiences and mechanisms to use these experiences. More specifically, we wish to promote the reuse of expert analysis already performed to propose an a priori analysis to address a new problem. The proposed approach is based on a representation of the context of the experience using conceptual markers and a conceptual representation of the analysis explicitly incorporating expert opinions and their combination.",2011,,no
Modelling Text File Evaluation Processes,Text file evaluation is an emergent topic in e-learning that responds to the shortcomings of the assessment based on questions with predefined answers. Questions with predefined answers are formalized in languages such as IMS Question & Test Interoperability Specification (QTI) and supported by many e-learning systems. Complex evaluation domains justify the development of specialized evaluators that participate in several business processes. The goal of this paper is to formalize the concept of a text file evaluation in the scope of the E-Framework a service oriented framework for development of e-learning systems maintained by a community of practice. The contribution includes an abstract service type and a service usage model. The former describes the generic capabilities of a text file evaluation service. The later is a business process involving a set of services such as repositories of learning objects and learning management systems.,2011,,no
Validation of the STRIVE model for coupling ecological processes and surface water flow,"The 1D model package STRIVE is verified for simulating the interaction between ecological processes and surface water flow. The model is general and can be adapted and further developed according to the research question. The hydraulic module, based on the Saint-Venant equations, is the core part. The presence of macrophytes influences the water quality and the discharge due to the flow resistance of the river, expressed by Manning's coefficient, and allows an ecological description of the river processes. Based on the advection-dispersion equation, water quality parameters are incorporated and modelled. Calculation of the water quantity parameters, coupled with water quality and inherent validation and sensitivity analysis, is the main goal of this research. An important study area is the River Aa near Poederlee (Belgium), a lowland river with a wealth of vegetation growth, where discharge and vegetation measurements are carried out on a regular basis. The developed STRIVE model shows good and accurate calculation results. The work highlights the possibility of STRIVE to model flow processes, water quality aspects and ecological interaction combined and separately. Coupling of discharges, water levels, amount of biomass and tracer values provides a powerful prediction modelling tool for the ecological behaviour of lowland rivers.",2011,10.2166/hydro.2010.067,no
A Granger Causality Measure for Point Process Models of Ensemble Neural Spiking Activity,"The ability to identify directional interactions that occur among multiple neurons in the brain is crucial to an understanding of how groups of neurons cooperate in order to generate specific brain functions. However, an optimal method of assessing these interactions has not been established. Granger causality has proven to be an effective method for the analysis of the directional interactions between multiple sets of continuous-valued data, but cannot be applied to neural spike train recordings due to their discrete nature. This paper proposes a point process framework that enables Granger causality to be applied to point process data such as neural spike trains. The proposed framework uses the point process likelihood function to relate a neuron's spiking probability to possible covariates, such as its own spiking history and the concurrent activity of simultaneously recorded neurons. Granger causality is assessed based on the relative reduction of the point process likelihood of one neuron obtained excluding one of its covariates compared to the likelihood obtained using all of its covariates. The method was tested on simulated data, and then applied to neural activity recorded from the primary motor cortex (MI) of a Felis catus subject. The interactions present in the simulated data were predicted with a high degree of accuracy, and when applied to the real neural data, the proposed method identified causal relationships between many of the recorded neurons. This paper proposes a novel method that successfully applies Granger causality to point process data, and has the potential to provide unique physiological insights when applied to neural spike trains.",2011,10.1371/journal.pcbi.1001110,no
Toward a better comprehension and modeling of hysteresis cycles in the water sorption-desorption process for cement based materials,"The aim of this work is to describe a method based on a simple representation of the pore size distribution, which is able to predict hysteresis phenomena encountered in water sorption-desorption isotherms, particularly for cementitious materials. The hysteresis effect due to network constrictivity is taken into account in order to extend models of transfer in porous media to situations involving wetting-drying cycles. This is not achieved in earlier models and their performance in terms of prediction in such conditions is thus limited. The present modeling is based on an idealized pore size distribution. This has three modes, associated with C-S-H pores, medium capillary pores, and large capillary pores including consideration of cracks. The distribution is assessed from the chemical composition of the cement, the formulation of the material, the degree of hydration, the total water porosity and the intrinsic permeability. (C) 2011 Elsevier Ltd. All rights reserved.",2011,10.1016/j.cemconres.2011.03.012,no
Improving the design and efficiency of the forced-air cooling process of fresh strawberries using computational modeling,"The aim of this work was to develop and validate a computational fluid dynamic model of the forced-air cooling process of fresh strawberries, and use it as a design tool to develop a cooling system aimed at promoting a rapid, uniform and more energy efficient process. The improved performance of the new design was experimentally confirmed. It not only significantly improved the uniform of the cooling process (essential requirement to decrease post-harvest losses), but it also replicated the cooling time of commercial designs while decreasing the pressure drop across the system by 70%. (c) 2011 Published by Elsevier B.V. Selection and/or peer-review under responsibility of 11th International Congress on Engineering and Food (ICEF 11) Executive Committee.",2011,10.1016/j.profoo.2011.09.184,no
Novel artificial neural network model for evaluating hardness of stir zone of submerge friction stir processed Al 6061-T6 plate,"The Al alloy 6061-T6 was friction stir processed at submerged condition and different tool rotation speeds v and processing speeds V. The effect of processing parameters on hardness of stir zone was investigated. In order to derive out the relationship between the hardness of stir zone and processing parameters and optimising them, some tests were carried out and a matrix of variation parameters of process was filled and used for training of an artificial neural network (ANN) model. A sensitivity analysis was carried out using the ANN model. It is shown that, among the two process parameters, the processing speed V is more important on stir hardness. In addition, a safe zone can be defined by ANN model in which superior hardness can be achieved.",2011,10.1179/174328409X425290,no
Hydrological field data from a modeller's perspective: Part 2: process-based evaluation of model hypotheses,"The current generation of hydrological models has been widely criticized for their inability to adequately simulate hydrological processes. In this study, we evaluate competing model representations of hydrological processes with respect to their capability to simulate observed processes in the Mahurangi River basin in Northland, New Zealand. In the first part of this two-part series, the precipitation, soil moisture, and flow data in the Mahurangi were used to estimate the dominant hydrological processes and explore several options for their suitable mathematical representation. In this paper, diagnostic tests are applied to gain several insights for model selection. The analysis highlights dominant hydrological processes (e. g. the importance of vertical drainage and baseflow compared to sub-surface stormflow), provides guidance for the choice of modelling approaches (e. g. implicitly representing sub-grid heterogeneity in soils), and helps infer appropriate values for model parameters. The approach used in this paper demonstrates the benefits of flexible model structures in the context of hypothesis testing, in particular, supporting a more systematic exploration of current ambiguities in hydrological process representation. The challenge for the hydrological community is to make better use of the available data, not only to estimate parameter values but also to diagnostically identify more scientifically defensible model structures. Copyright (C) 2010 John Wiley & Sons, Ltd.",2011,10.1002/hyp.7902,no
MEASUREMENT OF DEGREE OF SATURATION ON MODEL GROUND BY DIGITAL IMAGE PROCESSING,"The degree of saturation of ground is conventionally measured at discrete points using transducers, soil moisture sensors, etc. In this paper, a novel method was developed to directly measure the degree of saturation of continuous region of ground by noting the variation in color of the ground as the amount of moisture in the soil changes. In this research, a series of experiments was conducted for the purpose of developing a method to measure the degree of saturation of ground by digital image processing. From photo images taken at various soil moisture contents, the colors of the images were converted into numerical values which were then related to known degrees of saturation. The results of the experiments showed that a method to measure the degree of saturation of ground by image processing was possible. The relation between degree of saturation and luminance value can be expressed in terms of second degree function. Good results were obtained for two soil samples with different colors and grain size distributions. The margin of error was in the order of +/- 5%. The method was validated through vertical seepage tests, where good agreements were obtained between the measured degrees of saturation by tensiometers and those estimated from the proposed method. The method illustrates the possibility of measuring the degree of saturation of a larger portion of the ground, which is difficult to perform using conventional procedures. With this method, contour diagrams of degree of saturation can be produced, making it possible to visualize the propagation of the saturated region.",2011,,no
Evaluation of thermodynamic properties using GAB model to describe the desorption process of cocoa beans,"The desorption isotherms and thermodynamic properties of cocoa beans were obtained during the drying process of this product. The isotherms were determined by dynamic method for various temperature (25, 35, 45 and 55 degrees C) and relative humidity (RH) conditions (30, 40, 50, 60, 70 and 80%). Equilibrium moisture content data were correlated by the Guggenheim-Anderson-de Boer (GAB) model, which presented good fit to the data, according to statistical procedures. Equilibrium moisture content ranged from 5.90 to 16.67 d.b.; it increased with an increment in the RH and decreased with increased temperature at a constant RH. Enthalpy values for each model coefficient were encountered, ranging from -90.05 to 545.96 kJ kg(-1). The integral isosteric heat of desorption and differential entropy increased with decreased equilibrium moisture content, a tendency also found for Gibbs free energy.",2011,10.1111/j.1365-2621.2011.02719.x,no
A Critical Evaluation Study of Model-Log Metrics in Process Discovery,"The development of a well-defined evaluation framework for process discovery techniques is definitely one of the most important challenges within this subdomain of process mining. Any researcher in the field will acknowledge that such a framework is vital. With this paper, we aim to provide a tangible analysis of the currently available model-log evaluation metrics for mined control-flow models. Also, we will indicate strengths and weaknesses of the existing metrics and propose a number of opportunities for future research.",2011,,no
Evaluating management effects on nitrous oxide emissions from grasslands using the process-based DeNitrification-DeComposition (DNDC) model,"The development of agricultural mitigation strategies to reduce greenhouse gas (GHG) emissions is urgent in the context of climate change - land use interactions. In this study the DNDC biogeochemical model was used to study nitrous oxide (N(2)O) emissions from grazed grasslands in southern Ireland. The objectives of this study were: (1) to evaluate the DNDC model using a two year (2008-2009) data set of chamber measured N(2)O fluxes at eight grassland sites and (2) to investigate the impact of different management scenarios on N(2)O emissions including changes in i) inorganic nitrogen (N) fertilizer application rates ii) slurry application rates; and iii) animal density (livestock unit per hectare LU ha(-1)). The comparison of modeled daily DNDC fluxes (using a combination of measured and default soil parameters) and measured fluxes resulted in an r (coefficient correlation) = 0.48. To improve the model performance, the fluxes for 2008 were used in a calibration exercise during which the soil properties were optimized to obtain the best fit of N(2)O fluxes. This resulted in an improved model performance, with an r = 0.62. In a validation exercise using 2009 data, we used the model parameters set (e.g. soils) from the calibration exercise and this resulted in a model performance with an r = 0.57. The annual N(2)O fluxes (measured and modeled) were appreciably higher than those estimated using the IPCC emissions factor of 1.25%. In scenario analysis, the modeled N(2)O fluxes only increased/decreased on average +/-6% and +/-7% following a 50% increase/decrease of inorganic N and slurry N applications respectively. These modeled scenario % changes are much lower than the IPCC emission factor % changes of a 50% increase in N(2)O emissions for a 50% increase in nitrogen applied. An absolute change scenario (+/-50 kg) in inorganic N and slurry N resulted in greater change in N(2)O fluxes (+/-9% inorganic N and +/-17% slurry N) as compared to the relative change scenario (above). Furthermore, DNDC N(2)O flux estimates were not sensitive to changes in animal density (LU ha(-1)). The latter is a scenario limitation in the current model version. This study suggests that the calibration of soil parameters for Irish conditions is necessary for optimum simulation with DNDC and highlights the potential of management strategies for reducing N(2)O emissions from grazed grasslands. It further highlights the difference between DNDC and IPCC estimates that require further research. Published by Elsevier Ltd.",2011,10.1016/j.atmosenv.2011.07.046,no
In defense of the personal/impersonal distinction in moral psychology research: Cross-cultural validation of the dual process model of moral judgment,"The dual process model of moral judgment (DPM; Greene et al., 2004) argues that such judgments are influenced by both emotion-laden intuition and controlled reasoning. These influences are associated with distinct neural circuitries and different response tendencies. After reanalyzing data from an earlier study, McGuire et al. (2009) questioned the level of support for the dual process model and asserted that the distinction between emotion evoking moral dilemmas (personal dilemmas) and those that do not trigger such intuitions (impersonal dilemmas) is spurious. Using similar reanalysis methods on data reported by Moore, Clark, & Kane (2008), we show that the personal/impersonal distinction is reliable. Furthermore, new data show that this distinction is fundamental to moral judgment across widely different cultures (U. S. and China) and supports claims made by the DPM.",2011,,no
STATE OF THE ART REVIEW OF MODELLING ENTRAINMENT DEFECTS IN THE SHAPE CASTING PROCESS,"The entrainment of oxide films and bubbles into the bulk liquid material has been shown to have a detrimental effect on casting integrity when solid. A number of mechanisms have been shown to initiate the entrainment of oxide films, including: returning waves, plunging jets, bubble trails and fountains. The use of computational fluid dynamics (CFD) software packages, which are now widely available to the foundry engineer, has allowed him/her to improve casting system design by using qualitative parameters. Optimisation software is now an economically viable option for many foundries. However, optimisation for casting integrity requires a quantitative casting integrity assessment technique which allow the modelling and quantification of defects. Therefore, modelling and quantification of defects is becoming an ever more important research area to allow the optimisation software manufacturers to meet the needs of industry. The current methods of modelling surface film and bubble generated casting defects have been described and critically reviewed shedding light on the qualities and issues currently associated with the present available methods. It is clear that further investigations and development is still required to allow the accurate and efficient modelling of casting defects.",2011,,no
Model Problem for Integro-Differential Zakai Equation with Discontinuous Observation Processes,The existence and uniqueness in Holder spaces of solutions of the Cauchy problem to a stochastic parabolic integro-differential equation of the order alpha a parts per thousand currency sign2 is investigated. The equation considered arises in a filtering problem with a jump signal process and a jump observation process.,2011,10.1007/s00245-010-9131-8,no
Ammonia removal in anaerobic digestion by biogas stripping: An evaluation of process alternatives using a first order rate model based on experimental findings,"The feasibility of biogas stripping to remove ammonia in the anaerobic digestion of source segregated food waste was investigated. It was found in batch experiments that ammonia could be removed from digestate and that the removal followed 1st order kinetics with respect to total ammonia nitrogen concentration. Increasing temperature, biogas flow rate and initial pH all increased removal rates. Using kinetic data gathered in these experiments allowed the integration of ammonia stripping with an anaerobic digestion plant to be modelled for different configurations. Four scenarios were identified: post digestion, in situ, side-stream and pre-digestion ammonia removal relating to where in the process the ammonia stripping was performed. The modelling showed that in situ ammonia removal may be best able to reduce in-digester ammonia concentrations over a wide range of organic loading rates whereas pre-digestion showed most promise in terms of application due to the flexibility to control each part of the process separately. Further experimental work is required into these scenarios to confirm their viability. (C) 2011 Elsevier B.V. All rights reserved.",2011,10.1016/j.cej.2011.10.027,no
Ill-conditioned problems of dam safety monitoring models and their processing methods,"The focus of this paper is the ill-conditioned problems in the dam safety monitoring model. The reasons to give rise to the ill-conditioned problems in statistical models, deterministic models and hybrid models are analyzed in detail, and the criterions for ill-conditioned models are investigated. It is shown that safety monitoring models are not easy to be ill-conditioned if the number of influence factors is less than seven. Moreover, the models have a high accuracy and can meet the engineering requirements. Another frequently encountered problem in establishing a safety monitoring model is the existence of inflection points, which are often present in the mathematical model for the hydraulic components in deterministic models and hybrid models. The conditions for inflection points are studied and their treatments are suggested. Numerical example indicates that the treatments proposed in this paper are effective in removing the ill-conditioned problems.",2011,10.1007/s11431-011-4573-z,no
Control of modeling error in calibration and validation processes for predictive stochastic models,"The idea of adaptive control of modeling error is expanded to include ideas of statistical calibration, validation, and uncertainty quantification. Copyright (C) 2010 John Wiley & Sons, Ltd.",2011,10.1002/nme.3038,no
Evaluation of Deammonification Process by Response Surface Models,"The influence of the operational variables on the Anammox process has been generally researched considering each variable separately. However, the optimization of the process also requires the identification of the more significant variables and their possible interactions. Response surface models were successfully applied to evaluate the performance of the Anammox process in a deammonification system (i.e., one-stage biofilm Anammox process) taking into account the combined effects caused by two sets of three variables. Specific Anammox activity was measured by a manometric method and used as the response variable. The obtained models pointed out that the significant variables were the temperature, the value of pH, and the ratio between the unionized species of the substrates (free ammonia and free nitrous acid (FA/FNA)). There were interactions among them caused by chemical equilibriums. Total nitrogen concentration and ammonium concentration were found to be not significant in the tested range. According to the models, the optimum values of temperature, pH, and free ammonia to free nitrous acid ratio within the test ranges were, respectively, 30A degrees C, 7.0, and 0.3. Further research at higher temperatures and lower values of pH and FA/FNA ratios would be necessary in order to find the absolute optimum conditions for the process. The obtained model can be also useful in order to develop control strategies that take into account the significant variables and their optimum ranges. A strategy to control deammonification reactors has been proposed, according to the results of the modeling.",2011,10.1007/s11270-010-0479-9,no
A META MODEL OF THE INNOVATION PROCESS TO SUPPORT THE DECISION MAKING PROCESS USING STRUCTURAL COMPLEXITY MANAGEMENT,"The innovation process is characterized by numerous interactions of numerous domains. Cyclic interdependencies intensify the pressure in terms of quality and schedule, causing shortened testing phases, frequent releases of new models, and thus hardly calculable risks. Structural Complexity management is established in order to avoid wrong decisions, instable processes and error-prone solutions. Therefore, Structural Complexity Management evaluates system's characteristics by analyzing system's underlying structures across multiple domains, condensing each single analysis into one big matrix that represents multiple domains at a time. Identifying suitable perspectives, generating suitable models and using suitable analyze criteria are the challenges in this field. In order to support the manufacturing of innovative products and thus the evaluation and interpretation of the system's underlying structure this paper proposes a meta model. The created model describes the author's perspective on entities arising during the innovations process and their interactions. The proposed model is used to simplify the decision making processes and to enable the management of cyclic interdependencies during the innovation process.",2011,,no
Evaluating choices in multi-process landscape evolution models,"The interest in landscape evolution models (LEMs) that simulate multiple landscape processes is growing. However, modelling multiple processes constitutes a new starting point for which some aspects of the set up of LEMs must be re-evaluated. The objective of this paper is to demonstrate the practical significance of, and possibilities for such re-evaluation. We first discuss which simplifications must be made to set up LEMs. Then, simplifications of particular interest to the modelling of multiple processes are identified. Finally, case studies from New Zealand, Belgium and Croatia explore the performance of different model versions under several common choices for model and data simplifications. In these case studies we illustrate methods to make the choices, typically by i) comparing the results of different model versions, ii) assessing model validity or iii) indicating the sensitivity of models for different simplifications. The results indicate that LEM performance is strongly dependent on multi-process related choices, and that performance indicators can be used for ex-post testing of the influence of these choices. In particular, we demonstrate the significance of simplifications regarding the number of processes, presence of sinks and temporal resolution. (C) 2010 Elsevier B.V. All rights reserved.",2011,10.1016/j.geomorph.2010.10.007,no
"Optimum profit model based on order quantity, product price, and process quality level","The maximum expected profit model between the producers and the purchasers is an important objective for the supply chain system. The producer's profit needs to consider the problem of sales revenue and manufacturing cost. The purchaser's profit needs to consider the problem of order quantity and used cost of customer. How to get a trade-off between them should be an important topic. Chen and Liu (2007) presented the optimum profit model between the producers and the purchasers for the supply chain system with pure procurement policy. However, their model with simple manufacturing cost did not consider the used cost of customer. In this study, the modified Chen and Liu's model will be addressed for determining the optimum product and process parameters. The authors propose a modified Chen and Liu's model with indirectly measurable quality characteristic under the single stage screening procedure. The surrogate variable with high correlation with indirectly measurable quality characteristic will be directly measured. The used cost of customer can be obtained by adopting Taguchi's quadratic quality loss function. The optimum purchaser's order quantity, the producer's product price, and the process quality level will be jointly determined by maximizing the expected profit between them. (C) 2010 Elsevier Ltd. All rights reserved.",2011,10.1016/j.eswa.2010.12.046,no
Estimation and Validation of Gaussian Process Surrogate Models for Sensitivity Analysis and Design Optimization Based on the Mechanistic-Empirical Pavement Design Guide,"The Mechanistic-Empirical Pavement Design Guide (MEPDG) is a powerful predictor of pavement distress, but it is computationally expensive to evaluate. Analyses that require many MEPDG evaluations, such as sensitivity analysis and design optimization, become impractical because of the computational expense. These applications are important in achieving robust, reliable, and cost-effective pavement designs. This paper develops Gaussian process (GP) surrogate models that, with a trivial amount of computational expense, accurately approximate the results of the MEPDG for each relevant distress mode. The GP is validated in accordance with three model metrics: average predictive percent error, predictive coefficient of determination, and Bayes factor. The GP models are then exploited for sensitivity analysis and design optimization, making these tasks computationally affordable.",2011,10.3141/2226-13,no
Gross Error Positioning and Processing Method Based on Forest Grey Modeling,"The paper is committed to overcome the influence of gross error on the small quantity data of forest fire grey modeling. According to the quantity of the modeling data, Grey judgment of gross error and robust estimation theory is used separately for finding the gross error exit whether or not from the modeling data. And robust estimation theory and LIR algorithm can be used to process the gross error. From the examples, A quarter of fitting precision of robust estimation is less than 1%, and 75% is 1 similar to 5%; and half of fitting precision of LIR algorithm is less than 1%, and half is 1 similar to 5%. That is to say LIR algorithm provides a rapid, simple and practical way to build model of data which contains gross error or which contain missing data.",2011,10.4028/www.scientific.net/AMM.80-81.1262,no
Measurement of the Development of a Learning IT Organization Supported by a Model of Knowledge Acquisition and Processing,"The paper presents a model of knowledge acquisition and processing for the development of learning organizations. The theory of a learning organization provides neither metrics nor tools to measure its development The authors' studies in this field are based on their experience gathered after projects realized in real IT organizations. The authors have described the construction of the model and the methods of its verification through a series of experiments. It was proven that the model can help an organization in the orderly and formalized acquisition and processing of knowledge about its internal processes, which leads to controlled and permanent development.",2011,,no
A Query-Driven Approach for Checking the Semantic Correctness of Ontology-Based Process Representations,"The paper presents an approach to check the semantic correctness of business process models using queries in conjunction with an ontology-based process representation. The approach is based on the formalization of the semantics of individual model elements by annotating them with concepts of a formal ontology. In order to ensure semantic correctness, constraints are formalized as queries which are executed against the ontology-based process representation. The effectiveness of this approach is demonstrated by a user experiment. The experiment shows that searching for constraint violations using the query language produces more accurate results and is less time consuming in comparison to manual search when large models have to be checked.",2011,,no
A participatory evaluation confirms the social relevance of a community model for an intervention to promote educational success Outcome of a process and impact evaluation of a community-based intervention in a poor setting,"The present case study focuses on the implementation of a community-based school support intervention, which aims to facilitate educational success in socio-economically deprived areas of Montreal. An evaluation, conducted in close collaboration with the settings' actors, and comparing processes and impacts of traditionally implemented community-based institutional interventions, underlines innovative attributes of the hereby described community-based intervention and its effect on the protection of children developing in adverse psycho-social conditions. Sharing these results promotes the sustainability of the school-community partnership at the root of this initiative along with funding renewal and mobilization of new social partnerships. (Global Health Promotion, 2011; 18(1): pp. 106-109)",2011,10.1177/1757975910393192,no
The Customer Satisfaction Process Oriented Model (CS-Pro Mod): A new Theoretical Approach to Measure Customer Satisfaction,"The present paper deals with the validation of the Customer Satisfaction -Process Oriented Model (CS-ProMod), involving both offline and online services supplied by public administrations. Such an activity represents the final stage of an experimental phase carried out over an eighteen months period and aimed at allowing model release. The experimental activity involved a pilot-group of Italian public administrations. Customer satisfaction surveys were conducted on previously selected services (on-line and off-line). Structural Equation Modelling (SEM) was adopted to confirm causal relationship among model variables (i.e. satisfaction indicators and overall satisfaction). Furthermore, additional tests (other than the semantic content of indicators themselves) concerning reliability and predictive validity were set. Finally, an analysis of the problems arising from questionnaire administration phase was carried out, in order to reduce the risk of surveys' systematic errors. As a result of the above mentioned experimental activity, few major improvements were possible. In particular: Optimisation of satisfaction indicators according to the results of the confirmatory analysis; Optimisation of the data collection process (i.e. extended length of the survey or positioning of the questionnaire on the organisation web site when investigating on-line services). The paper examines the validation process of the CS-ProMod model which, in its concept, can be defined as an innovative, flexible and rigorous tool to manage customer satisfaction within the public administration sector. It is foreseen that the model be adopted by the Italian Government as a standard approach to assess the quality of public services. The outcomes reported envisage useful insight to optimize model structure and management.",2011,,no
Comparison of Modeling Approaches for Prediction of Cleaning Efficiency of the Electromagnetic Filtration Process,"The present study aims at applying different methods for predicting the cleaning efficiency of the electromagnetic filtration process (psi) in the mixtures of water and corrosion particles (rust) of low concentrations. In our study, artificial neural network (ANN), multivariable least square regression (MLSR), and mechanistic modelling approaches were applied and compared for prediction of the cleaning efficiency for the electromagnetic filtration process. The results clearly show that the use of ANN led to more accurate results than the mechanistic filtration and MLSR models. Therefore, it is expected that this study can be a contribution to the cleaning efficiency.",2011,,no
The Building of Models as Pathway to Understand the Therapeutic Process,"The present work highlights the necessity to re-think the paradigmatic, methodological and theoretical assumptions which for a long time have regulated the practices of research in the field of the study of therapeutic processes. Following this suggestion, and commenting on Ribeiro and Gon double dagger alves (2011), and Faccio, Centomo and Mininni (2011), I highlight the need to reframe the therapeutic change research in terms of a modelling approach. According to such perspective models arise through focusing on the general rule of the functioning of a change process within the local domain of a particular therapeutic encounter, making it possible both to manage it in line with the specific local aims, and to promote the deepening and the develop of scientific knowledge. This is a step towards a new research perspective able to promote the integration of empirical research and clinical practice.",2011,10.1007/s12124-011-9181-8,no
Synthetic Experiment in Evaluating the Usability Factor of the Requirement Change Propagation Process Model,"The proposed requirement change propagation (ReChaP) approach promotes significant supports in simplifying the tedious tasks of requirement change propagation to other software artefacts during software evolution. One of the ReChaP's pillars is the process model, which provides systematic guidelines to simplify the phenomenally time consuming and expensive efforts of the requirement change propagation process. This paper specifically reports on the preliminary results and the observation analysis for the conducted synthetic experiment in academic settings. The experiment's goal is to evaluate the usability quality factor of the process model in terms of five main criteria: efficiency, effectiveness, learnability, satisfaction and usefulness. Our initial findings observe that the proposed ReChaP process model is soundly demonstrated as sufficiently usable, practical enough, and meantime has ideally achieved reasonable percentages for the five comprehensive criteria of the measured usability factor.",2011,,no
Application of high-speed PIV and image processing to measuring particle velocity and concentration in a horizontal pneumatic conveying with dune model,"The purpose of this study focuses on analyzing the particle velocity and concentration characteristics in a horizontal pneumatic conveying with dune model, so as to reveal the mechanism of the low conveying velocity and saving-energy conveying. The test pipeline consisted of a horizontal smooth acrylic tube with an inside diameter of 80 mm and a length of about 5 m. The polyethylene particles of density 978 kg/m(3) and 952 kg/m(3) with diameters of 2.3 and 3.3 mm are used as conveying materials. High-speed Ply was first applied to measure the time-averaged particle velocity and was proven to be an efficient measurement technique in the pneumatic conveying. Then the particle velocity and concentration distributions of three locations were measured at mean air velocities of 12 m/s and 13 m/s and the solid mass flow rates of 0.45 kg/s and 0.43 kg/s. A comparison of the particle velocity and concentration profiles between dune model and non-dune model was performed. It is found that the particle concentration of using dune model becomes higher in the upper part of pipeline and becomes lower near the bottom of pipeline in the acceleration region. The particle velocities of using dune model are clearly higher than that of the conventional pneumatic conveying along pipeline and display a uniform profile at the downstream. It is also clear that the particles can be effectively accelerated by increasing air velocity and impacting the surface of dune model. The effect of dune model on the velocity profile of relatively small particles is larger than that of the larger particles and maintains to the downstream. (C) 2010 Elsevier B.V. All rights reserved.",2011,10.1016/j.powtec.2010.12.014,no
A Drawing Learning Support System with Auto-evaluating Function Based on the Drawing Process Model,"The purpose of this study is to develop a drawing learning support system using a networked environment. In this paper, first, we show the outline of the online drawing learning support system. Second, we describe the drawing process model that support individual drawing learning. Finally, we show three examples of learning with our system.",2011,,no
Accelerating the RTTOV-7 IASI and AMSU-A radiative transfer models on graphics processing units: evaluating central processing unit/graphics processing unit-hybrid and pure-graphics processing unit approaches,"The radiative transfer for television operational vertical sounder (RTTOV) is a widely-used radiative transfer model (RTM) for calculation of radiances for satellite infrared and microwave sensors, including the 8461-channel infrared atmospheric sounding interferometer (IASI) and the 15-band Advanced Microwave Sounding Unit-A (AMSU-A). In the era of hyperspectral sounders with thousands of spectral channels, the computation of the RTM becomes more time-consuming. The RTM performance in operational numerical weather prediction systems still limits the number of used channels in hyperspectral sounders to only a few hundred. To take full advantage of such high-resolution infrared observations, a computationally efficient radiative transfer model is needed to facilitate satellite data assimilation. In this paper, we develop the parallel implementation of the RTTOV-7 IASI and AMSU-A RTMs to run the predictor module on CPUs in pipeline with the transmittance and radiance modules on NVIDIA many-core graphics processing units (GPUs). We show that concurrent execution of RTTOV-7 IASI RTM on CPU and GPU, in addition to asynchronous data transfer from CPU to GPU, allows the GPU accelerated code running on the 240-core NVIDIA Tesla C1060 to reach a speedup of 461x and 1793x for 1- and 4-GPU configurations, respectively. To compute one day's amount of 1,296,000 IASI spectra, the CPU code running on the host AMD Phenom II X4 940 CPU core with 3.0 GHz will take 2.8 days. Thus, GPU acceleration reduced running time to 8.75 and 2.25 min on 1- and 4-GPU configurations, respectively. Speedup for the RTTOV AMSU-A RTM varied from 29x to 75x for 1 and 4 GPUs, respectively. To further boost the speedup of a multispectral RTM, we developed a novel pure-GPU version of the RTTOV AMSU-A RTM where the predictor module also runs on GPUs to achieve a 96% reduction in the host-to-device data transfer. The speedups for the pure-GPU AMSU-A RTM are significantly increased to 56x and 125x for 1- and 4-GPU configurations, respectively. C (C) 2011 Society of Photo-Optical Instrumentation Engineers (SPIE).",2011,10.1117/1.3658028,no
Cloud Processing of Gases and Aerosols in Air Quality Modeling,"The representations of cloud processing of gases and aerosols in some of the current state-of-the-art regional air quality models in North America and Europe are reviewed. Key processes reviewed include aerosol activation (or nucleation scavenging of aerosols), aqueous-phase chemistry, and wet deposition/removal of atmospheric tracers. It was found that models vary considerably in the parameterizations or algorithms used in representing these processes. As an emerging area of research, the current understanding of the uptake of water soluble organics by cloud droplets and the potential aqueous-phase reaction pathways leading to the atmospheric secondary organic aerosol (SOA) formation is also reviewed. Sensitivity tests using the AURAMS model have been conducted in order to assess the impact on modeled regional particulate matter (PM) from: (1) the different aerosol activation schemes, (2) the different below-cloud particle scavenging algorithms, and (3) the inclusion of cloud processing of water soluble organics as a potential pathway for the formation of atmospheric SOA. It was found that the modeled droplet number concentrations and ambient PM size distributions were strongly affected by the use of different aerosol activation schemes. The impact on the modeled average ambient PM mass concentration was found to be limited in terms of averaged PM2.5 concentration (similar to a few percents) but more significant in terms of PM1.0 (up to 10 percents). The modeled ambient PM was found to be moderately sensitive to the below-cloud particle scavenging algorithms, with relative differences up to 10% and 20% in terms of PM2.5 and PM10, respectively, when using the two different algorithms for the scavenging coefficient (Lambda) corresponding to the lower and upper bounds in the parameterization for Lambda. The model simulation with the additional cloud uptake and processing of water-soluble organic gases was shown to improve the evaluation statistics for modeled PM2.5 OA compared to the IMPROVE network data, and it was demonstrated that the cloud processing of water-soluble organics can indeed be an important mechanism in addition to the traditional secondary organic gas uptake to the particle organic phase.",2011,10.3390/atmos2040567,no
A dynamic sensory quality evaluation model based on panel data and linguistic information processing,"The sensory quality evaluation processes are widely used for many industrial products, which is evaluated according to the knowledge acquired via human senses (such as sight, taste, touch, smell). Because the evaluation information acquired by our senses is involving uncertainty, vagueness and imprecision, and the sensory quality information of products always has the panel data characteristic, so the sensory quality evaluation and certification is a multi-feature group decision-making problem based on panel linguistic information processing. This paper presents a proposed method for the development of accredited sensory evaluation methods, and shows the application of the linguistic 2-tuple representation model and its advantages. It involved several steps: gathering of sensory information from experts; computing a collective evaluation for each feature; computing collective evaluations for each object; dynamic aggregation; sensory evaluation and quality certification with global evaluation result.",2011,10.4028/www.scientific.net/AMM.44-47.3741,no
Analytic Hierarchy Process Model Evaluation on Surrounding Rock of Tunnel with Small Clear Spacing,"The stability evaluation of the surrounding rock of tunnel with small clear spacing is an important step in a tunnel construction process. In this paper an indicator system was proposed to evaluate the stability state of the central rock pillar between twin tunnel with small clear spacings; then an AHP (Analytic Hierarchy Process) model was constructed to calculate the weights of indicators; and finally, with fuzzy mathematic method, a fuzzy comprehensive evaluation model was set up to evaluate the stability state of the central rock pillar between twin tunnel with small clear spacings. Project cases showed that this method is practical and effective. (C) 2011 Published by Elsevier Ltd. Selection and/or peer-review under responsibility of the Organizers of 2011 International Conference on Energy and Environmental Science.",2011,10.1016/j.egypro.2011.10.845,no
Dimensional Quality Oriented Reliability Modeling for Complex Manufacturing Processes,"The stability of process is certainly one of the most important aspects to fulfill the task. Now, it is still a challenge to put forward a model to deal with the stability of manufacturing process mathematically. Aiming at solving the problem, this paper discusses the issues by the notion of manufacturing process reliability and its sensitivity. Based on state space model, the process performance function which is used to describe the relations between the key product characters (KPC) of the machined part and the key control characters (KCC) of multistage process in machining system has been built. Furthermore, the measure index to the process reliability of multistage machining system is proposed, which facilitates the forming of the ways to model the process reliability mathematically. To determine the weakest stage during machining process, the sensitivity of process reliability to KCC of the process has been put forward and the corresponding ways how to calculate it. Because of the multistage process is highly non-linear process, chaos optimization algorithm and mutative scale chaos optimization algorithm are used to calculate this kind of robust reliability index. Finally, A simple 2-D case study has been used to validate the proposed model. It shows that the process reliability can be calculated effectively.",2011,10.1080/18756891.2011.9727875,no
MODELING THE WATER BALANCE PROCESSES FOR UNDERSTANDING THE COMPONENTS OF RIVER DISCHARGE IN A NON-CONSERVATIVE WATERSHED,"The study was conducted in the Shibetsu watershed, eastern Hokkaido, Japan, to examine the possibility of using the Soil and Water Assessment Tool (SWAT) model in a non-conservative watershed (the surface watersheds are lying on a discontinuous impervious horizon) with external contribution (EXT. After confirming the capability of model simulation, the EXT was estimated to understand the components of river discharge. The EXT is difficult to measure directly and simulate by SWAT due to its subsurface circulation. In this study, the EXT was roughly estimated from the water balance equation using measured data. The average daily flux of EXT (1.38 nun d(-1)) was assumed as a point-source discharge in SWAT The simulation of daily streamflow during the calibration and validation periods produced satisfactory results, with R-2 values of 0.65 and 0.66, respectively. In addition, the simulated daily baseflow, monthly streamflow, surface runoff, and evapotranspiration (ET) all showed good agreement with the corresponding observations. Our simulation suggested that the EXT assigned as the assumed discharge in SWAT can help us to reasonably simulate the streamflow in the Shibetsu watershed. The EXT was then investigated indirectly by considering the difference between the observed streamflow and simulated streamflow using calibrated SWAT without adding the assumed EXT The result indicated that the EXT was an important water source in the Shibetsu watershed, accounting for 47% of streamflow during the study period.",2011,,no
Force and Form Error Models for Wavy Surface in Successive Peripheral Milling Process,"The successive peripheral milling process is commonly used to reduce the form error on the wavy surface caused by the cutting force-induced tool or work deflection. For such wavy surfaces, the radial depth of the cut, the engagement angle, and the chip load vary along the cutter axis. In this paper, an improved convolution force model and a tool deflection model for wavy surfaces in the successive peripheral milling process are presented. The effects of milling configuration on the cutting force, form error, and surface roughness in successive peripheral milling are studied. The models are validated by a series of machining experiments. The measurement results show that the milling configuration has a significant influence on the surface roughness of the finished part. A recommendation is given on how to allocate the radial depth of cut in successive milling to improve machined surface quality in terms of geometry accuracy and surface roughness.",2011,,no
A Modeling Method for Quality Prediction of Batch Process,"The traditional MPLS method have the defects of complex structure, extensive calculation, data unevenness in modeling and predicting for the quality prediction of the batch process. Mean value staged MICR method was proposed to overcome the disadvantage. The method used in modeling of tube hollow quality prediction can improve seamless tubes quality effectively. Its obvious benefits are low maintenance cost, good real time function, high reliability precision, and can be used on-line for the prediction and optimization on quality of tube hollow.",2011,,no
Output-Error Model Training for Gaussian Process Models,"The training of a regression model depends on the purpose of the model. When a black-box model of dynamic systems is trained, two purposes are particularly common: prediction and simulation. The purpose of this paper is to highlight the differences between the learning of a dynamic-system model for prediction and for simulation in the presence of noise for Gaussian process models. Gaussian process models are probabilistic, nonparametric models that recently generated interest in the machine-learning community. This method can also be used also for the modelling of dynamic systems, which is the main interest of the engineering community. The paper elaborates the differences between prediction- and simulation-purposed modelling in the presence of noise, which is more difficult in the case when we train the model for simulation. An example is given to illustrate the described differences.",2011,,no
Design and modeling of integrated Hall-effect sensor based on-line thickness measurement device for incremental sheet forming processes,"The use of Numerical Control and computers in manufacturing has enabled the development of new sheet forming processes. One of these flexible processes is called Incremental Sheet Forming (ISF) in which deformation is localized by the movement of a spherical or flat forming tool. ISF is carried out regularly by a CNC machine tool or by a Robot, which follows a tool-path generated by CAM programs, without the need for costly dies. Despite research progresses in understanding the deformation mechanism in ISF the process still needs a further optimization to guarantee the reliability required for industrial applications. This paper deals with the design of a new smart forming tool, applying FEM modeling and simulation, which is able to measure one of the key process parameters: the sheet thickness during the forming process. The authors analyze the possibility to use a Hall-effect sensor integrated into the forming tool for more precise on-line thickness measurement than what can be found in the literature and first results are reported.",2011,,no
Wavelet OHIF Elman neural network model and its predictive control of processing quality,"There are some difficulties in build the process quality prediction model based on Elman neural network. The traditional Sigmoid activation function often used in the hidden layer, while it is difficult to establish a quantitative relationship between the network size and resolution scale, therefore, a wavelet OHIF Elman neural network model is proposed in this paper, which full use of the neural network weights of the linear distribution and learning convex objective function, so it can avoid the local optimal nonlinear optimization problems. Simulation results show that the wavelet Elman OHIF Elman network decreased 12.9 percent compared with OHIF Elman network which used the sigmoid activation function in hidden layer.",2011,10.1117/12.889125,no
Evaluation of an Information-Processing Model Following Sexual Assault,"There is growing recognition that individuals vary in their response to traumatic experiences. Resick and Schnicke (1992) developed an information-processing model of trauma response patterns, theorizing that individuals vary in how they integrate the experience into their schematic beliefs. Specifically, individuals can respond to trauma by assimilation, altering the trauma to fit with extant schemas; accommodation, altering extant schemas; or overaccommodation, engaging in maladaptive schema change. Littleton (2007) supported that these response patterns are reflected in distinct coping patterns among rape victims. The current study utilized latent profile analysis (LPA) to replicate Littleton's (2007) findings in a sample of 340 college rape victims, as well as evaluated the extent to which these response patterns were related to distress, trauma-related schemas, revictimization risk behaviors, and revictimization. Results of the LPA supported the existence of the three response patterns. In addition, victims classified into the three response patterns differed in their distress, adherence to trauma-related schemas, and revictimization risk behaviors. While no significant differences in revictimization rates were found, revictimization was common. Implications of the findings for future research and intervention are discussed.",2011,10.1037/a0021381,no
Computational modelling and experimental validation of the thermal fusion bonding process in porous fibrous media,"This article presents a computational model of the thermal bonding of nonwovens using convective hot air and its experimental validation. A computational fluid dynamics model based on the continuum modelling approach and the theory of porous media is developed to treat the flow behaviour and heat transfer within the thermal bonding system. The model includes several components of a typical industrial machine including the conveyer belt, drum cover, drum, and the nonwoven web. Experimental measurements are used to supply appropriate boundary conditions for the simulations and to provide data for the validation of the numerically computed results. The model is concluded to be an accurate computational tool that could potentially replace the costly experiments and be employed in product development, process optimization, and machine design.",2011,10.1177/0954408910396785,no
Optimisation of the process control in a semiconductor company: model and case study of defectivity sampling,"This article studies the skip, under some assumptions, of process control operations. The case of one tool, one enhanced buffer and one metrology tool of a monotonic parameter is analysed. This article presents circumstances in which control plan can be optimised due to the buffer's behaviour. After discussing the industrial issue of defectivity, this article presents a literature review followed by the model and steps towards industrial development. Then demonstrator, which is applied at a case study of defectivity sampling, is presented. A test of over a 300-mm wafer fabrication data set shows serious improvements - around 35% of defectivity controls have been skipped compared to the static sampling plan.",2011,10.1080/00207543.2010.484429,no
Inference for Regression Models with Errors from a Non-invertible MA(1) Process,"This paper considers maximum likelihood estimation in a regression model when the errors follow a first-order moving average model which is non-invertible or nearly non-invertible. The latter corresponds to a moving average parameter theta that is equal to or close to 1. The joint limiting distribution of the maximum likelihood estimators (b) over cap and (theta) over cap of the regression parameter vector b and the moving average parameter theta is described. Unlike the case with standard time series models, the limiting distribution of (b) over cap depends on whether or not theta is being estimated. Specifically, the limit distribution of (b) over cap is non-normal if theta is also being estimated and is normal if theta is unestimated and equal to 1. The asymptotic behavior of the generalized likelihood ratio statistic for testing theta = 1 vs. theta < 1 is also studied and shown to perform well compared to the locally best invariant unbiased test of Tanaka (1990). We also indicate extensions to seasonal moving average models with a unit root. Copyright (C) 2010 John Wiley & Sons, Ltd.",2011,10.1002/for.1198,no
ENZYMATIC DEGRADATION OF POLYURETHANES. Part I. DECOMPOSITION PRODUCTS EVALUATION AND MATHEMATICAL MODELING OF PROCESS,"This paper constitutes a presentation of the factors influencing the enzymatic hydrolysis of polymers, the corresponding degradation mechanism and also a review of the literature regarding enzymatic degradation of polyurethanes (FUR) in general. Studies on the separation and identification of PUR degradation products released as a result of enzyme activity have been presented. Moreover, the literature regarding the mathematical aspects of degradation process was reviewed. The development of such a model for the in vitro enzymatic degradation of FUR is essential in understanding the kinetics of the process and enable the prediction of polymer behavior in vivo.",2011,,no
Modeling Stream Processing Applications for Dependability Evaluation,"This paper describes a modeling framework for evaluating the impact of faults on the output of streaming applications. Our model is based on three abstractions: stream operators, stream connections, and tuples. By composing these abstractions within a Stochastic Activity Network, we allow the modeling of complete applications. We consider faults that lead to data loss and to silent data corruption (SDC). Our framework captures how faults originating in one operator propagate to other operators down the stream processing graph. We demonstrate the extensibility of our framework by evaluating three different fault tolerance techniques: checkpointing, partial graph replication, and full graph replication. We show that under crashes that lead to data loss, partial graph replication has a great advantage in maintaining the accuracy of the application output when compared to checkpointing. We also show that SDC can break the no data duplication guarantees of a full graph replication-based fault tolerance technique.",2011,,no
Processing of Acoustic Signals and Computer Modeling Instrumental and Programming Measuring Complex for Acoustic Navigation Investigations,This paper describes the main systems and results of testing a mobile instrumental programming complex developed for experiments on investigation of the influence of hydrophysical processes in the marine environment on the quality of solving the navigation problems using stationary acoustic beacons.,2011,10.1134/S1063771011060029,no
Quality Improvement Models for Business Process Change - A German Case Study,"This paper describes the role of business phase models with regards to modern IT-service design. Hence, the usability of two models incorporated for IT-service design in modern business environments will be evaluated. The two examined models are namely the classic four-step model also referred to as PDCA-Circle of Deming on the one hand and the lean Six Sigma model on the other hand. The approach includes qualitative interviews with six different specialists of German companies in disjoint practices. This practice-diversity is needed to profile the current process of IT-service design over cross-sectional areas. Based on the standardized interviews, seven drivers of the research framework have been identified as quantification guidelines for the performance of phase models in current IT-service design. Thereafter, a case study of a large German insurance IT-department was analyzed. The results are showing differences in the explanatory power. Thereby, the lean Six Sigma model offers a wide set of tools for customer orientation, whereas the classic four-step model scores lower on average. Based on these insights, this paper analyzes if the findings can be generalized for other practices and how future IT-services can be supported by a modification of the theoretical frameworks.",2011,,no
Research on Geometric Errors Model And Identification of 5-Axis Machining Center For Processing Turbo Molecular Pump Rotor,"This paper investigates the accuracy evaluative factors of 5-axis turbo molecular pump rotor machining center.Using rigid body kinematics techniques and homogenous transformation matrices, the systemic geometric volumetric error model are established for error synthesis. Novel volumetric accuracy model can be verified effectiveness better estimation of machine performance, by means of physical simulation and measurement. The effect of the individual axis geometric errors can become increasingly significant as the chain of dependent axis is extended. The effect of the rotary axis and their errors cannot be ignored. Calculating volumetric accuracy as the maximum difference between the error vector at any two positions can be computationally intensive.",2011,10.4028/www.scientific.net/AMR.211-212.784,no
Importance of crop varieties and management practices: evaluation of a process-based model for simulating CO2 and H2O fluxes at five European maize (Zea mays L.) sites,"This paper is a modelling study of crop management impacts on carbon and water fluxes at a range of European sites. The model is a crop growth model (STICS) coupled with a process-based land surface model (ORCHIDEE). The data are online eddy-covariance observations of CO2 and H2O fluxes at five European maize cultivation sites. The results show that the ORCHIDEE-STICS model explains up to 75% of the observed daily net CO2 ecosystem exchange (NEE) variance, and up to 79% of the latent heat flux (LE) variance at five sites. The model is better able to reproduce gross primary production (GPP) variations than terrestrial ecosystem respiration (TER) variations. We conclude that structural deficiencies in the model parameterizations of leaf area index (LAI) and TER are the main sources of error in simulating CO2 and H2O fluxes. A number of sensitivity tests, with variable crop variety, nitrogen fertilization, irrigation, and planting date, indicate that any of these management factors is able to change NEE by more than 15%, but that the response of NEE to management parameters is highly site-dependent. Changes in management parameters are found to impact not only the daily values of NEE and LE, but also the cumulative yearly values. In addition, LE is shown to be less sensitive to management parameters than NEE. Multi-site model evaluations, coupled with sensitivity analysis to management parameters, thus provide important information about model errors, which helps to improve the simulation of CO2 and H2O fluxes across European croplands.",2011,10.5194/bg-8-1721-2011,no
Making sense of leaving care: The contribution of Bridges model of transition to understanding the psycho-social process,"This paper is based on research into the transition of young people leaving public care in Romania. Using this specific country example, the paper aims to contribute to present understandings of the psycho-social transition of young people from care to independent living by introducing the use of Bridges (2002) to build on existing theories and literature. The research discussed involved mixed methods design and was implemented in three phases: semi-structured interviews with 34 care leavers, focus groups with 32 professionals, and a professional-service user working group. The overall findings confirmed that young people experience two different, but interconnected transitions - social and psychological - which take place at different paces. A number of theoretical perspectives are explored to make sense of this transition including attachment theory, focal theory and identity. In addition, a new model for understanding the complex process of transitions was adapted from Bridges (2002) to capture the clear complexity of transition which the findings demonstrated in terms of their psycho-social transition. The paper concludes with messages for leaving and after care services with an emphasis on managing the psycho-social transition from care to independent living. (C) 2011 Elsevier Ltd. All rights reserved.",2011,10.1016/j.childyouth.2011.08.016,no
Offline Modeling for Product Quality Prediction of Mineral Processing Using Modeling Error PDF Shaping and Entropy Minimization,"This paper presents a novel offline modeling for product quality prediction of mineral processing which consists of a number of unit processes in series. The prediction of the product quality of the whole mineral process (i.e., the mixed concentrate grade) plays an important role and the establishment of its predictive model is a key issue for the plantwide optimization. For this purpose, a hybrid modeling approach of the mixed concentrate grade prediction is proposed, which consists of a linear model and a nonlinear model. The least-squares support vector machine is adopted to establish the nonlinear model. The inputs of the predictive model are the performance indices of each unit process, while the output is the mixed concentrate grade. In this paper, the model parameter selection is transformed into the shape control of the probability density function (PDF) of the modeling error. In this context, both the PDF-control-based and minimum-entropy-based model parameter selection approaches are proposed. Indeed, this is the first time that the PDF shape control idea is used to deal with system modeling, where the key idea is to turn model parameters so that either the modeling error PDF is controlled to follow a target PDF or the modeling error entropy is minimized. The experimental results using the real plant data and the comparison of the two approaches are discussed. The results show the effectiveness of the proposed approaches.",2011,10.1109/TNN.2010.2102362,no
Evaluation of moist processes during intense precipitation in km-scale NWP models using remote sensing and in-situ data: Impact of microphysics size distribution assumptions,"This study investigates the sensitivity of moist processes and surface precipitation during three extreme precipitation events over Belgium to the representation of rain, snow and hail size distributions in a bulk one-moment microphysics parameterisation scheme. Sensitivities included the use of empirically derived relations to calculate the slope parameter and diagnose the intercept parameter of the exponential snow and rain size distributions and sensitivities to the treatment of hail/graupel. A detailed evaluation of the experiments against various high temporal resolution and spatially distributed observational data was performed to understand how moist processes responded to the implemented size distribution modifications. Net vapour consumption by microphysical processes was found to be unaffected by snow or rain size distribution modifications, while it was reduced replacing formulations for hail by those typical for graupel, mainly due to intense sublimation of graupel. Cloud optical thickness was overestimated in all experiments and all cases, likely due to overestimated snow amounts. The overestimation slightly deteriorated by modifying the rain and snow size distributions due to increased snow depositional growth, while it was reduced by including graupel. The latter was mainly due to enhanced cloud water collection by graupel and reduced snow depositional growth. Radar reflectivity and cloud optical thickness could only be realistically represented by inclusion of graupel during a stratiform case, while hail was found indispensable to simulate the vertical reflectivity profile and the surface precipitation structure. Precipitation amount was not much altered by any of the modifications made and the general overestimation was only decreased slightly during a supercell convective case. (C) 2010 Elsevier B.V. All rights reserved.",2011,10.1016/j.atmosres.2010.08.017,no
Economic process control for multivariate quality characteristics with Hotelling's T-2 Charts under Gamma shock model,"This study is an extension of research conducted by Rahim and Banerjee (Journal of Naval Research Logistics 40 (1993), pp. 787-809) to construct an economic design approach of control chart for simultaneously monitoring several quality characteristics under a Gamma shock model with an increasing failure rate. Furthermore, measure the advantage of the Hotelling's T-2 control chart with a variable sampling interval (VSI) versus a fixed-length sampling interval (FSI) and a standard Shewhart sampling interval (SSI) under Gamma (lambda, 2) shock models based on an economic aspect. This primary contribution of this study is to find an optimal sampling interval to improve the traditional Hotelling's T-2 control chart under a non - exponential failure mechanism detecting the small process shift from considering the cost viewpoint. As a result in earlier investigations most control chart economic designs assumed the occurrence time of an assignable cause, which belongs to a random variable of exponential distribution with constant hazard rates because of their administrative simplicity. However, it may not be appropriate for some processes which deteriorate over time. Hence, this study employs a numerical example to indicate the solution procedure and to implement the sensitivity analysis while comparing the results of using various sampling interval approaches.",2011,,no
Evaluation of a Conjunctive Surface-Subsurface Process Model (CSSP) over the Contiguous United States at Regional-Local Scales,"This study presents a comprehensive evaluation on a Conjunctive Surface-Subsurface Process Model (CSSP) in predicting soil temperature-moisture distributions, terrestrial hydrology variations, and land-atmosphere exchanges against various in situ measurements and synthetic observations at regional-local scales over the contiguous United States. The CSSP, rooted in the Common Land Model (CoLM) with a few updates from the Community Land Model version 3.5 (CLM3.5), incorporates significant advances in representing hydrology processes with realistic surface (soil and vegetation) characteristics. These include dynamic surface albedo based on satellite retrievals, subgrid soil moisture variability of topographic controls, surface-subsurface flow interactions, and bedrock constraint on water table depths. As compared with the AmeriFlux tower measurements, the CSSP and CLM3.5 reduce surface sensible and latent heat flux errors from CoLM by 10 W m 22 on average, and have much higher correlations with observations for daily latent heat variations. The CSSP outperforms the CLM3.5 over the crop, grass, and shrub sites in depicting the latent heat annual cycles. While retaining the improvement for soil moisture in deep layers, the CSSP shows further advantage over the CLM3.5 in representing seasonal and interannual variations in root zones. The CSSP reduces soil temperature errors from the CLM3.5 (CoLM) by 0.2 (0.7) K at 0.1 mand 0.3 (0.6) K at 1 m; more realistically captures seasonal-interannual extreme runoff and streamflow over most regions and snow depth anomalies in high latitude (45 degrees-52 degrees N); and alleviates climatological water table depth systematic bias (absolute error) by about 1.2 (0.4) m. Clearly, the CSSP performance is overall superior to both the CoLM and CLM3.5. The remaining CSSP deficiencies and future refinements are also discussed.",2011,10.1175/2010JHM1302.1,no
A measurement model for experts knowledge-based systems algorithm using fuzzy analytic network process,"This study proposes an experts knowledge-based systems measurement model, the model using fuzzy analytic network process (FANP) to resolve the uncertainty and imprecision of evaluations during pre-negotiation stages, where the comparison judgments of a decision maker are represented as fuzzy triangular numbers. A novel fuzzy prioritization method, which derives crisp priorities (criteria weights and scores of alternatives) from consistent and inconsistent fuzzy comparison matrices, is also proposed. The applicability of the proposed model is demonstrated in a government purchase digital video recorder (DVR) system project study. The stability tests indicate the advantages of the proposal model in determining the value of model. Importantly, the proposed model can provide decision makers a reference material, making it highly applicable for academic and commercial purposes. Crown Copyright (C) 2010 Published by Elsevier Ltd. All rights reserved.",2011,10.1016/j.eswa.2010.12.144,no
Photooxidation processes for an azo dye in aqueous media: Modeling of degradation kinetic and ecological parameters evaluation,"Three photooxidation processes, UV/H(2)O(2), UV/S(2)O(8)(2-) and UV/O(3) were applied to the treatment of model wastewater containing non-biodegradable organic pollutant, azo dye Acid Orange 7 (AO7). Dye degradation was monitored using UV/VIS and total organic carbon (TOC) analysis, determining decolorization, the degradation/formation of naphthalene and benzene structured AO7 by-products, and the mineralization of model wastewater. The water quality during the treatment was evaluated on the bases of ecological parameters: chemical (COD) and biochemical (BOD(5)) oxygen demand and toxicity on Vibrio fischeri determining the EC(50) value. The main goals of the study were to develop an appropriate mathematic model (MM) predicting the behavior of the systems under investigation, and to evaluate the toxicity and biodegradability of the model wastewater during treatments. MM developed showed a high accuracy in predicting the degradation of AO7 when considering the following observed parameters: decolorization, formation/degradation of by-products and mineralization. Good agreement of the data predicted and the empirically obtained was confirmed by calculated standard deviations. The biodegradability of model wastewater was significantly improved by three processes after mineralizing a half of the initially present organic content. The toxicity AO7 model wastewater was decreased as well. The differences in monitored ecological parameters during the treatment indicated the formation of different by-products of dye degradation regarding the oxidant type applied. (C) 2010 Elsevier B.V. All rights reserved.",2011,10.1016/j.jhazmat.2010.10.087,no
Dynamic quality characteristics modelling based on brittleness theory in complex manufacturing processes,"To comprehensively and quantitatively analyse the influencing factors of dynamic quality characteristics in complex manufacturing processes, depending on the brittleness analysis of dynamic quality characteristics based on man-machine-environment influencing factors, this paper established a man-machine-environment brittleness model of dynamic quality characteristics based on the brittleness theory of complex system. According to the brittleness relation among the brittleness factors of dynamic quality characteristics (BFDQCs) in this model, a key man-machine-environment influencing factor identification method of dynamic quality characteristics was proposed by the definition of brittle risk degree, brittle coupling degree and brittle degree among BFDQCs. Finally, a case study for applicability was presented. The result showed that the proposed method was available, and can provide guidance for the analysis of dynamic quality characteristics and support for product reliability in complex manufacturing processes.",2011,10.1080/0951192X.2011.592996,no
Error Correction of Theory Model in Process-Stress Accelerated Test,"To correct error in theoretical model of process-stress accelerated test, a new calculation method is proposed. The new method, based on computer-aided calculation, can significantly reduce the error of the model. Theoretical data is calculated using both the new model algorithm, which is the root test method, and the old model algorithm. The results show that the old model algorithm can generate error more than 13% in the activation energy and error more than 150% in the extrapolated lifetime (Q <= 1.0eV), while the new model algorithm generates error less than 1% in activation energy, and error less than 4.1% in the extrapolated lifetime.",2011,10.4028/www.scientific.net/AMR.311-313.1677,no
Time Series Factorial Models with Uncertainty Measures: Applications to ARMA Processes and Financial Data,"To facilitate the task of identifying time series classes, we propose a nonparametric approach to defining new pertinent criteria. This approach combines a technical analysis of series of states and theory of information with factorial techniques of visualization. First, we apply this approach to the usual benchmarks in time series analysis, i.e., simulated ARMA processes. We show significant groupings and oppositions explained by entropies returning some well-known properties of autocorrelation functions. Having thus justified the methodology, we apply it to practical financial data. Second, we use the approach with entropies or uncertainty measures to analyze risk information in fund ratings and returns. The financial data analysis is applied to a set of 1500 European equity funds with both Morningstar and Europerformance ratings, for the period from 2005-2007. Our methodology derives groups of funds with high to low uncertainty measured on Morningstar ratings against low to high uncertainty on Europerformance, and conversely. We conclude that the two agencies provide different classifications of funds, and that, in a more general case, the higher predictive power is offered by the Morningstar ratings.",2011,10.1080/03610920903537277,no
On Maintaining Consistency of Process Model Variants,"Today's enterprises are dynamic where many variances of business process models can exist due to several reasons such as: the need to target different customer types, rely on particular IT systems or comply with specific country regulations. Automated maintenance of the consistency between process variants is an important goal that saves the time and efforts of process modelers. In this paper, we present a query-based approach to maintain consistency among process variants. We maintain the link between the variant process models by means of defining process model views. These views are defined using. BPMN-Q, a visual query language for business process models. Therefore, dynamic evaluation for the defined queries of the process views guarantee that the process modeler is able to get up-to-date and consistent status of the process model. In addition, our view-based approach allows the building of a holistic view of related variants of the same process model.",2011,,no
"Exploring an integrative model of infant behavior: What is the relationship among temperament, sensory processing, and neurobehavioral measures?","Traditionally, developmental psychology, occupational/physical therapy, and behavioral pediatrics view similar infant behaviors from temperament, sensory processing, or neurobehavioral theoretical perspectives. This study examined the relations between similar and unique summary scores of three infant assessments (Early Infancy Temperament Questionnaire - EITQ the Infant Sensory Profile - ISP, and the NICU Network Neurobehavioral Scale - NNNS) in a healthy sample of 100, one-month-old infants. A Principal Components Analysis of selected subscale scores derived from the three assessments suggested a three-factor model. Temperament and sensory summary scores had the strongest relations on two factors: Sensory-Affective Reactivity and Engagement. A third factor had strong relations between state regulation and motor competence. This new integrative model also validates an existing model and expands explanation of infant behavior across disciplines and methods which have significant implications for assessment, intervention, and management practices. (C) 2011 Elsevier Inc. All rights reserved.",2011,10.1016/j.infbeh.2011.01.003,no
The representation and processing of uncertain problems,"Uncertainty is everywhere, and there are many researches on uncertain problems. Soft Computing combined intelligent paradigms as Probabilistic Reasoning, Fuzzy Logic to deal with pervasive imprecision and uncertainty of the real-world problems. In this paper, four main Soft Computing methods, namely, Probability Theory, Fuzzy Set Theory, Rough Set Theory and Cloud Model are introduced briefly, and their representation and measure of uncertainty are discussed respectively. (C) 2011 Published by Elsevier Ltd. Selection and/or peer-review under responsibility of [CEIS 2011]",2011,10.1016/j.proeng.2011.08.365,no
Modeling and criticality evaluation of the voloxidation process,"Voloxidation is a necessary process in the dry reprocessing of spent nuclear fuels. The criticality evaluation plays a considerable role in the design of voloxidation apparatus. As conservative results are always preferred in a criticality evaluation, an optimized model was built in consideration of both the geometry of voloxidation apparatus and the occurring forms of evaluation material. The criticality evaluation of fresh UO(2) fuel and PWR spent fuel were then performed by employing Monte Carlo techniques, respectively. It is demonstrated that there is no criticality risk concerning the voloxidation process dealing with fresh UO(2) or PWR spent fuel if water does not intrude into the cell. However, if water intrudes and mixes with the fuels, the subcritical mass limit is 40.1 +/- 0.1 kg for fresh UO(2) and 19,155 +/- 50 kg for spent fuel. The contributions of (1)H and (235)U were analyzed quantitatively by the TSUNAMI code to clarify the competition between (1)H moderation effect and its dilution effect on the concentration of (235)U. Crown Copyright (C) 2011 Published by Elsevier Ltd. All rights reserved.",2011,10.1016/j.anucene.2011.06.014,no
Mathematical Model for Simulating the Springback Effect of Gel Matrixes During Drying Processes and Its Experimental Validation,"Volume change is one a fundamental aspect in characterization of drying processes. Several attempts to simulate the volume change during drying have been reported in the open literature. However, so far no theoretical approach has been used to support these simulations, especially when it comes to dealing with the springback effect. In this contribution, a theoretical model was built to predict the volume change including the springback phenomenon. The theory behind the present model is based on three physical mechanisms, which are represented by the shrinkage, collapse, and swelling functions. The resulting set of equations was implemented and solved in MATLAB(Mathworks, Inc., Natick, MA) by formulating the model as a constrained optimization problem. Data for three gels reported by an independent group and characterized by different profiles in terms of the springback effect were used to validate the model. This validation showed excellent agreement between the predictions obtained by this model and the experimental data. The average error lies somewhere between 1.6 and 4.4% depending on the gel. The information extracted from the parameters included in this theoretical model should assist in understanding the mechanisms that occur during processes involving moisture/solvent changes. Hence, the present model can be used as a reliable tool to predict volume changes and to understand the dynamic mechanisms involved in pore formation/disappearance during drying processes.",2011,10.1080/07373937.2011.599505,no
The Process-Knowledge Model of Health Literacy: Evidence from a Componential Analysis of Two Commonly Used Measures,"We investigated the effects of domain-general processing capacity (fluid ability such as working memory), domain-general knowledge (crystallized ability such as vocabulary), and domain-specific health knowledge for two of the most commonly used measures of health literacy (S-TOFHLA and REALM). One hundred forty six community-dwelling older adults participated; 103 had been diagnosed with hypertension. The results showed that older adults who had higher levels of processing capacity or knowledge (domain-general or health) performed better on both of the health literacy measures. Processing capacity interacted with knowledge: Processing capacity had a lower level of association with health literacy for participants with more knowledge than for those with lower levels of knowledge, suggesting that knowledge may offset the effects of processing capacity limitations on health literacy. Furthermore, performance on the two health literacy measures appeared to reflect a different weighting for the three types of abilities. S-TOFHLA performance reflected processing capacity as well as general knowledge, whereas performance on the REALM depended more on general and health knowledge than on processing capacity. The findings support a process-knowledge model of health literacy among older adults, and have implications for selecting health literacy measures in various health care contexts.",2011,10.1080/10810730.2011.604702,no
Modeling and Analysis of Image Dependence and Its Implications for Energy Savings in Error Tolerant Image Processing,"We present an analysis of the relationship between input images and energy consumption in error tolerant image processing. Under aggressive voltage scaling, the output image quality of image processing depends on input images for two reasons: 1) error tolerance among images is naturally disparate in terms of perceptual image quality assessment, and 2) the error rate under aggressive voltage scaling varies by input image types. Based on both effects, the supply voltage can be optimized for a given quality requirement so as to achieve ultralow power/energy dissipation. Our analysis demonstrates the significance of the accurate delay estimation, which depends on not only combinational inputs but also the previous state of the logic. We present a new sequential model for accurate error estimation. Based on the model, our experimental results demonstrate that different input image types lead to very different output quality. We also present the effect of process variation on the relationship between input image and output quality. The dependence of energy consumption on input images provides a new perspective for low-power multimedia and image processing system design.",2011,10.1109/TCAD.2011.2126573,no
The Pan-STARRS Synthetic Solar System Model: A Tool for Testing and Efficiency Determination of the Moving Object Processing System,"We present here the Pan-STARRS Moving Object Processing System (MOPS) Synthetic Solar System Model (S3M), the first-ever attempt at building a comprehensive flux-limited model of the major small-body populations in the solar system. The goal of the S3M is to provide a valuable tool in the design and testing of the MOPS software, and will be used in the monitoring of the upcoming Pan-STARRS 1 all-sky survey, which started science operations during late spring of 2010. The model is composed of synthetic populations of near-Earth objects (NEOs with a subpopulation of Earth impactors), the main-belt asteroids (MBAs), Jovian Trojans, Centaurs, trans-Neptunian objects (classical, resonant, and scattered trans-Neptunian objects [TNOs]), Jupiter-family comets (JECs), long-period comets (LPCs), and interstellar comets. The model reasonably reproduces the true populations to a minimum of V = 24.5, corresponding to approximately the expected limiting magnitude for Pan-STARRS's ability to detect moving objects. The NEO synthetic population has been extended to H < 25 (corresponding to objects of about 50 m in diameter), allowing for close flybys of the Earth to be modeled.",2011,,no
Problem-Solution Process by Means of a Hierarchical Metacognitive Model,"We propose a Metacognitive Model devoted to problem-solving. It stimulates abstraction, modification, and instantiation metacognitive activities. Our model holds a hierarchical structure, a learning paradigm, and a workflow to skills acquisition. Such a model is a reference for problem-solving processes.",2011,,no
Development and Validation of Mathematical Model for Tailrace Tunnel Ventilating Process in Hydropower Station,"When outdoor air flowing through the tailrace tunnel, it will be handled and its thermodynamic conditions will change under the differences of temperature and water vapor pressure between air and water surface, air and tunnel wall. Utilizing the handled air for space cooling in hydropower station is an energy saving, environmental protection and renewable application of natural cold source. In this paper, a detailed quasi-three-dimensional mathematical model of heat and moisture transfer of tailrace tunnel ventilating was developed and validated against the field test data from Yingxiuwan hydropower station, and the validation shows that the model predicts the test results very well. The model can be used to predict the heat and moisture performance of tailrace tunnel ventilating system.",2011,10.4028/www.scientific.net/AMM.71-78.4069,no
Process to measure particulate down-converting phosphors and create well-correlated software models of LED performance,"White light-emitting diodes that use down-converting phosphors have been utilized in the illumination industry for several years. In many cases, little information needs to be known about the physics and performance of the phosphor itself to design, optimize, and simulate the light emission of the LED for the purpose of creating secondary optics. However, the importance of accurately accounting for the effect of the phosphor cannot be overstated when designing the LED package or when performing a tolerance analysis, for instance. The difficulties in gathering or measuring the relevant performance metrics of the phosphors are significant barriers to achieving accurate predictions in illumination software packages. This paper describes a simple, repeatable process to measure several phosphor performance metrics that are used, in turn, to create a model of the same phosphor in a commercially-available illumination software package. The measured values are used either as direct inputs or are used to derive the proper inputs for the software. Derivations and discussion about the software model are included. The performance of the simulated phosphor will then be compared and correlated to the physical measurements. Finally, a model of an LED that uses this phosphor model is built in software and its simulated performance is compared to measured values.",2011,10.1117/12.875259,no
Modeling and Analysis of Operator Effects on Process Quality and Throughput in Mixed Model Assembly Systems,"With the increase of market fluctuation, assembly systems moved from a mass production scheme to a mass customization scheme. Mixed model assembly systems (MMASs) have been recognized as enablers of mass customization manufacturing. However, effective implementation of MMASs requires, among other things, a highly proactive and knowledgeable workforce. Hence, modeling the performance of human operators is critically important for effectively operating these manufacturing systems. But, certain cognitive factors have seldom been considered when it comes to modeling process quality of MMASs. Thus, the objective of this paper is to introduce an integrated modeling framework by considering the factors-both intrinsic (such as work experience, mental deliberation time, etc.) and extrinsic (such as task complexity)-that affect the operator's performance. The proposed model is justified based on the findings presented in the psychological literature. The effect of these factors on process operation performance is also investigated; these performance measures include process quality, throughput, and process capability in regard to handling complexity induced by product variety in MMASs. Two examples are used to demonstrate potential applications of the proposed model. [DOI: 10.1115/1.4003793]",2011,10.1115/1.4003793,no
Efficiency enhancement for discharge-pulse processing by adaptive control based on fuzzy models,"A discharge-pulse technology is investigated as a control object, and an inverse model is constructed for electrical-explosion energy conversion on the basis of experimental data with the use of the fuzzy approximation based on a fuzzy set theory. The fuzzy inverse model allows one to synthesize an adaptive control system for electrical-explosion energy conversion, which gives the possibility to provide the desired processing modes at the total space of states of the control object under uncertainty conditions for the technological conditions, the medium's characteristics, and the action of random external disturbances. The application of the adaptive control allows enhancing the efficiency for discharge-pulse technologies by 15-20%.",2012,10.3103/S1068375512040138,no
"Process analysis of regional ozone formation over the Yangtze River Delta, China using the Community Multi-scale Air Quality modeling system","A high O-3 episode was detected in urban Shanghai, a typical city in the Yangtze River Delta (YRD) region in August 2010. The CMAQ integrated process rate method is applied to account for the contribution of different atmospheric processes during the high pollution episode. The analysis shows that the maximum concentration of ozone occurs due to transport phenomena, including vertical diffusion and horizontal advective transport. Gas-phase chemistry producing O-3 mainly occurs at the height of 300-1500 m, causing a strong vertical O-3 transport from upper levels to the surface layer. The gas-phase chemistry is an important sink for O-3 in the surface layer, coupled with dry deposition. Cloud processes may contribute slightly to the increase of O-3 due to convective clouds or to the decrease of O-3 due to scavenging. The horizontal diffusion and heterogeneous chemistry contributions are negligible during the whole episode. Modeling results show that the O-3 pollution characteristics among the different cities in the YRD region have both similarities and differences. During the buildup period, the O-3 starts to appear in the city regions of the YRD and is then transported to the surrounding areas under the prevailing wind conditions. The O-3 production from photochemical reaction in Shanghai and the surrounding area is most significant, due to the high emission intensity in the large city; this ozone is then transported out to sea by the westerly wind flow, and later diffuses to rural areas like Chongming island, Wuxi and even to Nanjing. The O-3 concentrations start to decrease in the cities after sunset, due to titration of the NO emissions, but ozone can still be transported and maintain a significant concentration in rural areas and even regions outside the YRD region, where the NO emissions are very small.",2012,10.5194/acp-12-10971-2012,no
Transient simulation of an endothermic chemical process facility coupled to a high temperature reactor: Model development and validation,"A high temperature reactor (HTR) is a candidate to drive high temperature water-splitting using process heat. While both high temperature nuclear reactors and hydrogen generation plants have high individual degrees of development, study of the coupled plant is lacking. Particularly absent are considerations of the transient behavior of the coupled plant, as well as studies of the safety of the overall plant. The aim of this document is to contribute knowledge to the effort of nuclear hydrogen generation. In particular, this study regards identification of safety issues in the coupled plant and the transient modeling of some leading candidates for implementation in the Nuclear Hydrogen Initiative (NHI). The Sulfur Iodine (SI) and Hybrid Sulfur (HyS) cycles are considered as candidate hydrogen generation schemes. Three thermodynamically derived chemical reaction chamber models are coupled to a well-known reference design of a high temperature nuclear reactor. These chemical reaction chamber models have several dimensions of validation, including detailed steady state flowsheets, integrated loop test data, and bench scale chemical kinetics. The models and coupling scheme are presented here, as well as a transient test case initiated within the chemical plant. The 50% feed flow failure within the chemical plant results in a slow loss-of-heat sink (LOHS) accident in the nuclear reactor. Due to the temperature feedback within the reactor core the nuclear reactor partially shuts down over 1500 s. Two distinct regions are identified within the coupled plant response: (1) immediate LOHS due to the loss of the sulfuric acid decomposition section and (2) continuing slow LOHS due to the chemical species cascade throughout the plant. (C) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.nucengdes.2012.03.049,no
A PROPOSED ALGORITHM FOR THE VALIDATION OF THE OPTIMIZATION MODELS USED IN THE E-BUSINESS AND DECISION PROCESSES,"A major general problem existent in business nowadays is the one related to processes, their management and the links between them. Those elements differentiate business and create different development profiles in similar environmental conditions. Financial success is based on the management decisions taken in order to optimize the internal processes. These decisions, aggregated, will become a dynamic optimization model. This article aims to answer questions like: ""The proposed optimization model is a valid one?"", ""Does it brings a performance increase so as to justify its implementation?"", ""If the model is valid, what is the level of efficiency achieved against the expected one?"".",2012,,no
Comprehensive model for a slag bath in electroslag remelting process with a current-conductive mould,"A mathematical model was developed to describe the interaction of multiple physical fields in a slag bath during electroslag remelting (ESR) process with a current-conductive mould. The distributions of current density, magnetic induction intensity, electromagnetic force, Joule heating, fluid flow and temperature were simulated. The model was verified by temperature measurements during remelting 12CrMoVG steel with a slag of 50wt%-70wt% CaF2, 20wt%-30wt% CaO, 10wt%-20wt% Al2O3, and <= 10wt% SiO2 in a 600 mm diameter current-conductive mould. There is a good agreement between the calculated temperature results and the measured data in the slag bath. The calculated results show that the maximum values of current density, electromagnetic force and Joule heating are in the region between the corner electrodes and the conductivity element. The characteristics of current density distribution, magnetic induction intensity, electromagnetic force, Joule heating, velocity patterns and temperature profiles in the slag bath during ESR process with current-conductive mould were analyzed.",2012,10.1007/s12613-012-0555-9,no
Modeling and process evaluation of membrane bioreactor for removing biodegradable organic matter from water,"A membrane bioreactor (MBR) process was employed for removing biodegradable organic matter (BOM) and ozonation disinfection byproducts (DBPs) exemplified by total aldehydes from ozonated potable water. The BOM removal was to prevent or reduce microbial regrowth in water distribution systems and to reduce DBP forming potential after ozonation. A mathematical modeling approach was used as a tool for performance prediction and process design with implication to process upscaling. The modeling protocol integrated adsorption and biodegradation in liquid and adsorbent phases with model parameters obtained from independent experiments. The BOM was expressed as biodegradable dissolved organic carbon (BDOC), assimilable organic carbon (AOC), and total aldehydes. The MBR studies demonstrated the process effectiveness in removing AOC, BDOC and total aldehydes, and validated the model's predictive capability. Sensitivity studies qualitatively evaluated parameters influencing process dynamics. The simulation studies were unique in examining shutdown and startup effects on process dynamics and process recovery. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.ces.2012.08.013,no
Real-time product quality control for batch processes based on stacked least-squares support vector regression models,"A novel real-time final product quality control method for batch operations based on stacked least-squares support vector regression models (stacked LSSVR) is proposed. It combines midcourse correction (MCC) and batch-to-batch control. To enhance the model prediction accuracy and generalization capability, a stacked LSSVR approach is presented. Quality control is achieved by predicting the final product quality using stacked LSSVR models and adjusting process variables at some pre-specified decision points. Then a decision is made on whether or not control action is taken at every decision point. Once the control action is expected, the manipulated variable values are calculated and the control action is taken to bring the off-spec product quality back to the target. Then a batch-to-batch control is used to overcome the model plant mismatches and unmeasured disturbances. At last, the proposed modeling and quality control strategy is illustrated on a simulated batch reactor. (C) 2011 Elsevier Ltd. All rights reserved.",2012,10.1016/j.compchemeng.2011.05.015,no
Deterministic and stochastic model based run-to-run control for batch processes with measurement delays of uncertain duration,"A novel run-to-run control algorithm integrating deterministic and stochastic model based control is developed for batch processes with measurement delays of uncertain duration. This control algorithm is referred to as deterministic and stochastic model based control (DSMBC). The deterministic component responds quickly to deterministic changes while the stochastic component minimizes the effects arising from measurement delays of uncertain duration. The deterministic component uses a linear process model with parameters that are updated online. The stochastic component uses an error probability density function (PDF) to characterize the effects due to measurement delays and this error PDF is determined from deviations between the set-point and the available process output. To integrate the two control algorithms, the control input is determined by minimizing the weighted sum of the predicted error from the deterministic model and the information entropy of the error probability density distribution. Using a simulated setting where the rate of chemical vapor deposition is controlled, the performance of the proposed DSMBC is shown to be superior to that of EWMA. (C) 2011 Elsevier Ltd. All rights reserved.",2012,10.1016/j.jprocont.2011.10.011,no
Process validation of urban freight and logistics models,"A number of innovative modelling approaches for the analysis of urban freight demand and its impact upon the built environment and transport infrastructure have been proposed over the past several years. These range from new and more robust synthetic models to tour-based formulations based on truck survey data to agent-based microsimulation models. As impressive as these contributions are, most have only included nominal validation efforts, typically limited to comparing the flow estimates to observed traffic counts. In many cases in both research and practice the quality and quantity of these counts are disappointing, and definitive conclusions about model validity and accuracy are difficult to draw from them. Fortunately, increasing the number of counts is far from the only option open to modellers. A far more expansive practice known as process validation can not only overcome the limitations of count data, but admit a far wider spectrum of information, data, and knowledge to the task. This paper illustrates how the process was applied to a tour-based microsimulation model of urban freight, and offers suggestions how it can be more widely applied to freight and logistics models. (C) 2012 Published by Elsevier Ltd. Selection and/or peer-review under responsibility of the 7th International Conference on City Logistics",2012,10.1016/j.sbspro.2012.03.117,no
Empirical assessment of the Maximum Likelihood Estimator quality in a parametric counting process model for recurrent events,"A particular parametric model, based on the counting process theory, and aimed at the analysis of recurrent events is explored. The model is built in the context of reliability of repairable systems and is used to analyze failures of water distribution pipes. The proposed model accounts for aging of systems, for harmful effects of events on the state of systems, and for covariates, both fixed and varying in time. The parameters assessing the aging and the effects of fixed covariates are largely explored in the literature on recurrent events modeling and are considered as typical parameters, whereas the parameters assessing the harmful effects of events on the state of systems and the effects of time-dependent covariates are considered to be original and model-specific. The general usability of the model is empirically assessed in terms of normality and unbiasedness of the Maximum Likelihood Estimator (MLE) of model parameters. The results of a Monte Carlo study for the MLE are presented. The asymptotic behavior of the MLE is explored according to two asymptotic directions: the number of individuals under observation and the duration of the observation. Other possible scales, combining these two directions and governing the asymptotic behavior of the MLE, are also explored. The empirically stated asymptotic properties of the MLE are partially consistent with the theoretical results presented in the literature for typical model parameters. The model-specific parameters present specific trends in asymptotic behavior. The empirical results suggest that the number of observed events can uniquely govern the asymptotic behavior of typical parameters. Model-specific parameters may additionally depend on other criteria. (C) 2011 Elsevier B.V. All rights reserved.",2012,10.1016/j.csda.2011.08.003,no
A process-based fire parameterization of intermediate complexity in a Dynamic Global Vegetation Model,"A process-based fire parameterization of intermediate complexity has been developed for global simulations in the framework of a Dynamic Global Vegetation Model (DGVM) in an Earth System Model (ESM). Burned area in a grid cell is estimated by the product of fire counts and average burned area of a fire. The scheme comprises three parts: fire occurrence, fire spread, and fire impact. In the fire occurrence part, fire counts rather than fire occurrence probability are calculated in order to capture the observed high burned area fraction in areas of high fire frequency and realize parameter calibration based on MODIS fire counts product. In the fire spread part, post-fire region of a fire is assumed to be elliptical in shape. Mathematical properties of ellipses and some mathematical derivations are applied to improve the equation and assumptions of an existing fire spread parameterization. In the fire impact part, trace gas and aerosol emissions due to biomass burning are estimated, which offers an interface with atmospheric chemistry and aerosol models in ESMs. In addition, flexible time-step length makes the new fire parameterization easily applied to various DGVMs. Global performance of the new fire parameterization is assessed by using an improved version of the Community Land Model version 3 with the Dynamic Global Vegetation Model (CLM-DGVM). Simulations are compared against the latest satellite-based Global Fire Emission Database version 3 (GFED3) for 1997-2004. Results show that simulated global totals and spatial patterns of burned area and fire carbon emissions, regional totals and spreads of burned area, global annual burned area fractions for various vegetation types, and interannual variability of burned area are reasonable, and closer to GFED3 than CLM-DGVM simulations with the commonly used Glob-FIRM fire parameterization and the old fire module of CLM-DGVM. Furthermore, average error of simulated trace gas and aerosol emissions due to biomass burning is 7% relative to GFED3. Results suggest that the new fire parameterization may improve the global performance of ESMs and help to quantify fire-vegetation-climate interactions on a global scale and from an Earth system perspective.",2012,10.5194/bg-9-2761-2012,no
SANI (R) process realizes sustainable saline sewage treatment: Steady state model-based evaluation of the pilot-scale trial of the process,"A steady state model was developed for evaluating the sulfur cycle based SANI (R) process. The model comprises: 1) a COD-based anaerobic hydrolysis kinetics model to determine removal of biodegradable COD and sulfate under different hydraulic retention time (HRT) and sludge retention time (SRT), 2) an element (C, H, O, N, P, S), COD and charge mass balanced stoichiometric part for prediction of the concentrations of alkalinity (H2CO3* alkalinity + H2S alkalinity), COD, sulfate, sulfide, nitrate and free saline ammonia in anaerobic sulfate reduction, anoxic autotrophic denitrification and aerobic autotrophic nitrification, and 3) an inorganic carbon (HCO3-) and sulfide (H2S/HS-) mixed weak acid/base chemistry part for pH prediction. Through characterization of the sewage organic matter and determination of the anaerobic hydrolysis kinetic rate and other relevant parameters, the steady state model was calibrated to a pilot plant for the SANI (R) process. The model predictions agreed well with the experimental data of the pilot-scale trial, demonstrating that the model developed from this study can explain the causes and conditions for the different bioprocesses and minimal sludge production in the SANI (R) process. (C) 2011 Elsevier Ltd. All rights reserved.",2012,10.1016/j.watres.2011.11.031,no
A preliminary exposure assessment model for Bacillus cereus cells in a milk based beverage: Evaluating High Pressure Processing and antimicrobial interventions,"A stochastic exposure assessment model was developed to predict the final load (Nf, CFU/mL) of Bacillus cereus cells after being treated by High Pressure Processing (HPP) (100, 200, 300 MPa) and stored (10 degrees C, 15 days), in a milk-egg-cocoa (12% rich in polyphenols) mixture beverage. Results indicated that beverage supplemented with cocoa powder had final B. cereus concentration level of 6 CFU/mL at 95% probability, being below the infectious dose (10(4)-10(5) CFU per mL) (Granum & Baird-Parker, 2000). The most important factors affecting the final load of B. cereus cells in this beverage were ranged as follows: cocoa supplementation > HPP treatment > initial load (Log N-0, CFU/mL) > and storage time (t(STORAGE), days). Those results show one more the usefulness of Monte Carlo simulation in performing exposure assessment studies focused on safety management at an industrial level. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.foodcont.2012.01.063,no
Development and validation of NIR model using low-concentration calibration range: rapid analysis of Lonicera japonica solution in ethanol precipitation process,"A strategy for both low-concentration calibration range and large number of sample sets selection in the development of a PLS model is studied. A novel approach based on accuracy profile validated the accuracy and precision of the PLS model. The strategy was applied to the determination of chlorogenic acid content in Lonicera japonica using ethanol precipitation by near-infrared (NIR) transmission spectroscopy. The results found the determination coefficient (R-2), standard errors of calibration and prediction (SEC and SEP) were 0.9648, 71.2 ppm and 74.9 ppm, respectively. The further study showed that PLS model could be used to determine chlorogenic acid content based on accuracy profile, which has a lower limit of quantification (LLOQ) similar to 1700 ppm. Analytical properties such as accuracy, precision, range and linearity from validation criteria also demonstrated the feasibility of the strategy using a low-concentration calibration set in the PLS model, paving the way for analyses in Chinese Herbal Medicine (CHM) applications.",2012,10.1039/c2ay05607k,no
Automated Comparison of Process Improvement Reference Models based on Similarity Metrics,"A variety of reference models such as CMMI, COBIT or ITIL supports IT organizations to improve their processes. Although these process improvement reference models (IRM) cover different domains they also share some similarities. There are organizations that address multiple domains and want to take the guidance of different IRMs. As IRMs overlap in some processes, we present an approach to compare parts of IRMs (the IRMs' procedures) that is based on a common IRM integration model and on similarity metrics. Our approach enables organizations to efficiently adopt and assess multiple IRMs by automatically identifying similarities and specific details of the different IRMs.",2012,10.1109/APSEC.2012.150,no
Evaluation of the effect of plant sterols on the intestinal processing of cholesterol using an in vitro lipolysis model,"An in vitro lipolysis model was utilized to study the effect of stigmastanol (lipophilic phytosterol) and disodium ascorbyl phytostanol phosphate (DAPP) (modified hydrophilic phytostanol) on intestinal processing of cholesterol to gain further understanding of their cholesterol lowering mechanism. Lipolysis results showed that stigmastanol, if given in powder alone, had no effect on cholesterol processing probably due to its poor solubility. Stigmastanol suspension formulation re-distributed cholesterol from aqueous phase to oil and sediment phases. The water soluble DAPP has changed cholesterol distribution even more significantly by transferring cholesterol from aqueous phase to sediment phase. Moreover, the results provided evidence that DAPP inhibited triglyceride digestion in vitro. Considering DAPP as a surfactant with the same lipophilic sterol ring as bile salt, its ability to inhibit triglyceride lipolysis may be due to its competition with bile salt for the substrate surface, thereby hindering the lipolysis of triglyceride and inhibiting cholesterol solubilization with the lipolysis products. It can be speculated that the cholesterol lowering mechanism of DAPP during intestinal digestion is related to its ability to act as a surfactant closely resembling bile salt. (c) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.ijpharm.2012.07.040,no
Markov model of fatigue of a composite material with the poisson process of defect initiation,"As a development of the model where only one weak microvolume (WMV) and only a pulsating cyclic loading are considered, in the current version of the model, we take into account the presence of several weak sites where fatigue damage can accumulate and a loading with an arbitrary (but positive) stress ratio. The Poisson process of initiation of WMVs is considered, whose rate depends on the size of a specimen. The cumulative distribution function (cdf) of the fatigue life of every individual WMV is calculated using the Markov model of fatigue. For the case where this function is approximated by a lognormal distribution, a formula for calculating the cdf of fatigue life of the specimen (modeled as a chain of WMVs) is obtained. Only a pulsating cyclic loading was considered in the previous version of the model. Now, using the modified energy method, a loading cycle with an arbitrary stress ratio is ""transformed"" into an equivalent cycle with some other stress ratio. In such a way, the entire probabilistic fatigue diagram for any stress ratio with a positive cycle stress can be obtained. Numerical examples are presented.",2012,10.1007/s11029-012-9267-5,no
Bayesian model selection validates a biokinetic model for zirconium processing in humans,"Background: In radiation protection, biokinetic models for zirconium processing are of crucial importance in dose estimation and further risk analysis for humans exposed to this radioactive substance. They provide limiting values of detrimental effects and build the basis for applications in internal dosimetry, the prediction for radioactive zirconium retention in various organs as well as retrospective dosimetry. Multi-compartmental models are the tool of choice for simulating the processing of zirconium. Although easily interpretable, determining the exact compartment structure and interaction mechanisms is generally daunting. In the context of observing the dynamics of multiple compartments, Bayesian methods provide efficient tools for model inference and selection. Results: We are the first to apply a Markov chain Monte Carlo approach to compute Bayes factors for the evaluation of two competing models for zirconium processing in the human body after ingestion. Based on in vivo measurements of human plasma and urine levels we were able to show that a recently published model is superior to the standard model of the International Commission on Radiological Protection. The Bayes factors were estimated by means of the numerically stable thermodynamic integration in combination with a recently developed copula-based Metropolis-Hastings sampler. Conclusions: In contrast to the standard model the novel model predicts lower accretion of zirconium in bones. This results in lower levels of noxious doses for exposed individuals. Moreover, the Bayesian approach allows for retrospective dose assessment, including credible intervals for the initially ingested zirconium, in a significantly more reliable fashion than previously possible. All methods presented here are readily applicable to many modeling tasks in systems biology.",2012,10.1186/1752-0509-6-95,no
Research on a New Model System Based on Improved Process and Measurement Models,"Based on the characteristic and dedicated field of an enterprise and feature of its internal projects, a new set of clear and proper CMMI model system is described briefly by the example of process and measurement models involved in software requirement.",2012,10.1109/ICICEE.2012.294,no
ERROR BOUND FOR PIECEWISE DETERMINISTIC PROCESSES MODELING STOCHASTIC REACTION SYSTEMS,"Biological processes involving the random interaction of d species with integer particle numbers are often modeled by a Markov jump process on N-0(d). Realizations of this process can, in principle, be generated with Gillespie's classical stochastic simulation algorithm, but for very reactive systems this method is usually inefficient. Hybrid models based on piecewise deterministic processes offer an attractive alternative which can decrease the simulation time considerably in applications where species with rather low particle numbers interact with very abundant species. We investigate the convergence of the hybrid model to the original one for a class of reaction systems with two distinct scales. Our main result is an error bound which states that, under suitable assumptions, the hybrid model approximates the marginal distribution of the discrete species and the conditional moments of the continuous species up to an error of O(M-1), where M is the scaling parameter of the partial thermodynamic limit.",2012,10.1137/120871894,no
A BPMN Extension for Including Data Quality Requirements in Business Process Modeling,"BPMN is a notation for business process modeling through which it is possible to represent multiple characteristics of the analyzed business processes. However, although in a business process data play a fundamental role, it is still not possible to model data quality issues using BPMN due mainly to the lack of a specific notation. Since data quality is one of the main elements for achieving the business process goals, we aim to develop a comprehensive framework that supports the design of data quality-aware business processes. In this paper, we mainly focus on the part related to the elicitation and definition of data quality requirements and we present an extension of BPMN suitable to include them at a business process modeling level.",2012,,no
Proposal of Square Metrics for Measuring Business Process Model Complexity,Business Process (BP) metrics are used for controlling the quality and improving models. We give an overview of the existing metrics for describing various aspects of BP models. We propose simple yet practical square metrics for describing complexity of a BP model. These metrics are easy to interpret and provide some basic information about the structural complexity of the model. The proposed metrics are to be used with models built with Business Process Model and Notation (BPMN). It is currently the most widespread language used for BP modeling.,2012,,no
Modeling and Performance Evaluation of BPEL Processes: A Stochastic-Petri-Net-Based Approach,"Business Process Execution Language (BPEL) is considered as the de facto standard for Web service composition. To analyze the performance of composite service processes specified in BPEL gives the way to tell whether the process meets the performance requirements. In this paper, we propose a translation-based approach for performance analysis of BPEL processes, which employs a general stochastic Petri net (GSPN) as the intermediate representation. A set of translation rules is defined for constructs and activities of BPEL so that the processes specified in BPEL can be translated into the GSPN representations. Based on the GSPN representation of BPEL processes, we introduce a state-space method to calculate the expected-process-normal-completion-time as the performance estimate. In the case study, we obtain experimental data and conduct a confidence interval analysis to validate the feasibility and accuracy of the translation-based approach.",2012,10.1109/TSMCA.2011.2164064,no
Detecting structural errors in BPMN process models,"Business Process Modeling Notation (BPMN) has emerged as a standard notation to express the business process models. A lack of formal semantics in the BPMN can cause the syntactic and structural errors. The former requires less effort to be checked, while the later usually needs a complex state-space analysis to prove some properties, like the deadlock-freedom and the livelock-freedom. In this paper, we present an approach based on model checking for the automated verification of business process models. We illustrate the deadlocks, livelocks, and multiple termination problems, which can help the business modelers to avoid structural errors.",2012,,no
Evaluation of antioxidant activity of carotenoid extract from shrimp processing byproducts by in vitro assays and in membrane model system,"Carotenoid extracts from shrimp processing discards were evaluated for antioxidant activity. Crude extract and fractions rich in astaxanthin showed strong antioxidant activity as indicated by radical scavenging, reducing activity and metal chelating activity, comparable to that of the known antioxidants alpha-tocopherol and TBHQ. Singlet oxygen quenching activity of crude extract and its fractions was higher than that of alpha-tocopherol. Nitric oxide scavenging activity was also higher than alpha-tocopherol. The ability of the astaxanthin-rich fraction to inhibit the thermal oxidation of phospholipid liposomes, with a protection factor of 22.6 +/- 1.7 units, was better than that of alpha-tocopherol (8.5 +/- 1.5 units). The higher antioxidant activity of the astaxanthin-rich fraction was also indicated by a higher protection factor (14.1 +/- 3.2 units) compared to alpha-tocopherol (6.2 +/- 0.1 units) against singlet oxygen mediated oxidation of liposomes. The results indicate the potential of shrimp carotenoid extract as a natural antioxidant for possible use in food and biomedical applications. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.foodchem.2012.02.147,no
An evaluation of Substitute natural gas production from different coal gasification processes based on modeling,"Coal and lignite will play a significant role in the future energy production. However, the technical options for the reduction of CO2 emissions will define the extent of their share in the future energy mix. The production of synthetic or substitute natural gas (SNG) from solid fossil fuels seems to be a very attractive process: coal and lignite can be upgraded into a methane rich gas which can be transported and further used in high efficient power systems coupled with CO2 sequestration technologies. The aim of this paper is to present a modeling analysis comparison between substitute natural gas production from coal by means of allothermal steam gasification and autothermal oxygen gasification. In order to produce SNG from syngas several unit operations are required such as syngas cooling, cleaning, potential compression and, of course, methanation reactors. Finally the gas which is produced has to be conditioned i.e. removal of unwanted species, such as CO2 etc. The heat recovered from the overall process is utilized by a steam cycle, producing power. These processes were modeled with the computer software IPSEpro (TM). An energetic and exergetic analysis of the coal to SNG processes have been realized and compared. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.energy.2012.03.075,no
Evaluating the Effects of Climate Changes and LUCC on the Hydrological Processes Using Soil and Water Assessment Tool Models in Wangkuai Reservoir Watershed in China,"Comprehensive management of lake water's quality and quantity from river eco-hydrological aspect has become an increased concern in terms of lake water ecology and environment; Baiyandian is the biggest natural lake and an important eco-function area in North China. However, due to the sharp decrease of water quantity from upstream basin, ecological and environmental problems have occurred in this lake, including frequent drying up, water pollution, and the extinction of valuable species. In this study, a small catchment the Wangkuai reservoir watershed which is located in the upstream Baiyangdian was selected as the research area. Using 3S topography and land use data as inputs for a distributed hydrological process model (SWAT), we simulated the water cycle process in Wangkuai reservoir based on changes in climate and land use-land. We elementarily analyzed the main cause of the sharp decrease of water flow in the small watershed Wangkuai Reservoir. The results of the research showed that, in the 50 years between 1958 and 2007, the ten-year average runoff decreased by 75.04%; from 1985 to 2007, warming and drying climate trend decreased the average annual runoff by 64.22%; from 1985 to 2007, the area of wood land in this watershed decreased by 13.27%, grass land and farmland increased by 4.89% and 26.15%, respectively; the contribution rate of climate changes and land use-land cover changes to the decrease of runoff were about 60% and 40%, from which we could see that climate change was the main cause of the decrease of runoff. This research could provide reference for water resource management and water environment protection of Wangkuai Reservoir.",2012,10.1166/sl.2012.1853,no
Berry-Esseen bounds for wavelet estimator in semiparametric regression model with linear process errors,"Consider the semiparametric regression model Yi = xi beta + g(ti)_ + epsilon i, i = 1, ... , n, where the linear process errors epsilon(i) = Sigma(infinity)(j=-infinity) ajei-j with Sigma(infinity)(j=-infinity) vertical bar a(j)vertical bar < infinity, and {e(i)}are identically distributed and strong mixing innovations with zero mean. Under appropriate conditions, the Berry- Esseen type bounds of wavelet estimators for b and g(center dot) are established. Our results obtained generalize the results of nonparametric regression model by Li et al. to semiparametric regression model.",2012,10.1186/1029-242X-2012-44,no
Interactive Effects of Habitual Cuing and Media Features on Evaluation: A Dual-Process Model,"Consumers tend to express positive attitudes toward communication sources that reflect their predispositions toward these sources, that is, their habitual cuing. To move beyond the effects of communication sources such as advertising, word of mouth (WOM), and critical reviews, this article applies a dual-process model to determine how the interaction of habitual cuing and the media features of a movie influence evaluations of movie advertising. Six quasi-experimental designs and an analysis of variance to assess the data indicate that greater cultural discount of an original movie leads to more favorable consumer evaluations when the habitual cuing has come through WOM rather than reviews; numbered sequels also gain favorable evaluations as a result of forward spillover effects or reduced satiation. Finally, for named sequels, consumers exhibit more favorable evaluations when habitual cuing comes through reviews rather than WOM.",2012,10.1080/15213269.2012.675878,no
Clinical reasoning processes: unravelling complexity through graphical representation,"CONTEXT Clinical reasoning is a core skill in medical practice, but remains notoriously difficult for students to grasp and teachers to nurture. To date, an accepted model that adequately captures the complexity of clinical reasoning processes does not exist. Knowledge-modelling software such as MOT Plus (Modelling using Typified Objects [MOT]) may be exploited to generate models capable of unravelling some of this complexity. OBJECTIVES This study was designed to create a comprehensive generic model of clinical reasoning processes that is intended for use by teachers and learners, and to provide data on the validity of the model. METHODS Using a participatory action research method and the established modelling software (MOT Plus), knowledge was extracted and entered into the model by a cognitician in a series of encounters with a group of experienced clinicians over more than 250 contact hours. The model was then refined through an iterative validation process involving the same group of doctors, after which other groups of clinicians were asked to solve a clinical problem involving simulated patients. RESULTS A hierarchical model depicting the multifaceted processes of clinical reasoning was produced. Validation rounds suggested generalisability across disciplines and situations. CONCLUSIONS The MOT model of clinical reasoning processes has potentially important applications for use within undergraduate and graduate medical curricula to inform teaching, learning and assessment. Specifically, it could be used to support curricular development because it can help to identify opportune moments for learning specific elements of clinical reasoning. It could also be used to precisely identify and remediate reasoning errors in students, residents and practising doctors with persistent difficulties in clinical reasoning.",2012,10.1111/j.1365-2923.2012.04242.x,no
A Comprehensive Static Model for COREX Process,"COREX is the first industrially proven smelting reduction process in the world. In this paper, a static model has been developed based on mass and heat balances. Further, a zoned model for the melter-gasifier was established on the basis of the static model. The model is capable of calculating the consumption of iron ores, coal, fluxes, the volume and composition of the slag and the volume and composition of the reducing gas from melter-gasifier. The model enables the examination of the changes of material and heat flows induced by the variations of operation parameters and raw materials chemical compositions. The model allows optimization of operation parameters of the COREX process, which can thus be used to improve the plant operation under different conditions.",2012,10.2355/isijinternational.52.2186,no
Non-planar fault models: complexities induced by crustal layering in transcurrent faulting processes,"Crack models suitable to describe transcurrent faulting in the presence of a softer shallow layer are presented. We employ the asymptotic theory of generalized Cauchy kernel equations to study the singular behaviour of a strike-slip crack crossing the welded interface between two different media. If a planar crack cuts vertically across a material discontinuity, the dislocation density is bounded at the interface, but a stress drop discontinuity condition must be met, in which the stress drop is proportional to the local rigidity. Such a condition cannot be fulfilled in several cases. Two antiplane strain models are proposed to avoid this difficulty. In the first model (Fault bending model), the fault surface is affected by a sharp change of the dip angle at the intersection with the interface. An unbounded singularity in the dislocation density distribution (and in the stress field) appears at the interface and its dependence from model parameters is studied. A generalized stress drop condition is obtained which can be interpreted in terms of the interaction of the crack with the welded boundary condition. In the second model (Fault branching model), the crack is split into three interacting sections, with the lower section vertical and the other two sections inclined with respect to the vertical. In this case the order of the singularity at the interface remains undetermined. This result may be interpreted in terms of the further degree of freedom introduced by the presence of a second fracture in the upper layer. In the case of branching, two conditions are obtained for the stress drop on the three sections. According to both models, the generalized stress drop conditions may account for a stress drop ratio, between the soft layer and the hard basement, lower than the rigidity ratio.",2012,10.1111/j.1365-246X.2012.05517.x,no
Evaluation of an integrated Knowledge Discovery and Data Mining process model,"Data Mining projects are implemented by following the knowledge discovery process. This process is highly complex and iterative in nature and comprises of several phases, starting off with business understanding, and followed by data understanding, data preparation, modeling, evaluation and deployment or implementation. Each phase comprises of several tasks. Knowledge Discovery and Data Mining (KDDM) process models are meant to provide prescriptive guidance towards the execution of the end-to-end knowledge discovery process, i.e. such models prescribe how exactly each one of the tasks in a Data Mining project can be implemented. Given this role, the quality of the process model used, affects the effectiveness and efficiency with which the knowledge discovery process can be implemented and therefore the outcome of the overall Data Mining project. This paper presents the results of the rigorous evaluation of the Integrated Knowledge Discovery and Data Mining (IKDDM) process model and compares it to the CRISP-DM process model. Results of statistical tests confirm that the IKDDM leads to more effective and efficient implementation of the knowledge discovery process. (c) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.eswa.2012.02.044,no
Multiple model based LPV soft sensor development with irregular/missing process output measurement,"Data-driven soft sensors have been applied extensively in process industry for process monitoring and control. Linear soft sensors, which are only valid within a relatively small operating envelope, are considered to be insufficient in practice when the processes transit among several operating modes. Moreover, owing to a variety of causes such as malfunction of sensors, multiple rate sampling scheme for different process variables, etc., missing data problem is commonly experienced in process industry. In this paper, soft sensor development with irregular/missing output data is considered and a multiple model based linear parameter varying (LPV) modeling scheme is proposed for handling nonlinearity. The efficiency of the proposed algorithm is demonstrated through several numerical simulation examples as well as a pilot-scale experiment. It is shown through the comparison with the traditional missing data treatment methods in terms of the parameter estimation accuracy that the developed soft sensors enjoy improved performance by employing the expectation-maximization (EM) algorithm in handling the missing process data and model switching problem. (C) 2011 Elsevier Ltd. All rights reserved.",2012,10.1016/j.conengprac.2011.10.007,no
Dataflow Errors Detection in Business Process Model,"Despite the abundance of analysis techniques to discover control-flow errors in workflow designs, there is hardly any support for data-flow verification. Most techniques simply abstract from data while data dependencies can be the source of all kinds of errors. This paper focuses on the discovery of data-flow errors in workflows. There are many issues in the dataflow, like Redundant Data, Lost Data, Missing Data, Mismatched data, Inconsistent data, and Misdirected data. We present an analysis approach that uses so called ""The RWD Boolean Table Technique"" expressed in steps, I Split dataflow from control flow; II Create Boolean table for each data element/s; III Make comparative between RWD Boolean table for current task and next task until get the end of workflow; Typical errors include dataflow issue, like redundant Data, lost Data and missing Data.",2012,10.4028/www.scientific.net/AMM.130-134.1765,no
IC Immunity Modeling Process Validation Using On-Chip Measurements,Developing integrated circuit (IC) immunity models and simulation flow has become one of the major concerns of ICs suppliers to predict whether a chip will pass susceptibility tests before fabrication and avoid redesign cost. This paper presents an IC immunity modeling process including the standard immunity test applied to a dedicated test chip. An on-chip voltage sensor is used to characterize the radio frequency interference propagation inside the chip and thus validate the immunity modeling process.,2012,10.1007/s10836-012-5294-3,no
Energy evaluation method and its optimization models for process planning with stochastic characteristics: A case study in disassembly decision-making,"Disassembly is not only a premise of products recycling, but also an important link of products remanufacturing. However, used products suffer from the influence of a variety of uncertainties. The randomness of disassembly process is a significant feature. In this paper, a disassembly network is established, in which lengths of arc are stochastic variables with a specified power subject to specified distributions and denote removal times of parts, the energy evaluation method integrating two or more uncertain variables is proposed. According to different disassembly decision-making criteria, three types of typical stochastic programming models of a disassembly process are developed, namely the minimum expected value model, the maximum energy disassemblability degree model and D'-minimum energy model. The energy probability distributions are determined through the application of stochastic linear programming and maximum entropy principle. Synchronously, based on obtained theoretical probability distributions, the quantitative evaluation and stochastic programming of a disassembly process are realized. The simulation results show that the proposed method is feasible and effective to solve the stochastic programming issue with time-varying stochastic characteristics. (C) 2011 Elsevier Ltd. All rights reserved.",2012,10.1016/j.cie.2011.08.011,no
Evaluation model and algorithm of product disassembly process with stochastic feature,"Disassembly planning is considered as the optimization of disassembly sequences with the target of the shortest disassembly time, the lowest disassembly cost, and the minimum disassembly energy consumption. However, obsolete products suffer from the influence of a variety of uncertainties, the disassembly process of products has the strong uncertain feature. Traditionally, to account for this uncertainty, each removal operation or removal task is assumed to be an activity or event with certain probability, and the determination of the optimal path of a disassembly process is merely a probabilistic planning problem based on this assumption. In this article, based on the established stochastic disassembly network graph, combined with different disassembly decision-making criterion, typical stochastic models for disassembly time analysis are developed. In addition, a two-phase approach is proposed to solve the typical stochastic models. Initially, according to different removal probability density functions, disassembly probability density functions of feasible disassembly paths are determined by a time-domain method or frequency-domain method, and additionally, after the disassembly probability density functions have been obtained, the quantitative evaluation of a product disassembly process and stochastic optimization of feasible disassembly paths are realized by a numerical solution method. Finally, a numerical example is illustrated to test the proposed concepts and the effectiveness of the proposed approach.",2012,10.1007/s10098-011-0406-9,no
Modeling and Performance Evaluation of a Contract-based Electronic Signature Process,"Distributed systems become ubiquitous by allowing users access to a wide range of services at any time, anywhere, and from a variety of devices. In these open environments where there are many opportunities for both fraudulent services and misbehaving clients, service discovery systems are subject to security challenges. Controlling services' access is one of the fundamental issues that must be faced in the context of service discovery in distributed and open environments. Therefore, secure accesses and utilization of available services must be ensured for users. In our previous work, a contract-based approach for controlling the service access in a distributed computing context was presented. In this paper, we address the purpose and the usage of digital signature on negotiated electronic queries between a server and clients in service discovery systems and web service composition. The paper discusses the combined use of Timed Event Graphs and (max,+)-algebra to model, evaluate and optimize the performance of the signature process and client requests validation by a service provider (server). Based on an optimization resource allocation algorithm, an improvement study of the quality of service offered to the clients, in terms of waiting times and validation of their requests, is proposed. The results are reported and show the efficiency of the use of the proposed formal tools for performance analysis, evaluation and tuning of the considered process.",2012,,no
Biological evaluation of the bone healing process after application of two potentially osteogenic proteins: an animal experimental model,"doi: 10.1111/j.1741-2358.2011.00526.x Biological evaluation of the bone healing process after application of two potentially osteogenic proteins: an animal experimental model Objective: The aim of this work was to analyse qualitatively and quantitatively the newly formed bone after insertion of rhBMP-2 and protein extracted from Hevea brasiliensis (P-1), associated or not with a carrier in critical bone defects created in Wistar rat calvarial bone, using histological and histomorphometrical analyses. Materials and methods: Eighty-four male Wistar rats were used, divided into two groups, according to the period of time until the sacrifice (2 and 6 weeks). Each one of these groups was subdivided into six groups with seven animals each, according to the treatments: (1) 5 mu g of pure rhBMP-2, (2) 5 mu g of rhBMP-2/monoolein gel, (3) pure monoolein gel, (4) 5 mu g of pure P-1, (5) 5 mu g of P-1/monoolein gel and (6) critical bone defect controls. The animals were euthanised and the calvarial bone tissue removed for histological and histomorphometrical analyses. Result and conclusion: The results showed an improvement in the bone healing process using the rhBMP-2 protein, associated or not with a material carrier in relation to the other groups, and this process demonstrated to be time dependent.",2012,10.1111/j.1741-2358.2011.00526.x,no
"Esterification of Acrylic Acid and n-Butanol in a Pilot-Scale Reactive Distillation Column-Experimental Investigation, Model Validation, and Process Analysis","Due to a high risk of polymerization and complex thermodynamic behavior of the chemical system, the current production process for n-butyl acrylate synthesis is cost-intensive and challenging. Reactive distillation integrates chemical reactions and distillation into one unit at the same time and is one of the best-known examples of process intensification. To facilitate industrial application of this concept for the production of n-butyl acrylate, reliable experimental data are required. This article presents an experimental and theoretical investigation of the synthesis of n-butyl acrylate using reactive distillation. Experiments were conducted in a pilot-scale reactive distillation column, and the decisive operational parameters were varied. To predict the experimental results, a nonequilibrium-stage model was applied and the model was validated using the experimental data. The validated model was then used to perform a process analysis, showing trends in the conversion of acrylic acid and n-butanol and the purity of n-butyl acrylate that can be used for prospective optimization studies.",2012,10.1021/ie301934w,no
Application of Thermodynamic Models for Better Understanding and Optimizing the Hall-Heroult Process,"During the last decade, important improvements have been made in the application of thermodynamic models for studying the molten cryolite system used in the Hall-Heroult process. This approach allows a better understanding and paves the way for furthering developments in bath chemistry and molten metal processing. In this article, thermodynamic modeling is used to explore the operating windows in the reduction of alumina in molten cryolite. The impact of a range of concentrations of AlF3, CaF2, and Al2O3 in conventional or ""lithium-free"" baths is also discussed. Subsequently, the model was also used to evaluate the impact of additions of lithium fluoride to the bath. Conditions allowing an operation at lower cell voltages and lower bath temperatures were identified. The modeling approach described in this article is considered as an important innovation to revisit fundamentals, to constantly re-examine paradigms, and to identify potential modifications in bath chemistry for improving energy efficiency and productivity of modern prebaked Hall-Heroult cells.",2012,10.1007/s11837-012-0426-x,no
Concurrent Dynamic Visualizations With Expressive Petri Net Representations to Enrich the Understanding of Biological and Pathological Processes: an Application to Signaling Pathways,"Dynamic visualizations and expressive representations are needed in systems biology to handle multiple interactions occurring during the biological processes of biopathway representations. Dynamic visualizations allow users an ease of interaction with pathway models. At the same time, representations of biopathways should express how interactions take place. In spite of the fact that diverse databases provide users with pathways, their information and representation are frequently different from each other and show restricted interactions because of their static visualization. An adopted solution is to merge diverse representations to obtain a richer one. However, due to different formats and the multiple links involved in the pathway representations, the merge results frequently in erroneous models and in a tangle web of relations very hard to be manipulated. Instead, this work introduces a concurrent dynamic visualization (CDV) of the same pathway, which is retrieved from different sites and then transformed into Petri net representations to facilitate the understanding of their biological processes by interacting with them. We applied this approach to the analysis of the Notch signaling pathway, associated with cervical cancer; we obtained it from different sources which we compared and manipulated simultaneously by interacting with the provided CDV until the user generated a personalized pathway.",2012,,no
Zebrafish Models for Dyskeratosis Congenita Reveal Critical Roles of p53 Activation Contributing to Hematopoietic Defects through RNA Processing,"Dyskeratosis congenita (DC) is a rare bone marrow failure syndrome in which hematopoietic defects are the main cause of mortality. The most studied gene responsible for DC pathogenesis is DKC1 while mutations in several other genes encoding components of the H/ACA RNP telomerase complex, which is involved in ribosomal RNA(rRNA) processing and telomere maintenance, have also been implicated. GAR1/nola1 is one of the four core proteins of the H/ACA RNP complex. Through comparative analysis of morpholino oligonucleotide induced knockdown of dkc1 and a retrovirus insertion induced mutation of GAR1/nola1 in zebrafish, we demonstrate that hematopoietic defects are specifically recapitulated in these models and that these defects are significantly reduced in a p53 null mutant background. We further show that changes in telomerase activity are undetectable at the early stages of DC pathogenesis but rRNA processing is clearly defective. Our data therefore support a model that deficiency in dkc1 and nola1 in the H/ACA RNP complex likely contributes to the hematopoietic phenotype through p53 activation associated with rRNA processing defects rather than telomerase deficiency during the initial stage of DC pathogenesis.",2012,10.1371/journal.pone.0030188,no
Crop planning optimization model: the validation and verification processes,"Each optimization problem in the area of natural resources claims for a specific validation and verification (V&V) procedures which, for overwhelming majority of the models, have not been developed so far. In this paper we develop V&V procedures for the crop planning optimization models in agriculture when the randomness of harvests is considered and complex crop rotation restrictions must hold. We list the criteria for developing V&V processes in this particular case, discuss the restrictions given by the data availability and suggest the V&V procedures. To show its relevance, they are applied to recently constructed stochastic programming model aiming to serve as a decision support tool for crop plan optimization in South Moravian farm. We find that the model is verified and valid and if applied in practice-it thus offers a plausible alternative to standard decision making routine on farms which often leads to breaking the crop rotation rules.",2012,10.1007/s10100-011-0205-8,no
On parameter estimation in population models III: Time-inhomogeneous processes and observation error,"Essential to applying a mathematical model to a real-world application is calibrating the model to data. Methods for calibrating population models often become computationally infeasible when the population size (more generally the size of the state space) becomes large, or other complexities such as time-dependent transition rates, or sampling error, are present. Continuing previous work in this series on the use of diffusion approximations for efficient calibration of continuous-time Markov chains, I present efficient techniques for time-inhomogeneous chains and accounting for observation error. Observation error (partial observability) is accounted for by joint estimation using a scaled unscented Kalman filter for state-space models. The methodology will be illustrated with respect to models of disease dynamics incorporating seasonal transmission rate and in the presence of observation error, including application to two influenza outbreaks and measles in London in the pre-vaccination era. (C) 2012 Elsevier Inc. All rights reserved.",2012,10.1016/j.tpb.2012.03.001,no
Accelerating knowledge-based energy evaluation in protein structure modeling with Graphics Processing Units,"Evaluating the energy of a protein molecule is one of the most computationally costly operations in many protein structure modeling applications. In this paper, we present an efficient implementation of knowledge-based energy functions by taking advantage of the recent Graphics Processing Unit (CPU) architectures. We use DFIRE, a knowledge-based all-atom potential, as an example to demonstrate our CPU implementations on the latest NVIDIA Fermi architecture. A load balancing workload distribution scheme is designed to assign computations of pair-wise atom interactions to threads to achieve perfect or near-perfect load balancing in the symmetric N-body problem in DFIRE. Reorganizing atoms in the protein also improves the cache efficiency in Fermi CPU architecture, which is particularly effective for small proteins. Our DFIRE implementation on GPU (GPU-DFIRE) has exhibited a speedup of up to similar to 150 on NVIDIA Quadro FX3800M and similar to 250 on NVIDIA Tesla M2050 compared to the serial DFIRE implementation on CPU. Furthermore, we show that protein structure modeling applications, including a Monte Carlo sampling program and a local optimization program, can benefit from GPU-DFIRE with little programming modification but significant computational performance improvement. (C) 2011 Elsevier Inc. All rights reserved.",2012,10.1016/j.jpdc.2011.10.005,no
Process-evaluation of tropospheric humidity simulated by general circulation models using water vapor isotopic observations: 2. Using isotopic diagnostics to understand the mid and upper tropospheric moist bias in the tropics and subtropics,"Evaluating the representation of processes controlling tropical and subtropical tropospheric relative humidity (RH) in atmospheric general circulation models (GCMs) is crucial to assess the credibility of predicted climate changes. GCMs have long exhibited a moist bias in the tropical and subtropical mid and upper troposphere, which could be due to the mis-representation of cloud processes or of the large-scale circulation, or to excessive diffusion during water vapor transport. The goal of this study is to use observations of the water vapor isotopic ratio to understand the cause of this bias. We compare the three-dimensional distribution of the water vapor isotopic ratio measured from space and ground to that simulated by several versions of the isotopic GCM LMDZ. We show that the combined evaluation of RH and of the water vapor isotopic composition makes it possible to discriminate the most likely cause of RH biases. Models characterized either by an excessive vertical diffusion, an excessive convective detrainment or an underestimated in situ cloud condensation will all produce a moist bias in the free troposphere. However, only an excessive vertical diffusion can lead to a reversed seasonality of the free tropospheric isotopic composition in the subtropics compared to observations. Comparing seven isotopic GCMs suggests that the moist bias found in many GCMs in the mid and upper troposphere most frequently results from an excessive diffusion during vertical water vapor transport. This study demonstrates the added value of water vapor isotopic measurements for interpreting shortcomings in the simulation of RH by climate models.",2012,10.1029/2011JD016623,no
Implementing In-Stream Nutrient Processes in Large-Scale Landscape Modeling for the Impact Assessment on Water Quality,"For a long time, watershed models focused on the transport of chemicals from the terrestrial part of the watershed to the surface water bodies by leaching and erosion. After the substances had reached the surface water, they were routed through the channel network often without any further transformation. Today, there is a need to extend watershed models with in-stream processes to bring them closer to natural conditions and to enhance their usability as support tools for water management and water quality policies. This paper presents experience with implementing in-stream processes in a ecohydrological dynamic watershed model and its application on the large scale in the Saale River basin in Germany. Results demonstrate that new implemented water quality parameters like chlorophyll a concentrations or oxygen amount in the reach can be reproduced quite well, although the model results, compared with results achieved without taking into account algal and transformation processes in the river, show obvious improvement only for some of the examined nutrients. Finally, some climate and water management scenarios expected to impact in-stream processes in the Saale basin were run. Their results illustrate the relative importance of physical boundary conditions on the amount and concentration of the phytoplankton, which leads to the conclusion that measures to improve water quality should not only take nutrient inputs into account but also climate influences and river morphology.",2012,10.1007/s10666-012-9320-8,no
Online Measuring Method Using an Evolving Model Based Test Design for Optimal Process Stimulation and Modelling,"For data driven modelling the information content of system input data and measured output data is decisive for the achievable model quality of the underlying process. Process stimulation is targeted to maximize the information content per data sample, in order to limit the measurement time. Especially for processes with an increasing number of system inputs the experimental effort is continuously rising. Therefore methods for an efficient process excitation combined with advanced modelling strategies are necessary. In this context online methods, where the design of experiments and the model training are in parallel to the ongoing experiment are a very promising approach for an efficient generation of process models. The compliance with constraints on the system input as well as on the system output is essential in order to provide secure and stable operational conditions during the experiment. In this paper a recursive algorithm is proposed, which uses an evolving local model network for the online generation of optimal dynamic experiments under constraints. The effectiveness of the proposed method is demonstrated on a nonlinear dynamic exhaust temperature model of an engine and a comparison with a standard excitation signal is given.",2012,,no
NETWORK DEFENSE EFFICIENCY METRIC MODEL BASED ON ANALYTIC HIERARCHY PROCESSING,"For making more scientific and comprehensive quantitative measure on the security of the LAN, the method of measuring the defense performance with attack effect is proposed. Attack targets set whose elements include attack duration, control duration, interference duration, infection rates are proposed. And then based on the index set, a network defense performance model using analytic hierarchy process (AHP) method is presented. Simulation results show that the proposed measurement method and model can better measure the security of network protection systems.",2012,,no
Using the REA Approach to Modeling of IT Process Evaluation,"For many businesses, Information Technology (IT) solutions play a strategic role in gaining and maintaining competitive advantage. For several decades, a business information system infrastructure has been persistently developed and hence it is increasingly more complex. Well organized IT processes have become more crucial than ever before. IT executives (CIOs) have to make decisions based on high quality information concerning various features, how IT processes are planned, managed and improved. In order to successfully evaluate and manage IT processes, CIOs are supported by dedicated software and hardware solutions, best practices and standards. The IT processes evaluation is much more effective when the source data comes from tailored solutions for given information needs. There are a lot of opportunities to develop an information system architecture for the IT process evaluation using various software tools. Today, we are facing the important question of whether or not it is possible to create software strictly dedicated to an IT management domain, which also would be an integral part of the ERP architecture and more suitable for SMEs. An attempt to give an answer to this question is the main goal of this paper. We propose to use the Resource-Event-Agent (REA) approach to modeling IT process evaluation. This is an important assumption because REA lets us see an IT management domain both as a set of mutually connected business activities and as a part of the interests of accounting records. Therefore, the REA modeling makes it possible to describe the IT realm to satisfy information needs, for both accountants and non-accountants.",2012,,no
Research of Correlation-Model between Qualities Attributes and Quality Control Points in Assembly Process of the Complex Product Based on Network Flow,"For representing the coupling relationships among various correlate elements in the formation process of assembly qualities, the mapping relations between the structure domain and the quality domain was established on the basis of analyzing the hybrid structure assembly. Through classifying the quality characteristics, the quality domain was divided into two sub-sets which are quality control points set and quality attributes set. Defining the relevant elements in the model, such as vertex, arc, etc by the theory of network flow, the coupling relations among various quality control points in the formation process of assembly qualities were reflected. According to the sequence of quality control points entering into assembly environment, their corresponding hierarchy in the model were determined, which presents the timing sexual of the formation process of assembly qualities. At last, a correlation-model between quality attributes and quality control points in the assembly process of complex product based on network flow was established, which would be the theoretical basis for the dynamic tolerance optimization of quality control points. An example of an automobile active gear axle assembly was given to demonstrate the feasibility of the correlation-model.",2012,10.4028/www.scientific.net/AMR.403-408.3015,no
Virtual Fruit a Process-Based Model of Fruit Quality: Concepts and Usefulness,"Fruit quality is a multi-criteria concept that is difficult to consider in modelling. It is clear that all the processes involved in the quality of fruits cannot be integrated in biological models. But some degree of complexity is needed to consider quality and the effect of environment. We are just beginning to investigate this complexity. The complexity comes from both regulations and interactions between various plant processes. This paper presents some of these regulations and interactions, and shows how the main processes can be integrated in a Virtual fruit model developed by adapting and connecting existing models describing fruit growth and respiration, sugar and acid accumulation, ethylene production and development of cuticular cracks. The Virtual fruit was used to analyze the impact of a single mutation and fruit load on fruit physiology and quality. Such a virtual approach could lead to new ways of exploring the impact of mutations, or naturally occurring genetic variations, in silico under different environmental conditions. Finally, we illustrate how the Virtual fruit could be possibly used to design fruit ideotypes for quality. Since quality is defined by several traits, we consider this design as a multi-objective optimization problem to be solved using an evolutionary algorithm.",2012,,no
Evaluating spatial and temporal patterns of MODIS GPP over the conterminous US against flux measurements and a process model,"Gross primary productivity (GPP) quantifies the photosynthetic uptake of carbon by ecosystems and is an important component of the terrestrial carbon cycle. Empirical light use efficiency (LUE) models and process-based Farquhar, von Caemmerer, and Berry (FvCB) photosynthetic models are widely used for GPP estimation. In this paper, the MODIS GPP algorithm using the LUE approach and the Boreal Ecosystem Productivity Simulator (BEPS) based on the FvCB model in which a sunlit and shaded leaf separai:ion scheme is evaluated against GPP values derived from eddy-covariance (EC) measurements in a variety of ecosystems. Although the total GPP values simulated using these two models agree within 89% when they are averaged for the conterminous U.S., there are systematic differences between them in terms of their spatial and temporal distribution patterns. The spatial distribution of MODIS GPP therefore differs substantially from that produced by BEPS. These differences may be due to an inherent problem of the LUE modeling approach. When a constant maximum LUE value is used for a biome type, this simplification cannot properly handle the contribution of shaded leaves to the total canopy-level GPP. When GPP is modeled by BEPS as the sum of sunlit and shaded leaf GPP, the problem is minimized, i.e., at the low end, the relative contribution of shaded leaves to GPP is small and at the high end, the relative contribution of shaded leaves is large. Compared with monthly and annual GPP derived from eddy covariance data at 40 tower sites in North America, BEPS performed better than the MODIS GPP algorithm. The difference between MODIS and BEPS GPP widens as with the fraction of shaded leaves increases. The simpler LUE modeling approach should therefore be further improved to reduce this bias issue for effective estimation of regional and temporal GPP distributions. (c) 2012 Elsevier Inc. All rights reserved.",2012,10.1016/j.rse.2012.06.023,no
Hybrid Validation of Handwriting Process Modelling,"Handwriting process is one of the most complex processes of our biological repertory. Modelling such process remains difficult to implement. Several approaches were proposed in the literature. However, the validation results of these models remain less or more satisfactory. This paper deals with unconventional and conventional handwriting process characterization approaches based on the use of soft computing techniques namely the Radial Basis Function (RBF) neural networks and the use of mathematical models based on the recursive least squares algorithm. Modelling handwriting system as well as the hybrid validation of the proposed models constitutes the main contribution of this paper. The obtained simulation results of the hybrid validation models show a satisfactory agreement between responses of the developed models and the experimental Electromyographic signals (EMG) data then the efficiency of the proposed approaches. Applying the study is very interesting to elaborate a helpful system to those who suffer from physical handicaps.",2012,,no
Evaluation of the performance and limitations of empirical partition-relations and process based multisurface models to predict trace element solubility in soils,"Here we evaluate the performance and limitations of two frequently used model-types to predict trace element solubility in soils: regression based ""partition-relations"" and thermodynamically based ""multisurface models"", for a large set of elements. For this purpose partition-relations were derived for As, Ba, Cd, Co, Cr, Cu, Mo, Ni, Pb, Sb, Se, V. Zn. The multi-surface model included aqueous speciation, mineral equilibria, sorption to organic matter, Fe/Al-(hydr)oxides and clay. Both approaches were evaluated by their application to independent data for a wide variety of conditions. We conclude that Freundlich-based partition-relations are robust predictors for most cations and can be used for independent soils, but within the environmental conditions of the data used for their derivation. The multisurface model is shown to be able to successfully predict solution concentrations over a wide range of conditions. Predicted trends for oxy-anions agree well for both approaches but with larger (random) deviations than for cations. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.envpol.2012.03.011,no
Expressiveness and Understandability Considerations of Hierarchy in Declarative Business Process Models,"Hierarchy has widely been recognized as a viable approach to deal with the complexity of conceptual models. For instance, in declarative business process models, hierarchy is realized by sub-processes. While technical implementations of declarative sub-processes exist, their application, semantics, and the resulting impact on understandability are less understood yet-this research gap is addressed in this work. In particular, we discuss the semantics and the application of hierarchy and show how sub-processes enhance the expressiveness of declarative modeling languages. Then, we turn to the impact on the understandability of hierarchy on a declarative process model. To systematically assess this impact, we present a cognitive-psychology based framework that allows to assess the possible impact of hierarchy on the understandability of the process model.",2012,,no
Preliminary Evaluation of an Augmented Reality Collaborative Process Modelling System,"Identifying, modelling and documenting business processes requires the collaboration of many stakeholders that may be spread across companies in inter-organizational business settings. While there are many process modelling tools available, the support they provide for remote collaboration remains limited. This paper investigates the application of virtual environment and augmented reality technologies to remote process modelling, with the aim to assisting collaboration tasks by providing an increased sense of immersion in a shared workspace. We report on the evaluation of a prototype system with five informants. The results indicate that this approach to business process modelling is suited to remote collaborative task settings, and stakeholders may indeed benefit from using augmented reality interfaces.",2012,10.1109/CW.2012.18,no
A new process-based cost estimation and pricing model considering the influences of indirect consumption relationships and quality factors,"In a manufacturing environment containing complex consumption relationships and quality influences, the application of traditional activity-based costing (ABC) method is limited. In this paper, a new improved process-based model for cost estimation and pricing is presented. Through utilizing the input-output analysis method, the complex indirect consumption relationships (such as reciprocal relationships) of a manufacturing system are expressed. By solving these relationships, the consumption characteristics of all production activities (mainly presented by the activity rates) are extracted. Then with the consumption characteristics, the quality characteristics and usage amounts of these activities, the cost prices of products are estimated for their pricing. A case study is given based on the compressor products of a manufacturing company, and its effectiveness is shown. As the cost influences of complex consumption relationships and quality factors are fully considered, the proposed approach has a higher estimation accuracy than the traditional ABC method. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.cie.2012.06.010,no
CALIBRATION OF A DECISION-MAKING PROCESS IN A SIMULATION MODEL BY A BICRITERIA OPTIMIZATION PROBLEM,"In a previous paper, we developed an accurate simulation model of an Intensive Care Unit to study bed occupancy level (BOL). By means of accurate statistical analysis we were able to fit models to arrivals and length-of-stay of patients. We model doctors' patient discharge decisions and define a set of rules to determine the conditions for earlier or delayed discharge of certain patients, according to BOL. For the calibration of the rule parameters, we proposed a nonlinear stochastic optimization problem aimed at matching the model outputs with the real system outputs. In this paper, we improve the calibration of the rule parameters by including the principle of ""minimum medical intervention"" as a second objective function. We replace the previous objective function with a satisficing matching, in order to gain more degrees of freedom in the search for better rules according to the new objective.",2012,,no
A Selection Approach for Optimized Problem-Solving Process by Grey Relational Utility Model and Multicriteria Decision Analysis,"In business enterprises, especially the manufacturing industry, various problem situations may occur during the production process. A situation denotes an evaluation point to determine the status of a production process. A problem may occur if there is a discrepancy between the actual situation and the desired one. Thus, a problem-solving process is often initiated to achieve the desired situation. In the process, how to determine an action need to be taken to resolve the situation becomes an important issue. Therefore, this work uses a selection approach for optimized problem-solving process to assist workers in taking a reasonable action. A grey relational utility model and a multicriteria decision analysis are used to determine the optimal selection order of candidate actions. The selection order is presented to the worker as an adaptive recommended solution. The worker chooses a reasonable problem-solving action based on the selection order. This work uses a high-tech company's knowledge base log as the analysis data. Experimental results demonstrate that the proposed selection approach is effective.",2012,10.1155/2012/293137,no
TOOL SUPPORT FOR PROCESS MODELING USING PROXIMITY SCORE MEASUREMENT,"In Business Process Management System (BPMS), process modeling is a troublesome task for a designer with little or insufficient experience. It is widely recognized in practice that only a proficient process designer is able to utilize process modeling tools effectively. Furthermore, although a process modeling tool can be effective in BPMS, a considerable amount of effort is required from enterprises in order to reconfigure business processes for convenient process modeling environments. This paper proposes a proximity score measurement approach to facilitate process modeling. Our approach has three salient features. First, it utilizes a proximity score to provide an analysis about the degree to which an activity is related with another activity in business processes. We argue that this analysis is critical in assisting process designers to initiate their process design with the best possible process reference model. Second, we developed a suite of methods for convenient process modeling, particularly suitable for novice designers, including the method of determining the proximity score measurement (PSM), and the methods of finding the respective process reference model and calculating homogeneity. We demonstrate that a process reference model is a convenient and effective way for a designer lacking experience to be guided to design his own process model. The homogeneity score can help the process designer to determine the suitable class to which a new model may belong. This further facilitates the versioning of the process model. Third but not the least, we develop a prototype of our system and conduct the experiments to evaluate the effectiveness of our approach. Our experimental results show that the proximity score measurement approach is efficient and effective for process designers to perform process modeling in BPMS environments.",2012,,no
Modeling of Quality Control Points of Equipment Maintenance Process,"In order to control the maintenance process effectively and improve the maintenance quality, model for quality control points of equipment maintenance process is formulated in this paper. By analyzing the process of equipment maintenance systematically, Control elements of the process are selected as the control targets to build the equipment maintenance process quality control point model. Therefore the maintenance quality indexes can be determined. Using Analytic hierarchy process, the influence of maintenance quality indexes on the general process of the maintenance quality control points are analyzed and calculated. Further, evaluating method is proposed in this paper for the important influence factors of the quality",2012,,no
Spatial Estimation of Wafer Measurement Parameters Using Gaussian Process Models,"In the course of semiconductor manufacturing, various e-test measurements (also known as inline or kerf measurements) are collected to monitor the health-of-line and to make wafer scrap decisions preceding final test. These measurements are typically sampled spatially across the surface of the wafer from between-die scribe line sites, and include a variety of measurements that characterize the wafer's position in the process distribution. However, these measurements are often only used for wafer-level characterization by process and test teams, as the sampling can be quite sparse across the surface of the wafer. In this work, we introduce a novel methodology for extrapolating sparsely sampled e-test measurements to every die location on a wafer using Gaussian process models. Moreover, we introduce radial variation modeling to address variation along the wafer center-to-edge radius. The proposed methodology permits process and test engineers to examine e-test measurement outcomes at the die level, and makes no assumptions about wafer-towafer similarity or stationarity of process statistics over time. Using high volume manufacturing (HVM) data from industry, we demonstrate highly accurate cross-wafer spatial predictions of e-test measurements on more than 8,000 wafers.",2012,,no
"Contributions to model validation: hierarchy, process, and cessation","In the domain of dynamic modeling and simulation, the assurance of model validity is a prominent challenge. An extensive number of contributions concerning model tests, terminology, and the epistemological foundations of validation have been elaborated. These contributions, however, do not fully answer the questions for novice modelers, namely, which validation tests to choose, when and how to apply them, and at what point to cease their formal validation efforts. Our intention here is to help close this gap by introducing a complexity hierarchy of validation tests, an integrative validation process, and a decision heuristic about when to stop formal validation efforts. The paper concludes by providing directions for future research. Copyright (c) 2012 System Dynamics Society",2012,10.1002/sdr.1466,no
Steam methane reforming reaction process intensification by using a millistructured reactor: Experimental setup and model validation for global kinetic reaction rate estimation,"In the frame of steam methane reforming process intensification, a highly active and stable catalyst based on rhodium with catalyst formulation and structure adapted to millistructured reactors has been formulated. This catalyst has been tested in industrial conditions (800, 850 or 900 degrees C and 20 bars) on a single channel which is representative of one channel of a more complex millistructured SMR reactor. Then, a detailed mathematical model for acquisition of the global reaction kinetics with this new catalyst has been developed and validated from experimental catalytic tests. The developed kinetics is dependent of the catalyst microstructure. This study presents the set-up, the model, the experimental catalytic runs and the global kinetics estimation protocol. It demonstrates, on one hand, that millistructured reactor is suitable for kinetic data acquisition and, on the other hand, the possibility of SMR process intensification, for improved energy efficiency and process size reduction. (C) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.cej.2012.07.117,no
PROBING THE INNOVATIVE QUALITY SYSTEM STRUCTURE MODEL FOR NPD PROCESS BASED ON COMBINING DANP WITH MCDM MODEL,"In the highly competitive environment, new product development, technology, equipment and raw materials have progressed rapidly. It is a gradual trend that competitors continuously innovate on their product and product life cycle becomes shorter. The manager intends to achieve the highest customer satisfaction, product value and product continuity. In this study, we develop an effective quality assessment system to manage quality of new product development (NPD) process. First, the quality factors related to NPD process are selected by expert questionnaires. Second, the DEMATEL approach is used to explore the relevance of the quality factors of NPD process. Then, the DEMATEL is combined with ANP method to a new DANP approach to calculate the influential weights of quality factors. Finally, VIKOR is used to evaluate and improve the total performance of NPD process by using empirical analysis on a case study, to find out the performance gaps and to improve its scores. The results of this study will provide NPD team a guidance to continuously improve, track and meet the quality assurance of NPD process; consequently, the customers' needs can be satisfied.",2012,,no
"A Modeling Language for 3D Process Plant Layout Representation, Exchange and Visualization","In the nuclear industry, achieving Long Term Data Preservation is a requirement for nuclear power plants to be safely built, operated over five or six decades and retired. Among them, CAD data suffers from some strong dependencies on the software vendors and its data model thus leading to a possible weakness in the preservation workflow. This paper presents a modeling language, suitable for the 3D representation of a process plant layout, based upon a procedural Constructive Solide Geometry (CSG) approach. The language execution, as well as the layout rendering and exchange, are experimented using a platform independent implementation, based on free software and open standards.",2012,,no
Proposal of Automation of the Collaborative Modeling and Evaluation of Business Processes Using a Semantic Wiki,"In the paper a new architecture for the design and evaluation of business process models is proposed. The architecture is based on the use of a semantic wiki system. It supports a distributed and collaborative modeling approach. In such a case, models can be created by a possibly distributed team of analysts. Moreover, they can be gradually improved using an integrated quality metrics system. We give the main architectural assumptions for this approach. Requirements for the system are described along with a prototype implementation. This system offers a practical tool support for the automation of the design and evaluation of models in a distributed environment.",2012,,no
Towards Evaluating an Ontology-Based Data Matching Strategy for Retrieval and Recommendation of Security Annotations for Business Process Models,"In the Trusted Architecture for Securely Shared Services (TAS(3)) EC FP7 project we have developed a method to provide semantic support to the process modeler during the design of secure business process models. Its supporting tool, called Knowledge Annotator (KA), is using ontology-based data matching algorithms and strategy in order to infer the recommendations the best fitted to the user design intent, from a dedicated knowledge base. The paper illustrates how the strategy is used to perform the similarity (matching) check in order to retrieve the best design recommendation. We select the security and privacy domain for trust policy specification for the concept illustration. Finally, the paper discusses the evaluation of the results using the Ontology-based Data Matching Framework evaluation benchmark.",2012,,no
Dynamic Modeling of the Reactive Twin-Screw Corotating Extrusion Process: Experimental Validation by Using Inlet Glass Fibers Injection Response and Application to Polymers Degassing,"In this Article is described an original dynamic model of a reactive corotating twin-screw extrusion (TSE) process operated by the Rhodia Co. for the Nylon-66 degassing finishing step. To validate the model, dynamic experiments have been performed on a small-scale pilot plant. These experiments consist of a temporary injection of glass fibers at the inlet of the extruder after it has reached a given operating point. The outlet glass fibers mass fraction time variation is then measured. This experiment does not lead to the RTD measurement. As a matter of fact, due to the high quantity of glass fibers that is introduced, the behavior of the flow through the extruder is perturbed so that the glass fibers cannot be considered as an inert tracer. The dynamic model that we have published elsewhere (Choulak et al. Ind. Eng. Chem, Res. 2004, 43, 7373-7382) is adapted to take into account this nonlinear behavior of the extruder with respect to the glass fibers injection and is favorably compared to experimental results. The description of the degassing operation is also included in the model. The model allows simulations of the complete dynamic behavior of the process. When the steady state is reached, the good position of the degassing vent with respect to the partially and fully filled zones positions can also be checked, thus illustrating the way the model can be used for design purposes.",2012,10.1021/ie300698k,no
SOFTWARE PROCESS MEASURING MODEL,"In this paper the Software Process Measuring Model (SPMM) is described. SPMMis a method for software process assessment, quantitative measurement and improvement for software producing organizations (SPOs). It has been developed partly based on a renovation of the CMM/CMMI, Bootstrap and SPICE methods, standards ESAPSS 05, and ISO 90003. SPMM focuses on the software development process in software production enterprises. The article explains the central concept of gaining data about software engineering organizations with a thoroughly constructed questionnaire. It gives a ground to measure the quality maturity level of organization and its projects. The SPMM can be interpreted as a method for describing where an organization stands and what changes are to be recommended in the next steps. The main idea of the SPMM is to determine the process maturity profile of an SPO. The goals of a SPMM self-assessment are: a) to measure and develop an SPO maturity quality profile showing strengths and weaknesses of the SPO assessed, b) to derive the steps for improvement from the shown quality profile. The result of one day assessment in software production organization X(SPO X), and Project X within the SPOX which was held at the beginning of October 2010 is presented. The result of the assessment showed the total organization and methodology maturity levels of the Project X. The organization is on maturity level 2, 83. The methodology is on maturity level of 2, 48. The total maturity level of the organization of SPO X is on maturity level of 2, 42, and the methodology is on maturity level of 2, 57. The organization of the paper is as follows: after the introduction in section one, section two explains the reasons of the SPMM development. Section three depicts the SPMM development. The maturity level algorithm is explicated in the next section. Section five explains the evaluation of the SPO, the assessment results are in section six. The conclusion is given in section seven, and the list of literature in section eight.",2012,,no
Soft measurement model and its application in raw meal calcination process,"In this paper, a soft measurement model has been proposed by combining recursive fixed-memory principal component analysis (RFMPCA) with least squares support vector machines (LS-SVM). To solve outliers, missing data points of the outliers and deviation from normal values are detected. The RFMPCA was applied to the model, which not only solved drawbacks of conventional PCA and data saturation, but also simplified the LS-SVM structure and improved the training speed. The proposed model has been successfully applied to the decomposition process of Jiuganghongda Cement Plant in China. Industrial application results have shown that the soft measurement model has high accuracy and guidance to calciner temperature setting. (C) 2011 Elsevier Ltd. All rights reserved.",2012,10.1016/j.jprocont.2011.08.005,no
UNIFIED COMPLEXITY MODEL FOR H.264/AVC VIDEO PROCESSING ON MOBILE PLATFORM,"In this paper, a unified computational complexity model is proposed to predict the H.264/AVC encoding and decoding computing cycles on popular ARM featured mobile platform. We have developed an analytical complexity model considering the video spatial resolution (i.e., frame size), temporal resolution (i.e., frame rate), and amplitude resolution (i.e., signal amplitude which is usually controlled by compression quantization parameter (QP)). Our proposed model has been validated for H.264/AVC encoding, where x264 is chosen as the typical mobile H.264 encoder. The same analytical model is also extended and verified for the H.264/AVC decoding using FFmpeg. Extensive simulations have been carried out to experiment different scenarios using various video sources at different frame sizes (e.g., HD to QCIF), frame rates (e.g., 60 to 3.75 fps) and bit rates (either using constant QP or rate control). Results demonstrate the high accuracy of our proposed model, with the average relative prediction error less than 7%.",2012,,no
"Integration of Quality, Labor Risks Prevention, Environment and Ethical Management. Model Applied to R&D&D&I and Manufacturing Processes in an Organization","In this paper, it is proposed an integrated management model applied to R&D&D&I and manufacturing processes for an organization. The model intends to integrate the environmental, risk prevention and ethical aspects as well as research, development and innovation projects management in the general quality management structure proposed by ISO 9001:2008. Specifically, it tries to fulfill the standards ISO 9001, ISO 14001, OSHAS 18001, SGE 21 and UNE 166002. The global structure is based on the continuous improvement Plan-Do-Check-Act cycle. The main advantages of this integrated management system are detailed, specifically the optimization of the documentation which results in the reduction of costs of implementation and maintenance.",2012,10.4028/www.scientific.net/KEM.502.85,no
Clustering of Laser Measurements via the Dirichlet Process Mixture Model for Object Tracking,"In this paper, the Dirichlet process mixture model is used to describe the distribution of the whole laser measurements in a given scan. Then the number of clusters is inferred from the measurements by the Gibbs sampler. We focus on the automotive application which usually has a more complex environment. Due to the variant shapes and sizes of the real traffic objects, the multi-class DP-based clustering model, which is incorporated with a mixture prior distribution, is proposed to cluster the measurements more properly. The clustering results of the proposed method are compared with those of several existing clustering methods both in an expressway case and in an urban road case. The corresponding tracking performances are also analyzed and the improvements of the proposed method are presented.",2012,,no
The single processor total weighted completion time scheduling problem with the sum-of-processing-time based learning model,"In this paper, we analyse the single processor total weighted completion time scheduling problem with the learning effect, where the processing time of each job is a non-increasing function dependent on the sum of the normal processing times of preceding jobs. We prove that the considered problem is at least NP-hard. Moreover, a pseudopolynomial time dynamic programming algorithm that optimally solves the problem with a step learning function (curve) is constructed. Furthermore, fast approximation algorithms for the general version of the problem, where job processing times are described by arbitrary functions dependent on the sum of the normal job processing times, are provided. Their efficiency is verified numerically and for Weighted Shortest Processing Times algorithm a worst case analysis is also performed. (C) 2012 Elsevier Inc. All rights reserved.",2012,10.1016/j.ins.2012.02.043,no
A Quality-Distinction Model of IT Capabilities: Conceptualization and Two-Stage Empirical Validation Using CMMi Processes,"In this paper, we develop a model of information technology (IT) service provider capabilities termed the quality distinction (QD) model that is theoretically rooted in the resource-based view and quality management literatures, and operationalized using the widely used capability maturity model (CMM) framework. The QD model is theorized to consist of one dynamic capability (i.e., process adaptation capability) and three operational capabilities (i.e., life cycle, prevention quality, and appraisal quality capabilities). These four capabilities are initially operationalized using definitions of the 22 processes in the CMM integration framework. A panel of experts is used to assign the 22 processes to the four capabilities in the QD model. Rigorous scale-refinement procedures are used and 15 CMM processes are retained as a result. Survey data collected from IT service providers are, then, used to compare the theorized QD model with the staged and continuous models in the CMM framework. Results from a covariance-based structural equation modeling analysis provide good support to the hypothesis that the QD model is superior to the two CMM models (the staged and continuous representations), with the theorized model showing high fit indices and high psychometric properties. Results also provide support for the hypothesis that processes should be operationalized as routines with a combination of both ostensive and performative aspects.",2012,10.1109/TEM.2011.2165287,no
Preliminary evaluation of the runoff processes in a remote montane cloud forest basin using Mixing Model Analysis and Mean Transit Time,"In this study, the Mean Transit Time and Mixing Model Analysis methods are combined to unravel the runoff generation process of the San Francisco River basin (73.5?km2) situated on the Amazonian side of the Cordillera Real in the southernmost Andes of Ecuador. The montane basin is covered with cloud forest, sub-paramo, pasture and ferns. Nested sampling was applied for the collection of streamwater samples and discharge measurements in the main tributaries and outlet of the basin, and for the collection of soil and rock water samples. Weekly to biweekly water grab samples were taken at all stations in the period April 2007November 2008. Hydrometric data, Mean Transit Time and Mixing Model Analysis allowed preliminary evaluation of the processes controlling the runoff in the San Francisco River basin. Results suggest that flow during dry conditions mainly consists of lateral flow through the C-horizon and cracks in the top weathered bedrock layer, and that all subcatchments have an important contribution of this deep water to runoff, no matter whether pristine or deforested. During normal to low precipitation intensities, when antecedent soil moisture conditions favour water infiltration, vertical flow paths to deeper soil horizons with subsequent lateral subsurface flow contribute most to streamflow. Under wet conditions in forested catchments, streamflow is controlled by near surface lateral flow through the organic horizon. Exceptionally, saturation excess overland flow occurs. By absence of the litter layer in pasture, streamflow under wet conditions originates from the A horizon, and overland flow. Copyright (c) 2011 John Wiley & Sons, Ltd.",2012,10.1002/hyp.8382,no
Applying Catastrophe Theory to an Information-Processing Model of Problem Solving in Science Education,"In this study, we test an information-processing model (IPM) of problem solving in science education, namely the working memory overload model, by applying catastrophe theory. Changes in students' achievement were modeled as discontinuities within a cusp catastrophe model, where working memory capacity was implemented as asymmetry and the degree of field dependence/independence and logical thinking as bifurcation parameters. Data from achievement scores of high school students in nonalgorithmic problem solving (chemical, organic-synthesis problems) were used and analyzed, using dynamic difference equations and statistical regression techniques. The dependent measure was the score difference in problems of varying demand from M = 3 to M = 8. The cusp catastrophe models proved superior (R2 = .73.84) to the pre-post linear counterpart (R2 =.52-.66). The empirical evidence for the catastrophe effect supports the nonlinear model for the working memory overload hypothesis. The results add to research endeavors that built bridges between concepts of the theory of nonlinear dynamical systems and problem solving in science education, and to IPM as well. Finally, the theoretical and practical implications are discussed. (c) 2012 Wiley Periodicals, Inc. Sci Ed 96:392-410, 2012",2012,10.1002/sce.21002,no
From constrained stochastic processes to the nonlinear sigma model. Two old problems revisited,"In this work a method is presented to derive the generating functional in path integral form for a system with an arbitrary number of degrees of freedom and constrained by general conditions. The method is applied to the case of the dynamics of an inextensible chain subjected to external forces. Next, the generating functional of the inextensible chain is computed assuming that the interactions are switched off. Finally, the generating functional of a two-dimensional nonlinear sigma model with O(3) symmetry is derived exploiting its similarities with the model describing the dynamics of the inextensible chain. (C) 2011 Elsevier B.V. All rights reserved.",2012,10.1016/j.nuclphysb.2011.09.006,no
Model validation for precipitation in solvent-displacement processes,"In this work a model for precipitation of polymer nanoparticles in solvent-displacement processes is presented and validated. The model is based on computational fluid dynamics coupled with a population balance model. The standard k-epsilon turbulence model in combination with the enhanced wall treatment approach is used to describe mixing and particle formation in a confined impinging jets reactor. The interaction between turbulent fluctuations and particle formation (i.e., micro-mixing) is modelled with the so-called direct quadrature method of moments coupled with the interaction and exchange with the mean approach, whereas the population balance model is solved by using the quadrature method of moments. The model is used here for the first time to model the precipitation of polymer nanoparticles of poly-e-caprolactone via solvent-displacement with acetone and water as solvent and anti-solvent. Particle formation is described with the classical nucleation, molecular growth and aggregation steps and a discussion on the effect of the polymer molecules behaviour in the system is presented and its effect on the results of the models is shown. The relevant rates are derived from first principles and most of the parameters appearing in the model are identified through independent measurements or from theory. Results show good agreement with experimental data and prove that the approach is very interesting, but further work is needed because, as shown, molecular characteristics of the polymer molecules cannot be neglected and need to be linked with the macroscopic description of the system obtained by computational fluid dynamics. Strategies to assess the value of some missing model parameters via multi-scale modelling are also discussed. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.ces.2012.08.043,no
Implementation and Validation of a Three-Dimensional Multiphase-CFD-Model for Blast Furnace Processes,In this work models capable to describe processes in the blast furnace are developed. The implementation of a CFD-model accounting for the flow of coke as well as fluids is presented. By applying separate sets of conservation equations the governing balance equations are solved for both phases. Heterogeneous reactions and heat transfer effects are modelled by implementing source terms based on correlations describing the underlying physics. This model setup has been successfully validated using a number of heat transfer problems as well as setups including heterogeneous and homogeneous chemical reactions.,2012,10.3303/CET1229155,no
LOW-COST ENZYME-BASED BIOSENSOR FOR LACTIC ACID AMPEROMETRIC DETECTION Electrical Modeling and Validation for Clinical and Food Processing Applications,"In this work we present the preliminary resulting measurements of an enzyme-based biosensor for the amperometric detection of lactic acid (LA). The sensor is based on low-cost gold electrodes on polymeric substrate. The redox catalytic enzyme used for analyte amperometric detection is lactate oxidase (LOx) from Pediococcus sp. This enzyme has been immobilized over electrodes surfaces by direct adsorption methodologies. Analysis of the enzyme-modified electrodes have been carried out by means of Electrochemical Impedance Spectroscopy (EIS) and with the development of an equivalent electrical model, in order to improve the adsorption process. Biosensors performance have been evaluated with Cyclic Voltammetry (CVM) measurements in different lactic acid solutions with concentrations from 1 mu M up to 300 mM. The lactate sensitivity of this disposable biosensor results in about 6.24 mu A mM(-1) cm(-2).",2012,10.5220/0003867603800383,no
"Defatting of annatto seeds using supercritical carbon dioxide as a pretreatment for the production of bixin: Experimental, modeling and economic evaluation of the process","In this work, supercritical CO2 extraction for defatting of annatto seeds was studied; the objective was to obtain an extract rich in tocotrienols and the defatted rich-bixin seeds. The process conditions were selected from global yield isotherms assays performed at 313 and 333 K, and 20,31, and 40 MPa; the ratio of solvent mass (S) to feed mass (F) was 35. For this S/F the highest extraction yield was 22 mg of extract/g dried seeds obtained at 333 K and 40 MPa. At these conditions a kinetic experiment was done to estimate the required parameters needed to estimate the cost of manufacturing (COM). COM was estimated using a commercial simulator; the model SFE plant had two extractor vessels in order to simulate continuous operation. A pilot plant size unit and two industrial size units were considered. The COM decreased from 124.58 to 109.27 US$/kg of extract as the extraction vessels capacities increased from 0.1 and 0.5 m(3); for the pilot plant with 2 vessels of 0.005 m(3) the COM was 300.00 US$/kg of extract. (c) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.supflu.2012.01.004,no
The use of virtual measuring devices in teaching modeling of physical processes,"Informatization of education makes to reconsider traditional training courses of computer science, methods, technologies and the means of informatization applied in teaching other subjects. In this article we consider the problem of the creation and use of virtual measuring devices in teaching modeling of physical processes. The difficult or costly experimental conditions, the new equipment, the risks connected with carrying out of tests, bulky procedure of mathematical calculations, frequently lack of material resources, considerably constrain process of physical researches. Emergence of virtual laboratory of stands and simulators became one of solutions of a problem. The resulting stage of modeling is computing experiment, so in teaching modeling it is necessary to use virtual laboratory work - connected animated images, which simulate the experimental installation. The special system of virtual switches, windows to set parameters of experiment and manipulation with the mouse allow students to change operatively conditions of experiment and to make calculations or to build charts. Pedagogical meaning of laboratory researches is that students do the analysis results of modeling and conclusions himself. Application of such laboratory complexes allows making experiments, to observe and manage them in real time using a personal computer, that provides improvement quality of teaching and learning of teaching materials by students. Virtual laboratory installations supplement real laboratory installations. This is explained by the modeling capabilities of real experiment, high reliability of their work and rather low cost. As at performance of laboratory works most part of time is spent on understanding how to work with the installation, by downloading a virtual installation, the student has the opportunity to prepare in advance, having mastered the laboratory equipment, examining its performance in different modes. Using technology of virtual devices, it is possible to fully reproduce the actual installation as a virtual model, keeping all its functionality. Virtual installations on the monitor screen have visual similarity to real installations. Convenient management of the program facilitates understanding of studied processes. Also it is possible to create virtual devices. For today the most powerful and reliable tool for the development of virtual devices is the environment of graphic programming LabView of company National Instruments. This article contains the steps in conducting of laboratory workshop, their sequence promotes increase of effectiveness of the work, increases relationship of steps and lowers their complexity. (C) 2012 Published by Elsevier Ltd. Selection and/or peer review under responsibility of Prof. Ayse Cakir Ilhan",2012,10.1016/j.sbspro.2012.08.243,no
Designing A Model For Quality of Employee-Organization Relationships (EORs) Based On Analysis Hierarchical Process (AHP),"Interpersonal relationships created a scale that consists of three components: personal relationship, community relationship, and professional relationship Research in interpersonal Relationships and the psychology of interpersonal relationships shows that the following four outcomes are good indicator s of successful interpersonal relationships. In this survey we show a model for Quality of employee-organization relationships (EORs) and evaluation interpersonal Relationships by this model. an EOR is dynamic and can be measured using perceptions of either or both parties regarding four ""indicators representing the quality relationships or relationship outcomes: satisfaction, trust, commitment, and control mutuality. by this model and use AHP model for analysis this paper we evaluation type of interpersonal relationships. finally we found that to each Specific dimension of quality can be follow a particular interpersonal relationships, but the best approach is professional relationship. [Ali Akbar Farhangi, Sara moazen, Maryam Aliei. Designing A Model For Quality of Employee-Organization Relationships (EORs) Based On Analysis Hierarchical Process (AHP). Life Science Journal 2012;9(1):231-241]. (ISSN: 1097-8135). http://www.lifesciencesite.com. 33",2012,,no
Pharmaceutical Engineering Strategy for Quality Informatics on the IDEF0 Business Process Model,"Introduction Intricate modern pharmaceutical business activities strive to achieve lean development for the desired quality level by applying the quality by design (QbD) approach. Methods To engineer suitable information flows for quality development by this approach, a business process model written in the type 0 method of integrated definition language (IDEF0) was created for biopharmaceuticals development activities by analyzing actual company activities. Results and Discussion The model comprises engineering activities of product quality design, recipe development, process engineering, and production. In the QbD approach, the activities are hierarchized into five stages. Information flows that trigger plan-do-check-action (PDCA) cycles beyond the stages (vertical PDCA) as well as those in the same stage (horizontal PDCA) are defined. Conclusion With the model as reference, it becomes possible to design an extensive information sharing system applying the QbD approach to the activities necessary for a series of functions.",2012,10.1007/s12247-012-9140-z,no
Evaluation of a novel laparoscopic camera for characterization of renal ischemia in a porcine model using digital light processing (DLP) hyperspectral imaging,"Introduction: Digital light processing hyperspectral imaging (DLP (R) HSI) was adapted for use during laparoscopic surgery by coupling a conventional laparoscopic light guide with a DLP-based Agile Light source (OL 490, Optronic Laboratories, Orlando, FL), incorporating a 0(0) laparoscope, and a customized digital CCD camera (DVC, Austin, TX). The system was used to characterize renal ischemia in a porcine model. Methods: Laparoscopic DLP (R) HSI was performed in 2 groups of uni-nephric adult pigs that were each subjected to 90 minutes of warm ischemia. Group 1 (n=5) received tadalafil to mitigate ischemia/reperfusion injury while Group 2 (n=4) received no treatment. HSI-derived percentage of oxygenated hemoglobin (%HbO(2)) was plotted against time before, during and after ischemia. Serum creatinine measured pre-ischemia (D0), and on days 1, 3 and 7 post-ischemia, was correlated with %HbO(2) using the Spearman's rank test. %HbO(2) was compared for the groups using the Student's t-test. Results: Mean baseline and nadir %HbO(2) for tadalafil vs. controls was 74% vs. 75% and 50% vs. 54% respectively; values did not significantly differ between the groups at any sampled time point. D0 creatinine was significantly lower for the tadalafil group, but creatinine at all other time points was similar between the groups. Baseline and nadir %HbO(2) significantly correlated with D7 and D0 creatinine respectively in the tadalafil group alone. Conclusion: Laparoscopic DLP (R) HSI effectively characterizes renal ischemia, appears to be sensitive to changes in local blood flow, and is predictive of renal functional injury. Further parameterization and technological improvement is required before clinical incorporation.",2012,10.1117/12.907395,no
Evaluating and improving a model of nursing care delivery: A process of partnership,"Introduction: Evaluating and improving a model of nursing care is a fundamental part of clinical practice improvement. While Australian nurses are showing increasing interest in improving models of care delivery, more research is needed that addresses and articulates the processes attendant upon evaluating, re-designing and implementing improvements to the provision of nursing care. Providing nurses with an open opportunity to plan, act, observe and reflect on their practice promotes successful partnerships between academics and clinicians. Aim: The aim of this study was to evaluate and improve the model of nursing care delivery to patients in a general surgical ward using participatory action research. Method: Researchers conducted non-participant observations (n=9) of two hours duration across the 24h period. Focus groups (n=3) were used to share non-participant observation data with staff, providing them with an opportunity to reflect on their practice and explore possible solutions. Data was collected in 2008-2009. Results: Two main problem areas were identified as impeding the nurses' ability to provide care to patients: (i) practices and behaviours of nurses and (ii) infrastructure and physical layout of the ward. An overview of issues within each problem area is presented. Conclusion: Shifting the focus of task-centred care towards a more patient-centred care approach, results directly in improvements in resource utilisation, improved cost-effectiveness and job satisfaction for nursing staff. New ways of thinking about nursing processes and systems, workflow design and skill allocation will guide hospital administrators and managers in the effective and efficient allocation of nursing work in similar settings. (C) 2012 Royal College of Nursing, Australia. Published by Elsevier Australia (a division of Reed International Books Australia Pty Ltd). All rights reserved.",2012,10.1016/j.colegn.2012.07.003,no
An explanatory model of quality of life in schizophrenia: the role of processing speed and negative symptoms,"Introduction. Improving the quality of life of patients with schizophrenia is a major goal in managing this devastating disorder, but agreement is lacking about the factors that predict quality of life (QoL) over the course of the disorder. Methods. We examined 165 hospitalized patients with schizophrenia in this study. We included measures for psychiatric (PANSS, insight and affective symptoms) and cognitive symptoms. Confirmatory factor analysis established a cognitive structure composed of the following six factors: attention, processing speed, verbal memory, fluency, working memory and executive functioning. Quality of life was assessed using the Heinrichs-Hanlon-Carpenter Scale. Results. Age, duration of illness, presence of more severe negative symptoms and most cognitive factors correlated significantly with QoL indicators. Regression analysis showed that processing speed (PS) was by far the most important cognitive factor that predicted QoL. Moreover, the interaction between PS and negative symptoms, patient age and executive functions modified the effect of PS on QoL. Finally, positive symptoms and other socio-demographic data were not related to QoL in the current study. Conclusions. Our findings suggest that PS and negative symptoms predict QoL in schizophrenia.",2012,,no
Natural goethite reduced with dithionite: Evaluation of the reduction process by XANES and Mossbauer spectroscopy and application of the catalyst in the oxidation of model organic compounds,"Iron oxides have been used as catalyst in different industrial applications, prominently in the petrochemical field. Natural goethites are interesting materials since they have a high surface area compared to their synthetic counterparts and also have a high relative abundance in the environment. Chemical treatment with sodium dithionite may cause surface changes of the goethite improving its catalytic activity. In this work, a natural goethite was used after dithionite treatment in the catalytic oxidation of methylene blue and quinoline. Infrared spectroscopy (FTIR) and temperature programmed reduction (TPR) were utilized for characterization. Quinoline oxidation by-products were monitored by electrospray mass spectroscopy (ESI-MS). After treatment, hydrogen peroxide decomposition was enhanced, indicating a radical mechanism. The same mechanism can be ascribed for the catalytic oxidation of quinoline since hydroxylation intermediates were observed. The same kind of mechanism can also be proposed for the methylene blue oxidation. (C) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.molcata.2012.01.005,no
Multi-dimensional population balance modeling and experimental validation of continuous powder mixing processes,"It has been recognized that the application of quality by design (QbD) to continuous processing in the pharmaceutical industry leads to better process control, improved product quality and mitigates scale-up issues (Schaber et al., 2011), whereby a component of QbD involves the development quantitative model-based representation of the process. In this work a population balance model (PBM) framework has been developed to model the dynamics of a continuous powder mixing process which is an important and complex unit operation used in a pharmaceutical tablet manufacturing process. Our previous studies have shown that PBM is effective in determining the various critical quality attributes (CQAs) (relative standard deviation (RSD), API composition and residence time distribution (RTD)) associated with mixing. It can also account for the key design and process parameters such as mixer RPM, processing angle, blender dimensions and number of radial and axial compartments. The developed PBM has been quantitatively validated by fitting experimentally obtained values of the above mentioned CQAs for different operating conditions. The model is dynamic and computationally tractable compared to traditional discrete element model (DEM) representations of mixing processes. This lends credence to the use of the model as an effective tool in control and optimization of blending process and can have future implementation in designing a Process Analytical Technology (PAT) system which will allow considerable improvements on the current manufacturing framework. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.ces.2012.06.024,no
"Model for Integrated Management of Quality, Labor Risks Prevention, Environment and Ethical Aspects, Applied to R&D&D&I and Production Processes in an Organization","It is proposed an integrated management model for an organization. This model is based on the continuous improvement Plan-Do-Check-Act cycle and it intends to integrate the environmental, risk prevention and ethical aspects as well as research, development and innovation projects management in the general quality management structure proposed by ISO 9001:2008. It aims to fulfill the standards ISO 9001, ISO 14001, OSHAS 18001, SGE 21 y 166002.",2012,10.1063/1.4707650,no
Physically based modeling in surface water-groundwater interaction: How numerical tools help understand hydrological processes,"It is widely accepted that groundwater and surface water are not isolated components of the catchment water balance and that their interaction is important to stream ecology, water quality and water quantity. A quantitative understanding of the interaction between surface water and groundwater is therefore crucial for sustainable management of water resources. Physically based models can help to conceptualize and understand these interactions. The paper presents two examples of how such models can help to improve our understanding and therefore our management strategies. Some final conclusions on the importance of physically based modeling in understanding surface water groundwater interactions are drawn.",2012,,no
Evaluation of the DSSAT CSM-CROPGRO-Tomato Simulation Model for Processing Tomato (Lycopersicon esculentum Mill.) Production in Northern Italy,"Italian processing tomato market has its dominance at global level but very few studies have been made on modelling this crop. The Cropping System Model CSM-CROPGRO-Tomato model of DSSAT (Decision Support System for Agrotechnology Transfer) software, was tested using datasets collected from field experiment in Legnaro, northern Italy. The experiment was carried out in 2009 using four different transplanting dates starting from 11th March (TD1, TD2, TD3, and TD4) with ten-day intervals, four processing tomato varieties, and two agronomic practices (mulched and non-mulched soil). Plants under mulched conditions in all transplanting dates gave better performance in terms of yield, growth and water use efficiency. Under mulched conditions, NPT 63 variety had significantly better yield and water use efficiency than the other three varieties. Mulching the soil was a useful tool to decrease water consumption levels at the transplanting dates studied. Evaluating the model using non-mulched experimental datasets showed that index of agreement (d-Stat) values between observed yield and model simulation for the first planting date with the four varieties ranged between 0.69 and 0.99. The model was able to simulate growth development better for all varieties under TD2, TD3, and TD4 conditions. In order to validate simulation ability of the model for the final yield, further work should be done regarding the genotype coefficients for each variety under study.",2012,,no
A Conceptual Model of Knowledge Work Productivity for Software Development Process: Quality Issues,"Knowledge is considered as the main competitive asset of the organization. Work on the knowledge work productivity has barely begun, but the most important contribution that management needs to construct in the 21st century is not only to increase the productivity of knowledge work and knowledge workers in the new century. The quality of knowledge work productivity are becomes pivotal in the context of software development today. Software development is a knowledge-intensive activity and its success depends heavily on the developers' knowledge and experience. A conceptual model will be proposed on a way describing organization to improve quality of knowledge work productivity. The methodology begins with a reviewing a theoretical foundation and expert review that provides the scientific basis for knowledge work productivity specifically for software development. A questionnaire will be constructing in order to investigate the relationship between factors of knowledge work and quality of productivity on knowledge work. The respondents are software developers from Small Manufacturing Enterprise (SME). The data will be analyzed using Structural Equation Modeling (SEM) to identify the significant direct relationship effect among the factors. The proposed model will be helpful for the software developers to understand the determinant factors for knowledge works productivity.",2012,,no
Inductor Modeling with Layout-Dependent Effects in 40nm CMOS Process,"Layout-dependent effects (LDE) as they are encountered in modern semiconductor technology processes are addressed and considered in this work. In particular, their effect on inductor modeling is discussed based on experimental results of devices fabricated and characterized in a 40 nm technology process. The proposed vector based modeling approach is accounting for these effects and its validity is demonstrated by comparison to experimental data. Improved correlation to measured inductor metrics such as inductance L and quality factor Q is demonstrated by considering the layout-dependent effects.",2012,,no
Evaluation models for service oriented process in spare parts management,"Maintenance, repair, operation (MRO) and spare parts supply is a critical issue for expensive repairable items such as cutting tool in the metal cutting industry. A new type of servicization model in the cutting tool supply chain reveals that most research has focused on local optimization of operational issues such as tool monitoring, spare parts management, logistics, and scheduling problems in isolation. The innovation in technologies, such as radio frequency identification and web services, for information sharing provides the ability to plan and schedule the tool requirement cross different layers in the MRO supply chain. However, their impact on the, technology complexity and performance is uncertain. In order to provide evaluation approach at the design phase, this paper analyzes the service oriented process from two dimensions that evaluate technology complexity, and system performance. Implementation of these evaluation models helps organizations analyze the business processes and further improve them through the analysis.",2012,10.1007/s10845-010-0486-0,no
PHILOSOPHICAL PERSPECTIVES FOR STRATEGIC INNOVATION MODELS AND COMPREHENSIVE ANALYSIS PROCESSES,"Many tools and methods claim to be ""innovative"". Most belong either to project management, engineering design or creativity approaches. ""Innovation Management"" literature usually discusses ""success patterns"" for Innovation based on case studies, but hardly process the comprehensive support of innovation activities. It seems that there is a strategic gap between traditional idea-realization processes that focus on reliable project management and the diffuse situation in ever faster changing environments with unclear opportunities and risks. To professionally reinforce strategic innovation activities it is necessary to define a resilient framework. This paper discusses a new view on the field of innovation that is based on the comprehensiveness of philosophy. Fundamental definitions of early philosophers on the interdependencies of the ""co-evolution of the world"" are applied to define an ""Innovation Philosophy"". This is transformed into an ""Innovation Strategy"" that comprises a repeatable ""Innovation Process"" for guiding teams through Innovation Projects.",2012,,no
A review of methods for evaluation of maturity models for process improvement,"Maturity models are widely used in process improvement. The users of a maturity model should be confident that the weak points of the assessed processes can be found, and that the most valuable changes are introduced. Therefore, the evaluation of maturity models is an important activity. In this paper, a mapping study of the literature on the evaluation of maturity models is presented. Two databases are searched resulting in a set of relevant papers. The identified papers can be classified according to six categories, namely the maturity model under evaluation, type of evaluation, relation of the evaluators/authors to the maturity model, level of objectivity, main purpose of the paper, and size of study. Further, a framework of different evaluations of maturity models is developed, and the relevant papers are mapped to the framework. Finally, the relevant research on the evaluation of the maturity models in the Capability Maturity Model family is discussed in more detail. The result of this mapping study is a clear overview of how the evaluation of maturity models has been done, and some discussions are provided for further research on the evaluation of commonly used or newly developed maturity models. Copyright (C) 2011 John Wiley & Sons, Ltd.",2012,10.1002/smr.560,no
Process-form linkages in meander morphodynamics: Bridging theoretical modeling and real world complexity,"Meandering rivers are one of the most dynamic earth-surface systems. They play an important role in terrestrial-sediment fluxes, landscape evolution, and the dynamics of riverine ecosystems. Meandering rivers have been of fundamental interest to researchers across a wide range of disciplines, from fluvial geomorphology to fluid mechanics, from river engineering to landscape ecology, owing to the intriguing complexity of meander morphodynamics. This interest also comes from the socio-economic concerns due to the river hazards caused by bank erosion, channel change, and flooding, as well as the adverse responses of meandering rivers to human- and climate-induced changes in the environmental conditions. An in-depth, process-based understanding of the dynamics of meandering river-floodplain systems is critical in order to investigate the responses of these systems to the changes in environmental conditions. Over the last few decades, there have been significant advances in river meandering research, with contributions from both theoretical modeling and experimental and field-based research. This paper presents a detailed overview of river meandering research, particularly focusing on the advances in the process-based understanding of meander morphodynamics. It also discusses the standing challenges in addressing the dynamics of real meandering rivers and their floodplain patterns and processes, and potential future directions in river meandering research. The paper advocates the crucial need for bridging theoretical modeling with field- and laboratory-based research in order to inform accurate assessments of river-hazard risks and facilitate ecologically sound river-management and restoration practices with the aim of supporting healthy ecosystems.",2012,10.1177/0309133312451989,no
APPLICATION OF DAMAGE MODEL WITH MATERIAL FLOW EVALUATION TO HIGH TEMPERATURE COMPRESSION PROCESSES,"Mechanisms of ductile failure in a compression process are studied, based on microstructural considerations, with analytical and experimental investigations performed on a cylindrical specimen. Geometrical softening is able to occur by void if the shear band is generated inside of materials. The original Gurson model is used to analyze tensile deformation, and a modified Gurson model is proposed, which incorporates damage accumulation under shear and compression. This modified Gurson model is also applied to the void closure of sintered material under compression loading. However, the compressed materials must be considered in both the void nucleation and closure, because the void is nucleated and closed at a specific angle. The results of individual analyses of nucleation and closure do not agree with observations, but the analysis considered here, of both the nucleation and closure of a void, corresponds well with experimental results.",2012,10.1142/S0217984911500126,no
AIR QUALITY MODELLING OF SO2 EMISSIONS ASSOCIATED TO METALLURGICAL PROCESSES,"Metallurgical plants are one of the major industrial pollutants emitting mainly gases (SO2, NOx and CO2) and particulate matters containing heavy metals. The objective of this study is to compare two situations, regarding the SO2 emissions before and after the installation of the desulphurization system at a metallurgical plant. Two different sets of simulations were performed, considering a three days period for which meteorological data was available. The simulation results were compared to national SO2 air quality limits. The results show a significant decrease of SO2 concentrations after the installation of the desulphurization system, situated well within legal limits. Another objective of this study is to compare qualitatively the concentration data obtained from modelling with data measured on-site by a point-monitor.",2012,,no
Malware Detection in Smartphones Using Static Detection and Evaluation Model Based on Analytic Hierarchy Process,"Mobile malware is rapidly increasing and its detection has become a critical issue. In this study, we summarize the common characteristics of this malicious software on Android platform. We design a detection engine consisting of six parts: decompile, grammar parsing, control flow and data flow analysis, safety analysis, and comprehensive evaluation. In the comprehensive evaluation, we obtain a weight vector of 29 evaluation indexes using the analytic hierarchy process. During this process, the detection engine exports a list of suspicious API. On the basis of this list, the evaluation part of the engine performs a comprehensive evaluation of the hazard assessment of software. sample. Finally, hazard classification is given for the software. The false positive rate of our approach for detecting malware samples is 4. 7% and normal samples is 7.6%. The experimental results show that the accuracy rate of our approach is almost similar to the method based on virus signatures. Compared with the method based on virus signatures, our approach performs well in detecting unknown malware. This approach is promising for the application of malware detection.",2012,,no
A Review for Model Plant Mismatch Measures in Process Monitoring,"Model is usually necessary for the design of a control loop. Due to simplification and unknown dynamics, model plant mismatch is inevitable in the control loop. In process monitoring, detection of mismatch and evaluation of its influences are demanded. In this paper several mismatch measures are presented based on different model descriptions. They are categorized into different groups from different perspectives and their potential in detection and diagnosis is evaluated. Two case studies on mixing process and distillation process demonstrate the efficacy of the framework of mismatch monitoring.",2012,,no
Modeling of Writing Process for Two-Dimensional Magnetic Recording and Performance Evaluation of Two-Dimensional Neural Network Equalizer,"Modeling of a simple writing process considering intergranular exchange fields and magnetostatic interaction fields between grains is studied for two-dimensional magnetic recording (TDMR). A new designing method of a two-dimensional neural network equalizer with a mis-equalization suppression function (2D-NNEMS) for TDMR is also proposed. The bit-error rate (BER) performance of a low-density parity-check coding and iterative decoding system with the designed 2D-NNEMS is obtained via computer simulation using a read/write channel model employing the proposed writing process under TDMR specifications of 4 Tb/in(2), and it is compared with those for one- and two-dimensional finite impulse response equalizers (FIREs). It is clarified that the BER performance for the designed 2D-NNEMS is far superior to those for the FIREs.",2012,10.1109/TMAG.2012.2194988,no
Process of quantitative evaluation of validity of rock cutting model,"Most of complex technical systems, including the rock cutting process, are very difficult to describe mathematically due to limited human recognition abilities depending on achieved state in natural sciences and technology. A confrontation between the conception (model) and the real system often arises in the investigation of rock cutting process. Identification represents determination of the system based on its input and output in specified system class in a manner to obtain the determined system equivalent to the explored system. In case of rock cutting, the qualities of the model derived from a conventional energy theory of rock cutting are compared to the qualities of non-standard models obtained by scanning of the acoustic signal as an accompanying effect of the surroundings in the rock cutting process by calculated characteristics of the acoustic signal. The paper focuses on optimization using the specific cutting energy and possibility of optimization using the accompanying acoustic signal, namely by one of its characteristics, i.e. volume of total signal M representing the result of the system identification.",2012,,no
Construct factor evaluation model of Health Management Center selected by customers with Fuzzy Analytic Hierarchy Process,"National Health Insurance system has been continuously revised due to rapid changes of society since its establishment, which caused gradual decrease of hospital income year by year, so all hospitals take the initiative to develop self-financed items to partially increase hospital incomes, thus it is crucial to develop assessment model of Health Management Center. The research adopts Fuzzy Analytic Hierarchy Process (FAHP) for customers to make weight assessment on evaluation indexes of Health Management Center. Five major perspectives for customers' selection of Health Management Center are summarized, orderly including (1) Health Management Department (2) Personnel Service Department (3) Health Examination Service Department (4) Marketing Department (5) Environment Department. In addition, in the aspect of importance, ""regularly track recheck and provide timely medical service"", ""provide doctor's commentary and inspection report result and follow-up ambulatory care issues"" and ""reasonably charge"" are consider to be three major indexes in all weights. Research results can be submitted to relevant health examination institutes and personnel of the hospital for reference to earn the opportunity of developing new customers and improve service quality. (C) 2011 Elsevier Ltd. All rights reserved.",2012,10.1016/j.eswa.2011.07.094,no
Flow Sheet Model Evaluation of Nuclear Hydrogen Steelmaking Processes with VHTR-IS (Very High Temperature Reactor and Iodine-sulfur Process),"Nuclear hydrogen steelmaking (NHS) and nuclear hydrogen partial reduction steelmaking (NHPRS) systems were proposed using very high temperature reactor, and thermochemical hydrogen production iodine-sulfur process. Heat input and CO2 emissions of these systems were analyzed by heat and mass balance calculation. Total net heat input to the NHS system was 28.4 GJ/t-high quality steel (HQS), including material production, material transportation, and power generation. This value was much larger than that of a blast furnace steelmaking (BFS) system of 17.6 GJ/t-HQS. Reduction of hydrogen consumption in the shaft furnace and electricity consumption in the electric arc furnace were desired for lowering the heat input. Total net heat input of a NHPRS system was 31.9 GJ/t-HQS. Optimization of operation parameters such as the reduction ratio of partial reduced ore (PRO) and ratio of the PRO input to the blast furnace is desired to decrease the heat input. CO2 emissions of the NHS system and the NHPRS system were 9% and 50% of that from the BFS system. Substitution of coal by hydrogen and reduction of transportation weight contributed to the reduction. Steelmaking cost was also evaluated. When steelmaking scale of each system was unified to one million t-HQS/y, NHS was economically competitive to BFS and Midrex steelmaking. And NHS was advantageous at higher cost of resources.",2012,10.2355/isijinternational.52.1409,no
Similarity metrics for surgical process models,"Objective: The objective of this work is to introduce a set of similarity metrics for comparing surgical process models (SPMs). SPMs are progression models of surgical interventions that support quantitative analyses of surgical activities, supporting systems engineering or process optimization. Methods and materials: Five different similarity metrics are presented and proven. These metrics deal with several dimensions of process compliance in surgery, including granularity, content, time, order, and frequency of surgical activities. The metrics were experimentally validated using 20 clinical data sets each for cataract interventions, craniotomy interventions, and supratentorial tumor resections. The clinical data sets were controllably modified in simulations, which were iterated ten times, resulting in a total of 600 simulated data sets. The simulated data sets were subsequently compared to the original data sets to empirically assess the predictive validity of the metrics. Results: We show that the results of the metrics for the surgical process models correlate significantly (p < 0.001) with the induced modifications and that all metrics meet predictive validity. The clinical use of the metrics was exemplarily, as demonstrated by assessment of the learning curves of observers during surgical process model acquisition. Conclusion: Measuring similarity between surgical processes is a complex task. However, metrics for computing the similarity between surgical process models are needed in many uses in the field of medical engineering. These metrics are essential whenever two SPMs need to be compared, such as during the evaluation of technical systems, the education of observers, or the determination of surgical strategies. These metrics are key figures that provide a solid base for medical decisions, such as during validation of sensor systems for use in operating rooms in the future. (C) 2011 Elsevier B.V. All rights reserved.",2012,10.1016/j.artmed.2011.10.001,no
Surgical Workflow Management Schemata for Cataract Procedures Process Model-based Design and Validation of Workflow Schemata,"Objective: Workflow guidance of surgical activities is a challenging task. Because of variations in patient properties and applied surgical techniques, surgical processes have a high variability. The objective of this study was the design and implementation of a surgical workflow management system (SWFMS) that can provide a robust guidance for surgical activities. We investigated how many surgical process models are needed to develop a SWFMS that can guide cataract surgeries robustly. Methods: We used 100 cases of cataract surgeries and acquired patient-individual surgical process models (iSPMs) from them. Of these, randomized subsets iSPMs were selected as learning sets to create a generic surgical process model (gSPM). These gSPMs were mapped onto workflow nets as workflow schemata to define the behavior of the SWFMS. Finally, 10 iSPMs from the disjoint set were simulated to validate the workflow schema for the surgical processes. The measurement was the successful guidance of an iSPM. Results: We demonstrated that a SWFMS with a workflow schema that was generated from a subset of 10 iSPMs is sufficient to guide approximately 65% of all surgical processes in the total set, and that a subset of 50 iSPMs is sufficient to guide approx. 80% of all processes. Conclusion: We designed a SWFMS that is able to guide surgical activities on a detailed level. The study demonstrated that the high inter-patient variability of surgical processes can be considered by our approach.",2012,10.3414/ME11-01-0093,no
A Checking Consistency Framework Based on Multi-View Models Towards Business Process Model Repository,"On the purpose to adapt to ever complicated business models, an efficient consistency checking approach is needed for Business Process Model Repository to manage Multi-View models. We present a consistency checking framework to resolve the problems for service-based application. Firstly, based on business model analysis, a Multi-View business meta-model is built to act as a referred framework to find out relationships between different business elements. Then the meta-model is served as the basic criterion to check consistency of different business models. Next the integrity and consistency algorithms are proposed to cope with the Multi-View models. By means of our algorithms, inaccurate and lost relationship will be found and rebuilt at the same time. Lastly, a transportation process of one Logistics Company is given as a case study to verify the practical usability of the checking consistency framework. The result shows the proposed framework is a feasible method to cope with Multi-View models in Business Process Model Repository.",2012,10.1109/ICEBE.2012.64,no
Evaluation of the Risk OMT model for maintenance work on major offshore process equipment,"Operational safety is receiving more and more attention in the Norwegian offshore industry. Almost two thirds of all leaks on offshore installations in the period 2001-2005, according to the Risk Level Project by the Petroleum Safety Authority in Norway, resulted from manual operations and interventions, as well as shut-down and start-up. The intention with the Risk OMT (risk modelling - integration of organisational, human and technical factors) program has been to develop more representative models for calculation of leak frequencies as a function of the volume of manual operations and interventions. In the Risk OMT project a generic risk model has been developed and is adapted to use for specific failure scenarios. The model considers the operational barriers in event trees and fault trees, as well as risk influencing factors that determine the basic event probabilities in the fault trees. The full model, which applies Bayesian belief networks, is presented more thoroughly in a separate paper. This paper presents the evaluation of the model. The model has been evaluated through some case studies, and one important aspect is the evaluation of the importance of each risk influencing factor. In addition some risk-reducing measures have been proposed, and the paper presents how the effect of these measures has been evaluated by using the model. Finally, possible applications and recommendations for further work are discussed. (c) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.jlp.2012.01.001,no
Validation of a multi-segment spinal model for kinematic analysis and a comparison of different data processing techniques,"Optoelectronic motion capture technology is a useful tool in the quantitative dynamic assessment of the spine. In a clinical setting this may help gain a further understanding of underlining musculoskeletal pathology. It is therefore important that accurate measurements are made to allow data to be comparable across various investigations. This report outlines a new multi-segment spinal model and its validation. A mechanical model consisting of an upper thoracic (UT), lower thoracic and lumbar segment was developed allowing for range of motion assessment. An electrogoniometer and torsiometer were attached to the model to provide a control measurement. The UT segment was chosen for analysis and static trials were collected at angles ranging from 2-45 degrees. Kinematic data was captured using an optoelectronic motion capture system. Software computed angles corresponded well with the control measure. While highlighting the differences in the estimation of angles between software platforms, this study emphasizes the need for the clear description and understanding of the kinematic model used.",2012,10.3233/978-1-61499-067-3-151,no
FLEXIBILITY AND MODELING IN BUSINESS PROCESSES: A MULTI-DIMENSIONAL RELATIONSHIP,"Over the last few years, Business Process Management (BPM) has achieved increasing popularity and dissemination. An analysis of the underlying assumptions of BPM shows that it pursues two apparently contradicting goals: on the one hand it aims at formalising work practices into business process models; on the other hand, it intends to confer flexibility to the organization - i.e. to maintain its ability to respond to new and unforeseen situations. This paper analyses the relationship between formalisation and flexibility in business process modelling by means of an empirical case study of a BPM project in an aircraft maintenance company. A qualitative approach is adopted based on the Actor-Network Theory. The paper offers two major contributions: (a) it illustrates the sociotechnical complexity involved in BPM initiatives; (b) it points towards a multidimensional understanding of the relation between formalization and flexibility in BPM projects.",2012,,no
Numerical methods for optimal control problems appearing in model validation of dynamic processes with application to dynamic robot calibration,"Parameter estimation and optimal design of experiments are important steps in establishing models that reproduce a given process quantitatively correctly. The methods for parameter estimation and optimum experimental design are being more and more often applied in industry. The realization of methods in industrial practice shows however, that in order to use the complete potential of nonlinear optimum experimental design, we have to deal with several mathematical challenges. This article presents very effective algorithms for design of optimal experiments in dynamic systems and preliminary numerical results for dynamic robot calibration.",2012,10.1080/02533839.2012.624782,no
MEASUREMENT OF RESIDUAL STRESS FIELDS FOR VALIDATION OF MATERIAL PROCESS MODELS,"Physical process models for structural materials support the emerging discipline of integrated computational materials engineering (ICME), and a key output from such models is an estimate of the residual stress field in a work piece following production processes (e. g., forging, rolling, machining, welding) or post-production processes (e. g., rework, repair). Validation of process models requires a comparison of outputs to truth data that are typically derived from measurement. While this can be straightforward for certain outputs (e. g., distributions of grain size, hardness, tensile strength), validation of residual stress fields is more complicated. Several examples serve to illustrate these complicating factors and offer insights on measurement approaches for validation of ICME material process models.",2012,,no
Modeling and Analysis of Process Parameters for Evaluating Shrinkage Problems During Plastic Injection Molding of a DVD-ROM Cover,"Plastic injection molding plays a key role in the production of high-quality plastic parts. Shrinkage is one of the most significant problems of a plastic part in terms of quality in the plastic injection molding. This article focuses on the study of the modeling and analysis of the effects of process parameters on the shrinkage by evaluating the quality of the plastic part of a DVD-ROM cover made with Acrylonitrile Butadiene Styrene (ABS) polymer material. An effective regression model was developed to determine the mathematical relationship between the process parameters (mold temperature, melt temperature, injection pressure, injection time, and cooling time) and the volumetric shrinkage by utilizing the analysis data. Finite element (FE) analyses designed by Taguchi (L(27)) orthogonal arrays were run in the Moldflow simulation program. Analysis of variance (ANOVA) was then performed to check the adequacy of the regression model and to determine the effect of the process parameters on the shrinkage. Experiments were conducted to control the accuracy of the regression model with the FE analyses obtained from Moldflow. The results show that the regression model agrees very well with the FE analyses and the experiments. From this, it can be concluded that this study succeeded in modeling the shrinkage problem in our application.",2012,10.1007/s11665-011-9895-2,no
Evaluation of Space Charge Accumulation Processes in Small Size Polymeric Cable Models,"Polymeric insulation materials have not been used in HVDC cable systems until recently because of the tendency of polymers to deplete accumulated charges very slowly. Research on space charge injection, conduction and trapping mechanisms can reveal information about which new materials are the best candidates for HVDC cable insulation. In this paper, the space charge accumulation processes are evaluated in mini cable models consisting of several different XLPE based insulation materials and PE based semi-conductive layers. The main difference between the materials can be found in the type and concentration of additives. The influence of the semiconductor-insulation interface on the space charge accumulation threshold is investigated with the help of polarization characteristics obtained with space charge measurements conducted with the Pulsed Electro-Acoustic (PEA) method for cable geometry objects.",2012,,no
Population Balance Modelling and Experimental Validation for Synthesis of TiO2 Nanoparticles using Continuous Hydrothermal Process,"Population balance (PB) modelling is investigated as a tool to study hydrothermal synthesis (CHS) technique for nanomaterial formulation. In particular, the effects of solution concentration and reactor residence time on the particle size distribution were examined. For the purpose of model validation, the simulated results were compared with data obtained from experiments conducted using a continuous stirred tank reactor for production of nano-size TiO2 particles. Product composition was analysed using Fourier transform infrared spectroscopy, and particle size was characterised scanning electron microscopy, zetasizer and image analysis. Good agreement between experimental and simulation results was achieved.",2012,10.4028/www.scientific.net/AMR.508.175,no
Feasibility of geoelectrical monitoring and multiphase modeling for process understanding of gaseous CO2 injection into a shallow aquifer,"Potential pathways in the subsurface may allow upwardly migrating gaseous CO2 from deep geological storage formations to be released into near surface aquifers. Consequently, the availability of adequate methods for monitoring potential CO2 releases in both deep geological formations and the shallow subsurface is a prerequisite for the deployment of Carbon Capture and Storage technology. Geoelectrical surveys are carried out for monitoring a small-scale and temporally limited CO2 injection experiment in a pristine shallow aquifer system. Additionally, the feasibility of multiphase modeling was tested in order to describe both complex non-linear multiphase flow processes and the electrical behavior of partially saturated heterogeneous porous media. The suitability of geoelectrical methods for monitoring injected CO2 and geochemically altered groundwater was proven. At the test site, geoelectrical measurements reveal significant variations in electrical conductivity in the order of 15-30 %. However, site-specific conditions (e.g., geological settings, groundwater composition) significantly influence variations in subsurface electrical conductivity and consequently, the feasibility of geoelectrical monitoring. The monitoring results provided initial information concerning gaseous CO2 migration and accumulation processes. Geoelectrical monitoring, in combination with multiphase modeling, was identified as a useful tool for understanding gas phase migration and mass transfer processes that occur due to CO2 intrusions in shallow aquifer systems.",2012,10.1007/s12665-012-1669-0,no
Statistical process control for validating a classification tree model for predicting mortality - A novel approach towards temporal validation,"Prediction models are postulated as useful tools to support tasks such as clinical decision making and benchmarking. In particular, classification tree models have enjoyed much interest in the Biomedical Informatics literature. However, their prospective predictive performance over the course of time has not been investigated. In this paper we suggest and apply statistical process control methods to monitor over more than 5 years the prospective predictive performance of TM80+, one of the few classification-tree models published in the clinical literature. TM80+ is a model for predicting mortality among very elderly patients in the intensive care based on a multi-center dataset. We also inspect the predictive performance at the tree's leaves. This study provides important insights into patterns of (in)stability of the tree's performance and its ""shelf life"". The study underlies the importance of continuous validation of prognostic models over time using statistical tools and the timely recalibration of tree models. (C) 2011 Elsevier Inc. All rights reserved.",2012,10.1016/j.jbi.2011.08.015,no
Perceived consistency between process models,"Process-aware information systems typically involve various kinds of process stakeholders. That, in turn, leads to multiple process models that capture a common process from different perspectives and at different levels of abstraction. In order to guarantee a certain degree of uniformity, the consistency of such related process models is evaluated using formal criteria. However, it is unclear how modelling experts assess the consistency between process models, and which kind of notion they perceive to be appropriate. In this paper, we focus on control flow aspects and investigate the adequacy of consistency notions. In particular, we report findings from an online experiment, which allows us to compare in how far trace equivalence and two notions based on behavioural profiles approximate expert perceptions on consistency. Analysing 69 expert statements from process analysts, we conclude that trace equivalence is not suited to be applied as a consistency notion, whereas the notions based on behavioural profiles approximate the perceived consistency of our subjects significantly. Therefore, our contribution is an empirically founded answer to the correlation of behaviour consistency notions and the consistency perception by experts in the field of business process modelling. (C) 2010 Elsevier Ltd. All rights reserved.",2012,10.1016/j.is.2010.12.004,no
Quality of Business Process Models,"Processes modeling is done for a number of reasons in relation to enterprise modeling, business process modeling and information systems development, and is a widely used technique. In particular after the introduction of BPR and workflow in the nineties, much work has looked on quality of business process models. In this paper we present a specialization of a general framework for assessing quality of models to support the evaluation the quality of business process models. The specialization takes earlier work on quality of models, process quality and quality of business process models into account. Comparing the approaches we find on the one hand that the properties of business process model quality is subsumed by the generic framework on a high level, and that there are aspects in this framework that are not covered by the existing work on business process model quality. On the other hand, the comparison has resulted in an extension of the generic framework for these kinds of models, and in this way improved the practical applicability of the framework when applied to discussing the quality of business process models.",2012,,no
"Understanding project interdependencies: The role of visual representation, culture and process","Project portfolio management is central to many organizations' strategic processes and requires consideration of multiple factors and the ability to envision alternative future consequences to support strategic project portfolio decision making. Complex project portfolios with multiple project interdependencies are characteristic of many project environments, yet existing methods do not provide the clear understanding of project interdependencies that is required. This exploratory study aims to improve organizational understanding of project interdependencies through two loosely coupled avenues of investigation conducted in tandem in a telecommunications and a defense organization. The first avenue of research introduces a new type of visual representation and shows that the creation of graphical network displays of projects and their interdependencies can provide benefits by supporting communication and strategic portfolio decision making. The second avenue of research tests a conceptual model and highlights the importance of both the culture and processes in an organization's understanding of project interdependencies. (C) 2012 Elsevier Ltd. APM and IPMA. All rights reserved.",2012,10.1016/j.ijproman.2012.01.018,no
Modelling of shrinkage cavity defects during the wheel and belt casting process,"Properzi continuous casting is a wheel and belt casting process used for producing aluminium wire rod which is essential to the making of electrical cables and over head lines. One of the main concerns of Properzi process users is to ensure good quality of the final product and to avoid cast defects especially the presence of shrinkage cavity. Numerical models developed with the Alsim software, which allows an automatic calculation of gap dependent heat transfer coefficients at the metal-mould interface due to thermal deformation, are used in order to get a better understanding on the shrinkage cavity formation. Models show the effect of process parameters on the cavity defect development and provide initial guidance for users in order to avoid this kind of casting defect.",2012,10.1088/1757-899X/33/1/012056,no
Validation and research of the Three-Process Model of Alertness regulation predictions,"Purpose: Collecting and analyzing experimental data with the method of subjective scale. Established trend curve of alertness versus time and the Three-Process Model of Alertness regulation with specific population. Method: tracking the life of 12 subjects 3 days, getting the alertness data and the trend of alertness level, then comparing and verifying it with the trend of the Three-Process Model of Alertness regulation predictions. Conclusion: After comparing the model predictions data with experimental data, it was found that the model prediction results and experimental results have a strong correlation proved that and the model is valid. The trend of the model alertness level predictions and experiment alertness is similar. But there are some difference between alertness level peak and the speed of development. The Three - Process Model of Alertness regulation could be improved in adaptive to different specific population.",2012,,no
Evaluating a Dynamic Process Model of Wellbeing for Parents of Children With Disabilities: A Multi-Method Analysis,"Purpose: The purpose of this study was to evaluate possible determinants of parent wellbeing using a contextual model of parent adjustment. Method: One hundred forty parents of children with various disabilities (i.e., autism, intellectual disabilities, and other health impairments) participated in this investigation. Parents completed a survey consisting of basic demographic characteristics of the parent, child-disability characteristics, parent problem solving ability, access to information and resources, environmental/social supports, appraisals of threat and growth, and measures of life satisfaction and physical/mental health. Structural equation modeling was conducted to test a hypothesized contextual model of parent wellbeing. Results: Results indicated strong fit to the a priori model. After controlling for the contribution of parent demographic variables, the largest contributors to the prediction of parent wellbeing were parent problem solving ability, access to resources, environmental/social supports, and parent appraisals of threat. Child functional impairment was not significantly associated with parent wellbeing. Conclusions: Access to resources and environmental/social supports have a greater direct effect on parent wellbeing than parent and child demographic variables and disability severity. Threat appraisals have direct and mediating effects on parent wellbeing. Implications related to the importance of resources and environmental/social supports, appraisals of threat and growth, and problem solving abilities on the wellbeing of parents of children with disabilities are discussed.",2012,10.1037/a0027155,no
Bridging the gap between implicit and explicit understanding: How language development promotes the processing and representation of false belief,"Recent advancements in the field of infant false-belief reasoning have brought into question whether performance on implicit and explicit measures of false belief is driven by the same level of representational understanding. The success of infants on implicit measures has also raised doubt over the role that language development plays in the development of false-belief reasoning. In the current paper, we argue that children's performance on disparate measures cannot be used to infer similarities in understanding across different age groups. Instead, we argue that development must continue to occur between the periods when children can reason implicitly and then explicitly about false belief. We then propose mechanisms by which language associated with false-belief tasks facilitates this transition by assisting with both the processes of elicited response selection and the formation of metarepresentational understanding.",2012,10.1111/j.2044-835X.2011.02051.x,no
Bayesian approaches to the model selection problem in the analysis of latent stage-sequential process,"Recently, a great deal of attention has been paid to the stage-sequential process for the longitudinal data. A number of methods for analyzing stage-sequential processes have been derived from the family of finite mixture modeling. However, the research on the sequential process is rendered difficult by the fact that the number of latent components is not known a priori. To address this problem, we adopt the reversible jump MCMC (RJMCMC) and the Bayesian nonparametric approach, which provide a set of principles for the systematic model selection for the stage-sequential process. Using a latent class profile analysis, we evaluate the performance of RJMCMC and the Bayesian nonparametric method on the model selection problem. (C) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.csda.2012.03.015,no
On-line and Memory-based: Revisiting the Relationship Between Candidate Evaluation Processing Models,"Reexamining the relationship between the on-line and memory-based information processing models, this study presents a theoretical basis for the co-occurrence of on-line and memory-based processes and proposes a hybrid model. The study empirically tests the hybrid model by employing real-time tracking of participants' reactions to two candidates in a US presidential primary election debate. The findings confirm an independent, but complementary relationship between on-line and memory-based information processing in an individual's candidate evaluation and vote choice. The co-occurrence of the two modes applies to an individual's comparison of candidates as well. The implications of the hybrid model for the functioning of democracy are discussed.",2012,10.1007/s11109-011-9158-9,no
Online quality prediction of nonlinear and non-Gaussian chemical processes with shifting dynamics using finite mixture model based Gaussian process regression approach,"Reliable quality prediction of chemical processes often encounters different challenges including process nonlinearity and non-Gaussianity, shifting operating modes and dynamics, and system uncertainty. In this paper, a novel soft sensor prediction method is proposed by integrating finite mixture model (FMM) and nonlinear kernel Gaussian process regression (GPR). The finite mixture model is first estimated to identify the different operating modes of the process that correspond to the switching dynamics. Then the multiple localized Gaussian process regression models in the nonlinear kernel space are built to characterize the different dynamic relationships between process and quality variables within various operating modes. Further, the posterior probabilities of each new test sample with respect to different modes can be estimated through Bayesian inference strategy and used to incorporate multiple localized GPR models into a global model for quality variable prediction. The proposed FMM-GPR approach is applied to the Tennessee Eastman Chemical process with multiple operating modes and its performance is compared to that of the multi-model LSSVM method using two test cases. The soft sensor prediction results show that the FMM-GPR approach is superior to the LSSVM method in terms of much higher prediction accuracy and reliability. (c) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.ces.2012.07.018,no
Simple models for stomatal conductance derived from a process model: cross-validation against sap flux data,"Representation of stomatal physiology in models of plant-atmosphere gas exchange is minimal, and direct application of process-based models is limited by difficulty of parameter estimation. We derived simple models of stomatal conductance from a recent process-based model, and cross-validated them against measurements of sap flux (176365 d in length) in 36 individual trees of two age classes for two Eucalyptus species across seven sites in the mountains of southeastern Australia. The derived models which are driven by irradiance and evaporative demand and have two to four parameters that represent sums and products of biophysical parameters in the process model reproduced a median 8389% of observed variance in half-hourly and diurnally averaged sap flux, and performed similarly whether fitted using a random sample of all data or using 1 month of data from spring or autumn. Our simple models are an advance in predicting plant water use because their parameters are transparently related to reduced processes and properties, enabling easy accommodation of improved knowledge about how those parameters respond to environmental change and differ among species.",2012,10.1111/j.1365-3040.2012.02515.x,no
Evaluation of space-time point process models using super-thinning,"Rescaling, thinning, and superposition are useful methods for the residual analysis of spatial-temporal point processes. These techniques involve transforming the original point process into a new process that is a homogeneous Poisson process if and only if the fitted model is correct, so that one may inspect the residual process using standard tests for homogeneity as a means of assessing the goodness-of-fit of the model. Unfortunately, when the modeled conditional intensity of the original process is volatile, tests of homogeneity performed on residuals on the basis of these three residual methods tend to have low power. For such circumstances, we propose the method of super-thinning, which combines thinned residuals and superposition. This technique involves the use of a tuning parameter, k, which controls how much thinning and superposition are performed to homogenize the process. The method is applied to the assessment of a parametric spacetime point process model for the origin times and epicentral locations of recent major California earthquakes. Copyright (c) 2012 John Wiley & Sons, Ltd.",2012,10.1002/env.2168,no
"Research Perspectives on Unstable High-alpine Bedrock Permafrost: Measurement, Modelling and Process Understanding","Rock instability is believed to be causally linked to permafrost degradation, but it is difficult to demonstrate this directly because of the short record of slope failures in high mountains. While abductive scientific reasoning of increasing permafrost-related instability based on the short time frame of recorded rockfall events in high mountains is still difficult, our deductive systemic understanding points toward a strong process linkage between permafrost degradation and rock instability. Enhanced technical understanding of coupled thermo-hydro-mechanical processes and systemic geomorphic understanding of rock slope adjustment in space and over (reaction/relaxation) time are required to accurately predict hazards associated with the impact of climate change on permafrost in bedrock. We identify research needs in four major areas and at the interfaces between them: rock temperature measurement and modelling; remote sensing of rock walls; process understanding of rock mass instability; and flow propagation models of rock-ice avalanches. This short communication identifies key interfaces between research directions to gain a better understanding of trajectories of destabilisation in time and space. We propose coordinated systemic research with respect to scale dependent and transient thermal behaviour, coupled thermo-hydro-mechanical understanding, enhanced remote inventorying of rock wall instability and integrated approaches for a better understanding and modelling of mixed avalanches. Copyright (C) 2012 John Wiley & Sons, Ltd.",2012,10.1002/ppp.740,no
3D Cantilever Model Research on Roller Leveling Process of Plate with Lateral Buckling Defects,"Roller leveling of plate with lateral defects is a complex process in which material's longitudinal strain couples with the transverse one. Different from wavy defects, lateral buckling defects are plates' macroscopic deformation because of stress or fiber length difference between up and down surfaces of the plate. Aiming at plate roller leveling process, a simplified 3D cantilever bending FEM model has been suggested in this paper. In the model, plate's longitudinal bending and lateral warping are decoupled by using ABAQUS' element deactivation function. Moreover, the mechanism of roller leveling process for plates with lateral buckling defects has been developed. On the basis of comparing with the ideal flat plates' bending process, effects of technological parameters such as reduction, roller diameter and roller shape on the lateral buckling defects correction during roller leveling process are also studied.",2012,10.4028/www.scientific.net/AMR.572.290,no
COMPLEXITY AND AGENT BASED MODELS IN THE POLICY PROCESS,"Rosewell and Ormerod have collaborated for over a decade in Volterra Partners, building models using complexity principles for clients in the public and private sectors. We present examples of agent based models commissioned by policy makers and used as inputs into the decision making process. We discuss problems which arise within both public and private sectors when a complex modelling approach is proposed. For example, the standard approach to policy analysis, that of economics, has the advantage in that it often purports to give the right answer. Complex systems modelling in contrast emphasises uncertainty of outcome. The paper describes the way in which policy makers describe and identify problems, how they can be engaged, how they can 'buy in' to the results of a model, how to involve the decision maker in the validation of the model. These points are illustrated from actual models which have been built in practice.",2012,,no
DEVELOPING A KNOWLEDGE PROCESS QUALITY MODEL EVALUATION SYSTEM USING COMMONKADS,"Several knowledge management maturity models have been proposed in the last years. These models are used to evaluate the quality of knowledge management practices in the organizations. One of these models is the Knowledge Process Quality Model, which has five maturity levels. The acquisition of a high maturity level is usually expensive due to the evaluations and improvement processes that are often required for a positive final decision. With the aim of minimizing these costs, this paper proposes a Knowledge-Based System that tries to check if the company currently stands in compliance with a given KPQM maturity level. The actual evaluation process starts only if the system output is positive. This approach implies an important cost reduction by avoiding negative evaluations. The design of the system is based on the CommonKADS methodology, and its implementation was carried out with the Clips tool.",2012,10.5220/0003697504590464,no
A simulation process for asynchronous event processing systems: Evaluating performance and availability in transaction models,"Simulation is essential for understanding the performance and availability behavior of complex systems, but there are significant difficulties when trying to simulate systems with multiple components, which interact with asynchronous communication. A systematic process is needed, in order to cope with the complexity of asynchronous event processing and the failure semantics of the interacting components. We address this problem by introducing an approach that combines formal techniques for faithful representation of the complex system effects and a statistical analysis for simultaneously studying multiple simulation outcomes, in order to interpret them. Our process has been successfully applied to a synthetic workload for distributed transaction processing. We outline the steps followed towards generating a credible simulation model and subsequently we report and interpret the results of the applied statistical analysis. This serves as a proof of concept that the proposed simulation process can be also effective in other asynchronous system contexts, like for example distributed group communication systems, file systems and so on. (C) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.simpat.2012.07.007,no
Evaluation of Process Uncertainty in Activated Sludge Treatment by Probabilistic Modeling,"Simulation-based design and analysis of biotreatment systems depends to a great extent on the accuracy of the model parameters used. In many cases, site-specific parameter estimates are not available and parameter values recommended in the literature are adopted along with a measure of judgment to account for bioprocess uncertainty. To better quantify process certitude associated with this approach and thus provide guidance to process designers lacking site-specific parameter values, stochastic simulations were conducted and the empirical frequency distributions derived from the simulation results were interpreted in terms of process certitude. In general, the simulation results confirmed field experience and current practice in biotreatment from a probabilistic standpoint and provided context in terms of process certitude. The certitude of achieving specific treatment levels increased with sludge age, validating the application of a design safety factor on the minimum solids retention time. The increase in process certitude was not substantial, however, beyond a sludge age of about 4 days for the removal of organic material or about 10 days for the removal of ammonia. Process certitude was also declined when the hydraulic behavior of the treatment system approached complete mixing, which could result from such operational strategies as increasing the recycle ratio or the utilization of step feed. DOI: 10.1061/(ASCE)EE.1943-7870.0000570. (C) 2012 American Society of Civil Engineers.",2012,10.1061/(ASCE)EE.1943-7870.0000570,no
A Kind of Improved GQM Measurement Model for Software Process,"Software quality measurement is closely connected with process and product. With measurement and analysis on software process data, improvement and control on software can be achieved. Effective process performance model plays key role in understanding and controlling. Introduction of GQM model into software measurement process can achieve controllability and testability of software process. GQM uses organization target as measurement index to achieve measurement of software process quantitative. The core idea of CMM/CMMI is to build fact-based management and CMM/CMMI software process management is built on the basis of software measurement. Combining with CMM/CMMI measurement idea, GQM model was expanded to achieve a kind of improved process performance model.",2012,10.4028/www.scientific.net/AMR.341-342.550,no
ISTQB TEST PROCESS EVALUATION BY USING TMMI MODEL,"Software testing is considered as a significant process through software development life cycle that can be managed, measured and improved continuously. Test Improvement is often referred to as the continuous improvement of the quality, and the efficiency of the testing process. This approached raised on the basis that an improved test process leads to an improved quality of the product. ISTQB test suite is a well-known recent endeavor describes the process and included activities, furthermore, it introduces few test process improvement models. In the previous paper [1], we presented a new method to improve the ISTQB test process by mapping CTP and TPI NEXT models. In this paper we aim that to evaluate ISTQB test process with TMMI model to determine current maturity level and it's weakness points.",2012,,no
Solvent-Chamber Development in 3D-Physical-Model Experiments of Solvent-Vapour Extraction (SVX) Processes With Various Permeabilities and Solvent-Vapour Qualities,"Solvent-vapour extraction (SVX) processes offer an attractive alternative to thermal recovery processes by being less energy intensive and are more suitable for thinner, partially depleted reservoirs. A typical SVX process uses solvent injection to dilute the heavy oil by reducing its viscosity, allowing it to be mobilized for production. During this process, the injection of hydrocarbon solvents results in partial deasphalting of the heavy oil, thus reducing its viscosity and enhancing the process performance further. This work examined the formation and growth of solvent chambers in laterally and vertically spaced horizontal injector/producer well pairs in porous media with five different permeabilities and three different solvent-vapour qualities. Consolidation of the porous media caused by asphaltene precipitation was also analyzed. Thermal-imaging and model excavation studies were performed to investigate the formation and growth of solvent chambers for seven different experiments conducted on a large 3D-physical-model apparatus. The important findings from this study are as follows: During solvent injection, one or more solvent fingers develop between the injector and producer. The dominant solvent finger becomes a conduit that grows into a solvent chamber connected to the injection well in the upper portion of the reservoir, and develops into an oil-drainage conduit connected to the production well in the lower portion of the reservoir. Solvent dispersion layers are located on the margins of both the solvent chambers and the oil-drainage conduits. The location and development of these nonuniform solvent chambers and oil-drainage conduits are unpredictable, and the oil-drainage conduits do not grow significantly in diameter once connected to the production wellbore, limiting the wellbore inflow efficiency and conformity. Asphaltene precipitation and migration can aggravate this inflow problem, reducing the SVX process performance further. SVX performance can be improved by increasing the number and diameter of oil-drainage connections between the solvent chamber and the production well, and by controlling the oil deasphalting process. This can be performed by optimizing injection- and production-wellbore geometries, and by optimizing solvent-injection rates and vapour quality.",2012,,no
Evaluation of atmospheric boundary layer-surface process relationships in a regional climate model along an East Antarctic traverse,"Some primary physical relationships related to the surface climate and atmospheric boundary layer were examined over East Antarctica and evaluated in the regional climate model HIRHAM for 2005-2008. For stable conditions, the observation-derived relationship between wind-scaled sensible heat flux and air-surface temperature difference distinctively differs between different surface flux parameterizations. Some of them decrease the heat transfer coefficient CH for strongly stable conditions, while others, such as the Louis scheme, do not. However, HIRHAM's application of the Louis parameterization produces small CH for strongly stable conditions similar to observations and other schemes, likely because a surface roughness much larger than observed is used and the bulk Richardson number differs. For Zhongshan, the observed radiation-cloud, temperature-cloud, and temperature-wind relationships are reproduced in the model, though quantitative differences are evident. An observed longwave warming effect of clouds is larger in the model, while the reduction of downwelling shortwave radiation by clouds is twice as large in the model. The model partially reproduces an observed weak wind regime associated with atmospheric decoupling, but fails to reproduce increasing temperatures with increasing winds. The quantitative differences in the radiation-cloud relationship suggest that errors in cloud characteristics produce a significant deficiency in downwelling net radiation for clear and cloudy conditions. This deficiency is the likely cause of HIRHAM's strong cold bias in the surface temperature and positive bias in near-surface stability. The sensible heat flux analyses and a sensitivity test suggest that errors in the sensible heat flux relationship are not the primary cause.",2012,10.1029/2011JD016441,no
A Thermal and Process Variation Aware MTJ Switching Model and Its Applications in Soft Error Analysis,"Spin-transfer torque random access memory (STT-RAM) has recently gained increased attentions from circuit design and architecture societies. Although STT-RAM offers a good combination of small cell size, nanosecond access time and non-volatility for embedded memory applications, the reliability of STT-RAM is severely impacted by device variations and environmental disturbances. In this paper, we develop a compact switching model for magnetic tunneling junction (MTJ), which is the data storage device in STT-RAM cells. By leveraging the capability to simulate the impacts of thermal and process variations on MTJ switching, our model is able to analyze the diverse mechanisms of STT-RAM write operation failures. Besides the impacts of thermal and process variation, the soft error induced by radiation striking on the access transistor is another important threat to the MTJ reliability. It can also be analyzed by using our model. The incurred computation cost of our model is much less than the conventional macro-magnetic model, and hence, enabling its applications in comprehensive STT-RAM reliability analysis and design optimizations.",2012,,no
Evaluating subcontractor performance using Evolutionary Gaussian Process Inference Model,"Subcontractor Evaluation is one of the methods which general contractors use to evaluate subcontractor performance. The result is often used as a reference index for subcontractor choice during the outsourcing of activities within a project. Inappropriate subcontractor choice would have a direct impact on the duration, cost, quality, and safety of a project, leading to failure in achieving its goals and target profits. Therefore, this paper establishes a set of Evolutionary Gaussian Process Inference Model, which utilize a Gaussian Process to map the relationships between data input and output and uses Bayesian inference together with Particle Swarm Optimization to optimize the hyper-parameters of the Gaussian Process covariance function to obtain the best inference predictive ability. The model provides construction managers with quantitative measures of subcontractor performance in their selection process. [Min-Yuan Cheng, Chin-Chi Huang. Evaluating subcontractor performance using Evolutionary Gaussian Process Inference Model. Life Science Journal 2012; 9(2): 527-532]. (ISSN: 1097-8135). http://www.lifesciencesite.com. 79",2012,,no
Sustainable Energy Management and Quality Process Models Based on ISO50001:2011 The International Energy Management Standard,"Sustainable energy management and related green engineering design and manufacturing processes are affecting every aspect of our lives. Sustainable energy management, product and process engineering, green, lean and quality design, manufacturing / system energy management rules and principles are offered with a focus on the new ISO 50001:2011 International Energy Management Standard.",2012,,no
"Developing an analytical model for planning systems verification, validation and testing processes","System VVT (verification, validation, and testing) are three tasks of System Engineering that focus on ensuring that systems are designed and delivered to meet customer and engineering requirements in the best way possible. Most organizations use sub-optimal VVT processes and methods. The literature does not offer an effective approach for associating VVT methods to VVT activities in order to satisfy customer and engineering requirements. In many large and complex projects, the project manager faces the dilemma of how best to validate and verify customer and engineering requirements, respectively. In many cases, decisions are made in an intuitive manner. For a project with a small amount of requirements (e.g., design of a new chair, table, or a simple toy), optimum decisions for VVT methods to be included within the project are feasible. For projects with large amount of requirements, for example, design of a new payload (e.g., captive carriage of a fuel tank, camera pod or other equipment) on an aircraft, a structured process to evaluate the overall impact of VVT methods implemented in order to satisfy those requirements, and the risk involved by performing these and not other methods, is necessary. This paper proposes a model for selecting an appropriate VVT approach depending on the phase or the level of the product in the system hierarchy; the model is independent of project size or precedence. We present an analytical model that not only structures the decision process but also outputs the optimal VVT methods given Cost and Risk constraints. The analytical model was formulated as an optimization problem, where a function that associates Quality derived from incorporating VVT methods is maximized subject to Cost and/or Risk constraints. The use of the model is demonstrated on a sample problem. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.aei.2012.02.001,no
Robustness of System Equivalent Reduction Expansion Process on Spacecraft Structure Model Validation,"Test-analysis models are used in the validation of the finite element models of spacecraft structures. Here, a probabilistic approach is used to assess the robustness of a system equivalent reduction expansion process based test-analysis model when experimental and analytical modes contain different levels of inaccuracy. The approach is applied to three spacecraft models, and Monte Carlo simulations were used to determine the sensitivity of the normalized cross-orthogonality check to the system equivalent reduction expansion process reduced matrix. The effect of parameters used in this reduction and the amount of inaccuracies that can be tolerated in the modes before failing the normalized cross-orthogonality check were also determined. The results show that the probability to pass the normalized cross-orthogonality check is highly determined by the number of modes used in the reduction. The relation between capability of the finite element models to predict the frequency-response function and the quality of the model validation determined using normalized cross-orthogonality check is also investigated, and it is observed that the quantities are not always correlated. This study also shows that the sensor locations can be optimally chosen using the system equivalent reduction expansion process reduced mass matrix, and this can increase the probability to pass the normalized cross-orthogonality check.",2012,10.2514/1.J051476,no
"Use of consistency index, expert prioritization and direct numerical inputs for generic fuzzy-AHP modeling: A process model for shipping asset management","The aim of this paper is to develop a generic version of the conventional fuzzy-analytic hierarchy process (FAHP) method and investigate the shipping asset management (SAM) problem in the dry bulk shipping market. The recent literature has various applications of the FAHP, but these studies lack consistency control, use identical decision support rather than weighted expert choices, and lack measurable criteria. The proposed model, generic fuzzy-AHP (here after GF-AHP), provides a standard control of consistency on the decision matrix for the expert group. GF-AHP also improves the capabilities of the FAHP by executing direct numerical inputs without expert consultation. In practical business, some of the criteria can be easily calculated and expert consultation is a redundant process. Therefore. GF-AHP presents how to transform such numerical inputs to a priority scale. Finally, expertise differences on the decision group are reflected in the GF-AHP process by an expert weighting algorithm. (C) 2011 Elsevier Ltd. All rights reserved.",2012,10.1016/j.eswa.2011.08.056,no
To the Problem of Mathematical Modeling of the Oil Deposit Processes,The approaches to improve the effectiveness of oil field exploitation using mathematical models of the oil deposit processes for forecast oil well debit are realized. It has been shown that using formula Kolmogorov-Erofeev allowed to receive more adequate model. On the base system analysis of the oil deposit processes was carried out the selection and estimation of important oil deposit parameters and indexes using on the mathematical modeling.,2012,,no
"Quality Marks, Metrics, and Measurement Procedures for Business Process Models The 3QM-Framework","The availability of high-quality business process models is a central prerequisite for a successful process management. Nevertheless, in practice process models exhibit a large number of quality deficits, among them grammatical, content-related, and stylistic defects. In addition, there exist only very few approaches to determine the quality of business process models. In this paper, we present the 3QM-Framework, an analytical approach to systematically determine the quality of business process models. The 3QM-Framework makes three contributions: it provides quality marks, metrics, and measurement procedures to quantify the quality level as elements of a theoretically justified quality model. The applicability of the 3QM-Framework has been empirically evaluated in case studies. The results of a survey that was conducted among experts moreover attest its practical relevance.",2012,10.1007/s12599-012-0230-8,no
Model verification and evaluation of the rich-split process modification at an Australian-based post combustion CO2 capture pilot plant,"The CSIRO is involved in three CO2 capture pilot plants operating at different coal-fired power stations throughout Australia. The most recently completed of these is the Tarong CO2 capture pilot plant located at Tarong power station, Nanango, Queensland. The first phase of the experimental program with this pilot plant included operation with monoethanolamine (MEA). This involved parametric studies, process modifications, and finally implementation of 24 h operation. Operation of the pilot plant has shown MEA to be effective in capturing CO2 from the flue gas from Tarong Power Station. CO2 capture efficiencies of up to 94%, and regeneration energies as low as 3.6 MJ/kgCO(2) have been achieved. The design of the pilot plant was completed using a commercially available process modeling software tool. Results obtained from the pilot plant were then compared to the model predictions including temperature, solvent CO2 loading, and CO2 gas concentration profiles through the absorber column. A good match has been obtained between the modeling and pilot plant data, verifying the software can be used to predict the performance of the pilot plant when operating on MEA. During this project, the rich-split process modification was also evaluated. The results suggest that the rich-split modification can achieve some reduction in reboiler duty and a considerable reduction in the condenser duty. The amount of reduction is dependent on plant design, particularly the efficiency of the lean/rich heat exchanger. (c) 2012 Society of Chemical Industry and John Wiley & Sons Ltd",2012,10.1002/ghg.1295,no
Making Recommendations for Decision Processes Based on Aggregated Decision Data Models,"The decision making process is a sequence of (mostly mental) actions. But individual decision making is a fuzzy process that lacks a clear workflow structure. This issue may decrease the quality of data-centric business decisions where information must be processed in the right order and used at the right time. We argue that, when faced with such a decision, step-by-step recommendation provides help in steering the process and valuable guidance in improving it. Our Data Decision Model (DDM) is an acyclic graph that suits the fuzzy nature of decision processes. In our approach, the recommendation is based on an aggregated DDM extracted from a large number of individuals. This paper introduces two algorithms that, given a certain state of the process, provide suggestions for the next action the decision maker should perform.",2012,,no
Clinical Guideline Representation in a CDS: a Human Information Processing Method,"The Dutch Childhood Oncology Group (DCOG) has developed evidence-based guidelines for screening childhood cancer survivors for possible late complications of treatment. These paper-based guidelines appeared to not suit clinicians' information retrieval strategies; it was thus decided to communicate the guidelines through a Computerized Decision Support (CDS) tool. To ensure high usability of this tool, an analysis of clinicians' cognitive strategies in retrieving information from the paper-based guidelines was used as requirements elicitation method. An information processing model was developed through an analysis of think aloud protocols and used as input for the design of the CDS user interface. Usability analysis of the user interface showed that the navigational structure of the CDS tool fitted well with the clinicians' mental strategies employed in deciding on survivors screening protocols. Clinicians were more efficient and more complete in deciding on patient-tailored screening procedures when supported by the CDS tool than by the paper-based guideline booklet. The think-aloud method provided detailed insight into users' clinical work patterns that supported the design of a highly usable CDS system.",2012,10.3233/978-1-61499-101-4-427,no
Evaluation of a model system for the selective study of the lipid peroxidation process,"The effect of molecular environment on the peroxidation of linoleic acid (LA), a polyunsaturated fatty acid (PUFA), initiated by ferrous ions was investigated in acidic and neutral pH conditions. Mixed nonionic surfactants TWEEN (R)-20/LA micelles were established as a model system to obtain a surfactant-in-lipid aqueous system at high acidity level. The peroxidation of LA was induced by ferrous ions and the kinetics of the produced conjugated dienes was followed by UV measurements and the ferric thiocynate method. Ferrous ions were oxidized only by the preformed LA hydroperoxides, which under established conditions produced lipid alkoxyl and peroxyl radicals as the sole initiators of propagation. The results revealed the LA peroxidation process remained mainly unaffected within the 2.5<pH<5.5 range, while highly pH sensitive around pH 7. The propagation process prevailed at optimal concentrations of 500?mu M of LA and 280?mu M TWEEN (R)-20, and at the ferrous ion concentration up to 75?mu M, irrespective of the buffer used. Practical applications: A simple model system in water, suitable for the selective study of the lipid peroxidation propagation phase induced by ferrous ion is presented here. Fatty acids serve as model compounds susceptible to processes associated with oxidative radical initiated-modifications of lipids. The obtained results contribute to a better understanding of the oxidative behavior of lipids, particularly those soluble in nonionic surfactant micelles in acidic medium. The oxidative stability of the PUFA in model systems containing TWEEN (R)-20 and ferrous ion at low pH could be predicted and controlled by measuring the lipid hydroperoxide formation. The experimental conditions presented may also provide a suitable system for the study of the termination phase of lipid peroxidation.",2012,10.1002/ejlt.201200116,no
The Process Modeling and Analysis of Meat Production Quality Control based on Petri Nets,"The food safety risks may flow into food circulation through any link in supply chain. Based on an example of low temperature-heated meat production and the HACCP (Hazard Analysis Critical Control Points) method of meat production, this paper gives a detailed description of the meat quality control process and establishes a model of monitoring the low temperature-heated meat production quality by Petri net. The reachability, liveness and reversibility of the model will be verified by building up the reachable tree, and the boundedness and conservativeness of the model will be derived by using incidence matrix to analyze the invariant P. This method can offer a support for realizing the automatic monitoring during the quality control in the meat production process.",2012,,no
Understanding and Control of Dimethyl Sulfate in a Manufacturing Process: Kinetic Modeling of a Fischer Esterification Catalyzed by H2SO4,"The formation and fate of monomethyl sulfate (MMS) and dimethyl sulfate (DMS) were studied by proton NMR for a sulfuric acid catalyzed esterification reaction in methanol. The kinetic rate constants for DMS and MMS were determined at 65 degrees C by fitting time-dependent experimental data to a model using DynoChem. In refluxing methanol, sulfuric acid was converted to monomethyl sulfate (MMS) in nearly quantitative yield within 45 mm. Once formed, the MMS underwent a reversible esterification reaction to form DMS. Dimethylsulfate reacted with methanol to regenerate MMS and form dimethyl ether. A byproduct of the esterification reaction was water, which further consumed DMS through hydrolysis. On the basis of derived rate constants, in refluxing methanol, DMS would not be expected to exceed 4 ppm in the reaction mixture at equilibrium. In the presence of the carboxylic acid substrate, DMS was not detected in the reaction mixture. The reaction pathways of this system have been systematically investigated, and the results of this study will be presented.",2012,10.1021/op200323j,no
"Process Reuse in Product Development with 5D Models: Concepts, Similarity Measures and Querying Techniques","The goal of this paper is to establish a framework for process reuse in 'collaborative product development' (CPD) supported by 5D product models. 5D integrates 3D models with non-geometrical metadata, costs, and schedules. Concepts, formal definitions, and possible technical solutions are proposed for (1) the reuse of business processes in 'collaborative product development' and (2) the reuse of related models and data embedded in 5D models of products. Drawing from the analysis of product development processes and 5D models first-order logic statements for explicit and implicit process reuse are defined. The process reuse is proposed through process repositories, conceptualised workflows, the querying of process results (e.g., 3D VR models, procedures), and used production resources (i.e., labour, equipment, material, finance). This study may advance process reuse within complex design communities and inspire the development of systems for project managers and product engineers.",2012,,no
Process-evaluation of tropospheric humidity simulated by general circulation models using water vapor isotopologues: 1. Comparison between models and observations,"The goal of this study is to determine how H2O and HDO measurements in water vapor can be used to detect and diagnose biases in the representation of processes controlling tropospheric humidity in atmospheric general circulation models (GCMs). We analyze a large number of isotopic data sets (four satellite, sixteen ground-based remote-sensing, five surface in situ and three aircraft data sets) that are sensitive to different altitudes throughout the free troposphere. Despite significant differences between data sets, we identify some observed HDO/H2O characteristics that are robust across data sets and that can be used to evaluate models. We evaluate the isotopic GCM LMDZ, accounting for the effects of spatiotemporal sampling and instrument sensitivity. We find that LMDZ reproduces the spatial patterns in the lower and mid troposphere remarkably well. However, it underestimates the amplitude of seasonal variations in isotopic composition at all levels in the subtropics and in midlatitudes, and this bias is consistent across all data sets. LMDZ also underestimates the observed meridional isotopic gradient and the contrast between dry and convective tropical regions compared to satellite data sets. Comparison with six other isotope-enabled GCMs from the SWING2 project shows that biases exhibited by LMDZ are common to all models. The SWING2 GCMs show a very large spread in isotopic behavior that is not obviously related to that of humidity, suggesting water vapor isotopic measurements could be used to expose model shortcomings. In a companion paper, the isotopic differences between models are interpreted in terms of biases in the representation of processes controlling humidity.",2012,10.1029/2011JD016621,no
An integrated risk measurement and optimization model for trustworthy software process management,"The growing demand for higher trustworthiness of software poses an unprecedented challenge to the software industry. Risk management is the important part for high quality software development processes. However, under the constraints of project cost and duration, it is very difficult to establish the budget for risk management. To integrate efficient risk management and pure software process is the goal of this paper. We propose a software process model with risk management and cost control modules to help improve software process risk management. Furthermore, based on this process model, a measurement model that includes process risk and software trustworthiness metrics is presented. Through risk management effectiveness calculation methods and risk transfer assumptions, a software process risk optimization model is proposed. This model can be used to derive an optimized risk management scheme for the process of trustworthy software development, with constraints of process cost and duration. Simulation cases are then analyzed by this model framework. The results show that risk management is critical to enhance trustworthiness but risk management is an effective complement, rather than the most fundamental process, to enhance the trustworthiness of software. Software developers should adopt appropriate and optimal strategies about risk management inputs, especially in lower CMMI level companies. (C) 2011 Elsevier Inc. All rights reserved.",2012,10.1016/j.ins.2011.09.040,no
Digging through model complexity: using hierarchical models to uncover evolutionary processes in the wild,"The growing interest for studying questions in the wild requires acknowledging that eco-evolutionary processes are complex, hierarchically structured and often partially observed or with measurement error. These issues have long been ignored in evolutionary biology, which might have led to flawed inference when addressing evolutionary questions. Hierarchical modelling (HM) has been proposed as a generic statistical framework to deal with complexity in ecological data and account for uncertainty. However, to date, HM has seldom been used to investigate evolutionary mechanisms possibly underlying observed patterns. Here, we contend the HM approach offers a relevant approach for the study of eco-evolutionary processes in the wild by confronting formal theories to empirical data through proper statistical inference. Studying eco-evolutionary processes requires considering the complete and often complex life histories of organisms. We show how this can be achieved by combining sequentially all life-history components and all available sources of information through HM. We demonstrate how eco-evolutionary processes may be poorly inferred or even missed without using the full potential of HM. As a case study, we use the Atlantic salmon and data on wild marked juveniles. We assess a reaction norm for migration and two potential trade-offs for survival. Overall, HM has a great potential to address evolutionary questions and investigate important processes that could not previously be assessed in laboratory or short time-scale studies.",2012,10.1111/j.1420-9101.2012.02590.x,no
A context-aware data mining process model based framework for supporting evaluation of data mining results,"The knowledge discovery via data mining process (KDDM) is a multiple phase that aims to at a minimum semi-automatically extract new knowledge from existing datasets. For many data mining tasks, the evaluation phase is a challenging one for various reasons. Given this challenge several studies have presented techniques that could be used for the semi-automated evaluation of data mining results. When taken together, these studies suggest the possibility of a common multi-criteria evaluation framework. The use of such a multi-criteria evaluation framework, however, requires that relevant objectives, measures and preference function be identified. This implies that the context of the DM problem is particularly important for the evaluation phase of the KDDM process. Our framework utilizes and integrates a pair of established tightly coupled techniques (i.e. Value Focused Thinking (VFT) and the Goal-Question-Metric (GQM) methods) as well as established techniques from multi-criteria decision analysis in order to explicate and utilize context information in order to facilitate semi-automated evaluation. (C) 2011 Elsevier Ltd. All rights reserved.",2012,10.1016/j.eswa.2011.07.117,no
Changes in the initial stages of a glucose-proline Maillard reaction model system influences dairy product quality during thermal processing,"The Maillard reaction always occurs during the thermal processing of dairy products, which significantly influences their quality. In the present study, the initial stages of a glucose-proline model system were investigated in water and different types of buffer solutions. Results showed that phosphate buffer accelerated the reversible degradation of the initial stages of the reaction. The proposed catalysis mechanism was that hydrogenous and dihydric phosphate radical anions simultaneously accepted and donated protons for the conversion of the intermediates into N-glycosylamine. The catalysis mechanism was confirmed via testing and no reducing of hydrogenous and dihydric phosphate radical anions was observed during the reaction. Moreover, both N-(1-deoxy-d-fructos-1-yl) proline and its degradation compounds were analyzed. Results showed that degradation of N-(1-deoxy-d-fructos-1-yl) proline to form 5-hydroxymethyl-2-furaldehyde and formic acid was also accelerated by phosphate buffer. An interesting phenomenon was that citrate decreased 5-hydroxymethyl-2-furaldehyde formation, which might be because Strecker-type degradation occurred more easily than 1,2-enolization reaction in citrate buffer solution. However, this hypothesis has not been confirmed, and element label experiments should be carried out in the future.",2012,10.3168/jds.2011-4860,no
THE MODEL OF THE THREE HUMAN PERSONALITY TYPES: A UNIQUE TOOL FOR THE UNDERSTANDING OF THE INDIVIDUAL DIFFERENCES IN THE TEACHING-LEARNING PROCESS,"The Model of The Three Human Personality Types is a novel discovery that unravels the function of the human brain and how personality and learning are interrelated. According to this model all people are divided into only three human types with common personality and biological traits, Type A, B and C. The Three Human Personality Type Model is an innovative cognitive and neurobiological causal model of behavior. The purpose of this study is to present the Three Human Personality Type Model, as well as the unique benefits that this model offers to teachers and students. More than 20000 people were empirically studied, from which more than 10000 were patients. Also, the first-degree relatives of those were studied. All individuals have answered the Three Human Personality Type Model Questionnaire. The knowledge of ones individual's type is a key to the personalized teaching. It is a multidimensional tool for recognizing the personality and the genetically determined learning styles of the student and the teacher. The knowledge of ones individual's type promotes self-knowledge, hetero-knowledge and psychological and physical health. The model also provides insights on the appropriate vocational guidance for the student, as each type has different particularities, strengths and weaknesses, thus different jobs suit to each type. It is very important to know our student's type, so that we can easily identify his cognitive and learning abilities and skills and give him the right stepping stones to a happy, creative and successful life.",2012,,no
High-boiling-point petroleum fractions upgrading using the centrifugal reactive-molecular distillation process over catalyst: Mathematical modeling and simulation including experimental validation,"The modeling of the reactive molecular distillation process (centrifugal type), in which the molecular distillation process and reactive process occur simultaneously to upgrade heavy petroleum crude oil, is presented in this work. The mathematical model involves equations for the evaluation of the physicochemical properties, in-situ cracking reaction, heat, continuity and material balances. A case study is illustrated for an atmospheric petroleum residue (>673.15 K) of ""W"" crude oil. The influence of adding a catalyst (3 and 5 wt%) was examined under different operating conditions: the process temperature range was from 473.15 K to 523.15 K (considering a pressure between 40 50 Pa) and the constant feed flow rate was 1.473 kg h(-1). The results showed that the concentration of the pseudocomponent ""a"" shrank in both the s- and r- directions reaching an extent conversion of the feedstock about 50% with 3 and 5 wt% of catalyst. Due to the rapid temperature rise in the thin liquid film, the thickness of the film rapidly decreased in this region, whereas the amount of distillate from the split molecules continuously increased throughout the evaporator under the selected conditions. The experimental results agreed well with those obtained from the theoretical simulations, indicating the accuracy and reliability of the mathematical model, since the average percent error was no larger than 6.82%, 9.39% and 14.92% for the distillate flow rate and the extent of conversion in the distillate and in the residue stream, respectively.",2012,,no
Comparison of Nonmem 7.2 estimation methods and parallel processing efficiency on a target-mediated drug disposition model,"The paper compares performance of Nonmem estimation methods-first order conditional estimation with interaction (FOCEI), iterative two stage (ITS), Monte Carlo importance sampling (IMP), importance sampling assisted by mode a posteriori (IMPMAP), stochastic approximation expectation-maximization (SAEM), and Markov chain Monte Carlo Bayesian (BAYES), on the simulated examples of a monoclonal antibody with target-mediated drug disposition (TMDD), demonstrates how optimization of the estimation options improves performance, and compares standard errors of Nonmem parameter estimates with those predicted by PFIM 3.2 optimal design software. In the examples of the one-and two-target quasi-steady-state TMDD models with rich sampling, the parameter estimates and standard errors of the new Nonmem 7.2.0 ITS, IMP, IMPMAP, SAEM and BAYES estimation methods were similar to the FOCEI method, although larger deviation from the true parameter values (those used to simulate the data) was observed using the BAYES method for poorly identifiable parameters. Standard errors of the parameter estimates were in general agreement with the PFIM 3.2 predictions. The ITS, IMP, and IMPMAP methods with the convergence tester were the fastest methods, reducing the computation time by about ten times relative to the FOCEI method. Use of lower computational precision requirements for the FOCEI method reduced the estimation time by 3-5 times without compromising the quality of the parameter estimates, and equaled or exceeded the speed of the SAEM and BAYES methods. Use of parallel computations with 4-12 processors running on the same computer improved the speed proportionally to the number of processors with the efficiency (for 12 processor run) in the range of 85-95% for all methods except BAYES, which had parallelization efficiency of about 70%.",2012,10.1007/s10928-011-9228-y,no
Process modeling and kinetic evaluation of petroleum refinery wastewater treatment in a photocatalytic reactor using TiO2 nanoparticles,"The photocatalytic oxidation and mineralization of petroleum refinery wastewater in aqueous catalyst suspensions of titanium dioxide (TiO2), Degussa P25 (80% anatase, 20% rutile), were carried out in a batch circulating photocatalytic reactor. The experiments were conducted based on a central composite design (CCD) and analyzed using response surface methodology (RSM). In order to analyze the process, four significant variables viz. pH (2-10), catalyst concentration (0-200 mg/l), temperature (22.5-52.5 degrees C), and reaction time (30-150 min) and TCOD removal as the process response were studied. From the data derived from the factorial design, the ANOVA analysis revealed that the first-order effects of reaction time, pH, temperature and catalyst concentration and second-order effect of pH, catalyst concentration and temperature produce the main effect on TCOD removal efficiency. A maximum reduction in TCOD of more than 83% was achieved at the optimum conditions (pH of 4, catalyst concentration of 100 mg/l, temperature of 45 degrees C and reaction time of 120 min). The reaction kinetics showed that reactive activation energy for TCOD conversion was calculated to be 19.34 kJ/mol. (C) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.powtec.2012.01.003,no
Potts model based on a Markov process computation solves the community structure problem effectively,"The Potts model is a powerful tool to uncover community structure in complex networks. Here, we propose a framework to reveal the optimal number of communities and stability of network structure by quantitatively analyzing the dynamics of the Potts model. Specifically we model the community structure detection Potts procedure by a Markov process, which has a clear mathematical explanation. Then we show that the local uniform behavior of spin values across multiple timescales in the representation of the Markov variables could naturally reveal the network's hierarchical community structure. In addition, critical topological information regarding multivariate spin configuration could also be inferred from the spectral signatures of the Markov process. Finally an algorithm is developed to determine fuzzy communities based on the optimal number of communities and the stability across multiple timescales. The effectiveness and efficiency of our algorithm are theoretically analyzed as well as experimentally validated.",2012,10.1103/PhysRevE.86.016109,no
Multiway Gaussian Mixture Model Based Adaptive Kernel Partial Least Squares Regression Method for Soft Sensor Estimation and Reliable Quality Prediction of Nonlinear Multiphase Batch Processes,"The predictive model based soft sensor technique has become increasingly important to provide reliable online measurements, facilitate advanced process control and optimization, and improve product quality in process industries. The conventional soft sensors are normally single-model based and thus may not be appropriate for processes with shifting operating phases or conditions and the underlying changing dynamics. In this study, a multiway Gaussian mixture model (MGMM) based adaptive kernel partial least-squares (AKPLS) method is proposed to handle online quality prediction of batch or semibatch processes with multiple operating phases. The three-dimensional measurement data are first preprocessed and unfolded into two-dimensional matrix. Then, the multiway Gaussian mixture model is estimated in order to identify and isolate different operating phases. Further, the process and quality measurements are separated into multiple segments corresponding to those identified phases, and the various localized kernel PLS models are built in the high-dimensional nonlinear feature space to characterize the shifting dynamics across different operating phases. Using Bayesian inference strategy, each process measurement sample of a new batch is classified into a particular phase with the maximal posterior probability, and thus, the local kernel PLS model representing the identical phase can be adaptively chosen for online quality variable prediction. The presented soft sensor modeling method is applied to a simulated multiphase penicillin fermentation process, and the computational results demonstrate that the proposed MGMM-AKPLS approach is superior to the conventional kernel PLS model in terms of prediction accuracy and model reliability.",2012,10.1021/ie3020186,no
Dealing With Feeling: A Meta-Analysis of the Effectiveness of Strategies Derived From the Process Model of Emotion Regulation,"The present meta-analysis investigated the effectiveness of strategies derived from the process model of emotion regulation in modifying emotional outcomes as indexed by experiential, behavioral, and physiological measures. A systematic search of the literature identified 306 experimental comparisons of different emotion regulation (ER) strategies. ER instructions were coded according to a new taxonomy, and meta-analysis was used to evaluate the effectiveness of each strategy across studies. The findings revealed differences in effectiveness between ER processes: Attentional deployment had no effect on emotional outcomes (d(+) = 0.00), response modulation had a small effect (d(+) = 0.16), and cognitive change had a small-to-medium effect (d(+) = 0.36). There were also important within-process differences. We identified 7 types of attentional deployment, 4 types of cognitive change, and 4 types of response modulation, and these distinctions had a substantial influence on effectiveness. Whereas distraction was an effective way to regulate emotions (d(+) = 0.27), concentration was not (d(+) = -0.26). Similarly. suppressing the expression of emotion proved effective (d(+) = 0.32), but suppressing the experience of emotion or suppressing thoughts of the emotion-eliciting event did not (d(+) = -0.04 and -0.12, respectively). Finally, reappraising the emotional response proved less effective (d(+) = 0.23) than reappraising the emotional stimulus (d(+) = 0.36) or using perspective taking (d(+) = 0.45). The review also identified several moderators of strategy effectiveness including factors related to the (a) to-be-regulated emotion, (b) frequency of use and intended purpose of the ER strategy, (c) study design, and (d) study characteristics.",2012,10.1037/a0027600,no
PROBLEM-BASED EDUCATION PROCESS MODELLING AT THE LESSONS OF HISTORY OF LATVIA AT SECONDARY SCHOOL,"The problem-based learning is based on the student historic self-experience and the skill to update their experience. The history teaching for the teacher, the history learning for the students turns into collaboration at the lessons, where obviously up-bringing and self-upbringing processes also take place. The use of the problem-based learning or exteriorization is especially significant. Due to implementation of the problem-based learning method the greatest gain is that the student thinking changes. The peculiar conditions created during the lessons, where the student on the basis of his already gained knowledge, autonomously discovers and perceives the offered or arisen educational problem. Then they by thinking and action search and substantiate the most optimal variants for the solution of the problem. During the procedures in the lessons based on the educational problem and action theory, the student autonomous part of the cognition increases significantly. By solving the problem situations, the intensity of their thinking is reinforced for search of new knowledge and new approaches to solving of the tasks.",2012,,no
AHP-Based Evaluation Model for Optimal Selection Process of Patching Materials for Concrete Repair: Focused on Quantitative Requirements,"The process of selecting a repair material is a typical one of multi-criteria decision-making (MCDM) problems. In this study Analytical Hierarch Process was applied to solve this MCDM problem. Many factors affecting a process to select an optimal repair material can be classified into quantitative and qualitative requirements and this study handled only quantitative items. Quantitative requirements in the optimal selection model for repair material were divided into two parts, namely, the required chemical performance and the required physical performance. The former is composed of alkali-resistance, chloride permeability and electrical resistivity. The latter is composed of compressive strength, tensile strength, adhesive strength, drying shrinkage, elasticity and thermal expansion. The result of the study shows that this method is the useful and rational engineering approach in the problem concerning the selection of one out of many candidate repair materials even if this study was limited to repair material only for chloride-deteriorated concrete.",2012,10.1007/s40069-012-0009-9,no
A corpus-based computational model of metaphor understanding consisting of two processes,"The purpose of this study is to construct a computational model of metaphor understanding based on statistical corpora analysis and that includes dynamic interaction among features. The constructed model consists of two processes: a categorization process and a dynamic-interaction process. The categorization process model, which is based on the class inclusion theory, represents how a target is assigned to an ad hoc category of which the vehicle is a prototypical member. The dynamic-interaction process model represents how the target assigned to the ad hoc category is influenced and how emergent features are emphasized by dynamic interactions among features. The dynamic interaction is realized based on a recurrent neural network. The constructed model is able to highlight the emphasized features of a metaphorical expression. Finally, real-world experiments are conducted in order to verify the semantic validity of the constructed model of metaphor understanding with dynamic interactions. The results from the real-world experiments support the model incorporating dynamic interaction. (C) 2012 Elsevier B. V. All rights reserved.",2012,10.1016/j.cogsys.2012.03.001,no
Understanding Motivational Processes in University Rugby Players: A Preliminary Test of the Hierarchical Model of Intrinsic and Extrinsic Motivation at the Contextual Level,"The purpose of this study was to examine the relationship between perceptions of autonomy support, structure and involvement provided by the head coach and motivational processes at mid- and late-season in competitive rugby players. Participants (M-age = 20.17 years, SD = 1.61 years, Range = 18 to 27 years) completed assessments of perceived coaching style and psychological need fulfillment at the mid-season point (n(mid-season) = 102; 47.05% female) and motivation to continue playing rugby and perceived effort spent playing rugby at the late-season assessment (Nlate-season = 82; 53.64% female). Structural equation modeling analyses se provided support for a conceptual model whereby global perceptions of coach support predicted greater need fulfillment which, in turn, was associated with autonomous sport motivation and greater perceived effort. Overall, the results of this study lend partial support for Vallerand's contentions regarding the importance of motivation processes in sport and imply structure and involvement may be important components of a coach's interactional style that impact athletes' motivation.",2012,,no
Process of Judgment Error on Collision Risk by Modeling Judgment Patterns of Officers,"The purposes of marine accident analysis are to find out the cause and safety measures of marine accidents and to evaluate the effectiveness of the measures. Many marine accidents are considered to be due to human factors. Experiments with bridge simulators are effective for analyzing human factors in ship operation. Analytical approaches to some accidents with bridge simulators have already been carried out. However, analytical method has not been established yet. In our previous paper [1], as the first phase of analysis, we proposed a method to identify the most important task (bottle neck task) in the accident process. As a result of experiments and analysis of a collision accident, ""Judgment"" was identified as the bottle neck task for preventing collisions. However, in order to identify the primary cause of the accident, it is necessary to know the process of judgment error (background) of the accident. In this paper, as the second phase for analysis, process of judgment error in the collision accident is made clear by modeling judgment pattern of officers. From the result, two-stage judgment error is identified as the primary cause of the collision.",2012,10.1109/ICETET.2012.35,no
Research on Law Risks Identification and Evaluating Model in the Whole Process of Engineering Construction Project,"The research runs through all the process of engineering construction project. It makes identification about the law risks of construction project from the perspective of construction laws and regulations. Also it makes analysis about the risks of every stage. In the base of this analysis, we put forward a simple and effective evaluation model. Of course, we also describe the way to structure the model and how to use it. Taking early phase of construction project for example we make a questionnaire in the base of properties and characteristics of six types risks finding from this stage. After tackling and modeling these data which found from the questionnaire, we can get a comprehensive data about risk. And then we can evaluate the risk level of all stages in the project. At last, from the long run, we can provide the basis for risk management to all the engineering stages.",2012,,no
A comprehensive evaluation model for rockburst risk prediction based on analytic hierarchy process and probabilistic optimization,"The rockburst problems cause serious casualties and economic loss and become a major disaster in the field of industrial safety. According to the shortcomings both of the single assessment index and the comprehensive evaluation methods for rockburst prediction at present, and considering the randomness and relative accuracy of basic factors in geotechnical engineering, a comprehensive evaluation model for rockburst risk prediction on the base of analytic hierarchy process and probabilistic optimization is put forward. The model adopts multiclass indexes and a normalized thinking, coupled with the combination of qualitative and quantitative analysis. It can predict not only the intensity grade but also the occurring probability of rockburst. Moreover, a method with extended application including its condition is presented to make the model more applicable. The model is applied to deep-buried tunnel section of auxiliary tunnels of Jinping II hydropower station. The result shows that this model is more than available, but also comprehensive, simple and convenient.",2012,,no
Accounting for structural error and uncertainty in a model: An approach based on model parameters as stochastic processes,"The significance of model structure error and uncertainty (MSEU), sometimes referred to as conceptual error, is rarely adequately recognized. MSEU, moreover, is not an esoteric matter of little consequence to the formation of policy for environmental protection and ameliorating the prospective effects of climate change. The paper presents an approach to accounting for MSEU in which the parameters of a model are treated as stochastic processes and modeled as Generalized Random Walks. Our approach is inspired by the algorithms of recursive estimation and filtering theory. In particular, given an innovations representation of the model's structure, we are able to exploit the dichotomy of what is considered to be the (presumed known) in the model's structure and its complement, the (acknowledged unknown). Two conceptually different groups of model parameters attach to this dichotomy: those familiar to us as the conventional parameters in a model's structure; and those having to do with the way in which past (systematic) forecasting errors - in fact, the innovations errors - are distributed (fed back) into the generation of future predictions through a gain matrix (in the sense of filtering theory). A hypothetical biological system with nonlinear dynamics is specified as the prototypical case study for assessing and comparing the performance of our proposed approach with three other approaches to accounting for MSEU: model fitting error; the expansion of parametric uncertainty; and Bayesian model averaging. Our predictive test cases are constructed around future conditions in which the pattern of input disturbances of the system's behavior is broadly similar to that of their past observed pattern (as used for prior identification, or calibration, of the model). In specific terms, however, future input disturbance patterns are significantly different. Our new approach and that of Bayesian model averaging are found to perform well on this hypothetical system; the performances of the approaches of model fitting error and the expansion of parametric uncertainty are shown to be inferior. (C) 2011 Elsevier Ltd. All rights reserved.",2012,10.1016/j.envsoft.2011.08.015,no
WEAK ERROR ANALYSIS OF NUMERICAL METHODS FOR STOCHASTIC MODELS OF POPULATION PROCESSES,"The simplest, and most common, stochastic model for population processes, including those from biochemistry and cell biology, are continuous time Markov chains. Simulation of such models is often relatively straightforward, as there are easily implementable methods for the generation of exact sample paths. However, when using ensemble averages to approximate expected values, the computational complexity can become prohibitive as the number of computations per path scales linearly with the number of jumps of the process. When such methods become computationally intractable, approximate methods, which introduce a bias, can become advantageous. In this paper, we provide a general framework for understanding the weak error, or bias, induced by different numerical approximation techniques in the current setting. The analysis takes into account both the natural scalings within a given system and the step size of the numerical method. Examples are provided to demonstrate the main analytical results as well as the reduction in computational complexity achieved by the approximate methods.",2012,10.1137/110849699,no
Improved moraine age interpretations through explicit matching of geomorphic process models to cosmogenic nuclide measurements from single landforms,"The statistical distributions of cosmogenic nuclide measurements from moraine boulders contain previously unused information on moraine ages, and they help determine whether moraine degradation or inheritance is more important on individual moraines. Here, we present a method for extracting this information by fitting geomorphic process models to observed exposure ages from single moraines. We also apply this method to 94 Be-10 apparent exposure ages from 11 moraines reported in four published studies. Our models represent Be-10 accumulation in boulders that are exhumed over time by slope processes (moraine degradation), and the delivery of boulders with preexisting Be-10 inventories to moraines (inheritance). For now, we neglect boulder erosion and snow cover, which are likely second-order processes. Given a highly scattered data set, we establish which model yields the better fit to the data, and estimate the age of the moraine from the better model fit. The process represented by the better-fitting model is probably responsible for most of the scatter among the apparent ages. Our methods should help resolve controversies in exposure dating; we reexamine the conclusions from two published studies based on our model fits. (C) 2011 University of Washington. Published by Elsevier Inc. All rights reserved.",2012,10.1016/j.yqres.2011.12.002,no
A dual-process model of brand extension: Taxonomic feature-based and thematic relation-based similarity independently drive brand extension evaluation,"The success of a brand extension depends largely on the similarity between the brand and its extension product. Recent psychological and neuro-scientific evidence supports a dual-process model that distinguishes taxonomic feature-based similarity from thematic relation-based similarity. In addition to providing a parsimonious organizational framework for prior brand extension research, this dual-process model also provides novel predictions about the processing and evaluation of taxonomic brand extensions (e.g., Budweiser cola) and thematic brand extensions (e.g., Budweiser chips). Results indicate that taxonomic and thematic similarities independently contribute to branding professionals' and lay consumers' evaluations of real and hypothetical brand extensions (Studies 1A and 1B). Counter-intuitively, thematic brand extensions are processed more rapidly (Study 2), judged more novel, and evaluated more positively than taxonomic extensions (Study 3). When induced to consider the commonalities between the brand and the extension product, however, taxonomic extensions are judged more novel and evaluated more positively (Study 3). Implications for brand extension and marketing more generally are discussed. (C) 2011 Society for Consumer Psychology. Published by Elsevier Inc. All rights reserved.",2012,10.1016/j.jcps.2011.11.002,no
A distinct decision model for the evaluation and selection of a supplier for a chemical processing industry,"The supplier selection process has gained importance recently due to the considerable amount of revenue spent on purchasing. The intention of this work is to develop an appropriate hybrid model by integrating the analytical hierarchy process (AHP) and grey relational analysis (GRA) for supplier evaluation and selection, which comprises three stages. In Stage I, the most influential criteria are selected by mutual-information-based feature selection. Stage II focuses on the determination of the weights of the attributes using AHP, while Stage III is used for the determination of the best supplier using GRA. The proposed model is illustrated using the case study of an electroplating industry to highlight the effectiveness and flexibility of the model. The model effectively combines specialised knowledge, experience and quantitative data to select the best suppliers. This paper presents the model development, solution and application processes of the proposed hybrid model for supplier selection. The decision support software was implemented in Excel to automate supplier selection. The proposed hybrid model is applied to enhance the decision-making process in supplier selection and also helps decision makers to effectively select suppliers.",2012,10.1080/00207543.2011.624560,no
"New Process to Simultaneously Measure, Quantify, and Model Energy Efficient Performance","the term Energy Efficient Performance has been used for over a decade [examples [1], [2]) to describe high performance at optimized power levels. While power characterization methods and formal performance assessment techniques are well established and utilized (examples [3], [4]), no standard exists to quantify both metrics simultaneously as a means to establish energy efficient performance. As energy budgets continue to shrink and the expectation for increased performance remains, the need for a process that simultaneously models power versus performance tradeoffs becomes ever more critical. This paper describes a new process developed, prototyped, and implemented in Intel's client post-Si validation labs that is actively being used to measure, process, and model energy efficient performance. The paper will describe methods that simultaneously acquire power and performance measurements, post-process that power data into energy measurements, and model multi-dimensional energy vs. performance tradeoffs. It will also demonstrate how the process is actively being applied to isolate negative energy efficient performance outliers for debug and to tune for best energy efficient performance across multiple product segments.",2012,10.1109/MTV.2012.13,no
The Measurement of Thickness and Model Prediction of Hot Asphalt Mixture in the Rolling Compacting Process,"The thickness test and model prediction methods and rheological deformation theory have an important impact on quality control in the process of hot asphalt mixture field rolling compaction. DSL30 leveling instrument are used to measure the thickness in the process of hot asphalt mixture. Pt100 and Paperless Recorder are joined to test the temperature field and results of compaction thickness and temperature changes are obtained, and the model is established for mixture rheological deformation theory and compaction thickness prediction. The results shows that: by using the thickness and temperature field test methods, relevant data of hot asphalt mixture can be gained accurately, viscoplastic rheological theory correctly reflects its deformation behavior and the thickness forecast model can efficiently show the mixture layer. In this way, new methods are provided for the asphalt pavement quality control, rheological theory and temperature fields testing. The research on early damage to the asphalt pavement shows that damage is largely due to insufficient compaction. If the asphalt layer compaction has an undesirable effect i.e. high air voids and easily retained water in the pores lead to progressive asphalt membrane and aggregate adhesion loss. It could also lead to loose, pits and other water damage, and results in low strength, poor durability and easy generation of rutting [1]. In the process of mixing hot asphalt pavement construction, transportation, pavement and compaction, compaction of asphalt mixture form pavement is the final process of pavement formation. In case of insufficient compaction and improper process, fine road performance is hard to acquired despite of excellent design and high-quality materials [2].Therefore, asphalt pavement compaction can be taken as the most important construction process of asphalt mixture. And the field compaction thickness not only affects the pavement compaction quality control (degree of compaction), but also conerns the pavement quality acceptance (thickness). Therefore, the hot asphalt mixture scene rolling thickness control and method are rather important. Y. Lu applied finite element numerical simulation to study the asphalt mixture viscoelasticplastic mechanical deformation properties, and the application of pavement performance evaluation and permanent deformation of asphalt layer are successful obtained [3]. Khaled Anwar Kandil studied the effects of gradation characteristic and material properties on the asphalt mixture compaction by indoor and field tests, and the established two compaction model of indoor and outdoor to the evaluate compaction quality[4]. F.A. Van Vught analyzed asphalt mixture compaction; compaction thickness limits theory of test section and the compaction finite element numerical simulation [5]. G.D. Airey's research showed that the proportion of aggregate grade of asphalt mixture compaction has a greater impact [6]. He studied hot asphalt mixture viscoelastic plastic constitutive model and numerical simulation of pavement vibration compaction [7-9]. Thus, gradations of asphalt mixture, compaction, quality control method and theory have significant impacts on asphalt mixture compaction properties. Therefore, this paper combines the construction of hot asphalt mixture pavement compaction process, adopts DSL30 level test compaction thickness variation and Pt100 paperless recorder test temperature field, and analyzes asphalt mixture theory and test results, establishes compaction thickness prediction method. To carry out hot asphalt mixture field compaction thickness testing method, deformation theory and the prediction model of other aspects, the research content is the important part of the fund project.",2012,10.4028/www.scientific.net/AMM.204-208.1706,no
Thermo-kinetic and Physico-Chemical Modeling of Processes Generating Scaling Problems in Phosphoric Acid and Fertilizers Production Industries,"The wet-process phosphoric acid (WPPA) production is generally based on the use of sulfuric acid (H2SO4) for phosphate ore attack. One of the main issues encountered in this process is related to the uncontrolled formation of mineral deposits at sensitive steps. In the generated complex aqueous systems, the dissolved species and the resulting physicochemical interactions are closely dependent on the variability of both the phosphate rock quality and the operating conditions of the process (40-95 degrees C; H3PO4 and H2SO4 concentrations up to 20 mol/kg H2O, typically). A collaborative project (CA2PHOS) between OCP and BRGM aims to develop a computational tool dedicated to predict and quantify these mineral deposits in the context of WPPA production. The integrated work presented allows one to gain insight into the understanding of the complex behavior of such systems. It involves specific data such as the activity of water, measured by the hygroscopic method, the density and the chemical composition of the very highly saline aqueous solutions of interest, characteristics of the deposited minerals, and the use of the Pitzer formalism for the thermodynamic modeling of such complex system. (C) 2012 The Authors. Published by Elsevier Ltd. Selection and/or peer-review under responsibility of the Scientific Committee of SYMPHOS 2011",2012,10.1016/j.proeng.2012.09.447,no
Risk and Safety Program Performance Evaluation and Business Process Modeling,"There is increasing need for agencies to coordinate their interdependent risk assessment, risk management, and risk communication activities in compliance with risk program guidelines. In particular, there is a challenge to measure risk program compliance and maturity to guidelines such as the U.S. Office of Management and Budget (OMB) memorandum ""Updated Principles for Risk Analysis"" among others. This paper demonstrates a systemic approach to evaluate large-scale risk program maturity with utilization of business process modeling and self-assessment methods. This approach will be helpful to agencies implementing risk guidelines such as those of the OMB, the U.S. Government Accountability Office, the U.S. Department of Homeland Security, the U.S. Department of Defense, and others. This paper will be of interest to risk managers, agencies, and risk and safety analysts engaged in the conception, implementation, and evaluation of risk and safety programs.",2012,10.1109/TSMCA.2012.2199306,no
An In-Process Scaling Model: A Potential Framework for Data Monitoring Committees and Clinical Trial Quality Improvement,"This article discusses the application of a statistical process control methodology to improve the quality of real-time clinical trial monitoring, an especially valuable tool for data monitoring committees. This article outlines a method that may have value in bringing a data-based approach using the measures of site performance to define patterns of similarity or dissimilarity between the sites. These can then be used to provide an efficient and objective mechanism to identify those sites for closer scrutiny, monitoring, or further training, either at the end of the study or on an ongoing basis.",2012,10.1177/0092861511427864,no
Measuring and modeling absolute data for electron-induced processes,"This article draws a connecting line between a number of publications scattered in the literature with the aim of making some general conclusions. Emphasis is on dissociative electron attachment (DAE), since this process leads to chemical change, essential for many applications. Four domains of phenomena involved in DEA are identified: threshold or 'nonlocal' phenomena, the 'multidimensional' phenomena, the Feshbach resonance domain, and processes where scrambling of atoms occurs. The point is made that these four groups of phenomena are linked in real world, and that a unified theory describing all of them is required.",2012,10.1088/1742-6596/388/1/012001,no
"A process oriented characterization of tropical oceanic clouds for climate model evaluation, based on a statistical analysis of daytime A-train observations","This paper aims at characterizing how different key cloud properties (cloud fraction, cloud vertical distribution, cloud reflectance, a surrogate of the cloud optical depth) vary as a function of the others over the tropical oceans. The correlations between the different cloud properties are built from 2 years of collocated A-train observations (CALIPSO-GOCCP and MODIS) at a scale close to cloud processes; it results in a characterization of the physical processes in tropical clouds, that can be used to better understand cloud behaviors, and constitute a powerful tool to develop and evaluate cloud parameterizations in climate models. First, we examine a case study of shallow cumulus cloud observed simultaneously by the two sensors (CALIPSO, MODIS), and develop a methodology that allows to build global scale statistics by keeping the separation between clear and cloudy areas at the pixel level (250, 330 m). Then we build statistical instantaneous relationships between the cloud cover, the cloud vertical distribution and the cloud reflectance. The vertical cloud distribution indicates that the optically thin clouds (optical thickness < 1.5) dominate the boundary layer over the trade wind regions. Optically thick clouds (optical thickness > 3.4) are composed of high and mid-level clouds associated with deep convection along the ITCZ and SPCZ and over the warm pool, and by stratocumulus low level clouds located along the East coast of tropical oceans. The cloud properties are analyzed as a function of the large scale circulation regime. Optically thick high clouds are dominant in convective regions (CF > 80 %), while low level clouds with low optical thickness (< 3.5) are present in regimes of subsidence but in convective regimes as well, associated principally to low cloud fractions (CF < 50 %). A focus on low-level clouds allows us to quantify how the cloud optical depth increases with cloud top altitude and with cloud fraction.",2012,10.1007/s00382-012-1533-7,no
Analytical Hierarchy Process Model Applied to Port Urban Logistics Efficiency Commentary,"This paper aims to construct analysis model of port urban logistics arrangement using Delphi and analytical hierarchy process method, furthermore, establishment of fuzzy theory and analytical hierarchy process model and factor set. And calculate every index weight with the weighting method-G(1) based on differential principle and appraise comprehensive efficiency using fuzzy theory and analytical hierarchy process evaluation method. Using the average weighting method make quantitative disposal with indexes of appraisal factors and educes port urban logistics comprehensive efficiency outcome. And in so doing, validating Shanghai port urban logistics competitiveness actually which respects its efficiency status.",2012,10.4028/www.scientific.net/AMR.345.41,no
Optimal preventive maintenance strategy using two repair crews having different efficiencies: a quasi renewal process based modelling approach,"This paper considers randomly failing, single-unit equipment subject to a periodic preventive maintenance (PM) policy. In case of failure between successive perfect PM actions (renewals), imperfect repairs are performed following a decreasing quasi-renewal process. One of two different maintenance crews can perform the repairs. One team is more experienced, and consequently more efficient than the other, but more costly. A mathematical model is developed in order to determine the PM period, T, and the kth repair, during a PM period, after which the repair team should be changed, minimising the average total cost per time unit over an infinite time span. It is also proved that an optimal solution in terms of the PM period always exists for any given system lifetime distribution and any set of maintenance costs. Numerical examples are presented and the obtained results are discussed.",2012,10.1080/00207543.2012.671591,no
Modelling of piezoelectric actuators used in forging processes: principles and experimental validation,"This paper deals with the modelling of a piezoelectric stack actuator used to generate specific low frequency vibration waveforms to assist forging processes. Experimental results show that such waveforms reduce the necessary forging force during upsetting tests. The main problems which remain are defining the appropriate waveforms, predicting their influence on the process and the actuator and designing the control. Due to the complexity of the interactions between the different components of the system, a complete model of the process is needed. Such a model is developed here using an energetic macroscopic representation to preserve causality throughout the modelling. Simulation results are then compared to representative experimental results.",2012,,no
A Process Model to Guarantee Information Quality in Elective Surgery Information Systems,"This paper describes the system created by the Central Unit of National Waiting List for Surgery Management (UCGIC) to guarantee the quality of information extracted from National Health System (NHS) hospitals about elective surgery, covering the process of extraction, validation and qualification of data detail and indicators, to be carried out automatically by the information system that supports the waiting list for surgery management control - SIGLIC (Information System to Waiting List for Surgery Management). The need to build an appropriate process model has been growing since 2007. In 2010 the central database of SIGLIC had to receive data from nearly 164 hospital units, including public and private providers with conventions in the NHS for elective surgery, with a volume of nearly 1000GB and an annual increment approximately of 500GB. The data received was concerned to 881 different input variables and the volume of information transactions was nearly 5 million per year. Several problems concerning data quality in the central database started to arise once the data extraction and integration in SIGLIC involved the interface with several different hospital information systems and its volume started to increase fast. The solution found by UCGIC for addressing data quality and integrity received from hospital units was to build in 2011 a process model with automatic redundant system with different sources checking permanently data quality and interacting with all stakeholders involved with the same information sources. The model includes monthly data extractions submitted to a qualification process at the level of detail and indicators, against the defined standards and homologous variations, in order to provide accurate intelligence about national elective surgery. The validation process includes a management system of incidents, communications and escalation of problems, which reports the errors/incidents occurred, the communication to the stakeholder involved for correction and the escalation of the problem resolution if needed. The process management of data extraction and qualification is carried out through SIGLIC own screens/forms and reports. It has a dashboard and a procedure to ensure process control. There were also built scorecards with data aggregated by week. Studies have been conducted to assess the impacts and outcomes of this new approach. By this new model, UCGIC was able to assure that SIGLIC information is reliable and its performance indicators are correct and reflect the actual care provided to patients, the hospital performance according to care provided, the accurate evaluation of demand and supply in elective surgery and the necessary funding for the NHS. Also this automatic process is considerably less time and resource consuming, by saving nearly 5 days to a process which took 10 days and by allowing automatic reports of errors, turning its resolution with the hospital units more efficient and effective.",2012,,no
SIGNED DIRECTED GRAPH BASED MODELING AND ITS VALIDATION FROM PROCESS KNOWLEDGE AND PROCESS DATA,"This paper is concerned with the fusion of information from process data and process connectivity and its subsequent use in fault diagnosis and process hazard assessment. The Signed Directed Graph (SDG), as a graphical model for capturing process topology and connectivity to show the causal relationships between process variables by material and information paths, has been widely used in root cause and hazard propagation analysis. An SDG is usually built based on process knowledge as described by piping and instrumentation diagrams. This is a complex and experience-dependent task, and therefore the resulting SDG should be validated by process data before being used for analysis. This paper introduces two validation methods. One is based on cross-correlation analysis of process data with assumed time delays, while the other is based on transfer entropy, where the correlation coefficient between two variables or the information transfer from one variable to another can be computed to validate the corresponding paths in SDGs. In addition to this, the relationship captured by data-based methods should also be validated by process knowledge to confirm its causality. This knowledge can be realized by checking the reachability or the influence of one variable on another based on the corresponding SDG which is the basis of causality. A case study of an industrial process is presented to illustrate the application of the proposed methods.",2012,10.2478/v10006-012-0003-z,no
IMPROVING THE QUALITY OF PROCESSES BY DRAWING CAUSE-EFFECT DIAGRAM USING SOFTWARE INSTRUMENTS,"This paper observes the modality of building the cause - effect diagram, by using software instruments. To make an example, it was used the program STATISTICS 7, taking into account a case study regarding the identification of causes that brought at the analysed effect: exceeding the prescribed tolerance. Using software instruments helps us at a faster and efficient tracing of the cause - effect diagram.",2012,,no
"Solving a group layout design model of a dynamic cellular manufacturing system with alternative process routings, lot splitting and flexible reconfiguration by simulated annealing","This paper presents a novel mixed-integer non-linear programming model for the layout design of a dynamic cellular manufacturing system (DCMS). In a dynamic environment, the product mix and part demands are varying during a multi-period planning horizon. As a result, the best cell configuration for one period may not be efficient for successive periods, and thus it necessitates reconfigurations. Three major and interrelated decisions are involved in the design of a CMS; namely cell formation (CF), group layout (GL) and group scheduling (GS). A novel aspect of this model is concurrently making the CF and GL decisions in a dynamic environment. The proposed model integrating the CF and GL decisions can be used by researchers and practitioners to design GL in practical and dynamic cell formation problems. Another compromising aspect of this model is the utilization of multi-rows layout to locate machines in the cells configured with flexible shapes. Such a DCMS model with an extensive coverage of important manufacturing features has not been proposed before and incorporates several design features including alternate process routings, operation sequence, processing time, production volume of parts, purchasing machine, duplicate machines, machine capacity, lot splitting, intra-cell layout, inter-cell layout, multi-rows layout of equal area facilities and flexible reconfiguration. The objective of the integrated model is to minimize the total costs of intra and inter-cell material handling, machine relocation, purchasing new machines, machine overhead and machine processing. Linearization procedures are used to transform the presented non-linear programming model into a linearized formulation. Two numerical examples taken from the literature are solved by the Lingo software using a branch-and-bound method to illustrate the performance of this model. An efficient simulated annealing (SA) algorithm with elaborately designed solution representation and neighborhood generation is extended to solve the proposed model because of its NP-hardness. It is then tested using several problems with different sizes and settings to verify the computational efficiency of the developed algorithm in comparison with the Lingo software. The obtained results show that the proposed SA is able to find the near-optimal solutions in computational time, approximately 100 times less than Lingo. Also, the computational results show that the proposed model to some extent overcomes common disadvantages in the existing dynamic cell formation models that have not yet considered layout problems. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.cor.2012.01.012,no
Discrete element modelling and experimental validation for the falling process of dry granular steps,"This paper presents an experimental validation of discrete element method (DEM) simulation for falling process of dry granular steps. Three chute slopes were considered in the study. The corresponding physical experiments were carried out in our previous study. The DEM simulations matched well with the experimental results regarding the external flow characteristics such as flow regimes, surface profiles, final deposit angles, velocity profiles at the sidewall, receding upper granular surfaces and flow rates. Subsequently, the DEM simulations were used to explore the internal flow patterns of the granular collapse, including translational velocity profiles and angular velocity profiles inside the granular assembly. According to the DEM simulation results, a mixed velocity profile (an upper convex and lower concave profile) appears in the central part of the chute, whilst the velocity profile follows a sidewall-stabilized heap (SSH) rheology (a concave profile) near the sidewall. The chute inclination facilitates the transformation from the SSH rheology to the mixed velocity profile in a narrow channel. The DEM simulations not only verify the experimental observation but also enhance the understanding of flow patterns induced by the collapse of dry granular steps. (C) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.powtec.2012.08.001,no
Application and comparison of level control strategies in the slug flow problem using a mathematical model of the process,"This paper presents the application of the error-squared level control strategy Proportional Integral (PI) in the slug flow problem in oil production industry. For this purpose the dynamic model has been used for a pipeline-separator under slug flow with five Ordinary Differential Equations (ODEs), coupled, non-linear. The application of the error-squared level control strategy PI is performed through the methodology by bands, whose objective is to damp the oscillatory flow rate of the load that occur in production separators for equipment downstream of the process. This strategy is compared with the level control strategy PI conventional, widely used in industrial processes; and with the level control strategy PI, also, in the methodology by bands. Simulation results showed that the error-squared level control strategy PI in the methodology by bands, presented better results when compared with the level control strategy PI conventional, because reduced flow fluctuations caused by slug flow; and with the level control strategy PI in the methodology by bands, it probably happened because the first has highly respected the defined bands.",2012,10.4025/actascitechnol.v34i4.13020,no
Evaluating the efficiency of municipalities in collecting and processing municipal solid waste: A shared input DEA-model,"This paper proposed an adjusted ""shared-input version of the popular efficiency measurement technique Data Envelopment Analysis (DEA) that enables evaluating municipality waste collection and processing performances in settings in which one input (waste costs) is shared among treatment efforts of multiple municipal solid waste fractions. The main advantage of this version of DEA is that it not only provides an estimate of the municipalities overall cost efficiency but also estimates of the municipalities' cost efficiency in the treatment of the different fractions of municipal solid waste (MSW). To illustrate the practical usefulness of the shared input DEA-model, we apply the model to data on 293 municipalities in Flanders, Belgium, for the year 2008. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.wasman.2012.05.021,no
Evaluation of model-based control strategy based on generated setpoint schedules for NH4-N removal in a pilot-scale A(2)/O process,"This paper proposes a model-based control strategy that can predict the influent and effluent as well as control the effluent water quality in the A(2)/O (Anaerobic/Anoxic/Oxic) process for 1 day in advance. In the model-based control strategy, ANN (Artificial neural network) and modified ASM3+Bio-P model were used to predict the influent and effluent for 1 day in advance, respectively. When the predicted effluent NH4-N concentration was higher than the target value, the optimal setpoint schedules of the DO (Dissolved oxygen) could be deduced using a scenario simulation. The scenario simulation was carried out to obtain DO setpoint schedules to reach the effluent NH4-N concentration under the target value. The deduced optimal setpoint schedules were used to control the operation of the process for the next day. The results of model prediction showed that the behavior of the influent and effluent could be predicted successfully. The proposed model-based control strategy was tested in a pilot-scale A(2)/O process for 2 weeks, which confirmed that the effluent NH4-N concentration could be maintained steadily lower than the target value of 5 mg/L. The air flow rate during control period was increased by 9% of that during without control period. On the other hand, their corresponding average effluent NH4-N concentrations were 4.42 and 13.89 mg/L, respectively, which highlight the significant effect of this control strategy with only a slight increase in air flow rate. These results show that the developed model-based control strategy can be used successfully in the A(2)/O process and possibly other processes. (C) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.cej.2012.07.067,no
Technical Evaluation of the Electric Energy Production Process within a Distributed Generation Model for Gyms in Colombia,"This paper proposes a new model for electric energy production, using physical workout that is done daily in spinning bicycles and elliptical trainers in gyms. Two gyms were visited and the kinetic energy generated for the machines was evaluated in order to determine the most efficient machine and the amount of energy the machine can produce. Having the machine design, the next step is to simulate with the defined parameters, getting as a result the generated energy vs. consumed energy curve. Finally, a feasibility analysis is made using the free software Retscreen International V.4.0. This way, a model is proposed for energy generation using a group of those machines, in order to reach a possible implementation on Colombian gyms.",2012,,no
Unresolved Measurement Processing with Widely Separated Radars using Sparse Modeling,"This paper proposes sparse modeling for the processing of possibly unresolved measurements across multiple sensors. The sparse vector is created by discretizing the state space into a grid, then representing the received signal as a linear sum of the response due to reflections originating at those grid points. Our sensors measure the observed SNR at the output of the matched filter. These samples of the matched filter will represent a specific volume in Euclidian space corresponding with range bins. Hence, this work considers noncoherent processing, thus the phase information in each range bin is not used during processing. Target cross sections are as aspect-dependent, and widely separated sensors are considered, so the returns from different sensors are considered to be uncorrelated. Using our sparse model, these assumptions lead to several minimization problems that are related by their sparsity patterns. We demonstrate that by using our modeling technique, targets that may be unresolved in each sensor's measurement space may be resolvable by combining the spacial information from multiple sensors.",2012,,no
"Falling film melt crystallization (I): Model development, experimental validation of crystal layer growth and impurity distribution process","This paper was concerned with the model development and experimental validation of the detailed crystal layer growth and multi-ions impurity distribution process in the falling film melt crystallization (FFMC) model. The phosphoric acid (PA) was separated and purified by FFMC to obtain a hyperpure phosphoric acid (HPA), which was a vital electronic chemical in IT industry. To establish a valid model, which offered an easy and convenient path of the simulation, dynamic heat and mass balance, approaches were adopted to describe the variation of crystal layer growth rate along the crystallizer. An impurity balance approach was adopted to describe the change of distribution coefficient for multi-ion impurity. A criterion was proposed to determine the formation of branched-porous (B-P) structure. The model was validated by experimental results with various equipments and operational conditions and a good agreement was obtained. The effective distribution coefficient K-eff for multi-ion impurities were less than 0.2 (Na+), 0.25 (Fe3+) and 0.35 (Ca2+) with proper operation conditions. The resulting model was directly exploited to understand crystal layer growth and impurity distribution behaviors in FFMC from laboratory to industrial scale. More significantly, the model proposed a method for the separation effect evaluation and the key operational conditions (feed rate and cooling rate) determination which could readily develop optimal crystal layer growth route during industrial crystallization. In addition, the model was a vital base to describe the subsequent purification step of FFMC: sweating process. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.ces.2012.08.026,no
Agent based evaluation of dynamic city models A combination of human decision processes and an emission model for transportation based on acceleration and instantaneous speed,"This project presents a simulation tool to evaluate procedurally generated 3D city models with a set of agents representing pedestrians, the environment and urban street actors towards greenhouse gas emission from transportation. This empiric tool for architects and urban planners analyses, predicts and quantifies traffic fluctuations over time, and define the number of pedestrians, individual traffic and public transport in each area and street of a city. Examples show that the allocation of functions within a city contributes to the appearance of traffic congestion and therefore emissions. This tool simulates the decisions and returns information about the path occupants take and their individual experiences such as stress, effort and deviations. This allows planners to evaluate their design before implementation in an empirical way. (C) 2011 Elsevier B.V. All rights reserved.",2012,10.1016/j.autcon.2011.07.001,no
The validation of knowledge construction model based on constructivist approach to support ILL-structured problems solving process for industrial education and technology students,"This research aimed to study the internal validation and the external validation of the knowledge construction model based on constructivist approach to support process in solving ill-structured problems. The samples were 42 fourth-year undergraduate students from the Department of Educational Communications and Technology, King Mongkut's University of Technology Thonburi. There were 3 experts to verify the quality of internal validation of the model. The research design was based on Type II developmental research. The result from the internal validation of the model showed that the design of components inside the model was in accordance with the principles and theories adopted in the design. The study of the external validation of the model showed that the learners followed steps of the process in solving ill-structured problems in accordance with Jonassen's principle ( 1997) in solving ill-structured problems. The learning achievement of students passed the criteria of 80 percent. The students' opinion showed that the learning contents, the media on the network and the design were suitable for supporting knowledge construction and process in solving ill-structured problems.",2012,10.1016/j.sbspro.2012.06.399,no
"A lattice Boltzmann-based investigation of powder in-flight characteristics during APS process, part I: modelling and validation","This study aims to investigate turbulent plasma flow over spheroidal particles using the lattice Boltzmann (LB) method. A double population model D2Q9-D2Q4 is employed to calculate the plasma velocity and temperature fields. Along with the calculation process a conversion procedure is made between the LB and the physical unit systems, so that thermo-physical properties variation is fully accounted for and the convergence is checked in physical space. The configuration domain and the boundary condition treatment are selected based on the most cited studies in order to illustrate a realistic situation. The jet morphology analysis gives credible results by comparison with commonly published works. A second Lagrangian model has been developed to investigate the plasma-particles exchange during its in-flight. The tracking of the mu-sized particles allows concluding that our results are in sufficient agreement with those of the Jets&Poudres code and that the LB method account well for plasma jet physics which affects directly the particles in-flights.",2012,10.1504/PCFD.2012.048250,no
Discourse production after right brain damage: Gaining a comprehensive picture using a multi-level processing model,"This study examined the effects of right brain-damage (RBD) on oral discourse production using a multi-layered discourse processing model. Narrative and procedural discourse samples from participants with RBD and no brain damage were analysed in terms of seven broad areas corresponding to the processing levels of the model. Participants also completed attentior, cognitive, general communication and RBD assessments. Despite their normal performance on all assessments (except those on attention), the participants with RBD demonstrated statistically significant differences in syntactic complexity, clarity disruptors and dysfluencies, as well as in discourse grammar and clausal structure in the narratives and in cohesion in the procedures. A model-based theoretical explanation accounting for the deficits noted in participants with RBD, together with clinical guidelines, is provided. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.jneuroling.2012.01.001,no
Model Predictive Quality Control of Batch Processes,"This work addresses the problem of driving a batch process to a specified product quality using model predictive control (MPC) with data-driven models. To address the problem of unavailability of online quality measurements, an inferential quality model, which relates the process conditions over the entire batch duration to the final quality, is required for this problem. At a given sampling instant, the accuracy of this type of quality model, however, is sensitive to the prediction of the future (unknown) batch behavior. That is, errors in the predicted future data are propagated to the quality prediction, adding uncertainty to any control action based on the predicted quality. To address this ""missing data"" problem, we integrate a previously developed data-driven modeling methodology, which combines multiple local linear models with an appropriate weighting function to describe nonlinearities, with the inferential model in a predictive control framework. The key benefit of this approach is that the causality and nonlinear relationship between the future inputs and outputs are accounted for in predicting the final quality, resulting in more effective control action. The efficacy of the proposed predictive control design is illustrated via closed-loop simulations of an industrially relevant nylon-6,6 batch polymerization process.",2012,,no
Quality parameters assessment in kiwi jam during pasteurization. Modelling and optimization of the thermal process,"This work focuses on the optimization of the pasteurization process in kiwi jams, considering the influence of the containers size on the quality parameters through experimental measurements and kinetic models. A numerical finite element model was developed with the purpose of simulating the energy transfer during the retort thermal processing of the product. The temperatures predicted by the simulations were successfully validated against experimental data (average relative differences <5%). These temperatures were incorporated into the kinetic model that described the quality variations which corresponded well with the texture and colour variations experimentally measured. As a result the validated numerical model was used to design and evaluate equivalent thermal processes which allowed determine optimal operating conditions. These results could contribute to the optimization of thermal processing of kiwi jam in order to minimize quality losses, such as texture, colour and nutritional value. (C) 2012 The Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.",2012,10.1016/j.fbp.2012.03.001,no
Estimation of Kinetic Parameters and Mathematic Model Validation for Nylon-6 Process,"This work presents the simulation of the hydrolytic polymerization process of nylon-6 in a lab-scale semi-batch reactor, using epsilon-caprolactam as monomer and acetic acid as monofunctional acid chain terminator. The kinetic scheme comprises 6 reactions: 3 main reactions, 2 side reactions associated with the cyclic dimer formation and one monofunctional acid termination. Operating conditions were obtained from previous definitions and kinetic parameters were estimated from experimental data. The proposed optimization problem to estimate the kinetic parameters was solved by the Successive Quadratic Programming (SQP), a deterministic method. It was shown that the method is able to determine the final solution with good precision. The validity of the model was confirmed by comparison of the results obtained by computer simulation using the software Aspen Polymer Plus (R) and the process real data.",2012,,no
Evaluation of Regenerative Processes in a Rat Model of Mandibular Condyle Defect using in vivo Micro X-Ray Computed Tomography,"To determine whether the periosteum influences mandibular head regeneration after condylectomy in rats. male Wistar rats, the periosteum of part of the mandibular head was preserved or removed and the mandibular head was excised up to a section of the neck of the mandible. Radiological and histological findings were evaluated 0, 1, 2, 3, 4, 6, and 8 weeks later. Although bone regeneration was observed during the first postoperative week in both groups, restoration occurred from the bone stump and from a position distant from the bone stump when the periosteum was conserved. When the periosteum was removed, restoration occurred from the bone stump only. Regenerated bone mass was greater with the preserved periosteum. Bone mass increased until the sixth week after condylectomy in the periosteum preservation group. Our results confirm the importance of the periosteum in mandibular head regeneration after condylectomy in rats.",2012,,no
GPS Data Processing Models for Maritime Dynamic Precision Evaluation of Shipboard Tracking Equipments,"To get a better understanding of tracking capability and accuracy of the equipments onboard space tracking ships, it is necessary to perform precision evaluation. In order to achieve satisfactory results, dynamic flight experiment is organized, during which a series of new methods and technologies are applied, with GPS carrier wave phase difference technology as the center, among which GPS data processing technology is empathized. Based on above, a set of complete GPS data processing models for precision evaluation are established",2012,10.4028/www.scientific.net/AMR.466-467.1070,no
Challenges of a Validation Process Based on Models: An Industrial Case Study,"To those familiar with network modeling and its advantages across different phases of system development, the thought of generating test cases based on informal specifications would sound unreal. However, manual test generation is still a current practice in industry. Given that the use of models provides many distinct advantages, why is it not more widespread in the validation phase? This paper presents a case study of a network protocol validation. We present a process for formalizing system specifications and applying a model-based approach for test generation, and consider how AGATHA, a tool developed by an academic community, can encourage industrial interest in efficient practices such as model-based testing (MBT). The drawbacks and advantages observed during our experiments are discussed. (c) 2012 Alcatel-Lucent.",2012,10.1002/bltj.21533,no
Field measurement and modeling of near-bed sediment transport processes with fluid mud layer in Tokyo Bay,"Tokyo Bay is one of the estuaries in Japan with a high population of almost 26 million people in the basin area. One of the major concerns for the environment in this water area is the decreasing ecosystem functions including the deterioration of water and sediment qualities caused by various anthropogenic activities. Since the bottom sediments around almost the entire area of the inner bay consist of fine materials with a high organic content, which cause the deterioration of water quality through processes such as hypoxia, an understanding of the fine sediment dynamics in the Bay is crucial for an environmental assessment of the water area. This paper proposes a model for the key processes of fine sediment dynamics, which reflects field data about muddy bed structures and their dynamics obtained during the monitoring campaign in 2007. One of the specific features of the sediment in the Bay at present is the persistent existence of fluid mud layers (water content over 300 %) with a thickness of around a few decimeters, which might be caused by deposition of abundant organic particles due to eutrophication. The present study shows that diffusion flux model delivers quite reliable results for estimating erosion flux from the top of fluid mud layers after calibrating the model parameter against the time series data of vertical flux measured by an acoustic Doppler velocimeter system. This study also derives analytical solutions, based on the Bingham fluid concept, of advection flux in the fluid mud layer on which external shear stress force is applied.",2012,10.1007/s10236-012-0570-4,no
Milling force Modeling of Formed Milling Cutter for Turnout Processing and Experiment Validation,"Turnout is one of the key components of high-speed railway, which directly affects the maximum lateral allowable speed. The formed milling cutters are usually used in the processing of turnout, and the milling force is an active factor in the milling process which affects the functions of the machine tool and the enactment of process parameters, then the processing quality of turnouts is affected. In the paper, the theoretical milling force model for formed milling cutter is built based on the theory of metal cutting, and the working profile of curved switch rail is chosen as processing object in the milling process experiments, the validity of the milling force model is verified, The study results will provide theoretical basis for tool machine design and process parameters selection, and ensuring the processing quality of turnout.",2012,10.4028/www.scientific.net/AMR.538-541.921,no
A graph grammar-based formal validation of object-process diagrams,"Two basic requirements from a system's conceptual model are correctness and comprehensibility. Most modeling methodologies satisfy only one of these apparently contradicting requirements, usually comprehensibility, leaving aside problems of correctness and ambiguousness that are associated with expressiveness. Some formal modeling languages do exist, but in these languages a complete model of a complex system is fairly complicated to understand. Object-process methodology (OPM) is a holistic systems modeling methodology that combines the two major aspects of a system-structure and behavior-in one model, providing mechanisms to manage the complexity of the model using refinement-abstraction operations, which divide a complex system into many interconnected diagrams. Although the basic syntax and semantics of an OPM model are defined, they are incomplete and leave room for incorrect or ambiguous models. This work advances the formal definition of OPM by providing a graph grammar for creating and checking OPM diagrams. The grammar provides a validation methodology of the semantic and syntactic correctness of a single object-process diagram.",2012,10.1007/s10270-011-0201-4,no
Understanding the behaviour of gas in a geological disposal facility: modelling coupled processes and key features at different scales,"Understanding the behaviour of gas in a geological disposal facility (GDF) is an essential component of analysing the facility evolution and long-term (post-closure) safety performance. This includes the impacts of gas on the physico-chemical evolution of the GDF, and the release and migration of radionuclides in water and gas. The Nuclear Decommissioning Authority Radioactive Waste Management Directorate is participating in the EC FORGE (fate of repository gases) project (www.forgeproject.org) and conducting independent research. Key research themes are modelling the impacts of different host rocks on facility evolution including coupled processes, and upscaling the effects of small scale features that can significantly influence the evolution of the whole facility. Recent code developments have enabled coupled processes to be represented more realistically in models. This has significantly advanced understanding of facility evolution, as discussed in this paper, and will improve future assessment models. There is potential to further improve approaches to upscaling the effects of small scale features on strongly coupled processes, within the context of the EC FORGE project.",2012,10.1180/minmag.2012.076.8.49,no
Simulating urban growth processes incorporating a potential model with spatial metrics,"Urbanization is one phenomena that drives land use pattern change. Persistent rapid urbanization is associated with depletion of natural resources and worsening conditions in the urban environment. Monitoring urban development is, therefore, an absolute necessity in order to assure sustainable cities in the future. The main objective of this paper is to develop and apply an urban growth potential model incorporating spatial metrics. The model has been tested in Jinan City, China. Firstly, two satellite images (1989 and 2004 SPOT) were used to extract the land-cover. A general land use spatial pattern analysis, based on landscape metrics and a transformation matrix analysis, was conducted. Secondly, a moving window method was used to identify and capture the urbanization process through the PLAND landscape metric. The remote satellite data have been further processed: first to produce an initial state of the land-cover surface, and second to perform a time-series analysis and to assess the potential accuracy of the model application. In the second step, the calibrated model was used to predict the location of the urban growth over 16 years (2004-2020). The results indicated there will be a significant land use change until 2020. However, the spatial distribution of the potential growth areas is not homogenous. The study has confirmed the usefulness of a growth potential model incorporating the moving window method to predict urban growth trends and examining the impacts of urban development on natural resources. The results can provide decision support documents for urban planners and stakeholders with spatially explicit information for future planning and monitoring plans. (C) 2012 Elsevier Ltd. All rights reserved.",2012,10.1016/j.ecolind.2012.02.003,no
"A bi-objective possibilistic programming model for open shop scheduling problems with sequence-dependent setup times, fuzzy processing times, and fuzzy due dates","We are concerned with an open shop scheduling problem having sequence-dependent setup times. A novel bi-objective possibilistic mixed-integer linear programming model is presented. Sequence-dependent setup times, fuzzy processing times and fuzzy due dates with triangular possibility distributions are the main constraints of this model. An open shop scheduling problem with these considerations is close to the real production scheduling conditions. The objective functions are to minimize total weighted tardiness and total weighted completion times. To solve small-sized instances for Pareto-optimal solutions, an interactive fuzzy multi-objective decision making (FMODM) approach, called TH method proposed by Torabi and Hassini, is applied. Using this method, an equivalent auxiliary single-objective crisp model is obtained and solved optimally by the Lingo software. For medium to large size examples, a multi-objective particle swarm optimization (MOPSO) algorithm is proposed. This algorithm consists of a decoding procedure using a permutation list to reduce the search area in the solution space. Also, a local search algorithm is applied to generate good initial particle positions. Finally, to evaluate the effectiveness of the MOPSO algorithm, the results are compared with the ones obtained by the well-known SPEA-II, using design of experiments (DOE) based on some performance metrics. (c) 2011 Elsevier B.V. All rights reserved.",2012,10.1016/j.asoc.2011.11.019,no
Use of a process-based model for assessing the methane budgets of global terrestrial ecosystems and evaluation of uncertainty,"We assessed the global terrestrial budget of methane (CH4) by using a process-based biogeochemical model (VISIT) and inventory data for components of the budget that were not included in the model. Emissions from wetlands, paddy fields, biomass burning, and plants, as well as oxidative consumption by upland soils, were simulated by the model. Emissions from ruminant livestock and termites were evaluated by using an inventory approach. These CH4 flows were estimated for each of the model's 0.5 degrees x 0.5 degrees grid cells from 1901 to 2009, while accounting for atmospheric composition, meteorological factors, and land-use changes. Estimation uncertainties were examined through ensemble simulations using different parameterization schemes and input data (e.g., different wetland maps and emission factors). From 1996 to 2005, the average global terrestrial CH4 budget was estimated on the basis of 1152 simulations, and terrestrial ecosystems were found to be a net source of 308.3 +/- 20.7 Tg CH4 yr(-1). Wetland and livestock ruminant emissions were the primary sources. The results of our simulations indicate that sources and sinks are distributed highly heterogeneously over the Earth's land surface. Seasonal and interannual variability in the terrestrial budget was also assessed. The trend of increasing net emission from terrestrial sources and its relationship with temperature variability imply that terrestrial CH4 feedbacks will play an increasingly important role as a result of future climatic change.",2012,10.5194/bg-9-759-2012,no
Economic Production Quantity Model with Imperfect Quality during a Process Adjustment Period,"We consider a manufacturing process that generates non-conforming items until proper adjustment of the process is reached. Items produced after machine adjustment are assume perfect. The demand rate is assumed constant. The process stops when the production of conforming items is sufficient to cover the demand, then the cycle is repeated perpetually. Mathematical models for deterministic and random machine adjusting period are proposed. We find the optimal production quantity that results in minimum expected total cost. Two examples are presented. We also show that the optimal production size increases as the adjustment period increases, then at some value, it becomes constant.",2012,,no
"Snow Metamorphism and Albedo Process (SMAP) model for climate studies: Model validation using meteorological and snow impurity data measured at Sapporo, Japan","We developed a multilayered physical snowpack model named Snow Metamorphism and Albedo Process (SMAP), which is intended to be incorporated into general circulation models for climate simulations. To simulate realistic physical states of snowpack, SMAP incorporates a state-of-the-art physically based snow albedo model, which calculates snow albedo and solar heating profile in snowpack considering effects of snow grain size and snow impurities explicitly. We evaluated the performance of SMAP with meteorological and snow impurities (black carbon and dust) input data measured at Sapporo, Japan during two winters: 2007-2008 and 2008-2009, and found SMAP successfully reproduced all observed variations of physical properties of snowpack for both winters. We have thus confirmed that SMAP is suitable for climate simulations. With SMAP, we also investigated the effects of snow impurities on snowmelt at Sapporo during the two winters. We found that snowpack durations at Sapporo were shortened by 19 days during the 2007-2008 winter and by 16 days during the 2008-2009 winter due to radiative forcings caused by snow impurities. The estimated radiative forcings due to snow impurities during the accumulation periods were 3.7 W/m(2) (it corresponds to albedo reduction in 0.05) and 3.2 W/m(2) (albedo reduction in 0.05) for the 2007-2008 and 2008-2009 winters, respectively. While during the ablation periods they were 25.9 W/m(2) (albedo reduction in 0.18) and 21.0 W/m(2) (albedo reduction in 0.17) for each winter, respectively.",2012,10.1029/2011JF002239,no
Evaluation of Models for Predicting Spray Mist Diameter for Scaling-Up of the Fluidized Bed Granulation Process,"We evaluated models for predicting spray mist diameter suitable for scaling-up the fluidized bed granulation process. By precise selection of experimental conditions, we were able to identify a suitable prediction model that considers changes in binder solution, nozzle dimension, and spray conditions. We used hydroxypropyl cellulose (HPC), hydroxypropyl methylcellulose (HPMC), or polyvinylpyrrolidone (PVP) binder solutions, which are commonly employed by the pharmaceutical industry. Nozzle dimension and spray conditions for oral dosing were carefully selected to reflect manufacturing and small (1/10) scale process conditions. We were able to demonstrate that the prediction model proposed by Mulhem optimally estimated spray mist diameter when each coefficient was modified. Moreover, we developed a simple scale-up rule to produce the same spray mist diameter at different process scales. We confirmed that the Rosin-Rammler distribution could be applied to this process, and that its distribution coefficient was 1.43-1.72 regardless of binder solution, spray condition, or nozzle dimension.",2012,,no
Effects of process and/or observation errors on the stock-recruitment curve and the validity of the proportional model as a stock-recruitment relationship,"We examined the effects of process and observation errors on the selection of the stock-recruitment relationship (SRR) curve using simulations. When the process and observation errors were added to both spawning stock biomass and recruitment, the results were as follows: (1) When the proportional model was set as the true SRR model, there was a high probability that the Ricker or Beverton and Holt model was selected in response to the errors; (2) When the Ricker or Beverton and Holt model was set as the true SRR model, the proportional model was seldom selected in response to the errors; (3) The proportional SRR model should be accepted as the optimum SRR model for the Pacific stock of Japanese sardine; (4) We should use an SRR model that is constructed from more than two independent variables (spawning stock biomass, environmental factors, etc.) when we discuss management of fisheries resources.",2012,10.1007/s12562-011-0438-4,no
Group Think Decision Making Deficiency in the Requirements Engineering Process: Towards a Crowdsourcing Model,"We make an argument that requirements engineering, as a primer to information technology deployment in organizations, is largely failing because decision making in the requirements engineering (RE) process empathizes with the ethos of 'group think', an inefficient decision making model. We make this argument from the premise that technological artifacts that get inscribed within organizational culture and design do so through an initial process that involves prioritization, which in itself is theorized as an outcome of organizational discourse. We draw on critical realism's retroductive approach and the practice of 'group think' for grounding crowdsourcing in IT project prioritization. Our choice of crowdsourcing as a remedy for inefficiencies in IT Project prioritization is deliberate: it elevates the notion of group think as a theoretical lens; and is in line with the positioning of organizational decision making as deliberately constitutive and representative of stakeholder interests. However, we remain attentive guard against utopianism, but still make the claim that crowdsourcing provides us with some traction for seeking alternatives that can lead to a re-invigoration in RE research for effective practices in IT projects. The analyses links RE decision making deficiency to the prevalence of Organizational Resistance, Technical Incompetence, Lack of a Moral and Ethical Code and Inappropriate Leadership for Creativity.",2012,,no
Gaussian process modeling for measurement and verification of building energy savings,"We present a Gaussian process (GP) modeling framework to determine energy savings and uncertainty levels in measurement and verification (M&V) practices. Existing M&V guidelines provide savings calculation procedures based on linear regression techniques that are limited in their predictive and uncertainty estimation capabilities. We demonstrate that, unlike linear regression, GP models can capture complex nonlinear and multivariable interactions as well as multiresolution trends of energy behavior. In addition, because GP models are developed under a Bayesian setting, they can capture different sources of uncertainty in a more systematic way. We demonstrate that these capabilities can ultimately lead to significantly less expensive M&V practices. We illustrate the developments using simulated and real data settings. (c) 2012 Elsevier B.V. All rights reserved.",2012,10.1016/j.enbuild.2012.06.024,no
An efficient model for the crosscut optimisation problem in a wood processing mill,"We propose a dynamic programming model for the crosscut optimisation problem. This problem arises in the context of cutting lumber (rip-first) in order to obtain cubic blocks (cut-pieces) with required dimensions and surface characteristics. We propose a novel approach for matching the pattern of defects on all four surfaces of an incoming strip of wood with the surface requirements of the cut-pieces as specified in a given cut-bill, and determine an effective cutting pattern for each incoming strip accordingly. Our proposed dynamic programming model results in fast execution times as shown by the results of a computational experiment. The model is deployed in a software package that has been implemented successfully in several major rough mills.",2012,10.1080/00207543.2010.538446,no
Evaluating the unequal-variance and dual-process explanations of zROC slopes with response time data and the diffusion model,"We tested two explanations for why the slope of the z-transformed receiver operating characteristic (zROC) is less than 1 in recognition memory: the unequal-variance account (target evidence is more variable than lure evidence) and the dual-process account (responding reflects both a continuous familiarity process and a threshold recollection process). These accounts are typically implemented in signal detection models that do not make predictions for response time (RT) data. We tested them using RT data and the diffusion model. Participants completed multiple study/test blocks of an ""old""/""new"" recognition task with the proportion of targets and the test varying from block to block (.21, .32, .50, .68, or .79 targets). The same participants completed sessions with both speed-emphasis and accuracy-emphasis instructions. zROC slopes were below one for both speed and accuracy sessions, and they were slightly lower for speed. The extremely fast pace of the speed sessions (mean RT = 526) should have severely limited the role of the slower recollection process relative to the fast familiarity process. Thus, the slope results are not consistent with the idea that recollection is responsible for slopes below 1. The diffusion model was able to match the empirical zROC slopes and RT distributions when between-trial variability in memory evidence was greater for targets than for lures, but missed the zROC slopes when target and lure variability were constrained to be equal. Therefore, unequal variability in continuous evidence is supported by RT modeling in addition to signal detection modeling. Finally, we found that a two-choice version of the RTCON model could not accommodate the RT distributions as successfully as the diffusion model. (C) 2011 Elsevier Inc. All rights reserved.",2012,10.1016/j.cogpsych.2011.10.002,no
The Research of Agile Web Development Process and Quality Model,"With the arrival of WEB2.0 age, the Web application presents explosive development. It is the key of Web development that whether we can meet the demands flexibly, develop quickly,receive users feedback swiftly and make adjustments fast (namely ""Agile""). This thesis is to research the agile Web development process and its quality model based on SSH framework, trying to find the balance point between rapid development of Web application systems and the improvement of the quality of Web application system so that we can develop Web application systems which can not only satisfy the demands quickly but also enhance the quality of Web application systems.",2012,,no
Quality Assessment Model and Improvement Model for Screen Printing Process in Manufacturing of Touch Panels,"With the rapid development of technology, mobile consumer electronics such as cell phones, digital cameras, and laptops are receiving increasing consumer attention. Touch panels are already an essential part of many electronic devices, and functionality and quality of touch panels is a critical issue. This study investigated the screen printing process in the manufacture of touch panels and extracted essential critical to quality (CTQs). The process capability index C* (pmj) was employed to develop a process capability analysis model suitable for symmetric and asymmetric characteristics. The model enabled identification of CTQs that resulted in poor process capacity. A C* (pmj) index testing method and multiple comparison analysis method were also developed to obtain optimal settings for the CTQs resulting in poor capacity, thereby increasing process stability. The instruments developed in this study can assist touch panel manufacturers to enhance product quality as well as product competitiveness.",2012,,no
Testing structural equation models: the impact of error variances in the data generating process,"Yet another paper on fit measures? To our knowledge, very few papers discuss how fit measures are affected by error variance in the Data Generating Process (DGP). The present paper deals with this. Based upon an extensive simulation study, this paper shows that the effects of increased error variance differ significantly for various fit measures. In addition to error variance the effects depend on sample size and severity of misspecification. The findings confirm the general notion that good fit as measured by the chi-square, RMSEA and GFI etc. does not necessarily mean that the model is correctly specified and reliable. One finding is that the chi square test may give support to misspecified models in situations with a high level of error variance in the DGP, for small sample sizes. Another finding is that the chi-square test looses power also for large sample sizes when the model is negligible misspecified. Other results include incremental fit indices as NFI and RFI which prove to be more informative indicators under these circumstances. At the end of the paper we formulate some guidelines for use of different fit measures.",2012,10.1007/s11135-011-9466-5,no
"Methodological approach for calibration of building energy performance simulation models applied to a common ""measurement and verification"" process","A big challenge for energy savings performance contracting projects is the transparent and reliable determination of the energy consumption before and after retrofitting. This process, the so called measurement and verification (M&V) process, can be performed by means of calibrating simulation models with measured data, e.g. the energy bill data. There are guidelines for reporting and conducting this procedure. According to reports from case studies, it seems that they are carried out using several different approaches. The main goal of this work is to develop a consistent, practical approach for the M&V process. The approach supports visual inspection methods, parametric studies, and optimization methods. The visual inspection method can help to understand characteristics of a specific building depending on an input parameter change via graphs. However, the visual inspection method requires an extensive effort for this trial and error process and it is strongly dependent upon the users' experience. The automated parametric study can be applied both for calibration and sensitivity analysis of uncertain parameters. The big challenge for the practical use is the need for automating the process due to an enormous number of simulation cases and the required skills in applied statistics. Optimization algorithms quickly provide the user with a quantitatively best solution. However, they do not analyze the actual problems and therefore do not contribute to an understanding of the system under consideration. Furthermore, results should be examined carefully because the optimization may produce mathematically correct but physically meaningless results. Due to the present advantages and disadvantages of these methods, a calibration tool including all three methods would be desirable in order to predict the impact of energy efficient retrofitting projects.",2013,10.1002/bapi.201310070,no
"A Comprehensive AC/DC NBTI Model: Stress, Recovery, Frequency, Duty Cycle and Process Dependence","A comprehensive NBTI framework using the H/H-2 RD model for interface traps and 2 well model for hole traps has been proposed and used to predict DC and AC experiments. The framework is validated against experimental data from different DC stress and recovery conditions, AC frequency and duty cycle, measurement speed, and across SiON and HKMG devices having different gate insulator processes. Limitations of the alternative 2 stage model framework is discussed.",2013,,no
Towards a quality model for the evaluation of DSS based on KDD process,"A Decision Support System (DSS) based on Knowledge Discovery from Data (KDD) process is used to give confident knowledge to the final users in order to help them making right decisions. Such systems can be underused if the mined knowledge is unconfident, or if the system is hardly usable or unusable. Our target is to supply out a Quality Model (QM) ensuring a global evaluation of DSS based on KDD process (DSS/KDD). In our point of view, a QM should involve three dimensions: the evaluation of the DSS as a Software Product, as a User Interface and as a DSS. We should also take into account ISO recommendations. We intend to build a model which defines a set of criteria and allows measurement of a DSS/KDD quality evaluation using Goal-Question Method (GQM) and Analytic Hierarchy Process (AHP).",2013,,no
A comprehensive dual-scale wood torrefaction model: Application to the analysis of thermal run-away in industrial heat treatment processes,"A dual-scale model of the torrefaction of wood was developed and used to study industrial configurations. At the local scale, the computational code solves the coupled heat and mass transfer and the thermal degradation mechanisms of the wood components. At the global scale, the two-way coupling between the boards and the stack channels is treated as an integral component of the process. This model is used to investigate the effect of the stack configuration on the heat treatment of the boards. The simulations highlight that the exothermic reactions occurring in each single board can be accumulated along the stack. This phenomenon may result in a dramatic heterogeneity of the process and poses a serious risk of thermal runaway, which is often observed in industrial plants. The model is used to explain how thermal runaway can be lowered by increasing the airflow velocity, the sticker thickness or by gas flow reversal. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.ijheatmasstransfer.2013.03.066,no
Entrepreneurial training for girls empowerment in Lesotho: A process evaluation of a model programme,"A Girls Empowerment Programme held in 2010 in Lesotho, sub-Saharan Africa, focused on HIV/AIDS risk reduction and prevention, life skills, and entrepreneurial training (income-generating activities). Entrepreneurial training was a crucial part of equipping the camp attendees with basic skills to help them develop sustainable livelihoods. Such skills and financial independence are essential to enable rural girls to complete their secondary schooling (in a fee-based educational system) and to pursue a career, as well as to further help them be less susceptible to transactional sex and its significant risks. The results of a brief process evaluation with some nested supporting data showed considerable improvement in the girls' knowledge about income-generating activities. In addition, almost half of the camp attendees participated in further entrepreneurial training and about half of these girls went on to develop small businesses. Replication of this model of camp training is recommended and being explored in other African countries.",2013,10.1177/0081246313504685,no
Application of neuro-fuzzy modeling technique for operational problem solving in a CO2 capture process system,"A good understanding about relationships among key process parameters is important in optimizing operation and enhancing efficiency of the CO2 capture process system. This understanding would enable the operator to better analyze process conditions and become aware of ongoing trends or events so that timely and effective control actions can be taken for adjusting the relevant process parameters and efficiency of plant operations can be enhanced. The studies that focused on exploring the key parameters of the amine-based post-combustion CO2 capture process system implemented at the International Test Center of CO2 Capture (ITC) have revealed that among multiple data modeling techniques adopted, the adaptive-network-based fuzzy inference system (ANFIS) modeling approach generated satisfactory models for adequately describing the process system. This paper presents development and application of the four ANFIS models for solving four real-life problems encountered in operation of the CO2 capture process system. The testing results of the four developed models show that they can be applied for satisfactory solution of these problems. Some lessons and observations made during the application process are also discussed. (c) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.ijggc.2013.01.031,no
Calcium Looping Process: Experimental investigation of limestone performance regenerated under high CO2 partial pressure and validation of a carbonator model,"A key aspect of the Calcium Looping process is the implementation of the oxy-fuel combustion for the endothermic sorbent regeneration, which imposes high partial pressure of CO2 (75vol.-% dry). These conditions enhance lime sintering and thus, the decrease of the sorbent's maximum CO2 carbonation conversion (X-max), which is an important parameter that influences the CO2 capture. This paper presents results from tests performed in presence of high CO2 volumetric concentration in a 10kW(th) Dual Fluidized Bed system. High regeneration extents and CO2 capture efficiencies of more than 80% is attained with sorbent residual activity of 8,5%. A carbonator model is validated and active space time is proposed to be the main parameter to be used for up scaling design purposes. (C) 2013 The Authors. Published by Elsevier Ltd.",2013,10.1016/j.egypro.2013.05.101,no
Development and evaluation of a many-electron-multicenter classical-trajectory Monte Carlo model in charge-exchange processes involving collisions of multiply charged ions and CO,A many-electron-multicenter classical-trajectory Monte Carlo model is developed and tested in charge-exchange studies involving projectiles with charges from +3 to +10 and the CO molecule. The present model is contrasted against simpler single-center and independent-atom approaches. Total and state-selective charge-exchange cross sections are presented and are compared to recent data from the Jet Propulsion Laboratory where available.,2013,10.1103/PhysRevA.88.012714,no
Modelling and cost evaluation of electro-coagulation processes for the removal of anions from water,"A mathematical model previously proposed by our group has been modified for its application in the modelling of arsenates, nitrates and phosphates electro-coagulation with iron and aluminium. It classifies the coagulant and pollutant species attending to their reactivity in the electro-coagulation process on the basis of the experimental behaviour of the system. The enmeshment of anionic pollutants in a growing coagulant precipitate and their direct precipitation are the mechanisms considered. A good reproducibility of the experimental data is obtained (with r(2) above 0.9) with a small number of parameters. The model has been used to predict the amount of coagulant dose required under given conditions for the removal of the anionic pollutants, which allowed obtaining the costs of the electro-coagulation corresponding to the raw material, iron or aluminium sheets, and the energy costs for their electro-dissolution. Results showed that iron electro-coagulation is cheaper than aluminium electro-coagulation, and that the current density does not have an important influence on the costs within the range studied (0.2-4.0 mA cm(-2)). Attending to the results, arsenate and phosphate removal by electro-coagulation seem to be attractive from the economic point of view. (C) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.seppur.2013.01.035,no
Introduction of optical Newton Cradle model for understanding the N-solitons fission process under the action of higher order dispersion.,"A mechanism of creating a Newton's cradle (NC) in the form of a chain of solitons is proposed for understanding fission of higher-order soliton in optical fibers caused by higher-order dispersion. After the transformation of the initial N-soliton into a chain of fundamental quasi-solitons, the tallest one travels along the chain through elastic collisions with other solitons, and then escapes, while the other solitons remain in a bound state. Multiple releases of solitons take place if N is sufficiently large. The NC effect is robust against inclusion of the Raman and self-steepening terms.",2013,10.1117/12.2017616,no
System reliability of corroding pipelines considering stochastic process-based models for defect growth and internal pressure,"A methodology is presented to evaluate the time-dependent system reliability of pressurized pipelines that contain multiple active metal-loss corrosion defects and have been subjected to at least one inline inspection (ILI). The methodology incorporates a homogeneous gamma process-based corrosion growth model and a Poisson square wave process-based internal pressure model, and separates three distinctive failure modes, namely small leak, large leak and rupture. The hierarchical Bayesian method and Markov Chain Monte Carlo (MCMC) simulation are employed to characterize the parameters in the corrosion growth model based on data obtained from high-resolution inline inspections (ILIs). An example involving an in-service gas pipeline is used to validate the developed corrosion growth model and illustrate the proposed methodology for the system reliability analysis. Results of the parametric analysis indicate that both the uncertainties in the parameters of the growth model as well as their correlations must be accounted for in the reliability analysis. The proposed methodology will facilitate the application of reliability-based pipeline corrosion management programs. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.ijpvp.2013.06.002,no
"Validation of a process model of CO2 capture in an aqueous solvent, using an implicit molecular based treatment of the reactions","A model of a desorber for the recovery of aqueous monoethanolamine (MEA) solvent following the separation of carbon dioxide (CO2) from flue gas from a fossil fuel power plant is presented. This model is derived from a previously developed absorber model, by using the same rate-based stage and physical property models. The novelty of this modeling framework lies in the integration into a rate-based process model of the state-of-the-art SAFT-VR thermodynamic model, in which the physical and chemical interactions are treated simultaneously, assuming that the chemical reactions are at equilibrium. Such an approach reduces the amount of experimental data needed to model the interactions of the solvent with CO2. The implicit treatment of the chemical reactions in this formalism obviates the need to incorporate an enhancement factor or to use experimental data for the rate of reaction. The gPROMS software is employed to implement the desorber model and pilot plant data are used for the validation, without adjusting any model parameters. Very good predictions are obtained over a wide range of operating conditions. (C) 2013 The Authors. Published by Elsevier Ltd. Open access under CC BY-NC-ND license. Selection and/or peer-review under responsibility of GHGT",2013,10.1016/j.egypro.2013.06.032,no
Development and validation of a black carbon mixing state resolved three-dimensional model: Aging processes and radiative impact,"A new two-dimensional aerosol bin scheme, which resolves both aerosol size and black carbon (BC) mixing state for BC aging processes (e.g., condensation and coagulation) with 12 size x 10 mixing state bins, has been developed and implemented into the WRF-chem model (MS-resolved WRF-chem). The mixing state of BC simulated by this model is compared with direct measurements over the East Asian region in spring 2009. Model simulations generally reproduce the observed features of the BC mixing state, such as the size-dependent number fractions of BC-containing and BC-free particles and the coating thickness of BC-containing particles. This result shows that the model can simulate realistic BC mixing states in the atmosphere if condensation and coagulation processes are calculated explicitly with the detailed treatment of BC mixing state. Sensitivity simulations show that the condensation process is dominant for the growth of thinly coated BC particles, while the coagulation process is necessary to produce thickly coated BC particles. Off-line optical and radiative calculations assuming an average mixing state for each size bin show that the domain-and period-averaged absorption coefficient and heating rate by aerosols are overestimated by 30-40% in the boundary layer, compared with a benchmark simulation with the detailed treatment of mixing state. The absolute value of aerosol radiative forcing is also overestimated (10%, 3W m(-2)) at the surface. However, these overestimations are reduced considerably when all the parameters (including mass and number concentration) are calculated with the simple treatment of mixing state. This is because the overestimation of radiative parameters due to higher absorption efficiency (compared with the benchmark simulation) is largely canceled by the underestimation of BC concentrations due to efficient wet removal processes. The overall errors in radiative forcing can be much smaller because of this cancellation, but for the wrong reasons.",2013,10.1029/2012JD018446,no
Robust PLS model based product quality control strategy for solvent extraction process,"A novel product quality control strategy is presented in this paper. The quality control is achieved by predicting the product quality using a data-driven model and adjusting the manipulated variables when disturbances occur in the measured variables. The data-driven model employs robust partial least squares algorithm to predict off-line measured product quality, which can minimize the adverse effect of outliers in the training data set. Base on the robust regression model, the optimal control action are computed by solving a quadratic optimization problem under the constraint that the optimal projected solution must fall within the region of historical scores. The prediction and control performances are examined through a simulated solvent extraction process.",2013,,no
EVALUATION OF A THREE-DIMENSIONAL HYDRODYNAMIC MODEL APPLIED TO CHESAPEAKE BAY THROUGH LONG-TERM SIMULATION OF TRANSPORT PROCESSES,"A numerical model, the Curvilinear Hydrodynamics in 3-Dimensions, Waterway Experiment Station version (CH3D-WES), was applied to represent transport processes of the Chesapeake Bay. Grid resolution and spatial coverage, tied with realistic bathymetry, ensured dynamic responses along the channel and near the shoreline. The model was run with the forcing ranges from high frequency astronomical tides to lower frequency meteorological forcing, given by surface wind and heat flux, as well as hydrological forcing given by fresh water inflows both from upstream and distributed sources along the shoreline. To validate the model, a long-term simulation over seven-year time period between 1994 and 2000 was performed. The model results were compared with existing observation data including water level time series, which spans over a wide spectrum of time scales, and long-term variations in salinity structures over varying parts of the Bay. The validated model is set to provide an appropriate transport mechanism to the water quality model through linkage, warranting that the model takes into account the complexity in time and spatial scales associated with the dynamic processes in the Chesapeake.",2013,10.1111/jawr.12113,no
Image Processing as the Validation Method of Droplet Dispersion Modeling Process,"A pandemic of respiratory viral infections, which was spreading throughout the world during the first decade of the 21st century are believed to be easily contagious via direct contact through droplets and aerosol containing an infected particle which is spreading near the patient's environment. From the previous research, the role of droplet spreading during the coughing and talking are already investigated. However, massive viral infectious outbreaks during the SARS pandemic treatment procedure in the hospital on 2003 revealed that there is also a greater risk of viral dissemination process during the treatment practice. This project is aimed to understand about the droplet spreading characteristic during the dissemination process and to develop modeling method to make a prediction mechanism of droplet motion through a simple simulation program. The modeling has derived from basic theory of air viscosity, verified with droplet mist flow pattern images which are obtained through a digital image processing method. The results show that droplet modeling simulation which is based on the air viscosity factor gives an improved result to mimicking the droplet flow pattern, particularly during the operation of nebulizer. It also concluded that the droplet would perfectly evaporate as it travels more than 5 centimeters from the launch point and remain airborne during the treatment procedure.",2013,,no
A Mathematical Model for the Reduction Stage of the AOD Process. Part II: Model Validation and Results,"A process model was proposed by Jarvinen and co-authors for modelling the side-blowing decarburisation stage of the Argon-Oxygen Decarburisation (AOD) process. In Part I, a new mathematical model was derived for the reduction stage and coupled with the decarburisation model developed earlier. This paper, Part II, considers the validation of the model for the reduction stage with full-scale production data from a 150 t AOD converter in operation at Outokumpu Stainless Oy, Tornio Works, Finland. The results indicate that the model can accurately predict the end composition of the steel bath. Moreover, the model can be used to study rate phenomena during the reduction stage. Model predictions suggest that the reduction rate of chromium oxides is controlled initially by mass transfer of silicon onto the reaction surface and later by the diffusive mass transfer of chromium oxides in the slag droplets. Sensitivity of the model predictions to different initial bath temperatures, blowing times, ferrosilicon particle sizes and ferrosilicon feed rates was studied.",2013,10.2355/isijinternational.53.613,no
Process-based modelling to understand which ryegrass characteristics can increase production and decrease leaching in grazed grass-legume pastures,"A significant challenge for the pastoral farming systems is to maintain or increase production while reducing leaching of nitrogen, and for pastoral systems, this means reducing leaching from urine patches. Here we explore the potential impact of four ryegrass characteristics to increase pasture production and reduce leaching from ryegrass-white clover pastures. We focus on understanding which characteristics are desirable, the stage before investigating the achievability of those characteristics in a breeding program. Those characteristics were: the winter- or summer-dominance of growth (GP); the ability of the plant to intercept radiation at low pasture mass (LI); rooting depth (RD); and resistance to moderate water stress (WF). The impact of these ryegrass characteristics, both singly and combined within a ryegrass-clover pasture, was explored across a range of soils, climates, irrigation management, and urine patch concentrations using the process-based model APSIM. Of the four characteristics tested, LI was the most effective in increasing production and reducing leaching in all environments. The characteristics RD and WF were moderately effective, with RD having a greater impact on reducing leaching whereas WF had a greater effect on increasing production. The characteristic with the least impact was GP and it seems that ryegrass is currently well adapted for typical temperatures in New Zealand. The production and environmental effects of the characteristics were additive. The characteristics should be investigated further in the typically more diverse mixtures normally found in pastures but show promise for achieving improved production while reducing leaching provided they can be achieved in a breeding program.",2013,10.1071/CP13074,no
Analysis and validation of a run-of-mine ore grinding mill circuit model for process control,"A simple and novel non-linear model of a run-of-mine ore grinding mill circuit, developed for process control and estimation purposes, is validated. The model makes use of the minimum number of states and parameters necessary to produce responses that are qualitatively accurate. It consists of separate feeder, mill, sump and hydrocyclone modules that can be connected to model different circuit configurations. The model uses five states: rocks, solids, fines, water and steel balls. Rocks are defined as too large to be discharged from the mill, whereas solids, defined as particles small enough to leave the mill, consist of out-of-specification coarse ore and in-specification fine ore fractions. The model incorporates a unique prediction of the rheology of the slurry within the mill. A new hydrocyclone model is also presented. The model parameters are fitted to an existing plant's sampling campaign data and a step-wise procedure is given to fit the model to steady-state data. Simulation test results of the model are compared to sampling campaign data of the same plant at different steady-state conditions. The model shows promise in estimating important process variables such as mill power and product particle size and is deemed suitable for process control studies. (C) 2012 Elsevier Ltd. All rights reserved.",2013,10.1016/j.mineng.2012.10.009,no
Mathematical modelling and validation of the drying process in a Chimney-Dependent Solar Crop Dryer,"A simulation procedure describing the drying process within a Chimney-Dependent Solar Crop Dryer (CDSCD) has been developed. The simulation follows the authors' experimental work on the effect of varying drying chamber roof inclination on the ventilation and drying processes, and their work on the development of simulation code to help optimise ventilation in such dryers. The current paper presents the modelling and subsequent validation of the drying process inside the dryer, to come out with a design tool for the CDSCD. The work considers the height of the crop shelf above the drying-chamber base, crop resistance to airflow and the shading on the drying-chamber base and their effects on the drying process. The under-load condition temperatures and velocities are predicted to within a relative difference of 1.5% and 10%, respectively of the observed values. Even though the heat inertia of the physical model causes deviation between the predicted drying path and the observed drying path, the two paths tend to converge at the end of each drying cycle, with a general prediction to within 10% relative difference of the observed crop moisture content. The validation results show that the simulation code can serve as an effective tool for comparing and refining the designs of the CDSCD for optimum drying performance. (C) 2012 Elsevier Ltd. All rights reserved.",2013,10.1016/j.enconman.2012.11.007,no
Understanding the behaviour of pulsed laser dry and wet micromachining processes by multi-phase smoothed particle hydrodynamics (SPH) modelling,"A smoothed particle hydrodynamics (SPH) numerical model is developed to simulate the three-phase laser micro-machining process for medical coronary stent manufacture. The open-source code SPHysics is used to model the interaction between the laser beam and workpiece. This enables the melt flow behaviour in the non-linear pulsed fibre laser micro-machining process to be modelled. The developed model considers the conversion of laser energy into heat within a very thin surface layer, heat conduction into the parent material and the phase transition between solid, liquid and vapour. A good agreement with experimental data is obtained for predicting the penetration depth and melt ejection velocity. Water is also incorporated in this model to help explain the mechanism in laser wet micro-machining and drilling. It is demonstrated that the meshless characteristics of SPH are able to model the droplets ejected from kerf where it is difficult for conventional modelling. A static beam was used throughout the model development.",2013,10.1088/0022-3727/46/9/095101,no
Modeling cooling of ready-to-eat meats by 3D finite element analysis: Validation in meat processing facilities,"A three-dimensional (3D) finite element model for simulating heat transfer during cooling of irregular-shaped ready-to-eat meat products was developed and validated. The heat transfer model considered conduction as the governing equation, subject to convection, radiation and moisture evaporation boundary conditions. A 3D finite element algorithm developed in Java (TM) was used to solve the model. The algorithm generated solutions for meshes containing 4-node tetrahedral volume elements and 3-node triangular boundary elements. Product geometries were generated from CT-scan images of the meat products. The model was adapted to receive input parameters that can be easily provided by a meat processors including air relative humidity, air temperature, air velocity, type of casing, duration of water shower, product weight, and estimated core temperature of product prior to entering the cooling chamber. Model validation was conducted in four commercial facilities, under normal processing conditions. Temperatures predicted by the model were in agreement with observed values. Average root-mean-square error (RMSE) was 1.19 +/- 0.54 degrees C for core temperatures, 1.73 +/- 0.48 degrees C for temperatures 0.05 m from core to surface, and 2.01 +/- 1.01 degrees C for surface temperatures. The developed heat transfer model was integrated with predictive microbiology models through a food safety website: numodels4safety.unl.edu. The integration can be useful for estimating the severity of cooling deviations and resulting microbiological safety caused by unexpected cooling process disruptions. (C) 2012 Elsevier Ltd. All rights reserved.",2013,10.1016/j.jfoodeng.2012.11.024,no
Coupled simulation of DNAPL infiltration and dissolution in three-dimensional heterogeneous domains: Process model validation,"A three-dimensional multiphase numerical model was used to simulate the infiltration and dissolution of a dense nonaqueous phase liquid (DNAPL) release in two experimental flow cells containing different heterogeneous and well-characterized permeability fields. DNAPL infiltration was modeled using Brooks-Corey-Burdine hysteretic constitutive relationships. DNAPL dissolution was simulated using a rate-limited mass transfer expression with a velocity-dependent mass transfer coefficient and a thermodynamically based calculation of DNAPL-water interfacial area. The model did not require calibration of any parameters. The model predictions were compared to experimental measurements of high-resolution DNAPL saturations and effluent concentrations. The predicted concentrations were in close agreement with measurements for both domains, indicating that important processes were effectively captured by the model. DNAPL saturations greatly influenced mass transfer rates through their effect on relative permeability and velocity. Areas with low DNAPL saturation were associated with low interfacial areas, which resulted in reduced mass transfer rates and nonequilibrium dissolution. This was captured by the thermodynamic interfacial area model, while a geometric model overestimated the interfacial areas and the overall mass transfer. This study presents the first validation of the thermodynamic dissolution model in three dimensions and for high aqueous phase velocities; such conditions are typical for remediation operations, especially in heterogeneous aquifers. The demonstrated ability to predict DNAPL dissolution, only requiring prior characterization of soil properties and DNAPL release conditions, represents a significant improvement compared to empirical dissolution models and provides an opportunity to delineate the relationship between source zone architecture and the remediation potential for complex DNAPL source zones.",2013,10.1002/wrcr.20503,no
Mixing processes in the deep water of the Gulf of Elat (Aqaba): Evidence from measurements and modeling of the triple isotopic composition of dissolved oxygen,"A time series of the O-17 excess ((17)Delta) was measured in the Gulf of Elat (Aqaba) between May 2007 and August 2009. (17)Delta is unaffected by respiration; thus it is a unique, conservative tracer that preserves the signature acquired in the photic zone, the source region for deep-water formation. In this study we used (17)Delta to assess the ratio of photosynthetic O-2 to atmospheric O-2 in deep water. We observed an increase of (17)Delta by 20 per meg in the water residing below 300 m over a period of 3 months, followed by a decrease of 60 per meg over the next 7 months. These changes indicated penetration of photosynthetic O-2, followed by penetration of atmospheric O-2 into the deep water. To test whether vertical mixing could explain the observed variations in (17)Delta, we compared our results with simulated values obtained from a one-dimensional hydrodynamic model, which was extended to include dissolved O-2 isotopes. Although successfully reproducing the observed temperatures, salinities, and dissolved O-2 concentrations in the gulf, the model could not reproduce the observed variations in (17)Delta in the deep water. This discrepancy shows that horizontal mixing processes have an important role in the interaction between deep and surface water in the gulf. We suggest that for the most part, these processes occur along the coastal boundaries of the gulf.",2013,10.4319/lo.2013.58.4.1373,no
Understanding advances in the simulation of intraseasonal variability in the ECMWF model. Part II: The application of process-based diagnostics,"Abstract In Part I of this study it was shown that moving from a moisture-convergent- to a relative-humidity-dependent organized entrainment rate in the formulation for deep convection was responsible for significant advances in the simulation of the Madden-Julian Oscillation (MJO) in the ECMWF model. However, the application of traditional MJO diagnostics were not adequate to understand why changing the control on convection had such a pronounced impact on the representation of the MJO. In this study a set of process-based diagnostics are applied to the hindcast experiments described in Part I to identify the physical mechanisms responsible for the advances in MJO simulation. Increasing the sensitivity of the deep convection scheme to environmental moisture is shown to modify the relationship between precipitation and moisture in the model. Through dry-air entrainment, convective plumes ascending in low-humidity environments terminate lower in the atmosphere. As a result, there is an increase in the occurrence of cumulus congestus, which acts to moisten the mid troposphere. Due to the modified precipitation-moisture relationship more moisture is able to build up, which effectively preconditions the tropical atmosphere for the transition to deep convection. Results from this study suggest that a tropospheric moisture control on convection is key to simulating the interaction between the convective heating and the large-scale wave forcing associated with the MJO.",2013,10.1002/qj.2059,no
Modeling the Scheduling Problem of Break-up and Make-up Process for Cargo Trains in Chinese Railway Marshalling Yard,"According to the statistical data, Chinese railway uses only six percent of the whole world's network to meet the need of twenty-four percent of the whole world's transport task. So it is even more meaningful to put forward a series of models and algorithms to maximize the efficiency of all three levels of the transport task: strategic level, tactical level and daily task level. This article focuses on the scheduling problem in a single railway marshalling yard on the railway network, which belongs to the daily task level. To benefit from economies of scale, cars loaded with various kinds of commodities and have different origins and destinations should be grouped into a cargo-train. These cargo-trains operate between particular nodes of the network, called marshalling yards. At these marshalling yards, each arrival cargo-train should be broken-up to several cars, which are sorted according to their final destination, and made-up to form a new outbound cargo-train. Unlike the other network flow models, an MIP model is put forwarded to describe the scheduling problem in this paper. In addition, this MIP model is more efficient to get an optimal solution using the IBM-CPLEX after a series of conversions and cuts are done. Finally, a real case is used to test the efficiency of the model and the algorithm.",2013,,no
What is on your mind? Using the perceptual cycle model and critical decision method to understand the decision-making process in the cockpit,"Aeronautical decision-making is complex as there is not always a clear coupling between the decision made and decision outcome. As such, there is a call for process-orientated decision research in order to understand why a decision made sense at the time it was made. Schema theory explains how we interact with the world using stored mental representations and forms an integral part of the perceptual cycle model (PCM); proposed here as a way to understand the decision-making process. This paper qualitatively analyses data from the critical decision method (CDM) based on the principles of the PCM. It is demonstrated that the approach can be used to understand a decision-making process and highlights how influential schemata can be at informing decision-making. The reliability of this approach is established, the general applicability is discussed and directions for future work are considered. Practitioner Summary: This paper introduces the PCM, and the associated schema theory, as a framework to structure and explain data collected from the CDM. The reliability of both the method and coding scheme is addressed.",2013,10.1080/00140139.2013.809480,no
A grey-based DEMATEL model for evaluating business process management critical success factors,"Although business process management (BPM) is an important organizational practice for improving operational competitiveness of organizations, research has shown that as many as 60-80% of BPM initiatives are unsuccessful. This study provides a methodology to evaluate BPM implementation critical success factors (CSFs) that can aid project managers make proper BPM investment strategies. Through a review of the literature, eight CSFs for the successful implementation of BPM are identified. To help advance research on the implementation of BPM, this paper uses multi-site field study data with a novel grey-based DEMATEL (the decision making trial and evaluation laboratory) approach to visualize the structure of complicated causal relationships between these CSFs and obtain the influence level of these factors. The field study data uses three Chinese manufacturers as the setting. The four most important factors found in the field study, from amongst the identified CSFs, include Strategic alignment, Top management support, Project management and a Collaborative environment. We also found a number of direct and indirect relationships amongst the CSF factors. Insights into the application of the technique and results from both a research and managerial perspective are presented. Aggregate analysis for the methodology and future research directions are also introduced in the final section. (C) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.ijpe.2013.07.011,no
"Uncertainties of an Automated Optical 3D Geometry Measurement, Modeling, and Analysis Process for Mistuned Integrally Bladed Rotor Reverse Engineering","An automated reverse engineering process is developed that uses a structured light optical measurement system to collect dense point cloud geometry representations. The modeling process is automated through integration of software for point cloud processing, reverse engineering, solid model creation, grid generation, and structural solution. Process uncertainties are quantified on a calibration block and demonstrated on an academic transonic integrally bladed rotor. These uncertainties are propagated through physics-based models to assess impacts on predicted modal and mistuned forced response. Process details are discussed and recommendations made on reducing uncertainty. Reverse engineered parts averaged a deviation of 0.0002 in. (5 mu m) which did not significantly impact low and midrange frequency responses. High frequency modes were found to be sensitive to these uncertainties demonstrating the need for future refinement of reverse engineering processes.",2013,10.1115/1.4025000,no
"UNCERTAINTIES OF AN AUTOMATED OPTICAL 3D GEOMETRY MEASUREMENT, MODELING, AND ANALYSIS PROCESS FOR MISTUNED IBR REVERSE ENGINEERING","An automated reverse engineering process is developed that uses a structured light optical measurement system to collect dense point cloud geometry representations. The modeling process is automated through integration of software for point cloud processing, reverse engineering, solid model creation, grid generation, and structural solution. Process uncertainties are quantified on a calibration block and demonstrated on an academic transonic integrally bladed rotor. These uncertainties are propagated through physics-based models to assess impacts on predicted modal and mistuned forced response. Process details are discussed and recommendations made on reducing uncertainty Reverse engineered parts averaged a deviation of 0.0002 in. (5 mu m) which did not significantly impact low and mid-range frequency responses. High frequency modes were found to be sensitive to these uncertainties demonstrating the need for future refinement of reverse engineering processes.",2013,,no
Evaluation of Urban Regeneration Project Plans with an Integrated Model based on Analytic Network Process,"An integrated model based on ANP method which contains three major criteria, thirteen sub-criteria and thirty six factors was developed to evaluate urban regeneration project plans. To improve the traditional ANP method, a non-linear Programming equation which can be solved by the PS method was used to measure the inconsistency as well as to calculate the priorities in the pair-wise comparison stage of ANP. In addition, a DEMATEL method was employed to analyze the interdependencies of criteria/factors comprehensively. Five alternative plans of an urban regeneration project were adopted to test the effectiveness of the developed model. It was found the integrated model was feasible and effective to solve the problem of evaluation of the urban regeneration project plans.",2013,10.4028/www.scientific.net/AMM.253-255.102,no
Evaluation Model Based on Analytic Hierarchy Process and Applications in Indoor Air Quality Monitoring System,"Analytic Hierarchy Process (AHP), which was originally introduced by Professor T. L. Satty, is an effective method to make a quantitative analysis for the non-quantitative analytical case. In recent years, two evaluation models based on AHP have been proposed for indoor air quality (IAQ) by the environmental experts from home and abroad. One is Indoor Assessment by the Analytical Hierarchy (IAAH), and the other is Fuzzy AHP, which is combined fuzzy computing and AHP. In this paper, the two models are constructed and respectively applied in the IAQ evaluation. Here an experiment is designed to compare them from the applicable condition in the IAQ monitoring system. The results show that IAAH is suitable for the monitoring system which makes an environmental assessment at a long interval, e.g. 1 hour and 8 hours. On the contrary, Fuzzy AHP is more suitable for that which makes an environmental assessment at a short interval, e.g. 5 minutes and 10 minutes.",2013,10.1109/ICINIS.2013.9,no
Category-specific uncertainty modeling in clinical laboratory measurement processes,"Background: A statement of measurement uncertainty describes the quality of a clinical assay analysis result, and uncertainty models of clinical assays can be used to evaluate and optimize laboratory protocols designed to minimize the measurement uncertainty associated with an assay. In this study, we propose a methodology to lend systematic structure to the uncertainty modeling process. Methods: Clinical laboratory assays are typically classified based on the chemical reaction involved, and therefore, based on the assay analysis methodology. We use this fact to demonstrate that uncertainty models for assays within the same category are structurally identical in all respects except for the values of certain model parameters. This is accomplished by building uncertainty models for assays belonging to two categories - substrate assays based on optical absorbance analysis of endpoint reactions, and ion selective electrode (ISE) assays based on potentiometric measurements of electromotive force. Results: Uncertainty models for the substrate assays and the ISE assays are built, and for each category, a general mathematical framework for the uncertainty model is developed. The parameters of the general framework that vary from assay to assay for each category are identified and listed. Conclusions: Estimates of measurement uncertainty from the models were compared with estimates of uncertainty from quality control data from the clinical laboratory. We demonstrate that building a general modeling framework for each assay category and plugging in parameter values for each assay is sufficient to generate uncertainty models for an assay within a given category.",2013,10.1515/cclm-2013-0357,no
"Implementing a provider-initiated testing and counselling (PITC) intervention in Cape town, South Africa: a process evaluation using the normalisation process model","Background: Provider-initiated HIV testing and counselling (PITC) increases HIV testing rates in most settings, but its effect on testing rates varies considerably. This paper reports the findings of a process evaluation of a controlled trial of PITC for people with sexually transmitted infections (STI) attending publicly funded clinics in a low-resource setting in South Africa, where the trial results were lower than anticipated compared to the standard Voluntary Counselling and Testing (VCT) approach. Method: This longitudinal study used a variety of qualitative methods, including participant observation of project implementation processes, staff focus groups, patient interviews, and observation of clinical practice. Data were content analysed by identifying the main influences shaping the implementation process. The Normalisation Process Model (NPM) was used as a theoretical framework to analyse implementation processes and explain the trial outcomes. Results: The new PITC intervention became embedded in practice (normalised) during a two-year period (2006 to 2007). Factors that promoted the normalising include strong senior leadership, implementation support, appropriate accountability mechanisms, an intervention design that was responsive to service needs and congruent with professional practice, positive staff and patient perceptions, and a responsive organisational context. Nevertheless, nurses struggled to deploy the intervention efficiently, mainly because of poor sequencing and integration of HIV and STI tasks, a focus on HIV education, tension with a patient-centred communication style, and inadequate training on dealing with the operational challenges. This resulted in longer consultation times, which may account for the low test coverage outcome. Conclusion: Leadership and implementation support, congruent intervention design, and a responsive organisational context strengthened implementation. Poor compatibility with nurse skills on the level of the clinical consultation may have contributed to limiting the size of the trial outcomes. A close fit between the PITC intervention design and clinical practices, as well as appropriate training, are needed to ensure sustainability of the programme. The use of a theory-driven analysis promotes transferability of the results, and the findings are therefore relevant to the implementation of HIV testing and to the design and evaluation of complex interventions in other settings.",2013,10.1186/1748-5908-8-97,no
"Innovations in major system reconfiguration in England: a study of the effectiveness, acceptability and processes of implementation of two models of stroke care","Background: Significant changes in provision of clinical care within the English National Health Service (NHS) have been discussed in recent years, with proposals to concentrate specialist services in fewer centres. Stroke is a major public health issue, accounting for over 10% of deaths in England and Wales, and much disability among survivors. Variations have been highlighted in stroke care, with many patients not receiving evidence-based care. To address these concerns, stroke services in London and Greater Manchester were reorganised, although different models were implemented. This study will analyse processes involved in making significant changes to stroke care services over a short time period, and the factors influencing these processes. We will examine whether the changes have delivered improvements in quality of care and patient outcomes; and, in light of this, whether the significant extra financial investment represented good value for money. Methods/design: This study brings together quantitative data on 'what works and at what cost?' with qualitative data on 'understanding implementation and sustainability' to understand major system change in two large conurbations in England. Data on processes of care and their outcomes (e.g. morbidity, mortality, and cost) will be analysed to evidence services' performance before and after reconfiguration. The evaluation draws on theories related to the dissemination and sustainability of innovations and the 'social matrix' underlying processes of innovation. We will conduct a series of case studies based on stakeholder interviews and documentary analysis. These will identify drivers for change, how the reconfigurations were governed, developed, and implemented, and how they influenced service quality. Discussion: The research faces challenges due to: the different timings of the reconfigurations; the retrospective nature of the evaluation; and the current organisational turbulence in the English NHS. However, these issues reflect the realities of major systems change and its evaluation. The methods applied in the study have been selected to account for and learn from these complexities, and will provide useful lessons for future reconfigurations, both in stroke care and other specialties.",2013,10.1186/1748-5908-8-5,no
Different healing process of esophageal large mucosal defects by endoscopic mucosal dissection between with and without steroid injection in an animal model,"Background: Stricture formation is one of the major complications after endoscopic removal of large superficial squamous cell neoplasms of the esophagus, and local steroid injections have been adopted to prevent it. However, fundamental pathological alterations related to them have not been well analyzed so far. The aim of this study was to analyze the time course of the healing process of esophageal large mucosal defects resulting in stricture formation and its modification by local steroid injection, using an animal model. Methods: Esophageal circumferential mucosal defects were created by endoscopic mucosal dissection (ESD) for four pigs. One pig was sacrificed five minutes after the ESD, and other two pigs were followed-up on endoscopy and sacrificed at the time of one week and three weeks after the ESD, respectively. The remaining one pig was followed-up on endoscopy with five times of local steroid injection and sacrificed at the time of eight weeks after the ESD. The esophageal tissues of all pigs were subjected to pathological analyses. Results: For the pigs without steroid injection, the esophageal stricture was completed around three weeks after the ESD on both endoscopy and esophagography. Histopathological examination of the esophageal tissues revealed that spindle-shaped a-smooth muscle actin (SMA)-positive myofibroblasts arranged in a parallel fashion and extending horizontally were identified at the ulcer bed one week after the ESD, and increased contributing to formation of the stenotic luminal ridge covered with the regenerated epithelium three weeks after the ESD. The proper muscle layer of the stricture site was thinned with some myocytes which seemingly showed transition to the myofibroblast layer. By contrast, for the pig with steroid injection, esophageal stricture formation was not evident with limited appearance of the spindle-shaped myofibroblasts, instead, appearance of stellate or polygocal SMA-positive stromal cells arranged haphazardly in the persistent granulation tissue of the ulcer site. Conclusions: Proliferation of spindle-shaped myofibroblasts arranged in a parallel fashion is likely to play an important role in stricture formation after circumferential mucosal defects by esophageal ESD, which may be related to the thinning of the proper muscle layer in the healing course of the defects. Local steroid injection seems to be effective to prevent the stricture through the modification of this process.",2013,10.1186/1471-230X-13-72,no
Distance Measures for Surgical Process Models,"Background: The development of new resources, such as surgical techniques and approaches, results in continuous modification of surgery. To assess these modifications, it is necessary to use measures that quantify the impact of resources on surgical processes. Objectives: The objective of this work is to introduce and evaluate distance measurements that are able to represent differences in the courses of surgical interventions as processes. Methods: Hence, we present four different distance measures for surgical processes: the Jaccard distance, Levenshtein distance, Adjacency distance, and Graph matching; distance. These measures are formally introduced and evaluated by applying them to clinical data sets from laparoscopic training in pediatric surgery. Results: We analyzed the distances of 450; surgical processes using these four measures with a focus on the difference in surgical; processes performed by novices and by experienced surgeons. The Levenshtein and, Adjacency distances were best suited to measure distances between surgical processes. Conclusion: The measurement of distances between surgical processes is necessary to, estimate the benefit of new surgical techniques and strategies.",2013,10.3414/ME12-01-0111,no
A Bayesian model averaging based multi-kernel Gaussian process regression framework for nonlinear state estimation and quality prediction of multiphase batch processes with transient dynamics and uncertainty,"Batch processes are characterized by inherent nonlinearity, multiplicity of operating phases, between-phase transient dynamics and batch-to-batch uncertainty that pose significant challenges for accurate state estimation and quality prediction. Conventional multi-model strategies, however, may be ill-suited for multiphase batch processes because the localized models do not specially take into account the complex transient dynamics between two consecutive operating phases. In this study, a novel Bayesian model averaging based multi-kernel Gaussian process regression (BMA-MKGPR) approach is proposed for state estimation and quality prediction of nonlinear batch processes with multiple operating phases and between-phase transient dynamics. A kernel mixture model strategy is first used to identify the different operating phases of batch processes and then the multi-kernel GPR models are built for all the identified phases. Further, the between-phase transitional stage is determined by the posterior probabilities of measurement samples with respect to the two consecutive phases so that the Bayesian model averaging strategy can be designed to incorporate the two localized GPR models for handling the between-phase transient dynamics. For an arbitrary test sample within the transitional stage, its posterior probabilities with respect to the local models corresponding to the two consecutive phases are set as the adaptive weightings to integrate the corresponding local GPR models for state estimation and quality prediction. The proposed BMA-MKGPR approach is applied to a multiphase batch polymerization process and the result comparison demonstrates that the presented method can effectively handle multiple nonlinear operating phases, between-phase transient dynamics and process uncertainty with fairly high prediction accuracies. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.ces.2013.01.058,no
Multi-Kernel Gaussian Process Regression and Bayesian Model Averaging Based Nonlinear State Estimation and Quality Prediction of Multiphase Batch Processes,"Batch processes are characterized by inherent nonlinearity, multiplicity of operating phases, between-phase transient dynamics and batch-to-batch uncertainty that pose significant challenges for accurate state estimation and quality prediction. Conventional multi-model strategies, however, may be ill-suited for multiphase batch processes because the localized models do not specifically characterize the complex transient dynamics between two consecutive operating phases. In this study, a novel Bayesian model averaging based multi-kernel Gaussian process regression (BMA-MKGPR) approach is proposed for state estimation and quality prediction of nonlinear batch processes with multiple operating phases and between-phase transient dynamics. The new approach is applied to a simulated batch polymerization process and the result comparison shows that it can effective handle multiple nonlinear operating phases, between-phase transient dynamics and process uncertainty with high prediction accuracies.",2013,,no
An Effort Prediction Model Based on BPM Measures for Process Automation,"BPM software automation projects require different approaches for effort estimation for they are developed based on business process models rather than traditional requirements analysis outputs. In this empirical research we examine the effect of various measures for BPMN compliant business process models on the effort spent to automate those models. Although different measures are suggested in the literature, only a few studies exist that relate these measures to effort estimation. We propose that different perspectives of business process models need to be considered such as behavioral, organizational, functional and informational to determine the automation effort effectively. The proposed measures include number of activities, number of participating roles, number of outputs from the process and control flow complexity. We examine the effect of these measures on the automation effort and propose a prediction model developed by multiple linear regression analysis. The data were collected from a large IS integration project which cost 300 person-months along a three-year time frame. The results indicate that some of the measures collected have significant effect on the effort spent to develop the BPM automation software. We envision that prediction models developed by using the suggested approach will be useful to make accurate estimates of project effort for BPM intensive software development projects.",2013,,no
A Qualitative Research Perspective on BPM Adoption and the Pitfalls of Business Process Modeling,"Business Process Management (BPM) is used by organizations as a method to increase awareness and knowledge of business processes. Although many companies adopt BPM, there is still a notable insecurity of how to set it up in the most effective way. A considerate amount of research concerning partial aspects of BPM adoption has been done, such as the pitfalls of business process modeling. However, up until now hardly any empirical research has been conducted that aims at validating them. In this paper we address this research gap by conducting eleven in-depth interviews with BPM experts from various companies. We use the Grounded Theory approach to qualitatively analyze the data. Our contribution is twofold. First, we derive a conceptual framework showing the insights of BPM adoption by organizations. Second, we use the evidence from the interviews to discuss the pitfalls of business process modeling and show the countermeasures that are taken by companies.",2013,,no
Exploration on Software Complexity Metrics for Business Process Model and Notation,"Business Process Model and Notation (BPMN) is a graphical representation and notation for modeling complex business processes in diagrams. A simple BPMN diagram is easier to understand by all of the business stakeholders than a complex one. It is also easier for the developers to implement the corresponding systems. Complexity metrics can measure the complexity of a diagram. Only a few BPMN complexity metrics are found in the literature as BPMN is a recent development. To propose a new BPMN complexity metric, it is important to find suitable software complexity metrics which can be further adapted to develop a complexity metric for BPMN. This research surveys the existing software complexity metrics and the existing BPMN complexity metrics (i.e. McCabe Cyclomatic Complexity, Control-flow Complexity, and Halstead-based Process Complexity Metrics) to compare their performance and suitability in measuring the complexity of BPMN diagrams. The BPMN diagrams of the business processes of two Enterprise Resource Planning (ERP) open-source systems (i.e. Compiere and Openbravo ERP systems) are used in this research. The metrics values obtained are compared with empirical application and code measurement values (i.e. number of form-fields, number of files of code, and number of classes) of the two open-source systems. This research finds that the Halstead-based Process Complexity that has been proposed in the literature is useful in measuring the data complexity of BPMN diagrams. This means that the Halstead-based Process Complexity can be further elaborated to produce a BPMN complexity measure.",2013,,no
TOWARD A QUALITY FRAMEWORK FOR BUSINESS PROCESS MODELS,"Business process modeling is recognized as a key part of the business process lifecycle. It is during this stage that a conceptual model is produced by collecting business process requirements and representing them with a specific business process notation. While there has been much research into process modeling techniques, little has taken place with regard to the characteristics that should be considered for an effective assessment of the models' quality. This paper presents a synthesis of quality characteristics for business process models, based on a systematic review of the relevant literature. It then goes on to describe a reference model for the quality assessment of business process models, and to relate the aforementioned quality characteristics to existing relevant process model measures. These relations may help organizations to guide the improvement of their business process models according to their chosen quality characteristics.",2013,10.1142/S0218843013500032,no
Managing Structural and Textual Quality of Business Process Models,"Business process models are increasingly used for capturing business operations of companies. Such models play an important role in the requirements elicitation phase of to-be-created information systems and in as-is analysis of business efficiency. Many process modeling initiatives have grown considerably big in size involving dozens of modelers with varying expertise creating and maintaining hundreds, sometimes thousands of models. One of the roadblocks towards a more effective usage of these process models is the often insufficient provision of quality assurance. The aim of this paper is to give an overview on how empirical research informs structural and textual quality assurance of process models. We present selected findings and show how they can be utilized as a foundation for novel automatic analysis techniques.",2013,,no
The Study of Operating Overvoltage Suppression Measures in Black-Start Process Based on Conversion from BPA to PSCAD Component Models,"Calculating and assessing the no-load operating overvoltage is an important part in black-start scheme design. It's unable for the electromechanical transient software-BPA to simulate the electromagnetic transient overvoltage. So according to the transfer function of component models in BPA and PSCAD, this paper analyzes the conversion methods from BPA to PSCAD. The simulation results verify their feasibility. Then, it builds corresponding mathematical models of the shunt reactor, lightning arrester, closing resistor and phase controlled switch in a long distance transmission system with a single power source. Finally, based on one black-start path of Guangxi power grid, it builds overvoltage verification models in the PSCAD simulation platform. By observing the simulation waveforms and taking the overvoltage amplitude and duration as the rationale, it respectively compares different effects of the shunt reactor, lightning arrester, closing resistor and phase control technique on suppressing terminal overvoltage.",2013,,no
Thermodynamic Evaluation and Cold Flow Model Testing of an Indirectly Heated Carbonate Looping Process,"Carbon capture with subsequent compression and storage is a promising possibility for the reduction of CO2 emissions from coal-fired power generation. A very efficient post-combustion CO2 capture technology is the carbonate looping process. To further increase the process efficiency, a new concept is considered where the heat for calcination is transferred from an external combustor to the calciner by means of heat pipes. Some thermodynamic evaluations for a retrofit of a coal-fired host plant with the indirectly heated carbonate looping process and cold flow model tests of a 300-kW(th) test facility that will be erected in 2013 in Darmstadt are presented.",2013,10.1002/ceat.201300019,no
Genetic parameters of different measures of cheese yield and milk nutrient recovery from an individual model cheese-manufacturing process,"Cheese yield (CY) is an important technological trait in the dairy industry, and the objective of this study was to estimate the genetic parameters of cheese yield in a dairy cattle population using an individual model-cheese production procedure. A total of 1,167 Brown Swiss cows belonging to 85 herds were sampled once (a maximum of 15 cows were sampled per herd on a single test day, 1 or 2 herds per week). From each cow, 1,500 mL of milk was processed according to the following steps: milk sampling and heating, culture addition, rennet addition, gelation-time recording, curd cutting, whey draining and sampling, wheel formation, pressing, salting in brine, weighing, and cheese sampling. The compositions of individual milk, whey, and curd samples were determined. Three measures of percentage cheese yield (%CY) were calculated: %CYCURD, %CYSOLIDS and %CYWATER, which represented the ratios between the weight of fresh curd, the total solids of the curd, and the water content of the curd, respectively, and the weight of the milk processed. In addition, 3 measures of daily cheese yield (dCY, kg/d) were defined, considering the daily milk yield. Three measures of nutrient recovery (REC) were computed: RECFAT, RECPROTEIN, and RECSOLIDS which represented the ratio between the weights of the fat, protein, and total solids in the curd, respectively, and the corresponding nutrient in the milk. Energy recovery, RECENERGY, represented the energy content of the cheese versus that in the milk. For statistical analysis, a Bayesian animal model was implemented via Gibbs sampling. The effects of parity (1 to >= 4), days in milk (6 classes), and laboratory vat (15 vats) were assigned flat priors; those of herd-test-date, animal, and residual were given Gaussian prior distributions. Intra-herd heritability estimates of %CYCURD, %CYSOLIDS, and %CYWATER, ranged from 0.224 to 0.267; these were larger than the estimates obtained for milk yield (0.182) and milk fat content (0.122), and similar to that for protein content (0.275). Daily cheese yields showed heritability estimates similar to those of daily milk yield. The trait %CYWATER showed a highly positive genetic correlation with %CYSOLIDS (0.87) whereas their phenotypic correlation was moderate (0.37), and the fat and protein contents of milk showed high genetic correlations with %CY traits. The heritability estimates of RECPROTEIN and RECFAT were larger (0.490 and 0.208, respectively) than those obtained for the protein and fat contents of milk, and the genetic relationships between RECPROTEIN and RECFAT with milk protein and fat content were low or moderate; RECPROTEIN and RECFAT were moderately correlated with the %CY traits and highly correlated with RECSOLIDS and RECENERGY. Both RECSOLIDS and RECENERGY were heritable (0.274 and 0.232), and showed high correlations with each other (0.96) and with the %CY traits (0.83 to 0.97). Together, these findings demonstrate the existence of economically important, genetically determined variability in cheese yield that does not depend solely upon the fat and protein contents of milk, but also relies on the ability of the coagulum to retain the highest possible proportions of the available protein, fat, and water. Exploitation of this interesting genetic variation does not seem to be feasible through direct measurement of the phenotype in cows at the population level. Instead, further research is warranted to examine possible means for indirect prediction, such as through assessing the mid-infrared spectra of milk samples.",2013,10.3168/jds.2012-6517,no
Factors affecting variation of different measures of cheese yield and milk nutrient recovery from an individual model cheese-manufacturing process,"Cheese yield (CY) is the most important technological trait of milk, because cheese-making uses a very high proportion of the milk produced worldwide. Few studies have been carried out at the level of individual milk-producing animals due to a scarcity of appropriate procedures for model-cheese production, the complexity of cheese-making, and the frequent use of the fat and protein (or casein) contents of milk as a proxy for cheese yield. Here, we report a high-throughput cheese manufacturing process that mimics all phases of cheese-making, uses 1.5-L samples of milk from individual animals, and allows the simultaneous processing of 15 samples per run. Milk samples were heated (35 C for 40 mm), inoculated with starter culture (90 mm), mixed with rennet (51.2 international milk-clotting units/L of milk), and recorded for gelation time. Curds were cut twice (10 and 15 mm after gelation), separated from the whey, drained (for 30 min), pressed (3 times, 20 mm each, with the wheel turned each time), salted in brine (for 60 mm), weighed, and sampled. Whey was collected, weighed, and sampled. Milk, curd, and whey samples were analyzed for pH, total solids, fat content, and protein content, and energy content was estimated. Three measures of percentage cheese yield (%CY) were calculated: %CYCURD, %CYSOLIDS and %CYWATER) representing the ratios between the weight of fresh curd, the total solids of the curd, and the water content of the curd, respectively, and the weight of the milk processed. In addition, 3 measures of daily cheese yield (dCY, kg/d) were defined, considering the daily milk yield. Three measures of nutrient recovery (REC) were computed: RECFAT, RECPROTEIN, and RECSOLIDS, which represented the ratio between the weights of the fat, protein, and total solids in the curd, respectively, and the corresponding components in the milk. Energy recovery, RECENERGY) represented the energy content of the cheese compared with that in the milk. This procedure was used to process individual milk samples obtained from 1,167 Brown Swiss cows reared in 85 herds of the province of Trento (Italy). The assessed traits exhibited almost normal distributions, with the exception of RECFAT. The average values (SD) were as follows: %CYCURD = 14.97 1.86, %CYSOLIDS = 7.18 +/- 0.92, %CY - WATER 7.77 1.27, dCY(CURD) = 3.63 +/- 1.17, dCY(SOLIDS) = 1.74 0.57, dCY(WATER) = 1.88 +/- 0.63, RECFAT = 89.79 3.55, RECPROTEIN = 78.08 +/- 2.43, 51.88 3.52, and RECENERGY = 67.19 3.29. All traits were highly influenced by herd-test-date and days in milk of the cow, moderately influenced by parity, and weakly influenced by the utilized vat. Both %CYCURD and dCY(CURD) depended not only on the fat and protein (casein) contents of the milk, but also on their proportions retained in the curd; the water trapped in curd presented an higher variability than that of %CYSOLIDS. All REC traits were variable and affected by days in milk and parity of the cows. The described model cheese-making procedure and the results obtained provided new insight into the phenotypic variation of cheese yield and recovery traits at the individual level.",2013,10.3168/jds.2012-6516,no
Formal Modeling and Evaluation of Service-based Business Process Elasticity in the Cloud,"Cloud computing is a new delivery model for IT services. Cloud platforms are being increasingly used for the deployment and execution of service-based business processes (SBPs). Nevertheless, the provisioning of elastic infrastructures and/or platforms is not sufficient to provide users with elasticity at the level of SBPs. Therefore, there is a need to provide SBPs with mechanisms to scale their resource requirements up and down whenever possible. This can be achieved using mechanisms for duplicating and consolidating business services that compose the SBPs. In this paper, we propose a formal model for SBPs elasticity in the Cloud. We show that our model preserves the semantics of SBPs when services are duplicated or consolidated. We also propose a formal framework for the evaluation of elasticity strategies that decide on when and how many resources are required to ensure elasticity of SBPs.",2013,10.1109/WETICE.2013.42,no
Formal Modeling and Evaluation of Stateful Service-Based Business Process Elasticity in the Cloud,"Cloud environments are being increasingly used for deploying and executing business processes and particularly Service-based Business Processes (SBPs). One of the expected features of Cloud environments is elasticity at different levels. It is obvious that provisioning of elastic platforms is not sufficient to provide elasticity of the deployed business process. Therefore, SBPs should be provided with elasticity so that they would be able to adapt to the workload changes while ensuring the desired functional and non-functional properties. In this paper, we propose a formal model for stateful SBPs elasticity that features a duplication/consolidation mechanisms and a generic controller to define and evaluate elasticity strategies.",2013,,no
Supporting Flexibility of the CMMI Process Framework with a Multi-layered Process Model,"CMMI(Capability Maturity Model Integration) is a process framework which is organized by goals, practice and process area. The Maturity Level 3 stressed that the project level process must be tailored from the organization level process, that is to say, no matter how the project level process changes, it must satisfy the constraints of the organization level process. However, companies usually prefer to choose detailed and operational single-layered process models for applying CMMI. If some activities in process have been changed, the process model must be totally redesigned; this reduces the flexibility and increases the cost of adopting CMMI. In order to solve the above problems, this paper presents a multi-layered model, in which the upper layer is used to describe the CMMI organization level process (ie,the software development lifecycle) and the lower layer is used to describe the project level process model (ie,the procedure of each development group). At the same time, we provide delegation relationship in multi-layered process model to build the constraints between the upper layer and the lower layer process model, it brings the flexibility that the lower layered model (the project process) can change its activities on the premise that obeying the constraints of upper layered model (the organization process). Finally, in order to validate the practical effectiveness of this model, a CMMI case from GUODIAN Nanjing Automation Cd.,LTD is provided.",2013,10.1109/WISA.2013.83,no
Based on the Jump-Diffusion Process of the Coal Resources Development Investment Project Evaluation Model,"Coal resources development projects have many characteristics of large investment, irreversibility, long cycle and uncertainty and so on, due to the inherent defects of the traditional NPV method, which can't effectively deal with the uncertainty faced by the coal resources development investment, thus, the real value of the coal development investment projects are hardly assessed scientifically and rationally. Using the real options method, the paper builds a evaluation model on coal resources development investment project. The results show that using the model to assess the project value of coal resources development investment will be more scientific and rational, parameter analysis leads to the result that interest rates and convenience yields have a negative effect on the critical investment value of the projects, and the high jump frequency will reduce the critical investment value of the projects, while positive changes on jump range can increase.",2013,10.1007/978-3-642-33012-4_60,no
Regeneration qualification of cold trap using modeling validated by radiography and image processing,"Cold trap is a purification unit used in sodium cooled Fast Spectrum Reactors (FSRs) for maintaining the oxygen and hydrogen level in sodium within acceptable limits. It works on the principle of crystallization and precipitation of oxides and hydrides of sodium in a wire mesh, when the temperature of sodium is reduced below the saturation temperature. The sodium hydride gets accumulated in the secondary cold trap as a consequence of the continuous diffusion of hydrogen in sodium and with time the trap is fully loaded and becomes inoperable. The removal of these impurity deposits at intervals by keeping the cold trap in same location of the loop is known as in situ regeneration. After regeneration cold trap is qualified by gamma radiograph technique to ensure adequate removal of impurities before bringing the cold trap back to service in sodium for purification. The numerical results predict the impurity deposition pattern in the wire mesh region of cold trap. The mathematical model has been validated with experimental data obtained from model cold trap. This paper discusses the methodologies developed for qualification of regeneration of cold trap using radiography and image processing techniques for assessing impurity deposition before and after regeneration. (C) 2012 Elsevier B.V. All rights reserved.",2013,10.1016/j.nucengdes.2012.10.011,no
Validation of Mass-Transfer Model for VIPS Process using In Situ Measurements Performed by Near-Infrared Spectroscopy,"Combined experimental and modeling approaches were performed in order to investigate the influence of formulation and process parameters on mass transfers during VIPS process, using the water/N-methyl-pyrrolidone (NMP)/poly(ether imide) (PEI) system. The experiments were conducted using a thick polymer solution at increasing polymer concentrations for various operating conditions. The global water intake rate in the bulk solution was determined by gravimetric measurements (global), and in situ measurements were conducted by near-infrared spectroscopy at three points in the solution. In parallel, a fully predictive model was developed for predicting mass-transfer phenomena involved during the VIPS process. The comparison between experimental data and numerical predictions exhibited a good agreement for moderate polymer concentration, but for higher polymer concentrations, the model overestimated the nonsolvent-transfer rate. This result was explained by the aggregation process of the polymer chains due to water intake. The numerical predictions were improved by modifying the average hole-free volume expression. (C) 2012 American Institute of Chemical Engineers AIChE J, 59: 671-686, 2013",2013,10.1002/aic.13839,no
Information-Transfer PLS Model for Quality Prediction in Transition Periods of Batch Processes,"Compared to the steady phase, quality prediction in the transition period of batch process has been explored as a difficult task in recent years. Unlike the data behavior in the steady phase, there may be significant correlations among the data samples in each transition period. Without considering the relationships among different time slices (time pieces in the batch dataset), the performance of the quality prediction model may be degraded. In the present work, an information transfer PLS model is particularly proposed for quality prediction in transition periods of the batch process. By transferring the main data information of one time slice to the next one, different time slices in the transition period are connected. As a result, most available data information in the previous time slices can be efficiently used for quality modeling of a specific time slice in the transition period. For performance evaluation of the developed method, a case study of an industrial injection molding process is provided.",2013,10.1021/ie303267u,no
Real Complexity vs. Apparent Complexity in Modeling Social Processes,"Complexity science, developed in the last few decades, starts from the unproved presupposition that the foundations of the processes which manifest themselves as complex are simple. Thus, it was easy to build quickly various models for complex processes. But, in the current stage of development we start, little by little, to become aware of the main limitations introduced by the deep simplicity hypothesis. Indeed, it is hard to explain the high phe-nomenological complexity by deep structural simplicity. Between phenomenon and structure we propose the intermediate concept of architecture, as a supporting environment for maximizing the complexity captured in our models.",2013,,no
Modelling and validation: Casting of Al and TiAl alloys in gravity and centrifugal casting processes,"Components which best utilise the properties of high temperature titanium alloys are characterised by thin sections of a few millimetres thickness and hundreds of millimetres length. These alloys however are difficult to work with, being highly reactive in a molten state, necessitating a low superheat during processing. Centrifugal casting is therefore utilised as a candidate production method, as under the centrifugal force, metal can rapidly fill thicknesses substantially less than a millimetre before it solidifies. However, due to the high liquid metal velocity developed there is a high risk of turbulent flow and of the trapping of any gas present within the liquid metal. This challenging application involves a combination of complex rotating geometries, significant centrifugal forces and high velocity transient free surface flows, coupled with simultaneous heat transfer and solidification. Capturing these interacting physical phenomena, free surface flows, trapped air and associated defects is a complex modelling task. Building upon earlier work on computational modelling the authors have previously described to capture and validate the fluid dynamics behaviour of rotating systems, this contribution considers the modelling and validation of such systems to capture the coupled flow and thermal solidification behaviour and associated defect development. A bench-mark test case is employed to validate the effect of solidification on the fluidity of an aluminium alloy. Validation is also performed against a series of casting experiments to establish the models ability to capture the filling process and predict defects due to air entrapment within the solidified metal. (C) 2013 Elsevier Inc. All rights reserved.",2013,10.1016/j.apm.2013.03.030,no
Separate effects identification via casting process modeling for experimental measurement of U-Pu-Zr alloys,"Computational simulations of gravity casting processes for metallic U-Pu-Zr nuclear fuel rods have been performed using a design-of-experiments technique to determine the fluid flow, liquid heat transfer, and solid heat transfer parameters which most strongly influence the process solidification speed and fuel rod porosity. The results are used to make recommendations for the best investment of experimental time and effort to measure process parameters. (C) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.jnucmat.2013.07.016,no
Theoretical Modelling of Metallurgical Defect Closing-Up Processes During Forming a Forging,"Computer modelling using an FEM-based program, i.e. Forge 2008, was carried out. Laboratory modelling of the process of free hot forging in shaped anvils was conducted to close up metallurgical defects. Results obtained from the simulation modelling were processed by using a commercial statistical package, i.e. Statistics 6. 0 PL. The influence of the anvil shape and main parameters of the forging process on closing up metallurgical defects were determined. On the basis of the investigation carried out, the optimal values of main forging technological parameters and suitable groups of anvils to be used in particular forging stages are proposed for the elimination of metallurgical defects.",2013,,no
AN INTEGRATIVE DESIGN ANALYSIS PROCESS MODEL WITH CONSIDERATIONS FROM QUALITY ASSURANCE,"Computer-based design analysis activities are an essential part of most product development projects in industry. An effective integration of the analysis activity into the product development process, especially when the design analysis is performed not by the engineering designer but by an analyst, internal or external to the company, is therefore very valuable. The contribution in this work is a design analysis process model that tries to eliminate some integration issues (transmission of incorrect information, disagreement on activities...) through the use of quality assurance techniques and procedures: quality checks, verification and validation, and uncertainty treatment. The process model is formulated in general terms so that it can be adapted to particular product development processes available in the industry.",2013,,no
Measuring the Performance of Process Synchronization with the Model-Driven Approach,"Concurrent and parallel computing can be used to effectively speed up computations. However, the overall performance gain depends on the way how the concurrency has been introduced to the program. Considering process synchronization as one of the most important aspects in concurrent programming, this paper describes the possibility of application of the model-driven approach in the concept of a software framework for measuring the performance of process synchronization algorithms. Such framework could help determine and analyze the features of specific synchronization techniques in different cases, depending on different input parameters.",2013,,no
Population Balance Model Validation and Predictionof CQAs for Continuous Milling Processes: toward QbDin Pharmaceutical Drug Product Manufacturing,"Continuous tablet manufacturing has been investigated for its potential advantages (e.g., cost, efficiency, and controllability) over more conventional batch processes. One avenue for tablet manufacturing involves roller compaction followed by milling to form compactible granules. A better understanding of these powder processes is needed to implement Quality by Design in pharmaceutical manufacturing. In this study, ribbons of microcrystalline cellulose were produced by roller compaction and milled in a conical screen mill. A full factorial experiment was performed to evaluate the effects of ribbon density, screen size, and impeller speed on the product size distribution and steady-state mass holdup of the mill. A population balance model was developed to simulate the milling process, and a parameter estimation technique was used to calibrate the model with a subset of experimental data. The calibrated model was then simulated at other processing conditions and compared with additional unused experimental data. Statistical analyses of the results showed good agreement, demonstrating the model's predictive capability in quantifying milled product critical quality attributes within the experimental design space. This approach can be used to optimize the design space of the process, enabling Quality by Design.",2013,10.1007/s12247-013-9155-0,no
Conceptual Model for the Specification of the Quality Properties of the Critical Business Process,"Critical Business Processes (CBP) and the study of the aspects that allow to specify formally their properties, it has now become a subject of extensive research. Because of the multiplicity of elements involved in the treatment of CBP and its definition has evolved over time in this article provides a historical review of the concepts related to the formal specification of the properties of the CBP and proposes some models based conceptual definitions on this topic. Additionally, we introduce the definition of verifiable properties of the CBP. This article is part of a research in progress that aims to facilitate the process of formal specification of the properties of the CBP.",2013,,no
Nash bargaining game model for two parallel stages process evaluation with shared inputs,"Data envelopment analysis is a non-parametric technique for evaluating peer decision making units (DMUs) with using multiple inputs to produce multiple outputs. In the real world, DMUs usually have complex structures. One of these structures is a two-stage process with intermediate measures. In this structure, there are two stages and each stage uses inputs to produce outputs, separately where the outputs of the first stage are the inputs for the second stage. Cooperative model such as centralized model and non-cooperative model are game theoretic approaches to evaluate two-stage processes. Non-cooperative model supposes that one of the stages is the leader and another stage is the follower, whereas in the centralized model, both stages are evaluated simultaneously. In this paper, we propose a game theoretic model based on the Nash bargaining game to calculate weights when parallel stages with shared inputs compete to reach a high efficiency in the competitive strategy. Two data sets including the bank branches and thermal power plants in Iran are used to show the abilities of proposed model. This model can be applied in other processes such as supply chain, manufacturing and public service units.",2013,10.1007/s00170-012-4498-0,no
Data Pre-Processing Evaluation for Text Mining: Transaction/Sequence Model,"Data pre-processing presents the most time consuming phase in the whole process of knowledge discovery. The complexity of data pre-processing depends on the data sources used. The aim of this work is to determine to what extent it is necessary to carry out the time consuming data pre-processing in the process of discovering sequential patterns in e-documents. We used the transaction/sequence model for text representation and sequence rule analysis as a method of modelling. We compare four datasets of different quality obtained from texts and pre-processed in different ways: data with identified the paragraph sequences, data with identified the sentence sequences, data with identified the paragraph sequences without stop words and data with identified the sentence sequences without stop words. We try to assess the impact of these advanced techniques of data pre-processing on the quantity and quality of the extracted rules. The results confirm some initial assumptions, but they also show that the stop words removal has a substantial impact on the quantity and quality of extracted rules in case of paragraph sequence identification. Contrary, in case of sentence sequence identification, removing the stop words has not any significant impact on the quantity and quality of extracted rules. (C) 2013 The Authors. Published by Elsevier B.V. and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science",2013,10.1016/j.procs.2013.05.286,no
Modeling and prediction of machining quality in CNC turning process using intelligent hybrid decision making tools,"Decision-making process in manufacturing environment is increasingly difficult due to the rapid changes in design and demand of quality products. To make decision making process (selection of machining parameters) online, effective and efficient artificial intelligent tools like neural networks are being attempted. This paper proposes the development of neural network models for prediction of machining parameters in CNC turning process. Experiments are designed based on Taguchi's Design of Experiments (DoE) and conducted with cutting speed, feed rate, depth of cut and nose radius as the process parameters and surface roughness and power consumption as objectives. Results from experiments are used to train the developed neuro based hybrid models. Among the developed models, performance of neural network model trained with particle swarm optimization model is superior in terms of computational speed and accuracy. Developed models are validated and reported. Signal-to-noise (S/N) ratios of responses are calculated to identify the influences of process parameters using analysis of variance (ANOVA) analysis. The developed model can be used in automotive industries for deciding the machining parameters to attain quality with minimum power consumption and hence maximum productivity. (C) 2012 Elsevier B. V. All rights reserved.",2013,10.1016/j.asoc.2012.03.071,no
Making Sense of Declarative Process Models: Common Strategies and Typical Pitfalls,"Declarative approaches to process modeling are regarded as well suited for highly volatile environments as they provide a high degree of flexibility. However, problems in understanding and maintaining declarative business process models impede often their usage. In particular, how declarative models are understood has not been investigated yet. This paper takes a first step toward addressing this question and reports on an exploratory study investigating how analysts make sense of declarative process models. We have handed out real-world declarative process models to subjects and asked them to describe the illustrated process. Our qualitative analysis shows that subjects tried to describe the processes in a sequential way although the models represent circumstantial information, namely, conditions that produce an outcome, rather than a sequence of activities. Finally, we observed difficulties with single building blocks and combinations of relations between activities.",2013,,no
Adapting Data Processing To Compare Model and Experiment Accurately: A Discrete Element Model and Magnetic Resonance Measurements of a 3D Cylindrical Fluidized Bed,"Discrete element modeling is being used increasingly to simulate flow in fluidized beds. These models require complex measurement techniques to provide validation for the approximations inherent in the model. This paper introduces the idea of modeling the experiment to ensure that the validation is accurate. Specifically, a 3D, cylindrical gas-fluidized bed was simulated using a discrete element model (DEM) for particle motion coupled with computational fluid dynamics (CFD) to describe the flow of gas. The results for time-averaged, axial velocity during bubbling fluidization were compared with those from magnetic resonance (MR) experiments made on the bed. The DEM-CFD data were postprocessed with various methods to produce time-averaged velocity maps for comparison with the MR results, including a method which closely matched the pulse sequence and data processing procedure used in the MR experiments. The DEM-CFD results processed with the MR-type time-averaging closely matched experimental MR results, validating the DEM-CFD model. Analysis of different averaging procedures confirmed that MR time-averages of dynamic systems correspond to particle-weighted averaging, rather than frame-weighted averaging, and also demonstrated that the use of Gaussian slices in MR imaging of dynamic systems is valid.",2013,10.1021/ie401896x,no
Water Quality Modelling and Control in a Water Treatment Process,"Drinking water quality is an important issue around the world since the low quality water causes health-related problems and economic losses. To ensure high quality water an efficient monitoring and control of a water treatment process is essential. In this study, two common quality variables of treated water, turbidity and residual aluminium, are modelled using the cross-validation method. Selected variables for developing the models are easy and reliable to measure on-line from raw water source. The linguistic equation (LE) approach based on nonlinear scaling and linear interactions produces models, which can be used in addition to predicting the water quality, for monitoring and controlling the water treatment process. The goal of the control simulation was to minimize the turbidity by controlling the coagulation chemical dose and see how this affects the residual aluminium level in drinking water. The results showed that the developed models were accurate and followed the changes in measured water quality variable. Results of the control simulation suggest that the water quality can be improved by proper control and optimizing the chemical dosing, as minimizing the turbidity reduces the residual aluminium level.",2013,10.1109/EUROSIM.2013.31,no
"In Vitro and In Vivo Evaluation of Amorphous Solid Dispersions Generated by Different Bench-Scale Processes, Using Griseofulvin as a Model Compound","Drug polymer-based amorphous solid dispersions (ASD) are widely used in the pharmaceutical industry to improve bioavailability for poorly water-soluble compounds. Spray-drying is the most common process involved in the manufacturing of ASD material. However, spray-drying involves a high investment of material quantity and time. Lower investment manufacturing processes such as fast evaporation and freeze-drying (lyophilization) have been developed to manufacture ASD at the bench level. The general belief is that the overall performance of ASD material is thermodynamically driven and should be independent of the manufacturing process. However, no formal comparison has been made to assess the in vivo performance of material generated by different processes. This study compares the in vitro and in vivo properties of ASD material generated by fast evaporation, lyophilization, and spray-drying methods using griseofulvin as a model compound and hydroxypropyl methylcellulose acetate succinate as the polymer matrix. Our data suggest that despite minor differences in the formulation release properties and stability of the ASD materials, the overall exposure is comparable between the three manufacturing processes under the conditions examined. These results suggest that fast evaporation and lyophilization may be suitable to generate ASD material for oral evaluation. However, caution should be exercised since the general applicability of the present findings will need to be further evaluated.",2013,10.1208/s12248-013-9469-3,no
Behavior validation of production systems within different phases of the engineering process Focused on behavior modeling,"Due to increasing complexity of production systems as well as necessary reduction of time-to-market engineers are faced with new challenges within the engineering process today. To make this process more efficient and to improve the quality of engineering results the use of a previous simulative validation of these results is increasing. But within each engineering phase different simulation tools and models are used which are usually not consistent to each other respectively not reusable in other tools. In this paper a modular, adaptable, and extensible simulation framework will be presented. It is applicable in multiple engineering phases by using libraries of engineering artifacts depending on the process.",2013,,no
Modeling and Analysis of the Discrete Manufacturing Product Quality with Information Formation Process for Traceability,"Due to the various organizations and production processes of the product life cycle involved in the process of the formation of discrete manufacturing product quality information, there are many uncertain factors, making the process have high complexity. How to describe this process accurately to identify potential problems in the process effectively is extremely important. Therefore, the paper presents the product quality formation process modeling method based on Petri net; explores how to use Petri net tool to describe the product material flow and information flow in the product quality information formation process, as well as how to apply the theory of Petri net to analyze the state of the process, thereby potential problems in this process can be identified effectively. The paper verifies the effectiveness of the method with an example.",2013,10.4028/www.scientific.net/AMM.345.477,no
Evaluation of the influence of polyoxide-based surfactants on the separation process of model emulsions of asphaltenes using the FTIR-ATR technique,"During extraction of crude oil, water is generally present in the oil. This water-in-oil (w/o) mixture undergoes turbulent flow that promotes sheer forces, resulting in the appearance of emulsions. These emulsions can be highly stable due to the presence of compounds with polar characteristics such as asphaltenes, which act as natural emulsifiers and form resistant films at the oilwater interface. Nonionic surfactants based on polyoxides are widely used to prevent the formation or to break down w/o emulsions. To shed more light on the destabilization mechanism of w/o emulsions promoted by these surfactants, in this study the techniques of tensiometry and Fourier transform infrared spectroscopy with attenuated total reflectance (FTIR-ATR) were applied to study the interface formed by poly(ethylene oxide)-poly(propylene oxide) (PEO-PPO) block copolymers and asphaltenic petroleum fractions. Initially, the critical micelle concentration of the copolymers in aqueous solution was determined. The results agreed with those found by tensiometry. The bottle test was used to evaluate the break-down of the w/o emulsions in the presence of the PEO-PPO block copolymers, and the results presented good agreement with those obtained by tensiometry and FTIR-ATR. (c) 2012 Wiley Periodicals, Inc. J. Appl. Polym. Sci., 2013",2013,10.1002/app.38292,no
"Interplay between field observations and numerical modeling to understand temporal pulsing of tree root throw processes, Canadian Rockies, Canada","During the cycle of forest disturbance, regeneration, and maturity, tree mortality leading to topple is a regular occurrence. When tree topple occurs relatively soon after mortality and if the tree has attained some threshold diameter at breast height (dbh) at the time of death, then notable amounts of soil may be upheaved along with the root wad. This upheaval may result in sediment transfers and soil production. A combination of field evidence and numerical modeling is used herein to gain insights regarding the temporal dynamics of tree topple, associated root throw processes, and pit-mound microtopography. Results from our model of tree population dynamics demonstrate temporal patterns in root throw processes in subalpine forests of the Canadian Rockies, a region in which forests are affected largely by wildfire disturbance. As the forest regenerates after disturbance, the new cohort of trees has to reach a critical dbh before significant root plate upheaval can occur; in the subalpine forests of the Canadian Rockies, this may take up to similar to 10(2) years. Once trees begin to reach this critical dbh for root plate upheaval, a period of sporadic root throw arises that is caused by mortality of trees during competition. In due course, another wildfire will occur on the landscape and a period of much increased root throw activity then takes place for the next several decades; tree sizes and, therefore, the amount of sediment disturbance will be greater the longer the time period since the previous fire. Results of previous root throw studies covering a number of regional settings are used to guide an exercise in diffusion modeling with the aim of defining a range of reasonable diffusion coefficients for pit-mound degradation; the most appropriate values to fit the field data ranged from 0.01 m(2) y(-1) to 0.1 m(2) y(-1). A similar exercise is then undertaken that is guided by our field observations in subalpine forests of the Canadian Rockies. For these forests, the most appropriate range of diffusion coefficients is in the range 0.001 m(2) y(-1) to 0.01 m(2) y(-1). Finally, the model of tree population dynamics is combined with the model of pit-mound degradation to demonstrate the integration of these combined processes on the appearance of pit-mound microtopography and soil bioturbation in subalpine forests of the Canadian Rockies. We conclude that the appearance of notable pit-mound microtopography is limited to very specific time periods and is not visible for much of the time. Most of the hillslope plot is affected by root throw during the 1000-year model run time. (C) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.geomorph.2013.04.017,no
E-Business Process Modeling Issues: From the Viewpoint of Inter-organizational Process Efficiency and Information Sharing,"E-business process modeling is an important factor in developing schemes of e-business. In order to develop schemes of e-business in or among firms, they have to reform their business process and information systems, considering changes that enable the companies to achieve new value. In this paper, the authors overview related works on developing e-business schemes, consider issues concerning e-business process modeling issues especially for Japanese companies bearing in mind their management style, and propose the idea of a modeling tool for supporting inter-organizational process flow adjustment. (C) 2013 The Authors. Published by Elsevier B.V.",2013,10.1016/j.procs.2013.09.164,no
Using the Viable System Model (VSM) to structure information processing complexity in disaster response,"Earthquakes, hurricanes, flooding and terrorist attacks continue to threaten our society and, when the worst happens, lives depend on different agencies to manage the response. The literature shows that there is significant potential for operational research (OR) to aid disaster management and that, whilst some of this potential has been delivered, there is more that OR can contribute. In particular, OR can provide detailed support to analysing the complexity of information processing - an essential topic as failure could cause response agencies to act on low quality information or act too slowly - putting responders and victims at risk. However, there is a gap in methods for analysing information processing whilst delivering rapid response. This paper explores how OR can fill this gap through taking a Viable System Model (VSM) approach to analyse information processing. It contributes to the OR literature by showing how VSM can support the analysis of information processing as well as how the OR modelling technique can be further strengthened to facilitate the task. (C) 2012 Elsevier B.V. All rights reserved.",2013,10.1016/j.ejor.2012.06.032,no
Process-based modelling to understand the impact of ryegrass diversity on production and leaching from grazed grass-clover dairy pastures,"Ecological studies often suggest that natural grasslands with high species diversity will grow more biomass and leach less nitrogen (N). If this diversity effect also applies to fertilised and irrigated pastures with controlled removal of herbage, it might be exploited to design pastures that can assist the dairy industry to maintain production while reducing N leaching losses. The purpose of this study was to test whether pasture mixtures with a high functional diversity in ryegrass traits will confer on the system higher water- and N-use efficiency. The hypothesis was tested using a process-based model in which pasture mixtures were created with varying levels of diversity in ryegrass traits likely to affect pasture growth. Those traits were: the winter- or summer-dominance of growth, the ability of the plant to intercept radiation at low pasture mass, and rooting depth. Pasture production, leaching and water- and N-use efficiency were simulated for management typical of a dairy pasture. We found that the performance of the diverse ryegrass-clover mixtures was more strongly associated with the performance of the individual components than with the diversity across the components. Diverse pasture mixtures may confer other benefits, e.g. pest or disease resistance and pasture persistence. The testing here was within a selection of ryegrasses, and the greater possible diversity across species may produce different effects. However, these results suggest that highly performing pastures under fertilised and irrigated grazed conditions are best constructed by selecting components that perform well individually than by deliberately introducing diversity between components.",2013,10.1071/CP13263,no
Description and validation of production processes in the coral reef ecosystem model CAFFEE (Coral-Algae-Fish-Fisheries Ecosystem Energetics) with a fisheries closure and climatic disturbance,"Ecosystem models are well-established tools to investigate the effects of human activities and natural events on marine ecosystems. Recent modeling approaches advocate the integration of physical and biological processes and the coupling of fully-represented ecosystems. Here, we describe a coral reef ecosystem simulation model (CAFFEE) linking reef cover dynamics, benthic and pelagic production and metabolism, detrital pathways and reef formation processes. The model integrates 27 functional groups with coupled two-way trophic and spatial interactions and includes dynamic adjustment of benthic production and consumption processes. The model is validated with a 40 year time series of coral reef field data from Kenyan reefs responding to two disturbances: fisheries closures and a strong thermal anomaly, the 1998 coral bleaching event. The model simulations successfully replicated a number of patterns observed in empirical data, in particular the progression of communities toward greater calcium carbonate deposition following fisheries closure, and the temporal shift to algal-domination after bleaching events. Further validation of the model was garnered from comparisons of emergent model outputs with ecological ratios commonly used to parameterize comparable systems. The model simulations indicate trade-offs among the organic and inorganic (calcium carbonate) processes in coral reefs whereby fisheries closures promote inorganic production and bleaching disturbances favor the organic production processes of the reef. (C) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.ecolmodel.2013.05.012,no
The Efficiency Research in the Process of Handling the Multidimensional Data Mining With the Method of Model Establishing Based on AHP,"Efficiency and exactness play an utterly important role in ensuring the analysis and prediction in the process of multidimensional data mining. Especially in the complex data environment, it requests a higher standard quality of data mining. Based on this, this article set the intricate data environment as the research target and put forward the main body structure and apply efficiency with the method of model establishing based on AHP so that it can provide a brand new apply tactic for the data mining in complex data environment.",2013,,no
MODELLING AND MEASUREMENT OF BUBBLE FORMATION AND GROWTH IN ELECTROFLOTATION PROCESSES,"Electroflotation is used in the water treatment industry for the recovery of suspended particles. In this study the bubble formation and release of hydrogen bubbles generated electrolytically from a platinum cathode was investigated. Previously, it was found that both the growth rate and detachment diameter increased with increasing wire diameter. Conversely, current density had little effect on the released bubble size. It was also found that the detached bubbles rapidly increased in volume as they rose through the liquid as a result of decreasing hydrostatic pressure and high levels of dissolved hydrogen gas in the surrounding liquid. The experimental system was computationally modelled using a Lagrangian-Eulerian Discrete Particle approach. It was revealed that desorption of gaseous solutes from the electrolyte solution, other than hydrogen, may have a significant impact on the diameter variation of the formed bubbles. The simulation confirmed that liquid circulation, either forced or induced by the rising bubble plume, influences both the hydrogen supersaturation (concentration) in the neighbourhood of the electrode and the size of the resulting bubbles.",2013,10.2478/cpe-2013-0026,no
Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes,"Eliciting and representing experts' remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding segments of the experts' eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts' viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians' diagnostic viewing behaviors informs about medical images' understanding to correct diagnosis.",2013,10.1109/CVPR.2013.284,no
DEVELOPMENT OF EVALUATION MODELS OF MANPOWER NEEDS FOR DISMANTLING THE DRY CONVERSION PROCESS-RELATED EQUIPMENT IN URANIUM REFINING AND CONVERSION PLANT (URCP),"Evaluation models for determining the manpower needs for dismantling various types of equipment in uranium refining and conversion plant (URCP) have been developed. The models are widely applicable to other uranium handling facilities. Additionally, a simplified model was developed for easily and accurately calculating the manpower needs for dismantling dry conversion process-related equipment (DP equipment). It is important to evaluate beforehand project management data such as manpower needs to prepare an optimized decommissioning plan and implement effective dismantling activity. The Japan Atomic Energy Agency (JAEA) has developed the project management data evaluation system for dismantling activities (PRODIA code), which can generate project management data using evaluation models. For preparing an optimized decommissioning plan, these evaluation models should be established based on the type of nuclear facility and actual dismantling data. In URCP, the dry conversion process of reprocessed uranium and others was operated until 1999, and the equipment related to the main process was dismantled from 2008 to 2011. Actual data such as manpower for dismantling were collected during the dismantling activities, and evaluation models were developed using the collected actual data on the basis of equipment classification considering the characteristics of uranium handling facility.",2013,10.1115/ICEM2013-96097,no
Evaluating Social Tagging for Business Process Models,Finding business process models in a model repository is a challenge that needs to be tackled for efficient business process management. Existing process model similarity measures compare models based on named elements and model structure. Social tagging enriches models with so-called tags - words or short phrases describing the content of the model. The tags given to models offer another possibility to judge about the similarity between models. In this paper we compare both approaches based on a study conducted with students. We discuss first insights and perspectives for tag-based search for process models.,2013,,no
In situ measurement using FBGs of process-induced strains during curing of thick glass/epoxy laminate plate: experimental results and numerical modelling,"For large composite structures, such as wind turbine blades, thick laminates are required to withstand large in-service loads. During the manufacture of thick laminates, one of the challenges met is avoiding process-induced shape distortions and residual stresses. In this paper, embedded fibre Bragg grating sensors are used to monitor process-induced strains during vacuum infusion of a thick glass/epoxy laminate. The measured strains are compared with predictions from a cure hardening instantaneous linear elastic (CHILE) thermomechanical numerical model where different mechanical boundary conditions are employed. The accuracy of the CHILE model in predicting process-induced internal strains, in what is essentially a viscoelastic boundary value problem, is investigated. A parametric study is furthermore performed to reveal the effect of increasing the laminate thickness. The numerical model predicts the experimental transverse strains well when a tied boundary condition at the tool/part interface is used and the tool thermal expansion is taken into account. However, the CHILE approach is shown to overestimate residual strains after demoulding because of the shortcomings of the model in considering viscoelastic effects. The process-induced strain magnitude furthermore increases when the laminate thickness was increased, owing mainly to a decrease in through-thickness internal transverse stresses. Copyright (c) 2012 John Wiley & Sons, Ltd.",2013,10.1002/we.1550,no
A novel ASM2 and SVM compensation method for the effluent quality prediction model of A(2)O process,"For the soft measurement of water quality for sewage treatment process, a novel prediction model is proposed to predict the effluent water quality in this paper, which combines the mechanism model with compensation model. Firstly, the ASM2 model is built as the mechanism model to imitate the sewage treatment process, as well as PSO algorithm is used to adjust the kinetic parameters of the ASM2 model. Next, SVM regression is adopted to compensate the prediction error of mechanism model. Finally, the model is tested with real data collected in a sewage treatment plant. The simulation results show that the model can obtain accuracy prediction results and reflect the behavior of sewage treatment efficiently.",2013,,no
Understanding the freezing of biopharmaceuticals: First-principle modeling of the process and evaluation of its effect on product quality,"Freezing and thawing are important process steps in the manufacture of numerous biopharmaceuticals. It is well established that these process steps can significantly influence product quality attributes (PQA). Herein, we describe a physico-mathematical model to predict product temperature profiles based on the freezing program as input parameter in a commercial freeze-thaw module. Applying this model, the time from first nucleation until the last point to freeze (LPF) reaching -5 degrees C and the time from -5 degrees C at LPF to -30 degrees C at LPF was varied to study the effect on PQA in a full factorial design. Effects of process parameter settings on a typical fully formulated, highly concentrated monoclonal antibody (mAb) solution as well as highly concentrated mAb solution formulated with buffer only were investigated. We found that both process phases affected PQA, such as aggregates by size-exclusion chromatography, polydispersity index by dynamic light scattering, and number of subvisible particles and turbidity in a complex way. In general, intermediate cooling and freezing times resulted in overall optimized PQA. Fully formulated mAb solution containing cryoprotectant and nonionic surfactant was significantly less affected by freezing-thawing than mAb solution formulated in buffer only. (c) 2013 Wiley Periodicals, Inc. and the American Pharmacists Association J Pharm Sci 102:2495-2507, 2013",2013,10.1002/jps.23642,no
An information theoretic model of information processing in the Drosophila olfactory system: the role of inhibitory neurons for system efficiency,"Fruit flies (Drosophila melanogaster) rely on their olfactory system to process environmental information. This information has to be transmitted without system-relevant loss by the olfactory system to deeper brain areas for learning. Here we study the role of several parameters of the fly's olfactory system and the environment and how they influence olfactory information transmission. We have designed an abstract model of the antennal lobe, the mushroom body and the inhibitory circuitry. Mutual information between the olfactory environment, simulated in terms of different odor concentrations, and a sub-population of intrinsic mushroom body neurons (Kenyon cells) was calculated to quantify the efficiency of information transmission. With this method we study, on the one hand, the effect of different connectivity rates between olfactory projection neurons and firing thresholds of Kenyon cells. On the other hand, we analyze the influence of inhibition on mutual information between environment and mushroom body. Our simulations show an expected linear relation between the connectivity rate between the antennal lobe and the mushroom body and firing threshold of the Kenyon cells to obtain maximum mutual information for both low and high odor concentrations. However, contradicting all-day experiences, high odor concentrations cause a drastic, and unrealistic, decrease in mutual information for all connectivity rates compared to low concentration. But when inhibition on the mushroom body is included, mutual information remains at high levels independent of other system parameters. This finding points to a pivotal role of inhibition in fly information processing without which the system efficiency will be substantially reduced.",2013,10.3389/fncom.2013.00183,no
Steam Gasification Process of Chlorine-rich Shredder Granules: Experiments and Flow-sheeting Modelling for Process Evaluation and Scale-up,"Gasification technology is gaining more and more importance, due to its engineering property of energy conversion of feedstock material into valuable gaseous process fuel. Using waste material the gasification process appears even more interesting, mostly when an ecological drawback like the production of low-pH and chlorine-rich syngas is turned into a substantial advantage. This is given when the acid gas is used for particular applications such as steel scrap preheating and simultaneous surface cleaning before its utilisation in steel plants. In this paper a lab-scale steam gasification process for the production of a chlorine-rich gas is presented. The produced syngas shows an interesting heating value as well as adequate chlorine content for its utilisation in the mentioned application. The overall process has been evaluated by means of flow-sheeting models to assess its performances in comparison with alternative solutions. Models are intended to calculate mass and energy balances as well as to evaluate the optimum process operating conditions considering the downstream utilisation of the syngas. Results of the models are presented in comparison with experimental data. Finally an outlook is given with regard to possible model applications as guidelines for process scale-up and optimisation.",2013,10.3303/CET1335220,no
A comparative evaluation of stochastic-based inference methods for Gaussian process models,"Gaussian Process (GP) models are extensively used in data analysis given their flexible modeling capabilities and interpretability. The fully Bayesian treatment of GP models is analytically intractable, and therefore it is necessary to resort to either deterministic or stochastic approximations. This paper focuses on stochastic-based inference techniques. After discussing the challenges associated with the fully Bayesian treatment of GP models, a number of inference strategies based on Markov chain Monte Carlo methods are presented and rigorously assessed. In particular, strategies based on efficient parameterizations and efficient proposal mechanisms are extensively compared on simulated and real data on the basis of convergence speed, sampling efficiency, and computational cost.",2013,10.1007/s10994-013-5388-x,no
Modeling and Validation of Heat and Mass Transfer in Individual Coffee Beans during the Coffee Roasting Process Using Computational Fluid Dynamics (CFD),"Heat and mass transfer in individual coffee beans during roasting were simulated using computational fluid dynamics (CFD). Numerical equations for heat and mass transfer inside the coffee bean were solved using the finite volume technique in the commercial CFD code Fluent; the software was complemented with specific user-defined functions (UDFs). To experimentally validate the numerical model, a single coffee bean was placed in a cylindrical glass tube and roasted by a hot air flow, using the identical geometrical 3D configuration and hot air flow conditions as the ones used for numerical simulations. Temperature and humidity calculations obtained with the model were compared with experimental data. The model predicts the actual process quite accurately and represents a useful approach to monitor the coffee roasting process in real time. It provides valuable information on time-resolved process variables that are otherwise difficult to obtain experimentally, but critical to a better understanding of the coffee roasting process at the individual bean level. This includes variables such as time-resolved 3D profiles of bean temperature and moisture content, and temperature profiles of the roasting air in the vicinity of the coffee bean.",2013,10.2533/chimia.2013.291,no
Optical surfacing process optimization using parametric smoothing model for mid-to-high spatial frequency error control,"High performance optical systems aiming for very low background noise from scattering or a sharp point spread function with high encircled energy often specify their beam wavefront quality in terms of a structure function or power spectral density function, which requires a control of mid-to-high spatial frequency surface errors during the optics manufacturing process. Especially for fabrication of large aspheric optics, achieving the required surface figure irregularities over the mid-to-high spatial frequency range becomes a challenging task as the polishing lap needs to be compliant enough to conform to the varying local surface shapes under the lap. This compliance degrades the lap's smoothing capability, which relies on its rigidity. The smoothing effect corrects the mid-to-high spatial frequency errors as a polishing lap removes low spatial frequency (i.e. larger than the lap size) errors on the optical surface. Using a parametric smoothing model developed to quantitatively describe the smoothing effects during Computer Controlled Optical Surfacing (CCOS) processes, actual CCOS data from large aspheric optics fabrication projects have been analyzed and studied. The measured surface error maps were processed with the model to compare different polishing runs using various polishing parameters. The results showing the smoothing effects of mid-to-high spatial frequency surface irregularity will be presented to provide some insights for a CCOS process optimization in terms of smoothing efficiency.",2013,10.1117/12.2028816,no
Steel Property and Process Models for Quality Control and Optimization,"High quality and low variability in the properties of the products are the main goals in manufacturing. The quality of the product is verified by testing different properties. It can be improved with models developed for event prediction. This paper presents with application examples the modelling steps required for effective process modelling. First, the pre-processing and feature extraction phase are illustrated. The modelling phase concentrates especially on the heteroscedasticity problem that is commonly present in industrial applications. The process monitoring and control parameter optimization based on these models is presented, as well as the solution for the lack of observations for the dependent variable. Many of the developed models are in daily use in different process states in steel industry. They enable the design of new products and the analysis of the effects of different process parameters on variability reduction. The proposed methods are application independent.",2013,10.4028/www.scientific.net/MSF.762.301,no
A Comprehensive Wiring Diagram of the Protocerebral Bridge for Visual Information Processing in the Drosophila Brain,"How the brain perceives sensory information and generates meaningful behavior depends critically on its underlying circuitry. The protocerebral bridge (PB) is a major part of the insect central complex (CX), a premotor center that may be analogous to the human basal ganglia. Here, by deconstructing hundreds of PB single neurons and reconstructing them into a common three-dimensional framework, we have constructed a comprehensive map of PB circuits with labeled polarity and predicted directions of information flow. Our analysis reveals a highly ordered information processing system that involves directed information flow among CX subunits through 194 distinct PB neuron types. Circuitry properties such as mirroring, convergence, divergence, tiling, reverberation, and parallel signal propagation were observed; their functional and evolutional significance is discussed. This layout of PB neuronal circuitry may provide guidelines for further investigations on transformation of sensory (e.g., visual) input into locomotor commands in fly brains.",2013,10.1016/j.celrep.2013.04.022,no
"Carbon forestry in West Africa: The politics of models, measures and verification processes","In a context of neo-liberal environmental governance, imperatives for global climate change mitigation are motivating a new round of policy initiatives and projects aimed at carbon forestry: conserving and enhancing forest carbon stocks, and trading these values in emerging carbon markets. In this context modelling and measurement, always significant in framing and justifying forest policy initiatives, are of renewed importance, with a growing array of protocols focused on counting and accounting for forest carbon as a commodity. This article draws on perspectives from science and technology studies and environmental discourse analysis to explore how these modelling and measurement processes are being co-constructed with forest carbon policies and political economies, and applied in project design in local settings. Document analysis and key informant interviews are used to track and illustrate these processes in a pair of case studies of forest carbon projects in Sierra Leone and Ghana. These are chosen to highlight different project types - focused respectively on forest reserve and farm-forestry - in settings with multi-layered histories of people-forest relations, landscape change and prior project intervention. The analysis shows how longer established framings and assessments of deforestation are being reinvoked and re-worked amidst current carbon concerns. We demonstrate that measurement processes are not just technical but social and political, carrying and thus cementing particular views of landscape and social relations that in turn make likely particular kinds of intervention pathway, with fortress style conservation or plantations becoming the dominant approach. In the process, other possibilities including alternative pathways that might treat and value carbon as part of complex, lived-in landscapes, or respond more adaptively to less equilibrial people-forest relations, are occluded. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.gloenvcha.2013.07.008,no
A comprehensive process of reverse engineering from 3D meshes to CAD models,"In an industrial context, most manufactured objects are designed using CAD (Computer-Aided Design) software. For visualization, data exchange or manufacturing applications, the geometric model has to be discretized into a 3D mesh composed of a finite number of vertices and edges. However, the initial model may sometimes be lost or unavailable. In other cases, the 3D discrete representation may be modified, e.g. after numerical simulation, and no longer corresponds to the initial model. A retro-engineering method is then required to reconstruct a 3D continuous representation from the discrete one. In this paper, we present an automatic and comprehensive retro-engineering process dedicated mainly to 3D meshes obtained initially by mechanical object discretization. First, several improvements in automatic detection of geometric primitives from a 3D mesh are presented. Then a new formalism is introduced to define the topology of the object and compute the intersections between primitives. The proposed method is validated on 3D industrial meshes. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.cad.2013.06.004,no
The evaluation of the extended transmission power consumption (ETPC) model to perform communication type processes,"In energy-aware information systems, it is critical to discuss how to select a server for each request from clients in order to not only achieve performance objectives but also reduce the total power consumption of a system. In order to design and evaluate the server selection algorithm, it is critical to define the power consumption model of a server to perform processes. In our previous studies, the transmission power consumption (TPC) model of a server is proposed to perform transmission processes. In the TPC model, the electric power consumption of cooling devices is assumed to be constant. Cooling devices like fans consume the electric power in a server. Thus, the total power consumption of a server depends on not only computation and communication devices but also cooling devices. In this paper, we propose the extended TPC (ETPC) model to take into account the power consumption of cooling devices. We validate the TPC and ETPC models by measuring the power consumption and transmission time of a server.",2013,10.1007/s00607-012-0222-z,no
Performance Evaluation of Academic-Document Writing Process using Software Reliability Models,"In Japan, university education has been required to teach university students ""technical writing skill"" as well as ""basic scholastic achievement"" and ""specialized knowledge"" by the progress of globalization. By focuses on making of the manuscript co-authored in the university laboratory, this paper discusses an improvement of technical writing skill. We educate the technical writing skill of the student as we improve the quality of the study by the academic-document writing process. The problems of this process do not advance on scheduled to depend on the skill of the student. In this paper, we will consider the method of document production support, by representing document production as a software reliability growth model. Also, we will make it a realistic model in order to add incomplete correction to document writing process.",2013,,no
Key Process Variable Identification for Quality Classification Based on PLSR Model and Wrapper Feature Selection,"In modern manufacturing, hundreds of process variables are collected, and it is usually difficult to identify the most informative ones. Partial Least Square Regression provides an efficient way to evaluate each variable, but it cannot evaluate any variable subset as a whole. In the paper, a new framework of key process variable identification is proposed. It combines PLSR model and wrapper feature selection to firstly assess every variable individually and then the top variables in groups. Five datasets are tested, and the average classification accuracy is higher and the key process variables identified are less than the available approaches.",2013,10.1007/978-3-642-33012-4_27,no
Fidelity-Beltrami-Sparsity Model for Inverse Problems in Multichannel Image Processing,"In multichannel image processing, many tasks, such as image denoising, image deblurring, and image inpainting, can be included in the fields of inverse problems and high-dimensional data processing. To get better performance, there are two key points that should be considered seriously. On the one hand, most of these inverse problems are ill-posed, and the prior constraints are necessary to transform them into well-posed problems by reducing the solution space. On the other hand, the information of the coupling among different channels is important in multichannel image processing, where each channel of the image is usually processed separately. In this paper, we propose the fidelity-Beltrami-sparsity (FBS) model for the inverse problems in multichannel image processing, which can simultaneously exploit two important priors, i.e., the smoothness prior and the sparsity prior. As we know, the smoothness prior and the sparsity prior have been widely validated for an image, respectively, so inverse problems with these two priors in image processing are more likely to be well-posed. In addition, the proposed model can also utilize the coupling among different channels of the image by building these constraints on the image manifold. More importantly, the FBS model is numerically calculable, although the model associated with the fidelity term, the smoothness constraint, and the sparsity constraint is not convex or differentiable. Finally, we present the numerical results of the proposed method as well as the comparative results for different inverse problems, namely, image denoising, image deblurring, and image inpainting, in multichannel image processing. Quantitative comparisons by the peak signal-to-noise ratio as well as the visual effect indicate the better performance of the proposed method.",2013,10.1137/120862168,no
A Comparative Study of Modelling Methodologies Using a Concept of Process Consistency,"In our research we are interested in a comparative study of application modelling methodologies for analyzing non trivial and hard-to-reach properties of the business processes. Our attention was attracted to such a non trivial property as the consistency of business processes. Poor consistency of business processes can affect company profits and lead to the loss of regular customers and reputation in the market.. Checking this property of the business process helps to reveal hidden bugs in the process model, but requires considerable labor costs and analytics. In our research we compared two approaches to verifying consistency of business processes. The first approach is based on generating object life cycles for each object type used in the process and supported by such special tool as an extension for IBM WebSphere Business Modeler. Another one is a proposition to use DEMO methodology for verifying consistency. The results of the research show that DEMO methodology significantly reduces labor costs and improves quality of analysis.",2013,,no
"Two EPQ models with imperfect production processes, inspection errors, planned backorders, and sales returns","In practice the items received in a lot may contain defective items, and during the screening process to eliminate the defective items, the inspector may incorrectly classify a non-defective item as defective (a Type I error) or incorrectly classify a defective item as non-defective (a Type II error). In this paper, we develop two economic production quantity models with imperfect production processes, inspection errors, planned backorders, and sales returns. A closed form solution is obtained for the optimal production lot size and the maximum shortage level for both models. We provide two numerical examples, one in which the defective probability and Type I and Type II inspection errors follow uniform distributions, and the second in which we assume they follow beta distributions. Sensitivity analyses are performed to see the impact of the defective probability, the probability of the Type I inspection error, the probability of the Type II inspection error, the holding cost, and the backordering cost on the optimal solutions. We obtain similar results on the sensitivity analyses for both numerical examples. The results show that the time factor of when to sell the defective items has a significant impact on the optimal production lot size and the backorder quantity. The results also show that if customers are willing to wait for the next production when a shortage occurs, it is profitable for the company to have planned backorders although it incurs a penalty cost for the delay. (C) 2012 Elsevier Ltd. All rights reserved.",2013,10.1016/j.cie.2012.10.005,no
Consistency verification of UML diagrams based on process bisimulation,"In the development of a software system using UML, consistency between state machine diagrams and sequence diagrams is crucial. This study proposes a verification method for the consistency of a sequence diagram and state machine diagrams. The proposed method represents state machine diagrams and a sequence diagram as processes, and can verify the consistency by checking weak simulation of the processes. We confirms the method could detect inconsistency with an example.",2013,10.1109/PRDC.2013.25,no
Selectivity problem for fine chemical reactions leading to non volatile products: Process configuration and boundary diagrams,"In the fine chemical and pharma industry a number of reaction processes undergo selectivity losses since the target product (typically a low volatility compound) can further react with one of the reactants leading to by-products that must be separated from the target product itself and finally disposed as wastes. In such a situation, even performing the reaction in a standard semi-batch reactor requires a large excess of one reactant to limit the selectivity drop. However, operating the reactor with a large excess of one reactant lowers the reactor productivity. In this work, a process configuration is discussed that, limiting the target product accumulation in the reactor, allows achieving both high process selectivity and high reactor productivity. Moreover, general and easy to use boundary diagrams have been developed, through which the optimal process operating parameters can be easily estimated without solving the mathematical model of the system. Several boundary diagrams are provided for different systems together with a general procedure for their use. (C) 2012 Elsevier Ltd. All rights reserved.",2013,10.1016/j.ces.2012.11.037,no
Incorporation of Physiologically Based Pharmacokinetic Modeling in the Evaluation of Solubility Requirements for the Salt Selection Process: A Case Study Using Phenytoin,"In the pharmaceutical industry, salt is commonly used to improve the oral bioavailability of poorly soluble compounds. Currently, there is a limited understanding on the solubility requirement for salts that will translate to improvement in oral exposure. Despite the obvious need, there is very little research reported in this area mainly due to the complexity of such a system. To our knowledge, no report has been published to guide this important process and salt solubility requirement still remains unanswered. Physiologically based pharmacokinetic (PBPK) modeling offers a means to dynamically integrate the complex interplay of the processes determining oral absorption. A sensitivity analysis was performed using a PBPK model describing phenytoin to determine a solubility requirement for phenytoin salts needed to achieve optimal oral bioavailability for a given dose. Based on the analysis, it is predicted that phenytoin salts with solubility greater than 0.3 mg/mL would show no further increases in oral bioavailability. A salt screen was performed using a variety of phenytoin salts. The piperazine and sodium salts showed the lowest and highest aqueous solubility and were tested in vivo. Consistent with our analysis, we observed no significant differences in oral bioavailability for these two salts despite an approximate 60 fold difference in solubility. Our study illustrates that higher solubility salts sometimes provide no additional improvements in oral bioavailability and PBPK modeling can be utilized as an important tool to provide guidance to the salt selection and define a salt solubility requirement.",2013,10.1208/s12248-013-9519-x,no
Modeling and evaluation of the filling process of vacuum-assisted compression resin transfer molding,"In the present study, a modified vacuum-assisted compression resin transfer molding (VACRTM) process has been developed to reduce the cycling period. The process uses an elastic bag placed between the upper mold and the preform to replace the mobile rigid mold in compression resin transfer molding. During resin injection, the bag is pulled upward by the vacuum applied in between the upper mold and the bag, and a loose fiber stack is then present. Resin is easily injected into the mold. Once enough volume of resin is injected, the compression pressure is applied on the bag, which compacts the preform and drives the resin through the remaining dry preform. Numerical results show that the bag compression phase is much longer than the resin injection one. A multistage compression strategy can be used to control the compression time. Due to inherent process defects, a higher volume of the injected liquid is essential and thus leads to a longer injection and compression phase in order to inject and squeeze the excess resin. The late compression is very slow in draining the residual resin. As compared with resin transfer molding, VACRTM can reduce the mold-filling time/injection pressure.",2013,10.1515/polyeng-2012-0160,no
Mathematical and numerical modeling of the effect of input-parameters on the flushing efficiency of plasma channel in EDM process,"In the present study, the temperature distribution on the surface of workpiece and tool during a single discharge in the electrical discharge machining process has been simulated using ABAQUS code finite element software. The temperature dependency of material properties and the expanding of plasma channel radius with time have been employed in the simulation stage. The profile of temperature distribution has been utilized to calculate the dimensions of discharge crater. Based on the results of FEM and the experimental observations, a numerical analysis has been developed assessing the contribution of input-parameters on the efficiency of plasma channel in removing the molten material from molten puddles on the surfaces of workpiece and tool at the end of each discharge. The results show that the increase in the pulse current and pulse on-time have converse effects on the plasma flushing efficiency, as it increases by the prior one and decreases by the latter one. Later, the introduced formulas for plasma flushing efficiency based on regression model were utilized to predict the cardinal parameter of recast layer thickness on the electrodes which demands expensive empirical tests to be obtained. (C) 2012 Elsevier Ltd. All rights reserved.",2013,10.1016/j.ijmachtools.2012.10.004,no
Prediction of quality responses in micro-EDM process using an adaptive neuro-fuzzy inference system (ANFIS) model,"In the present trend of technological development, micro-machining is gaining popularity in the miniaturization of industrial products. In this work, a hybrid process of micro-wire electrical discharge grinding and micro-electrical discharge machining (EDM) is used in order to minimize inaccuracies due to clamping and damage during transfer of electrodes. The adaptive neuro-fuzzy inference system (ANFIS) and back propagation (BP)-based artificial neural network (ANN) models have been developed for the prediction of multiple quality responses in micro-EDM operations. Feed rate, capacitance, gap voltage, and threshold values were taken as the input parameters and metal removal rate, surface roughness and tool wear ratio as the output parameters. The results obtained from the ANFIS and the BP-based ANN models were compared with observed values. It is found that the predicted values of the responses are in good agreement with the experimental values and it is also observed that the ANFIS model outperforms BP-based ANN.",2013,10.1007/s00170-013-4731-5,no
Evaluation of Structural Changes in the Coal Specimen Heating Process and UCG Model Experiments for Developing Efficient UCG Systems,"In the underground coal gasification (UCG) process, cavity growth with crack extension inside the coal seam is an important phenomenon that directly influences gasification efficiency. An efficient and environmentally friendly UCG system also relies upon the precise control and evaluation of the gasification zone. This paper presents details of laboratory studies undertaken to evaluate structural changes that occur inside the coal under thermal stress and to evaluate underground coal-oxygen gasification simulated in an ex-situ reactor. The effects of feed temperature, the direction of the stratified plane, and the inherent microcracks on the coal fracture and crack extension were investigated using some heating experiments performed using plate-shaped and cylindrical coal specimens. To monitor the failure process and to measure the microcrack distribution inside the coal specimen before and after heating, acoustic emission (AE) analysis and X-ray CT were applied. We also introduce a laboratory-scale UCG model experiment conducted with set design and operating parameters. The temperature profiles, AE activities, product gas concentration as well as the gasifier weight lossess were measured successively during gasification. The product gas mainly comprised combustible components such as CO, CH4, and H-2 (27.5, 5.5, and 17.2 vol% respectively), which produced a high average calorific value (9.1 MJ/m(3)).",2013,10.3390/en6052386,no
On the Angular Resolution Limit for Array Processing in the Presence of Modeling Errors,"In this correspondence, we study the impact of modeling errors on the angular resolution limit (ARL) for two closely spaced sources in the context of array processing. Particularly, we follow two methods based on the well-known Lee and Smith's criteria using the Cramer-Rao lower bound (CRB) to derive closed-form expressions of the ARL with respect to (w.r.t.) the error variance. We show that, as the signal-to-noise ratio increases, the ARL does not fall into zero (contrary to the classical case without modeling errors) and converge to a fixed limit depending on the method for which we give a closed-form expression. One can see that at high SNR, the ARL in the Lee and Smith sense are linear as a function of the error variance. We also investigate the influence of the array geometry on the ARL based on the Lee's and Smith's criteria.",2013,10.1109/TSP.2013.2269900,no
"Comprehensive approach for process modeling and optimization in cold forging considering interactions between process, tool and press","In this paper a comprehensive approach is presented for the consideration of the interactions between process, tool and machine during the design of cold forging tools and processes by simulation. The interactions occur due to the high forming loads in cold forging and yield considerable deflections of press and tooling system. These, in turn, influence the workpiece dimensions. The entire approach comprises an efficient determination of the deflection characteristic of stroke-controlled press and tooling system and its condensed modeling in combination with the FE simulation of a cold forging process. Building on that, an analytic process model is developed that is based on a set of variant simulations. It permits an optimization of the values of influencing parameters to achieve high workpiece accuracy without subsequent adjusting effort. Initially, the analytic process model required a high number of variant simulations. By acquiring knowledge on the specific process behavior in an analysis of effects and interactions a considerable reduction of simulation runs by a factor of almost 12 was achieved in the case study on full forward extrusion. The approach is supplemented by an analytic model of the die load. In addition, scatter and uncertainties of target values depending on the ones of the influencing parameters can be estimated by applying the Monte Carlo method to the analytic process model. (C) 2013 Published by Elsevier B.V.",2013,10.1016/j.jmatprotec.2012.09.004,no
Business Process Modeling and the Robust PNS Problem,"In this paper we define and investigate a new direction of the P-graph-based Business Process Modeling which we call the robust PNS problem. We consider the model where for each operating unit two costs are given, it has a nominal cost and an extended cost, and we know that at most b operating units have the extended cost, the others will have the nominal cost. We present a branch and bound based exact solution algorithm for the general problem, and a faster, polynomial time dynamic programming algorithm for the case of the hierarchycal problems.",2013,,no
Quest for Efficiency: Examining Cognitive Processes Underlying the Use of 3D Modeling Tools,"In this paper we examine the strategies used in 3D modeling for their efficiency. Our study explores the underlying cognitive process that drives design thinking as well the choice of strategies for using specific features in a given CAD software. We take a cognitive task analysis approach to examine our question. Of a total sample of 19 participants, the strategies of the fastest and slowest users are compared to identify areas of improvement for software development as well as user training.",2013,,no
A New Numerical Simulation Model for Shrinkage Defect during Squeeze Casting Solidification Process,"In this paper, according to the characteristics of squeeze casting solidification process, the calculation model (FDM format) of the partial differential equations with high thermal conductivity is used to the numerical simulation of temperature field. Dynamic isolated multi-molten pool judgment method is used to determine the position of the pool and FEM is used to calculate the pressure of pool center. If the pressure of molten pool center has been down to 0, the liquid metal closed in the scale will be solidification under the condition of no pressure, and will shrinkage based on the way of gravity shrinkage. The equivalent liquid surface descending method of isolated molten pool is used to predict the formation of shrinkage defect; the simulation result is coinciding with experimental data.",2013,10.4028/www.scientific.net/AMR.641-642.309,no
EFFICIENCY MEASURING OF THE PRODUCTION UNITS IN EDUCATION PROCESS WITH CCR DEA MODEL,"In this paper, an impact of the framing effect is analyzed. The effectiveness of selected decisions, made by decision-makers with different frames on the issue of the education, is analyzed. This analysis is calculated by Data Envelopment Analysis (DEA) method. Decision making units (DMUs) correspond to the teaching courses. DMUs are distributed at the discretion of decision-makers in individual frames of the preferences and the expectations. In each of these frames various forms of the teaching are used. The aim of this paper is to assess the effectiveness of each course and to show, how can the individual frames affect the efficiency of decisions and thereby influence the decision-makers' overall benefit.",2013,,no
Comprehensive Modeling and Analysis of Copper Wire Bonding Process,"In this paper, the copper property under various ultrasonic powers was firstly investigated by experiment. It is found the ultrasound has an acoustic softening effect on copper free air balls (FABs). The stress-strain behavior of copper FABs that accounted for acoustic softening effect was achieved by combining experiment and numerical simulation. Then a non-linear finite element (FE) model was built to study the copper wire bonding process. The model considered the sliding status transition at the bonding stage and can give an accurate prediction on the catering issue. Finally, one case study about the BAR effect on the bonding stress was conducted. The bonding stress can be reduced by optimizing the BAR value of the bonded ball.",2013,,no
A Transient Model of Lightning Breakdown Process Based on Photographic Measurements,"In this paper, the positive pre-discharge under lightning voltage impulses has been analyzed. The standard lightning voltages up to (u) over cap = 2.4 MV have been applied to the air gaps up to s = 4 m. The pre-discharge current of rod-electrode in mA-range has been measured. Additionally, during the very short time that the pre-discharges are growing in the atmospheric air (mu s-rang), some high speed photos have been recorded from the instantaneous structure of pre-discharge by a high speed camera. Based on the taken photos, the electric charge distribution has been calculated. As the main goal, the charge simulation method has been modified to calculate the electric field with the existence of the spatial electric charge, which has been produced by the ionization and deionization (recombination) activities during the development of pre-discharge in the atmospheric air. The variation of the electric field during the pre-discharge (e.g. during a period of about 200 ns) has been calculated. The simulations demonstrate plausible results and verify the dependency of the pre-discharge development velocity in the air on the electric field.",2013,,no
Empirical likelihood confidence regions for semi-varying coefficient models with linear process errors,"In this paper, we apply the empirical likelihood method to semi-varying coefficient models with linear process errors, propose two empirical log-likelihood ratio statistics and show that their limiting distributions are two weighted sums of independent chi-square distributions with 1 degree of freedom. By estimating the unknown weights consistently, we not only construct the empirical likelihood confidence regions for the parametric component, but also construct the point-wise empirical likelihood confidence regions and the simultaneous empirical likelihood confidence bands for the varying-coefficient functions. Monte Carlo simulation results show that the proposed empirical likelihood confidence regions have better coverage probabilities and shorter median lengths than the normal approximation confidence regions ignoring the correlation information.",2013,10.1080/10485252.2012.751385,no
Application of DEA-DA Model and Method on Quality Control in Production Process,"In this paper, we conducted intensive research on the application of DEA-DA model and method on quality control in production process, and put forward the applied train of thought and the measure. Otherwise we chose 10 kinds of equipment to verify the model. At last, we put forward the measure to distinguish the well quality product from newcome ones and determined a newcome sample's quality well or not.",2013,10.4028/www.scientific.net/AMM.300-301.1529,no
Dynamic performance modelling and measuring for machine tools with continuous-state wear processes,"In this paper, we consider the problem of using empirical continuous-state wear data of machine tools to estimate the dynamic lifetime distribution and to measure the performance of a machining process subject to stochastic tool-wear evolution. Machining systems are dynamic processes whose performance variable is usually characterised by the amount of tool wear that advances gradually with a continuous range of values. To accurately capture the performance of these continuous-state wear processes, neither traditional models such as the binary-state models nor multi-state models are suitable. In this paper, an exponential mixed-effects (EME) model is first developed. The EME model is subsequently transformed into a linear mixed-effects (LME) model to enhance the fit and predictability of the wear process data. The LME models take into consideration the correlations among repeated wear measurements collected at different time points within each subject. We then implement the expectation-maximisation (EM) algorithm to obtain the full maximum likelihood estimates (MLEs) of the parameters of the LME models whose asymptotic normal distributions can be used to acquire approximate confidence intervals and a testing hypothesis for the parameters. In addition, to measure the dynamic performance of tools, the amount of wear over time estimated from LME models is compared with a given tool-failure threshold. Consequently, we obtain the reliability of the tool and the estimation of its residual-lifetime distribution, which is critical information for the tool replacement or maintenance strategy. Finally, the lower and upper wear prediction limits of the 95% confidence level are presented. A practical application of the proposed methodology is illustrated throughout the paper.",2013,10.1080/00207543.2013.793858,no
Software Reliability Modeling with Software Metrics Data via Gaussian Processes,"In this paper, we describe statistical inference and prediction for software reliability models in the presence of covariate information. Specifically, we develop a semiparametric, Bayesian model using Gaussian processes to estimate the numbers of software failures over various time periods when it is assumed that the software is changed after each time period and that software metrics information is available after each update. Model comparison is also carried out using the deviance information criterion, and predictive inferences on future failures are shown. Real-life examples are presented to illustrate the approach.",2013,10.1109/TSE.2012.87,no
MODELLING AND EVALUATION OF DETERIORATION PROCESS WITH MAINTENANCE ACTIVITIES,"In this paper, we present an approach which allows evaluation of various possible maintenance scenarios with respect to both reliability and economic criteria. The method is based on the concept of a life curve and discounted cost used to study the effect of equipment aging under different maintenance strategies. The deterioration process is first described by a Markov model and then its various characteristics are used to develop the equipment life curve and to quantify other reliability parameters. Based on these data, effects of various ""what-if"" maintenance scenarios can be examined and their efficiency compared. Simple life curves are combined to model equipment deterioration undergoing diverse maintenance actions, while computing other parameters of the model allows evaluation of additional critical factors, such as the probability of equipment failure. Additionally, the paper deals with the problem of the model adjustment so that the computed repair frequencies are close to the historical values, which is very important in practical applications of the method. Moreover, we discuss the problems which may arise if automatic adjustment is used in cases when the hypothetical maintenance policies go beyond the conditions upon which the original model was built.",2013,,no
Empirical Likelihood for Nonparametric Models Under Linear Process Errors,"In this paper, we study the construction of confidence intervals for a nonparametric regression function under linear process errors by using the blockwise technique. It is shown that the blockwise empirical likelihood (EL) ratio statistic is asymptotically 2 distributed. The result is used to obtain EL based confidence intervals for the nonparametric regression function. The finite-sample performance of the method is evaluated through a simulation study.",2013,10.1111/anzs.12022,no
Optimal excess-of-loss reinsurance and investment problem for an insurer with jump-diffusion risk process under the Heston model,"In this paper, we study the optimal excess-of-loss reinsurance and investment problem for an insurer with jump-diffusion risk model. The insurer is allowed to purchase reinsurance and invest in one risk-free asset and one risky asset whose price process satisfies the Heston model. The objective of the insurer is to maximize the expected exponential utility of terminal wealth. By applying stochastic optimal control approach, we obtain the optimal strategy and value function explicitly. In addition, a verification theorem is provided and the properties of the optimal strategy are discussed. Finally, we present a numerical example to illustrate the effects of model parameters on the optimal investment-reinsurance strategy and the optimal value function. (C) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.insmatheco.2013.08.004,no
CFD modeling and its validation of non-Newtonian fluid flow in a microparticle production process using fan jet nozzles,"In this paper, we use computational fluid dynamics for the design of microcapsules for drug release with biomedical purposes. The microcapsules are generated using a nozzle with a particular design where the air is injected in a chamber in such a way that focuses the liquid flow through the orifice breaking and dispersing the liquid solution. The typical solutions used for microcapsule generation are polymeric, in this work alginate-based, whose theological behavior is non-Newtonian. The liquid is dispersed into liquid drops using air at high velocity. We record the process using high-speed video techniques and determine the mean drop size in order to validate the results obtained by the model. The predictions using CFD techniques are promising as a mean to speed up the particle design process. (C) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.powtec.2013.05.050,no
Evaluation of dynamic membrane formation and filtration models at constant pressure in a combined coagulation/dynamic membrane process in treating polluted river water,"In this study, a dynamic membrane (DM) was formed by flocs produced by river water pollutants reacting with polyaluminum chloride (PAC). The DM was used to separate the flocs from the mixed liquor at constant pressure. Two support media for the DM were tested: non-woven fabric filter and dacron mesh. The formation time of the DM was determined according to the critical flux. The formation process and filtration mechanisms were evaluated based on typical crossflow membrane fouling models and combinations of them. When a non-woven fabric filter was used as the support media, the formation time of the DM was 70 min at 4 cm water head drop (WHD) and 20 min at 8 cm WHD. When dacron mesh was used as the support media, the membrane formation required 100 min at 4 cm WHD and 70 min at 8 cm WHD. During DM formation on a non-woven fabric filter, the internal pore blocking plays the dominant role. But when dacron mesh is used as the support media, complete blocking and intermediate blocking dominate. After the formation of a DM, the combined models have good agreement with the experimental data compared with the typical filtration models.",2013,10.2166/ws.2013.599,no
Analysis of the effect of a new process control agent technique on the mechanical milling process using a neural network model: Measurement and modeling,"In this study, a new process control agent (PCA) technique called as gradual process control agent technique was developed and the new technique was compared with conventional process control agent technique. In addition, a neural network (ANN) approach was presented for the prediction of effect of gradual process control agent technique on the mechanical milling process. The structural evolution and morphology of powders were investigated using SEM and particle size analyzer techniques. The experimental results were used to train feed forward and back propagation learning algorithm with two hidden layers. The four input parameters in the proposed ANN were the milling time, the gradual PCA content, previous PCA content and gradual PCA content. The particle size was the output obtained from the proposed ANN. By comparing the predicted values with the experimental data it is demonstrated that the ANN is a useful, efficient and reliable method to determine the effect of gradual process control agent technique on the mechanical milling process. Crown Copyright (C) 2013 Published by Elsevier Ltd. All rights reserved.",2013,10.1016/j.measurement.2013.02.005,no
Experimental validation of a 2-D population balance model for spray coating processes,"In this study, a series of spray coating experiments were carried out in a pilot scale Forberg-style paddle mixer to validate a previously published multi-dimensional population balance (PB) model (Li et al., 2011). Experiments were carried out using unimodal distributions of large (mean mass 0.45 g) and small (mean mass 0.27 g) particles, as well as bimodal distributions with a 1:1 mass ratio of the large and small particles. Experiments were performed at Froude numbers from 0.6 to 2 with the final coating mass to core particle mass ratio being 0.05. A fully two-dimensional distribution of the coated particles, with respect to solid core mass and coating mass, was obtained for each batch operation. For unimodal core particle distributions, the coating mass coefficient of variation was proportional to t(-0.5) where t is the coating time, consistent with both the PB model and other published coating models. For the bimodal experimental series, the coating growth rate varied with particle mass m according to the relationship G=m(r) where r=0.37. This value for the growth expression exponent r is smaller than that predicted for random coating (r=2/3) or observed experimentally in fluidized coating systems (r=1). This preferential spraying of small particles in the mechanical mixer indicates that the size dependence of coating mass growth rate is a strong function of the equipment geometry and particle flow field. When a growth exponent r=0.37 was used, the coating distribution prediction from a discrete element method-population balance simulation compares well with the experimental results. Published by Elsevier Ltd.",2013,10.1016/j.ces.2012.02.036,no
CFD modeling to predict diffused date syrup yield and quality from sugar production process,"In this study, diffusion process of sugar from date is modeled using a commercial computational fluids dynamics (CFD) code FLUENT 6.3.23 (Fluent Inc., USA). A two phases CFD model was developed using an Eulerian-Eulerian approach to calculate the date volume fraction transferred during time from date phase to water phase. The diffusion process was studied as function of three date varieties (Manakher, Lemsi and Alligue), three speeds of agitation (0, 50 and 100 rpm) and three date/water ratio (0.25, 0.50 and 0.75). The results revealed that, for mass transfer, the numerical data were in good agreement with the experimental data indicating the R-2 of 0.84. Using a Lemsi date variety, the optimal condition of diffusion were 50 rpm and 0.75 for speed of agitation and date/water ratio respectively. (c) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.jfoodeng.2013.04.011,no
Metrics and Diagnostics for Precipitation-Related Processes in Climate Model Short-Range Hindcasts,"In this study, several metrics and diagnostics are proposed and implemented to systematically explore and diagnose climate model biases in short-range hindcasts and quantify how fast hindcast biases approach to climate biases with an emphasis on tropical precipitation and associated moist processes. A series of 6-day hindcasts with NCAR and the U.S. Department of Energy Community Atmosphere Model, version 4 (CAM4) and version 5 (CAM5), were performed and initialized with ECMWF operational analysis every day at 0000 UTC during the Year of Tropical Convection (YOTC). An Atmospheric Model Intercomparison Project (AMIP) type of ensemble climate simulations was also conducted for the same period. The analyses indicate that initial drifts in precipitation and associated moisture processes (""fast processes"") can be identified in the hindcasts, and the biases share great resemblance to those in the climate runs. Comparing to Tropical Rainfall Measuring Mission (TRMM) observations, model hindcasts produce too high a probability of low-to intermediate-intensity precipitation at daily time scales during northern summers, which is consistent with too frequently triggered convection by its deep convection scheme. For intense precipitation events (>25 mm day(-1)), however, the model produces a much lower probability partially because the model requires a much higher column relative humidity than observations to produce similar precipitation intensity as indicated by the proposed diagnostics. Regional analysis on precipitation bias in the hindcasts is also performed for two selected locations where most contemporary climate models show the same sign of bias. Based on moist static energy diagnostics, the results suggest that the biases in the moisture and temperature fields near the surface and in the lower and middle troposphere are primarily responsible for precipitation biases. These analyses demonstrate the usefulness of these metrics and diagnostics to diagnose climate model biases.",2013,10.1175/JCLI-D-12-00235.1,no
Validation of Formability of Laminated Sheet Metal for Deep Drawing Process using GTN Damage Model,"In this study, we studied formability of PET/PVC laminated sheet metal which named VCM (Vinyl Coated Metal). VCM offers various patterns and good-looking metal steel used for appliances such as refrigerator and washing machine. But, this sheet has problems which are crack and peeling of film when the material is formed by deep drawing process. To predict the problems, we used finite element method and GTN (Gurson-Tvergaard-Needleman) damage model to represent damage of material. We divided the VCM into 3 layers (PET film, adhesive and steel added PVC) in finite element analysis model to express the crack and peeling phenomenon. The material properties of each layer are determined by reverse engineering based on tensile test result. Furthermore, we performed the simple rectangular deep drawing and simulated it. The simulation result shows good agreement with drawing experiment result in position, punch stroke of crack occurrence. Also, we studied the fracture mechanism of PET film on VCM by comparing the width direction strain of metal and PET film.",2013,10.1063/1.4850047,no
Granule size distribution for a multi-chamber fluidized-bed melt granulator: Modeling and validation using process measurement data,"In this work, a steady-state model of a multi-chamber fluidized-bed granulator used for urea production is developed and validated. To this aim, mass, energy and population balances are solved for all the fluidized beds. Regarding the population balance equation (PBE), pure coating or the combined mechanisms of coating and elutriation are taken into account. Both PBE formulations are analytically solved and a new solution methodology is proposed to handle inlet solid streams distributed in different size grids and to minimize the solution errors propagation expected when a set of PBEs in series has to be solved. By comparison with experimental data, it is found that the model including coating and elutriation gives a better representation of the particles size distribution with respect to the results found when pure coating is assumed. Besides, the results indicate that the fines are removed almost completely in the first and second chambers, being the amount of fines in the subsequent chambers negligible. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.ces.2013.08.012,no
Evaluation of Variational Principle Based Model for LDPE Large Scale Film Blowing Process,"In this work, variational principle based film blowing model combined with Pearson and Petrie formulation, considering non-isothermal processing conditions and novel generalized Newtonian model allowing to capture steady shear and uniaxial extensional viscosities has been validated by using experimentally determined bubble shape and velocity profile for LDPE sample on large scale film blowing line. It has been revealed that the minute change in the flow activation energy can significantly influence the film stretching level.",2013,10.1063/1.4802607,no
Representation in Nonelected Participatory Processes: How Residents Understand the Role of Nonprofit Community-based Organizations,"Increasing public participation in administrative decision making has become an important goal in many public agencies. To determine who should participate in an open decision-making process, many public agencies develop purposive selection strategies to counteract the obvious biases that arise from self-selection. To accomplish this, leaders of nonprofit organizations, assumed to represent the views of the residents they serve, are often asked to serve as community representatives in a variety of participatory processes. As these nonelected representatives are not formally authorized or accountable, more knowledge is needed about the degree to which residents perceive this representation as legitimate. We use a mixed-methods approach to address whether residents believe nonprofit community-based organizations can play a legitimate representational role, what organizational characteristics they see as contributing to legitimate representation, and whether they view certain types of organizations as better embodying these characteristics. Findings reveal that residents believe nonprofit community-based organizations could make good representatives. Perceptions of the representatives communication process and ability to produce tangible outcomes led to increased trust of a potential representative, a crucial precursor to legitimate nonelected representation. Although more research is needed to confirm whether these views are broadly generalizable, these findings begin to provide guidance to agencies seeking legitimate community representatives to serve in participatory processes.",2013,10.1093/jopart/mus043,no
Modeling and validation of deformation process for incremental sheet forming,"Incremental Sheet Forming (ISF) is an emerging sheet metal prototyping technology where a part is formed as one or more stylus tools are moving in a pre-determined path and deforming the sheet metal locally while the sheet blank is clamped along its periphery. A deformation analysis of incremental forming process is presented in this paper. The analysis includes the development of an analytical model for strain distributions based on part geometry and tool paths, numerical simulations of the forming process with LS-DYNA, and experimental validation of strain predictions using Digital Image Correlation (DIC) techniques. Three kinds of parts include hyperbolic cone, skew cone and elliptical cone are constructed and used as examples for the study. Analytical, numerical and experimental results are compared, and excellent correlations are found. It is demonstrated that the analytical model developed in this paper is reliable and efficient in the prediction of strain distributions for incremental forming process. (C) 2013 Published by Elsevier Ltd on behalf of The Society of Manufacturing Engineers.",2013,10.1016/j.jmapro.2013.01.003,no
Use of the Extended Parallel Processing Model to Evaluate Culturally Relevant Kernicterus Messages,"Introduction: Kernicterus is a serious but easily preventable disease in newborns that is not well-known even by some health care professionals. This study evaluated a parent guide and poster on kernicterus awareness and prevention generated by the Centers for Disease Control and Prevention. The Extended Parallel Processing Model was used as a framework for creating the interview protocol and analyzing the results. Method: In-depth interviews were conducted with four parents and six health care personnel of different ethnicities to evaluate the materials. Content for the parent guide and poster was held constant, but photos were varied according to the ethnicity of the baby (white, African American, or Hispanic) and the language in which the interviews were conducted (English and Spanish). Results: The parent guide was evaluated positively, but reactions to the poster were varied. The consensus was that the poster drew more attention than the pocket guide but lacked sufficient information about what jaundice is or how to treat it, while the pocket guide provided information, especially with regard to efficacy. The Extended Parallel Processing Model claims that when efficacy is equal to or higher than perceived threat, respondents should engage in recommended responses, which was the general finding from these interviews. Discussion: Recommendations for improvements of the materials are presented. The focus on different ethnicities in the materials was perceived as unnecessary and potentially counter-productive. Both parents and health care professionals mentioned the lack of information regarding treatment. Providing information on the length and effectiveness of treatment for jaundice and kernicterus might increase efficacy in averting the threat in both conditions. J Pediatr Health Care. (2013) 27, 33-40.",2013,10.1016/j.pedhc.2011.06.003,no
Three-Stage Decomposition Modeling for Quality of Gas-Phase Polyethylene Process Based on Adaptive Hinging Hyperplanes and Impulse Response Template,"It is difficult to establish a dynamic model of quality variables in polymerization, because of the high complexity of the nonlinear process behavior and time-consuming laboratory quality measurements. A decomposition scheme is presented in this paper to model the dynamic nonlinear behaviors of the gas-phase polyethylene process; by decomposing the overall process based on its structure into three relatively simpler problems. However, difficulties still come from nonlinearity and nonuniform sampling. Especially, the samples of the product quality in the transitional process are too deficient for traditional data-driven methods. In order to further overcome the difficulties incurred by nonlinearity, the adaptive hinging hyperplanes (AHH) model is adopted. Meanwhile, an impulse response template (IRT) is introduced to model the accumulation process dynamics to deal with the scarcity of sampled data. The proposed model is validated to be effective in comparison with black-box models on the melt index data of 10 transitional processes.",2013,10.1021/ie303370x,no
Accounting for attribute non-attendance and common-metric aggregation in a probabilistic decision process mixed multinomial logit model: a warning on potential confounding,"Latent class models offer an alternative perspective to the popular mixed logit form, replacing the continuous distribution with a discrete distribution in which preference heterogeneity is captured by membership of distinct classes of utility description. Within each class, preference homogeneity is usually assumed, although interactions with observed contextual effects are permissible. A natural extension of the fixed parameter latent class model is a random parameter latent class model which allows for another layer of preference heterogeneity within each class. A further extension is to overlay attribute processing rules such as attribute non-attendance (ANA) and aggregation of common-metric attributes (ACMA). This paper sets out the random parameter latent class model with ANA and ACMA, and illustrates its application using a stated choice data set in the context of car commuters and non-commuters choosing amongst alternative packages of travel times and costs pivoted around a recent trip in Australia. What we find is that for the particular data set analysed, in the presence of attribute processing together with the discrete distributions defined by latent classes, that adding an additional layer of heterogeneity through random parameters within a latent class only very marginally improves on the statistical contribution of the model. Nearly all of the additional fit over the fixed parameter latent class model is added by the account for attribute processing. This is an important finding that might suggest the role that attribute processing rules play in accommodating attribute heterogeneity, and that random parameters within class are essentially a potentially confounding effect. An interesting finding, however, is that the introduction of random parameters increases the probability of membership to full attribute attendance classes, which may suggest that some individuals assign a very low marginal disutility (but not zero) to specific attributes or that there are very small differences in the marginal disutility of common-metric attributes, and this is being accommodated by random parameters, but not observed under a fixed parameter latent class model.",2013,10.1007/s11116-012-9447-0,no
Evaluation of linearly solvable Markov decision process with dynamic model learning in a mobile robot navigation task,"Linearly solvable Markov Decision Process (LMDP) is a class of optimal control problem in which the Bellman's equation can be converted into a linear equation by an exponential transformation of the state value function (Tociorov, 2009h). In an LMDB the optimal value function and the corresponding control policy are obtained by solving an eigenvalue problem in a discrete state space or an eigenfunction problem in a continuous state using the knowledge of the system dynamics and the action, state, and terminal cost functions. In this study, we evaluate the effectiveness of the LMDP framework in real robot control, in which the dynamics of the body and the environment have to be learned from experience. We first perform a simulation study of a pole swing-up task to evaluate the effect of the accuracy of the learned dynamics model on the derived the action policy. The result shows that a crude linear approximation of the non-linear dynamics can still allow solution of the task, despite with a higher total cost. We then perform real robot experiments of a battery-catching task using our Spring Dog mobile robot platform. The state is given by the position and the size of a battery in its camera view and two neck joint angles. The action is the velocities of two wheels, while the neck joints were controlled by a visual servo controller. We test linear and bilinear dynamic models in tasks with quadratic and Guassian state cost functions. In the quadratic cost task, the LMDP controller derived from a learned linear dynamics model performed equivalently with the optimal linear quadratic regulator (LOB). In the non-quadratic task, the LMDP controller with a linear dynamics model showed the best performance. The results demonstrate the usefulness of the LMDP framework in real robot control even when simple linear models are used for dynamics learning.",2013,10.3389/fnbot.2013.00007,no
Testing the realism of model structures to identify karst system processes using water quality and quantity signatures,"Many hydrological systems exhibit complex subsurface flow and storage behavior. Runoff observations often only provide insufficient information for unique process identification. Quantitative modeling of water and solute fluxes presents a potentially more powerful avenue to explore whether hypotheses about system functioning can be rejected or conditionally accepted. In this study we developed and tested four hydrological model structures, based on different hypotheses about subsurface flow and storage behavior, to identify the functioning of a large Mediterranean karst system. Using eight different system signatures, i.e., indicators of particular hydrodynamic and hydrochemical characteristics of the karst system, we applied a novel model evaluation strategy to identify the best conceptual model representation of the karst system within our set of possible system representations. Our approach to test model realism consists of three stages: (1) evaluation of model performance with respect to system signatures using automatic calibration, (2) evaluation of parameter identifiability using Sobol's sensitivity analysis, and (3) evaluation of model plausibility by combining the results of stages (1) and (2). These evaluation stages eliminated three out of four model structures and lead to a unique hypothesis about the functioning of the studied karst system. We used the estimated parameter values to further quantify subsurface processes. The chosen model is able to simultaneously provide high performances for eight system signatures with realistic parameter values. Our approach demonstrates the benefits of interpreting different tracers in a hydrologically meaningful way during model evaluation and identification.",2013,10.1002/wrcr.20229,no
Evaluating the effects of process parameters on maximum extrusion pressure using a new artificial neural network-based (ANN-based) partial-modeling technique,"Metal extrusion process accounts for the production of the majority of industrial and domestic aluminum sections. A major limitation to the success of any extrusion operation is the capability of the particular extrusion press to meet the maximum pressure requirements for that operation. In the present work, the effects of industrial extrusion process parameters and their interactions on the resulting maximum extrusion pressure, of an industrially extruded aluminum alloy, have been studied using a newly devised ANN-based partial modeling technique. Two operating parameters (initial billet temperature and ram speed) and three geometrical parameters (extrusion ratio, profile average thickness, and number of die cavities) were investigated. The main objective for developing this modeling technique is to overcome the limitations of presently available statistical modeling tools, as foreseen by the modeling needs for a complex thermo-mechanical process such as extrusion. The main present limitations are accounting for non-linearity in the process behavior, incorporating interaction effects and a meaningful determination of the highly significant process parameters and/or interactions. These three features have been, collectively, incorporated into the present model by means of combining statistical analysis of variance into ANN and by using a partial sum of squares analysis, which we propose to call the ""present factor analysis."" Normal linear regression has been also employed for comparison purposes. According to the present model, maximum extrusion pressure has shown various degree of non-linearity in behavior with respect to the different process parameters and their significant interactions. It has been found that variations in the maximum extrusion pressure are mainly a function of initial billet temperature and its interactions with other process parameters, especially the ram speed. The present ANN-based model has shown superior prediction capabilities compared to the linear model with a marginal overall prediction error value of +/- 2.5 %.",2013,10.1007/s00170-013-4852-x,no
Dysregulation of Mitochondrial Quality Control Processes Contribute to Sarcopenia in a Mouse Model of Premature Aging,"Mitochondrial DNA (mtDNA) mutations lead to decrements in mitochondrial function and accelerated rates of these mutations has been linked to skeletal muscle loss (sarcopenia). The purpose of this study was to investigate the effect of mtDNA mutations on mitochondrial quality control processes in skeletal muscle from animals (young; 3-6 months and older; 8-15 months) expressing a proofreading-deficient version of mtDNA polymerase gamma (PolG). This progeroid aging model exhibits elevated mtDNA mutation rates, mitochondrial dysfunction, and a premature aging phenotype that includes sarcopenia. We found increased expression of the mitochondrial biogenesis regulator peroxisome proliferator-activated receptor gamma coactivator-1 alpha (PGC-1 alpha) and its target proteins, nuclear respiratory factor 1 (NRF-1) and mitochondrial transcription factor A (Tfam) in PolG animals compared to wild-type (WT) (P<0.05). Muscle from older PolG animals displayed higher mitochondrial fission protein 1 (Fis1) concurrent with greater induction of autophagy, as indicated by changes in Atg5 and p62 protein content (P<0.05). Additionally, levels of the Tom22 import protein were higher in PolG animals when compared to WT (P<0.05). In contrast, muscle from normally-aged animals exhibited a distinctly different expression profile compared to PolG animals. Older WT animals appeared to have higher fusion (greater Mfn1/Mfn2, and lower Fis1) and lower autophagy (Beclin-1 and p62) compared to young WT suggesting that autophagy is impaired in aging muscle. In conclusion, muscle from mtDNA mutator mice display higher mitochondrial fission and autophagy levels that likely contribute to the sarcopenic phenotype observed in premature aging and this differs from the response observed in normally-aged muscle.",2013,10.1371/journal.pone.0069327,no
Integrating Validation Techniques for Process-based Models,"Model checking has been established as an appropriate technology to validate behavioral properties of (business) process based systems. However, further validation technologies (e. g. for structural properties) may be of interest for process models. We propose a concept to integrate validation technologies in a unique system with a common user interface.",2013,10.5220/0004568232463253,no
Model-Based Verification and Validation of the SMAP Uplink Processes,"Model-Based Systems Engineering (MBSE) is being used increasingly within the spacecraft design community because of its benefits when compared to document-based approaches. As the complexity of projects expands dramatically with continually increasing computational power and technology infusion, the time and effort needed for verification and validation (V&V) increases geometrically. Using simulation to perform design validation with system-level models earlier in the life cycle stands to bridge the gap between design of the system (based on system-level requirements) and verifying those requirements/validating the system as a whole. This case study stands as an example of how a project can validate a system-level design earlier in the project life cycle than traditional V&V processes by using simulation on a system model. Specifically, this paper describes how simulation was added to a system model of the Soil Moisture Active-Passive (SMAP) mission's uplink process. Also discussed are the advantages and disadvantages of the methods employed and the lessons learned; which are intended to benefit future model-based and simulation-based V&V development efforts.",2013,,no
Moral Ambivalence: Modeling and Measuring Bivariate Evaluative Processes in Moral Judgment,"Moral judgments often appear to arise from quick affectively toned intuitions rather than from conscious application of moral principles. Sometimes people feel that an action they observe or contemplate could be judged as either right or wrong. Models of moral intuition need to specify mechanisms that could account for such moral ambivalence. The basic implication of moral ambivalence is that right and wrong are regions of a bivariate scale rather than a bipolar scale. The former allows for equally strong positive and negative evaluations of a stimulus, but the latter requires one evaluation to get weaker as the other one gets stronger (Cacioppo & Berntson, 1994). Covariation of evaluative activations is supported by classic animal research on approach-avoidance conflict showing that when rats are both rewarded and punished in the goal region of a runway, their approach and avoidance tendencies both increase as they get closer to the goal. The relevance of this research to moral judgment is underscored by recent studies indicating that judgments of right and wrong are fundamentally expressions of approach and avoidance motivation. Experimental and historical analyses illustrate 2 potential effects of ambivalence on moral judgment, vacillation and suppression, and a proposed model shows how the bivariate scale can be applied to existing formulations, including Haidt's moral foundations theory. Studies of moral judgment that use rating scales, questionnaires, or interviews should give participants the option to express ambivalence (e. g., ""can't decide,"" ""don't know"") instead of requiring definitive judgments, which is the current practice.",2013,10.1037/a0034527,no
Using state machines to model the Ion Torrent sequencing process and to improve read error rates,"Motivation: The importance of fast and affordable DNA sequencing methods for current day life sciences, medicine and biotechnology is hard to overstate. A major player is Ion Torrent, a pyrosequencing- like technology which produces flowgrams - sequences of incorporation values - which are converted into nucleotide sequences by a basecalling algorithm. Because of its exploitation of ubiquitous semiconductor technology and innovation in chemistry, Ion Torrent has been gaining popularity since its debut in 2011. Despite the advantages, however, Ion Torrent read accuracy remains a significant concern. Results: We present FlowgramFixer, a new algorithm for converting flowgrams into reads. Our key observation is that the incorporation signals of neighboring flows, even after normalization and phase correction, carry considerable mutual information and are important in making the correct base-call. We therefore propose that base-calling of flowgrams should be done on a read-wide level, rather than one flow at a time. We show that this can be done in linear-time by combining a state machine with a Viterbi algorithm to find the nucleotide sequence that maximizes the likelihood of the observed flowgram. FlowgramFixer is applicable to any flowgram-based sequencing platform. We demonstrate FlowgramFixer's superior performance on Ion Torrent Escherichia coli data, with a 4.8% improvement in the number of high-quality mapped reads and a 7.1% improvement in the number of uniquely mappable reads.",2013,10.1093/bioinformatics/btt212,no
"Understanding decimal proportions: Discrete representations, parallel access, and privileged processing of zero","Much of the research on mathematical cognition has focused on the numbers 1, 2, 3, 4, 5, 6, 7, 8, and 9, with considerably less attention paid to more abstract number classes. The current research investigated how people understand decimal proportions - rational numbers between 0 and 1 expressed in the place-value symbol system. The results demonstrate that proportions are represented as discrete structures and processed in parallel. There was a semantic interference effect: When understanding a proportion expression (e.g., ""0.29""), both the correct proportion referent (e.g., 0.29) and the incorrect natural number referent (e.g., 29) corresponding to the visually similar natural number expression (e.g., ""29"") are accessed in parallel, and when these referents lead to conflicting judgments, performance slows. There was also a syntactic interference effect, generalizing the unit-decade compatibility effect for natural numbers: When comparing two proportions, their tenths and hundredths components are processed in parallel, and when the different components lead to conflicting judgments, performance slows. The results also reveal that zero decimals - proportions ending in zero - serve multiple cognitive functions, including eliminating semantic interference and speeding processing. The current research also extends the distance, semantic congruence, and SNARC effects from natural numbers to decimal proportions. These findings inform how people understand the place-value symbol system, and the mental implementation of mathematical symbol systems more generally. (C) 2013 Elsevier Inc. All rights reserved.",2013,10.1016/j.cogpsych.2013.01.002,no
Error Assessment for the Coherency Matrix-Based Spectral Representation Method in Multivariate Random Processes Simulation,"Multivariate random processes are usually simulated by the spectral representation method (SRM). According to the matrix for decomposition, the SRM has two main types, that is, the SRM based on the decomposition of the power spectral density (PSD) matrix denoting the PSD matrix-based SRM, and the SRM based on the decomposition of the coherency matrix denoting the coherency matrix based-SRM. The stochastic errors of the PSD for the PSD matrix-based SRM have been given. This paper presents the stochastic errors of the PSD for the coherency matrix-based SRM, and makes a comparison of these errors for the PSD matrix-based SRM. For the random amplitudes formulas and random phase formula and Cholesky decomposition method, the stochastic errors of the PSDs for the PSD matrix-based SRM are the same as or the coherency matrix-based SRM, whereas for the random phases formula and eigendecomposition method and random phases formula and root decomposition method, they are different. However, the differences are slight when taking into account the sum of the PSD functions' stochastic errors.",2013,10.1061/(ASCE)EM.1943-7889.0000563,no
A farm-to-fork model to evaluate the level of polyacetylenes in processed carrots,"Naturally occurring aliphatic C-17 polyacetylene compounds [falcarinol (FaOH), falcarindiol (FaDOH) and falcarindiol-3-acetate (FaDOAc)] in carrots are known for their bioactivity and health benefits. This study assesses the impact of pre- and postharvest processes (including food processing stages) on the level of polyacetylenes and evaluates subsequent human exposure using Monte Carlo simulation techniques. The model includes data inputs from both experimental and published literature sources. The sensitivity analysis highlights the importance of cultivar selection and agronomic factors. The sensitivity analysis also showed that peeling, blanching and boiling time of carrots have a significant negative influence on the level of polyacetylenes with correlation coefficients of -0.15, -0.14 and -0.19 for FaOH, -0.47, -0.23 and -0.20 for FaDOH and -0.29, -0.26 and -0.25 for FaDOAc, respectively. The scenario analysis shows the practical application of the proposed model for industrial processing of carrots. This model could facilitate food processors in optimising critical processing factors such as peeling and cutting prior to processing of carrots.",2013,10.1111/ijfs.12133,no
Finite element modelling and experimental investigation of single point incremental forming process of aluminum sheets: influence of process parameters on punch force monitoring and on mechanical and geometrical quality of parts,"New trends in sheet metal forming are rapidly developing and several new forming processes have been proposed to accomplish the goals of flexibility and cost reduction. Among them, Incremental CNC sheet forming operations (ISF) are a relatively new sheet metal forming processes for small batch production and prototyping. In single point incremental forming (SPIF), the final shape of the component is obtained by the CNC relative movements of a simple and small punch which deform a clamped blank into the desired shape and which appear quite promising. No other dies are required than the ones used in any conventional sheet metal forming processes. As it is well known, the design of a mechanical component requires some decisions about the mechanical resistance and geometrical quality of the parts and the product has to be manufactured with a careful definition of the process set up. The use of computers in manufacturing has enabled the development of several new sheet metal forming processes, which are based upon older technologies. Although standard sheet metal forming processes are strongly controlled, new processes like single point incremental sheet forming can be improved. The SPIF concept allows to increase flexibility and to reduce set up costs. Such a process has a negative effect on the shape accuracy by initiating undesired rigid movement and sheet thinning. In the paper, the applicability of the numerical technique and the experimental test program to incremental forming of sheet metal is examined. Concerning the numerical simulation, a static implicit finite element code ABAQUS/Standard is used. These two techniques emphasize the necessity to control some process parameters to improve the final product quality. The reported approaches were mainly focused on the influence of four process parameters on the punch force trends generated in this forming process, the thickness and the equivalent plastic deformation distribution within the whole volume of the workpiece: the initial sheet thickness, the wall angle, the workpiece geometry and the nature of tool path contours controlled through CNC programming. The tool forces required to deform plastically the sheet around the contact area are discussed. The effect of the blank thickness and the tool path on the punch load and the deformation behaviour is also examined with respect to several tool paths. Furthermore, the force acting on the traveling tool is also evaluated. Similar to the sheet thickness, the effect of wall angle and part geometry on the load evolution, the distribution of calculated equivalent plastic strain and the variation of sheet thickness strain are also discussed. Experimental and numerical results obtained allow having a better knowledge of mechanical and geometrical responses from different parts manufactured by SPIF with the aim to improve their accuracy. It is also concluded that the numerical simulation might be exploited for optimization of the incremental forming process of sheet metal.",2013,10.1007/s12289-012-1101-z,no
Mdodeling a nitrite-dependent anaerobic methane oxidation process: Parameters identification and model evaluation,"Nitrite-dependent anaerobic methane oxidation (n-damo) is a recently discovered process that is intermediated by n-damo bacteria that oxidize methane with nitrite to generate nitrogen gas. In this work, a kinetic model based on Monod type kinetics and diffusion-reaction model was developed to describe the bioprocess. Some key kinetic parameters needed in the model were obtained from a series of batch activity tests and a sequencing batch reactor (SBR) operation over 100 days. The growth rate, decay rate, methane affinity constant, nitrite affinity constant and inhibition constant were 0.0277 +/- 0.0022 d(-1), 0.00216 +/- 0.00010d(-1), 0.092 +/- 0.005 mmol L-1, 0.91 +/- 0.09 mmol L-1 and 4.1 +/- 0.5 mmol L-1 for n-damo bacteria at 30 degrees C, respectively. The results showed that the model could simulate actual performance of the SBR in the first 76 days, that methane was not a limiting factor at atmospheric pressure for its high affinity, and that the optimum nitrite concentration was 1.92 mmol L-1. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.biortech.2013.08.001,no
Model-based evaluations of operating parameters on CANON process in a membrane-aerated biofilm reactor,"Nitrogen compounds, such as ammonium, naturally appear in most wastewaters necessitating treatment in order to prevent oxygen depletion and eutrophication of surface water bodies. Conventional biological nitrogen removal from wastewater usually performed using possible set-ups of sequential aerobic nitrification and anoxic denitrification processes. The completely autotrophic nitrogen removal over nitrite (CANON) process is a combination of partial nitrification an Anaerobic Ammonia Oxidation (ANAMMOX) in which the aerobic ammonium oxidizers and ANAMMOX bacteria perform two sequential reactions under oxygen-limited conditions. CANON process is suitable to remove ammonium from wastewaters characterized by the low content of organic carbon yet abundance of ammonium. It performs with limited volume and reasonable budget; hence, any available nitrification unit might be converted into a new improved one rather easily while benefiting from flexibility of CANON control strategies. To better understand such process' behaviour, one might initially start with developing a mathematical model as a useful tool. In this research, the modelling of CANON process was demonstrated in a Membrane Aerated Biofilm Reactor (MABR) through the ASM3 reference model. It was shown that, with 0.7 mm biofilm thickness the optimal nitrogen removal might be obtained, when the ammonium concentration in influents reached 130 gN/m(3) and DO equaled to 1.3 gO(2)/m(3).",2013,10.1080/19443994.2013.768050,no
Geant4 model validation of Compton suppressed system for process monitoring of spent fuel,"Nuclear material accountancy is of continuous concern for the regulatory, safeguards, and verification communities. In particular, spent nuclear fuel reprocessing facilities pose one of the most difficult accountancy challenges: monitoring highly radioactive, fluid sample streams in near real-time. The Multi-Isotope Process monitor will allow for near-real-time indication of process alterations using passive gamma-ray detection coupled with multivariate analysis techniques to guard against potential material diversion or to enhance domestic process monitoring. The Compton continuum from the dominant 661.7 keV Cs-137 fission product peak obscures lower energy lines which could be used for spectral and multivariate analysis. Compton suppression may be able to mitigate the challenges posed by the high continuum caused by scattering. A Monte Carlo simulation using the Geant4 toolkit is being developed to predict the expected suppressed spectrum from spent fuel samples to estimate the reduction in the Compton continuum. Despite the lack of timing information between decay events in the particle management of Geant4, encouraging results were recorded utilizing only the information within individual decays without accounting for accidental coincidences. The model has been validated with single and cascade decay emitters in two steps: as an unsuppressed system and with suppression activated. Results of the Geant4 model validation will be presented.",2013,10.1007/s10967-012-1988-3,no
"The AMELIE project: failure mode, effects and criticality analysis: a model to evaluate the nurse medication administration process on the floor","Objective The objective of this article was to critically evaluate the causes of adverse drug events during the nurse medication administration process in paediatric care units in order to identify and prioritize interventions that need to be implemented. Methodology This is a failure mode, effects and criticality analysis (FMECA) study. A multidisciplinary committee composed of nurses, pharmacists, physicians and risk managers evaluated through consensus the process of administering medications at the Centre hospitalier universitaire de Sainte-Justine. By mapping the process, all the failure modes were identified and associated with at least one cause each. Using a summary grid, each failure mode was evaluated by rating frequency (from 1 to 9), likelihood of failure detection (from 0 to 100%) and severity (from 1 to 9) using adapted versions of already published scales. Results A 10-member committee was set up, and it met eight times between January and April 2010. In the two specialized paediatric units selected (n = 38 beds), an average number of approximately 20 000 drug doses was administered monthly from about 400 non-proprietary names. Through consensus, the committee identified 16 processes and 53 failure modes. While frequency and severity were based on perceptions that could be objectivized with local data and scientific documentation, the likelihood of detection was mainly based on individual perception. Conclusion FMECA is a useful approach to improve the medication process.",2013,10.1111/j.1365-2753.2011.01799.x,no
Mediators of Longitudinal Changes in Measures of Adiposity in Teenagers Using Parallel Process Latent Growth Modeling,"Objective: The aim of the study was to evaluate mediating effects of energy balance-related behaviors on measures of adiposity in the Dutch Obesity Intervention in Teenagers-study (DOiT). Design and Methods: DOiT was an 8-month behavioral intervention program consisting of educational and environmental components and evaluated in 18 prevocational secondary schools in the Netherlands (n=1,108, baseline age 12.7 years, 50% girls). Outcome measures were changes in body mass index (BMI), waist circumference, and sum of skinfold thickness. Self-reported consumption of sugar-containing beverages and high caloric snacks, active transport to/from school, and screen-viewing behaviors were the hypothesized mediators. Data were collected at 0, 8, 12, and 20 months. For the data analysis, parallel process latent growth modeling was used. Results: Total sugar-containing beverages consumption mediated the intervention effects on BMI (ab=-0.01, 95% CI=-0.20, -0.001). The intervention group lowered their sugar-containing beverages consumption more than controls (B=-0.14, 95% CI=-0.22, -0.11) and this, in turn, led to smaller increases in BMI. No significant mediated effect by the targeted behaviors was found for waist circumference or sum of skinfolds. Conclusions: Future school-based overweight prevention interventions may target decreasing sugar-containing beverages consumption.",2013,10.1002/oby.20463,no
Healing process after total cricoidectomy and laryngotracheal reconstruction: Endoscopic and histologic evaluation in a canine model,"Objective: The surgical procedure for subglottic stenosis is technically challenging when the vocal cords are involved and concomitant management for glottic involvement is required. After total cricoidectomy and laryngotracheal anastomosis, T-tube placement for 3 to 6 months is recommended. Bone grafts might shorten this period. We report the histologic and endoscopic changes after total cricoidectomy with or without bone grafts in a canine model to suggest an appropriate period for T-tube placement and the necessity for bone grafts. Methods: Ten dogs underwent total cricoidectomy and laryngotracheal anastomosis with or without bone grafts harvested from the ribs. Endoscopic examination was performed monthly, and 1 dog from both groups was humanely killed at 1, 2, 3, 6, and 12 months. The T-tube was removed before death in the dogs killed at 1, 2, and 3 months and at 3 and 6 months in those killed at 6 and 12 months, respectively. Results: Endoscopically, the glottic opening was in good condition in all dogs, except for 1 that had glottic stenosis. Histologically, active lymphocyte infiltration was observed in dense collagen fibers at the anastomosis at 1 month. At 2 and 3 months, fibroblasts were evident, suggesting active collagen fiber production. At 6 and 12 months, the collagen fibers had become looser. The bone grafts were intact and did not influence the surrounding tissue. Conclusions: In the canine model, 6 months of T-tube placement is probably sufficient; however, 3 months of placement might not be. Additionally, no difference was found between the dogs with and without a bone graft. (J Thorac Cardiovasc Surg 2013;145:847-53)",2013,10.1016/j.jtcvs.2012.03.041,no
"Using a Systematic Conceptual Model for a Process Evaluation of a Middle School Obesity Risk-Reduction Nutrition Curriculum Intervention: Choice, Control & Change","Objective: To use and review a conceptual model of process evaluation and to examine the implementation of a nutrition education curriculum, Choice, Control & Change, designed to promote dietary and physical activity behaviors that reduce obesity risk. Design: A process evaluation study based on a systematic conceptual model. Setting: Five middle schools in New York City. Participants: Five hundred sixty-two students in 20 classes and their science teachers (n = 8). Main Outcome Measures: Based on the model, teacher professional development, teacher implementation, and student reception were evaluated. Also measured were teacher characteristics, teachers' curriculum evaluation, and satisfaction with teaching the curriculum. Analysis: Descriptive statistics and Spearman rho correlation for quantitative analysis and content analysis for qualitative data were used. Results: Mean score of the teacher professional development evaluation was 4.75 on a 5-point scale. Average teacher implementation rate was 73%, and the student reception rate was 69%. Ongoing teacher support was highly valued by teachers. Teacher satisfaction with teaching the curriculum was highly correlated with student satisfaction (P < .05). Teacher perception of amount of student work was negatively correlated with implementation and with student satisfaction (P < .05). Conclusions and Implications: Use of a systematic conceptual model and comprehensive process measures improves understanding of the implementation process and helps educators to better implement interventions as designed.",2013,10.1016/j.jneb.2012.07.002,no
Can we simplify the hospital accreditation process? Predicting accreditation decisions from a reduced dataset of focus priority standards and quality indicators: results of predictive modelling,"Objectives: Accreditation in France relies on a mandatory 4-year cycle of self-assessment and a peer review of 82 standards, among which 14 focus priority standards (FPS). Hospitals are also required to measure yearly quality indicators (QIs-5 in 2010). On advice given by the accreditation committee of HAS (Haute Autorite en Sante), based on surveyors proposals and relying mostly on compliance to standards, accreditation decisions are taken by the board of HAS. Accreditation is still perceived by hospitals as a burdensome process and a simplification would be welcomed. The hypothesis was that a more limited number of criteria might give sufficient amount of information on hospitals overall quality level, appraised today by accreditation decisions. Design: The accuracy of predictions of accreditation decisions given by a model, Partial Least Square-2 Discriminant Analysis (PLS2-DA), using only the results of FPS and QIs was measured. Accreditation decisions (full accreditation (A), recommendations or reservation (B), remit decision or non-accreditation (C)), results of FPS and QIs were considered qualitative variables. Stability was assessed by leave one out cross validation (LOOCV). Setting and participants: All French 489 acute care organisations (ACO) accredited between June 2010 and January 2012 were considered, 304 of them having a rehabilitation care sector (RCS). Results: Accuracy of prediction of accreditation decisions was good (89% of ACOs and 91% of ACO-RCS well classified). Stability of results appeared satisfactory when using LOOCV (87% of ACOs and 89% of ACO-RCS well classified). Identification of worse hospitals was correct (90% of ACOs and 97% of ACO-RCS predicted C were actually C). Conclusions: Using PLS2-DA with a limited number of criteria (QIs and FPS) provides an accurate prediction of accreditation decisions, especially for underperforming hospitals. This could support accreditation committees which give advices on accreditation decisions, and allow fast-track handling of 'safe' reports.",2013,10.1136/bmjopen-2013-003289,no
Solving the Distinctiveness - Blindness Debate: A Unified Model for Understanding Banner Processing,"Online designers have widely adopted banners as a popular online advertising format. However, because of their low click-through rates, marketers have recently questioned the effectiveness of banners. A phenomenon called ""banner blindness"" suggests that salient stimuli, such as banners, are often missed by Internet users. This contradicts the distinctiveness view, which argues that salient stimuli are more likely to attract a user's attention and enhance the click-through rate. To solve this debate, we develop a research framework to explain from an evolutionary perspective how the banner processing mode evolves. More specifically, we develop a process model that shows the transitions between three banner processing modes - automatic salience capture, controlled salience suppression, and automatic salience suppression. In addition, a unified variance model is proposed to solve the distinctiveness - blindness debate. Specifically, we propose that the habituation level and the task type can moderate the effects of structural factors and semantic factors on attention. We also discuss empirical strategies for examining the model and future research.",2013,,no
AN INTEGRATED MODEL OF PARALLEL PROCESSING AND PSO ALGORITHM FOR SOLVING OPTIMUM HIGHWAY ALIGNMENT PROBLEM,"Optimum highway alignment is among the most substantial, but large and complicated topics in transportation area. Infinite number of feasible solutions, numerous local optima and the constrained feature of the problem, associated with complex and mainly non-linear constraints, has put an extra effort into the problem solving process. This paper focuses on solving highway alignment optimization problem using an integrated model of parallel processing and particle swarm optimization algorithm. To achieve this goal, algorithm parallelization is done in synchronous and asynchronous manner. For assessing parallel performance, corresponding indexes are evaluated. SRTM3 databank is used for solving real-world problems. The penalty function approach is employed for dealing with constraints. The successful application of the model is investigated on two real-world route location problems.",2013,,no
Using Markov Decision Process for Recommendations Based on Aggregated Decision Data Models,"Our research is placed in the context of business decision making processes. We look at decision making as at a workflow of (mostly mental) activities directed at choosing one decision alternative. Our goal is to direct the flow of decision activities such that the relevant alternatives are properly evaluated. It is outside our purpose to recommend which alternative should be chosen. Since business decision making is data-centric, we use a Decision Data Model (DDM). It is automatically mined from a log containing the decision maker's actions while interacting with business software. The recommendation is based on an aggregated DDM that shows what many decision makers have done in the same decision situation. In our previous work we created algorithms that seek a local optimum. In this paper we show how the recommendation based on DDM problem can be mapped to a Markov Decision Process (MDP). The aim is to use MDP to find a global optimal decision making strategy.",2013,,no
New Insight on High-k/Metal Gate Reliability Modeling for Providing Guidelines for Process Development,"Process changes with the view of improving performances should not be done at the expense of reliability. In this respect, degradation mechanisms modeling must provide guidelines for process orientations. In this paper, NBTI and TDDB physical mechanisms are discussed together with the process levers for the control of reliability.",2013,,no
A Comprehensive Benchmarking Framework (CoBeFra) for Conformance Analysis between Procedural Process Models and Event Logs in ProM,"Process mining encompasses the research area which is concerned with knowledge discovery from information system event logs. Within the process mining research area, two prominent tasks can be discerned. First of all, process discovery deals with the automatic construction of a process model out of an event log. Secondly, conformance checking focuses on the assessment of the quality of a discovered or designed process model in respect to the actual behavior as captured in event logs. Hereto, multiple techniques and metrics have been developed and described in the literature. However, the process mining domain still lacks a comprehensive framework for assessing the goodness of a process model from a quantitative perspective. In this study, we describe the architecture of an extensible framework within ProM, allowing for the consistent, comparative and repeatable calculation of conformance metrics. For the development and assessment of both process discovery as well as conformance techniques, such a framework is considered greatly valuable.",2013,,no
A Hierarchical Markov Model to Understand the Behaviour of Agents in Business Processes,"Process mining techniques are able to discover process models from event logs but there is a gap between the low-level nature of events and the high-level abstraction of business activities. In this work we present a hierarchical Markov model together with mining techniques to discover the relationship between low-level events and a high-level description of the business process. This can be used to understand how agents perform activities at run-time. In a case study experiment using an agent-based simulation platform (AOR), we show how the proposed approach is able to discover the behaviour of agents in each activity of a business process for which a high-level model is known.",2013,,no
A study on the effects of routing symbol design on process model comprehension,"Process modeling grammars are used to create models of business processes. In this paper, we discuss how different routing symbol designs affect an individual's ability to comprehend process models. We conduct an experiment with 154 students to ascertain which visual design principles influence process model comprehension. Our findings suggest that design principles related to perceptual discriminability and pop out improve comprehension accuracy. Furthermore, semantic transparency and aesthetic design of symbols lower the perceived difficulty of comprehension. Our results inform important principles about notational design of process modeling grammars and the effective use of process modeling in practice. (C) 2012 Elsevier B.V. All rights reserved.",2013,10.1016/j.dss.2012.10.037,no
Modeling and validation of business process families,"Process modeling is an expensive task that needs to encompass requirements of different stakeholders, assure compliance with different standards, and enable the flexible adaptivity to newly emerging requirements in today's dynamic global market. Identifying reusability of process models is a promising direction towards reducing the costs of process modeling. Recent research has offered several solutions. Such solutions promote effective and formally sound methods for variability modeling and configuration management. However, ensuring behavioral validity of reused process models with respect to the original process models (often referred to as reference process models) is still an open research challenge. To address this challenge, in this paper, we propose the notion of business process families by building upon the well-known software engineering discipline software product line engineering. Business process families comprise (i) a variability modeling perspective, (ii) a process model template (or reference model), and (iii) mappings between (i) and (ii). For business process families, we propose a correct validation algorithm ensuring that each member of a business process family adheres to the core intended behavior that is specified in the process model template. The proposed validation approach is based on the use of Description Logics, variability is represented by using the well-known Feature Models and behavior of process models is considered in terms of control flow patterns. The paper also reports on the experience gained in two external trial cases and results obtained by measuring the tractability of the implementation of the proposed validation approach. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.is.2012.11.010,no
Quality-Related Process Monitoring Based on Total Kernel PLS Model and Its Industrial Application,"Projection to latent structures (PLS) model has been widely used in quality-related process monitoring, as it can establish a mapping relationship between process variables and quality index variables. To enhance the adaptivity of PLS, kernel PLS (KPLS) as an advanced version has been proposed for nonlinear processes. In this paper, we discuss a new total kernel PLS (T-KPLS) for nonlinear quality-related process monitoring. The new model divides the input spaces into four parts instead of two parts in KPLS, where an individual subspace is responsible in predicting quality output, and two parts are utilized for monitoring the quality-related variations. In addition, fault detection policy is developed based on the T-KPLS model, which is more well suited for nonlinear quality-related process monitoring. In the case study, a nonlinear numerical case, the typical Tennessee Eastman Process (TEP) and a real industrial hot strip mill process (HSMP) are employed to access the utility of the present scheme.",2013,10.1155/2013/707953,no
A MSVM Quality Pattern Recognition Model for Dynamic Process,"Quality abnormal pattern recognition for dynamic process is the key problem to achieve the online quality control and diagnose of automatic production. Firstly, this paper analyzed the quality patterns of dynamic process. Secondly, we established recognition model of quality recognition in dynamic process using MSVM and compared the SVM recognition accuracy of different kernel functions for different quality patterns. Simulation experiment indicates that different SVM classifiers should choose specified kernel functions to recognition quality patterns. At last, we established MSVM recognition model of quality pattern in dynamic process using multi-kernel function according to the experiment results.",2013,10.4028/www.scientific.net/AMM.433-435.555,no
Understanding Health Disparities: The Relevance of the Stress Process Model,"Race and socioeconomic status (SES) health disparities imply massive impact in terms of unequal suffering and dramatic social and economic costs. It is clear that status differences in the availability, use, and effectiveness of medical care and in a variety of health behaviors are implicated in these disparities. However, it is equally clear that adjustments for these differences leave the majority of race and SES health disparities unexplained. Despite wide acceptance of the idea that differences in stress exposure may contribute importantly to such disparities, it is contended that the stress hypothesis has never been effectively tested because of misclassification in the distinction between the disordered and the well and the inadequate estimation of differences in exposure to social stressors. This article reviews the empirical basis for this contention and describes an ongoing community study designed to more effectively evaluate the hypothesis that lifetime stress exposure represents a fundamental factor in observed race and SES health disparities. It is suggested that the application of aspects of the approach described may advance the capacity of future research to fully evaluate the mental health significance of the stress process.",2013,10.1177/2156869313488121,no
Understanding and Supporting Reflective Learning Processes in the Workplace: The CSRL Model,"Reflective learning is a mechanism to turn experience into learning. As a mechanism for self-directed learning, it has been found to be critical for success at work. This is true for individual employees, teams and whole organizations. However, most work on reflection can be found in educational contexts, and there is only little work regarding the connection of reflection on individual, group and organization levels. In this paper, we propose a model that can describe cases of reflective learning at work (CSRL). The model represents reflective learning processes as intertwined learning cycles. In contrast to other models of reflective learning, the CSRL model can describe both individual and collaborative learning and learning that impacts larger parts of an organization. It provides terminology to describe and discuss motivations for reflective learning, including triggers, objectives for and objects of reflective learning. The paper illustrates how the model helps to analyse and differentiate cases of reflective learning at work and to design tool support for such settings.",2013,,no
SDG-based Model Validation in Chemical Process Simulation,"Signed direct graph (SDG) theory provides algorithms and methods that can be applied directly to chemical process modeling and analysis to validate simulation models, and is a basis for the development of a software environment that can automate the validation activity. This paper is concentrated on the pretreatment of the model validation. We use the validation scenarios and standard sequences generated by well-established SDG model to validate the trends fitted from the simulation model. The results are helpful to find potential problems, assess possible bugs in the simulation model and solve the problem effectively. A case study on a simulation model of boiler is presented to demonstrate the effectiveness of this method.",2013,10.1016/S1004-9541(13)60554-6,no
Representing and evaluating the landscape freeze/thaw properties and their impacts on soil impermeability: Hydrological processes in the community land model version 4,"Snow cover at high latitudes is an excellent natural insulator that can maintain the underlying ground at a higher temperature than the overlying atmosphere. Soil impermeability usually varies when snow cover accumulates, which is closely related to soil and landscape freeze/thaw status. How snow cover affects the landscape frozen fraction and soil impermeability and how this impermeability regulates hydrological processes in cold regions have not been fully assessed and quantified. In order to understand these processes, this study performed a series of experiments by using the Community Land Model version 4 (CLM4). We first simulated the top-soil-layer ice, snow ice, and canopy ice to calculate the landscape frozen fraction, which was evaluated based on the Special Sensor Microwave/Imager (SSM/I) observed landscape freeze/thaw earth system data record (FT-ESDR) in two selected regions at high latitudes. Then two soil impermeability parameterizations were validated against various in situ and satellite observations. The results suggest the following: (1) compared to SSM/I FT-ESDR, CLM4 can capture the overall landscape freeze/thaw status in the regions north of 60 degrees N in boreal winter and spring; (2) as the snow cover fraction approaches unity, the CLM4-simulated landscape frozen fraction is mainly controlled by the snow ice amount, resulting in step changes between SSM/I FT-ESDR observed and CLM4-simulated landscape frozen fractions; and (3) in most of the cold regions, the timing of the boreal spring runoff simulations is improved by reducing the impermeable area in high landscape frozen fraction regions.",2013,10.1002/jgrd.50576,no
"ADVANCES IN SOIL EROSION RESEARCH: PROCESSES, MEASUREMENT, AND MODELING","Soil erosion by the environmental agents of water and wind is a continuing global menace that threatens the agricultural base that sustains our civilization. For over 70 years, ASABE members have been at the forefront of research to understand erosion processes, measure erosion and related processes, and model very complex sediment detachment, transport, and deposition. The ASABE Erosion Control Group (SW-22) and Erosion Control Research (SW-223) committees periodically sponsor international symposia to provide an avenue for exchange of ideas and information by engineers, scientists, and students from around the world. The two most recent symposia were the Soil Erosion Research for the 21st Century Symposium held in January 2001 in Honolulu, Hawaii, and the International Symposium on Erosion and Landscape Evolution (ISELE) held in September 2011 in Anchorage, Alaska. This article describes these two events, provides a description of major outcomes, and introduces a collection of papers that were presented as part of the 2011 ISELE in Alaska. The ISELE sessions focused on seven themes: water erosion process research; aeolian erosion and fugitive dust emission; highly disturbed, urban, and arid lands; erosion measurement and assessment; prevention and control of upland and in-stream erosion; soil erosion modeling; and impacts of global change on erosion and landscape evolution. More than 120 people from 16 countries attended the ISELE and gave 112 oral and poster presentations. From those presentations, 24 papers were accepted for publication in Transactions of the ASABE (22 papers, this issue) and Applied Engineering in Agriculture (two papers, next issue). The results from these symposia and in these papers show active and vibrant research to address soil erosion problems, particularly in the face of global environmental changes.",2013,,no
Metallurgical Consideration about Abnormal Events during Slab Continuous Casting Process in the Quality Diagnosis Model,"Some kinds of abnormal events occurred possibly during casting process have been taken into full consideration in the slab quality online diagnosis & analysis mathematic model, CISDI_SQDS ONLINE R2011, the effects of which on relevant crack formation index have been introduced into the model in terms of the event level. As for the capture modes of event status, there are two styles, one is manual set mode, the other one is automatic judgment mode. And also, the effect ranges of all the events in the casting strand have been discussed respectively. The mathematic model has been put into industrial application successfully in Xinjiang Bayi Steel, which shows obviously that, the capture method of event status is accurate and reliable, the influence factors of events to quality defects are reasonable and can reflect the effect degree normally.",2013,10.4028/www.scientific.net/AMM.395-396.1179,no
COMPREHENSIVE MODELING OF THE FORMATION PROCESS OF SOUND-QUALITY,"Sound quality is not an inherent property of sounds, but sound quality happens in a complex process which is specified by judgments where the ""character"" of the sound is compared to conceptual references that represent the expectations of the listeners. The references are task- and listener-specific. Here, the question will be addressed of how the complex process of quality formation may be modeled with a comprehensive model of auditory processing that contains both bottom-up (signals driven) as well as top-down (hypothesis-driven) processes. The generation of hypotheses requires inherent knowledge, namely, a specific aural-world model. To obtain ground-truth data for the world model, sound quality can be analyzed in terms of plausibility. It can be attempted to assess plausibility in an indirect fashion, observing listeners' involvement and immersion into auditory scenes and, thereby, implicitly considering the meaning associated with it. In this way, instead of mere form-related fidelity, the ability of sounds under test to convey meaning to listeners could also be considered, in other words, taking the function aspect into account, in addition to the form aspect. In this paper, a general architecture for such a comprehensive modeling system will be proposed and discussed for the example of evaluating sound reproduction systems.",2013,,no
Modelling of the lock-in thermography process through finite element method for estimating the rail squat defects,"Squats are a major problem on the world railways. The non-destructive evaluation technique is becoming increasingly attractive in the detection of near surface defects on track. Non-destructive thermal evaluation is one such method of inspection technique that can be used for the detection of near surface defects. Its sub-group of lock-in thermography is under analysis. Lock-in thermography utilizes an infrared camera to detect the thermal waves and then produces a thermal image, which displays the local thermal wave variation in phase or amplitude. There are few studies into the actual experimental representation of complex subsurface defects when concerning lock-in thermography processes. While this may be less of a concern given the purpose of numerical defect characterization to reduce the need for extensive experimental pre-tests, the necessity for (artificial) representations of a defect will inevitably be required for validation. The research outlined in this paper examines the use of 3D finite element modelling (FEM) as a potential flexible tool in simulating the lock-in thermography process for detecting squats in track. In addition, lock-in analysis proved that the correct frequency range had to be selected for the material to detect the defect. As maximum positive and negative phase angles were located at ""optimum"" frequencies, at certain frequencies lead to minimal phase angle difference to which the defects were not detectable (blind frequency) by using the incorrect testing. The 3D finite element method has advantage for determining the ""optimum"" thermal excitation frequencies compare with experimental investigation. The experimental results show that 3D FEM models can be used to defect the location and the depth of squats in the railway. (C) 2012 Elsevier Ltd. All rights reserved.",2013,10.1016/j.engfailanal.2012.10.024,no
Students' mathematics understanding in modelling processes of change,"Students' mathematics understanding in modelling processes of change In the context of a design study for lesson material for mathematical modelling of processes of change, modelling processes were studied that were executed by students in the highest grade of Dutch pre-university education. The research question was whether these students understood the nature of differential equations as descriptors of such processes. Fourteen students participated in the study, all of who followed the advanced mathematics curriculum. Using an open questionnaire and an assignment that was solved while thinking aloud, data were collected about students' perceptions of differential equations and the way they model processes of change. Students remained relatively long in an orientation phase and often tried to find a direct formula to describe the process, rather than to formulate a differential equation. They struggled with the concept of the derivative as a description of change. An important outcome is the use of physical units contributing to successful modelling exercises.",2013,,no
Thermodynamic modelling and evaluation of a two-stage thermal process for waste gasification,"Tar generation and ash disposal represent the strongest barrier for use of fluid bed gasification for waste treatment, whereas sufficing for both is only possible with expensive cleaning systems and further processing. The use of plasma within an advanced two-stage thermal process is able to achieve efficient cracking of the complex organics to the primary syngas constituents whilst limiting the electric power demand. This study focused on the thermodynamic assets of using a two-stage thermal process over the conventional single-stage approach. These include, for example, the fact that the primary thermal waste decomposition is performed in conditions of optimal stoichiometric ratio for the gasification reactants. Furthermore, staging the oxidant injection in two separate intakes significantly improves the efficiency of the system, reducing the plasma power consumption. A flexible model capable of providing reliable quantitative predictions of product yield and composition after the two-stage process has been developed. The method has a systematic structure that embraces atom conservation principles and equilibrium calculation routines, considering all the conversion stages that lead from the initial waste feed to final products. The model was also validated with experimental data from a demonstration plant. The study effectively demonstrated that the two-stage gasification system significantly improves the gas yield of the system and the carbon conversion efficiency, which are crucial in other single stage systems, whilst maintaining high energy performances. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.fuel.2013.02.037,no
Study of process characteristics of Abrasive flow machining (AFM) for Ti-6Al-4V and validation with process model,"The aerospace components made of titanium alloy with internal passages have complex geometries and their surface smoothness play an important role on the improvement of fuel efficiency. Hence, a surface finishing method that can produce a conforming surface finish to all internal surface features is required. This paper studies the process characteristics of AFM for Ti-6Al-4V experimentally. In addition, validation of process model based on the tribological interaction between AFM media and Ti-6Al-4V with empirical data is carried out. The theoretical plots agreed well with experimental results on empty set20mmID cylinder although the theoretical plots show over prediction in some conditions.",2013,10.4028/www.scientific.net/AMR.797.411,no
Conceptualization of Approaches and Thought Processes Emerging in Validating of Model in Mathematical Modeling in Technology Aided Environment,"The aim of the present study is to conceptualize the approaches displayed for validation of model and thought processes provided in mathematical modeling process performed in technology-aided learning environment. The participants of this grounded theory study were nineteen secondary school mathematics student teachers. The data gathered from the video recordings received while participants were solving the given problems, the written responses related to the solution of the problems, the GeoGebra solution files and the observation notes taken by the researchers in the process of problem solving. The constant comparative analysis based on open, axial, and selective coding methods were used for data analysis through the grounded theory. The five substeps were emerged from data analyzing related to the process of the validation of model. It was determined that these steps were covered by discussing the unexpected results in real-life situations, comparing the real life results with the estimation based on the experiences or measurements, the problem data, the situations given in video and pictures and the decision-making about the adequacy of the model. In this study, it was seen that the technology-aided environment enriched the cognitive processes in validating step. With the help of this research's results, it was constructed a detailed description related to the cognitive processes which can be displayed for validating of model in mathematical modeling. It was considered that the study would bring a different perspective to the researchers towards the process of mathematical modeling.",2013,,no
Economic evaluation of a combined photo-Fenton/MBR process using pesticides as model pollutant. Factors affecting costs,"The aim of this paper is to carry out an economic assessment on a solar photo-Fenton/MBR combined process to treat industrial ecotoxic wastewater. This study focuses on the impact of the contamination present in wastewater, the photochemical oxidation, the use of an MBR as biological process and the plant size on operating and amortization costs. As example of ecotoxic pollutant, a mixture of five commercial pesticides commonly used in the Mediterranean area has been used, ranging from 500 mg/L to 50 mg/L, expressed as dissolved organic carbon concentration. The economic evaluation shows that (i) the increase in pollution load does not always involve an increase in photo-Fenton costs because they also depend on organic matter mineralization; (ii) the use of an MBR process permits lower photochemical oxidation requirements than other biological treatments, resulting in approximately 20% photo-Fenton cost reduction for highly polluted wastewater; (iii) when pollution load decreases, the contribution of reactant consumption to the photo-Fenton process costs increase with regard to amortization costs; (iv) 30% total cost reduction can be gained treating higher daily volumes, obtaining competitive costs that vary from 1.1-1.9(sic)/m(3), depending on the pollution load. (C) 2012 Elsevier B.V. All rights reserved.",2013,10.1016/j.jhazmat.2012.11.015,no
TRANSLATING EMOTIONS - THE REPRESENTATION AND PROCESSING OF EMOTION-LADEN AND EVALUATIVE WORDS IN BILINGUAL AND MONOLINGUAL INDIVIDUALS FROM SERBIA,"The aim of this study was to analyze, compare and contrast emotion-laden and evaluative words in monolingual and bilingual individuals from Serbia. More precisely, our research goal was to prepare an interethnic comparison of affective and connotative meaning systems in Hungarian and Serbian monolingual and Hungarian-Serbian bilingual individuals. The empirical part of the study paid special attention to emotion-laden and evaluative words at a verbal level, examining specific concepts related to emotions. Our participants were bilinguals and monolinguals from Vojvodina, Serbia. The research method used was the semantic differential technique developed by Charles Osgood, which allows the participants to express their attitudes. Here they rated positive and negative emotion, emotion-laden, emotion-evocative and evaluative concepts and neutral words. Our results show that mainly there are no differences between the monolingual and bilingual groups, but few words exist that have a different emotional meaning and charge in Serbian and Hungarian language.",2013,,no
Implementation of Quality by Design Case Study Development of a process model for pulsed spray fluidised bed granulation process,"The aim of this work was to investigate the effects of different factors of pulsed frequency, binder spray rate and atomization pressure of the top-spray fluidised bed granulation process on the granule size and yield using the Box-Behnken design method. Mathematical models were developed to predict the mean size of granules and yield with high accuracy, indicating the reliability and effectiveness using the Box-Behnken experimental design method to study the fluidised bed granulation process.",2013,,no
"Wind-Borne Dispersal of a Parasitoid: the Process, the Model, and its Validation","The aphelinid parasitoid Eretmocerus hayati Zolnerowich & Rose (Hymenoptera: Aphelinidae) was recently released in Australia as a biocontrol agent against the crop pest Bemisia tabaci Gennadius (Hemiptera: Aleyrodidae). It was found that the parasitoid can spread over several kilometers in a single generation and continue laying eggs for over a fortnight. A simple wind-advection model was fitted to emergence data from a first release between Fassifern and Kalbar, Queensland, and its predictive ability was tested against the second release near Carnarvon, Western Australia. The fitting of the model was used to develop several hypotheses about the dispersal of E. hayati, which were validated by the second release: E. hayati flies in the same direction as the wind to a distance proportional to the wind speed; this wind-borne flight takes place at any time during daylight hours; a flight is attempted every day after emergence unless there are high wind conditions during that day; and the high wind condition that will delay flight is wind speeds in excess of approximate to 2 m/s. This model of E. hayati dispersal may be contrasted with previous models fitted for Eretmocerus species, for which dispersal was dominated by diffusion processes, and parasitoid spread was constrained to the scales of tens and hundreds of meters.",2013,10.1603/EN12243,no
Model of evaluation and certification of a skills workshop integrating technology and processes. Pilot study for skills formation in construction engineering at UCN,"The article describes a pilot implementation of a model evaluation and certification of skills in the university environment, particularly in the career of Construction Engineering at the Catholic University of the North. We describe the construction process of the standard and the design and validation of assessment instruments. It also presents the main findings related to performance and student satisfaction. The results show that the evaluation model has a significant effect on learning and student achievement",2013,,no
Evaluating Maintainability of MDA Software Process Models,"The description of a software process is called a process model. As well as traditional software processes/methods (e.g. RUP, XP, OSDP, etc.) an MDA software process requires the selection of metamodels and mapping rules for the generation of the transformation chain that produces models and application code. Before software process enactment, process models should be evaluated in order to improve some quality attributes and maintainability is one of the main factors for software process reuse and improvement. This paper presents a conceptual framework including a metrics suite to evaluate maintainability of MDA process models. We also describe an empirical assessment involving three case studies where the metrics suite was applied to over five MDA process models. We compared the results indicated by the measurements with software engineer opinions surveyed by an online questionnaire. We found that the results from the metrics-based conceptual framework application match software engineers' perceptions.",2013,,no
Modelling of homogeneously catalysed reactive distillation processes in packed columns: Experimental model validation,"The design of reactive distillation processes requires reliable and accurate models to significantly decrease the expensive and time consuming experimental work. Different modelling approaches of varying complexity are available in the open literature. However, only few publications exist in which the question of the optimal modelling depth is discussed for homogeneously catalysed processes. Unlike these publications, we used experimental data in the present study to compare them with simulation results using different modelling depths for homogeneous reactive distillation processes. The nonequilibrium-stage model using the Maxwell-Stefan equations, the nonequilibrium-stage model using effective diffusion coefficients, the equilibrium-stage model including reaction kinetics, and the equilibrium-stage model assuming chemical equilibrium were investigated. The homogeneously catalysed transesterification of dimethyl carbonate with ethanol, for which pilot-scale experimental data were available, was used as a test system. (C) 2012 Elsevier Ltd. All rights reserved.",2013,10.1016/j.compchemeng.2012.07.015,no
EVALUATION OF THE SEMI-AUTOMATED CRIME-SPECIFIC DIGITAL TRIAGE PROCESS MODEL,"The digital forensic process as traditionally laid out is very time intensive - it begins with the collection, duplication and authentication of every piece of digital media prior to examination. Digital triage, a process that takes place prior to this standard methodology, can be used to speed up the process and provide valuable intelligence without subjecting digital evidence to a full examination. This quick intelligence can be used in the field for search and seizure guidance, in the office to determine if media is worth sending out for an examination, or in the laboratory to prioritize cases for analysis. For digital triage to become accepted by the forensic community, it must be modeled, tested and peer reviewed, but there have been very few attempts to model digital triage. This work describes the evaluation of the Semi-Automated Crime-Specific Digital Triage Process Model, and presents the results of five experimental trials.",2013,,no
Quality by design for wet granulation in pharmaceutical processing: Assessing models for a priori design and scaling,"The effective implementation of Quality by Design (QbD) for wet granulation requires quantitative models for design and scaling of granulators. This paper reviews that current state of the art of granulation modeling, especially with respect to QbD in the pharmaceutical industry. Our thesis is that the state of the art use of high shear wet granulation models for QbD should be at the intermediate level between the extremes of data-driven empirical models and first principles models, i.e., regime-based models. These models have a sound mechanistic basis and can be used for both formulation design and process scaling. Although not fully predictive, regime map approaches are a significant advancement over a typical design of experiments approach, where little is known about the process. In addition, regime maps are particularly useful for process scale-up/down. This review focuses on the status of wet granulation regime maps and critically assesses their applicability to real, rather than model formulations. The approach is illustrated by two case studies. The first case study evaluates the effect of changing the properties of one excipient, lactose, on formulation behavior. Changing lactose size distribution leads to substantial changes in both the formulation drop penetration time and dynamic yield stress, which can lead to changes in operating regime. A size measurement technique that accurately measures the fine end of the distribution is necessary to predict the effect of changing the lactose size distribution. The second case study uses the regime map approach to propose a design space and scaling rules for a high drug load formulation, and successfully validate the scaling rules for scaling from a 10 l to a 75 l granulation. This case study emphasizes the importance of spray distribution to avoid operation in the mechanical dispersion regime and subsequent lump formation as a granulation is scaled up. (C) 2012 Elsevier B.V. All rights reserved.",2013,10.1016/j.powtec.2012.07.013,no
CB2 Receptor Deficiency Increases Amyloid Pathology and Alters Tau Processing in a Transgenic Mouse Model of Alzheimer's Disease,"The endocannabinoid CB2 receptor system has been implicated in the neuropathology of Alzheimer's disease (AD). In order to investigate the impact of the CB2 receptor system on AD pathology, a colony of mice with a deleted CB2 receptor gene, CNR2, was established on a transgenic human mutant APP background for pathological comparison with CB2 receptor-sufficient transgenic mice. J20 APP (PDGFB-APPSwInd) mice were bred over two generations with CNR2(-/-) (Cnr2(tm1Dgen)/J) mice to produce a colony of J20 CNR2(+/+) and J20 CNR2(-/-) mice. Seventeen J20 CNR2(+/+) mice (12 females, 5 males) and 16 J20 CNR2(-/-) mice (11 females, 5 males) were killed at 12 months, and their brains were interrogated for AD-related pathology with both biochemistry and immunocytochemistry (ICC). In addition to amyloid-dependent endpoints such as soluble A beta production and plaque deposition quantified with 6E10 staining, the effect of CB2 receptor deletion on total soluble mouse tau production was assayed by using a recently developed high-sensitivity assay. Results revealed that soluble A beta 42 and plaque deposition were significantly increased in J20 CNR2(-/-) mice relative to CNR2(+/+) mice. Microgliosis, quantified with ionized calcium-binding adapter molecule 1 (Iba-1) staining, did not differ between groups, whereas plaque associated microglia was more abundant in J20 CNR2(-/-) mice. Total tau was significantly suppressed in J20 CNR2(-/-) mice relative to J20 CNR2(+/+) mice. The results confirm the constitutive role of the CB2 receptor system both in reducing amyloid plaque pathology in AD and also support tehpotential of cannabinoid therapies targeting CB2 to reduce A beta; however, the results suggest that interventions may have a divergent effect on tau pathology.",2013,10.2119/molmed.2013.00140,no
Dynamic modeling and validation of a lignocellulosic enzymatic hydrolysis process - A demonstration scale study,"The enzymatic hydrolysis process is one of the key steps in second generation biofuel production. After being thermally pretreated, the lignocellulosic material is liquefied by enzymes prior to fermentation. The scope of this paper is to evaluate a dynamic model of the hydrolysis process on a demonstration scale reactor. The following novel features are included: the application of the Convection-Diffusion-Reaction equation to a hydrolysis reactor to assess transport and mixing effects; the extension of a competitive kinetic model with enzymatic pH dependency and hemicellulose hydrolysis; a comprehensive pH model; and viscosity estimations during the course of reaction. The model is evaluated against real data extracted from a demonstration scale biorefinery throughout several days of operation. All measurements are within predictions uncertainty and, therefore, the model constitutes a valuable tool to support process optimization, performance monitoring, diagnosis and process control at full-scale studies. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.biortech.2013.10.029,no
"GeoTemp (TM) 1.0: A MATLAB-based program for the processing, interpretation and modelling of geological formation temperature measurements","The evaluation of potential and resources during geothermal exploration requires accurate and consistent temperature characterization and modelling of the sub-surface. Existing interpretation and modelling approaches of 1D temperature measurements are mainly focusing on vertical heat conduction with only few approaches that deals with advective heat transport. Thermal regimes are strongly correlated to rock and fluid properties. Currently, no consensus exists for the identification of the thermal regime and the analysis of such dataset. We developed a new framework allowing the identification of thermal regimes by rock formations, the analysis and modelling of wireline logging and discrete temperature measurements by taking into account the geological, geophysical and petrophysics data. This framework has been implemented in the GeoTemp software package that allows the complete thermal characterization and modelling at the formation scale and that provides a set of standard tools for the processing wireline and discrete temperature data. GeoTemp (TM) operates via a user friendly graphical interface written in Matlab that allows semiautomatic calculation, display and export of the results. Output results can be exported as Microsoft Excel spreadsheets or vector graphics of publication quality. Geolemp (TM) is illustrated here with an example geothermal application from Western Australia and can be used for academic, teaching and professional purposes. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.cageo.2013.04.003,no
External Economies Evaluation of Wind Power Engineering Project Based on Analytic Hierarchy Process and Matter-Element Extension Model,"The external economies of wind power engineering project may affect the operational efficiency of wind power enterprises and sustainable development of wind power industry. In order to ensure that the wind power engineering project is constructed and developed in a scientific manner, a reasonable external economies evaluation needs to be performed. Considering the interaction relationship of the evaluation indices and the ambiguity and uncertainty inherent, a hybrid model of external economies evaluation designed to be applied to wind power engineering project was put forward based on the analytic hierarchy process (AHP) and matter-element extension model in this paper. The AHP was used to determine the weights of indices, and the matter-element extension model was used to deduce final ranking. Taking a wind power engineering project in Inner Mongolia city as an example, the external economies evaluation is performed by employing this hybrid model. The result shows that the external economies of this wind power engineering project are belonged to the ""strongest"" level, and ""the degree of increasing region GDP,"" ""the degree of reducing pollution gas emissions,"" and ""the degree of energy conservation"" are the sensitive indices.",2013,10.1155/2013/848901,no
Fouling control and optimization of a drinking water membrane filtration process with real-time model parameter adaptation using fluorescence and permeate flux measurements,"The implementation of fouling control strategies is crucial for the effective maintenance and long-term application of ultrafiltration (UF) membranes in drinking water treatment. Membrane cleaning methods such as membrane back-washing protocols are typically employed to remove the build-up of natural water species (foulants) on the surface and/or in the pores of the membranes, which causes membrane fouling, contributes to increased trans-membrane pressure and shortens membrane lifetime. In a previous work by the authors, a fluorescence-based principal component (PC) modelling approach, which was able to forecast membrane fouling behaviour of a bench-scale UF membrane set-up over a future time horizon, was introduced. This approach also proved suitable for estimating the optimum future membrane back-washing times required for controlling fouling and minimizing the energy demand for drinking water production. In this study, the forecasting ability of this model was improved by updating the model parameters with current process measurements. The Extended Kalman filter (EKF) approach was used to achieve this objective. The EKF approach accomplished real-time adaptive estimation of key model parameters based on either real-time UF flux measurements or PC scores related to fluorescence measurements of membrane permeate. The model predictions and the corresponding experimental UF flux data for different membrane fouling situations revealed that on-line permeate flux-based parameter adaptation showed improved model predictions as compared to PC score-based adaptation, especially for longer filtration times. The improvements in the accuracy of the fluorescence-based model forecasts also aided the estimation of optimum back-washing times with better accuracy resulting in considerable energy savings compared to back-washing estimates obtained without real-time parameter adaptation. (C) 2012 Elsevier Ltd. All rights reserved.",2013,10.1016/j.jprocont.2012.10.001,no
Process-Driven Data Quality Management Through Integration of Data Quality into Existing Process Models Application of Complexity-Reducing Patterns and the Impact on Complexity Metrics,"The importance of high data quality and the need to consider data quality in the context of business processes are well acknowledged. Process modeling is mandatory for process-driven data quality management, which seeks to improve and sustain data quality by redesigning processes that create or modify data. A variety of process modeling languages exist, which organizations heterogeneously apply. The purpose of this article is to present a context-independent approach to integrate data quality into the variety of existing process models. The authors aim to improve communication of data quality issues across stakeholders while considering process model complexity. They build on a keyword-based literature review in 74 IS journals and three conferences, reviewing 1,555 articles from 1995 onwards. 26 articles, including 46 process models, were examined in detail. The literature review reveals the need for a context-independent and visible integration of data quality into process models. First, the authors present the enhancement of existing process models with data quality characteristics. Second, they present the integration of a data-quality-centric process model with existing process models. Since process models are mainly used for communicating processes, they consider the impact of integrating data quality and the application of patterns for complexity reduction on the models' complexity metrics. There is need for further research on complexity metrics to improve the applicability of complexity reduction patterns. Lacking knowledge about interdependency between metrics and missing complexity metrics impede assessment and prediction of process model complexity and thus understandability. Finally, our context-independent approach can be used complementarily for data quality integration with specific process modeling languages.",2013,10.1007/s12599-013-0297-x,no
PBL-Test: a Model to Evaluate the Maturity of Teaching Processes in a PBL Approach,"The increasing application of student-centered teaching approaches to solve real problems, driven by the market's demand for professionals with better skills, has prompted the use of PBL in different areas, including in Computing. However, since this represents a paradigm shift in education, its implementation is not always well understood, which adversely affects its effectiveness. Within this context, this paper puts forward a model for assessing the maturity of teaching processes under the PBL approach, the PBL-Test, with a view to identifying points for improvement. The concept of maturity is defined in terms of teaching processes adhering to PBL principles, taken from an analysis of the following authors: Savery & Duffy (1995), Barrows (2001) Peterson (1997) and Alessio (2004). With a view to validating the applicability of the model, an empirical study was conducted by applying the PBL-Test to three skills in the Computing area. Results showed that although the model has shown it needs further enhancement, it has already been possible to identify improvements in PBL teaching processes that clearly affect the effectiveness of the approach.",2013,,no
Bayesian calibration of mathematical models: Optimization of model structure and examination of the role of process error covariance,"The integration of Bayesian inference techniques with mathematical modeling offers a promising means to improve ecological forecasts and management actions over space and time, while accounting for the uncertainty underlying model predictions. In this study, we address two important questions related to the ramifications of the statistical assumptions typically made about the model structural error and the prospect of Bayesian calibration to guide the optimization of model complexity. Regarding the former issue, we examine statistical formulations that whether postulate conditional independence or explicitly accommodate the covariance among the error terms for various model endpoints. Our analysis evaluates the differences in the posterior parameter patterns and predictive performance of a limiting nutrient (phosphate)-phytoplankton-zooplankton-detritus (particulate phosphorus) model calibrated with three alternative statistical configurations. The lessons learned from this exercise are combined with those from a second comparative analysis that aims to optimize model structure. In particular, we selected three formulas of the zooplankton mortality term (linear, hyperbolic, sigmoidal) and examine their capacity to determine the posterior parameterization as well as the reproduction of the observed ecosystem patterns. Our analysis suggests that the statistical characterization of the model error as well as the mathematical representation of specific ecological processes can be influential to the inference drawn by a modeling exercise. Our findings could be useful when selecting the most suitable statistical framework for model calibration and/or making informative decisions about model structure optimization. In the absence of adequate prior knowledge, we also advocate the use of Bayesian model averaging for obtaining weighted averages of the forecasts from different model structures and/or statistical descriptions of the process error terms. (C) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.ecoinf.2013.07.001,no
INTEGRATED BSC AND DEA MODELS IN THE FUNCTION OF MEASURING THE SUCCESS OF THE TEACHING PROCESS,"The intention of this paper is to show and present a multi-criteria approach in measuring the success of high-education institutions, primarily from the point of success in studies. Our approach, actually integrates two concepts: BSC (Balanced Scorecard) and DEA (Data Envelopment Analysis) with the goal to improve and develop the extended BSC model. Measuring success is to be presented through all the four perspectives of BSC, and the emphasis is to be put on measuring the efficiency of the teaching process, where there are students with fluctuating entrance values/inputs, such as their average grades from high school, number of points at the entrance exam, and output values: average grades in the course of studying and the length of studies.",2013,,no
Business Processes and Technical Processes A comprehensive meta model for execution and development,"The interaction between business processes and technical processes is getting more and more important. Throughout this paper, we present how concepts that are used in automation technology can be transferred into concepts that are suitable for use in business process modeling, too. An important aspect is the description of the procedures which have to be executed. In contrast to technical systems, humans as executing agents can, but do not have to follow predefined procedures due to their ability to use their knowledge to work independently in different situations. In a second part, the paper discusses the generation of business processes by project business processes on a meta model, and provides a reference model for a project business layer model.",2013,,no
Design Management Absorption Model: A Framework to Describe and Measure the Absorption Process of Design Knowledge by SMEs with Little or no Prior Design Experience,"The introduction of new design knowledge or design resources in companies with little or no design experience has been at the core of design support programmes in several countries. Scholars investigated the use of design and identified different design and design management capabilities to deploy design effectively in companies of all sizes. However, how design and design management capability is built in SMEs with little or no prior design experience is insufficiently investigated. Based on the absorptive capacity construct from the broader field of innovation studies, this paper proposes a comprehensive design management absorption model that includes design management capabilities enabling design absorption in SMEs with little or no prior design experience as well as indicators to measure the progress of absorption. The model allows for analysing and guiding the process companies go through when using design as a strategic resource for the first time.",2013,10.1111/caim.12022,no
Validation of a model for radon-induced background processes in electrostatic spectrometers,"The Karlsruhe Tritium Neutrino (KATRIN) experiment investigating tritium beta-decay close to the endpoint with unprecedented precision has stringent requirements on the background level of less than 10(-2) counts per second. Electron emission during the alpha-decay of Rn-219,Rn-220 atoms in the electrostatic spectrometers of KATRIN is a serious source of background exceeding this limit. In this paper we compare extensive simulations of Rn-induced background to specific measurements with the KATRIN pre-spectrometer to fully characterize the observed Rn-background rates and signatures and determine generic Rn emanation rates from the pre-spectrometer bulk material and its vacuum components.",2013,10.1088/0954-3899/40/8/085102,no
Analysis and study of the process enterprise knowledge management model based on the recommendation algorithm,"The knowledge sharing is one of the most important characteristics of knowledge management. In the traditional model of knowledge management, employees only select the sharing knowledge through independent action, and operating behavior between employees of the same type did not reflect reference. This paper is the integration of the recommendation algorithm of data mining and the traditional knowledge ontology knowledge management model, proposing the process enterprise knowledge management model based on the recommendation algorithm, and knowledge management framework of knowledge as the main body, the field of process-driven and recommendation process for the behavior. To recommend the appropriate knowledge for the staff improves the efficiency of enterprise employees staff to the knowledge and promote the application and innovation of knowledge of the enterprise.",2013,10.4028/www.scientific.net/AMM.411-414.1099,no
Effect of land cover on atmospheric processes and air quality over the continental United States - a NASA Unified WRF (NU-WRF) model study,"The land surface plays a crucial role in regulating water and energy fluxes at the land-atmosphere (L-A) interface and controls many processes and feedbacks in the climate system. Land cover and vegetation type remains one key determinant of soil moisture content that impacts air temperature, planetary boundary layer (PBL) evolution, and precipitation through soil-moisture-evapotranspiration coupling. In turn, it will affect atmospheric chemistry and air quality. This paper presents the results of a modeling study of the effect of land cover on some key L-A processes with a focus on air quality. The newly developed NASA Unified Weather Research and Forecast (NU-WRF) modeling system couples NASA's Land Information System (LIS) with the community WRF model and allows users to explore the L-A processes and feedbacks. Three commonly used satellite-derived land cover datasets - i.e., from the US Geological Survey (USGS) and University of Maryland (UMD), which are based on the Advanced Very High Resolution Radiometer (AVHRR), and from the Moderate Resolution Imaging Spectroradiometer (MODIS) - bear large differences in agriculture, forest, grassland, and urban spatial distributions in the continental United States, and thus provide an excellent case to investigate how land cover change would impact atmospheric processes and air quality. The weeklong simulations demonstrate the noticeable differences in soil moisture/temperature, latent/sensible heat flux, PBL height, wind, NO2/ozone, and PM2.5 air quality. These discrepancies can be traced to associate with the land cover properties, e.g., stomatal resistance, albedo and emissivity, and roughness characteristics. It also implies that the rapid urban growth may have complex air quality implications with reductions in peak ozone but more frequent high ozone events.",2013,10.5194/acp-13-6207-2013,no
Cross Validation and Maximum Likelihood estimations of hyper-parameters of Gaussian processes with model misspecification,"The Maximum Likelihood (ML) and Cross Validation (CV) methods for estimating covariance hyper-parameters are compared, in the context of Kriging with a misspecified covariance structure. A two-step approach is used. First, the case of the estimation of a single variance hyper-parameter is addressed, for which the fixed correlation function is misspecified. A predictive variance based quality criterion is introduced and a closed-form expression of this criterion is derived. It is shown that when the correlation function is misspecified, the CV does better compared to ML, while ML is optimal when the model is well-specified. In the second step, the results of the first step are extended to the case when the hyper-parameters of the correlation function are also estimated from data. (c) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.csda.2013.03.016,no
Study on Measured Data Processing Method of Model Test for Relative Motions between Two Side-by-side Replenishment Ships in Waves,"The method to process measured data of ship model test with contact measurement to study the characteristics of relative motion between two side-by-side ships was investigated. The reference co-ordinate systems and the relative motions of two side-by-side replenishment ships were defined. Then the integration transform method which combines Fast Fourier Transform(FFT) with Empirical Mode Decomposition(EMD) self-adaptive filter was applied to transform measured acceleration signal into displacement. The EMD adaptive filter is designed, and the integration results from acceleration to displacement with and without EMD adaptive filter were compared, which shows that the EMD adaptive filter can effectively eliminate the noise embedded in the measured data. At last, the relative motions are acquired, which are consistent with the practical situation.",2013,10.4028/www.scientific.net/AMM.239-240.1089,no
"State Space Realization of Wood Drying Process: Modeling, Simulation and Evaluation in Reality","The modeling of a wood drying process using a state space realization is considered. Based on balance equations for energy and mass in the air gap between the timbers, on the wood surface and within the wood, a state space realization consisting of linear and nonlinear parts is proposed which can describe the moisture content and temperature behavior of timber inside a drying kiln. Finally, the proposed state space realization is illustrated with a simulation and achieved results are evaluated by real measurements.",2013,10.1109/SMC.2013.805,no
Real-Time Optimization of an Industrial-Scale Vapor Recompression Distillation Process. Model Validation and Analysis,"The modeling, analysis, and simulation validation of an industrial-scale depropenizer column (Refinaria de Capuava, Maui, Sao Paulo) owned by Petrobras SA. are carried out using equation-oriented and sequential modular approaches. The model implemented in the equation-oriented environment proved to be suitable for real-time optimization (RTO) applications due to its robustness, fast convergence, and ability to represent the real process. The analysis of the model allows better understanding of the process and establishment of boundaries to process specifications and the parameters updated in the RTO cycle; furthermore, it is shown that feed composition and column pressure drop are critical aspects in the model to represent the actual process.",2013,10.1021/ie303345z,no
Improvement of the Noah land surface model for warm season processes: evaluation of water and energy flux simulation,"The Noah model is a land surface model of the National Centers for Environmental Prediction. It has been widely used in regional coupled weather and climate models (i.e. Weather Research and Forecasting Model, Eta Mesoscale Model) and global coupled weather and climate models (i.e. National Centers for Environmental Prediction Global Forecast System, Climate Forecast System). Therefore, its continued improvement and development are keys to enhancing our weather and climate forecast ability and water and energy flux simulation accuracy. North American Land Data Assimilation System phase 1 (NLDAS-1) experiments indicated that the Noah model exhibited substantial bias in latent heat flux, total runoff and land skin temperature during the warm season, and such bias can significantly affect coupled weather and climate models. This paper presents a study to improve the Noah model by adding model parameterization processes such as including seasonal factor on leaf area index and root distribution and selecting optimal model parameters. We compared simulated latent heat flux, mean annual runoff and land skin temperature from the Noah control and test versions with measured latent heat flux, land surface skin temperature, mean annual runoff and satellite-retrieved land surface skin temperature. The results show that the test version significantly reduces biases in latent heat, total runoff and land skin temperature simulation. The test version has been used for the NLDAS phase 2 (NLDAS-2) to produce 30-year water flux, energy flux and state variable products to support the US drought monitor of National Integrated Drought Information System. Copyright (C) 2012 John Wiley & Sons, Ltd.",2013,10.1002/hyp.9214,no
"A numerical study of pellet model consistency with respect to molar and mass average velocities, pressure gradients and porosity models for methanol synthesis process: Effects of flux models on reactor performance","The objective of this work is to compare mass- and mole based diffusion flux models, convection, fluid velocity and pore structure models for methanol synthesis process. Steady-state models have been derived and solved using least-squares spectral method (LSM) to describe the evolution of species composition, pressure, velocity, total concentration and diffusion fluxes in porous pellets for methanol synthesis. Mass diffusion fluxes are described according to the rigorous Maxwell Stefan model, dusty gas model and the more simple Wilke model. These fluxes are defined with respect to molar- and mass averaged velocities. The different effects of choosing the random- and parallel pore models have been investigated. The effects of Knudsen diffusion have been investigated. The result varies significantly in the dusty gas model. The effectiveness factors have been calculated for the methanol synthesis process for both mass- and mole based pellet models. The values of effectiveness factors for both mass- and mole based pellet models do not vary so much. The effect of Wilke-, Maxwell-Stefan- and dusty gas mass diffusion fluxes on the reactor performance have been studied. Steady-state heterogeneous fixed bed reactor model is derived where the intra-particle mass diffusion fluxes in the voids of the pellet are described by Wilke-, Maxwell-Stefan- and dusty gas models. Furthermore, the total computational efficiency of the heterogeneous fixed bed reactor model is calculated with several closures for the intra-particle mass diffusion fluxes. The model evaluations revealed that: - The mass- and mole based pellet models are not completely consistent. However, the small deviation (less than 2%) between mass- and mole based pellet models is due to the model equations are not fully consistent. If one pellet model is to be chosen for the methanol synthesis process, the optimal diffusion flux model is the Maxwell-Stefan model. - The parallel pore model is deviating from the random pore model for the methanol synthesis process. The results of both the parallel-and random pore models have been compared with experimental data available in the literature. It is found that the result of the parallel pore model is well agreement with experimental data. - A small but significant differences in the mole fraction profiles of methanol along the reactor axis where the diffusion fluxes are described according to the Wilke-, Maxwell-Stefan- and dusty gas models. - The consistent models are about 20 and 60% more computationally expensive than the simplified and not always consistent Wilke model. It is recommended to use the consistent models instead of Wilke approach. (c) 2012 The Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.",2013,10.1016/j.cherd.2012.09.003,no
THE EFFECT OF MODEL CHANGE PROCESSES ON THE OVERALL EQUIPMENT EFFECTIVENESS IN CLOTHING PRODUCTION,"The overall equipment effectiveness is necessary for determining weak points in order to provide optimum equipment conditions. Meanwhile, it focuses on removing the factors such as unexpected malfunctions, speed losses and quality losses in the process. These losses are commonly seen in the transition between orders and in the process for adaptation to a new product in clothing enterprises. This process has a big importance for providing effectiveness in the current understanding of production in which the numbers of orders decrease but the numbers of models increase. This study deals with the effect of the order change time on the overall equipment efficiency.",2013,,no
Some Aspects of Modeling for Management in the Process of Forming a Students' Number in Higher Educational Institutions as the Actual Economic Educational Problem,The paper deals with modeling of the formation of a students' number of higher educational institution using the interpolation and interlination operators depending on an education cost and university rating. The dependences obtained can be used in making management decision about pricing optimal from the point of maximizing a plan for a students' number. The dependence of the educational service cost on the university rating in condition of economic transition and sustainable development is obtained.,2013,,no
Model for Knowledge Representation of Multidimensional Measurements Processing Results in the Environment of Intelligent GIS,"The paper describes models for knowledge representation, i.e. extracted at different steps of multidimensional measurements processing procedure, in the context of JDL data fusion model. Models are developed taking into account the requirements of geo information systems environment. As a case study system of conditions lighting which implements the models for oceanographic data processing is described.",2013,,no
Dynamic modeling and validation of 2-ethyl-hexenal hydrogenation process,"The paper evaluates, by modeling and simulation, 2-ethyl-hexenal hydrogenation process in catalytic trickle bed three-phase reactors. The mathematical model consists of balance equations for gas and liquid phases. Reaction rate equations, transport models and mass balances were coupled to generalized heterogeneous models which were solved with respect to time and space with algorithms suitable for partial differential equations. The importance of mass transfer resistance inside the catalyst pellets as well as the dynamics of the different phases being present in the reactor is revealed. The dynamic mathematical model presented can be used to analyze and understand the interaction of various processes that take place inside the hydrogenation reactor and also to make preliminary calculation of experimental parameters. Another important use of the mathematical model is to determine the optimal operation conditions and to design the control system. The model is implemented in Matlab and tested in simulations achieving successful results. (C) 2012 Elsevier Ltd. All rights reserved.",2013,10.1016/j.compchemeng.2012.11.012,no
Certifiable Development Process for Information Processing in the Measurement Domain on the Base of 3W-Models,"The paper presents a certifiable development process applicable to hardware and software development for information processing in the measurement domain, especially the dynamic weighing technology. Thereby it is based on the developments of prototypes for new and predecessor systems and product stages. Particular attention is paid to the possibility of calibration.",2013,10.1109/COMPSACW.2013.121,no
Matching of Linear Dynamic Model of Process based on known Frequency Characteristic using a Non-Quadratic Measure of Model Error,"The paper presents a new approach to derivation of a linear model representing an approximation of dynamic response determined for a model estimated within discrete time identification. The approximation is calculated using with new algorithm, that minimize the approximation error with application a non-quadratic performance index. The used performance index is quite close to the absolute norm of error, but has better convergence features. The recursive formula for derivation of the approximation model coefficients is derived and tested on two different examples of two dynamic processes.",2013,,no
ONLINE EM ESTIMATION OF THE DIRICHLET PROCESS MIXTURES SCALE PARAMETER TO MODEL THE GPS MULTIPATH ERROR,"The performance of GPS is strongly degraded in a multipath environment. The multipath impact the distribution of the additive noise corrupting the distance measurements between the satellites and the GPS receiver. In this paper, this distribution is assumed unknown and modeled in a flexible way by using the Bayesian non parametric framework and more precisely the Dirichlet process mixtures. Nevertheless, these latter depend on the so-called scale parameter which can be difficult to tune a priori. The originality of our approach consists in adapting a recent version of the online EM algorithm, developed by Cappe for hidden Markov models, to compute a maximum a posteriori estimate of the scale parameter. Then, as the proposed model is non linear and non Gaussian, the EM-based scale parameter estimation is coupled with a Rao-Blackwellized particle filter for the joint estimation of the mobile location and the distance measurement noise distribution.",2013,,no
Quantitative measurements of anaerobic digestion process parameters using near infrared spectroscopy and local calibration models,"The performance of local calibration models for quantitative measurements of ammonium and acetate on samples from an anaerobic digestion process was examined. The local calibration methods used were locally weighted regression (LWR) and multi-layer partial least squares (ML-PLS) regression. The results of these two methods were compared to each other and to the results from the global partial least squares (PLS) model regression as well. For ammonium, both the Local methods performed excellently in comparison with global PLS models. However, the results from the 150 LWR models regressed for ammonium also showed that the accuracy can be highly dependent on the different combination alternatives for model parameter settings and pre-processing alternatives. For this reason, a number of distance measures were evaluated as local subset selection methods in ML-PLS. The benefits of an optimised layer structure and the iterative approach in ML-PLS were also evaluated for ammonium. This showed that some benefits can be obtained by optimising the layer structure, at least in the sense that the number of layers can be reduced,and that there can be a significant advantage in using an iterative approach in the selection of the local subset of calibration data. The local calibration methods were also evaluated for acetate but, in this case, the benefits compared to global PLS calibration models were fairly insignificant with ML-PLS and none at all with LWR.",2013,10.1255/jnirs.1033,no
Introduction of a ternary diagram for comprehensive evaluation of gasification processes for ash-rich coal,"The present paper addresses the development of a comprehensive thermodynamic approach for the evaluation of gasification processes. A ternary diagram is introduced for a South African coal with an elevated ash content of 25.3 wt.%(wf). The ternary diagram allows the evaluation of most of the commercially applied gasification technologies depending on the three variables O-2, H2O and coal mass flow. Cold gas efficiency, dry CH4 yield, specific syngas production, H-2/CO ratio, CO/C and CH4/C selectivity as well as temperature and carbon conversion were selected as performance measures. Based on literature data, generic models of the commercial Shell, Siemens, ConocoPhillips, HTW and GE coal gasifications systems were developed enabling an integration into the ternary diagram at standardized boundary conditions. The graphical approach indicates the existence of optimum configurations for the specific gasifier types and leads to an individual potential assessment. At a typical gasification pressure of 30 bar, a theoretical maximum cold gas efficiency of 87.4% was identified at a temperature of 980 degrees C for the above mentioned coal, whereas the maximum syngas yield of 2.09 m(3)(H-2 + CO STP)/kg(waf) was located at 1135 degrees C. It is shown that only fluid-bed or two-stage processes have the potential to achieve these global maxima. The sensitivity of these maxima to varying ash contents from 5 to 45 wt.% and to coal rank is investigated as well. The study is concluded by the introduction of a simplified user diagram which was derived in order to drive a process towards the identified maxima. (C) 2012 Elsevier Ltd. All rights reserved.",2013,10.1016/j.fuel.2012.01.069,no
Dynamic modeling and experimental validation of a water desalination prototype by solar energy using humidification dehumidification process,"The present study investigates the dynamic functioning of a new solar desalination process based on the humidification/dehumidification (HD) principle. The design and the principle of working of the desalination system are described. The paper deals with the modeling, simulation aspects, and experimental validation of the main components of the system. Mathematical models were established including the mechanism of heat and mass transfer in the various components. These models use real meteorological data to predict the performance of a thermal solar driven system. Series of experiments were conducted and experimental results were then compared with the simulation results. Our findings showed that the developed models could be used to design and help in predicting the behavior of the installation in various meteorological conditions. (C) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.desal.2013.05.011,no
Data-driven model predictive quality control of batch processes,"The problem of driving a batch process to a specified product quality using data-driven model predictive control (MPC) is described. To address the problem of unavailability of online quality measurements, an inferential quality model, which relates the process conditions over the entire batch duration to the final quality, is required. The accuracy of this type of quality model, however, is sensitive to the prediction of the future batch behavior until batch termination. In this work, we handle this missing data problem by integrating a previously developed data-driven modeling methodology, which combines multiple local linear models with an appropriate weighting function to describe nonlinearities, with the inferential model in a MPC framework. The key feature of this approach is that the causality and nonlinear relationships between the future inputs and outputs are accounted for in predicting the final quality and computing the manipulated input trajectory. The efficacy of the proposed predictive control design is illustrated via closed-loop simulations of a nylon-6,6 batch polymerization process with limited measurements. (c) 2013 American Institute of Chemical Engineers AIChE J, 59: 2852-2861, 2013",2013,10.1002/aic.14063,no
A problem structuring method for ecosystem-based management: The DPSIR modelling process,"The purpose of this paper is to learn from Complex Adaptive Systems (CAS) theory to inform the development of Problem Structuring Methods (PSMs) both in general and in the specific context of marine management. The focus on marine management is important because it is concerned with a CAS (formed through the interconnection between natural systems, designed systems and social systems) which exemplifies their particularly 'wicked' nature. Recognition of this compels us to take seriously the need to develop tools for knowledge elicitation and structuring which meet the demands of CAS. In marine management, chief among those tools is the DPSIR (Drivers - Pressures - State Changes - Impacts Responses) model and, although widely applied, the extent to which it is appropriate for dealing with the demands of a CAS is questionable. Such questioning is particularly pertinent in the context of the marine environment where there is a need to not only recognise a broad range of stakeholders (a question of boundary critique) but also to manage competing knowledge (economic, local and scientific) and value claims. Hence this paper emphasises how a CAS perspective might add impetus to the development of a critical perspective on DPSIR and PSM theory and practice to promote a more systemic view of decision-making and policy development. (C) 2012 Elsevier B.V. All rights reserved.",2013,10.1016/j.ejor.2012.11.020,no
Analysis of the Efficiency of Applied Virtual Simulation Models and Real Learning Systems in the Process of Education in Mechatronics,"The rapid development of science and technology sets high demands for schools and faculties in terms of educating students to be able to manage complex technological systems. On the other hand, the application of new technologies in modern industry requires the creation of real and virtual laboratories capable of producing the conditions for the rapid and reliable transfer of new knowledge and skills from teachers to students. Having this in mind, the main focus of this paper is the realization of a module as a combination of virtual and real learning systems. The advantage of these systems is reflected in a possibility of creating virtual laboratories using three-dimensional models that simulate real industrial systems. Research shows that the use of virtual didactic systems in modern education increases motivation for learning, reduces learning time and enables modelling and simulation of real systems. However, in order to improve professional competencies, an interaction of simulation models with real industrial systems is needed.",2013,,no
Modeling of on-line measurement for rolling the rings with blank size errors in vertical hot ring rolling process,"The size errors of the ring blanks have a considerable effect on the vertical hot ring rolling process and the ring products' dimensional precision. In this paper, the geometric relationship between the ring's outer diameter and the measuring roll's displacement is studied, and then, the influence of the blank size errors on this geometric relationship is analyzed. Subsequently, a modeling method of the measurement model for vertical hot ring rolling process is proposed, that is, the ring's outer diameter can be measured indirectly through the relations of the ring's outer diameter with respect to the measuring roll's displacement and the ring blank's outer diameter. Using this modeling method, the on-line measurement model of a certain type of vertical hot ring rolling mill was established, and the verification experiments were also conducted. The experiments showed that this on-line measurement model could calculate the ring's outer diameter precisely, and the absolute errors of the formed rings were less than +/- 1.5 mm. The research results of this paper have general significance for the on-line measurement and process control of vertical hot ring rolling process.",2013,10.1007/s00170-013-4726-2,no
Intelligent modeling using fuzzy rule-based technique for evaluating wood carbonization process parameters,"The structural changes of the porosity in three wood species in a pyrolysis system at several temperature ranging and time periods were investigated to study the wood carbonization characteristics. Rectangular cuboid wood samples were dried and then carbonized in an inert atmosphere furnace and their mass and dimensional changes were recorded before and after process. SEM observation indicated that anatomical feature of final porous carbon remains unchanged with respect to the initial wood precursor. This research also intends to develop an intelligence model based on fuzzy logic theory. The model considers the final density as the end result of the process and establishes relations with carbonization process parameter (carbonization temperature, carbonization time period, initial density of wood) on the basis of fuzzy linguistic rules. Besides, a regression equation was established between above parameters and afterward, considering the constant of the derived model, significance of each one was identified. The results of the fuzzy model were found to be very close to the experimental data and show the possibility of improving rule-based modeling for such engineering challenges.",2013,10.1007/s00170-013-4935-8,no
Research on Relevance Modeling Method for Equipment Testability in Manufaturing Process,"The task of equipment testability design should be carried out during manufacturing process, to improve the diagnostic capabilities of equipment. According to the equipment testing requirements, equipment failure modes and effects analysis were completed, all possible failure modes of the equipment design and manufacturing process were ensured, as well as the causes and impact of each failure mode, whereby the equipment functions and structure were divided. Then, the relevance model was used for testability analysis and modeling of equipment in manufaturing process, relevance graphical model and mathematics model of component units related to equipment were builded, and the preferred method was used to consider the reliability and cost, the equipment diagnostic tree was builded, the equipment testing program was presented. A numerical example indicates that the study can effectively improve the efficiency of the equipment testing and enhance the economy of the test in manufaturing process.",2013,10.4028/www.scientific.net/AMM.252.372,no
A temperature dependent multi-ion model for time accurate numerical simulation of the electrochemical machining process. Part III: Experimental validation,"The temperature distribution and shape evolution during electrochemical machining (ECM) are the result of a large number of interacting physical processes. Electrolyte flow, electrical conduction, ion transport, electrochemical reactions, heat generation and heat transfer strongly influence one another, making modeling and numerical simulation of ECM a very challenging procedure. In part I [1], a temperature dependent multi-ion transport and reaction model (MITReM) is put forward which considers mass transfer as a consequence of diffusion, convection and migration, combined with the electroneutrality condition and linearized temperature dependent polarization relations at the electrode-electrolyte interface. The flow field is calculated using the incompressible laminar Navier-Stokes equations for viscous flow. The local temperature is obtained by solving internal energy balance, enabling the use of temperature dependent expressions for several physical properties such as the ion diffusion coefficients and electrolyte viscosity. In part II [2], the temperature dependent MITReM is used to simulate ECM of stainless steel in aqueous NaNO3 electrolyte solution. The effects of temperature, electrode thermal conduction, reaction heat generation, electrolyte flow and water depletion are investigated and a comparison is made between the temperature dependent potential model and MITReM. In this third part, the theoretical model is validated against ECM experiments in a flow-channel cell. The model is further optimized by including the effect of metal hydration and non-linear polarization relations. A close match is obtained between experiment and simulation. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.electacta.2013.04.059,no
Error estimation properties of Gaussian process models in stochastic simulations,"The theoretical relationship between the prediction variance of a Gaussian process model (GPM) and its mean square prediction error is well known. This relationship has been studied for the case when deterministic simulations are used in GPM, with application to design of computer experiments and metamodeling optimization. This article analyzes the error estimation of Gaussian process models when the simulated data observations contain measurement noise. In particular, this work focuses on the correlation between the GPM prediction variance and the distribution of prediction errors over multiple experimental designs, as a function of location in the input space. The results show that the error estimation properties of a Gaussian process model using stochastic simulations are preserved when the signal-to-noise ratio in the data is larger than 10, regardless of the number of training points used in the metamodel. Also, this article concludes that the distribution of prediction errors approaches a normal distribution with a variance equal to the GPM prediction variance, even in the presence of significant bias in the GPM predictions. (C) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.ejor.2012.12.033,no
Cluster analysis by self-organizing maps: An application to the modelling of water quality in a treatment process,"The unit processes in water treatment involve many complex physical and chemical phenomena which are difficult to assess using traditional data analysis methods. Moreover, measurement data gathered from the process is often challenging with respect to modelling purposes, because there is a lack of continuous online measurements, for which sparse laboratory measurement data have to be conducted to compensate them. This paper reports on the application of self-organizing map (SOM) techniques combined with K-means clustering to model water quality in the treatment of drinking water. At the first phase of the study, a SOM was produced by using both on-line and laboratory data of the treatment process and raw water. At the second phase, the reference vectors of the map were classified by K-means algorithm into clusters, which can be used to present different states of the process. At the final phase, the results were interpreted by analyzing the reference vectors in the clusters. The introduced approach offers a straightforward method for assessing the essential characteristics of the process. In addition, the results clearly demonstrate some challenges in the modelling of water quality in treatment processes. (C) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.asoc.2013.01.027,no
Fundamental al Problems of Modeling the Fracture Processes in Concrete I: Micromechanics and Localization of Damages,"Theory of concrete fracture, despite all the efforts of numerous researchers, still did not provide the clear answer to the problem of modeling the fracture processes of concrete. Three well known theories are at hand: fracture mechanics, plasticity theory and mechanics of continuous damages. The fundamental assumptions, those theories are based on, do not completely correspond to the nature of concrete. They all are confronted with numerous problems, out of which the four are fundamental: damages micromechanics, damages localization, size effects and the dilemma when to apply the phenomenological and when the micromechanical approach to considering this problem. In this paper are considered the first two of those problems. (C) 2013 The Authors. Published by Elsevier Ltd.",2013,10.1016/j.proeng.2013.09.029,no
Fundamental Problems of Modeling the Fracture Processes in Concrete II: Size Effect and Selection of the Solution Approach,"Theory of concrete fracture, despite all the efforts of numerous researchers, still did not provide the clear answer to the problem of modeling the fracture processes of concrete. Three well known theories are at hand: fracture mechanics, plasticity theory and mechanics of continuous damages. The fundamental assumptions, those theories are based on, do not completely correspond to the nature of concrete. They all are confronted with numerous problems, out of which the four are fundamental: damages micromechanics, damages localization, size effects and the dilemma when to apply the phenomenological and when the micromechanical approach to considering this problem. In this paper are considered the last two of those problems: size effect (scaling laws) and decision making what would be the best way in solving problems of modeling the fracture processes in concrete and concrete structures. (C) 2013 The Authors. Published by Elsevier Ltd.",2013,10.1016/j.proeng.2013.09.030,no
Understanding and Designing Business Process Modelling for Emergency Plan,"Thereafter the Fukushima setback, different resolutions are on-going to improved arrangements among agencies responsible for the safety of such facilities and jointly developed reasonable precautions that will mitigate future calamities. This paper accomplished to recognize and represent suggested business processes for the Malaysian Nuclear Power Generating Program (NPP) based on Alter's Work System Theory (WST). The work system method often considered as effective in describing the organizational system such as NPP. This method applied to frame and understands the user requirements within the business case of NPP. As a result, the business process was determined in respect of the emergency plan accordingly. This case study analytically processed by adopting qualitative grounding processes. These processes involved coding interpretation which providing structural analysis of the suggested system requirements in a form of business process modelling. This analysis comprising process fabrication using member checking technique and workflow driven approach, which then transformed into business process simulation in the future.",2013,,no
Spotting Terminology Deficiencies in Process Model Repositories,"Thinking in business processes and using process models for their documentation has become common practice in companies. In many cases this documentation encompasses more than thousands of models. One of the key challenges is achieving consistency of the process model terminology. Especially, the usage of synonym and homonym words is one of the most severe problems for terminological consistency. Therefore, this paper presents an automatic approach to identify synonym and homonym words in model repositories. We challenged the approach against three model collection from practice that are assumed to have different levels of terminological consistency. The evaluation shows that the approach is capable to fulfill these goals and to identify meaningful synonym and homonym candidates for follow-up resolution.",2013,,no
Validating a Safety Climate Model in Metal Processing Industries: A Replication Study,"This paper attempts to replicate a safety climate model originally tested in Australia to assess its applicability in a different context: namely, across production workers in 22 medium-sized metal processing organizations in Austria. The model postulates that safety knowledge and safety motivation mediate the relation between safety climate on the one hand and safety compliance and participation on the other. Self-report data from 1075 employees were analyzed using structural equation modeling (SEM). The results of the replication study largely confirmed the original safety climate model. However, in addition to indirect effects, direct links between safety climate and actual safety behavior were found.",2013,,no
Sensitivity Analysis of Models for High Consistency Refining Process,"This paper deals with the sensitivity analysis of models for the high consistency refining process in the pulp and paper industry. Several models reported in literature are considered, which are screened for their potential use in the design of controllers for refiners or in the optimization of refiner operation. For the two most promising models a sensitivity analysis is conducted which suggests that the temperature profile inside the refiner and the inlet consistency are the most important process variables. It is further suggested that both variables variables are utilized to control the refining process based on these models. Moreover, computer simulations with constant, linear increasing and parabolic temperature profiles are performed, which confirm the importance of the temperature profile for the entropy based model.",2013,,no
A model-based methodology for the analysis and design of atomic layer deposition processes-Part II: Experimental validation and mechanistic analysis,"This paper demonstrates the experimental validation and mechanistic analysis of the continuous cross-flow atomic layer deposition (ALD) reactor model developed in the first article of this series (Holmqvist et al., in press). A general nonlinear parameter estimation problem was formulated to identify the kinetic parameters involved in the developed ALD gassurface reaction mechanism, governing ZnO film growth, from ex situ film thickness measurements. The presented methodology for comprehensive model assessment considers the statistical uncertainty of least-squares estimates and its ultimate impact on the model predicted response. Joint inference regions were determined to assess the significance of parameter estimates and results indicate that all estimates involved in the precursor half-reactions were adequately determined. The reparameterization of the Arrhenius equation effectively decreased the characteristically high correlations between Arrhenius parameters, leading to improvement in precision of individual parameter estimates. Model predictions of the spatially dependent film thickness profile with narrow confidence band were in good agreement with both calibration and validation experimental data, respectively, under a wide range of operating conditions. The subsequent extensive theoretical analysis exhibits that the experimentally validated model successfully reproduces the detailed process dynamics revealed by in situ quartz crystal microbalance and quadrupole mass spectroscopy diagnostics, and thereby provides a supplementary analysis tool. Finally, the univariate sensitivity analysis revealed the mechanistic dependence of all the measured process operating parameters on the spatially dependent film thickness profile, resolved at the level of a single pulse sequence. Hence, the presented model-based framework serves as a means to guide future research efforts in the field of ALD process optimization. (c) 2012 Elsevier Ltd. All rights reserved.",2013,10.1016/j.ces.2012.06.063,no
The Integrated Vehicle Health Management Development Process: Verification and Validation of Simulation Models,"This paper describes an end-to-end Integrated Vehicle Health Management (IVHM) development process with a strong emphasis on the verification and validation of simulation models constructed during its implementation. The simulations are both physical and functional representations of the complex system being considered. The paper proposes guidelines in developing the appropriate functional model, followed by a novel technique in which the qualitative information captured in the functional representation is verified and validated against the quantitative information offered by the physical model of the same system. Further, both physical and functional models are verified by comparison with rig data. This verification and validation process enables the development of an automated Functional FMECA (Failure Modes Effects and Criticality Analysis) by systematically capturing all the effects of the considered failure modes on the rest of the system components. The concepts engaged in this process are demonstrated on a laboratory UAV fuel delivery system test rig, but they have the ability to be further applied to both new and legacy hi-tech high-value systems.",2013,10.3303/CET1333021,no
"A Matter of Life and Death: Validating, Qualifying and Documenting Models for Simulating Flow-Related Accident Scenarios in the Process Industry","This paper describes an integrated approach for validating, qualifying and documenting numerical models for simulating complex systems. Although the example used to illustrate the process entails simulations of accident scenarios in the petroleum and process industries by means of computational fluid dynamics (CFD), the methodology is not restricted to any particular model or system. CFD tools are applicable to various aspects of societal safety, including transportation, storage and use of various energy carriers, as well as malicious attacks involving toxic gas or condensed explosives. The approach adopted involves a continuous process where relevant validation cases are classified according to the physical phenomena involved, and prioritized based on parameters such as relevance for typical applications of the model system, measurement quality and repeatability, availability of data, spatial scale, materials or substances used, etc. A model evaluation protocol (MEP) provides guidelines for prioritizing the various validation cases, and for evaluating the simulation results. Statistical methods and visualization techniques are employed for describing the validation range and the associated uncertainties of the model system. Use of the methodology is illustrated for a typical application of the commercial CFD tool FLACS: large-scale gas explosions in congested geometries. The results highlight some of the inherent challenges associated with the interpretation of results from large-scale experiments, and demonstrate how such challenges can be addressed during the model evaluation process. The methodology can be extended to include sensitivity studies and advanced optimization schemes for key model parameters.",2013,10.3303/CET1331032,no
Inverse Gaussian process-based corrosion growth model for energy pipelines considering the sizing error in inspection data,"This paper describes an inverse Gaussian process-based model to characterize the growth of the depth of corrosion defects on underground energy pipelines based on inspection data. The model is formulated in a hierarchical Bayesian framework, which allows consideration of uncertainties from different sources. The Markov Chain Monte Carlo (MCMC) simulation techniques are used to evaluate the probabilistic characteristics of the model parameters by incorporating the defect depths reported by multiple in-line inspections (ILIs) as well as the prior knowledge about these parameters. The bias and random scattering error associated with the ILI tool as well as the correlation between the random scattering errors associated with different ILL tools are considered in the analysis. An example involving real ILL data collected from an in-service pipeline is employed to illustrate the application of the growth model. The results indicate that the model in general can predict the growth of corrosion defects reasonably well. The proposed model can be used to facilitate the development and application of reliability-based pipeline corrosion management. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.corsci.2013.04.020,no
A Process for Maintaining Heterogeneous Models Consistency through Change Synchronization,"This paper falls into the context of modeling complex systems according to various viewpoints. More precisely, it presents an iterative process of heterogeneous models consistency management - by taking into account various types of evolution - based on building a correspondence model. In the case of models evolution, this process is intended to capture changes in the models, to list modifications to be made in the impacted models and finally to update the correspondence model for a future iteration.",2013,,no
"Capacitive Sensor for Measuring the Filled of Post-Tensioned Ducts. Experimental Setup, Modeling and Signal Processing","This paper presents a newly developed system for bridges post-tensioned devices inspection. These devices are generally composed of cables placed inside a duct, the residual space being filled with a particular cement to avoid corrosion. The nondestructive experimental setup uses a capacitive sensor that gives some relevant information about the electrical properties of the materials located inside the duct. Then, using an original modeling of the interactions between the sensor and the post-tensioned device, a signal processing extracts useful information from the sensors data, like the thickness of an eventual air gap between the cement and the duct. Experimental and theoretical derivations and comparisons are presented in the different parts of this paper.",2013,10.1109/JSEN.2012.2219392,no
A genetic algorithm for solving a multi-floor layout design model of a cellular manufacturing system with alternative process routings and flexible configuration,"This paper presents a novel integer linear programming model for designing multi-floor layout of cellular manufacturing systems (CMS). Three major and interrelated decisions are involved in the design of a CMS; namely cell formation (CF), group layout (GL), and group scheduling (GS). A novel aspect of this model is concurrently making the CF and GL decisions to achieve an optimal design solution in a multi-floor factory. Other compromising aspects are: multi-floor layout to form cells in different floors is considered, multi-rows layout of equal area facilities in each cell is allowed, cells in flexible shapes are configured, and material handling cost based on the distance between the locations assigned to machines are calculated. Such an integrated CMS model with an extensive coverage of important manufacturing features has not been proposed before and this model incorporates several design features including alternative process routings, operation sequence, processing time, production volume of parts, duplicate machines, machine capacity, new machine purchasing, lot splitting, material flow between machines, intra-cell layout, inter-cell layout, multi-floor layout and flexible configuration. The objective is to minimize the total costs of intra-cell, inter-cell, and inter-floor material handling, new machines purchasing and machine processing. Two numerical examples are solved by the Lingo software to verify the performance of the proposed model and illustrate the model features. Sensitive analysis is also implemented on some model parameters. An improved genetic algorithm (GA) is proposed to derive near-optimal solutions for the integrated model because of its NP hardness. It is then tested using several problems with different sizes and settings to verify the computational efficiency of the developed algorithm in comparison to a classic simulated annealing algorithm and the Lingo software. The obtained results show the efficiency of proposed GA in terms of objective function value and computational time.",2013,10.1007/s00170-012-4370-2,no
Tool for Automatic Layout of Business Process Model Represented by UML Activity Diagram,This paper presents a software tool for the automatic visualization of the UML activity diagram representing a business process model. The implemented tool takes the XMI-based representation of a programmatically generated activity diagram as the input and automatically generates its layout in accordance with the UMLDI specification. This auto layout feature is implemented as an Eclipse-Topcased plug-in named VisualAD.,2013,,no
A Dynamic Approach to Process Design: A Pattern for Extending the Flexibility of Process Models,"This paper presents a specific approach to Business Process design by combining selected principles of Adaptive Case Management, traditional modeling of processes executable in Business Process Management Systems, and a constraint-based approach to process design. This combined approach is intended for business situations, where traditional process models with rigid structures can lead to limitations of business flexibility. We propose a process design pattern that is suitable for the modeling of ad-hoc processes within common BPMS-based systems. The pattern can be used to define a process structure in a declarative constraint-based manner. Further, we present an application of the approach in an actual project, which is an end-to-end BPM project from an insurance business. The project uncovered needs for an extended flexibility of process structures. This along with requirements based on ad-hoc processes led to advancement in the presented approach. This paper presents a versatile, generally applicable solution, which was later tailored for the purpose of the aforementioned project and led to the successful satisfaction of the requirements. The approach is part of a more comprehensive research effort - complex BPM adoption methodology BPM4SME designed primarily for Small and Medium Enterprises, which put emphasis on the agility of the BPM adoption process and consequent flexible implementations of BPMS-based systems.",2013,,no
Verification of a River Ice Process Model Using Continuous Field Measurements,"This paper uses both numerical modeling and field measurements to describe river ice processes. River ice regime plays a significant role in river hydraulics and morphology in higher latitudes of the northern hemisphere, with important implications for hydro-electrical power generation operations. An ice-cover changes various river characteristics, including water stage and velocity distribution. These variations can influence sediment transport and channel morphodynamics, particularly during extreme conditions when ice-jamming and break-up can locally accelerate the flow, and ice can mechanically scour the river bed and banks. Although the influence of ice on rivers is tremendous and clear, it is not well studied largely due to the difficulty and danger of river ice field studies, especially during the unstable break-up period. In this paper we studied river ice conditions in the Lower Nelson River, Northern Manitoba, Canada, during winter 2012 using both numerical modeling and In-situ autonomous field measurements to validate the model predictions. The accuracy of numerical models is an intrinsic concern, especially in the analysis of dynamic processes such as river ice, and verification with field data is essential.. The ICESIM was originally developed by Acres International Limited (now Hatch) in 1973 for studies of the Nelson River hydroelectric plants; since then it has been continuously improved. The field data were collected from a bottom mounted instrument pod deployed below the ice-cover for a four-month period (March-June 2012), which included both the stable cover and break-up periods. Instrumentation included a 546 kHz ASL Multi-Functional Acoustic Water Column Profiler (MF-AWCP) and 1200kHz TELEDYN RD Instrument Acoustic Doppler Current Profiler for measurements of water velocity. The MF-AWCP could measure frazil ice particles in the water column and more importantly, the variation in the condition and thickness of the ice cover. In this paper we demonstrate the model capability by comparing measured and model-calculated results, focusing specifically on the model's ability to predict ice thickness and timing of ice-cover stages i.e. freeze-up, stable ice-cover, and breakup.",2013,,no
Evaluation of control strategies for drinking water treatment plants using a process model,"This research adds a method to evaluate control strategies to the design methodology for drinking water treatment plants. A process model dealing with parameters related to the calcium carbon dioxide equilibrium was set up. Using the process model, the existing control strategy was compared with a new control strategy and the effects of two different sets of input data were studied. It was demonstrated that the efficiency of the pellet softening process and the plant's capacity were increased, and that chemicals and energy usage were reduced. At the same time, the deviation of the total hardness of the produced water to the desired value was decreased.",2013,10.2166/aqua.2013.031,no
"Applying the Context, Input, Process, Product Evaluation Model for Evaluation, Research, and Redesign of an Online Master's Program","This study aimed to evaluate and redesign an online master's degree program consisting of 12 courses from the informatics field using a context, input, process, product (CIPP) evaluation model. Research conducted during the redesign of the online program followed a mixed methodology in which data was collected through a CIPP survey, focus-group interview, and open-ended questionnaire. An initial CIPP survey sent to students, which had a response rate of approximately 60%, indicated that the Fuzzy Logic course did not fully meet the needs of students. Based on these findings, the program managers decided to improve this course, and a focus group was organized with the students of the Fuzzy Logic course in order to obtain more information to help in redesigning the course. Accordingly, the course was redesigned to include more examples and visuals, including videos; student-instructor interaction was increased through face-to-face meetings; and extra meetings were arranged before exams so that additional examples could be presented for problem-solving to satisfy students about assessment procedures. Lastly, the modifications to the Fuzzy Logic course were implemented, and the students in the course were sent an open-ended form asking them what they thought about the modifications. The results indicated that most students were pleased with the new version of the course.",2013,,no
Evaluation of Model Simulation and Field Experiment on Collision-Attachment Efficiency for Operational Factors in Dissolved Air Flotation Process,"This study conducted a series of tests and simulations to estimate collision-attachment efficiency using a trajectory analysis method, to evaluate model sensitivity due to major impact factors in designing and operating dissolved air flotation (DAF) for the various types of wastewaters, and to find the possibility of model application in determining the flotation efficiency of DAF. Collision-attachment efficiency between a bubble and a particle in the model is described by hydrodynamics related to stream function in Stokes' flow and surface forces based on a classic DLVO theory, encoded using Matlab language software, and performed numerical analysis using the 5th Runge-Kutta method. Model simulation examined the effects of two major factors, size and zeta potential, on collision-attachment efficiency during flotation and field experiment evaluated to model results. Both results from the simulation and field experiment indicated that the larger particles presented the better collision and removal efficiency and the effect of particle zeta potential on collision-attachment efficiencies was increased as the particle zeta potential increased from the negative domain to the positive domain. Therefore, the collision-attachment efficiency model using the trajectory analysis method may be used in estimating the flotation efficiency and in explaining its characteristics.",2013,10.1080/01496395.2013.804838,no
"Sulfate-reduction, sulfide-oxidation and elemental sulfur bioreduction process: Modeling and experimental validation","This study describes the sulfate-reducing (SR) and sulfide-oxidizing (SO) process using Monod-type model with best-fit model parameters both being reported and estimated. The molar ratio of oxygen to sulfide (R-OS) significantly affects the kinetics of the SR + SO process. The S-0 is produced by SO step but is later consumed by sulfur-reducing bacteria to lead to ""rebound"" in sulfide concentration. The model correlated well all experimental data in the present SR + SO tests and the validity of this approach was confirmed by independent sulfur bioreduction tests in four denitrifying sulfide removal (DSR) systems. Modeling results confirm that the ratio of oxygen to sulfide is a key factor for controlling S-0 formation and its bioreduction. Overlooking S bioreduction step would overestimate the yield of S-0. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.biortech.2013.07.113,no
Integrated Population Balance Model Development and Validation of a Granulation Process,"This study is concerned with the development of an integrated three-dimensional population balance model (PBM) that describes the combined effect of key granulation mechanisms that occur during the course of a granulation process. Results demonstrate the importance of simulating the different mechanisms within a population balance model framework to elucidate realistic granulation dynamics. The incorporation of liquid addition in the model also aids in demarcating the dynamics in different regimes such as premixing, granulation (during liquid addition) and wet massing (after liquid addition). For the first time, the effect of primary particle size distributions and mode of binder addition on key granule properties was studied using an integrated PBM. Experimental data confirms the validity of the overall model as compared to traditional models in the literature that do not integrate the different granulation mechanisms.",2013,10.1080/02726351.2013.767295,no
CFD modeling of the high-temperature HVPE growth of aluminum nitride layers on c-plane sapphire: from theoretical chemistry to process evaluation,"This study presents numerical modeling based on a relatively limited number of gas-phase and surface reactions to simulate the growth rate of aluminum nitride layers on AlN templates and c-plane sapphire in a broad range of deposition parameters. Modeling results have been used to design particular experiments in order to understand the influence of the process parameters on the crystal quality of AlN layers grown in a high-temperature hydride vapor-phase epitaxy process fed with NH3, AlCl3, and H-2. Modeling results allow to access to very interesting local quantities such as the surface site ratio and local supersaturation. The developed universal model starting from local parameters might be easily transferred to other reactor geometry and process conditions. Among the investigated parameters (growth rate, temperature, local supersaturation, gas-phase N/Al ratio, and local surface site N/Al ratio), only the growth rate/supersaturation or growth rate/temperature relationships exhibit a clear process window to use in order to succeed in growing epitaxial AlN layers on c-plane sapphire or AlN templates. Gas-phase N/Al ratio and local surface site N/Al ratio seem to play only a secondary role in AlN epitaxial growth.",2013,10.1007/s00214-013-1419-8,no
Model based evaluation of six energy integration schemes applied to a small-scale gasification process for power generation,"This work considers the use of spent poultry litter as a fuel for on-site power generation. On-site use eliminates the need for the transportation of biomass to centralised plants and the associated bio-security issues. This work utilised process simulation to investigate six process integration schemes applied to a small scale gasification unit with a gas turbine prime mover. The model was used to evaluate schemes involving atmospheric gasification, pressurised gasification and recuperation of energy from the gas turbine exhaust gases. The recuperation of residual heat to preheat air and produced gases was performed with the aim of achieving the highest electrical efficiency. The cold gasification and exergy efficiencies were in the ranges of 58.4-79.5% and 46.8-65.7%, respectively, which mainly increased with increasing ER and then after achieving the maximum value declined. The preferred configuration of the proposed 200-kW process achieved electrical efficiencies between 26% and 33.5%. (C) 2013 Elsevier Ltd. All rights reserved.",2013,10.1016/j.biombioe.2013.03.024,no
"On the inclusion of ground-based gravity measurements to the calibration process of a global rainfall-discharge reservoir model: case of the Durzon karst system (Larzac, southern France)","This work examines the relevance of the inclusion of ground-based gravity data in the calibration process of a global rainfall-discharge reservoir model. The analysis is performed for the Durzon karst system (Larzac, France). The first part of the study focuses on the hydrological interpretation of the ground-based gravity measurements. The second part of the study investigates further the information content of the gravity data with respect to water storage dynamics modelling. The gravity-derived information is found unable to either reduce equifinality of the single-objective, discharge-based model calibration process or enhance model performance through assimilation.",2013,10.1007/s12665-012-1856-z,no
Spatial heterodyne spectrometer: modeling and interferogram processing for calibrated spectral radiance measurements,"This work presents a radiometric model of a spatial heterodyne spectrometer (SHS) and a corresponding interferogram-processing algorithm for the calculation of calibrated spectral radiance measurements. The SHS relies on Fourier Transform Spectroscopy (FTS) principles, and shares design similarities with the Michelson Interferometer. The advantages of the SHS design, including the lack of moving parts, high throughput, and instantaneous spectral measurements, make it suitable as a field-deployable instrument. Operating in the long-wave infrared (LWIR), the imaging SHS design example included provides the capability of performing chemical detection based on reflectance and emissivity properties of surfaces of organic compounds. This LWIR SHS model outputs realistic, interferometric data and serves as a tool to find optimal SHS design parameters for desired performance requirements and system application. It also assists in the data analysis and system characterization. The interferogram-processing algorithm performs flat-fielding and phase corrections as well as apodization before recovering the measured spectral radiance from the recorded interferogram via the Inverse Fourier Transform (IFT). The model and processing algorithm demonstrate results comparable to those in the literature with a noise-equivalent change in temperature of 0.35K. Additional experiments show the algorithm's real-time processing capability, indicating the LWIR SHS system presented is feasible",2013,10.1117/12.2023765,no
Tier 2 Team Processes and Decision-Making in a Comprehensive Three-Tiered Model,"Three-tiered models of academic and behavioral support are being increasingly adopted across the nation, and with that adoption has come an increasing message that designing and implementing effective practices alone is not enough. Systems are needed to help staff to collectively implement best practices. These systems, as well as effective processes for collecting and using data for decision-making, are necessary at each tier. This article outlines the systems considerations and decision-making processes for Tier 2 adopted by one elementary school in the implementation of a combined response to intervention (RTI) and school-wide positive behavior support (SWPBS) model.",2013,10.1177/1053451212463961,no
The influence of the segmentation process on 3D measurements from cone beam computed tomography-derived surface models,"To compare the accuracy of linear and angular measurements between cephalometric and anatomic landmarks on surface models derived from 3D cone beam computed tomography (CBCT) with two different segmentation protocols was the aim of this study. CBCT scans were made of cadaver heads and 3D surface models were created of the mandible using two different segmentation protocols. A high-resolution laser surface scanner was used to make a 3D model of the macerated mandibles. Twenty linear measurements at 15 anatomic and cephalometric landmarks between the laser surface scan and the 3D models generated from the two segmentation protocols (commercial segmentation (CS) and doctor's segmentation (DS) groups) were measured. The interobserver agreement for all the measurements of the all three techniques was excellent (intraclass correlation coefficient 0.97-1.00). The results are for both groups very accurate, but only for the measurements on the condyle and lingual part of the mandible, the measurements in the CS group is slightly more accurate than the DS group. 3D surface models produced by CBCT are very accurate but slightly inferior to reality when threshold-based methods are used. Differences in the segmentation process resulted in significant clinical differences between the measurements. Care has to be taken when drawing conclusions from measurements and comparisons made from different segmentations, especially at the condylar region and the lingual side of the mandible.",2013,10.1007/s00784-012-0881-3,no
Evaluation of gastric processing and duodenal digestion of starch in six cereal meals on the associated glycaemic response using an adult fasted dynamic gastric model,To identify the key parameters involved in cereal starch digestion and associated glycaemic response by the utilisation of a dynamic gastro-duodenal digestion model. Potential plasma glucose loading curves for each meal were calculated and fitted to an exponential function. The area under the curve (AUC) from 0 to 120 min and total digestible starch was used to calculate an in vitro glycaemic index (GI) value normalised against white bread. Microscopy was additionally used to examine cereal samples collected in vitro at different stages of gastric and duodenal digestion. Where in vivo GI data were available (4 out of 6 cereal meals) no significant difference was observed between these values and the corresponding calculated in vitro GI value. It is possible to simulate an in vivo glycaemic response for cereals when the gastric emptying rate (duodenal loading) and kinetics of digestible starch hydrolysis in the duodenum are known.,2013,10.1007/s00394-012-0386-5,no
Assessment of model validity of analytic network process using structural equations modelling: an application of supplier evaluation problem,"To model decision problems properly has a direct effect on the quality of the solution. The aim of this study is to minimise the errors arising from the decision maker during modelling for increasing problem-solving efficiency of analytic network process (ANP) method. In order to achieve this aim, a two-stage approach was proposed in this study: 1 the decision problem was modelled by ANP based on experts' opinions 2 the model is assessed by structural equation modelling (SEM) for evaluate its validity and to purify it from expert sourced errors. To prove that, a supplier evaluation problem examined and modelled with ANP. Then the ANP model is discussed as a structural model and verified statistically by SEM. After evaluation, better representation of the decision problem has been provided. This study is the first to statistically evaluate the ability of modelling of ANP approach that is widely used in literature. [Submitted 03 January 2011; Revised 26 May 2011; Accepted 21 June 2011].",2013,10.1504/EJIE.2013.051597,no
A stochastic model of attack process for the evaluation of security metrics,"To trust a computer system that is supposed to be secure, it is necessary to predict the degree to which the system's security level can be achieved when operating in a specific environment under cyber attacks. In this paper, we propose a state-based stochastic model for obtaining quantitative security metrics representing the level of a system's security. The main focus of the study is on how to model the progression of an attack process over time. The basic assumption of our model is that the time parameter plays the essential role in capturing the nature of an attack process. In practice, the attack process will terminate successfully, possibly after a number of unsuccessful attempts. What is important is, indeed, the estimation of how long it takes to be conducted. The proposed stochastic model is parameterized based on a suitable definition of time distributions describing attacker's actions and system's reactions over time. For this purpose, probability distribution functions are defined and assigned to transitions of the model for characterizing the temporal aspects of the attacker and system behavior. With the definition of the distributions, the stochastic model will be recognized to be a semi-Markov chain. This mathematical model will be analytically solved to calculate the desirable quantitative security metrics, such as mean time to security failure and steady-state security. The proposed method shows a systematic development of the stochastic modeling techniques and concepts, used frequently in the area of dependability evaluation, for attack process modeling. Like any other modeling method, the proposed model is also constructed based on some underlying assumptions, which are specific to the context of security analysis. (C) 2013 Elsevier B.V. All rights reserved.",2013,10.1016/j.comnet.2013.03.011,no
"Evaluation and Analysis of Two Village Level Models for Sweet Sorghum Cane Processing in Ilocos Norte, Philippines","Two sweet sorghum cane processing systems consisting of a stationary cane mill (SCM) and a mobile cane mill (MCM) that became operational as a village-level sweet sorghum processing system in Ilocos Norte were evaluated. With SCM, the stalk crusher was permanently established in barangay Bungon, Batac, Ilocos Norte as the central processing station (CPS). Harvested stalks were hauled by trucks from production sites to the CPS for processing. For the MCM model, a reconditioned jeep carries the crusher to the plantation sites where stalks are crushed. Extracted juices were hauled to the CPS for processing. Cost wise, MCM is PhP 100,000 more expensive than SCM (PhP 150,000) due to the cost of the jeep. The stakeholders had high confidence that the two village level cane mill systems could fastrack sweet sorghum processing into a desired product which could translate to income generation and profitability. The advantages of SCM over MCM are the lower energy cost and lower biomass fuel cost of pasteurization due to the 80% bagasse component of fuel. With the MCM, biomass weight lost due to staging did not occur, and crushing efficiency (CE) and juice yield were greater by 5.15% and 309 kg, respectively than the SCM. This must be due to the closer roller clearance in the MCM. The CE of a modified laboratory model (MLCM) was reported at 51.6% greater by 22.15% than the SCM (29.45%) and by 16.95% than the MCM (34.6%). Such factors should be given particular considerations in the SCM or MCM to increase CE and make them more effective for village level sweet sorghum processing. If improved, the SCM and MCM systems will be significant initiatives which could be replicated for use in sweet sorghum communities intending to indulge in the bioethanol industry.",2013,,no
Incorporating Setting Information for Maintenance-Free Quality Modeling of Batch Processes,"Typically, the operation condition of the batch process is changed frequently, following different recipes or manufacturing various production grades. For quality prediction purpose, the prediction model should also be updated or rebuilt, which leads to a significant model maintenance effort, especially for those processes which have various phases. To reduce such effort, a maintenance-free method is proposed in this article, which incorporates the setting information of the batch process for modeling. The whole process variations are separated into two parts: setting information related and other quality related variations. By constructing a relationship between setting variables and other process variables, the data variations explained by the setting information can be efficiently removed. Then, a robust regression model connecting process variables to the quality variable is developed in different phases of the batch process. The feasibility and effectiveness of the proposed method is evaluated through an industrial injection molding process. (C) 2012 American Institute of Chemical Engineers AIChE J, 59: 772-779, 2013",2013,10.1002/aic.13864,no
Cross-Domain Model Building and Validation (CDMV): A New Modeling Strategy to Reinforce Understanding of Nanomanufacturing Processes,"Understanding nanostructure growth faces issues of limited data, lack of physical knowledge, and large process uncertainties. These issues result in modeling difficulty because a large pool of candidate models almost fit the data equally well. Through the Integrated Nanomanufacturing and Nanoinformatics (INN) strategy, we derive the process models from physical and statistical domains, respectively, and reinforce the understanding of growth processes by identifying the common model structure across two domains. This cross-domain model building strategy essentially validates models by domain knowledge rather than by (unavailable) data. It not only increases modeling confidence under large uncertainties, but also enables insightful physical understanding of the growth kinetics. We present this method by studying the weight growth kinetics of silica nanowire under two temperature conditions. The derived nanowire growth model is able to provide physical insights for prediction and control under uncertainties.",2013,10.1109/TASE.2013.2243433,no
ACQUIRE HIGH QUALITY MESHES OF SCALE MODELS FOR AN AUTOMATIC MODELLING PROCESS,"Urban scale models depicting whole towns such as the hundred-scale model collection known as plans-reliefs are a valuable source of information of cities and their surroundings. These physical representations of French strongholds from the 17th through the 19th century suffer from many problems that are, among other things, wear and tear or the lack of visibility and accessibility. A virtual collection would allow remote accessibility for visitors as well as history researchers. Moreover, it may also be linked to other digital collections and therefore, promote the collection to make people come to the museums to see the physical scale models. We also work on other physical town scale models like Epinal for which the scale is a bit higher. In a first part, we define a protocol for acquiring 3D meshes of town scale models from both photogrammetric and scanning methods. Then we compare the results of both methods The photogrammetric protocol has been elaborated by choosing the most accurate software, 123DCatch, which asks for about 60 pictures, and defining the settings needed to obtain exploitable photographs. In the same way, we defined the devices and settings needed for the laser scan acquisition method. In a second part, we segment the 3D meshes in planes by using Geomagic, which has been chosen between several programs, for its accurate resulting geometry.",2013,,no
The Effect of Visual Representation Style in Problem-Solving: A Perspective from Cognitive Processes,"Using results from a controlled experiment and simulations based on cognitive models, we show that visual presentation style can have a significant impact on performance in a complex problem-solving task. We compared subject performances in two isomorphic, but visually different, tasks based on a card game of SET. Although subjects used the same strategy in both tasks, the difference in presentation style resulted in radically different reaction times and significant deviations in scanpath patterns in the two tasks. Results from our study indicate that low-level subconscious visual processes, such as differential acuity in peripheral vision and low-level iconic memory, can have indirect, but significant effects on decision making during a problem-solving task. We have developed two ACT-R models that employ the same basic strategy but deal with different presentations styles. Our ACT-R models confirm that changes in low-level visual processes triggered by changes in presentation style can propagate to higher-level cognitive processes. Such a domino effect can significantly affect reaction times and eye movements, without affecting the overall strategy of problem solving.",2013,10.1371/journal.pone.0080550,no
Bottom-up modeling of Al/Ni multilayer combustion: Effect of intermixing and role of vacancy defects on the ignition process,"Vapor deposited multilayered aluminum/oxide and bimetallics are promising materials for Micro Electro Mechanical System technologies as energy carriers, for instance, microinitiators or heat microsources in biological or chemical applications. Among these materials, the Al/Ni couple has received much attention both experimentally and theoretically. However, the detailed relation between the chemical composition of the intermixed interfacial regions and its impact on the ignition capabilities remains elusive. In this contribution, we propose a two-fold strategy combining atomistic density functional theory (DFT) calculations and a macroscopic 1D model of chemical kinetics. The DFT calculations allow the description of the elementary chemical processes (involving Al, Ni atoms and vacancies basic ingredients) and to parameterize the macroscopic model, in which the system is described as a stack of infinite layers. This gives the temporal evolution of the system composition and temperature. We demonstrate that the amount of vacancies, originating from the deposition process and the Al and Ni lattice mismatch, plays a critical role on both the ignition time and the temperature. The presence of vacancies enhances the migration of atoms between layers and so dramatically speeds up the atomic mixing at low temperatures far below ignition temperature, also pointing to the relation between experimental deposition procedures and ageing of the nanolaminates. (C) 2013 AIP Publishing LLC.",2013,10.1063/1.4807164,no
The Influence of Notational Deficiencies on Process Model Comprehension,"Visual process models are helpful when analyzing and improving complex organizational processes. However, the symbol sets used in different modeling notations vary in perceptual discriminability, visual expressiveness, and semantic transparency; such factors are likely to influence a notation's perception and cognitive effectiveness. In this paper, we investigate whether the basic symbol sets in visual process modeling languages influence comprehension and cognitive load of process models. For this purpose, we analyze four different symbol sets in an experiment with model comprehension tasks carried out by 136 participants. Our results indicate that notational deficiencies concerning perceptual discriminability and semiotic clarity have measurable effects on comprehension, cognitive load, and the time needed to understand the models.",2013,,no
"Modeling, simulation and experimental validation of a pad humidifier used in solar desalination process","Water is an extremely important commodity for the improvement of arid and semi-arid environments. As for the water production technology, desalination techniques based on either thermal or membrane separation methods such as multi-stage flash, multi-effect, vapor compression and reverse osmosis are only reliable for large capacity ranges of 100-50,000 m(3)/day of freshwater production, inefficient from the view point of energy consumption and generally coupled to fossil fuel sources that have a negative impact on the environment. Hence, for the improvement of these regions it is necessary to shift from fossil fuel usage to some environmentally friendly energy source, such as solar energy, as it is available abundantly in such environments. The aim of the present paper is to study numerically and experimentally a pad humidifier used in a seawater solar desalination unit based on humidification-dehumidification principle. The pad humidifier dynamic modeling is based on various heat and mass balance equations. The effect of operating parameters on the pad humidifier characteristics has been investigated. To validate the computer program, a comparison between the experimental and theoretical results was conducted, and a good agreement had been obtained.",2013,10.1080/19443994.2012.694207,no
Towards Completeness and Lawfulness of Business Process Models,"We address the existing gaps between business process models, enterprise architecture (EA) models and, external regulations that hinder completeness and lawfulness of business process models. As a solution we propose a high-level architecture for business process knowledge management that bridges the identified gaps. We use de-facto industry standards for modelling business processes and EA - BPMN and ArchiMate. We propose to use Bunge-Wand-Weber (BWW) model as a theoretical foundation for storing business process knowledge represented in BPMN and ArchiMate models and storing elements that address lawfulness of the models. BWW model provides a framework for systematically storing internally maintained process knowledge (models) and externally maintained knowledge (regulations), and supporting completeness and lawfulness of the models. Thus, the main contributions of our approach is supporting completeness and lawfulness of business process models and supporting the creation of information services to increase efficiency in the business process modelling context.",2013,,no
Evaluating the ability of process based models to project sea-level change,"We evaluate the ability of process based models to reproduce observed global mean sea-level change. When the models are forced by changes in natural and anthropogenic radiative forcing of the climate system and anthropogenic changes in land-water storage, the average of the modelled sea-level change for the periods 1900-2010, 1961-2010 and 1990-2010 is about 80%, 85% and 90% of the observed rise. The modelled rate of rise is over 1 mm yr(-1) prior to 1950, decreases to less than 0.5 mm yr(-1) in the 1960s, and increases to 3 mm yr(-1) by 2000. When observed regional climate changes are used to drive a glacier model and an allowance is included for an ongoing adjustment of the ice sheets, the modelled sea-level rise is about 2 mm yr(-1) prior to 1950, similar to the observations. The model results encompass the observed rise and the model average is within 20% of the observations, about 10% when the observed ice sheet contributions since 1993 are added, increasing confidence in future projections for the 21st century. The increased rate of rise since 1990 is not part of a natural cycle but a direct response to increased radiative forcing (both anthropogenic and natural), which will continue to grow with ongoing greenhouse gas emissions.",2013,10.1088/1748-9326/8/1/014051,no
Reading time data for evaluating broad-coverage models of English sentence processing,"We make available word-by-word self-paced reading times and eye-tracking data over a sample of English sentences from narrative sources. These data are intended to form a gold standard for the evaluation of computational psycholinguistic models of sentence comprehension in English. We describe stimuli selection and data collection and present descriptive statistics, as well as comparisons between the two sets of reading times.",2013,10.3758/s13428-012-0313-y,no
Development and evaluation of spatial point process models for epidermal nerve fibers,"We propose two spatial point process models for the spatial structure of epidermal nerve fibers (ENFs) across human skin. The models derive from two point processes, Phi(b) and Phi(e), describing the locations of the base and end points of the fibers. Each point of Phi(e) (the end point process) is connected to a unique point in Phi(b) (the base point process). In the first model, both Phi(e) and Phi(b) are Poisson processes, yielding a null model of uniform coverage of the skin by end points and general baseline results and reference values for moments of key physiologic indicators. The second model provides a mechanistic model to generate end points for each base, and we model the branching structure more directly by defining Phi(e) as a cluster process conditioned on the realization of Phi(b) as its parent points. In both cases, we derive distributional properties for observable quantities of direct interest to neurologists such as the number of fibers per base, and the direction and range of fibers on the skin. We contrast both models by fitting them to data from skin blister biopsy images of ENFs and provide inference regarding physiological properties of ENFs. (C) 2013 Elsevier Inc. All rights reserved.",2013,10.1016/j.mbs.2013.03.001,no
Deriving problem frames from business process and object analysis models,"While Problem Frames have become a useful approach for requirements analysis, little research has been made to explore how to derive them from a complex problem context. The purpose of this paper is to propose such an approach. The proposed approach consists of three steps to drive the development of Problem Frames. In the first step, business process models are developed to capture the behavioural view of the problem context. In the second step, object analysis models are used to capture the structural view of the problem context. Together, these two views collectively and adequately capture the early context knowledge. These two types of model will then be used in the third step to construct context diagrams and derive Problem Frames. A complex real-world problem - equity trading problem - is used to illustrate this approach.",2013,10.1111/j.1468-0394.2012.00632.x,no
Process Model Storage Solutions: Proposition and Evaluation,"With the development of Business Process Management (BPM) technology, more and more organizations use process models to describe their business processes. These process models are modified frequently for higher management efficiency and effectiveness. This causes each process models have multiple versions. Since those models focus on the same business behavior, they are similar in structure. Based on process model similarity, model storage can be improved. To store these models efficiently, five process model storage solutions are presented. Experiments are designed to compare those solutions from time and space aspects.",2013,,no
Development and evaluation of ForestGrowth-SRC a process-based model for short rotation coppice yield and spatial supply reveals poplar uses water more efficiently than willow,"Woody biomass produced from short rotation coppice (SRC) poplar (Populus spp.) and willow (Salix spp.) is a bioenergy feedstock that can be grown widely across temperate landscapes and its use is likely to increase in future. Process-based models are therefore required to predict current and future yield potential that are spatially resolved and can consider new genotypes and climates that will influence future yield. The development of a process-based model for SRC poplar and willow, ForestGrowth-SRC, is described and the ability of the model to predict SRC yield and water use efficiency (WUE) was evaluated. ForestGrowth-SRC was parameterized from a process-based model, ForestGrowth for high forest. The new model predicted annual above ground yield well for poplar (r2 = 0.91, RMSE = 1.46 ODT ha-1 yr-1) and willow (r2 = 0.85, RMSE = 1.53 ODT ha-1 yr-1), when compared with measured data from seven sites in contrasting climatic zones across the United Kingdom. Average modelled yields for poplar and willow were 10.3 and 9.0 ODT ha-1 yr-1, respectively, and interestingly, the model predicted a higher WUE for poplar than for willow: 9.5 and 5.5 g kg-1 respectively. Using regional mapped climate and soil inputs, modelled and measured yields for willow compared well (r2 = 0.58, RMSE = 1.27 ODT ha-1 yr-1), providing the first UK map of SRC yield, from a process-based model. We suggest that the model can be used for predicting current and future SRC yields at a regional scale, highlighting important species and genotype choices with respect to water use efficiency and yield potential.",2013,10.1111/j.1757-1707.2012.01191.x,no
"Implementation of dynamic crop growth processes into a land surface model: evaluation of energy, water and carbon fluxes under corn and soybean rotation","Worldwide expansion of agriculture is impacting the earth's climate by altering carbon, water, and energy fluxes, but the climate in turn is impacting crop production. To study this two-way interaction and its impact on seasonal dynamics of carbon, water, and energy fluxes, we implemented dynamic crop growth processes into a land surface model, the Integrated Science Assessment Model (ISAM). In particular, we implemented crop-specific phenology schemes and dynamic carbon allocation schemes. These schemes account for light, water, and nutrient stresses while allocating the assimilated carbon to leaf, root, stem, and grain pools. The dynamic vegetation structure simulation better captured the seasonal variability in leaf area index (LAI), canopy height, and root depth. We further implemented dynamic root distribution processes in soil layers, which better simulated the root response of soil water uptake and transpiration. Observational data for LAI, above-and belowground biomass, and carbon, water, and energy fluxes were compiled from two AmeriFlux sites, Mead, NE, and Bondville, IL, USA, to calibrate and evaluate the model performance. For the purposes of calibration and evaluation, we use a corn-soybean (C4-C3) rotation system over the period 2001-2004. The calibrated model was able to capture the diurnal and seasonal patterns of carbon assimilation and water and energy fluxes for the corn-soybean rotation system at these two sites. Specifically, the calculated gross primary production (GPP), net radiation fluxes at the top of the canopy, and latent heat fluxes compared well with observations. The largest bias in model results was in sensible heat flux (SH) for corn and soybean at both sites. The dynamic crop growth simulation better captured the seasonal variability in carbon and energy fluxes relative to the static simulation implemented in the original version of ISAM. Especially, with dynamic carbon allocation and root distribution processes, the model's simulated GPP and latent heat flux (LH) were in much better agreement with observational data than for the static root distribution simulation. Modeled latent heat based on dynamic growth processes increased by 12-27% during the growing season at both sites, leading to an improvement in modeled GPP by 13-61% compared to the estimates based on the original version of the ISAM.",2013,10.5194/bg-10-8039-2013,no
