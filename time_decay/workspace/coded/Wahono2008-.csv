Document Title,Abstract,Year,PDF Link,label,code,time
Evaluating capture-recapture models with two inspectors,"Capture-recapture (CR) models have been proposed as an objective method for controlling software inspections. CR models were originally developed to estimate the size of animal populations. In software, they have been used to estimate the number of defects in an inspected artifact. This estimate can be another source of information for deciding whether the artifact requires a reinspection to ensure that a minimal inspection effectiveness level has been attained. Little evaluative research has been performed thus far on the utility of CR models for inspections with two inspectors. We report on an extensive Monte Carlo simulation that evaluated capture-recapture models suitable for two inspectors assuming a code inspections context. We evaluate the relative error of the CR estimates as well as the accuracy of the reinspection decision made using the CR model. Our results indicate that the most appropriate capture-recapture model for two inspectors is an estimator that allows for inspectors with different capabilities. This model always produces an estimate (i.e., does not fail), has a predictable behavior (i.e., works well when its assumptions are met), will have a relatively high decision accuracy, and will perform better than the default decision of no reinspections. Furthermore, we identify the conditions under which this estimator will perform best.",2001,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.512.320&rep=rep1&type=pdf,yes,yes,1486912032.806269
Regression via Classification applied on software defect estimation,"In this paper we apply Regression via Classification (RvC) to the problem of estimating the number of software defects. This approach apart from a certain number of faults, it also outputs an associated interval of values, within which this estimate lies with a certain confidence. RvC also allows the production of comprehensible models of software defects exploiting symbolic learning algorithms. To evaluate this approach we perform an extensive comparative experimental study of the effectiveness of several machine learning algorithms in two software data sets. RvC manages to get better regression error than the standard regression approaches on both datasets",2008,https://pdfs.semanticscholar.org/4cde/710b845e5332cae51c4219feecd89fc6cceb.pdf,yes,yes,1486912030.435479
Assessment of a New Three-Group Software Quality Classification Technique: An Empirical Case Study,"The primary aim of risk-based software quality classification models is to detect, prior to testing or operations, components that are most-likely to be of high-risk. Their practical usage as quality assurance tools is gauged by the prediction-accuracy and cost-effective aspects of the models. Classifying modules into two risk groups is the more commonly practiced trend. Such models assume that all modules predicted as high-risk will be subjected to quality improvements. Due to the always-limited reliability improvement resources and the variability of the quality risk-factor, a more focused classification model may be desired to achieve cost-effective software quality assurance goals. In such cases, calibrating a three-group (high-risk, medium-risk, and low-risk) classification model is more rewarding. We present an innovative method that circumvents the complexities, computational overhead, and difficulties involved in calibrating pure or direct three-group classification models. With the application of the proposed method, practitioners can utilize an existing two-group classification algorithm thrice in order to yield the three risk-based classes. An empirical approach is taken to investigate the effectiveness and validity of the proposed technique. Some commonly used classification techniques are studied to demonstrate the proposed methodology. They include, the C4.5 decision tree algorithm, discriminant analysis, and case-based reasoning. For the first two, we compare the three-group model calibrated using the respective techniques with the one built by applying the proposed method. Any two-group classification technique can be employed by the proposed method, including those that do not provide a direct three-group classification model, e.x., logistic regression and certain binary classification trees, such as CART. Based on a case study of a large-scale industrial software system, it is observed that the proposed method yielded promising results. For a given classification technique, the expected cost of misclassification of the proposed three-group models were significantly better (generally) when compared to the techniques direct three-group model. In addition, the proposed method is also evaluated against an alternate indirect three-group classification method.",2005,http://link.springer.com/article/10.1007%2Fs10664-004-6191-x,yes,yes,1486912025.901709
Empirical Analysis of Software Fault Content and Fault Proneness Using Bayesian Methods,"We present a methodology for Bayesian analysis of software quality. We cast our research in the broader context of constructing a causal framework that can include process, product, and other diverse sources of information regarding fault introduction during the software development process. In this paper, we discuss the aspect of relating internal product metrics to external quality metrics. Specifically, we build a Bayesian network (BN) model to relate object-oriented software metrics to software fault content and fault proneness. Assuming that the relationship can be described as a generalized linear model, we derive parametric functional forms for the target node conditional distributions in the BN. These functional forms are shown to be able to represent linear, Poisson, and binomial logistic regression. The models are empirically evaluated using a public domain data set from a software subsystem. The results show that our approach produces statistically significant estimations and that our overall modeling method performs no worse than existing techniques.",2007,http://ieeexplore.ieee.org/document/4302779/?arnumber=4302779,yes,yes,1486912001.810316
Predicting fault prone modules by the Dempster-Shafer belief networks,"This paper describes a novel methodology for predicting fault prone modules. The methodology is based on Dempster-Shafer (D-S) belief networks. Our approach consists of three steps: first, building the D-S network by the induction algorithm; second, selecting the predictors (attributes) by the logistic procedure; third, feeding the predictors describing the modules of the current project into the inducted D-S network and identifying fault prone modules. We applied this methodology to a NASA dataset. The prediction accuracy of our methodology is higher than that achieved by logistic regression or discriminant analysis on the same dataset.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240314,yes,yes,1486912001.208268
Object oriented software quality prediction using general regression neural networks,"This paper discusses the application of General Regression Neural Network (GRNN) for predicting the software quality attribute -- fault ratio. This study is carried out using static Object-Oriented (OO) measures (64 in total) as the independent variables and fault ratio as the dependent variable. Software metrics used include those concerning inheritance, size, cohesion and coupling. Prediction models are designed using 15 possible combinations of the four categories of the measures. We also tested the goodness of fit of the neural network model with the standard parameters. Our study is conducted in an academic institution with the software developed by students of Undergraduate/Graduate courses.",2004,http://dl.acm.org/citation.cfm?id=1022515,yes,yes,1486911999.988883
Application of neural networks for software quality prediction using object-oriented metrics,"The paper presents the application of neural networks in software quality estimation using object-oriented metrics. Quality estimation includes estimating reliability as well as maintainability of software. Reliability is typically measured as the number of defects. Maintenance effort can be measured as the number of lines changed per class. In this paper, two kinds of investigation are performed: predicting the number of defects in a class; and predicting the number of lines change per class. Two neural network models are used: they are Ward neural network; and General Regression neural network (GRNN). Object-oriented design metrics concerning inheritance related measures, complexity measures, cohesion measures, coupling measures and memory allocation measures are used as the independent variables. GRNN network model is found to predict more accurately than Ward network model.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235412,yes,yes,1486911999.988877
Software quality prediction using median-adjusted class labels,Software metrics aid project managers in predicting the quality of software systems. A method is proposed using a neural network classifier with metric inputs and subjective quality assessments as class labels. The labels are adjusted using fuzzy measures of the distances from each class center computed using robust multivariate medians,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007518,yes,yes,1486911999.378452
Classification-tree models of software-quality over multiple releases,"Software quality models are tools for focusing software enhancement efforts. Such efforts are essential for mission-critical embedded software, such as telecommunications systems, because customer-discovered faults have very serious consequences and are very expensive to repair. We present an empirical study that evaluated software quality models over several releases to address the question, How long will a model yield useful predictions? We also introduce the Classification And Regression Trees (CART) algorithm to software reliability engineering practitioners. We present our method for exploiting CART features to achieve a preferred balance between the two types of misclassification rates. This is desirable because misclassifications of fault-prone modules often have much more severe consequences than misclassifications of those that are not fault-prone. We developed two classification-tree models based on four consecutive releases of a very large legacy telecommunications system. Forty-two software product, process, and execution metrics were candidate predictors. The first software quality model used measurements of the first release as the training data set and measurements of the subsequent three releases as evaluation data sets. The second model used measurements of the second release as the training data set and measurements of the subsequent two releases as evaluation data sets. Both models had accuracy that would be useful to developers.",2000,http://ieeexplore.ieee.org/document/809316/?arnumber=809316,yes,yes,1486911998.189488
A practical method for the software fault-prediction,"In the paper, a novel machine learning method, SimBoost, is proposed to handle the software fault-prediction problem when highly skewed datasets are used. Although the method, proved by empirical results, can make the datasets much more balanced, the accuracy of the prediction is still not satisfactory. Therefore, a fuzzy-based representation of the software module fault state has been presented instead of the original faulty/non-faulty one. Several experiments were conducted using datasets from NASA Metrics Data Program. The discussion of the results of experiments is provided.",2007,http://ieeexplore.ieee.org/document/4296695/,yes,yes,1486911996.70762
Software Quality Analysis of Unlabeled Program Modules With Semisupervised Clustering,"Software quality assurance is a vital component of software project development. A software quality estimation model is trained using software measurement and defect (software quality) data of a previously developed release or similar project. Such an approach assumes that the development organization has experience with systems similar to the current project and that defect data are available for all modules in the training data. In software engineering practice, however, various practical issues limit the availability of defect data for modules in the training data. In addition, the organization may not have experience developing a similar system. In such cases, the task of software quality estimation or labeling modules as fault prone or not fault prone falls on the expert. We propose a semisupervised clustering scheme for software quality analysis of program modules with no defect data or quality-based class labels. It is a constraint-based semisupervised clustering scheme that uses k-means as the underlying clustering algorithm. Software measurement data sets obtained from multiple National Aeronautics and Space Administration software projects are used in our empirical investigation. The proposed technique is shown to aid the expert in making better estimations as compared to predictions made when the expert labels the clusters formed by an unsupervised learning algorithm. In addition, the software quality knowledge learnt during the semisupervised process provided good generalization performance for multiple test data sets. An analysis of program modules that remain unlabeled subsequent to our semisupervised clustering scheme provided useful insight into the characteristics of their software attributes",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4100780,yes,yes,1486911996.271842
Empirical Analysis of Object-Oriented Design Metrics for Predicting High and Low Severity Faults,"In the last decade, empirical studies on object-oriented design metrics have shown some of them to be useful for predicting the fault-proneness of classes in object-oriented software systems. This research did not, however, distinguish among faults according to the severity of impact. It would be valuable to know how object-oriented design metrics and class fault-proneness are related when fault severity is taken into account. In this paper, we use logistic regression and machine learning methods to empirically investigate the usefulness of object-oriented design metrics, specifically, a subset of the Chidamber and Kemerer suite, in predicting fault-proneness when taking fault severity into account. Our results, based on a public domain NASA data set, indicate that 1) most of these design metrics are statistically related to fault-proneness of classes across fault severity, and 2) the prediction capabilities of the investigated metrics greatly depend on the severity of faults. More specifically, these design metrics are able to predict low severity faults in fault-prone classes better than high severity faults in fault-prone classes",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1717471,yes,yes,1486911996.271837
Applying machine learning to software fault-proneness prediction,"The importance of software testing to quality assurance cannot be overemphasized. The estimation of a module's fault-proneness is important for minimizing cost and improving the effectiveness of the software testing process. Unfortunately, no general technique for estimating software fault-proneness is available. The observed correlation between some software metrics and fault-proneness has resulted in a variety of predictive models based on multiple metrics. Much work has concentrated on how to select the software metrics that are most likely to indicate fault-proneness. In this paper, we propose the use of machine learning for this purpose. Specifically, given historical data on software metric values and number of reported errors, an Artificial Neural Network (ANN) is trained. Then, in order to determine the importance of each software metric in predicting fault-proneness, a sensitivity analysis is performed on the trained ANN. The software metrics that are deemed to be the most critical are then used as the basis of an ANN-based predictive model of a continuous measure of fault-proneness. We also view fault-proneness prediction as a binary classification task (i.e., a module can either contain errors or be error-free) and use Support Vector Machines (SVM) as a state-of-the-art classification method. We perform a comparative experimental study of the effectiveness of ANNs and SVMs on a data set obtained from NASA's Metrics Data Program data repository.",2008,http://www.sciencedirect.com/science/article/pii/S0164121207001240,yes,yes,1486911996.27183
A systematic review of software fault prediction studies,"This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle.",2008,http://www.sciencedirect.com/science/article/pii/S0957417408007215,yes,yes,1486911995.889221
Software quality prediction using mixture models with EM algorithm,"The use of the statistical technique of mixture model analysis as a tool for early prediction of fault-prone program modules is investigated. The expectation-maximum likelihood (EM) algorithm is engaged to build the model. By only employing software size and complexity metrics, this technique can be used to develop a model for predicting software quality even without the prior knowledge of the number of faults in the modules. In addition, Akaike Information Criterion (AIC) is used to select the model number which is assumed to be the class number the program modules should be classified. The technique is successful in classifying software into fault-prone and non fault-prone modules with a relatively low error rate, providing a reliable indicator for software quality prediction",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883780,yes,yes,1486911995.889219
A novel method for early software quality prediction based on support vector machine,"The software development process imposes major impacts on the quality of software at every development stage; therefore, a common goal of each software development phase concerns how to improve software quality. Software quality prediction thus aims to evaluate software quality level periodically and to indicate software quality problems early. In this paper, we propose a novel technique to predict software quality by adopting support vector machine (SVM) in the classification of software modules based on complexity metrics. Because only limited information of software complexity metrics is available in early software life cycle, ordinary software quality models cannot make good predictions generally. It is well known that SVM generalizes well even in high dimensional spaces under small training sample conditions. We consequently propose a SVM-based software classification model, whose characteristic is appropriate for early software quality predictions when only a small number of sample data are available. Experimental results with a medical imaging system software metrics data show that our SVM prediction model achieves better software quality prediction than some commonly used software quality prediction models",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544736,yes,yes,1486911995.889214
Extract rules from software quality prediction model based on neural network,"To get a highly reliable software product to the market on schedule, software engineers must allocate resources on the fault-prone software modules across the development effort. Software quality models based upon data mining from past projects can identify fault-prone modules in current similar development efforts. So that resources can be focused on fault-prone modules to improve quality prior to release. Many researchers have applied the neural networks approach to predict software quality. Although neural networks have shown their strengths in solving complex problems, their shortcoming of being 'black boxes' models has prevented them from being accepted as a common practice for fault-prone software modules prediction. That is a significant weakness, for without the ability to produce comprehensible decisions; it is hard to trust the reliability of neural networks that address real-world problems. We introduce an interpretable neural network model for software quality prediction. First, a three-layer feed-forward neural network with the sigmoid function in hidden units and the identity function in output unit was trained. The data used to train the neural network is collected from an earlier release of a telecommunications software system. Then use clustering genetic algorithm (CCA) to extract comprehensible rules from the trained neural network. We use the rule set extracted from the trained neural network to detect the fault-prone software modules of the later release and compare the predicting results with the neural network predicting results. The comparison shows that although the rule set's predicting accuracy is a little less than the trained neural network, it is more comprehensible.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374186,yes,yes,1486911995.542633
An empirical comparison and characterization of high defect and high complexity modules,"We analyzed a large set of complexity metrics and defect data collected from six large-scale software products, two from IBM and four from Nortel Networks, to compare and characterize the similarities and differences between the high defect (HD) and high complexity modules. We observed that the most complex modules often have an acceptable quality and HD modules are not typically the most complex ones. This observation was statistically validated through hypothesis testing. Our analyses also indicated that the clusters of modules with the highest defects are usually those whose complexity rankings are slightly below the most complex ones. These results should help us better understand the complexity behavior of HD modules and guide future software development and research efforts.",2003,http://www.sciencedirect.com/science/article/pii/S0164121202001267,yes,yes,1486911994.937306
Tree-based software quality estimation models for fault prediction,"Complex high-assurance software systems depend highly on reliability of their underlying software applications. Early identification of high-risk modules can assist in directing quality enhancement efforts to modules that are likely to have a high number of faults. Regression tree models are simple and effective as software quality prediction models, and timely predictions from such models can be used to achieve high software reliability. This paper presents a case study from our comprehensive evaluation (with several large case studies) of currently available regression tree algorithms for software fault prediction. These are, CART-LS (least squares), S-PLUS, and CART-LAD (least absolute deviation). The case study presented comprises of software design metrics collected from a large network telecommunications system consisting of almost 13 million lines of code. Tree models using design metrics are built to predict the number of faults in modules. The algorithms are also compared based on the structure and complexity of their tree models. Performance metrics, average absolute and average relative errors are used to evaluate fault prediction accuracy.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011339,yes,yes,1486911994.937302
An investigation of the effect of module size on defect prediction using static measures,"We used several machine learning algorithms to predict the defective modules in five NASA products, namely, CM1, JM1, KC1, KC2, and PC1. A set of static measures were employed as predictor variables. While doing so, we observed that a large portion of the modules were small, as measured by lines of code (LOC). When we experimented on the data subsets created by partitioning according to module size, we obtained higher prediction performance for the subsets that include larger modules. We also performed defect prediction using class-level data for KC1 rather than the method-level data. In this case, the use of class-level data resulted in improved prediction performance compared to using method-level data. These findings suggest that quality assurance activities can be guided even better if defect prediction is performed by using data that belong to larger modules.",2005,http://promise.site.uottawa.ca/proceedings/pdf/10.pdf,yes,yes,1486911994.703112
An empirical study of predicting software faults with case-based reasoning,"The resources allocated for software quality assurance and improvement have not increased with the ever-increasing need for better software quality. A targeted software quality inspection can detect faulty modules and reduce the number of faults occurring during operations. We present a software fault prediction modeling approach with case-based reasoning (CBR), a part of the computational intelligence field focusing on automated reasoning processes. A CBR system functions as a software fault prediction model by quantifying, for a module under development, the expected number of faults based on similar modules that were previously developed. Such a system is composed of a similarity function, the number of nearest neighbor cases used for fault prediction, and a solution algorithm. The selection of a particular similarity function and solution algorithm may affect the performance accuracy of a CBR-based software fault prediction system. This paper presents an empirical study investigating the effects of using three different similarity functions and two different solution algorithms on the prediction accuracy of our CBR system. The influence of varying the number of nearest neighbor cases on the performance accuracy is also explored. Moreover, the benefits of using metric-selection procedures for our CBR system is also evaluated. Case studies of a large legacy telecommunications system are used for our analysis. It is observed that the CBR system using the Mahalanobis distance similarity function and the inverse distance weighted solution algorithm yielded the best fault prediction. In addition, the CBR models have better performance than models based on multiple linear regression.",2006,http://suraj.lums.edu.pk/~cs661s07/Papers/An%20empirical%20study%20of%20predicting%20software%20faults%20with%20case-based%20reasoning.pdf,yes,yes,1486911994.703105
A Unified Framework for Defect Data Analysis Using the MBR Technique,"Failures of mission-critical software systems can have catastrophic consequences and, hence, there is strong need for scientifically rigorous methods for assuring high system reliability. To reduce the V&V cost for achieving high confidence levels, quantitatively based software defect prediction techniques can be used to effectively estimate defects from prior data. Better prediction models facilitate better project planning and risk/cost estimation. Memory based reasoning (MBR) is one such classifier that quantitatively solves new cases by reusing knowledge gained from past experiences. However, it can have different configurations by varying its input parameters, giving potentially different predictions. To overcome this problem, we develop a framework that derives the optimal configuration of an MBR classifier for software defect data, by logical variation of its configuration parameters. We observe that this adaptive MBR technique provides a flexible and effective environment for accurate prediction of mission-critical software defect data.",2006,http://ieeexplore.ieee.org/document/4031878/,yes,yes,1486911994.486937
Comparing software prediction techniques using simulation,"The need for accurate software prediction systems increases as software becomes much larger and more complex. We believe that the underlying characteristics: size, number of features, type of distribution, etc., of the data set influence the choice of the prediction system to be used. For this reason, we would like to control the characteristics of such data sets in order to systematically explore the relationship between accuracy, choice of prediction system, and data set characteristic. It would also be useful to have a large validation data set. Our solution is to simulate data allowing both control and the possibility of large (1000) validation cases. The authors compare four prediction techniques: regression, rule induction, nearest neighbor (a form of case-based reasoning), and neural nets. The results suggest that there are significant differences depending upon the characteristics of the data set. Consequently, researchers should consider prediction context when evaluating competing prediction systems. We observed that the more ""messy"" the data and the more complex the relationship with the dependent variable, the more variability in the results. In the more complex cases, we observed significantly different results depending upon the particular training set that has been sampled from the underlying data set. However, our most important result is that it is more fruitful to ask which is the best prediction system in a particular context rather than which is the ""best"" prediction system.",2001,http://dspace.brunel.ac.uk/bitstream/2438/1102/3/Comparing%20Software%20Prediction%202001.pdf,yes,yes,1486911994.486929
Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings,"Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4527256,yes,yes,1486911994.301969
Software defect association mining and defect correction effort prediction,"Much current software defect prediction work focuses on the number of defects remaining in a software system. In this paper, we present association rule mining based methods to predict defect associations and defect correction effort. This is to help developers detect software defects and assist project managers in allocating testing resources more effectively. We applied the proposed methods to the SEL defect data consisting of more than 200 projects over more than 15 years. The results show that, for defect association prediction, the accuracy is very high and the false-negative rate is very low. Likewise, for the defect correction effort prediction, the accuracy for both defect isolation effort prediction and defect correction effort prediction are also high. We compared the defect correction effort prediction method with other types of methods - PART, C4.5, and Naive Bayes - and show that accuracy has been improved by at least 23 percent. We also evaluated the impact of support and confidence levels on prediction accuracy, false-negative rate, false-positive rate, and the number of rules. We found that higher support and confidence levels may not result in higher prediction accuracy, and a sufficient number of rules is a precondition for high prediction accuracy.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1599417,yes,yes,1486911994.301956
Predicting defect-prone software modules using support vector machines,"Effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. Support vector machines (SVM) have been successfully applied for solving both classification and regression problems in many applications. This paper evaluates the capability of SVM in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four NASA datasets. The results indicate that the prediction performance of SVM is generally better than, or at least, is competitive against the compared models.",2008,http://www.sciencedirect.com/science/article/pii/S016412120700235X,yes,yes,1486911994.171336
Empirical assessment of machine learning based software defect prediction techniques,"The wide-variety of real-time software systems, including telecontrol/telepresence systems, robotic systems, and mission planning systems, can entail dynamic code synthesis based on runtime mission-specific requirements and operating conditions. This necessitates the need for dynamic dependability assessment to ensure that these systems perform as specified and not fail in catastrophic ways. One approach in achieving this is to dynamically assess the modules in the synthesized code using software defect prediction techniques. Statistical models; such as stepwise multi-linear regression models and multivariate models, and machine learning approaches, such as artificial neural networks, instance-based reasoning, Bayesian-belief networks, decision trees, and rule inductions, have been investigated for predicting software quality. However, there is still no consensus about the best predictor model for software defects. In this paper; we evaluate different predictor models on four different real-time software defect data sets. The results show that a combination of IR and instance-based learning along with the consistency-based subset evaluation technique provides a relatively better consistency in accuracy prediction compared to other models. The results also show that """"size"""" and """"complexity"""" metrics are not sufficient for accurately predicting real-time software defects.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544801,yes,yes,1486911994.171333
Mining software repositories for comprehensible software fault prediction models,"Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization (ACO)-based classification technique AntMiner+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by AntMiner+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the AntMiner+ models can be considered superior to the latter models.",2008,http://www.sciencedirect.com/science/article/pii/S0164121207001902,yes,yes,1486911993.931694
Electromagnetic environment analysis of a software park near transmission lines,"The electromagnetic environments (EMEs) of the planned Zhongguancun Software Park near transmission lines, including electrical field, magnetic field, and ground potential rise under three cases of lightning stroke, normal operation, and short-circuit faults, are assessed by numerical analysis. The power frequency EMEs of the software park are below the maximum ecological allowed exposure values for the general public; , nevertheless, the power frequency magnetic field may interfere with the sensitive computer display unit. The influence of short-circuit fault in two different cases of remote short circuit and neighboring short circuit on the software park is discussed. The main problem we must pay attention to is the ground potential rise in the software park due to neighboring short-circuit fault; it would threaten the safe operation of electronic devices in the software park. On the other hand, the lightning stroke is a serious threat to the software park. How to improve the EMEs of the software park is discussed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1315789,no,no,1486912032.806275
Assessing optimal software architecture maintainability,"Over the last decade, several authors have studied the maintainability of software architectures. In particular, the assessment of maintainability has received attention. However, even when one has a quantitative assessment of the maintainability of a software architecture, one still does not have any indication of the optimality of the software architecture with respect to this quality attribute. Typically, the software architect is supposed to judge the assessment result based on his or her personal experience. In this paper, we propose a technique for analysing the optimal maintainability of a software architecture based on a specified scenario profile. This technique allows software architects to analyse the maintainability of their software architecture with respect to the optimal maintainability. The technique is illustrated and evaluated using industrial cases",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914981,no,no,1486912032.806274
Automated design flaw correction in object-oriented systems,"Software inevitably changes. As a consequence, we observe the phenomenon referred to as """"software entropy"""" or """"software decay"""": the software design continually degrades making maintenance and functional extensions overly costly if not impossible. There exist a number of approaches to identify design flaws (problem detection) and to remedy them (refactoring). There is, however, a conceptual gap between these two stages: There is no appropriate support for the automated mapping of design flaws to possible solutions. Here we propose an integrated, quality-driven and tool-supported methodology to support object-oriented software evolution. Our approach is based on the novel concept of """"correction strategies"""". Correction strategies serve as reference descriptions that enable a human-assisted tool to plan and perform all necessary steps for the safe removal of detected design flaws, with special concern towards the targeted quality goals of the restructuring process. We briefly sketch our tool chain and illustrate our approach with the help of a medium-sized real-world case-study.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281418,no,no,1486912032.806272
Fuzzy set approach to the assessment of student-centered learning,"Assessment of student learning is an important task in a teaching and learning process. It has a strong influence on students' approaches to learning and their outcomes. Development in tertiary education has shifted its emphasis from teacher-centered learning to student-centered learning. In a student-centered learning environment, criterion-referenced assessment techniques are often used in current education research and practice. However, it sometimes happens that the assessment criteria and their corresponding weights are solely determined by the lecturers in charge. This may reduce the interest of students' participation and lower the quality of their learning. This paper presents an integrated fuzzy set approach to assess the outcomes of student-centered learning. It uses fuzzy set principles to represent the imprecise concepts for subjective judgment and applies a fuzzy set method to determine the assessment criteria and their corresponding weights. Based on the commonly agreed assessment criteria, students' learning outcomes are evaluated on a fuzzy grade scale. The proposed fuzzy set approach incorporates students' opinions into assessment and allows them to have a better understanding on the assessment criteria. It aims at encouraging students to participate in the whole learning process and providing an open and fair environment for assessment",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=848079,no,no,1486912032.806271
Determination of a failure probability prognosis based on PD - diagnostics in gis,"In complex high voltage components local insulation defects can cause partial discharges (PD). Especially in highly stressed gas insulated switchgear (GIS) these PD affected defects can lead to major blackouts. Today each PD activity on important insulation components causes an intervention of an expert, who has to identify and analyze the PD source and has to decide: Are the modules concerned to be switched off or can they stay in service? To reduce these cost and time intensive expert interventions, this contribution specifies a proposal which combines an automated PD defect identification procedure with a quantifiable diagnosis confidence. A risk assessment procedure is described, which is based on measurements of phase resolved PD pulse sequence data and a subsequent PD source identification. A defect specific risk is determined and then integrated within a failure probability software using the Farmer diagram. The risks of failure are classified into three levels. The uncertainty of the PD diagnosis is assessed by applying different PD sources and comparisons with other evaluation concepts as well as considering system theoretical investigations. It is shown that the PD defect specific risk is the key aspect of this approach which depends on the so called criticality range and the main PD impact aspects PD location, time dependency, defect property and (over) voltage dependency.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4712675,no,no,1486912032.80627
Improving the Efficiency of Misuse Detection by Means of the q-gram Distance,"Misuse detection-based intrusion detection systems (IDS) perform search through a database of attack signatures in order to detect whether any of them are present in incoming traffic. For such testing, fault-tolerant distance measures are needed. One of the appropriate distance measures of this kind is constrained edit distance, but the time complexity of its computation is too high. We propose a two-phase indexless search procedure for application in misuse detection-based IDS that makes use of q-gram distance instead of the constrained edit distance. We study how well q-gram distance approximates edit distance with special constraints needed in IDS applications. We compare the performances of the search procedure with the two distances applied in it. Experimental results show that the procedure with the q-gram distance implemented achieves for higher values of q almost the same accuracy as the one with the constrained edit distance implemented, but the efficiency of the procedure that implements the q-gram distance is much better.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4627086,no,no,1486912032.806267
An exploratory study of groupware support for distributed software architecture evaluation process,"Software architecture evaluation is an effective means of addressing quality related issues quite early in the software development lifecycle. Scenario-based approaches to evaluate architecture usually involve a large number of stakeholders, who need to be collocated for evaluation sessions. Collocating a large number of stakeholders is an expensive and time-consuming exercise, which may prove to be a hurdle in the wide-spread adoption of architectural evaluation practices. Drawing upon the successful introduction of groupware applications to support geographically distributed teams in software inspection, and requirements engineering disciplines, we propose the concept of distributed architectural evaluation using Internet-based collaborative technologies. This paper illustrates the methodology of a pilot study to assess the viability of a larger experiment intended to investigate the feasibility of groupware support for distributed software architecture evaluation. In addition, the results of the pilot study provide some interesting findings on the viability of groupware-supported software architectural evaluation process.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371923,no,no,1486912032.806266
Fault tolerance design in JPEG 2000 image compression system,"The JPEG 2000 image compression standard is designed for a broad range of data compression applications. The new standard is based on wavelet technology and layered coding in order to provide a rich feature compressed image stream. The implementations of the JPEG 2000 codec are susceptible to computer-induced soft errors. One situation requiring fault tolerance is remote-sensing satellites, where high energy particles and radiation produce single event upsets corrupting the highly susceptible data compression operations. This paper develops fault tolerance error-detecting capabilities for the major subsystems that constitute a JPEG 2000 standard. The nature of the subsystem dictates the realistic fault model where some parts have numerical error impacts whereas others are properly modeled using bit-level variables. The critical operations of subunits such as discrete wavelet transform (DWT) and quantization are protected against numerical errors. Concurrent error detection techniques are applied to accommodate the data type and numerical operations in each processing unit. On the other hand, the embedded block coding with optimal truncation (EBCOT) system and the bitstream formation unit are protected against soft-error effects using binary decision variables and cyclic redundancy check (CRC) parity values, respectively. The techniques achieve excellent error-detecting capability at only a slight increase in complexity. The design strategies have been tested using Matlab programs and simulation results are presented.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416865,no,no,1486912032.806264
Dynamic load balancing performance in cellular networks with multiple traffic types,"Several multimedia applications are being introduced to cellular networks. Since the quality of service (QoS) requirements such as bandwidth for different services might be different, the analysis of conventional multimedia cellular networks has been done using multi-dimensional Markov-chains in previous works. In these analyses, it is assumed that a call request will be blocked if the number of available channels is not sufficient to support the service. However, it has been shown in previous works that the call blocking rate can be reduced significantly, if a dynamic load balancing scheme is employed. In this paper, we develop an analytical framework for the analysis of dynamic load balancing schemes with multiple traffic types. To illustrate the impact of dynamic load balancing on the performance, we study the integrated cellular and ad hoc relay (iCAR) system. Our results show that with a proper amount of load balancing capability (i.e., load balancing channels), the call blocking probability for all traffic types can be reduced significantly.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1404713,no,no,1486912032.806261
Analyzing Software System Quality Risk Using Bayesian Belief Network,"Uncertainty during the period of software project development often brings huge risks to contractors and clients. Developing an effective method to predict the cost and quality of software projects based on facts such as project characteristics and two-side cooperation capability at the beginning of the project can aid us in finding ways to reduce the risks. Bayesian belief network (BBN) is a good tool for analyzing uncertain consequences, but it is difficult to produce precise network structure and conditional probability table. In this paper, we build up the network structure by Delphi method for conditional probability table learning, and learn to update the probability table and confidence levels of the nodes continuously according to application cases, which would subsequently make the evaluation network to have learning abilities, and to evaluate the software development risks in organizations more accurately. This paper also introduces the EM algorithm to enhance the ability in producing hidden nodes caused by variant software projects.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4403073,no,no,1486912032.014329
Projecting advanced enterprise network and service management to active networks,"Active networks is a promising technology that allows us to control the behavior of network nodes by programming them to perform advanced operations and computations. Active networks are changing considerably the scenery of computer networks and, consequently, affect the way network management is conducted. Current management techniques can be enhanced and their efficiency can be improved, while novel techniques can be deployed. This article discusses the impact of active networks on current network management practice by examining network management through the functional areas of fault, configuration, accounting, performance and security management. For each one of these functional areas, the limitations of the current applications and tools are presented, as well as how these limitations can be overcome by exploiting active networks. To illustrate the presented framework, several applications are examined. The contribution of this work is to analyze, classify, and assess the various models proposed in this area, and to outline new research directions",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=980542,no,no,1486912032.014328
Dealing with missing software project data,"Whilst there is a general consensus that quantitative approaches are an important part of successful software project management, there has been relatively little research into many of the obstacles to data collection and analysis in the real world. One feature that characterises many of the data sets we deal with is missing or highly questionable values. Naturally this problem is not unique to software engineering, so we explore the application of two existing data imputation techniques that have been used to good effect elsewhere. In order to assess the potential value of imputation we use two industrial data sets. Both are quite problematic from an effort modelling perspective because they contain few cases, have a significant number of missing values and the projects are quite heterogeneous. We examine the quality of fit of effort models derived by stepwise regression on the raw data and data sets with values imputed by various techniques is compared. In both data sets we find that k-nearest neighbour (k-NN) and sample mean imputation (SMI) significantly improve the model fit, with k-NN giving the best results. These results are consistent with other recently published results, consequently we conclude that imputation can assist empirical software engineering.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232464,no,no,1486912032.014326
Iterative processing algorithm to detect biases in assessments,"In order to assess a large number of diverse projects as fairly and rapidly as possible, a procedure often adopted is to use a panel consisting of a large number of experts, only a small number of whom assess each project. Since no one expert assesses all the projects, conscious or unconscious bias regarding overall standards by any expert will advantage or disadvantage the projects assessed by that particular expert. This paper presents an iterative algorithm that has been used successfully to detect and correct for such biases. Each expert's assessments are modeled as differing from the ideal as a result of a shift of mean and having a standard deviation that is too low or too high. This model is used in conjunction with the concept of """"paired assessments"""" to account for individual projects being of unusually high or low quality and so to evaluate the discrepancy from the ideal marks. The same computer program also has applications in the peer-review or expert-evaluation of research proposals, and any other situation involving subjective assessments by a restricted number of persons.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183677,no,no,1486912032.014325
Formalizing UML class diagrams-a hierarchical predicate transition net approach,"Unified Modeling Language (UML) has been widely accepted as the standard object-oriented development methodology in the software industry. However, many graphical notations in UML only have informal English definitions and thus are error-prone and cannot be formally analyzed. We present our preliminary results on an approach to formally define UML class diagrams using hierarchical predicate transition nets (HPrTNs). We show how to define the main concepts related to class diagrams using HPrTN elements",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884721,no,no,1486912032.014323
QoS evaluation of VoIP end-points,"We evaluate the QoS of a number of VoIP end-points, in terms of mouth-to-ear (M2E) delay, clock skew, silence suppression behavior and robustness to packet loss. Our results show that the M2E delay depends mainly on the receiving end-point. Hardware IP phones, when acting as receivers, usually achieve a low average M2E delay (45-90 ms) under low jitter conditions. Software clients achieve an average M2E delay from 65 ms to over 400 ms, depending on the actual implementation. All tested end-points can compensate for clock skew, although some suffer from occasional playout buffer underflow. Only a few of the tested end-points support silence suppression. We find that these silence detectors have a relatively long hangover time (> 0.5 sec), and they may falsely detect music as silence. All hardware IP phones we tested support some form of packet loss concealment better than silence substitution. The concealment generally works well for two to three consecutive losses at 20 ms packet intervals, but voice will quickly deteriorate beyond that.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1203932,no,no,1486912032.014322
Understanding the sources of software defects: a filtering approach,"The paper presents a method proposal of how to use product measures and defect data to enable understanding and identification of design and programming constructs that contribute more than expected to the defect statistics. The paper describes a method that can be used to identify the most defect-prone design and programming constructs and the method proposal is illustrated on data collected from a large software project in the telecommunication domain. The example indicates that it is feasible, based on defect data and product measures, to identify the main sources of defects in terms of design and programming constructs. Potential actions to be taken include less usage of particular design and programming constructs, additional resources for verification of the constructs and further education into how to use the constructs",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852475,no,no,1486912032.014321
On the evaluation of JavaSymphony for cluster applications,"In the past few years, increasing interest has been shown in using Java as a language for performance-oriented distributed and parallel computing. Most Java-based systems that support portable parallel and distributed computing either require the programmer to deal with intricate low level details of Java which can be a tedious, time-consuming and error-prone task, or prevent the programmer from controlling locality of data. In contrast to most existing systems, JavaSymphony - a class library written entirely in Java - allows to control parallelism, load balancing and locality at a high level. Objects can be explicitly distributed and migrated based on virtual architectures which impose a virtual hierarchy on a distributed/parallel system of physical computing nodes. The concept of blocking/nonblocking remote method invocation is used to exchange data among distributed objects and to process work by remote objects. We evaluate the JavaSymphony programming API for a variety of distributed/parallel algorithms which comprises backtracking, N-body, encryption/decryption algorithms and asynchronous nested optimization algorithms. Performance results are presented for both homogeneous and heterogeneous cluster architectures. Moreover, we compare JavaSymphony with an alternative well-known semi-automatic system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137772,no,no,1486912032.014319
Comparative Study of Fault-Proneness Filtering with PMD,"Fault-prone module detection is important for assurance of software quality. We have proposed a novel approach for detecting fault-prone modules using spam filtering technique, named Fault-proneness filtering. In order to show the effectiveness of fault-proneness filtering, we conducted comparative study with a static code analysis tool, PMD. In the study, Fault-proneness filtering obtains higher F<sub>1</sub> than PMD.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700354,no,no,1486912032.014317
Dynamic characterization study of flip chip ball grid array (FCBGA) on peripheral component interconnect (PCI) board application,"This paper outlines and discusses the new mechanical characterization metrologies applied on PCI board envelope. 'The dynamic responses of PCI board were monitored and characterized using accelerometer and strain gauges. PCI board performances were analyzed to differentiate its high risk areas through analysis of board strain responses to solder joint crack. Board """"strain states"""" analysis methodology was introduced to provide immediate accurate board bending modes and deflection associated with experimental results. Using this methodology, it eases the board bend mode analysis which can capture the board strain performance limit at the same time. In addition, high speed camera (HSC) tool was incorporated into the evaluation to understand the boards bend history under shock test. This allows better view of the bending moment and matching to defect locations for corrective action implementation. Detailed failure analysis mapping of solder joint crack percentages was successfully gathered to support those findings. Key influences, such as thermal/mechanical enabling preload masses and shock input profiles on solder joint crack severity were conducted as well to understand the potential risk modulators for SJR performance. Furthermore, commercial simulation software analysis tool was applied to correlate the board's bend modes and predict the high risk solder joint location; which is important for product enabling solutions design. As a result, a system level stiffener solution was designed. Hence, with this characterization and validation concept, a practical stiffener solution for PCI application was validated through a special case study to improve the board SJR performance in its use condition.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598243,no,no,1486912032.014315
Intelligent Java Analyzer,"This paper presents a software metric working prototype to evaluate Java programmer's profiles. In order to automatically detect source code patterns, a Multi Layer Perceptron neural network is applied. Features determined from such patterns constitute the basis for systempsilas programmer profiling. Results presented here show that the proposed prototype is a confident approach for support in the software quality assurance process.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641074,no,no,1486912031.227098
An Ant Colony System Hybridized with Randomized Algorithm for TSP,"Ant algorithms are a recently developed, population- based approach which has been successfully applied to several NP-hard combinatorial optimization problems. In this paper, through an analysis of the constructive procedure of the solution in the ant colony system (ACS),we present an ant colony system hybridized with randomized algorithm(RAACS). In RAACS, only partial cities are randomly chosen to compute the state transition probability. Experimental results for solving the traveling salesman problems(TSP) with both ACS and RAACS demonstrate that averagely speaking, the proposed method is better in both the quality of solutions and the speed of convergence compared with the ACS.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287897,no,no,1486912031.227097
An empirical study of software reuse vs. defect-density and stability,"The paper describes results of an empirical study, where some hypotheses about the impact of reuse on defect-density and stability, and about the impact of component size on defects and defect-density in the context of reuse are assessed, using historical data (data mining) on defects, modification rate, and software size of a large-scale telecom system developed by Ericsson. The analysis showed that reused components have lower defect-density than non-reused ones. Reused components have more defects with highest severity than the total distribution, but less defects after delivery, which shows that that these are given higher priority to fix. There are an increasing number of defects with component size for non-reused components, but not for reused components. Reused components were less modified (more stable) than non-reused ones between successive releases, even if reused components must incorporate evolving requirements from several application products. The study furthermore revealed inconsistencies and weaknesses in the existing defect reporting system, by analyzing data that was hardly treated systematically before.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317450,no,no,1486912031.227096
A taxonomy for software voting algorithms used in safety-critical systems,"Voting algorithms are used to provide an error masking capability in a wide range of highly dependable commercial & research applications. These applications include N-Modular Redundant hardware systems and diversely designed software systems based on N-Version Programming. The most sophisticated & complex algorithms can even tolerate malicious (or Byzantine) subsystem errors. The algorithms can be implemented in hardware or software depending on the characteristics of the application, and the type of voter selected. Many voting algorithms have been defined in the literature, each with particular strengths and weaknesses. Having surveyed more than 70 references from the literature, a functional classification is used in this paper to provide taxonomy of those voting algorithms used in safety-critical applications. We classify voters into three categories: generic, hybrid, and purpose-built voters. Selected algorithms of each category are described, for illustrative purposes, and application areas proposed. Approaches to the comparison of algorithm behavior are also surveyed. These approaches compare the acceptability of voter behavior based on either statistical considerations (e.g., number of successes, number of benign or catastrophic results), or probabilistic computations (e.g., probability of choosing correct value in each voting cycle or average mean square error) during q voting cycles.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1331674,no,no,1486912031.227094
"Performance Assurance via Software Rejuvenation: Monitoring, Statistics and Algorithms","We present three algorithms for detecting the need for software rejuvenation by monitoring the changing values of a customer-affecting performance metric, such as response time. Applying these algorithms can improve the values of this customer-affecting metric by triggering rejuvenation before performance degradation becomes severe. The algorithms differ in the way they gather and use sample values to arrive at a rejuvenation decision. Their effectiveness is evaluated for different sets of control parameters, including sample size, using simulation. The results show that applying the algorithms with suitable choices of control parameters can significantly improve system performance as measured by the response time",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1633532,no,no,1486912031.227093
Assessing learning progress and quality of teaching in large groups of students,"The classic tool of assessing learning progress are written tests and assignments. In large groups of students the workload often does not allow in depth evaluation during the course. Thus our aim was to modify the course to include active learning methods and student centered teaching. We changed the course structure only slightly and established new assessment methods like minute papers, short tests, mini-projects and a group project at the end of the semester. The focus was to monitor the learning progress during the course so that problematic issues could be addressed immediately. The year before the changes 26.76 % of the class failed the course with a grade average of 3.66 (Pass grade is 4.0/30 % of achievable marks). After introducing student centered teaching, only 14 % of students failed the course and the average grade was 3.01. Grades were also distributed more evenly with more students achieving better results. We have shown that even in large groups of students with > 100 participants student centered and active learning is possible. Although it requires a great work overhead on the behalf of the teaching staff, the quality of teaching and the motivation of the students is increased leading to a better learning environment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4649803,no,no,1486912031.227092
On Co-Training Style Algorithms,"During the past few years, semi-supervised learning has become a hot topic in machine learning and data mining, since manually labeling training examples is a tedious, error prone and time-consuming task in many practical applications. As one of the most predominant semi-supervised learning algorithms, co-training has drawn much attention and shown its superiority in many applications. So far, there have been a variety of variants of co-training algorithms aiming to settle practical problems. In order to launch an effective co-training process, these variants as a whole create their diversities in four different ways, i.e. two-view level, underlying classifiers level, datasets level and active learning level. This paper gives a review on co-training style algorithms just from this view and presents typical examples and analysis for each level respectively.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4667971,no,no,1486912031.22709
An Empirical Study on a Specification-Based Program Review Approach,"Program review is an effective technique for detecting faults in software systems by reading and analyzing program code. However, challenges still remain in providing systematic and rigorous review techniques. We have recently developed a rigorous review approach and a software tool that provide reviewers with support in analyzing whether a program accurately implements the functions and properties defined in its specification. In this paper, we describe an empirical study of the application of our review approach and tool to a software system for automated teller machines (ATMs). We also discuss the effectiveness of the review approach, as well as some weaknesses, based on the results of our study, and suggest potential solutions to the problems encountered during the study",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4024050,no,no,1486912031.227089
What makes finite-state models more (or less) testable?,"This paper studies how details of a particular model can effect the efficacy of a search for detects. We find that if the test method is fixed, we can identity classes of software that are more or less testable. Using a combination of model mutators and machine learning, we find that we can isolate topological features that significantly change the effectiveness of a defect detection tool. More specifically, we show that for one defect detection tool (a stochastic search engine) applied to a certain representation (finite state machines), we can increase the average odds of finding a defect from 69% to 91%. The method used to change those odds is quite general and should apply to other defect detection tools being applied to other representations.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115019,no,no,1486912031.227087
Regression benchmarking with simple middleware benchmarks,"The paper introduces the concept of regression benchmarking as a variant of regression testing focused at detecting performance regressions. Applying the regression benchmarking in the area of middleware development, the paper explains how regression benchmarking differs from middleware benchmarking in general. On a real-world example of TAO, the paper shows why the existing benchmarks do not give results sufficient for regression benchmarking, and proposes techniques for detecting performance regressions using simple benchmarks.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1395179,no,no,1486912031.227084
An empirical study on groupware support for software inspection meetings,"Software inspection is an effective way to assess product quality and to reduce the number of defects. In a software inspection, the inspection meeting is a key activity to agree on collated defects, to eliminate false positives, and to disseminate knowledge among the team members. However, inspection meetings often require high effort and may lose defects found in earlier inspection steps due to ineffective meeting techniques. Only few tools are available for this task. We have thus been developing a set of groupware tools to lower the effort of inspection meetings and to increase their efficiency. We conducted an experiment in an academic environment with 37 subjects to empirically investigate the effect of groupware tool support for inspection meetings. The main findings of the experiment are that tool support considerably lowered the meeting effort, supported inspectors in identifying false positives, and reduced the number of true defects lost.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240289,no,no,1486912030.435477
Detecting Malicious Manipulation in Grid Environments,"Malicious manipulation of jobs results endangers the efficiency and performance of grid computing applications. The presence of nodes interested in depreciating jobs results may be detected and minimized with the usage of fault tolerance techniques. In order to detect this kind of nodes, this paper presents a distributed and hierarchical diagnosis model based on comparison and reputation, which can be applied to both public and private grids. This strategy defines the status of a node according to its level of confidence, measured through its behavior. The proposed model was submitted to simulations to evaluate its effectiveness under different quota of malicious nodes. The results reveals that 8 test rounds can detect practically all malicious nodes and even with less rounds, the correctness remains high without a significant overhead increase",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032413,no,no,1486912030.435476
Performability Models for Multi-Server Systems with High-Variance Repair Durations,"We consider cluster systems with multiple nodes where each server is prone to run tasks at a degraded level of service due to some software or hardware fault. The cluster serves tasks generated by remote clients, which are potentially queued at a dispatcher. We present an analytic queueing model of such systems, represented as an M/MMPP/1 queue, and derive and analyze exact numerical solutions for the mean and tail-probabilities of the queue-length distribution. The analysis shows that the distribution of the repair time is critical for these performability metrics. Additionally, in the case of high-variance repair times, the model reveals so-called blow-up points, at which the performance characteristics change dramatically. Since this blowup behavior is sensitive to a change in model parameters, it is critical for system designers to be aware of the conditions under which it occurs. Finally, we present simulation results that demonstrate the robustness of this qualitative blow-up behavior towards several model variations.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273028,no,no,1486912030.435475
Assessing quality of web based systems,"This paper proposes an assessment model for Web-based systems in terms of non-functional properties of the system. The proposed model consists of two stages: (i) deriving quality metrics using goal-question-metric (GQM) approach; and (ii) evaluating the metrics to rank a Web based system using multi-element component comparison analysis technique. The model ultimately produces a numeric rating indicating the relative quality of a particular Web system in terms of selected quality attributes. We decompose the quality objectives of the web system into sub goals, and develop questions in order to derive metrics. The metrics are then assessed against the defined requirements using an assessment scheme.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493613,no,no,1486912030.435473
A methodology for testbed validation and performance assessment of network/service management systems,"Delivery of multimedia real time flows over multi-domain IP networks require end-to-end quality of service guarantees. In order to manage and control the high-level services (video on demand, IPTV, etc.) as well as the network connectivity services across multiple domains in a coherent way, a distributed but integrated management system is proposed. Such a system has been defined, specified, and is currently implemented in the framework of the ENTHRONE European project. The system is being validated and assessed through several complex interconnected test-beds/pilots. This paper proposes a test methodology for validating the network service management functionalities on a multi-domain test-bed environment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588771,no,no,1486912030.435472
Microarchitectural Support for Program Code Integrity Monitoring in Application-specific Instruction Set Processors,"Program code in a computer system can be altered either by malicious security attacks or by various faults in microprocessors. At the instruction level, all code modifications are manifested as bit flips. In this work, we present a generalized methodology for monitoring code integrity at run-time in application-specific instruction set processors (ASIPs), where both the instruction set architecture (ISA) and the underlying micro architecture can be customized for a particular application domain. We embed monitoring microoperations in machine instructions, thus the processor is augmented with a hardware monitor automatically. The monitor observes the processor's execution trace of basic blocks at run-time, checks whether the execution trace aligns with the expected program behavior, and signals any mismatches. Since microoperations are at a lower software architecture level than processor instructions, the microarchitectural support for program code integrity monitoring is transparent to upper software levels and no recompilation or modification is needed for the program. Experimental results show that our microarchitectural support can detect program code integrity compromises with small area overhead and little performance degradation",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211901,no,no,1486912030.43547
Measuring the Strength of Indirect Coupling,"It is widely accepted that coupling plays an important role in software quality, particularly in the areas of software maintenance, so effort should be made to keep coupling levels to a minimum in order to reduce the complexity of the system. We have previously introduced the concept of """"indirect"""" coupling - coupling formed by relationships/dependencies that are not directly evident - with the belief that high levels of indirect coupling can constitute greater costs to maintenance as it is harder to detect. In this paper we extend our previous studies by proposing metrics that can advance our understanding of the exact relationship between indirect coupling and maintainability. In particulars the metrics focus on the reflection of """"strength"""" as it is a fundamental component of coupling. We present our observations on the results of applying the metrics to existing Java applications.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4159684,no,no,1486912030.435469
A scalable distributed QoS multicast routing protocol,"Many Internet multicast applications such as teleconferencing and remote diagnosis have quality-of-service (QoS) requirements. It is a challenging task to build QoS constrained multicast trees with high performance, high success ratio, low overhead, and low system requirements. This paper presents a new scalable QoS multicast routing protocol (SoMR) that has very small communication overhead and requires no state outside the multicast tree. SoMR achieves the favorable tradeoff between routing performance and overhead by carefully selecting the network sub-graph in which it conducts the search for a path that can support the QoS requirement, and by auto-tuning the selection according to the current network conditions. Its early-warning mechanism helps to detect and route around the real bottlenecks in the network, which increases the chance of finding feasible paths for additive QoS requirements. SoMR minimizes the system requirements; it relies only on the local state stored at each router. The routing operations are completely decentralized.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1312682,no,no,1486912030.435467
Test-Suite Augmentation for Evolving Software,"One activity performed by developers during regression testing is test-suite augmentation, which consists of assessing the adequacy of a test suite after a program is modified and identifying new or modified behaviors that are not adequately exercised by the existing test suite and, thus, require additional test cases. In previous work, we proposed MATRIX, a technique for test-suite augmentation based on dependence analysis and partial symbolic execution. In this paper, we present the next step of our work, where we (I) improve the effectiveness of our technique by identifying all relevant change-propagation paths, (2) extend the technique to handle multiple and more complex changes, (3) introduce the first tool that fully implements the technique, and (4) present an empirical evaluation performed on real software. Our results show that our technique is practical and more effective than existing test-suite augmentation approaches in identifying test cases with high fault-detection capabilities.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4639325,no,no,1486912030.435464
An effective fault-tolerant routing methodology for direct networks,"Current massively parallel computing systems are being built with thousands of nodes, which significantly affect the probability of failure. M. E. Gomex proposed a methodology to design fault-tolerant routing algorithms for direct interconnection networks. The methodology uses a simple mechanism: for some source-destination pairs, packets are first forwarded to an intermediate node, and later, from this node to the destination node. Minimal adaptive routing is used along both subpaths. For those cases where the methodology cannot find a suitable intermediate node, it combines the use of intermediate nodes with two additional mechanisms: disabling adaptive routing and using misrouting on a per-packet basis. While the combination of these three mechanisms tolerates a large number of faults, each one requires adding some hardware support in the network and also introduces some overhead. In this paper, we perform an in-depth detailed analysis of the impact of these mechanisms on network behaviour. We analyze the impact of the three mechanisms separately and combined. The ultimate goal of this paper is to obtain a suitable combination of mechanisms that is able to meet the trade-off between fault-tolerance degree, routing complexity, and performance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1327925,no,no,1486912029.668681
GPFlow: An Intuitive Environment for Web Based Scientific Workflow,"Increasingly scientists are using collections of software tools in their research. These tools are typically used in concert, often necessitating laborious and error prone manual data reformatting and transfer. We present an intuitive workflow environment to support scientists with their research. The workflow, GPFlow, wraps legacy tools, presenting a high level, interactive Web based frontend to scientists. The workflow backend is realized by a commercial grade workflow engine (BizTalk). The workflow model is inspired by spreadsheets and is novel in its support for an intuitive method of interaction as required by many scientists, for example, bioinformaticians. We apply GPFlow to two bioinformatics experiments and demonstrate its flexibility and simplicity",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031553,no,no,1486912029.66868
Case-base reasoning in vehicle fault diagnostics,"This paper presents our research in case-based reasoning (CBR) with application to vehicle fault diagnosis. We have developed a distributed diagnostic agent system, DDAS that detects faults of a device based on signal analysis and machine learning. The CBR techniques presented are used to rind root cause of vehicle faults based on the information provided by the signal agents in DDAS. Two CBR methods are presented, one used directly the diagnostic output from the signal agents and another uses the signal segment features. We present experiments conducted on real vehicle cases collected from auto dealers and the results show that both method are effective in finding root causes of vehicle faults.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1223990,no,no,1486912029.668678
Embedded system engineering using C/C++ based design methodologies,This paper analyzes and compares the effectiveness of various system level design methodologies in assessing performance of embedded computing systems from the earliest stages of the design flow. The different methodologies are illustrated and evaluated by applying them to the design of an aircraft pressurization system (APS). The APS is mapped on a heterogeneous hardware/software platform consisting of two ASICs and a microcontroller. The results demonstrate the high impact of computer aided design (CAD) tools on design time and quality.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1409904,no,no,1486912029.668677
Mining Object Usage Models,"Programs usually follow many implicit programming rules or patterns, violations of which frequently lead to failures. This thesis proposes a novel approach to statically mine object usage models representing such patterns for objects used in a program. Additionally, we will describe how object usage models can be used to automatically detect defects, increase program understanding and support programmers by providing code templates. In preliminary experiments the proposed method detected two previously unknown bugs in open source software.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222696,no,no,1486912029.668675
Ground Penetrating Radar: A Smart Sensor for the Evaluation of the Railway Trackbed,"Ground Penetrating Radar (GPR) has become an increasingly attractive method for the engineering community, in particular for shallow high-resolution applications such as railway trackbed evaluation. It is a non-destructive smart sensing technique, which can be applied dynamically to achieve a continuous profile of the trackbed structure. Due to recent hardware and software improvements, real time cursory analysis can be performed in the field. Based on collected field data, the present paper investigates the applicability of the GPR smart sensor system in terms of the railways trackbed assessment and concludes on the capability of the GPR sensing technique to assess adequately the ballast quality and the trackbed formation.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258127,no,no,1486912029.668674
Application fault tolerance with Armor middleware,"Many current approaches to software-implemented fault tolerance (SIFT) rely on process replication, which is often prohibitively expensive for practical use due to its high performance overhead and cost. The adaptive reconfigurable mobile objects of reliability (Armor) middleware architecture offers a scalable low-overhead way to provide high-dependability services to applications. It uses coordinated multithreaded processes to manage redundant resources across interconnected nodes, detect errors in user applications and infrastructural components, and provide failure recovery. The authors describe the experiences and lessons learned in deploying Armor in several diverse fields.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1405971,no,no,1486912029.668672
Fabrication of SiGe-On-Insulator by Improved Ge Condensation Technique,"Silicon germanium on insulator (SGOI) is a straightforward material for ultimate device scaling. This substrate combines two advantages: high carrier's velocity of the Si<sub>1 - x</sub>Ge<sub>x </sub> alloy and low parasitic capacitance due to the presence of a buried oxide. Several fabrication techniques for SGOI substrates, as SMOX, SMART-CUTtrade or liquid phase epitaxy have been proposed. Tezuka et al. present a new approach involving an epitaxial growth of low Ge contents SiGe alloy on SOI substrate followed by a high temperature oxidation. By selective oxidation of Silicon and diffusion of Germanium within the remaining SGOI layer, Ge content increases. A high Ge concentration SGOI layer is then obtained. Ge condensation technique is based on two competitive mechanisms: silicon oxidation involving Ge pill up at the oxide interface and Ge diffusion within the SiGe layer. Both take place during the high temperature oxidation. To favour Ge diffusion and carry out homogeneous SGOI profiles, we propose an improved Ge condensation technique with a multi-steps oxidation. Samples have been characterized by spectroscopic ellipsometry (SE), X ray reflection (XRR), X-ray fluorescence (XRF) and secondary ions mass spectroscopy (SIMS), transmission electronic microscopy (TEM) to assess the process quality for uniform SGOI fabrication with different Ge contents. Relaxation of SGOI layers has been observed either by atomic force microscopy or Raman spectroscopy. Influence of oxidation time has been studied and well defined oxidation recipes are proposed to obtain different Ge content SGOI substrates. Numerical simulations including the initial parameters, i.e., top SOI thickness, SiGe grown layer thickness and compositions have been studied by Athena software",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1716025,no,no,1486912029.66867
"A two layered case based reasoning approach to text summarization, based on summarization pattern","What actually is done in case of text summarization in case based reasoning terminology is that, the situation is defined as the ensemble of some consecutive sentences, and the solution is the set of the sentences selected as the outcome of the summarization process. In order to make a quality summary considering the context, a semantic understanding, seems to be important. In this respect we propose an approach to use a two layered CBR approach. Regarding this, we proposed an approach to text summarization based on two layered case based reasoning framework. Regarding this, the primary CBR cycle tries to make a summary of the source text, and the secondary CBR cycle tries to detect the context, and changes the bias values (fixed values) related to the primary CBR modules.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1242398,no,no,1486912029.668669
Building a requirement fault taxonomy: experiences from a NASA verification and validation research project,"Fault-based analysis is an early lifecycle approach to improving software quality by preventing and/or detecting pre-specified classes of faults prior to implementation. It assists in the selection of verification and validation techniques that can be applied in order to reduce risk. This paper presents our methodology for requirements-based fault analysis and its application to National Aeronautics and Space Administration (NASA) projects. The ideas presented are general enough to be applied immediately to the development of any software system. We built a NASA-specific requirement fault taxonomy and processes for tailoring the taxonomy to a class of software projects or to a specific project. We examined requirement faults for six systems, including the International Space Station (ISS), and enhanced the taxonomy and processes. The developed processes, preliminary tailored taxonomies for critical/catastrophic high-risk (CCHR) systems, preliminary fault occurrence data for the ISS project, and lessons learned are presented and discussed.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251030,no,no,1486912029.668666
Optimized multipinhole design for mouse imaging,"To enhance high-sensitivity focused mouse imaging using multipinhole SPECT on a dual head camera, a fast analytical method was used to predict the contrast-to-noise ratio (CNR) in many points of a homogeneous cylinder for a large number of pinhole collimator designs with modest overlap. The design providing the best overall CNR, a configuration with 7 pinholes, was selected. Next, the pinhole pattern was made slightly irregular to reduce multiplexing artifacts. Two identical, but mirrored 7-pinhole plates were manufactured. In addition, the calibration procedure was refined to cope with small deviations of the camera from circular motion. First, the new plates were tested by reconstructing a simulated homogeneous cylinder measurement. Second, a Jaszczak phantom filled with 37 MBq <sup>99m</sup>Tc was imaged on a dual head gamma camera, equipped with the new pinhole collimators. The image quality before and after refined calibration was compared for both heads, reconstructed separately and together. Next, 20 short scans of the same phantom were performed with single and multipinhole collimation to investigate the noise improvement of the new design. Finally, two normal mice were scanned using the new multipinhole designs to illustrate the reachable image quality of abdomen and thyroid imaging. The simulation study indicated that the irregular patterns suppress most multiplexing artifacts. Using body support information strongly reduces the remaining multiplexing artifacts. Refined calibration improved the spatial resolution. Depending on the location in the phantom, the CNR increased with a factor of 1 to 2.5 using the new instead of a single pinhole design. The first proof of principle scans and reconstructions were successful, allowing the release of the new plates and software for preclinical studies in mice.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774304,no,no,1486912028.910089
Task-oriented modelling of autonomous decentralised systems,"An ongoing project of the Programme for Highly Dependable Systems (PHDS) at the University of the Witwatersrand is the development of a dependable decentralised system using readily available hardware and software components. An experimental system has been developed to support multiple-task applications with different levels of criticality. Fault-tolerant protocols are used to detect faults, to mask incorrect results from faulty nodes. A task in a faulty node can he recovered through a system reconfiguration or task reallocation. A faulty node can he repaired and reintegrated into the system. This paper focuses on modelling the system under the occurrence of faults, reconfiguration and repair. The method developed can be used to evaluate individual task's reliability, risk and availability",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=880904,no,no,1486912028.910088
Application of Entropy-Based Markov Chains Data Fusion Technique in Fault Diagnosis,"This paper proposes an entropy-based Markov (EMC) chain fusion technique to solve the problem that the sample set is incompletion in fault diagnostic field. Firstly, the concept about probability Petri net is defined. It can calculate the fault occurred probability from incidence matrix based on the complemental information. Secondly, probability Petri net diagnostic model is designed from diagnostic rules that obtained by Skowron default rule generation method after the sample set is reduced by rough set theory. And in order to simplify the framework of the diagnostic model, Petri net model is designed as distributed form. Finally, depending on the diagnosis of distributed diagnostic model, EMC technique will be used to obtain consensus output if the places that represent fault in the model have several tokens. The diagnostic result is the consensus output that with the maximum of posterior probability after normalized treatment. The design is described by an example about rotating machinery fault diagnosis, and is proved availability by test sample set.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721813,no,no,1486912028.910086
Testing for missing-gate faults in reversible circuits,"Logical reversibility occurs in low-power applications and is an essential feature of quantum circuits. Of special interest are reversible circuits constructed from a class of reversible elements called k-CNOT (controllable NOT) gates. We review the characteristics of k-CNOT circuits and observe that traditional fault models like the stuck-at model may not accurately represent their faulty behavior or test requirements. A new fault model, the missing gate fault (MGF) model, is proposed to better represent the physical failure modes of quantum technologies. It is shown that MGFs are highly testable, and that all MGFs in an N-gate k-CNOT circuit can be detected with from one to [N/2] test vectors. A design-for-test (DFT) method to make an arbitrary circuit fully testable for MGFs using a single test vector is described. Finally, we present simulation results to determine (near) optimal test sets and DFT configurations for some benchmark circuits.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376543,no,no,1486912028.910085
Fault prognosis using dynamic wavelet neural networks,"Prognostic algorithms for condition based maintenance of critical machine components are presenting major challenges to software designers and control engineers. Predicting time-to-failure accurately and reliably is absolutely essential if such maintenance practices are to find their way into the industrial floor. Moreover, means are required to assess the performance and effectiveness of these algorithms. This paper introduces a prognostic framework based upon concepts from dynamic wavelet neural networks and virtual sensors and demonstrates its feasibility via a bearing failure example. Statistical methods to assess the performance of prognostic routines are suggested that are intended to assist the user in comparing candidate algorithms. The prognostic and assessment methodology proposed here may be combined with diagnostic and maintenance scheduling methods and implemented on a conventional computing platform to serve the needs of industrial and other critical processes",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949467,no,no,1486912028.910083
A Discrete Differential Operator for Direction-based Surface Morphometry,"This paper presents a novel directional morphometry method for surfaces using first order derivatives. Non-directional surface morphometry has been previously used to detect regions of cortical atrophy using brain MRI data. However, evaluating directional changes on surfaces requires computing gradients to obtain a full metric tensor. Non-directionality reduces the sensitivity of deformation-based morphometry to area-preserving deformations. By proposing a method to compute directional derivatives, this paper enables analysis of directional deformations on surfaces. Moreover, the proposed method exhibits improved numerical accuracy when evaluating mean curvature, compared to the so-called cotangent formula. The directional deformation of folding patterns was measured in two groups of surfaces and the proposed methodology allowed to defect morphological differences that were not detected using previous non-directional morphometry. The methodology uses a closed-form analytic formalism rather than numerical approximation and is readily generalizable to any application involving surface deformation.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4408886,no,no,1486912028.910082
Probability models for high dynamic range imaging,"Methods for expanding the dynamic range of digital photographs by combining images taken at different exposures have recently received a lot of attention. Current techniques assume that the photometric transfer function of a given camera is the same (modulo an overall exposure change) for all the input images. Unfortunately, this is rarely the case with today's camera, which may perform complex nonlinear color and intensity transforms on each picture. In this paper, we show how the use of probability models for the imaging system and weak prior models for the response functions enable us to estimate a different function for each image using only pixel intensity values. Our approach also allows us to characterize the uncertainty inherent in each pixel measurement. We can therefore produce statistically optimal estimates for the hidden variables in our model representing scene irradiance. We present results using this method to statistically characterize camera imaging functions and construct high-quality high dynamic range (HDR) images using only image pixel information.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1315160,no,no,1486912028.910081
Usability Evaluation Based on Web Design Perspectives,"Given the growth in the number and size of Web Applications worldwide, Web quality assurance, and more specifically Web usability have become key success factors. Therefore, this work proposes a usability evaluation technique based on the combination of Web design perspectives adapted from existing literature, and heuristics. This new technique is assessed using a controlled experiment aimed at measuring the efficiency and effectiveness of our technique, in comparison to Nielsen's heuristic evaluation. Results indicated that our technique was significantly more effective than and as efficient as Nielsen's heuristic evaluation.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343742,no,no,1486912028.910079
Historical Value-Based Approach for Cost-Cognizant Test Case Prioritization to Improve the Effectiveness of Regression Testing,"Regression testing has been used to support software testing activities and assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive because it requires many test case executions, and the number of test cases increases sharply as the software evolves. In this paper, we propose the Historical Value-Based Approach, which is based on the use of historical information, to estimate the current cost and fault severity for cost-cognizant test case prioritization. We also conducted a controlled experiment to validate the proposed approach, the results of which proved the proposed approachpsilas usefulness. As a result of the proposed approach, software testers who perform regression testing are able to prioritize their test cases so that their effectiveness can be improved in terms of average percentage of fault detected per cost.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579792,no,no,1486912028.910078
Diagnosis and prognosis of bearings using data mining and numerical visualization techniques,"Traditionally, condition-based monitoring techniques have been used to diagnose failure in rotary machinery by application of low-level signal processing and trend analysis techniques. Such techniques consider small windows of data from large data sets to give preliminary information of developing fault(s) or failure precursor(s). However, these techniques only provide information of a minute portion of a large data set, which limits the accuracy of predicting the remaining useful life of the system. Diagnosis and prognosis (DAP) techniques should be able to identify the origin of the fault(s), estimate the rate of its progression and determine the remaining useful life of the system. This research demonstrates the use of data mining and numerical visualization techniques for diagnosis and prognosis of bearing vibration data. By using these techniques a comprehensive understanding of large vibration data sets can be attained. This approach uses intelligent agents to isolate particular bearing vibration characteristics using statistical analysis and signal processing for data compression. The results of the compressed data can be visualized in 3-D plots and used to track the origination and evolution of failure in the bearing vibration data. The Bearing Test Bed is used for applying measurable static and dynamic stresses on the bearing and collecting vibration signatures from the stressed bearings",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918553,no,no,1486912028.910075
Application of neural networks and filtered back projection to wafer defect cluster identification,"During an electrical testing stage, each die on a wafer must be tested to determine whether it functions as it was originally designed. In the case of a clustered defect on the wafer, such as scratches, stains, or localized failed patterns, the tester may not detect all of the defective dies in the flawed area. To avoid the defective dies proceeding to final assembly, an existing tool is currently used by a testing factory to detect the defect cluster and mark all the defective dies in the flawed region or close to the flawed region; otherwise, the testing factory must assign five to ten workers to check the wafers and hand mark the defective dies. This paper proposes two new wafer-scale defect cluster identifiers to detect the defect clusters, and compares them with the existing tool used in the industry. The experimental results verify that one of the proposed algorithms is very effective in defect identification and achieves better performance than the existing tool.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1188820,no,no,1486912028.151877
A replicated experiment of usage-based and checklist-based reading,"Software inspection is an effective method to detect faults in software artefacts. Several empirical studies have been performed on reading techniques, which are used in the individual preparation phase of software inspections. Besides new experiments, replications are needed to increase the body of knowledge in software inspections. We present a replication of an experiment, which compares usage-based and checklist-based reading. The results of the original experiment show that reviewers applying usage-based reading are more efficient and effective in detecting the most critical faults from a user's point of view than reviewers using checklist-based reading. We present the data of the replication together with the original experiment and compares the experiments. The main result of the replication is that it confirms the result of the original experiment. This replication strengthens the evidence that usage-based reading is an efficient reading technique.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357907,no,no,1486912028.151875
Managing MPICH-G2 Jobs with WebCom-G,"This paper discusses the use of WebCom-G to handle the management & scheduling of MPICH-G2 (MPI) jobs. Users can submit their MPI applications to a WebCom-G portal via a Web interface. WebCom-G then selects the machines to execute the application on, depending on the machines available to it and the number of machines requested by the user. WebCom-G automatically & dynamically constructs a RSL script with the selected machines and schedules the job for execution on these machines. Once the MPI application has finished executing, results are stored on the portal server, where the user can collect them. A main advantage of this system is fault survival, if any of the machines fail during the execution of a job, WebCom-G can automatically handle such failures. Following a machine failure, WebCom-G can create a new RSL script with the failed machines removed, incorporate new machines (if they are available) to replace the failed ones and re-launch the job without any intervention from the user. The probability of failures in a grid environment is high, so fault survival becomes an important issue",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609978,no,no,1486912028.151874
Probabilistic regression suites for functional verification,"Random test generators are often used to create regression suites on-the-fly. Regression suites are commonly generated by choosing several specifications and generating a number of tests from each one, without reasoning which specification should he used and how many tests should he generated from each specification. This paper describes a technique for building high quality random regression suites. The proposed technique uses information about the probablity of each test specification covering each coverage task. This probability is used, in tun, to determine which test specifications should be included in the regression suite and how many tests should, be generated from each specification. Experimental results show that this practical technique can he used to improve the quality, and reduce the cost, of regression suites. Moreover, it enables better informed decisions regarding the size and distribution of the regression suites, and the risk involved.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1322436,no,no,1486912028.151873
Class point: an approach for the size estimation of object-oriented systems,"In this paper, we present an FP-like approach, named class point, which was conceived to estimate the size of object-oriented products. In particular, two measures are proposed, which are theoretically validated showing that they satisfy well-known properties necessary for size measures. An initial, empirical validation is also performed, meant to assess the usefulness and effectiveness of the proposed measures to predict the development effort of object-oriented systems. Moreover, a comparative analysis is carried out, taking into account several other size measures.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1392720,no,no,1486912028.151871
Softgoal Traceability Patterns,"Goal oriented methods help software engineers to model high-level systemic goals, propose and evaluate architectural solutions, and detect and resolve conflicts that occur. This paper describes a new technique, known as softgoal traceability patterns, for enabling reusable class mechanisms such as design patterns to be applied within a goal-oriented framework. Softgoal traceability patterns increase the reliability of a design in respect to its goals through the automated generation of design elements and the establishment of bidirectional traces between goals and design. These traces are used to monitor the integrity of the design in respect to architectural quality goals, and to support impact analysis when design changes are proposed. Softgoal traceability patterns are described using the well-known Observer pattern and then expanded with a more complex pattern that incorporates authentication",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022002,no,no,1486912028.15187
An industrial environment for high-level fault-tolerant structures insertion and validation,"When designing a VLSI circuits, most of the efforts are now performed at levels of abstractions higher than gate. Correspondingly to this clear trend, there is a growing request to tackle safety-critical issues directly at the RT-level. This paper presents a complete environment for considering safety issues at the RT level. The environment was implemented and tested by an industry for devising a sample safety-critical device. Designers were permitted to assess the effects of transient faults, automatically add fault-tolerant structures, and validate the results working on the same circuit descriptions and acting in a coherent framework. The evaluation showed the effectiveness of the proposed environment.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011143,no,no,1486912028.151868
"Measurement, prediction and risk analysis for Web applications","Accurate estimates of development effort play an important role in the successful management of larger Web development projects. By applying measurement principles to measure qualities of the applications and their development processes, feedback can be obtained to help understand, control and improve products and processes. The objective of this paper is to present a Web design and authoring prediction model based on a set of metrics which were collected using a case study evaluation (CSE). The paper is organised into three parts. Part I describes the CSE in which the metrics used in the prediction model were collected. These metrics were organised into five categories: effort metrics, structure metrics, complexity metrics, reuse metrics and size metrics. Part II presents the prediction model proposed, which was generated using a generalised linear model (GLM), and assesses its predictive power. Finally, part III investigates the use of the GLM as a framework for risk management",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915541,no,no,1486912028.151867
Towards scalable proofs of robot swarm dependability,The concept of robot swarm has demonstrated its relevance in many safety critical applications as a cost-effective solution providing natural fault-tolerance by large number of mutually replacing agents. A critical factor to the swarm functionality is the high complexity of intra swarm coordination.We propose a fully distributed coordination algorithm that uses parameters like bidding distance and random waiting time between decision and action. Another key result is a formal method for predicting the success of swarm missions that rely on given coordination algorithm. The scalability of the model checking based proof method is addressed and a state symmetry based solution proposed.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4657513,no,no,1486912028.151866
A software implementation of a genetic algorithm based approach to network intrusion detection,"With the rapid expansion of Internet in recent years, computer systems are facing increased number of security threats. Despite numerous technological innovations for information assurance, it is still very difficult to protect computer systems. Therefore, unwanted intrusions take place when the actual software systems are running. Different soft computing based approaches have been proposed to detect computer network attacks. This paper presents a genetic algorithm (GA) based approach to network intrusion detection, and the software implementation of the approach. The genetic algorithm is employed to derive a set of classification rules from network audit data, and the support-confidence framework is utilized as fitness function to judge the quality of each rule. The generated rules are then used to detect or classify network intrusions in a real-time environment. Unlike most existing GA-based approaches, because of the simple representation of rules and the effective fitness function, the proposed method is easier to implement while providing the flexibility to either generally detect network intrusions or precisely classify the types of attacks. Experimental results show the achievement of acceptable detection rates based on benchmark DARPA data sets on intrusions, while no other complementary techniques or relevant heuristics are applied.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1434896,no,no,1486912028.151862
Software Reliability Analysis by Considering Fault Dependency and Debugging Time Lag,"Over the past 30 years, many software reliability growth models (SRGM) have been proposed. Often, it is assumed that detected faults are immediately corrected when mathematical models are developed. This assumption may not be realistic in practice because the time to remove a detected fault depends on the complexity of the fault, the skill and experience of personnel, the size of debugging team, the technique(s) being used, and so on. During software testing, practical experiences show that mutually independent faults can be directly detected and removed, but mutually dependent faults can be removed iff the leading faults have been removed. That is, dependent faults may not be immediately removed, and the fault removal process lags behind the fault detection process. In this paper, we will first give a review of fault detection & correction processes in software reliability modeling. We will then illustrate the fact that detected faults cannot be immediately corrected with several examples. We also discuss the software fault dependency in detail, and study how to incorporate both fault dependency and debugging time lag into software reliability modeling. The proposed models are fairly general models that cover a variety of known SRGM under different conditions. Numerical examples are presented, and the results show that the proposed framework to incorporate both fault dependency and debugging time lag for SRGM has a better prediction capability. In addition, an optimal software release policy for the proposed models, based on cost-reliability criterion, is proposed. The main purpose is to minimize the cost of software development when a desired reliability objective is given",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688079,no,no,1486912027.397375
An Object Oriented Complexity Metric Based on Cognitive Weights,"Complexity in general is defined as """"the degree to which a system or component has a design or implementation that is difficult to understand and verify """". Complexity metrics are used to predict critical information about reliability and maintainability of software systems. Object oriented software development requires a different approach to software metrics. In this paper, an attempt has been made to propose a metric for an object oriented code, which calculates the complexity of a class at method level. The proposed measure considers the internal architecture of the class, subclass, and member functions, while other proposed metrics for object oriented programming do not. An attempt has also been made to evaluate and validate the proposed measure in terms of Weyuker's properties and against the principles of measurement theory. It has been found that seven of nine Weyuker's properties have been satisfied by the proposed measure. It also satisfies most of the parameters required by the measurement theory perspective, hence establishes as a well-structured one.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341883,no,no,1486912027.397374
A Systematic Approach for Integrating Fault Trees into System Statecharts,"As software systems are encompassing a wide range of fields and applications, software reliability becomes a crucial step. The need for safety analysis and test cases that have high probability to uncover plausible faults are necessities in proving software quality. System models that represent only the operational behavioral of a system are incomplete sources for deriving test cases and performing safety analysis before the implementation process. Therefore, a system model that encompasses faults is required. This paper presents a technique that formalizes a safety model through the incorporation of faults with system specifications. The technique focuses on introducing semantic faults through the integration of fault trees with system specifications or statechart. The method uses a set of systematic transformation rules that tries to maintain the semantics of both fault trees and statechart representations during the transformation of fault trees into statechart notations.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591544,no,no,1486912027.397372
Classifying Software Changes: Clean or Buggy?,"This paper introduces a new technique for predicting latent software bugs, called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean, with a 78 percent accuracy and a 60 percent buggy change recall on average. Change classification has several desirable qualities: 1) The prediction granularity is small (a change to a single file), 2) predictions do not require semantic information about the source code, 3) the technique works for a broad array of project types and programming languages, and 4) predictions can be made immediately upon the completion of a change. Contributions of this paper include a description of the change classification approach, techniques for extracting features from the source code and change histories, a characterization of the performance of change classification across 12 open source projects, and an evaluation of the predictive power of different groups of features.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4408585,no,no,1486912027.397371
Software detection mechanisms providing full coverage against single bit-flip faults,"Increasing design complexity for current and future generations of microelectronic technologies leads to an increased sensitivity to transient bit-flip errors. These errors can cause unpredictable behaviors and corrupt data integrity and system availability. This work proposes new solutions to detect all classes of faults, including those that escape conventional software detection mechanisms, allowing full protection against transient bit-flip errors. The proposed solutions, particularly well suited for low-cost safety-critical microprocessor-based applications, have been validated through exhaustive fault injection experiments performed on a set of real and synthetic benchmark programs. The fault model taken into consideration was single bit-flip errors corrupting memory cells accessible to the user by means of the processor instruction set. The obtained results demonstrate the effectiveness of the proposed solutions.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1369518,no,no,1486912027.397369
Optimizing the planning and executing of software independent verification and validation (IV&V) in mature organizations,"To an organization involved in the construction of mission critical software, the safety and reliability of critical systems including their software is of utmost importance. The use of an independent group to provide verification and validation (IV&V) is intended to improve the quality of the software products. We seek to optimize the planning and execution of IV&V activities upon organizations that are already assessed with a certain level of process maturity, such as proposed by the Capability Maturity Model Integrated (CMMI).",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342660,no,no,1486912027.397368
Automated Generation and Assessment of Autonomous Systems Test Cases,"Verification and validation testing of autonomous spacecraft routinely culminates in the exploration of anomalous or faulted mission-like scenarios. Prioritizing which scenarios to develop usually comes down to focusing on the most vulnerable areas and ensuring the best return on investment of test time. Rules-of-thumb strategies often come into play, such as injecting applicable anomalies prior to, during, and after system state changes; or, creating cases that ensure good safety-net algorithm coverage. Although experience and judgment in test selection can lead to high levels of confidence about the majority of a system's autonomy, it's likely that important test cases are overlooked. One method to fill in potential test coverage gaps is to automatically generate and execute test cases using algorithms that ensure desirable properties about the coverage. For example, generate cases for all possible fault monitors, and across all state change boundaries. Of course, the scope of coverage is determined by the test environment capabilities, where a faster-than-real-time, high-fidelity, software-only simulation would allow the broadest coverage. Even real-time systems that can be replicated and run in parallel, and that have reliable set-up and operations features provide an excellent resource for automated testing. Making detailed predictions for the outcome of such tests can be difficult, and when algorithmic means are employed to produce hundreds or even thousands of cases, generating predicts individually is impractical, and generating predicts with tools requires executable models of the design and environment that themselves require a complete test program. Therefore, evaluating the results of large number of mission scenario tests poses special challenges. A good approach to address this problem is to automatically score the results based on a range of metrics. Although the specific means of scoring depends highly on the application, the use of formal scoring - metrics has high value in identifying and prioritizing anomalies, and in presenting an overall picture of the state of the test program. In this paper we present a case study based on automatic generation and assessment of faulted test runs for the Dawn mission, and discuss its role in optimizing the allocation of resources for completing the test program.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526484,no,no,1486912027.397367
Fault injection testing for distributed object systems,Interface based fault injection testing (IFIT) is proposed as a technique to assess the fault tolerance of distributed object systems. IFIT uses the description of an object's interface to generate application dependent faults. A set of application independent faults is also proposed. IFIT reveals inadequacies of the fault recovery mechanisms present in the application. The application of IFIT to different distributed object systems is described,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941680,no,no,1486912027.397365
Hybrid Prediction Model for improving Reliability in Self-Healing System,"In ubiquitous environments, which involve an even greater number of computing devices, with more informal modes of operation, this type of problem have rather serious consequences. In order to solve these problems when they arise, effective reliable systems are required. Also, system management is changing from a conventional central administration, to autonomic computing. However, most existing research focuses on healing after a problem has already occurred. In order to solve this problem, a prediction model is required to recognize operating environments and predict error occurrence. In this paper, a hybrid prediction model through four algorithms supporting self-healing in autonomic computing is proposed. This prediction model adopts a selective healing model, according to system situations for self-diagnosing and prediction of problems using four algorithms. In this paper, a hybrid prediction model is adopted to evaluate the proposed model in a self-healing system. In addition, prediction is compared with existing research and the effectiveness is demonstrated by experiment",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691368,no,no,1486912027.397364
Expert System for Power Quality Disturbance Classifier,"Identification and classification of voltage and current disturbances in power systems are important tasks in the monitoring and protection of power system. Most power quality disturbances are non-stationary and transitory and the detection and classification have proved to be very demanding. The concept of discrete wavelet transform for feature extraction of power disturbance signal combined with artificial neural network and fuzzy logic incorporated as a powerful tool for detecting and classifying power quality problems. This paper employes a different type of univariate randomly optimized neural network combined with discrete wavelet transform and fuzzy logic to have a better power quality disturbance classification accuracy. The disturbances of interest include sag, swell, transient, fluctuation, and interruption. The system is modeled using VHSIC hardware description language (VHDL), a hardware description language, followed by extensive testing and simulation to verify the functionality of the system that allows efficient hardware implementation of the same. This proposed method classifies, and achieves 98.19% classification accuracy for the application of this system on software-generated signals and utility sampled disturbance events.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4265719,no,no,1486912027.397361
QoS-Satisfied Pathover Scheme in FMIPv6 Environment,"Using the application of bluck data transfer, we investigate the performance of QoS-satisfied pathover for transport layer mobility scheme such as mSCTP in FMIPv6 envirmonment. We find that existing scheme has some defects in aspect of pathover and throughput. Based on this, we make a potential change to mSCTP by adding QoS-Measurement-Chunk, which is used to take into account information about wireless link condition in reselection/handover process of FMIPv6 network, we proposed a scheme with an algorithm named congestion-oriented pathover (COPO) to detect congestion of the primary path using back-to-back RTTs and adapt change of the wireless link parameters. A demonstrate using simulation is provided showing how the proposed scheme provides better performance to pathover and throughput.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722312,no,no,1486912026.645656
Intelligent fault diagnosis technique based on causality diagram,"We discuss the knowledge expression, reasoning and probability computing in causality diagram, which is developed from the belief network and overcomes some shortages. The model of causality diagram used for system fault diagnosis is brought forward, and the model constructing method and reasoning algorithm are also presented. At last, an application example in the fault diagnosis of the nuclear power plant is given which shows that the method is effect.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1340973,no,no,1486912026.645654
Cost-efficient Automated Visual Inspection system for small manufacturing industries based on SIFT,"This paper presents a cost efficient automated visual inspection (AVI) system for small industriespsila quality control system. The complex hardware and software make current AVI systems too expensive to afford for small-size manufacturing industries. Proposed approach to AVI systems is based on an ordinary PC with a medium resolution camera without any other extra hardware. The scale invariant feature transform (SIFT) is used to acquire good accuracy and make it applicable for different situations with different sample sizes, positions, and illuminations. Proposed method can detect three different defect types as well as locating and measuring defect percentage for more specialized utilization. To evaluate the performance of this system different samples with different sizes, shapes, and complexities are used and the results show that proposed system is highly applicable to different applications and is invariant to noise, illumination changes, rotation, and transformation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4762145,no,no,1486912026.645653
A systematic risk management approach employed on the CloudSat project,"The CloudSat Project has developed a simplified approach for fault tree analysis and probabilistic risk assessment. A system-level fault tree has been constructed to identify credible fault scenarios and failure modes leading up to a potential failure to meet the nominal mission success criteria. Risk ratings and fault categories have been defined for each low-level event (failure mode) and a streamlined probabilistic risk assessment has been completed. Although this technique or process will mature and evolve on a schedule that emphasizes added value throughout the development life cycle, it has already served to confirm that project personnel are concentrating risk reduction or elimination/retirement measures in the appropriate areas. A cursory evaluation with an existing fault tree analysis and probabilistic risk assessment software application has helped to validate this simplified approach. It is hoped that this will serve as a model for other NASA flight projects",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931738,no,no,1486912026.645652
"Comments on """"The confounding effect of class size on the validity of object-oriented metrics""""","It has been proposed by El Emam et al. (ibid. vol.27 (7), 2001) that size should be taken into account as a confounding variable when validating object-oriented metrics. We take issue with this perspective since the ability to measure size does not temporally precede the ability to measure many of the object-oriented metrics that have been proposed. Hence, the condition that a confounding variable must occur causally prior to another explanatory variable is not met. In addition, when specifying multivariate models of defects that incorporate object-oriented metrics, entering size as an explanatory variable may result in misspecified models that lack internal consistency. Examples are given where this misspecification occurs.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1214331,no,no,1486912026.64565
Constructing the Model of Propylene Distillation Based on Neural Networks,"The model of propylene distillation helps improve the quality of propylene products. This paper proposes a methodology of constructing the model of propylene distillation based on the neural network technique. The strategy of adjusting the neural network-based model of propylene distillation with rough sets is proposed. A numerical example of the neural network-based model for actual propylene distillation is provided. A comparison is made between the predicted results from the model and the actual results, which validates the effectiveness of the model of propylene distillation.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4450924,no,no,1486912026.645649
Predicting Risk as a Function of Risk Factors,"In previous research, we showed that risk factors have a significant negative effect on reliability (e.g., failure occurrence). In this research, we show that it is feasible to predict risk (i.e., the probability that risk factors are related to discrepancy reports occurring on a software release). This is an important advance over the previous research because discrepancy reports are available in the requirements phase - when the cost and labor required to correct faults is low, whereas failure data only becomes available in the test phase - when the cost and labor required to correct faults is high. Although using historical failure data to drive traditional software reliability models would produce greater prediction accuracy, the opportunity to provide early prediction of reliability, using risk factors, outweighs this advantage",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521201,no,no,1486912026.645648
A new wavelet-based method for detection of high impedance faults,"Detecting high impedance faults is one of the challenging issues for electrical engineers. Over-current relays can only detect some of the high impedance faults. Distance relays are unable to detect faults with impedance over 100 Omega. In this paper, by using an accurate model for high impedance faults, a new wavelet-based method is presented. The proposed method, which employs a 3 level neural network system, can successfully differentiate high impedance faults from other transients. The paper also thoroughly analyzes the effect of choice of mother wavelet on the detection performance. Simulation results which are carried out using PSCAD/EMTDC software are summarized",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1600519,no,no,1486912026.645646
A methodology for architectural-level risk assessment using dynamic metrics,"Risk assessment is an essential process of every software risk management plan. Several risk assessment techniques are based on the subjective judgement of domain experts. Subjective risk assessment techniques are human-intensive and error-prone. Risk assessment should be based on product attributes that we can quantitatively measure using product metrics. This paper presents a methodology for risk assessment at the early stages of the development lifecycle, namely the architecture level. We describe a heuristic risk assessment methodology that is based on dynamic metrics obtained from UML specifications. The methodology uses dynamic complexity and dynamic coupling metrics to define complexity factors for the architecture elements (components and connectors). Severity analysis is performed using FMEA (failure mode and effect analysis), as applied to architecture simulation models. We combine severity and complexity factors to develop heuristic risk factors for the architecture components and connectors. Based on component dependency graphs that were developed earlier for reliability analysis, and using analysis scenarios, we develop a risk assessment model and a risk analysis algorithm that aggregates the risk factors of components and connectors to the architectural level. We show how to analyze the overall risk factor of the architecture as the function of the risk factors of its constituting components and connectors. A case study of a pacemaker is used to illustrate the application of the methodology",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885873,no,no,1486912026.645645
Web service group testing with windowing mechanisms,"ASTRAR provides a framework for testing Web services (WS) using the group testing technique. This paper extends the basic two-phase testing process and introduces the windowing mechanism to further improve testing efficiency. Rather than testing a large number of WS simultaneously, WS are divided into subsets called windows and testing is exercised window by window. Testing results are analyzed for different strategies such as using all of the historical data, using the most recent windows, and using the current window only. Based on the results, test cases are ranked according to their potency to detect faults; and oracles and the confidence level of each oracle are established for individual test cases at runtime. In addition, different strategies are proposed to determine the optimal window size at runtime. By incorporating the windowing mechanism, the two-phase training and volume testing process becomes a continuous learning process and the basic group testing process becomes more adaptive to dynamically changing environment.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1551151,no,no,1486912026.645642
Assessing the dependability of OGSA middleware by fault injection,"This paper presents our research on devising a dependability assessment method for the upcoming OGSA 3.0 middleware using network level fault injection. We compare existing DCE middleware dependability testing research with the requirements of testing OGSA middleware and derive a new method and fault model. From this we have implemented an extendable fault injector framework and undertaken some proof of concept experiments with a simulated OGSA middleware system based around Apache SOAP and Apache Tomcat. We also present results from our initial experiments, which uncovered a discrepancy with our simulated OGSA system. We finally detail future research, including plans to adapt this fault injector framework from the stateless environment of a standard Web service to the stateful environment of an OGSA service.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238079,no,no,1486912025.901713
Design centering using an approximation to the constraint region,"The paper discusses the applicability of the piecewise-ellipsoidal approximation (PEA) to the acceptability region for solution of various design problems. The PEA technique, originally developed and tested for linear discrete circuits described in the frequency domain, is briefly reviewed. It is shown that PEA is a generic mathematical method and its applicability is extended to linear and nonlinear systems (not necessary electrical) described in time or frequency domains. The architecture of a software implementing the technique is introduced and approximations to the acceptability regions for the given design specifications for integrated circuits (CMOS amplifier, clock driver) and multidomain systems (servomechanism) are constructed and their accuracy checked. Then, some standard optimal-design algorithms (i.e., worst case parametric yield maximization, yield versus cost optimization) are redesigned to exploit the PEA properties (e.g., local convexity/concavity) and make them more effective. The algorithms are confronted with design problems, and quality of the resulting designs is assessed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275606,no,no,1486912025.901712
Model-based optimization revisited: Towards real-world processes,"The application of empirically determined surrogate models provides a standard solution to expensive optimization problems. Over the last decades several variants based on DACE (design and analysis of computer experiments) have provided excellent optimization results in cases where only a few evaluations could be made. In this paper these approaches are revisited with respect to their applicability in the optimization of production processes, which are in general multiobjective and allow no exact evaluations. The comparison to standard methods of experimental design shows significant improvements with respect to prediction quality and accuracy in detecting the optimum even if the experimental outcomes are highly distorted by noise. The universally assumed sensitivity of DACE models to nondeterministic data can therefore be refuted. Additionally, a practical example points out the potential of applying EC-methods to production processes by means of these models.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631199,no,no,1486912025.90171
Two-Dimensional Software Reliability Models and Their Application,"In general, the software-testing time may be measured by two kinds of time scales: calendar time and test-execution time. In this paper, we develop two-dimensional software reliability models with two-time measures and incorporate both of them to assess the software reliability with higher accuracy. Since the resulting software reliability models are based on the familiar non-homogeneous Poisson processes with two-time scales, which are the natural extensions of one-dimensional models, it is possible to treat both the time data simultaneously and effectively. We investigate the dependence of test-execution time as a testing effort on the software reliability assessment, and validate quantitatively the software reliability models with two-time scales. We also consider an optimization problem when to stop the software testing in terms of two-time measurements",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041882,no,no,1486912025.901708
Fault-oriented software robustness assessment for multicast protocols,"This paper reports a systematic approach for detecting software defects in multicast protocol implementations. We deploy a fault-oriented methodology and an integrated test system targeting software robustness vulnerabilities. The primary method is to assess protocol implementation by non-traditional interface fault injection that simulates network attacks. The test system includes a novel packet driving engine, a PDU generator based on Strengthened BNF notation and a few auxiliary tools. We apply it to two multicast protocols, IGMP and PIM-DM, and investigate their behaviors under active functional attacks. Our study proves its effectiveness for promoting production of more reliable multicast software.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201160,no,no,1486912025.901706
On-demand overlay networking of collaborative applications,"We propose a new overlay network, called Generic Identifier Network (GIN), for collaborative nodes to share objects with transactions across affiliated organizations by merging the organizational local namespaces upon mutual agreement. Using local namespaces instead of a global namespace can avoid excessive dissemination of organizational information, reduce maintenance costs, and improve robustness against external security attacks. GIN can forward a query with an O(1) latency stretch with high probability and achieve high performance. In the absence of a complete distance map, its heuristic algorithms for self configuration are scalable and efficient. Routing tables are maintained using soft-state mechanisms for fault tolerance and adapting to performance updates of network distances. Thus, GIN has significant new advantages for building an efficient and scalable distributed hash table for modern collaborative applications across organizations",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1651211,no,no,1486912025.901705
A unified framework for monitoring data streams in real time,"Online monitoring of data streams poses a challenge in many data-centric applications, such as telecommunications networks, traffic management, trend-related analysis, Web-click streams, intrusion detection, and sensor networks. Mining techniques employed in these applications have to be efficient in terms of space usage and per-item processing time while providing a high quality of answers to (1) aggregate monitoring queries, such as finding surprising levels of a data stream, detecting bursts, and to (2) similarity queries, such as detecting correlations and finding interesting patterns. The most important aspect of these tasks is their need for flexible query lengths, i.e., it is difficult to set the appropriate lengths a priori. For example, bursts of events can occur at variable temporal modalities from hours to days to weeks. Correlated trends can occur at various temporal scales. The system has to discover """"interesting"""" behavior online and monitor over flexible window sizes. In this paper, we propose a multi-resolution indexing scheme, which handles variable length queries efficiently. We demonstrate the effectiveness of our framework over existing techniques through an extensive set of experiments.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410105,no,no,1486912025.901703
Anshan: Wireless Sensor Networks for Equipment Fault Diagnosis in the Process Industry,"Wireless sensor networks provide an opportunity to enhance the current equipment diagnosis systems in the process industry, which have been based so far on wired networks. In this paper, we use our experience in the Anshan Iron and Steel Factory, China, as an example to present the issues from the real field of process industry, and our solutions. The challenges are three fold: First, very high reliability is required; second, energy consumption is constrained; and third, the environment is very challenging and constrained. To address these issues, it is necessary to put systematic efforts on network topology and node placement, network protocols, embedded software, and hardware. In this paper, we propose two technologies i.e. design for reliability and energy efficiency (DRE), and design for reconfiguration (DRC). Using these techniques we developed Anshan, a wireless sensor network for monitoring the temperature of rollers in a continuously annealing line and detecting equipment failures. Project Anshan includes 406 sensor nodes and has been running for four months continuously.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4557769,no,no,1486912025.901701
Software quality analysis with the use of computational intelligence,"Effectiveness and clarity of software objects, their adherence to coding standards and programming habits of programmers are important features of overall quality of software systems. This paper proposes an approach towards a quantitative software quality assessment with respect to extensibility, reusability, clarity and efficiency. It exploits techniques of Computational Intelligence (CI) that are treated as a consortium of granular computing, neural networks and evolutionary techniques. In particular, we take advantage of self-organizing maps to gain a better insight into the data, and study genetic decision trees-a novel algorithmic framework to carry out classification of software objects with respect to their quality. Genetic classifiers serve as a """"quality filter"""" for software objects. Using these classifiers, a system manager can predict quality of software objects and identify low quality objects for review and possible revision. The approach is applied to an object-oriented visualization-based software system for biomedical data analysis",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1006667,no,no,1486912025.901699
A stress-point resolution system based on module signatures,"This paper introduces a framework to provide design and testing guidance through a stress-point resolution system based on a module signature for module categorization. A stress-point resolution system includes stress-point identification and the selection of appropriate mitigation activities for those identified stress-points. Progress has been made in identifying stress-point to target the most fault-prone modules in a system by the module signature classification technique. Applying the stress-point prediction method on a large Motorola production system with approximately 1500 modules and comparing the classified modules to change reports, misclassification errors occurred at a rate of less than 2%. After identifying the stress point candidates, localized remedial actions should be undertaken. This algorithmic classification may suggest more insights into defect analysis and correction activities to enhance the software development strategies of software designers and testers.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1270743,no,no,1486912025.208312
Software-based erasure codes for scalable distributed storage,"This paper presents a new class of erasure codes, Lincoln Erasure codes (LEC), applicable to large-scale distributed storage that includes thousands of disks attached to multiple networks. A high-performance software implementation that demonstrates the capability to meet these anticipated requirements is described. A framework for evaluation of candidate codes was developed to support in-depth analysis. When compared with erasure codes based on the work of Reed-Solomon and Luby (2000), tests indicate LEC has a higher throughput for encoding and decoding and lower probability of failure across a range of test conditions. Strategies are described for integration with storage-related hardware and software.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194852,no,no,1486912025.208311
Considering fault removal efficiency in software reliability assessment,"Software reliability growth models (SRGMs) have been developed to estimate software reliability measures such as the number of remaining faults, software failure rate, and software reliability. Issues such as imperfect debugging and the learning phenomenon of developers have been considered in these models. However, most SRGMs assume that faults detected during tests will eventually be removed. Consideration of fault removal efficiency in the existing models is limited. In practice, fault removal efficiency is usually imperfect. This paper aims to incorporate fault removal efficiency into software reliability assessment. Fault removal efficiency is a useful metric in software development practice and it helps developers to evaluate the debugging effectiveness and estimate the additional workload. In this paper, imperfect debugging is considered in the sense that new faults can be introduced into the software during debugging and the detected faults may not be removed completely. A model is proposed to integrate fault removal efficiency, failure rate, and fault introduction rate into software reliability assessment. In addition to traditional reliability measures, the proposed model can provide some useful metrics to help the development team make better decisions. Software testing data collected from real applications are utilized to illustrate the proposed model for both the descriptive and predictive power. The expected number of residual faults and software failure rate are also presented.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1206460,no,no,1486912025.208309
One approach to the metric baselining imperative for requirements processes,"The success of development projects in customer-oriented industries depends on reliable processes for the definition and maintenance of requirements. With the sustained, severe reduction in the rush to new technology, this widely accepted fact has become increasingly evident in the networking industry. Customers now focus on high product quality as they strive for economy of operation. Enhancing product quality necessitates enhancing processes, which in turn can necessitate applying more accurate (and precise) measures. Finding process deviations and identifying patterns of product deficiencies are critical steps to achieving high quality products. We describe the application of quantitative process control (QPC) during early development phases to establish and maintain baseline distributions characterizing RMCM&T processes, and to monitor their evolutions. Metric baselining as described includes key metric identification, and data normalization, filtering, and categorization. Empirical baselining provides the statistical sensitivity to detect requirements process problems, and to support targeted identification of particular requirements-related patterns in defects.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232767,no,no,1486912025.208308
Software FMEA techniques,"Assessing the safety characteristics of software driven safety critical systems is problematic. The author has performed software FMEA on embedded automotive platforms for brakes, throttle, and steering with promising results. Use of software FMEA at a system and a detailed level has allowed visibility of software and hardware architectural approaches which assure safety of operation while minimizing the cost of safety critical embedded processor designs. Software FMEA has been referred to in the technical literature for more than fifteen years. Additionally, software FMEA has been recommended for evaluating critical systems in some standards, notably draft IEC 61508. Software FMEA is also provided for in the current drafts of SAE ARP 5580. However, techniques for applying software FMEA to systems during their design have been largely missing from the literature. Software FMEA has been applied to the assessment of safety critical real-time control systems embedded in military and automotive products. The paper is a follow on to and provides significant expansion to the software FMEA techniques originally described by the author in the 1993 RAMS paper Validating The Safety Of Real-Time Control Systems Using FMEA",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816294,no,no,1486912025.208307
An experimental framework for comparative digital library evaluation: the logging scheme,"Evaluation of digital libraries assesses their effectiveness, quality and overall impact. In this paper we present a novel, multi-level logging framework that will provide complete coverage of the different aspects of DL usage for user-system interactions. Based on this framework, we can analyse for various DL stakeholders the logging data according to their specific interests. In addition, analysis tools and a freely accessible log data repository will yield synergies and sustainability in DL evaluation and encourage a community for DL evaluation by providing for discussion on a common ground",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4119144,no,no,1486912025.208306
Improving the performance of speech recognition systems using fault-tolerant techniques,"In this paper, using of fault tolerant techniques are studied and experimented in speech recognition systems to make these systems robust to noise. Recognizer redundancy is implemented to utilize the strengths of several recognition methods that each one has acceptable performance in a specific condition. Duplication-with-comparison and NMR methods are experimented with majority and plurality voting on a telephony Persian speech-enabled IVR system. Results of evaluations present two promising outcomes, first, it improves the performance considerably; second, it enables us to detect the outputs with low confidence.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4697199,no,no,1486912025.208304
Quality-Aware Retrieval of Data Objects from Autonomous Sources for Web-Based Repositories,"The goal of this paper is to develop a framework for designing good data repositories for Web applications. The central theme of our approach is to employ statistical methods to predict quality metrics. These prediction quantities can be used to answer important questions such as: How soon should the local repository be synchronized to have a quality of at least 90% precision with certain confidence level? Suppose the local repository was synchronized three days ago, how many objects could have been deleted at the remote source since then?",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497600,no,no,1486912025.208302
An empirical study on testing and fault tolerance for software reliability engineering,"Software testing and software fault tolerance are two major techniques for developing reliable software systems, yet limited empirical data are available in the literature to evaluate their effectiveness. We conducted a major experiment to engage 34 programming teams to independently develop multiple software versions for an industry-scale critical flight application, and collected faults detected in these program versions. To evaluate the effectiveness of software testing and software fault tolerance, mutants were created by injecting real faults occurred in the development stage. The nature, manifestation, detection, and correlation of these faults were carefully investigated. The results show that coverage testing is generally an effective means to detecting software faults, but the effectiveness of testing coverage is not equivalent to that of mutation coverage, which is a more truthful indicator of testing quality. We also found that exact faults found among versions are very limited. This result supports software fault tolerance by design diversity as a creditable approach for software reliability engineering. Finally we conducted domain analysis approach for test case generation, and concluded that it is a promising technique for software testing purpose.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251036,no,no,1486912025.2083
Efficient data broadcast scheme on wireless link errors,"As portable wireless computers become popular, mechanisms to transmit data to such users are of significant interest. Data broadcast is effective in dissemination-based applications to transfer the data to a large number of users in the asymmetric environment where the downstream communication capacity is relatively much greater than the upstream communication capacity. Index based organization of data transmitted over wireless channels is very important to reduce power consumption. We consider an efficient (1:m) indexing scheme for data broadcast on unreliable wireless networks. We model the data broadcast mechanism on the error prone wireless networks, using the Markov model. We analyze the average access time to obtain the desired data item and find that the optimal index redundancy (m) is SQRT[Data/{Index*(1-p)<sup></sup>K}], where p is the failure rate of the wireless link, Data is the size of the data in a broadcast cycle, Index is the size of index, and K is the index level. We also measure the performance of data broadcast schemes by parametric analysis",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=920471,no,no,1486912025.208297
Fault tolerant XGFT network on chip for multi processor system on chip circuits,"This paper presents a fault-tolerant eXtended Generalized Fat Tree (XGFT) Network-On-Chip (NOC) implemented with a new fault-diagnosis-and-repair (FDAR) system. The FDAR system is able to locate faults and reconfigure switch nodes in such a way that the network can route packets correctly despite the faults. This paper presents how the FDAR finds the faults and reconfigures the switches. Simulation results are used for showing that faulty XGFTs could also achieve good performance, if the FDAR is used. This is possible if deterministic routing is used in faulty parts of the XGFTs and adaptive Turn-Back (TB) routing is used in faultless parts of the network for ensuring good performance and Quality-of-Service (QoS). The XGFT is also equipped with parity bit checks for detecting bit errors from the packets.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515723,no,no,1486912024.5138
The Impact of National Culture and Social Pr esence on Trust and Communication Quality within Collabor ative Groups,"In this empirical study we examine the impact of national culture and social presence on interpersonal trust in both culturally homogeneous and heterogeneous groups. Results demonstrate that interpersonal trust is higher in homogeneous, low-individualism groups (represented by Chinese participants) than that in homogeneous, high-individualism groups (represented by U.S. participants); however, interpersonal trust in heterogeneous groups is lower for low-individualism than high-individualism group members. It is also found that social presence has a positive impact on interpersonal trust; however, a difference in social presence between groups supported by two collaborative technologies is not detected. In addition, perceived communication quality is reported highest in face-to-face (FtF) groups without the support of collaborative software (CS), followed by FtF, CS-supported groups, and then virtual, CS groups. These findings have important implications for trust building in global groups as well as for the design of collaborative technologies in support of virtual groups",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076392,no,no,1486912024.513799
Neural network detection and identification of actuator faults in a pneumatic process control valve,"This paper establishes a scheme for detection and identification of actuator faults in a pneumatic process control valve using neural networks. First, experimental performance parameters related to the valve step responses, including dead time, rise time, overshoot, and the steady state error are obtained directly from a commercially available software package for a variety of faulty operating conditions. Acquiring training data in this way has eliminated the need for additional instrumentation of the valve. Next, the experimentally determined performance parameters are used to train a multilayer perceptron network to detect and identify incorrect supply pressure, actuator vent blockage and diaphragm leakage faults. The scheme presented here is novel in that it demonstrates that a pattern recognition approach to fault detection and identification, for pneumatic process control valves, using features of the valve step response alone, is possible.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1013191,no,no,1486912024.513797
Evaluation of effects of pair work on quality of designs,"Quality is a key issue in the development of software products. Although the literature acknowledges the importance of the design phase of software lifecycle and the effects of the design process and intermediate products on the final product, little progress has been achieved in addressing the quality of designs. This is partly due to difficulties associated in defining quality attributes with precision and measurement of the many different types and styles of design products, as well as problems with assessing the methodologies utilized in the design process. In this research we report on an empirical investigation that we conducted to examine and evaluate quality attributes of design products created through a process of pair-design and solo-design. The process of pair-design methodology involves pair programming principles where two people work together and periodically switch between the roles of driver and navigator. The evaluation of the quality of design products was based on ISO/IEC 9126 standards. Our results show some mixed findings about the effects of pair work on the quality of design products.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402003,no,no,1486912024.513796
"QoS tradeoffs for guidance, navigation, and control","Future space missions will require onboard autonomy to reduce data, plan activities, and react appropriately to complex dynamic events. Software to support such behaviors is computationally-intensive but must execute with sufficient speed to accomplish mission goals. The limited processing resources onboard spacecraft must be split between the new software and required guidance, navigation, control, and communication tasks. To-date, control-related processes have been scheduled with fixed execution period, then autonomy processes are fit into remaining slack time slots. We propose the use of quality-of-service (QoS) negotiation to explicitly trade off the performance of all processing tasks, including those related to spacecraft control. We characterize controller performance based on exhaustive search and a Lyapunov optimization technique and present results that analytically predict worst-case performance degradation characteristics. The results are illustrated by application to a second-order linear system with a linear state feedback control law.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035311,no,no,1486912024.513795
Predicting Architectural Styles from Component Specifications,"Software Product Lines (SPL), Component Based Software Engineering (CBSE) and Commercial Off The Shelf (COTS) components provide a rich supporting base for creating software architectures. Further, they promise significant improvements in the quality of software configurations that can be composed from pre-built components. Software architectural styles provide a way for achieving a desired coherence for such component-based architectures. This is because the different architectural styles enforce different quality attributes for a system. If the architectural style of an emergent system could be predicted in advance, a System Integrator could make necessary changes to ensure that the quality attributes dictated by the system requirements were satisfied before the actual system was deployed and tested. In this paper we propose a model for predicting architectural styles based on use cases that need to be met by a system configuration. Moreover, our technique can be used to determine stylistic conformance and hence indicate the presence or absence of architectural drift",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1620123,no,no,1486912024.513793
Generating wrappers for command line programs: the Cal-Aggie Wrap-O-Matic project,"Software developers writing new software have strong incentives to make their products compliant to standards such as CORBA, COM, and Java Beans. Standards compliance facilitates interoperability, component based software assembly, and software reuse, thus leading to improved quality and productivity. Legacy software, on the other hand, is usually monolithic and hard to maintain and adapt. Many organizations, saddled with entrenched legacy software, are confronted with the need to integrate legacy assets into more modern, distributed, componentized systems that provide critical business services. Thus, wrapping legacy systems for interoperability has been an area of considerable interest. Wrappers are usually constructed by hand which can be costly and error-prone. We specifically target command-line oriented legacy systems and describe a tool framework that automates away some of the drudgery of constructing wrappers for these systems. We describe the Cal-Aggie Wrap-O-Matic system (CAWOM), and illustrate its use to create CORBA wrappers for: a) the JDB debugger, thus supporting distributed debugging using other CORBA components; and b) the Apache Web server, thus allowing remote Web server administration, potentially mediated by CORBA-compliant security services. While CORBA has some limitations, in several relatively common settings it can produce better wrappers at lower cost.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919098,no,no,1486912024.513792
Using Stochastic AI Techniques to Achieve Unbounded Resolution in Finite Player Goore Games and its Applications,"The Goore Game (GG) introduced by M. L. Tsetlin in 1973 has the fascinating property that it can be resolved in a completely distributed manner with no intercommunication between the players. The game has recently found applications in many domains, including the field of sensor networks and quality-of-service (QoS) routing. In actual implementations of the solution, the players are typically replaced by learning automata (LA). The problem with the existing reported approaches is that the accuracy of the solution achieved is intricately related to the number of players participating in the game -which, in turn, determines the resolution. In other words, an arbitrary accuracy can be obtained only if the game has an infinite number of players. In this paper, we show how we can attain an unbounded accuracy for the GG by utilizing no more than three stochastic learning machines, and by recursively pruning the solution space to guarantee that the retained domain contains the solution to the game with a probability as close to unity as desired. The paper also conjectures on how the solution can be applied to some of the application domains",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4219038,no,no,1486912024.51379
WinSURE: a new Windows interface to the SURE program,"WinSURE is new interface to the Semi-Markov Range Evaluator (SURE) program, a reliability analysis program used for calculating upper and lower bounds on the operational and death state probabilities for a large class of semi-Markov models. The SURE program was developed in the late 1980s for the Unix environment and has been distributed free-of-charge for over a decade. The WinSURE program is a port of the SURE program to the Windows 98 operating system, providing the same functionality as the original program, but with a Windows-based graphical user interface. The program provides a rapid computational capability for semi-Markov models useful in describing the fault-handling behavior of fault-tolerant computer systems. The only modeling restriction imposed by the program is that the nonexponential recovery transitions must be fast in comparison to the mission time-a desirable attribute of all fault-tolerant reconfigurable systems. The WinSURE reliability analysis method utilizes a fast bounding theorem based on means and variances that enables the calculation of upper and lower bounds on system reliability. This paper presents an overview of the functionality of the WinSURE program, describe the graphical user interface, and illustrate the use of the program on some simple example problems",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=886873,no,no,1486912024.513789
An infinite server queueing approach for describing software reliability growth: unified modeling and estimation framework,"In general, the software reliability models based on the nonhomogeneous Poisson processes (NHPPs) are quite popular to assess quantitatively the software reliability and its related dependability measures. Nevertheless, it is not so easy to select the best model from a huge number of candidates in the software testing phase, because the predictive performance of software reliability models strongly depends on the fault-detection data. The asymptotic trend of software fault-detection data can be explained by two kinds of NHPP models; finite fault model and infinite fault model. In other words, one needs to make a hypothesis whether the software contains a finite or infinite number of faults, in selecting the software reliability model in advance. In this article, we present an approach to treat both finite and infinite fault models in a unified modeling framework. By introducing an infinite server queueing model to describe the software debugging behavior, we show that it can involve representative NHPP models with a finite and an infinite number of faults. Further, we provide two parameter estimation methods for the unified NHPP based software reliability models from both standpoints of Bayesian and nonBayesian statistics. Numerical examples with real fault-detection data are devoted to compare the infinite server queueing model with the existing one under the same probability circumstance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371911,no,no,1486912024.513786
Dependability modelling of homogeneous and heterogeneous distributed systems,"In the past few years we have developed an experimental distributed system that supports multi-task applications with different levels of criticality. Software implemented fault-tolerant protocols are used to support dependable computing. This paper first presents Markov models of a distributed system under the occurrence of faults, reconfiguration and repair. As a part of our overall project, these models are intended for solving our particular problems, like assessing the merits of redundant schemes, task allocation and reallocation policies, and fault handling used in our experimental system. However, these models are developed in a generic way. They can also be used in evaluating individual task's reliability, risk and availability under various redundant schemes in any homogeneous distributed system. Then, we extend our study in analysing the dependability of the heterogeneous system consisting of a number homogeneous distributed systems connected through gateways",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=917412,no,no,1486912023.817316
Diagnostics using airborne survey and fault location systems as the means to increase OHTL reliability,"Airborne survey application for diagnostics of overhead transmission lines (OHTL) is quite relevant for power utilities of industrialized counties where power network counts thousands or millions km, of which considerable part has reached 30-50 year lifetime and older. Airborne survey based on aerial scanning as a method of OHTL condition monitoring, is efficient instrument of detection of the line elements deviation off regular condition, serves as a convenient facility of network utility inventory. Advantage of aerial scanning is a combination of high survey accuracy with high work productivity. Processing of digital survey data allows to get essential data required for OHTL reliability analysis: precise span lengths, sag and tension values, conductor clearance to ground, crossed and adjacent objects, clearance to vegetation, distance to nearby trees that may damage OHTL if fallen. For analysis of OHTL reliability, existing software packages allow to carry out modeling condition of separate elements and entire line under extreme ice and wind loads, check safety of conductor clearance to ground and crossed lines under condition of significant conductor overheating determined by necessity to ensure transmission under long-term or short-term (but considerable) load increase. Collection, storage, systemizing, practical use of survey data for development and implementing management decisions and rational usage of network resources is reasonable to accomplish with a specialized information system. Information system helps to provide integrate OHTL monitoring data, modules of record and analysis of technical condition of separate components and entire line, 2D and 3D representation of objects with high georeference accuracy. One of negative examples of insufficient OHTL reliability is fault current caused by lightning, conductor or insulator mechanical damage, etc. Duration of OHTL malfunction, timing and success of emergency elimination depends greatly on accuracy of fa- ult location (FL) on line. Advanced FL system allows to locate fault with accuracy of 5 to 150 m. Combined with aerial scanning data and visualizing line section detected by FL system essentially improves efficiency of service technology, emergency recovery of electric network by maintenance crew, and hence increases system reliability of power objects.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4524479,no,no,1486912023.817315
Performance and cost optimization for multiple large-scale grid workflow applications,"Scheduling large-scale applications on the Grid is a fundamental challenge and is critical to application performance and cost. Large-scale applications typically contain a large number of homogeneous and concurrent activities which are main bottlenecks, but open great potentials for optimization. This paper presents a new formulation of the well-known NP-complete problems and two novel algorithms that addresses the problems. The optimization problems are formulated as sequential cooperative games among workflow managers. Experimental results indicate that we have successfully devised and implemented one group of effective, efficient, and feasible approaches. They can produce soultuins of significantly better performance and cost than traditional algorithms. Our algorithms have considerably low time complexity and can assign 1,000,000 activities to 10,000 processors within 0.4 second on one Opteron processor. Moreover, the solutions can be practically performed by workflow managers, and the violation of QoS can be easily detected, which are critical to fault tolerance.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5348835,no,no,1486912023.817313
Performance validation of fault-tolerance software: a compositional approach,"Discusses the lessons learned in the modeling of a software fault tolerance solution built by a consortium of universities and industrial companies for an Esprit project called TIRAN (TaIlorable fault-toleRANce framework for embedded applications). The requirements of high flexibility and modularity for the software have lead to a modeling approach that is strongly based on compositionality. Since the interest was in assessing both the correctness and the performance of the proposed solution, we have cared for these two aspects at the same time, and, by means of an example, we show how this was a central aspect of our analysis.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941422,no,no,1486912023.817312
Reducing Complexity of Software Deployment with Delta Configuration,"Deploying a modern software service usually involves installing several software components, and configuring these components properly to realize the complex interdependencies between them. This process, which accounts for a significant portion of information technology (IT) cost, is complex and error-prone. In this paper, we propose delta configuration - an approach that reduces the cost of software deployment by eliminating a large number of choices on parameter values that administrators have to make during deployment. In delta configuration, the complex software stack of a distributed service is first installed and tested in a test environment. The resulting software images are then captured and used for deployment in production environments. To deploy a software service, we only need to copy these pre-configured software images into a production environment and modify them to account for the difference between the test environment and a production environment. We have implemented a prototype system that achieves software deployment using delta configuration of the configuration state captured inside virtual machines. We perform a case study to demonstrate that our scheme leads to substantial reduction in complexity for the customer, over the traditional software deployment method.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258590,no,no,1486912023.817311
Hardware/Software Design Considerations for Automotive Embedded Systems,"An increasing number of safety-critical functions is taken over by embedded systems in today's automobiles. While standard microcontrollers are the dominant hardware platform in these systems, the decreasing costs of new devices as field programmable gate arrays (FPGAs) make it interesting to consider them for automotive applications. In this paper, a comparison of microcontrollers and FPGAs with respect to safety and reliability properties is presented. For this comparison, hardware fault handling was considered as well as software fault handling. Own empirical evaluations in the area of software fault handling identified advantages of FPGAs with respect to the encapsulation of real-time functions. On the other hand, several dependent failures were detected in versions developed independently on microcontrollers and FPGAs.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4626025,no,no,1486912023.81731
Progress in real-time fault tolerance,"This paper discusses progress in the field of real-time fault tolerance. In particular, it considers synchronous vs. asynchronous fault tolerance designs, maintaining replica consistency, alternative fault tolerance strategies, including checkpoint restoration, transactions, and consistent replay, and custom vs. generic fault tolerance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1353010,no,no,1486912023.817308
Predicting class libraries interface evolution: an investigation into machine learning approaches,"Managing the evolution of an OO system constitutes a complex and resource-consuming task. This is particularly true for reusable class libraries since the user interface must be preserved for version compatibility. Thus, the symptomatic detection of potential instabilities during the design phase of such libraries may help avoid later problems. This paper introduces a fuzzy logic-based approach for evaluating the stability of a reusable class library interface, using structural metrics as stability indicators. To evaluate this new approach, we conducted a preliminary study on a set of commercial C++ class libraries. The obtained results are very promising when compared to those of two classical machine learning approaches, top down induction of decision trees and Bayesian classifiers",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896734,no,no,1486912023.817306
Trends in EM susceptibility of IT equipment,"Information technology equipment and specifically personal computers (PCs) are an essential and integral part of our business and every day lives. Upset or disruption of these systems from intentional or unintentional electromagnetic interference, is untenable, especially if the equipment is used in a security or safety critical application. The susceptibility level for several PCs has been assessed using the mode stirred (reverberation) chamber technique. Results are provided which demonstrate the good repeatability of the method used and trends in the susceptibility level with respect to PC specification, build quality, and batch quality.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1325793,no,no,1486912023.817305
Refactoring for Parameterizing Java Classes,"Type safety and expressiveness of many existing Java libraries and their client applications would improve, if the libraries were upgraded to define generic classes. Efficient and accurate tools exist to assist client applications to use generic libraries, but so far the libraries themselves must be parameterized manually, which is a tedious, time-consuming, and error-prone task. We present a type- constraint-based algorithm for converting non-generic libraries to add type parameters. The algorithm handles the full Java language and preserves backward compatibility, thus making it safe for existing clients. Among other features, it is capable of inferring wildcard types and introducing type parameters for mutually-dependent classes. We have implemented the algorithm as a fully automatic refactoring in Eclipse. We evaluated our work in two ways. First, our tool parameterized code that was lacking type parameters. We contacted the developers of several of these applications, and in all cases they confirmed that the resulting parameterizations were correct and useful. Second, to better quantify its effectiveness, our tool parameterized classes from already-generic libraries, and we compared the results to those that were created by the libraries' authors. Our tool performed the refactoring accurately-in 87% of cases the results were as good as those created manually by a human expert, in 9% of cases the tool results were better, and in 4% of cases the tool results were worse.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222605,no,no,1486912023.817302
Size and Frequency of Class Change from a Refactoring Perspective,"A previous study by Bieman et al., investigated whether large, object-oriented classes were more susceptible to change than smaller classes. The measure of change used in the study was the frequency with which the features of a class had been changed over a specific period of time. From a refactoring perspective, the frequency of class change is of value But even for a relatively simple refactoring such as 'rename method', multiple classes may undergo minor modification without any net increase in class (and system) size. In this paper, we suggest that the combination of 'versions of a class and number of added lines of code ' in the bad code 'smell' detection process may give a better impression of which classes are most suitable candidates for refactoring; as such, effort in detecting bad code smells should apply to classes with a high growth rate as well as a high change frequency. To support our investigation, data relating to changes from 161 Java classes was collected. Results concluded that it is not necessarily the case that large classes are more change-prone than relatively smaller classes. Moreover, the bad code smell detection process is informed by using the combination of change frequency and class size as a heuristic.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383093,no,no,1486912023.125137
Software fault injection for survivability,"In this paper, we present an approach and experimental results from using software fault injection to assess information survivability. We define information survivability to mean the ability of an information system to continue to operate in the presence of faults, anomalous system behavior, or malicious attack. In the past, finding and removing software flaws has traditionally been the realm of software testing. Software testing has largely concerned itself with ensuring that software behaves correctly-an intractable problem for any non-trivial piece of software. In this paper, we present off-nominal testing techniques, which are not concerned with the correctness of the software, but with the survivability of the software in the face of anomalous events and malicious attack. Where software testing is focused on ensuring that the software computes the specified function correctly, we are concerned that the software continues to operate in the presence of faults, unusual system events or malicious attacks",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=821531,no,no,1486912023.125136
A Rate Control Scheme Based on MAD Weighted Model for H.264/AVC,"Under the network bandwidth and the delay constrained, the rate control has become a key technique for video coding in order to obtain consecutive and high quality reconstructive video picture. Rate control scheme of the basic unit in H.264/AVC mainly adopted the linear MAD predict model and the quadratic rate distortion model, in the process of implementing, after coding a macroblock, the parameters of the models will be updated, and then computes the quantization parameters of the current macroblock. So, its computation cost is very high, and its complexity is also very high. Through analyzing the rate control scheme of the H.264/AVC JVT G012rl, the paper proposed an improved low complexity the MAD weighted predict model, and make the accurate rate control in the macroblock layer, and carried out it in the JM98 platform of JVT reference software in H.264/AVC. Extensive experiment results show the complexity of this scheme is lower than the JVT-G012rl of H.264/AVC, and the average PSNR of the usually standard test sequences increased O.lldB, at the same time, its accuracy of the rate control of the QCIF sequences averagely improved 0.498 kpbs.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4340496,no,no,1486912023.125134
Predicting fault incidence using software change history,"This paper is an attempt to understand the processes by which software ages. We define code to be aged or decayed if its structure makes it unnecessarily difficult to understand or change and we measure the extent of decay by counting the number of faults in code in a period of time. Using change management data from a very large, long-lived software system, we explore the extent to which measurements from the change history are successful in predicting the distribution over modules of these incidences of faults. In general, process measures based on the change history are more useful in predicting fault rates than product metrics of the code: For instance, the number of times code has been changed is a better indication of how many faults it will contain than is its length. We also compare the fault rates of code of various ages, finding that if a module is, on the average, a year older than an otherwise similar module, the older module will have roughly a third fewer faults. Our most successful model measures the fault potential of a module as the sum of contributions from all of the times the module has been changed, with large, recent changes receiving the most weight",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=859533,no,no,1486912023.125133
Airborne sensor concept to image shallow-buried targets,This paper develops an airborne sensor concept to detect and image shallow-buried targets with a focus on the remote sensing of landmines. Our ongoing ground-based bistatic ground-penetrating radar (GPR) experiments have demonstrated deep penetration and subwavelength resolution. Simulation software (ground penetrating radar processing-GPRP) was developed and validated using experimental results. Extrapolation of the experimental results to higher frequencies using the simulation software indicates the ability to provide high-quality images of shallow-buried targets.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=999724,no,no,1486912023.125132
Faults in grids: why are they so bad and what can be done about it?,"Computational grids have the potential to become the main execution platform for high performance and distributed applications. However, such systems are extremely complex and prone to failures. We present a survey with the grid community on which several people shared their actual experience regarding fault treatment. The survey reveals that, nowadays, users have to be highly involved in diagnosing failures, that most failures are due to configuration problems (a hint of the area's immaturity), and that solutions for dealing with failures are mainly application-dependent. Going further, we identify two main reasons for this state of affairs. First, grid components that provide high-level abstractions when working, do expose all gory details when broken. Since there are no appropriate mechanisms to deal with the complexity exposed (configuration, middleware, hardware and software issues), users need to be deeply involved in the diagnosis and correction of failures. To address this problem, one needs a way to coordinate different support teams working at the grids different levels of abstraction. Second, fault tolerance schemes today implemented on grids tolerate only crash failures. Since grids are prone to more complex failures, such those caused by heisenbugs, one needs to tolerate tougher failures. Our hope is that the very heterogeneity, that makes a grid a complex environment, can help in the creation of diverse software replicas, a strategy that can tolerate more complex failures.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261694,no,no,1486912023.125131
Optimization of backside micromachined CMOS inductors for RF applications,The quality factor of backside micromachined CMOS inductors is optimized for high frequency applications. Up to 90% improvement of the peak quality factor is predicted for 10 nH inductors according to the simulations over previously published results in this technology. Extensive tests will be performed for the fabricated inductors with an improved post-processing procedure,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857394,no,no,1486912023.125129
Applying Posterior Probability Support Vector Machine to Evaluate the Market Adaptability of the Product of Tourism Agency,The product of the tourism agency can be divided into two classes of the pushing product and the pulling product. It is the very pivotal and significative step of product designing to evaluate the market adaptability of pushing product of the tourism agency. The paper studies the corner of the tour product market and illuminates that the rootstock is insufficient pushing product market adaptability analysis of the tourism agency. The product market adaptability analysis is regarded as a pattern recognition problem of two categories that the product is well adaptability or not the first time. A method based on the posterior probability support vector machine (PPSVM) is applied to evaluate the market adaptability of the product of the tourism agency after comparing the existing analysis method. In the last the technique is proved valid using demonstration analysis and the PPSVM model has the better average prediction accuracy and generality than the other models discussed in literature,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4114511,no,no,1486912023.125127
Predicting Emergent Properties of Component Based Systems,"Software product lines (SPL), component based software engineering (CBSE) and commercial off the shelf (COTS) components provide a rich supporting base for creating software architectures. Further, they promise significant improvements in the quality of software configurations that can be composed from pre-built components. Software architectural styles provide a way for achieving a desired coherence for such component-based architectures. This is because the different architectural styles enforce different quality attributes for a system. If the architectural style of an emergent system could be predicted in advance, the system architect could make necessary changes to ensure that the quality attributes dictated by the system requirements were satisfied before the actual system was deployed. In this paper we propose a model for predicting architectural styles, and hence the quality attributes, based on use cases that need to be satisfied by a system configuration. Our technique can be used to determine stylistic conformance and hence indicate the presence or absence of architectural drift",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127299,no,no,1486912023.125126
High level extraction of SoC architectural information from generic C algorithmic descriptions,"The complexity of nowadays, algorithms in terms of number of lines of codes and cross-relations among processing algorithms that are activated by specific input signals, goes far beyond what the designer can reasonably grasp from the """"pencil and paper"""" analysis of the (software) specifications. Moreover, depending on the implementation goal different measures and metrics are required at different steps of the implementation methodology or design flow of SoC. The process of extracting the desired measures needs to be supported by appropriate automatic tools, since code rewriting, at each design stage, may result resource consuming and error prone. This paper presents an integrated tool for automatic analysis capable of producing complexity results based on rich and customizable metrics. The tool is based on a C virtual machine that allows extracting from any C program execution the operations and data-flow information, according to the defined metrics. The tool capabilities include the simulation of virtual memory architectures.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1530961,no,no,1486912023.125123
Software Library Usage Pattern Extraction Using a Software Model Checker,"The need to manually specify temporal properties of software systems is a major barrier to wider adoption of software model checking, because the specification of software temporal properties is a difficult, time-consuming, and error-prone process. To address this problem, we propose to automatically extract software library usage patterns, which are one type of temporal specifications. Our approach uses a model checker to check a set of software library usage pattern candidates against existing programs using that library, and identifies valid patterns based on model checking results. These valid patterns can help programmers learn about common software library usage. They can also be used to check new programs using the same library. We applied our approach to C programs using the OpenSSL library and the C standard library, and extracted valid usage patterns using BLAST. We also successfully used the extracted valid usage patterns to detect an error in an open source project hosted by SourceForge.net",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019592,no,no,1486912022.438973
"Software Effort, Quality, and Cycle Time: A Study of CMM Level 5 Projects","The Capability Maturity Model (CMM) has become a popular methodology for improving software development processes with the goal of developing high-quality software within budget and planned cycle time. Prior research literature, while not exclusively focusing on CMM level 5 projects, has identified a host of factors as determinants of software development effort, quality, and cycle time. In this study, we focus exclusively on CMM level 5 projects from multiple organizations to study the impacts of highly mature processes on effort, quality, and cycle time. Using a linear regression model based on data collected from 37 CMM level 5 projects of four organizations, we find that high levels of process maturity, as indicated by CMM level 5 rating, reduce the effects of most factors that were previously believed to impact software development effort, quality, and cycle time. The only factor found to be significant in determining effort, cycle time, and quality was software size. On the average, the developed models predicted effort and cycle time around 12 percent and defects to about 49 percent of the actuals, across organizations. Overall, the results in this paper indicate that some of the biggest rewards from high levels of process maturity come from the reduction in variance of software development outcomes that were caused by factors other than software size",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4084133,no,no,1486912022.438971
Industrial real-time regression testing and analysis using firewalls,"Industrial real-time systems are complex and need to be thoroughly tested before being released to the customer. We have found that last minute changes are often responsible for the introduction of defects, causing serious problems for the customer. We demonstrate that these defects can be introduced into real-time software in diverse ways, and there is no simple regression testing method that can deal with all of these defect sources. This paper describes the application of a testing firewall for regression testing whose form differs depending upon the defect. The idea of the testing firewall is to limit the regression testing to those potentially affected system elements directly dependent upon changed system elements, and then to thoroughly test these elements. This has resulted in substantial savings in regression testing costs, and yet has been effective in detecting critical defects with significant implication in terms of customer acceptance at ABB. Empirical studies are reported for these experiences in an industrial setting.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357786,no,no,1486912022.43897
Teaching the process of code review,"Behavioural theory predicts that interventions that improve individual reviewers' expertise also improve the performance of the group in Software Development Technical Reviews (SDTR) [C. Sauer et al.,(2000)]. This includes improvements both in individual's expertise in the review process, as well as their ability to find defects and distinguish true defects from false positives. We present findings from University training in these skills using authentic problems. The first year the course was run it was designed around actual code review sessions, the second year this was expanded to enable students to develop and trial their own generic process for document reviews. This report considers the values and shortcomings of the teaching program from an extensive analysis of the defect detection in the first year, when students were involved in a review process that was set up for them, and student feedback from the second year when students developed and analysed their own process.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1290480,no,no,1486912022.438968
Evaluation of gradient descent learning algorithms with an adaptive local rate technique for hierarchical feedforward architectures,"Gradient descent learning algorithms (namely backpropagation and weight perturbation) can significantly increase their classification performances by adopting a local and adaptive learning rate management approach. We present the results of the comparison of the classification performance of the two algorithms in a tough application: quality control analysis in the steel industry. The feedforward network is hierarchically organized (i.e. tree of multilayer perceptrons). The comparison has been performed starting from the same operating conditions (i.e. network topology, stopping criterion, etc.): the results show that the probability of correct classification is significantly better for the weight perturbation algorithm",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857895,no,no,1486912022.438967
Camera-marker alignment framework and comparison with hand-eye calibration for augmented reality applications,"An integral part of every augmented reality system is the calibration between camera and camera-mounted tracking markers. Accuracy and robustness of the AR overlay process is greatly influenced by the quality of this step. In order to meet the very high precision requirements of medical skill training applications, we have set up a calibration environment based on direct sensing of LED markers. A simulation framework has been developed to predict and study the achievable accuracy of the backprojection needed for the scene augmentation process. We demonstrate that the simulation is in good agreement with experimental results. Even if a slight improvement of the precision has been observed compared to well-known hand-eye calibration methods, the subpixel accuracy required by our application cannot be achieved even when using commercial tracking systems providing marker positions within very low error limits.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544687,no,no,1486912022.438966
Scalable hardware-algorithm for mark-sweep garbage collection,"The memory-intensive nature of object-oriented languages such as C++ and Java has created the need for high-performance dynamic memory management. Object-oriented applications often generate higher memory intensity in the heap region. Thus, a high-performance memory manager is needed to cope with such applications. As today's VLSI technology advances, it becomes increasingly attractive to map software algorithms such as malloc(), free() and garbage collection into hardware. This paper presents a hardware design of a sweeping function (for mark-and-sweep garbage collection) that fully utilizes the advantages of combinational logic. In our scheme, the bit sweep can detect and sweep the garbage in a constant time. Bit-map marking in software can improve the cache performance and reduce number of page faults; however, it often requires several instructions to perform a single mark. In our scheme, only one hardware instruction is required per mark. Moreover, since the complexity of the sweeping phase is often higher than the marking phase, the garbage collection time may be substantially improved. The hardware complexity of the proposed scheme (bit-sweeper) is O(n), where n represents the size of the bit map",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=874643,no,no,1486912022.438964
Improving Coverage in Functional Testing,"Input-predicate/output (IP/O)<sub>n</sub>-chains coverage criterion, originally proposed for black-box testing of telecommunications software, is adapted to white-box testing of programs written in block-structured languages. This criterion is based on the analysis of the effects of inputs on predicates and outputs in a program. It requires that each such effect in a program is examined at least once during testing and thus provides a means of capturing the implemented functionality and checking the consistency of the program with respect to its functional requirements. It is shown that its fault-detecting ability is higher than the all-uses criterion, and compares favorably with the required k-tuples<sup>+</sup> criterion",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032274,no,no,1486912022.438963
The SASHA architecture for network-clustered web servers,"We present the Scalable, Application-Space, Highly-Available (SASHA) architecture for network-clustered web servers that demonstrates high performance and fault tolerance using application-space software and Commercial-Off-The-Shelf (COTS) hardware and operating systems. Our SASHA architecture consists of an application-space dispatcher, which performs OSI layer 4 switching using layer 2 or layer 3 address translation; application-space agents that execute on server nodes to provide the capability for any server node to operate as the dispatcher, a distributed state-reconstruction algorithm; and a token-based communications protocol that supports self-configuring, detecting and adapting to the addition or removal of servers. The SASHA architecture of clustering offers a flexible and cost-effective alternative to kernel-space or hardware-based network-clustered servers with performance comparable to kernel-space implementations",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966817,no,no,1486912022.438961
Architecture-centric software evolution by software metrics and design patterns,"It is shown how software metrics and architectural patterns can be used for the management of software evolution. In the presented architecture-centric software evolution method the quality of a software system is assured in the software design phase by computing various kinds of design metrics from the system architecture, by automatically exploring instances of design patterns and anti-patterns from the architecture, and by reporting potential quality problems to the designers. The same analysis is applied in the implementation phase to the software code, thus ensuring that it matches the quality and structure of the reference architecture. Finally, the quality of the ultimate system is predicted by studying the development history of previous projects with a similar composition of characteristic software metrics and patterns. The architecture-centric software evolution method is supported by two integrated software tools, the metrics and pattern-mining tool Maisa and the reverse-engineering tool Columbus",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995795,no,no,1486912022.438958
Non-FPGA-based Field-programmable Self-repairable (FPSR) Microarchitecture,"A non-FPGA-based adaptable microarchitecture is presented for fault/defect-tolerance. This paper also introduces an architecture-level fault/defect recovery capability implemented in an adaptable architecture with field-programmable self-repair (FPSR). This FPSR scheme that dynamically cures delay/permanent faults and soft-errors detected at circuit- and architecture-level, respectively, is demonstrated as a means to overcome the limitations of circuit-level fault/defect tolerance. The FPSR adaptable architecture was developed without employing reconfigurable devices (e.g., FPGAs). This architecture is adaptable enough to fix errors by reasserting different patterns and delays of the recovering signals via rerouted alternative resources/paths for the same operation without causing the same faults again. In order to dynamically respond to FPSR operations with less redundancy, the adaptable microarchitecture generates and delivers alternative sequences of repair signal patterns via its adaptable architecture; these can be implemented in ASIC, while continuously and seamlessly supporting post-fabrication defect-prevention in both hardware and software at system levels.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4584260,no,no,1486912021.756436
HACKER: human and computer knowledge discovered event rules for telecommunications fault management,"Visualization integrated with data mining can offer 'human-assisted computer discovery' and 'computer-assisted human discovery'. Such a visual environment; reduces the time to understand complex data, thus enabling practical solutions to many real world problem to be developed far more rapidly than either humans or computers operating independently. In doing so the remarkable perceptual abilities that humans possess can be utilized, such as the capacity to recognize lanes quickly, and detect the subtlest changes in size, color, shape, movement or texture. One such complex real world problem is fault management in global telecommunication systems. These system have a large amount of built in redundancy to ensure robustness and quality of service. Unfortunately, this means that when a fault does occur, it can trigger a cascade of alarm events as individual parts of the system discover and report fallen making it difficult to locate the origin of the fault. This alarm behavior has been described as appearing to an operator as non-deterministic, yet it does result in a large data mountain that is ideal for data mining. The paper presents a visualization data mining prototype that incorporates the principles of human and computer discovery, the combination of computer-assisted human discovery with human-assisted computer discovery through a three-tier framework. The prototype is specifically designed to assist in the semi-automatic discovery of previously unknown alarm rules that can then be utilized in commercial role based component solutions, """"business rules"""", which are at the heart of many of todays fault management systems.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1175746,no,no,1486912021.756434
Noise identification with the k-means algorithm,"The presence of noise in a measurement dataset can have a negative effect on the classification model built. More specifically, the noisy instances in the dataset can adversely affect the learnt hypothesis. Removal of noisy instances will improve the learnt hypothesis; thus, improving the classification accuracy of the model. A clustering-based noise detection approach using the k-means algorithm is presented. We present a new metric for measuring the potentiality (noise factor) of an instance being noisy. Based on the computed noise factor values of the instances, the clustering-based algorithm is then used to identify and eliminate p% of the instances in the dataset. These p% of instances are considered the most likely to be noisy among the instances in the dataset - the p% value is varied from 1% to 40%. The noise detection approach is investigated with respect to two case studies of software measurement data obtained from NASA software projects. The two datasets are characterized by the same thirteen software metrics and a class label that classifies the program modules as fault-prone and not fault-prone. It is shown that as more noisy instances are removed, classification accuracy of the C4>5 learner improves. This indicates that the removed instances are most likely noisy instances that attributed to poor classification accuracy.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374211,no,no,1486912021.756433
Applying a Formal Requirements Method to Three NASA Systems: Lessons Learned,"Recently, a formal requirements method called SCR (software cost reduction) was used to specify software requirements of mission-critical components of three NASA systems. The components included a fault protection engine, which determines how a spacecraft should respond to a detected fault; a fault detection, isolation and recovery component, which, in response to an undesirable event, outputs a failure notification and raises one or more alarms; and a display system, which allows a space crew to monitor and control on-orbit scientific experiments. This paper demonstrates how significant and complex requirements of one of the components can be translated into an SCR specification and describes the errors detected when the authors formulated the requirements in SCR. It also discusses lessons learned in using formal methods to document the software requirements of the three components. Based on the authors' experiences, the paper presents several recommendations for improving the quality of requirements specifications of safety-critical aerospace software.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4161594,no,no,1486912021.756432
Proactive maintenance tools for transaction oriented wide area networks,"The motivation of the work presented in this paper comes from a real network management center in charge of supervising a very large hybrid telecommunications/data transaction-oriented network. We present a set of tools that we have developed and implemented in the AT&T Transaction Access Services (TAS) network, in order to automate and facilitate the process of diagnosing network faults and identifying the potentially affected elements, resources and customers. Specifically in this paper we describe the development implementation and use of the following systems: (a) the TAS Information and Tracking System (TIMATS) that provides a common framework for the storage and retrieval of provisioning, capacity management and maintenance data; (b) the Transactions Event Viewer (TEVIEW) system that generates, filters, and presents diagnostic events that indicate system occurrences or conditions that may cause a degradation of the service; and (c) the Transaction Instantaneous Anomaly Notification (TRISTAN) system which implements an adaptive network anomaly detection software that detects network and service anomalies of TAS as dynamically defined violations of the base-lined performance characteristics and profiles",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=830433,no,no,1486912021.75643
New directions in measurement for software quality control,"Assessing and controlling software quality is still an immature discipline. One of the reasons for this is that many of the concepts and terms that are used in discussing and describing quality are overloaded with a history from manufacturing quality. We argue in this paper that a quite distinct approach is needed to software quality control as compared with manufacturing quality control. In particular, the emphasis in software quality control is in design to fulfill business needs, rather than replication to agreed standards. We will describe how quality goals can be derived from business needs. Following that, we will introduce an approach to quality control that uses rich causal models, which can take into account human as well as technological influences. A significant concern of developing such models is the limited sample sizes that are available for eliciting model parameters. In the final section of the paper we will show how expert judgment can be reliably used to elicit parameters in the absence of statistical data. In total this provides an agenda for developing a framework for quality control in software engineering that is freed from the shackles of an inappropriate legacy.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1267623,no,no,1486912021.756429
An Evolution Model for Software Modularity Assessment,"The value of software design modularity largely lies in the ability to accommodate potential changes. Each modularization technique, such as aspect-oriented programming and object-oriented design patterns, provides one way to let some part of a system change independently of all other parts. A modularization technique benefits a design if the potential changes to the design can be well encapsulated by the technique. In general, questions in software evolution, such as which modularization technique is better and whether it is worthwhile to refactor, should be evaluated against potential changes. In this paper, we present a decision-tree-based framework to generally assess design modularization in terms of its changeability. In this framework, we formalize design evolution questions as decision problems, model software designs and potential changes using augmented constraint networks (ACNs), and represent design modular structure before and after envisioned changes using design structure matrices (DSMs) derived from ACNs. We formalize change impacts using an evolution vector to precisely capture well-known informal design principles. As a preliminary evaluation, we use this model to compare the aspect-oriented and object-oriented observer pattern in terms of their ability to accommodate envisioned changes. The results confirm previous published results, but in formal and quantitative ways.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273470,no,no,1486912021.756428
A methodology for constructing maintainability model of object-oriented design,"It is obvious that qualities of software design heavily affects on qualities of software ultimately developed. One of claimed advantages of object-oriented paradigm is the ease of maintenance. The main goal of this work is to propose a methodology for constructing maintainability model of object-oriented software design model using three techniques. Two subcharacteristics of maintainability: understandability and modifiability are focused in this work. A controlled experiment is performed in order to construct maintainability models of object-oriented designs using the experimental data. The first maintainability model is constructed using metrics-discriminant technique. This technique analyzes the pattern of correlation between maintainability levels and structural complexity design metrics applying discriminant analysis. The second one is built using weighted-score-level technique. The technique uses a weighted sum method by combining understandability and modifiability levels which are converted from understandability and modifiability scores. The third one is created using weighted-predicted-level technique. Weighted-predicted-level uses a weighted sum method by combining predicted understandability and modifiability level, obtained from applying understandability and modifiability models. This work presents comparison of maintainability models obtained from three techniques.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357962,no,no,1486912021.756426
Transient stability assessment of an electric power system using trajectory sensitivity analysis,"In this paper it is presented a methodology to assess the transient stability of an electric power system using trajectory sensitivity analysis. This approach studies the variations of the system variables with respect to the small variations in initial conditions and parameters. Trajectory sensitivity functions of the post fault system with respect to relevant parameters are evaluated. The developed technique is combined with an accurate, flexible and efficient hybrid method that allows detailed modelling of the different devices of the power network, the simulation of distinct contingency scenarios, as well as the suitable identification of the different modes of instability. Moreover, it indicates unequivocally the set of the critical machines and allows to easily evaluate the transient stability margin and the critical clearing time.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1492194,no,no,1486912021.756425
Comparison and Assessment of Improved Grey Relation Analysis for Software Development Effort Estimation,"The goal of software project planning is to provide a framework that allows project manager to make reasonable estimates of the resources. In fact, software development is highly unpredictable - only 10% of projects on time and budget. Thus, it is very important for software project managers to accurately and precisely estimate software development effort since the resources are limited. One of the most widely used approaches of software effort estimation is the analogy method. Since the method of analogy is constructed on the foundation of distance-based similarity, there are still some drawbacks and restrictions for application. For example, the anomalistic and outlying values will influence the function to determine similarity. Contrarily, grey relational analysis (GRA) is a distinct measurement from the traditional distance scale and can dig out the realistic law from small-sample data. In this paper, we show how to apply GRA to evaluate the effort estimation results for different data sequences and to compare its accuracy with that of Analogy method. Experimental result shows that the GRA provides a better predictive performance than other methods. We can see that the GRA is more suitable for predicting software development effort with unbalanced dataset",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4037100,no,no,1486912021.756422
A Software Factory for Air Traffic Data,"Modern information systems require a flexible, scalable, and upgradable infrastructure that allows communication, and subsequently collaboration, between heterogeneous information processing and computing environments. Heterogeneous systems often use different data representations for the same data items, limiting collaboration and increasing the cost and complexity of system integration. Although this problem is conceptually straightforward, the process of data conversion is error prone, often dramatically underestimated, and surprisingly complex. The complexity is often the result of the non-standard data representations that are used by computing systems in the aviation domain. This paper describes work that is being done to address this challenge. A prototype software factory for air traffic data is being built and evaluated. The software factory provides the capability to create data and interface models for use in the air traffic domain. The model will allow the user to specify entities such as data items, scaling, units, headers and footers, representation, and coding. The factory automatically creates a machine usable data representation. A prototype for a Domain Specific Language to assist in this task is being developed. This paper describes the scope of the work and the overall approach.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272181,no,no,1486912021.070497
"Filtering, Robust Filtering, Polishing: Techniques for Addressing Quality in Software Data","Data quality is an important aspect of empirical analysis. This paper compares three noise handling methods to assess the benefit of identifying and either filtering or editing problematic instances. We compare a 'do nothing' strategy with (i) filtering, (ii) robust filtering and (Hi) filtering followed by polishing. A problem is that it is not possible to determine whether an instance contains noise unless it has implausible values. Since we cannot determine the true overall noise level we use implausible val.ues as a proxy measure. In addition to the ability to identify implausible values, we use another proxy measure, the ability to fit a classification tree to the data. The interpretation is low misclassification rates imply low noise levels. We found that all three of our data quality techniques improve upon the 'do nothing' strategy, also that the filtering and polishing was the most effective technique for dealing with noise since we eliminated the fewest data and had the lowest misclassification rates. Unfortunately the polishing process introduces new implausible values. We believe consideration of data quality is an important aspect of empirical software engineering. We have shown that for one large and complex real world data set automated techniques can help isolate noisy instances and potentially polish the values to produce better quality data for the analyst. However this work is at a preliminary stage and it assumes that the proxy measures of lity are appropriate.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343737,no,no,1486912021.070495
Evaluation of an efficient control-oriented coverage metric,"Dynamic verification, the use of simulation to determine design correctness, is widely used due to its tractability for large designs. A serious limitation of dynamic techniques is the difficulty in determining whether or not a test sequence is sufficient to detect all likely design errors. Coverage metrics are used to address this problem by providing a set of goals to be achieved during the simulation process; if all coverage goals are satisfied then the test sequence is assumed to be complete. Many coverage metrics have been proposed but no effort has been made to identify a correlation between existing metrics and design quality. In this paper we present a technique to evaluate a coverage metric by examining its ability to ensure the detection of real design errors. We apply our evaluation technique to our control-oriented coverage metric to verify its ability to reveal design errors.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4695895,no,no,1486912021.070494
Consolidating software tools for DNA microarray design and manufacturing,"As the human genome project progresses and some microbial and eukaryotic genomes are recognized, a novel technology, DNA microarray (also called gene chip, biochip, gene microarray, and DNA chip) technology, has attracted increasing number of biologists, bioengineers and computer scientists recently. This technology promises to monitor the whole genome at once, so that researchers can study the whole genome on the global level and have a better picture of the expressions among millions of genes simultaneously. Today, it is widely used in many fields - disease diagnosis, gene classification, gene regulatory network, and drug discovery. We present a concatenated software solution for the entire DNA array flow exploring all steps of a consolidated software tool. The proposed software tool has been tested on Herpes B virus as well as simulated data. Our experiments show that the genomic data follow the pattern predicted by simulated data although the number of border conflicts (quality of the DNA array design) is several times smaller than for simulated data. We also report a trade-off between the number of border conflicts and the running time for several proposed algorithmic techniques employed in the physical design of DNA arrays.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1403119,no,no,1486912021.070492
Validation of mission critical software design and implementation using model checking [spacecraft],"Over the years, the complexity of space missions has dramatically increased with more of the critical aspects of a spacecraft's design being implemented in software. With the added functionality and performance required by the software to meet system requirements, the robustness of the software must be upheld. Traditional software validation methods of simulation and testing are being stretched to adequately cover the needs of software development in this growing environment. It is becoming increasingly difficult to establish traditional software validation practices that confidently confirm the robustness of the design in balance with cost and schedule needs of the project. As a result, model checking is emerging as a powerful validation technique for mission critical software. Model checking conducts an exhaustive exploration of all possible behaviors of a software system design and as such can be used to detect defects in designs that are typically difficult to discover with conventional testing approaches.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1067982,no,no,1486912021.07049
Reliability of fault tolerant control systems: Part I,"The reliability analysis of fault-tolerant control systems is performed using Markov models. Reliability properties peculiar to fault-tolerant control systems are emphasized. As a consequence, coverage of failures through redundancy management can be severely limited. It is shown that in the early life of a system composed of highly reliable subsystems, the reliability of the overall system is affine with respect to coverage, and inadequate coverage induces dominant single point failures. The utility of some existing software tools for assessing the reliability of fault tolerant control systems is also discussed",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981100,no,no,1486912021.070489
Using Online Competitor's Inventory Information for Pricing,"Information displayed on an e-commerce site can be used not just by the intended customers but also by competitors. In the paper, we examine the effect of such proactive information use in the setting of e-commerce retailing where duopoly e-tailers set their prices of a commodity that is in short supply. While e-tailers enhance their service quality by making stockout information available online, that inventory information could also be used by competitors to determine their prices. Each e-tailer can launch software agents to detect its competitor's inventory position and make its price decision contingent on that position. We show that when customer reservation value is relatively high, and e-tailers do not resemble each other in terms of fill rate, both e-tailers choose to adopt the software agent technology and price dynamically at equilibrium. The high availability e-tailer can charge higher prices and enjoy a higher profit level than the low availability e-tailer. More customers prefer to visit the high fill rate e-tailer first under the dynamic pricing scheme than under the static pricing scheme. Because total search costs are reduced, social welfare is improved under the new dynamic pricing scheme",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076803,no,no,1486912021.070487
Online rotor bar breakage detection of three phase induction motors by wavelet packet decomposition and artificial neural network,Online detection algorithm for induction motor rotor bar breakage is presented using a multi-layer perception network (MLP) and wavelet packet decomposition (WPD). New features of rotor bar faults are obtained by wavelet packet decomposition of the stator current. These features are of multiple frequency resolutions and obviously differentiate the healthy and faulty conditions. Features with different frequency resolutions are used together with the speed slip as the input sets of a 4-layer perceptron network. The algorithm is evaluated on a small three-phase induction motor with experiments. The laboratory results show that the proposed method is able to detect the faulty conditions with high accuracy. This algorithm is also applicable to the detection of other electrical faults of induction motors,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=954448,no,no,1486912021.070486
A Probabilistic Approach to Predict Changes in Object-Oriented Software Systems,"Predicting the changes in the next release of a software system has become a quest during its maintenance phase. Such a prediction can help managers to allocate resources more appropriately which results in reducing costs associated with software maintenance activities. A measure of change-proneness of a software system also provides a good understanding of its architectural stability. This research work proposes a novel approach to predict changes in an object oriented software system. The rationale behind this approach is that in a well-designed software system, feature enhancement or corrective maintenance should affect a limited amount of existing code. The goal is to quantify this aspect of quality by assessing the probability that each class will change in a future generation. Our proposed probabilistic approach uses the dependencies obtained from the UML diagrams, as well as other data extracted from source code of several releases of a software system using reverse engineering techniques. The proposed systematic approach has been evaluated on a multi-version medium size open source project namely JFlex, the fast scanner generator for Java. The obtained results indicate the simplicity and accuracy of our approach in the comparison with existing methods in the literature",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145022,no,no,1486912021.070484
Features-Pooling Blind JPEG Image Steganalysis,"In this research, we introduce a new blind steganalysis in detecting grayscale JPEG images. Features-pooling method is employed to extract the steganalytic features and the classification is done by using neural network. Three different steganographic models are tested and classification results are compared to the five state-of-the-art blind steganalysis.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700006,no,no,1486912021.070482
Modeling and Verifying Configuration in Service Deployment,"When deploying a service, software systems required by the service should be configured correctly. However, since software has tens or hundreds of configuration parameters and the parameters of different software may have simple or complex, explicit or implicit dependencies and constraints, to configure multiple software systems in service deployment becomes a difficult, error-prone and time-consuming task. In this paper, we propose a model based approach to automated configuration. Motivated by two real cases found in IBM software products and solutions, our approach has three contributions. Firstly, a meta-model of configuration, called software resource configuration model, is defined for integrating configuration parameters and experiences of different software into a global model. Secondly, a set of configuration rules are designed for verifying incorrect configuration that violates constraints specified by service deployment engineers. Thirdly, a supporting tool, called Comfort, is implemented and evaluated by the motivating cases",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026927,no,no,1486912020.381753
A Middleware Architecture for Replica Voting on Fuzzy Data in Dependable Real-time Systems,"Majority voting among replicated data collection devices enhances the trust-worthiness of data flowing from a hostile external environment. It allows a correct data fusion and dissemination by the end-users, in the presence of content corruptions and/or timing failures that may possibly occur during data collection. In addition, a device may operate on fuzzy inputs, thereby generating a data that occasionally deviates from the reference datum in physical world. In this paper, we provide a QoS-oriented approach to manage the data flow through various system elements. The application-level QoS parameters we consider are timeliness and accuracy of data. The underlying protocol-level parameters that influence data delivery performance are the data sizes, network bandwidth, device asynchrony, and data fuzziness. A replica voting protocol takes into account the interplay between these parameters as the faulty behavior of malicious devices unfolds in various forms during data collection. Our QoS-oriented approach casts the well-known fault-tolerance techniques, namely, 2-phase voting, with control mechanisms that adapt the data delivery to meet the end-to-end constraints - such as latency, data integrity, and resource cost. The paper describes a middleware architecture to realize our QoS-oriented approach to the management of replicated data flows.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4267988,no,no,1486912020.381752
An integrated wired-wireless testbed for distance learning on networking,"This paper addresses a remote testbed for distance learning designed to allow the investigation of various issues related to QoS management in wired/wireless networks used to support real-time applications. Several aspects, such as traffic handling in routers, congestion control and node mobility management, can be experimentally assessed. The testbed comprises various operating modes that the user can select to configure the traffic flows and modify the operational conditions of the network. A peculiarity of the testbed is node mobility support, which allows problems related to handoff and distance to be tackled. The testbed provides for both on-line measurements, through software modules which allow the user to monitor the network while it is operating, and off-line analysis of network behavior through log file inspection",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4152771,no,no,1486912020.381751
SNP Data Consulting Program,"In the post genome era, considerable effort has been put into genetic association study with single nucleotide polymorphisms (SNPs) to investigate genes affecting traits, for example diseases and response to drugs. Although various software tools for SNP association study read plain text files as input data, their formats is not standardized. Manual data conversion may cause incorrect input. In addition, validity of analysis may be lost by experimental fault and by data not under assumption of analysis method. To detect various errors in input data, we implemented 19 rules as SNP data consulting program. The program can also infer first cause of error and then suggest how user should correct it. We demonstrate the program is effective not only for in-house data but also for published data where errors are expected to have been removed. With this program, biologist would be able to perform intended and valid analysis",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4142229,no,no,1486912020.381749
Performance management in component-oriented systems using a Model Driven ArchitectureTM approach,"Developers often lack the time or knowledge to profoundly understand the performance issues in largescale component-oriented enterprise applications. This situation is further complicated by the fact that such applications are often built using a mix of in-house and commercial-off-the-shelf (COTS) components. This paper presents a methodology for understanding and predicting the performance of component-oriented distributed systems both during development and after they have been built. The methodology is based on three conceptually separate parts: monitoring, modelling and performance prediction. Performance predictions are based on UML models created dynamically by monitoring-and-analysing a live or under-development system. The system is monitored using non-intrusive methods and run-time data is collected. In addition, static data is obtained by analysing the deployment configuration of the target application. UML models enhanced with performance indicators are created based on both static and dynamic data, showing performance hot spots. To facilitate the understanding of the system, the generated models are traversable both horizontally at the same abstraction level between transactions, and vertically between different layers of abstraction using the concepts defined by the Model Driven Architecture. The system performance is predicted and performance-related issues are identified in different scenarios by generating workloads and simulating the performance models. Work is under way to implement a framework for the presented methodology with the current focus on the Enterprise Java Beans technology.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137712,no,no,1486912020.381748
Assessing the maintainability benefits of design restructuring using dependency analysis,"Software developers and project managers often have to assess the quality of software design. A commonly adopted hypothesis is that a good design should cost less to maintain than a poor design. We propose a model for quantifying the quality of a design from a maintainability perspective. Based on this model, we propose a novel strategy for predicting the """"return on investment"""" (ROI) for possible design restructurings using procedure level dependency analysis. We demonstrate this approach with two exploratory Java case studies. Our results show that common low level source code transformations change the system dependency structure in a beneficial way, allowing recovery of the initial refactoring investment over a number of maintenance activities.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232477,no,no,1486912020.381747
"Mathematical modeling, performance analysis and simulation of current Ethernet computer networks","This work describes an object oriented software, to be portable among different types of computer platforms, able to perform the simulation of different components of Ethernet local and long distance area networks by using the TCP/IP protocol. It is also able to detect problems in projects and operation of communication networks through the separate or joint analysis of these elements. The functions of the system are as follows: (a) analysis of elements from different layers and protocols of the simulated network (reference to OSI model); (b) analysis and efficiency measurement (quality of transmission, transfer rate, error rate) of the information transmitted on the network; (c) network performance evaluation and link capability analysis; (d) analysis of error detection and further correction capability as well as analysis of network failure tolerance. The software works using mathematical models that represent elements of different layers (reference to OSI model) of the network to be simulated as well as the performance of the abovementioned joint elements.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1032612,no,no,1486912020.381745
Software reliability estimation in grey system theory,"It is necessary for the software system to work in an acceptable degree of reliability and quality. Stochastic modelling, queueing systems and network models, neural networks models, wavelet models, etc are some of interpretative methods to forecast the reliability of software system. But these approaches contain some limitations. It is still not immature for software reliability prediction. A scientific method to predict the software system reliability is presented in the paper. The grey forecasting method is effective in time series data analysis like software reliability in case of lacking information or bad data. We give the design of grey forecasting model for software system reliability. A practical experiment data is given with comparisons to demonstrate the validity of the forecast data and actual data of software reliability from one of software system development in a company to demonstrate the applicability of the proposed method above to forecast the software reliability data.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4443285,no,no,1486912020.381744
Incorporating imperfect debugging into software fault processes,"For the traditional SRGMs, it is assumed that a detected fault is immediately removed and is perfectly repaired with no new faults being introduced. In reality, it is impossible to remove all faults from the fault correction process and have a fault-free effect on the software development environment. In order to relax this perfect debugging assumption, we introduce the possibility of imperfect debugging phenomenon. Furthermore, most of the traditional SRGMs have focused on the failure detection process. Consideration of fault correction process in the existing models is limited. However, to achieve desired level of software quality, it is very important to apply powerful technologies for removing the errors in the fault correction process. Therefore, we divide these processes into different two nonhomogeneous Poisson processes (NHPPs). Moreover, these models are considered to be more practical to depict the fault-removal phenomenon in software development.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1414597,no,no,1486912020.381742
Quantifying the effects of defective block detectors in a 3D whole body pet camera,"A comparison study was conducted in order to assess the image quality of a clinical 3D whole body PET scanner (Siemens Biograph 16 HiRez) in the condition of failure of one or more block detectors. A data set was acquired using the NEMA image quality phantom when all detectors were functioning normally. The ratio of the activity in the four smallest spheres to the background region was 8.27:1. Defective blocks were then simulated by zeroing the appropriate lines of response in the sinograms. Eight different combinations of defects are considered ranging from the case of no defect up to a complete bucket failure (12 blocks). Images were reconstructed with both OSEM and FBP using the manufacturer's software. The images were examined both qualitatively and according to the NEMA NU 2-2001 protocol for contrast, variability, and residual error. The results show that despite very visible artefacts appearing in the images the NEMA contrast analysis was very similar for all defect cases. The variability increased for all cases with simulated defective blocks. The contrast results demonstrate that a method of qualitatively evaluating the images is required in addition to the quantitative analysis. The preliminary data examined in this study suggest that data acquired when there are two defective blocks in the system might still produce clinically useable images.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437057,no,no,1486912020.381739
Availability requirement for fault management server,"In this paper, we examine the availability requirement for the fault management server in high-availability communication systems. According to our study, we find that the availability of the fault management server does not need to be 99.999% in order to guarantee a 99.999% system availability as long as the fail-safe ratio (the probability that the failure of the fault management server will not bring the system down) and the fault coverage ratio (the probability that the failure in the system can be detected and recovered by the fault management server) are sufficiently high. Tradeoffs can be made among the availability of the fault management server, the fail-safe ratio and the fault coverage ratio to optimize system availability. A cost-effective design for the fault management server is proposed in this paper",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960654,no,no,1486912019.693349
Cross-Layer Transmission Scheme with QoS Considerations for Wireless Mesh Networks,"IEEE 802.11 wireless networks utilizes a hard handoff scheme when a station is travelling from one area of coverage to another one within a transmission duration. In IEEE 802.11s wireless mesh networks, the handoff procedure the transmitted data will first be buffered in the source MAP and be not relayed to the target MAP until the handoff procedure is finished. Besides, there are multi-hop in the path between the source station and the destination station. In each pair of neighboring MAPs, contention is needed to transmit data. The latency for successfully transmitting data is seriously lengthened so that the deadlines of data frames are missed with high probabilities. In this paper, we propose a cross-layer transmission (CLT) scheme with QoS considerations for IEEE 802.11 mesh wireless networks. By utilizing CLT, the ratios of missing deadlines will be significantly improved to conform strict time requirements for real-time multimedia applications. We develop a simulation model to investigate the performance of CLT. The capability of the proposed scheme is evaluated by a series of experiments, for which we have encouraging results.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599919,no,no,1486912019.693347
Using Change Propagation Probabilities to Assess Quality Attributes of Software Architectures 1,"<div style=""""font-variant: small-caps; font-size: .9em;"""">First Page of the Article</div><img class=""""img-abs-container"""" style=""""width: 95%; border: 1px solid #808080;"""" src=""""/xploreAssets/images/absImages/01618432.png"""" border=""""0"""">",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1618432,no,no,1486912019.693346
Evaluating low-cost fault-tolerance mechanism for microprocessors on multimedia applications,"We evaluate a low-cost fault-tolerance mechanism for microprocessors, which can detect and recover from transient faults, using multimedia applications. There are two driving forces to study fault-tolerance techniques for microprocessors. One is deep submicron fabrication technologies. Future semiconductor technologies could become more susceptible to alpha particles and other cosmic radiation. The other is the increasing popularity of mobile platforms. Recently cell phones have been used for applications which are critical to our financial security, such as flight ticket reservation, mobile banking, and mobile trading. In such applications, it is expected that computer systems will always work correctly. From these observations, we propose a mechanism which is based on an instruction reissue technique for incorrect data speculation recovery which utilizes time redundancy. Unfortunately, we found significant performance loss when we evaluated the proposal using the SPEC2000 benchmark suite. We evaluate it using MediaBench which contains more practical mobile applications than SPEC2000",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=992702,no,no,1486912019.693344
Modeling Software Contention Using Colored Petri Nets,"Commercial servers, such as database or application servers, often attempt to improve performance via multi-threading. Improper multi-threading architectures can incur contention, limiting performance improvements. Contention occurs primarily at two levels: (1) blocking on locks shared between threads at the software level and (2) contending for physical resources (such as the cpu or disk) at the hardware level. Given a set of hardware resources and an application design, there is an optimal number of threads that maximizes performance. This paper describes a novel technique we developed to select the optimal number of threads of a target-tracking application using a simulation-based colored Petri nets (CPNs) model. This paper makes two contributions to the performance analysis of multi-threaded applications. First, the paper presents an approach for calibrating a simulation model using training set data to reflect actual performance parameters accurately. Second, the model predictions are validated empirically against the actual application performance and the predicted data is used to compute the optimal configuration of threads in an application to achieve the desired performance. Our results show that predicting performance of application thread characteristics is possible and can be used to optimize performance.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4770577,no,no,1486912019.693343
Dependability analysis of a client/server software system with rejuvenation,"Long running software systems are known to experience an aging phenomenon called software aging, one in which the accumulation of errors during the execution of software leads to performance degradation and eventually results in failure. To counteract this phenomenon an active fault management approach, called software rejuvenation, is particularly useful. It essentially involves gracefully terminating an application or a system and restarting it in a clean internal state. We deal with dependability analysis of a client/server software system with rejuvenation. Three dependability measures in the server process, steady-state availability, loss probability of requests and mean response time on tasks, are derived from the well-known hidden Markovian analysis under the time-based software rejuvenation scheme. In numerical examples, we investigate the sensitivity of some model parameters to the dependability measures.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173241,no,no,1486912019.693341
An Efficient Network Anomaly Detection Scheme Based on TCM-KNN Algorithm and Data Reduction Mechanism,"Network anomaly detection plays a vital role in securing network security and infrastructures. Current research focuses concentrate on how to effective reduce high false alarm rate and usually ignore the fact that the poor quality data for the modeling of normal patterns as well as the high computational cost make the current anomaly detection methods not act as well as we expect. Based on these, we first propose a novel data mining scheme for network anomaly detection in this paper. Moreover, we adopt data reduction mechanisms (including genetic algorithm (GA) based instance selection and filter based feature selection methods) to boost the detection performance, meanwhile reduce the computational cost of TCM-KNN. Experimental results on the well-known KDD Cup 1999 dataset demonstrate the proposed method can effectively detect anomalies with high detection rates, low false positives as well as with high confidence than the state-of-the-art anomaly detection methods. Furthermore, the data reduction mechanisms would greatly improve the performance of TCM-KNN and make it be a good candidate for anomaly detection in practice.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4267564,no,no,1486912019.69334
Predicting software suitability using a Bayesian belief network,"The ability to reliably predict the end quality of software under development presents a significant advantage for a development team. It provides an opportunity to address high risk components earlier in the development life cycle, when their impact is minimized. This research proposes a model that captures the evolution of the quality of a software product, and provides reliable forecasts of the end quality of the software being developed in terms of product suitability. Development team skill, software process maturity, and software problem complexity are hypothesized as driving factors of software product quality. The cause-effect relationships between these factors and the elements of software suitability are modeled using Bayesian belief networks, a machine learning method. This research presents a Bayesian network for software quality, and the techniques used to quantify the factors that influence and represent software quality. The developed model is found to be effective in predicting the end product quality of small-scale software development efforts.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607435,no,no,1486912019.693338
Business-Driven Optimization of Policy-Based Management solutions,"We consider whether the off-line compilation of a set of Service Level Agreements (SLAs) into low-level management policies can lead to the runtime maximization of the overall business profit for a service provider. Using a simple Web application hosting SLA template for a utility service provider, we derive low-level QoS management policies and validate their consistency. We show how the default first come first served (FCFS) mechanism for the runtime scheduling of triggered policies fails to deliver an all times maximum business profit for the service provider. To achieve a better business profit, first a penalty/reward model that is derived from the SLA Service Level Objectives (SLOs) is used to assign runtime utility tags to triggered policies. Then three policy scheduling algorithms, which are based on the prediction of the future state of the running SLAs, are used to drive the runtime actions of the Policy Decision Point (PDP). The prediction function per see involved the unsolved problem of predicting in realtime the evolution of the transient state of a variant of an M/M/Ct/Ct queue. A simple approximative solution to the latter problem is provided. Finally, using the VS policy simulator tool, comparative simulation results for the business profit generated by each of the proposed policy scheduling algorithms are presented. VS is a novel tool which we have developed to respond to the increasing need of benchmarking SLA and policy-based management solutions.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258542,no,no,1486912019.693336
SeaWASP: A small waterplane area twin hull autonomous platform for shallow water mapping,"Students with Santa Clara University (SCU) and the Monterey Bay Aquarium Research Institute (MBARI) are developing an innovative platform for shallow water bathymetry. Bathymetry data is used to analyze the geography, ecosystem, and health of marine habitats. However, current methods for shallow water measurements typically involve large, manned vessels. These vessels may pose a danger to themselves and the environment in shallow, semi-navigable waters. Small vessels, however, are prone to disturbance by the waves, tides, and currents of shallow water. The SCU / MBARI autonomous surface vessel (ASV) is designed to operate safely, stably in waters > 1 m and without significant manned support. Final deployment will be at NOAA's Kasitsna Bay Laboratory in Alaska. The ASV utilizes several key design components to provide stability, shallow draft, and long-duration unmanned operations. Bathymetry is measured with a multibeam sonar in concert with DVL and GPS sensors. Pitch, roll, and heave are minimized by a Small Waterplane Area Twin Hull (SWATH) design. The SWATH has a submerged hull, small water-plane area, and high mass to damping ratio, making it less prone to disturbance and ideal for accurate data collection. Precision sensing and actuation is controlled by onboard autonomous algorithms. Autonomous navigation increases the quality of the data collection and reduces the necessity for continuous manning. The vessel has been operated successfully in several open water test environments, including Elkhorn Slough, CA, Steven's Creek, CA, and Lake Tahoe, NV. It is currently is in the final stages of integration and test for its first major science mission at Orcas Island, San Juan Islands, WA, in August, 2008. The Orcas Island deployment will feature design upgrades implemented in Summer, 2008, including additional batteries for all-day power (minimum eight hours), active ballast, real-time data monitoring, updated autonomous control electronics and software, and data- editing using in-house bathymetry mapping software, MB-System. This paper will present the results of the Orcas Island mission and evaluate possible design changes for Alaska. Also, we will include a discussion of our shallow water bathymetry design considerations and a technical overview of the subsystems and previous test results. The ASV has been developed in partnership with Santa Clara University, the Monterey Bay Aquarium Research Institute, the University of Alaska Fairbanks, and NOAA's West Coat and Polar Regions Undersea Research Center.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5347598,no,no,1486912019.693333
Checking inside the black box: regression testing based on value spectra differences,"Comparing behaviors of program versions has become an important task in software maintenance and regression testing. Traditional regression testing strongly focuses on black-box comparison of program outputs. Program spectra have recently been proposed to characterize a program's behavior inside the black box. Comparing program spectra of program versions offers insights into the internal behavior differences between versions. We present a new class of program spectra, value spectra, which enriches the existing program spectra family. We compare the value spectra of an old version and a new version to detect internal behavior deviations in the new version. We use a deviation-propagation call tree to present the deviation details. Based on the deviation-propagation call tree, we propose two heuristics to locate deviation roots, which are program locations that trigger the behavior deviations. We have conducted an experiment on seven C programs to evaluate our approach. The results show that our approach can effectively expose program behavior differences between versions even when their program outputs are the same, and our approach reports deviation roots with high accuracy for most programs.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357787,no,no,1486912019.008389
Legion: Lessons Learned Building a Grid Operating System,"Legion was the first integrated grid middleware architected from first principles to address the complexity of grid environments. Just as a traditional operating system provides an abstract interface to the underlying physical resources of a machine, Legion was designed to provide a powerful virtual machine interface layered over the distributed, heterogeneous, autonomous, and fault-prone physical and logical resources that constitute a grid. We believe that without a solid, integrated, operating system-like grid middleware, grids will fail to cross the chasm from bleeding-edge supercomputing users to more mainstream computing. This work provides an overview of the architectural principles that drove Legion, a high-level description of the system with complete references to more detailed explanations, and the history of Legion from first inception in August 1993 through commercialization. We present a number of important lessons, both technical and sociological, learned during the course of developing and deploying Legion.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1398013,no,no,1486912019.008387
The reliability analysis of thermal design software system,"For the software reliability testing the model of software reliability estimation, which is based on the random Poisson process, has been used, which determines and allows to forecast the software fault probability and its reliability in the set moment of time. The software environment of computer-aided testing for the verification of computational software for the solution of thermal conductivity problems has been developed. Keywords - software reliability,",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4558757,no,no,1486912019.008386
Estimation of parametric sensitivity for defects size distribution in VLSI defect/fault analysis,The parametric sensitivity of defect size distribution in VLSI defect/fault analysis is evaluated. The use of special software tool FIESTA for the computational experiment aimed at estimation of the significance of parameters in expressions approximating the actual defect distribution is considered. The obtained experimental results and their usefulness have been analysed,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003317,no,no,1486912019.008384
Fault Tolerance and Recovery of Scientific Workflows on Computational Grids,"In this paper, we describe the design and implementation of two mechanisms for fault-tolerance and recovery for complex scientific workflows on computational grids. We present our algorithms for over-provisioning and migration, which are our primary strategies for fault-tolerance. We consider application performance models, resource reliability models, network latency and bandwidth and queue wait times for batch-queues on compute resources for determining the correct fault-tolerance strategy. Our goal is to balance reliability and performance in the presence of soft real-time constraints like deadlines and expected success probabilities, and to do it in a way that is transparent to scientists. We have evaluated our strategies by developing a Fault-Tolerance and Recovery (FTR) service and deploying it as a part of the Linked Environments for Atmospheric Discovery (LEAD) production infrastructure. Results from real usage scenarios in LEAD show that the failure rate of individual steps in workflows decreases from about 30% to 5% by using our fault-tolerance strategies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4534303,no,no,1486912019.008383
A validation fault model for timing-induced functional errors,"The violation of timing constraints on signals within a complex system can create timing-induced functional errors which alter the value of output signals. These errors are not detected by traditional functional validation approaches because functional validation does not consider signal timing. Timing-induced functional errors are also not detected by traditional timing analysis approaches because the errors may affect output data values without affecting output signal timing. A timing fault model, the Mis-Timed Event (MTE) fault model, is proposed to model timing-induced functional errors. The MTE fault model formulates timing errors in terms of their effects on the lifespans of the signal values associated with the fault. We use several examples to evaluate the MTE fault model. MTE fault coverage results shows that it efficiently captures an important class of errors which are not targeted by other metrics",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966703,no,no,1486912019.008382
6th ICSE workshop on component-based software engineering: automated reasoning and prediction,"Component-based technologies and processes have been deployed in many organizations and in many fields over the past several years. However, modeling, reasoning about, and predicting component and system properties remains challenging in theory and in practice. CBSE6 builds on previous workshops in the ICSE/CBSE series, and in 2003 is thematically centered on automated composition theories. Composition theories support reasoning about, and predicting, the runtime properties of assemblies of components. Automation is a practical necessity for applying composition theories in practice. Emphasis is placed in this workshop on composition theories that are well founded theoretically, are verifiable or falsifiable, automated by tools, and supported by practical evaluation.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201280,no,no,1486912019.00838
The Role of System Behavior in Diagnostic Performance,"The diagnostic performance of system built-in-test has historically suffered from deficiencies such as high false alarm rates, high undetected failure rates and high fault isolation ambiguity. In general these deficiencies impose a burden on maintenance resources and can affect mission readiness and effectiveness. Part of the problem has to do with the blurred distinction between physical faults and the test failures used to detect those faults. A greater part of the problem has to do with the test limits used to establish pass/fail criteria. If the limits do not reflect system behavior that is to be expected, given its current no fault (or fault) status, then a test fail result can often be a false alarm, and a test pass result can often constitute an undetected fault. A model based approach to prediction of system behavior can do much to alleviate the problem.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4519031,no,no,1486912019.008379
"Software-Based Online Detection of Hardware Defects Mechanisms, Architectural Support, and Evaluation","As silicon process technology scales deeper into the nanometer regime, hardware defects are becoming more common. Such defects are bound to hinder the correct operation of future processor systems, unless new online techniques become available to detect and to tolerate them while preserving the integrity of software applications running on the system. This paper proposes a new, software-based, defect detection and diagnosis technique. We introduce a novel set of instructions, called access-control extension (ACE), that can access and control the microprocessor's internal state. Special firmware periodically suspends microprocessor execution and uses the ACE instructions to run directed tests on the hardware. When a hardware defect is present, these tests can diagnose and locate it, and then activate system repair through resource reconfiguration. The software nature of our framework makes it flexible: testing techniques can be modified/upgraded in the field to trade off performance with reliability without requiring any change to the hardware. We evaluated our technique on a commercial chip-multiprocessor based on Sun's Niagara and found that it can provide very high coverage, with 99.22% of all silicon defects detected. Moreover, our results show that the average performance overhead of software-based testing is only 5.5%. Based on a detailed RTL-level implementation of our technique, we find its area overhead to be quite modest, with only a 5.8% increase in total chip area.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4408248,no,no,1486912019.008378
User's perception of quality of service provision,"The world today is driven by the information exchange providing support for the national and global cooperation. The supporting telecommunications infrastructures are becoming more complex providing the platform for the user driven real-time applications over the large geographical distances. The essential decisions made concerning the state welfare, heath systems, education, business, national security and defence, depend on quality of service provision of telecommunications and data networks. Regardless of the technology supporting the information flows, the final verdict on the quality of service is made by the end user. As a result, it is essential to assess the quality of service provision in the light of user's perception. This article presents a cost effective methodology to assess the user's perception of quality of service provision utilizing the existing Staffordshire University Network.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1290802,no,no,1486912019.008375
Prioritizing Software Inspection Results using Static Profiling,"Static software checking tools are useful as an additional automated software inspection step that can easily be integrated in the development cycle and assist in creating secure, reliable and high quality code. However, an often quoted disadvantage of these tools is that they generate an overly large number of warnings, including many false positives due to the approximate analysis techniques. This information overload effectively limits their usefulness. In this paper we present ELAN, a technique that helps the user prioritize the information generated by a software inspection tool, based on a demand-driven computation of the likelihood that execution reaches the locations for which warnings are reported. This analysis is orthogonal to other prioritization techniques known from literature, such as severity levels and statistical analysis to reduce false positives. We evaluate feasibility of our technique using a number of case studies and assess the quality of our predictions by comparing them to actual values obtained by dynamic profiling.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026864,no,no,1486912018.330887
A Systematic Review of Technical Evaluation in Telemedicine Systems,"We conducted a systematic review of the literature to critically analyse the evaluation and assessment frameworks that have been applied to telemedicine systems. Subjective methods were predominantly used for technical evaluation (59 %), e.g. Likert scale. Those including objective measurements (41%) were restricted to simple metrics such as network time delays. Only three papers included a rigorous standards based objective approach. Our investigation has been unable to determine a definitive standards-based telemedicine evaluation framework that exists in the literature that may be applied systematically to assess and compare telemedicine systems. We conclude that work needs to be done to address this deficiency. We have therefore developed a framework that has been used to evaluate videoconferencing systems telemedicine applications. Our method seeks to be simple to allow relatively inexperienced users to make measurements, is objective and repeatable, is standards based, is inexpensive and requires little specialist equipment. We use the EIA 1956 broadcast test card to assess resolution, grey scale and for astigmatism. Colour discrimination is assessed with the TE 106 and Ishihara 24 colour scale chart. Network protocol analysis software is used to assess network performance (throughput, delay, jitter, packet loss)",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463255,no,no,1486912018.330885
Structure and Interpretation of Computer Programs,"Call graphs depict the static, caller-callee relation between """"functions """" in a program. With most source/target languages supporting functions as the primitive unit of composition, call graphs naturally form the fundamental control flow representation available to understand/develop software. They are also the substrate on which various inter- procedural analyses are performed and are integral part of program comprehension/testing. Given their universality and usefulness, it is imperative to ask if call graphs exhibit any intrinsic graph theoretic features - across versions, program domains and source languages. This work is an attempt to answer these questions: we present and investigate a set of meaningful graph measures that help us understand call graphs better; we establish how these measures correlate, if any, across different languages and program domains; we also assess the overall, language independent software quality by suitably interpreting these measures.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4549888,no,no,1486912018.330884
Coupling Metrics for Predicting Maintainability in Service-Oriented Designs,"Service-oriented computing (SOC) is emerging as a promising paradigm for developing distributed enterprise applications. Although some initial concepts of SOC have been investigated in the research literature, and related technologies are in the process of adoption by an increasing number of enterprises, the ability to measure the structural attributes of service-oriented designs thus predicting the quality of the final software product does not currently exist. Therefore, this paper proposes a set of metrics for quantifying the structural coupling of design artefacts in service-oriented systems. The metrics, which are validated against previously established properties of coupling, are intended to predict the quality characteristic of maintainability of service-oriented software. This is expected to benefit both research and industrial communities as existing object-oriented and procedural metrics are not readily applicable to the implementation of service-oriented systems.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4159685,no,no,1486912018.330883
Exploring the Relationship of a File's History and Its Fault-Proneness: An Empirical Study,"Knowing which particular characteristics of software are indicators for defects is very valuable for testers in order to allocate testing resources appropriately. In this paper, we present the results of an empirical study exploring the relationship between history characteristics of files and their defect count. We analyzed nine open source Java projects across different versions in order to answer the following questions: 1)Do past defects correlate with a filepsilas current defect count? 2) Do late changes correlate with a filepsilas defect count? 3) Is the file's age a good indicator for its defect count? The results are partly surprising. Only 4 of 9 programs show moderate correlation between a file's defects in previous and in current releases in more than the half of analysed releases. In contrast to our expectations, the oldest files represent the most fault-prone files. Additionally, late changes influence filepsilas defect count only partly.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670296,no,no,1486912018.330881
A high-level notation for developing network management applications: resource description and manipulation language,"The increasing size, distribution and heterogeneity of today's networks make their management very costly, inefficient and error prone. We are seeking to replace labor-intensive network management with one that is software-intensive. This paper proposes a high-level notation, Resource Description and Manipulation Language (RDML), for facilitating the development of network management applications. As an important aspect of the Architecture of Self-Management Distributed System (ASMDS), RDML is defined as a purely declarative language and serves to describe Virtual Managed Object Class (VMOC), including its attributes and available methods. VMOC can unite object attributes from different sources aid methods enforced on the object attributes into a virtual managed class. RDML bridges the gap between management applications and management data from variety of resources. Because of the simple syntax and the strong describing ability of RDML it is easy to define VMOC in terms of management goals",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=983516,no,no,1486912018.330879
Investigation of Hyper-NA Scanner Emulation for Photomask CDU Performance,"As the semiconductor industry moves toward immersion lithography using numerical apertures above 1.0 the quality of the photomask becomes even more crucial. Photomask specifications are driven by the critical dimension (CD) metrology within the wafer fab. Knowledge of the CD values at resist level provides a reliable mechanism for the prediction of device performance. Ultimately, tolerances of device electrical properties drive the wafer linewidth specifications of the lithography group. Staying within this budget is influenced mainly by the scanner settings, resist process, and photomask quality. Tightening of photomask specifications is one mechanism for meeting the wafer CD targets. The challenge lies in determining how photomask level metrology results influence wafer level imaging performance. Can it be inferred that photomask level CD performance is the direct contributor to wafer level CD performance? With respect to phase shift masks, criteria such as phase and transmission control are generally tightened with each technology node. Are there other photomask relevant influences that effect wafer CD performance? A comprehensive study is presented supporting the use of scanner emulation based photomask CD metrology to predict wafer level within chip CD uniformity (CDU). Using scanner emulation with the photomask can provide more accurate wafer level prediction because it inherently includes all contributors to image formation related to the 3D topography such as the physical CD, phase, transmission, sidewall angle, surface roughness, and other material properties. Emulated images from different photomask types were captured to provide CD values across chip. Emulated scanner image measurements were completed using an AIMS(TM)45-193i with its hyper-NA, through-pellicle data acquisition capability including the Global CDU Map(TM) software option for AIMS(TM) tools. The through-pellicle data acquisition capability is an essential prerequisite for capturing final CD- - U data (after final clean and pellicle mounting) before the photomask ships or for re-qualification at the wafer fab. Data was also collected on these photomasks using a conventional CD-SEM metrology system with the pellicles removed. A comparison was then made to wafer prints demonstrating the benefit of using scanner emulation based photomask CD metrology.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5760346,no,no,1486912018.330878
Predicting and measuring quality of service for mobile multimedia,We show how an understanding of human perception and the simulation of a mobile IP network may be used to tackle the relevant issues of providing acceptable quality of service for mobile multimedia,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=881578,no,no,1486912018.330876
Winner take all experts network for sensor validation,"The validation of sensor measurements has become an integral part of the operation and control of modern industrial equipment. The sensor under a harsh environment must be shown to consistently provide the correct measurements. Analysis of the validation hardware or software should trigger an alarm when the sensor signals deviate appreciably from the correct values. Neural network based models can be used to estimate critical sensor values when neighboring sensor measurements are used as inputs. The discrepancy between the measured and predicted sensor values may then be used as an indicator for sensor health. The proposed winner take all experts (WTAE) network is based on a `divide and conquer' strategy. It employs a growing fuzzy clustering algorithm to divide a complicated problem into a series of simpler sub-problems and assigns an expert to each of them locally. After the sensor approximation, the outputs from the estimator and the real sensor value are compared both in the time domain and the frequency domain. Three fault indicators are used to provide analytical redundancy to detect the sensor failure. In the decision stage, the intersection of three fuzzy sets accomplishes a decision level fusion, which indicates the confidence level of the sensor health. Two data sets, the Spectra Quest Machinery Fault Simulator data set and the Westland vibration data set, were used in simulations to demonstrate the performance of the proposed WTAE network. The simulation results show the proposed WTAE is competitive with or even superior to the existing approaches",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897405,no,no,1486912018.330874
Quantifying Software Maintainability Based on a Fault-Detection/Correction Model,"The software fault correction profiles play significant roles to assess the quality of software testing as well as to keep the good software maintenance activity. In this paper we develop a quantitative method to evaluate the software maintainability based on a stochastic model. The model proposed here is a queueing model with an infinite number of servers, and is related to the software fault- detection/correction profiles. Based on the familiar maximum likelihood estimation, we estimate quantitatively both the software reliability and maintainability with real project data, and refer to their applicability to the software maintenance practice.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459636,no,no,1486912018.330872
A Fault Detection Mechanism for Service-Oriented Architecture Based on Queueing Theory,"SOA is an ideal solution to application building, since it reuses the existing services as many as possible. The fault tolerance is one important capability to ensure the SOA- based applications are high reliable and available. However, fault tolerance is such a complex issue for most SOA providers that they hardly provide this capability in their products. This paper provides a queuing-theory-based algorithm to fault detection, which can be used to detect the services whose performance becomes unsatisfactory at runtime according to the QoS descriptor. Based on this algorithm, this paper also gives the reference models of the extended service and the architecture of fault-tolerance control center of enterprise services bus for SOA-based applications.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385227,no,no,1486912017.653675
Rule-based noise detection for software measurement data,"The quality of training data is an important issue for classification problems, such as classifying program modules into the fault-prone and not fault-prone groups. The removal of noisy instances will improve data quality, and consequently, performance of the classification model. We present an attractive rule-based noise detection approach, which detects noisy instances based on Boolean rules generated from the measurement data. The proposed approach is evaluated by injecting artificial noise into a clean or noise-free software measurement dataset. The clean dataset is extracted from software measurement data of a NASA software project developed for realtime predictions. The simulated noise is injected into the attributes of the dataset at different noise levels. The number of attributes subjected to noise is also varied for the given dataset. We compare our approach to a classification filter, which considers and eliminates misclassified instances as noisy data. It is shown that for the different noise levels, the proposed approach has better efficiency in detecting noisy instances than the C4.5-based classification filter. In addition, the noise detection performance of our approach increases very rapidly with an increase in the number of attributes corrupted.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431478,no,no,1486912017.653674
Body of knowledge for software quality measurement,"Measuring quality is the key to developing high-quality software. The author describes two approaches that help to identify the body of knowledge software engineers need to achieve this goal. The first approach derives knowledge requirements from a set of issues identified during two standards efforts: the IEEE Std. 1061-1998 for a Software Quality Metrics Methodology and the American National Standard Recommended Practice for Software Reliability (ANSI/AIAA R-013-1992). The second approach ties these knowledge requirements to phases in the software development life cycle. Together, these approaches define a body of knowledge that shows software engineers why and when to measure quality. Focusing on the entire software development life cycle, rather than just the coding phase, gives software engineers the comprehensive knowledge they need to enhance software quality and supports early detection and resolution of quality problems. The integration of product and process measurements lets engineers assess the interactions between them throughout the life cycle. Software engineers can apply this body of knowledge as a guideline for incorporating quality measurement in their projects. Professional licensing and training programs will also find it useful",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=982919,no,no,1486912017.653672
"Software's secret sauce: the """"-ilities"""" [software quality]","If beauty is in the eye of the beholder, then quality must be as well. We live in a world where beauty to one is a complete turnoff to another. Software quality is no different. We have the developer's perspective, the end users perspective, the testers perspective, and so forth. As you can see, meeting the requirements might be different from being fit for a purpose, which can also be different from complying with rules and regulations on how to develop and deploy the software. Yet we can think of all three perspectives as ways to determine how to judge and assess software quality. These three perspectives tie directly to the persistent software attributes focus section in this issue and, consequently, to the concept of software """"-ilities"""". The -ilities (or software attributes) are a collection of closely related behaviors that by themselves have little or no value to the end users but that can greatly increase a software application or system's value when added.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1353217,no,no,1486912017.653671
Comparison of physical and software-implemented fault injection techniques,"This paper addresses the issue of characterizing the respective impact of fault injection techniques. Three physical techniques and one software-implemented technique that have been used to assess the fault tolerance features of the MARS fault-tolerant distributed real-time system are compared and analyzed. After a short summary of the fault tolerance features of the MARS architecture and especially of the error detection mechanisms that were used to compare the erroneous behaviors induced by the fault injection techniques considered, we describe the common distributed testbed and test scenario implemented to perform a coherent set of fault injection campaigns. The main features of the four fault injection techniques considered are then briefly described and the results obtained are finally presented and discussed. Emphasis is put on the analysis of the specific impact and merit of each injection technique.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1228509,no,no,1486912017.65367
Considering Both Failure Detection and Fault Correction Activities in Software Reliability Modeling,"Software reliability is widely recognized as one of the most significant aspects of software quality and is often determined by the number of software uncorrected faults in the system. In practice, it is essential for fault correction prediction, because this correction process consumes a heavy amount of time and resources to predict whether reliability goals have been achieved. Therefore, in this paper we discuss a general framework of the modeling of the failure detection and fault correction process. Under this general framework, we not only verify the existing non-homogeneous poisson process (NHPP) models but also derive several new NHPP models. In addition, we show that these approaches cover a number of well-known models under different conditions. Finally, numerical examples are shown to illustrate the results of the integration of the detection and correction processes",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4142608,no,no,1486912017.653668
Dynamic QoS for commercial VoIP in heterogeneous networks,"The R equation, derived by ITU for network planning that forms part of the E-Model (ITU-T G.107), has been the backbone of quality-of-service (QoS) prediction of telephony for decades and it has been very successful in predicting the Mean Opinion Score (MOS) and QoS for telephone systems. In this paper we question its applicability to Voice-over-IP (VoIP) systems and propose a preliminary VoIP-eM equation and research directions to examine this question. The issues to examine include the linearity of adding the disparate transmission parameters, the random assumption behind the packet loss and interpretation of I<sub>d</sub> and I<sub>e</sub> equation parameters representing impairments due to echo and delay, and impairments from transmission equipment, respectively. Further examination into the combined behavior of packet loss and packet delay, and other impairment scenarios including the environmental factors such as background conditions will also form a part of this investigation. In addition, demographics and languages spoken may show the variability in the R to MOS prediction and will therefore be investigated. The new QoS models will be evaluated for CODEC G.711, G.729, AMR, and GSM-FR. The proposed work will lead towards developing a dynamic QoS monitoring and active control system for the future heterogeneous VoIP networks.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669472,no,no,1486912017.653667
Making an SCI fabric dynamically fault tolerant,"In this paper we present a method for dynamic fault tolerant routing for SCI networks implemented on Dolphin Interconnect Solutions hardware. By dynamic fault tolerance, we mean that the interconnection network reroutes affected packets around a fault, while the rest of the network is fully functional. To the best of our knowledge this is the first reported case of dynamic fault tolerant routing available on commercial off the shelf interconnection network technology without duplicating hardware resources. The development is focused around a 2-D torus topology, and is compatible with the existing hardware, and software stack. We look into the existing mechanisms for routing in SCI. We describe how to make the nodes that detect the faulty component do routing decisions, and what changes are needed in the existing routing to enable support for local rerouting. The new routing algorithm is tested on clusters with real hardware. Our tests show that distributed databases like MySQL can run uninterruptedly while the network reacts to faults. The solution is now part of Dolphin Interconnect Solutions SCI driver, and hardware development to further decrease the reaction time is underway.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536137,no,no,1486912017.653665
SA@Work A Field Study of Software Architecture and Software Quality at Work,"Designing and maintaining a software architecture that strikes the right balance between conflicting quality attributes is a daunting task facing every software architect. In the SA@Work project we have conducted ethnographical field studies of practicing software architects in four Danish software companies to study architectural work in general and architectural techniques in particular. In this paper, we describe observed techniques related to architectural quality as input to the architectural body of knowledge. Second, these techniques are classified according to the quality view classification framework of Garvin. Our analysis shows that techniques for assessing and ensuring quality in software architecture predominately view quality as an intrinsic quality of the architecture itself and less view it as related to business and users. This hints at a need to extend the architectpsilas toolbox and may explain observed mismatches between architectural work and agile processes.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724573,no,no,1486912017.653664
Data embedding in audio signals,"This paper presents results of two methods of embedding digital audio data into another audio signal for secure communication. The data-embedded, or stego, signal is created for transmission by modifying the power spectral density or the phase spectrum of the cover audio at the perceptually masked frequencies in each frame in accordance with the covert audio data. Embedded data in each frame is recovered from the quantized frames of the received stego signal without synchronization or reference to the original cover signal. Using utterances from Texas Instruments Massachusetts Institute of Technology (TIMIT) databases, it was found that error-free data recovery resulted in voiced and unvoiced frames, while high bit-errors occurred in frames containing voiced/unvoiced boundaries. Modifying the phase, in accordance with data, led to higher successful retrieval than modifying the spectral density of the cover audio. In both cases, no difference was detected in perceived speech quality between the cover signal and the received stego signal",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931292,no,no,1486912017.653661
Criteria for developing clinical decision support systems,"The use of archived information and knowledge derived from data-driven system, both at the point of care and retrospectively, is critical to improving the balance between healthcare expenditure and healthcare quality. Data-driven clinical decision support, augmented by performance feedback and education, is a logical addition to consensus- and evidence-based approaches on the path to widespread use of intelligent search agents, expert recognition and warning systems. We believe that these initial applications should (a) capture and archive, with identifiable end-points, complete episode-of-care information for high-complexity, high-cost illnesses, and (b) utilize large numbers of these cases to drive risk-adjusted individualized probabilities for patients requiring care at the time of intervention",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941732,no,no,1486912016.974277
Assessing the Dependability of SOAP RPC-Based Web Services by Fault Injection,"This paper presents our research on devising a dependability assessment method for SOAP-based Web Services using network level fault injection. We compare existing DCE middleware dependability testing research with the requirements of testing SOAP RPC-based applications and derive a new method and fault model for testing web services. From this we have implemented an extendable fault injector framework and undertaken some proof of concept experiments with a system based around Apache SOAP and Apache Tomcat. We also present results from our initial experiments, which uncovered a discrepancy within our system. We finally detail future research, including plans to adapt this fault injector framework from the stateless environment of a standard web service to the stateful environment of an OGSA service.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410959,no,no,1486912016.974276
Finding Causes of Software Failure Using Ridge Regression and Association Rule Generation Methods,"An important challenge in finding latent errors in software is to find predicates which have the most effect on program failure. Since predicates have mutual effects on each other, it is not a good solution to analyze them in isolation, without considering the simultaneous effects of other predicates on failure. The aim is to detect those predicates which are best bug predictors and meanwhile have the least effects among themselves. To achieve this, recursive ridge regression method has been applied. In order to determine the main causes of program failure, the association rule generation is used to detect those predicates which are most often observed with bug predictors in faulty executions. Based on the detected predicates, the faulty paths in control flow graph are introduced to the debugger. Our empirical results on two well-known test suites, EXIF and Siemens imply that the proposed approach could detect main causes of program failure with more accuracy.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617480,no,no,1486912016.974275
"It is software process, stupid: next millennium software quality key","The economic impacts of software in our economy are discussed. The current economic state has been attributed to the Information Technology which has not only generated a large number of jobs but also contributed to the productivity and quality of the operations in our digital economy. The next millennium will be digital and obviously software has a very important role to play in our personal, national, and international scenes. Software will be essential in all our products from Space Station and airplanes to automobiles, television, pagers, and cellular telephones, For any organization, for that matter, even nations to succeed in the digital millennium software will be very essential. One who develops software better, cheaper, and faster will become the leader with enormous economic advantage. The key to success in software development will depend upon the software processes used to build them. Many national and international organizations have developed models based on processes to improve quality, productivity and cycle time of software development. Software Engineering Institute's Capability Maturity Model (CMM) is one such model. CMM helps in assessing the software processes and their capabilities and continuously improve them to higher levels of maturity. Similarly, ISO 9000 and the related ISO 9001 for software quality are based on software processes. This paper will bring out the importance of software processes particularly in managing the quality of software based on national and international initiatives",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=847929,no,no,1486912016.974273
Interactive Software and Hardware Faults Diagnosis Based on Negative Selection Algorithm,"Both hardware and software of computer systems are subject to faults. However, traditional methods, ignoring the relationship between software fault and hardware fault, are ineffective to diagnose complex faults between software and hardware. On the basis of defining the interactive effect to describe the process of the interactive software and hardware fault, this paper present a new matrix-oriented negative selection algorithm to detect faults. Furthermore, the row vector distance and matrix distance are constructed to measure elements between the self set and detector set. The experiment on a temperature control system indicates that the proposed algorithm has good fault detection ability, and the method is applicable to diagnose interactive software and hardware faults with small samples.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4525255,no,no,1486912016.974272
An application of zero-inflated Poisson regression for software fault prediction,"Poisson regression model is widely used in software quality modeling. When the response variable of a data set includes a large number of zeros, Poisson regression model will underestimate the probability of zeros. A zero-inflated model changes the mean structure of the pure Poisson model. The predictive quality is therefore improved. In this paper, we examine a full-scale industrial software system and develop two models, Poisson regression and zero-inflated Poisson regression. To our knowledge, this is the first study that introduces the zero-inflated Poisson regression model in software reliability. Comparing the predictive qualities of the two competing models, we conclude that for this system, the zero-inflated Poisson regression model is more appropriate in theory and practice.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989459,no,no,1486912016.97427
A Multi-Objective Software Quality Classification Model Using Genetic Programming,"A key factor in the success of a software project is achieving the best-possible software reliability within the allotted time & budget. Classification models which provide a risk-based software quality prediction, such as fault-prone & not fault-prone, are effective in providing a focused software quality assurance endeavor. However, their usefulness largely depends on whether all the predicted fault-prone modules can be inspected or improved by the allocated software quality-improvement resources, and on the project-specific costs of misclassifications. Therefore, a practical goal of calibrating classification models is to lower the expected cost of misclassification while providing a cost-effective use of the available software quality-improvement resources. This paper presents a genetic programming-based decision tree model which facilitates a multi-objective optimization in the context of the software quality classification problem. The first objective is to minimize the """"Modified Expected Cost of Misclassification"""", which is our recently proposed goal-oriented measure for selecting & evaluating classification models. The second objective is to optimize the number of predicted fault-prone modules such that it is equal to the number of modules which can be inspected by the allocated resources. Some commonly used classification techniques, such as logistic regression, decision trees, and analogy-based reasoning, are not suited for directly optimizing multi-objective criteria. In contrast, genetic programming is particularly suited for the multi-objective optimization problem. An empirical case study of a real-world industrial software system demonstrates the promising results, and the usefulness of the proposed model",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4220788,no,no,1486912016.974269
Incremental fault-tolerant design in an object-oriented setting,"With the increasing emphasis on dependability in complex, distributed systems, it is essential that system development can be done gradually and at different levels of detail. We propose an incremental treatment of faults as a refinement process on object-oriented system specifications. An intolerant system specification is a natural abstraction from which a fault-tolerant system can evolve. With each refinement step a fault and its treatment are introduced, so the fault-tolerance of the system increases during the design process. Different kinds of faults are identified and captured by separate refinement relations according to how the tolerant system relates to abstract properties of the intolerant one in terms of safety, and liveness. The specification language utilized is object-oriented and based upon first-order predicates on communication traces. Fault-tolerance refinement relations are formalized within this framework",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990023,no,no,1486912016.974268
Dynamic reconfiguration for Java applications using AOP,"One of the characteristics of contemporary software systems is their ability to adapt to evolutionary computing needs and environments. Dynamic reconfiguration is a way to make changes to software systems at runtime, while typical software changes involve the shutting down and rebooting of software systems. Therefore, dynamic reconfiguration can provide continuous availability for software systems. However, its processes are still complicated and error- prone due to the intervention of human beings. This research describes an aspect-oriented approach to dynamic reconfiguration for Java applications, providing software maintainers with systematic and controlled reconfiguration processes. The features of aspect-oriented programming systems, such as aspect weaving and code instrumentation, are appropriate to the problems of dynamic reconfiguration. This proposed approach is intended to minimize the efforts of software engineers, to enable automated dynamic reconfiguration, and to ensure the integrity of software systems. The primary domain of the research is component-based applications where the addition, removal, and replacement of components might be needed.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4494287,no,no,1486912016.974266
Software product improvement with inspection. A large-scale experiment on the influence of inspection processes on defect detection in software requirements documents,"In the early stages of software development, inspection of software documents is the most effective quality assurance measure to detect defects and provides timely feedback on quality to developers and managers. The paper reports on a controlled experiment that investigates the effect of defect detection techniques on software product and inspection process quality. The experiment compares defect detection effectiveness and efficiency of a general reading technique that uses checklist based reading, and a systematic reading technique, scenario based reading, for requirements documents. On the individual level, effectiveness was found to be higher for the general reading technique, while the focus of the systematic reading technique led to a higher yield of severe defects compared to the general reading technique. On a group level, which combined inspectors' contributions, the advantage of a reading technique regarding defect detection effectiveness depended on the size of the group, while the systematic reading technique generally exhibited better defect detection efficiency",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=874427,no,no,1486912016.974263
Network management agent allocation scheme in mesh networks,"In this letter, we propose a scheme for constructing a reliable alarm detection structure in the communication networks. We investigate how to allocate a minimal set of management agents and still cover all alarms in mesh networks under the assumption that alarms are delivered along provisioned paths. We also consider the probabilistic nature of alarm loss and propose an efficient scheme for allocating a minimal set of agents while keeping the overall alarm loss probability below a threshold.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254067,no,no,1486912016.298478
A Coverage-Based Handover Algorithm for High-speed Data Service,"4G supports various types of services. The coverage of high-speed data service is smaller than that of low-speed data service, which make the high-speed data service users occur dropping before reaching the handover area. In order to solve the problem, this paper proposes A Coverage-based Handover Algorithm for High-speed Data Service (CBH), which extends the coverage of high-speed data service by reducing source rate and makes these users acquire """";transient coverage"""";. Meanwhile, FSES (Faint Sub-carrier Elimination Strategy) is introduced, which utilizes the """";transient handover QoS""""; and reduces the handover effect to target cell for high-speed data service users. The simulation results shows that the new algorithm can improve the whole system performance, reduce the handover dropping probability and new call blocking probability, enhance the resource utilization ratio.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4489556,no,no,1486912016.298476
Predicting for MTBF Failure Data Series of Software Reliability by Genetic Programming Algorithm,"At present, most of software reliability models have to build on certain presuppositions about software fault process, which also brings on the incongruence of software reliability models application. To solve these problems and cast off traditional models' multi-subjective assumptions, this paper adopts genetic programming (GP) evolution algorithm to establishing software reliability model based on mean time between failures' (MTBF) time series. The evolution model of GP is then analyzed and appraised according to five characteristic criteria for some common-used software testing cases. Meanwhile, we also select some traditional probability models and the neural network model to compare with the new GP model separately. The result testifies that the new model evolved by GP has the higher prediction precision and better applicability, which can improve the applicable inconsistency of software reliability modeling to some extent",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021519,no,no,1486912016.298475
"Monitoring and Improving the Quality of ODC Data using the """"ODC Harmony Matrices"""": A Case Study","Orthogonal defect classification (ODC) is an advanced software engineering technique to provide in-process feedback to developers and testers using defect data. ODC institutionalization in a large organization involves some challenging roadblocks such as the poor quality of the collected data leading to wrong analysis. In this paper, we have proposed a technique ('Harmony Matrix') to improve the data collection process. The ODC Harmony Matrix has useful applications. At the individual defect level, results can be used to raise alerts to practitioners at the point of data collection if a low probability combination is chosen. At the higher level, the ODC Harmony Matrix helps in monitoring the quality of the collected ODC data. The ODC Harmony Matrix complements other approaches to monitor and enhances the ODC data collection process and helps in successful ODC institutionalization, ultimately improving both the product and the process. The paper also describes precautions to take while using this approach",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691408,no,no,1486912016.298473
A routing methodology for achieving fault tolerance in direct networks,"Massively parallel computing systems are being built with thousands of nodes. The interconnection network plays a key role for the performance of such systems. However, the high number of components significantly increases the probability of failure. Additionally, failures in the interconnection network may isolate a large fraction of the machine. It is therefore critical to provide an efficient fault-tolerant mechanism to keep the system running, even in the presence of faults. This paper presents a new fault-tolerant routing methodology that does not degrade performance in the absence of faults and tolerates a reasonably large number of faults without disabling any healthy node. In order to avoid faults, for some source-destination pairs, packets are first sent to an intermediate node and then from this node to the destination node. Fully adaptive routing is used along both subpaths. The methodology assumes a static fault model and the use of a checkpoint/restart mechanism. However, there are scenarios where the faults cannot be avoided solely by using an intermediate node. Thus, we also provide some extensions to the methodology. Specifically, we propose disabling adaptive routing and/or using misrouting on a per-packet basis. We also propose the use of more than one intermediate node for some paths. The proposed fault-tolerant routing methodology is extensively evaluated in terms of fault tolerance, complexity, and performance.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608003,no,no,1486912016.298472
Constructing a Reading Guide for Software Product Audits,"Architectural knowledge is reflected in various artifacts of a software product. In the case of a software product audit this architectural knowledge needs to be uncovered and its effects assessed, in order to evaluate the quality of the software product. A particular problem is to find and comprehend the architectural knowledge that resides in the software product documentation. The amount of documents, and the differences in for instance target audience and level of abstraction, make it a difficult job for the auditors to find their way through the documentation. This paper discusses how the use of a technique called latent semantic analysis can guide the auditors through the documentation to the architectural knowledge they need. Using latent semantic analysis, we effectively construct a reading guide for software product audits.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077028,no,no,1486912016.29847
"Extend the meaning of """"R"""" to """"R<sup>4</sup>"""" in ART (automated software regression technology) to improve quality and reduce R&D and production costs","Regression testing has been conventionally employed to check the effectiveness of a solution, track existing issues and any new issues created by the result of fixing the old issues. Positioned at the tail end of the software cycle, regression testing technology can hardly influence or contribute to earlier phases such as architect, design, implementation or device testing. Extending the """"R"""" in ART to R<sup>4</sup> (regression, research, retain & grow expertise and early exposure) has been proving. R<sup>4</sup> is not only providing ART with more powerful tools to detect issues as early as in the architect phase, but also arming R&D software with more proactive practices to avoid costly catastrophic problems from propagating to customer sites. This paper attempts to share some best practices and contributions from Cisco-ARF (a Cisco automated regression/research facility) whose charter is to ensure the quality of product lines running on tens of million lines of code. These award-winning practices have proven to save multi-million dollars in repair costs, thousands of engineering hours, and continue to set the higher standards for testing technology under proactive leadership and management to gain higher quality and customer satisfaction.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407078,no,no,1486912016.298469
Procedure call duplication: minimization of energy consumption with constrained error detection latency,"This paper presents a new software technique for detecting transient hardware errors. The objective is to guarantee data integrity in the presence of transient errors and to minimize energy consumption at the same time. Basically, we duplicate computations and compare their results to detect errors. There are three choices for duplicate computations: (1) duplicating every statement in the program and comparing their results, (2) re-executing procedures with duplicated procedure calls and comparing the results, (3) re-executing the whole program and comparing the final results. Our technique is the combination of (1) and (2): Given a program, our technique analyzes procedure call behavior of the program and determines which procedures should have duplicated statements (choice (1)) and which procedure calls should be duplicated (choice (2)) to minimize energy consumption while controlling error detection latency constraints. Then, our technique transforms the original program into the program that is able to detect errors with reduced energy consumption by re-executing the statements or procedures. In benchmark program simulation, we found that our technique saves over 25% of the required energy on average compared to previous techniques that do not take energy consumption into consideration",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966768,no,no,1486912016.298468
ACAR: Adaptive Connectivity Aware Routing Protocol for Vehicular Ad Hoc Networks,"Developing routing protocol for vehicular ad hoc networks (VANET) is a challenging task due to potentially large network sizes, rapidly changing topology and frequent network disconnections, which can cause failure or inefficiency in traditional ad hoc routing protocols. We propose an adaptive connectivity aware routing (ACAR) protocol that addresses these problems by adaptively selecting an optimal route with the best network transmission quality based on the statistical and realtime density data that are gathered through an on-the-fly density collection process. The protocol consists of two parts: (1) select an optimal route, consisting of road segments, with the best estimated transmission quality (2) in each road segment in the selected route, select the most efficient multi-hop path that will improve delivery ratio and throughput. The optimal route can be selected using our new connectivity model that takes into account vehicles densities and traffic light periods to estimate transmission quality at road segments, which considers the probability of connectivity and data delivery ratio for transmitting packets. In each road segment along the optimal path, each hop is selected to minimize the packet error rate of the entire path. Our simulation results show that the proposed ACAR protocol outperforms existing VANET routing protocols in terms of data delivery ratio, throughput and data packet delay. In addition, ACAR works very well even if accurate statistical data is not available.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4674267,no,no,1486912016.298466
How good is your blind spot sampling policy,"Assessing software costs money and better assessment costs exponentially more money. Given finite budgets, assessment resources are typically skewed towards areas that are believed to be mission critical. This leaves blind spots: portions of the system that may contain defects which may be missed. Therefore, in addition to rigorously assessing mission critical areas, a parallel activity should sample the blind spots. This paper assesses defect detectors based on static code measures as a blind spot sampling method. In contrast to previous results, we find that such defect detectors yield results that are stable across many applications. Further, these detectors are inexpensive to use and can be tuned to the specifics of the current business situations.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281737,no,no,1486912016.298463
Delivering packets during the routing convergence latency interval through highly connected detours,"Routing protocols present a convergence latency for all routers to update their tables after a fault occurs and the network topology changes. During this time interval, which in the Internet has been shown to be of up to minutes, packets may be lost before reaching their destinations. In order to allow nodes to continue communicating during the convergence latency interval, we propose the use of alternative routes called detours. In this work we introduce new criteria for selecting detours based on network connectivity. Detours are chosen without the knowledge of which node or link is faulty. Highly connected components present a larger number of distinct paths, thus increasing the probability that the detour will work correctly. Experimental results were obtained with simulation on random Internet-like graphs generated with the Waxman method. Results show that the fault coverage obtained through the usage of the best detour is up to 90%. When the three best detours are considered, the fault coverage is up to 98%.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311919,no,no,1486912015.624425
Real-Time Model-Based Fault Detection and Diagnosis for Alternators and Induction Motors,"This paper describes a real-time model-based fault detection and diagnosis software. The electric machines diagnosis system (EMDS) covers field winding shorted-turns fault in alternators and stator windings shorted-turns fault in induction motors. The EMDS has a modular architecture. The modules include: acquisition and data treatment; well-known parameters estimation algorithms, such as recursive least squares (RLS) and extended Kalman filter (EKF); dynamic models for faults simulation; faults detection and identification tools, such as M.L.P. and S.O.M. neural networks and fuzzy C-means (FCM) technique. The modules working together detect possible faulty conditions of various machines working in parallel through routing. A fast, safe and efficient data manipulation requires a great DataBase managing system (DBMS) performance. In our experiment, the EMDS real-time operation demonstrated that the proposed system could efficiently and effectively detect abnormal conditions resulting in lower-cost maintenance for the company.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270639,no,no,1486912015.624423
Algorithm-based fault tolerance for spaceborne computing: basis and implementations,"We describe and test the mathematical background for using checksum methods to validate results returned by a numerical subroutine operating in a fault-prone environment that causes unpredictable errors in data. We can treat subroutines whose results satisfy a necessary condition of a linear form; the checksum tests compliance with this necessary condition. These checksum schemes are called algorithm-based fault tolerance (ABFT). We discuss the theory and practice of setting numerical tolerances to separate errors caused by a fault from those inherent in finite-precision numerical calculations. Two series of tests are described. The first tests the general effectiveness of the linear ABFT schemes we propose, and the second verifies the correct behavior of our parallel implementation of them. We find that under simulated fault conditions, it is possible to choose a fault detection scheme that for average case matrices can detect 99% of faults with no false alarms, and that for a worst-case matrix population can detect 80% of faults with no false alarms",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=878453,no,no,1486912015.624422
A data collection scheme for reliability evaluation and assessment-a practical case in Iran,"Data collection is an essential element of reliability assessment and many utilities throughout the world have established comprehensive procedures for assessing the performance of their electric power systems. Data collection is also a constituent part of quantitative power system reliability assessment in which system past performance and prediction of future performance are evaluated. This paper presents an overview of the Iran electric power system data collection scheme and the procedure to its reliability analysis. The scheme contains both equipment reliability data collection procedure and structure of reliability assessment. The former constitutes generation, transmission and distribution equipment data. The latter contains past performance and predictive future performance of the Iran power system. The benefits of this powerful data base within an environment of change and uncertainty will help utilities to keep down cost, while meeting the multiple challenges of providing high degrees of reliability and power quality of electrical energy.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1460147,no,no,1486912015.62442
Detection strategies: metrics-based rules for detecting design flaws,"In order to support the maintenance of an object-oriented software system, the quality of its design must be evaluated using adequate quantification means. In spite of the current extensive use of metrics, if used in isolation metrics are oftentimes too fine grained to quantify comprehensively an investigated design aspect (e.g., distribution of system's intelligence among classes). To help developers and maintainers detect and localize design problems in a system, we propose a novel mechanism - called detection strategy - for formulating metrics-based rules that capture deviations from good design principles and heuristics. Using detection strategies an engineer can directly localize classes or methods affected by a particular design flaw (e.g., God Class), rather than having to infer the real design problem from a large set of abnormal metric values. We have defined such detection strategies for capturing around ten important flaws of object-oriented design found in the literature and validated the approach experimentally on multiple large-scale case-studies.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357820,no,no,1486912015.624419
A monitoring sensor management system for grid environments,"Large distributed systems, such as computational grids, require a large amount of monitoring data be collected for a variety of tasks, such as fault detection, performance analysis, performance tuning, performance prediction and scheduling. Ensuring that all necessary monitoring is turned on and that the data is being collected can be a very tedious and error-prone task. We have developed an agent-based system to automate the execution of monitoring sensors and the collection of event data",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=868639,no,no,1486912015.624418
Effect of preventive rejuvenation in communication network system with burst arrival,"Long running software systems are known to experience an aging phenomenon called software aging, one in which the accumulation of errors during the execution of software leads to performance degradation and eventually results in failure. To counteract this phenomenon a proactive fault management approach, called software rejuvenation, is particularly useful. It essentially involves gracefully terminating an application or a system and restarting it in a clean internal state. In this paper, we perform the dependability analysis of a client/server software system with rejuvenation under the assumption that the requests arrive according to the Markov modulated Poisson process. Three dependability measures, steady-state availability, loss probability of requests and mean response time on tasks, are derived through the hidden Markovian analysis based on the time-based software rejuvenation scheme. In numerical examples, we investigate the sensitivity of some model parameters to the dependability measures.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1452041,no,no,1486912015.624416
Establishing software product quality requirements according to international standards,"Software product quality is an important concern in the computer environment and whose immediate results are appreciated in all the activities where computers are used. The ISO/IEC 9126 standard series settle a software product quality model, for example, in the annex, shows the identification of the quality requirements like a necessary step for product quality. However, the standard does not included the way to get quality requirements, neither how to establish metrics levels. Establishing quality requirements and metric levels seems to be simple activities but they could be annoying and prone to errors if there is not a systematic approach for the process. This article presents a proposal for establishing product quality requirements according to the ISO/IEC 9126 standard.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642457,no,no,1486912015.624414
Rapid 3D Transesophageal Echocardiography using a fast-rotating multiplane transducer,"3D transesophageal echocardiography (3D TEE) with acquisition gating for electrocardiogram (ECG) and respiration is slow, cumbersome for the patient and prone to motion artifacts. We realized a rapid 3D TEE solution based on a standard multiplane TEE probe, extended with a fast-rotating transducer array (FR-TEE). The fast left-right rotation allows acquisition of sufficient image data from the entire rotation range for the full heart cycle within one breath-hold. No ECG- or respiration-gating is applied. In normal mode, the probe has uncompromised optimal 2D quality. 10 seconds of image data with ECG and angle values are recorded and post-processed with specially developed 4D reconstruction software based on normalized convolution interpolation. High quality 3D images of phantoms were acquired, accurately depicting the imaged objects. Sequences of reconstructed 3D volumes of a cyclic moving (4D) balloon phantom show only minimal temporal artifacts. Preliminary results on 5 open-chest pigs and 3 humans showed the overall anatomy as well as valvular details with good diagnostic accuracy and high temporal and spatial resolution. A bicuspid aortic valve was diagnosed from the 3D reconstructions and confirmed by a separate 2D exam, proving the 3D diagnostic capabilities of FR-TEE.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803166,no,no,1486912015.624413
Software review for automatic test equipment,"The nature of test set programming can be tedious and repetitive. A test engineer can often fall victim to puffing blinders on when programming by overlooking errors when reviewing their own work. To avoid this, it makes sense to treat software like a published work where a reviewer, independent of the original programming team, checks the software for design, quality, and errors. This paper describes a disciplined and consistent process for reviewing Automatic Test Equipment (ATE) software. This type of independent review process is comprised of four major steps: Receiving, Processing, Reporting, and Following-Up. It can be conducted and repeated throughout the development life cycle to improve the quality of the software. Early involvement can influence design changes that could lead to simpler and more manageable software. Several errors can be detected prior to its release by reviewing the software with software tools such as PC-LintTM or Understand for C++TM. Having the discipline to follow this simple process can bring about software manageability for future modifications, easier to read software, and software that contains fewer errors.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609096,no,no,1486912015.62441
SPDW: A Software Development Process Performance Data Warehousing Environment,"Metrics are essential in the assessment of the quality of software development processes (SDP). However, the adoption of a metrics program requires an information system for collecting, analyzing, and disseminating measures of software processes, products and services. This paper describes SPDW, an SPD data warehousing environment developed in the context of the metrics program of a leading software operation in Latin America, currently assessed as CMM Level 3. SDPW architecture encompasses: 1) automatic project data capturing, considering different types of heterogeneity present in the software development environment; 2) the representation of project metrics according to a standard organizational view; and 3) analytical functionality that supports process analysis. The paper also describes current implementations, and reports experiences on the use of SPDW by the organization",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4090251,no,no,1486912014.954783
Studying effect of location and resistance of inter-turn faults on fault current in power transformers,"Inter-turn (turn-to-turn) fault is one of the most important failures which could occur in power transformers. This phenomenon could seriously reduce the useful life length of transformers. Meanwhile, transformer protection schemes such as differential relays are not able to detect this kind of fault. This type of fault should be studied carefully to determine its features and characteristics. In this paper the effect of fault location and fault resistance on the amplitude of fault current is studied. It is found that change of fault location along the winding has considerable effect on fault current amplitude. It would also be shown that, even small fault resistance could have major effect on fault current amplitude. In this paper, a real 240/11 kV, 27 MVA transformer is used for simulation studies.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4468934,no,no,1486912014.954782
Two controlled experiments assessing the usefulness of design pattern documentation in program maintenance,Using design patterns is claimed to improve programmer productivity and software quality. Such improvements may manifest both at construction time (in faster and better program design) and at maintenance time (in faster and more accurate program comprehension). The paper focuses on the maintenance context and reports on experimental tests of the following question: does it help the maintainer if the design patterns in the program code are documented explicitly (using source code comments) compared to a well-commented program without explicit reference to design patterns? Subjects performed maintenance tasks on two programs ranging from 360 to 560 LOC including comments. The experiments tested whether pattern comment lines (PCL) help during maintenance if patterns are relevant and sufficient program comments are already present. This question is a challenge for the experimental methodology: A setup leading to relevant results is quite difficult to find. We discuss these issues in detail and suggest a general approach to such situations. A conservative analysis of the results supports the hypothesis that pattern-relevant maintenance tasks were completed faster or with fewer errors if redundant design pattern information was provided. The article provides the first controlled experiment results on design pattern usage and it presents a solution approach to an important class of experiment design problems for experiments regarding documentation,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1010061,no,no,1486912014.954781
A predictive QoS control strategy for wireless sensor networks,"The number of active sensors in a wireless sensor network has been proposed as a measure, albeit limited, for quality of service (QoS) for it dictates the spatial resolution of the sensed parameters. In very large sensor network applications, the number of sensor nodes deployed may exceed the number required to provide the desired resolution. Herein we propose a method, dubbed predictive QoS control (PQC), to manage the number of active sensors in such an over-deployed network. The strategy is shown to obtain near lifetime and variance performance in comparison to a Bernoulli benchmark, with the added benefit of not requiring the network to know the total number of sensors available. This benefit is especially relevant in networks where sensors are prone to failure due to not only energy exhaustion but also environmental factors and/or those networks where nodes are replenished over time. The method also has advantages in that only transmitting sensors need to listen for QoS control information and thus enabling inactive sensors to operate at extremely low power levels",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542810,no,no,1486912014.95478
Bivariate Software Fault-Detection Models,"In this paper, we develop bivariate software fault-detection models with two time measures: calendar time (day) and test-execution time (CPU time) and incorporate both of them to assess the quantitative software reliability with higher accuracy. The resulting stochastic models are characterized by a simple binomial process and the bivariate order statistics of software fault-detection times with different time scales.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291048,no,no,1486912014.954778
New generator split-phase transverse differential protection based on wavelet transform,"This paper presents a new split-phase transverse differential protection for a large generator based on wavelet transform. Research results show that there is almost no harmonic component on normal conditions, but it will produce great high-frequency current component when an internal fault occurs, which can be used to detect generator internal fault. With decomposition and reconstruction of the transient currents with wavelet transform, the high-frequency band fault currents are exploited in the new scheme. And the realization of the proposed protection device is also described in this paper, including the relay software and hardware design. The results from the experimental and field tests demonstrate that the new scheme is successful in detecting the generator internal fault. It has higher sensitivity and selectivity than the traditional protection scheme",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1705536,no,no,1486912014.954777
On the Automation of Software Fault Prediction,"This paper discusses the issues involved in building a practical automated tool to predict the incidence of software faults in future releases of a large software system. The possibility of creating such a tool is based on the authors' experience in analyzing the fault history of several large industrial software projects, and constructing statistical models that are capable of accurately predicting the most fault-prone software entities in an industrial environment. The emphasis of this paper is on the issues involved in the tool design and construction and an assessment of the extent to which the entire process can be automated so that it can be widely deployed and used by practitioners who do not necessarily have any particular statistical or modeling expertise",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691668,no,no,1486912014.954775
Bad-Smell Metrics for Aspect-Oriented Software,"Aspect-oriented programming (AOP) is a new programming paradigm that improves separation of concerns by decomposing the crosscutting concerns in aspect modules. Bad smells are metaphors to describe software patterns that are generally associated with bad design and bad programming of object-oriented programming (OOP). New notions and different ways of thinking for developing aspect-oriented (AO) software inevitably introduce bad smells which are specific bad design and bad programming in AO software called AO bad smells. Software metrics have been used to measure software artifact for a better understanding of its attributes and to assess its quality. Bad-smell metrics should be used as indicators for determining whether a particular fraction of AO code contains bad smells or not. Therefore, this paper proposes definition of metrics corresponding to the characteristic of each AO bad smell as a means to detecting them. The proposed bad-smell metrics are validated and the results show that the proposed bad- smell metrics can preliminarily indicate bad smells hidden in AO software.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276524,no,no,1486912014.954774
Software-Based Failure Detection and Recovery in Programmable Network Interfaces,"Emerging network technologies have complex network interfaces that have renewed concerns about network reliability. In this paper, we present an effective low-overhead fault tolerance technique to recover from network interface failures. Failure detection is based on a software watchdog timer that detects network processor hangs and a self-testing scheme that detects interface failures other than processor hangs. The proposed self-testing scheme achieves failure detection by periodically directing the control flow to go through only active software modules in order to detect errors that affect instructions in the local memory of the network interface. Our failure recovery is achieved by restoring the state of the network interface using a small backup copy containing just the right amount of information required for complete recovery. The paper shows how this technique can be made to minimize the performance impact to the host system and be completely transparent to the user.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4339198,no,no,1486912014.954772
Reliable upgrade of group communication software in sensor networks,"Communication is critical between nodes in wireless sensor networks. Upgrades to their communication software need to be done reliably because residual software errors in the new module can cause complete system failure. We present a software architecture, called cSimplex, which can reliably upgrade multicast-based group communication software in sensor networks. Errors in the new module are detected using statistical checks and a stability definition that we propose. Error recovery is done by switching to a well-tested, reliable safety module without any interruption in the functioning of the system. cSimplex has been implemented and demonstrated in a network of acoustic sensors with mobile robots functioning as base stations. Experimental results show that faults in the upgraded software can be detected with an accuracy of 99.71% on average. The architecture, which can be easily extended to other reliable upgrade problems, will facilitate a paradigm shift in system evolution from static design and extensive testing to reliable upgrades of critical communication components in networked systems, thus also enabling substantial savings in testing time and resources.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1203359,no,no,1486912014.95477
Instantiating and detecting design patterns: putting bits and pieces together,"Design patterns ease the designing, understanding, and re-engineering of software. Achieving a well-designed piece of software requires a deep understanding and a good practice of design patterns. Understanding existing software relies on the ability to identify architectural forms resulting from the implementation of design patterns. Maintaining software involves spotting places that can be improved by using better design decisions, like those advocated by design patterns. Nevertheless, there is a lack of tools automatizing the use of design patterns to achieve well-designed pieces of software, to identify recurrent architectural forms, and to maintain software. We present a set of tools and techniques to help OO software practitioners design, understand, and re-engineer a piece of software using design-patterns. A first prototype tool, PATTERNS-BOX, provides assistance in designing the architecture of a new piece of software, while a second prototype tool, PTIDEJ, identifies design patterns used in an existing one. These tools, in combination, support maintenance by highlighting defects in an existing design, and by suggesting and applying corrections based on widely-accepted design pattern solutions.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989802,no,no,1486912014.277373
Towards automatic transcription of Syriac handwriting,"We describe a method implemented for the recognition of Syriac handwriting from historical manuscripts. The Syriac language has been a neglected area for handwriting recognition research, yet is interesting because the preponderance of scribe-written manuscripts offers a challenging yet tractable medium for OCR research between the extremes of typewritten text and free handwriting. Like Arabic, Syriac is written in a cursive form from right-to-left, and letter shape depends on the position within the word. The method described does not need to find character strokes or contours. Both whole words and character shapes were used in recognition experiments. After segmentation using a novel probabilistic method, features of these shapes are found that tolerate variation in formation and image quality. Each shape is recognised individually using a discriminative support vector machine with 10-fold cross-validation. We describe experiments using a variety of segmentation methods and combinations of features on characters and words. Images from scribe-written historical manuscripts are used, and the recognition results are compared with those for images taken from clearer 19th century typeset documents. Recognition rates vary from 61-100%, depending on the algorithms used and the size and source of the data set.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1234126,no,no,1486912014.277372
Management of Virtual Machines on Globus Grids Using GridWay,"Virtual machines are a promising technology to overcome some of the problems found in current grid infrastructures, like heterogeneity, performance partitioning or application isolation. In this work, we present straightforward deployment of virtual machines in globus grids. This solution is based on standard services and does not require additional middleware to be installed. Also, we assess the suitability of this deployment in the execution of a high throughput scientific application, the XMM-Newton scientific analysis system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228276,no,no,1486912014.27737
Predicting accurate and actionable static analysis warnings,"Static analysis tools report software defects that may or may not be detected by other verification methods. Two challenges complicating the adoption of these tools are spurious false positive warnings and legitimate warnings that are not acted on. This paper reports automated support to help address these challenges using logistic regression models that predict the foregoing types of warnings from signals in the warnings and implicated code. Because examining many potential signaling factors in large software development settings can be expensive, we use a screening methodology to quickly discard factors with low predictive power and cost-effectively build predictive models. Our empirical evaluation indicates that these models can achieve high accuracy in predicting accurate and actionable static analysis warnings, and suggests that the models are competitive with alternative models built without screening.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814145,no,no,1486912014.277369
"Patterns, frameworks, and middleware: their synergistic relationships","The knowledge required to develop complex software has historically existed in programming folklore, the heads of experienced developers, or buried deep in the code. These locations are not ideal since the effort required to capture and evolve this knowledge is expensive, time-consuming, and error-prone. Many popular software modeling methods and tools address certain aspects of these problems by documenting how a system is designed However they only support limited portions of software development and do not articulate why a system is designed in a particular way, which complicates subsequent software reuse and evolution. Patterns, frameworks, and middleware are increasingly popular techniques for addressing key aspects of the challenges outlined above. Patterns codify reusable design expertise that provides time-proven solutions to commonly occurring software problems that arise in particular contexts and domains. Frameworks provide both a reusable product-line architecture [1] - guided by patterns - for a family of related applications and an integrated set of collaborating components that implement concrete realizations of the architecture. Middleware is reusable software that leverages patterns and frameworks to bridge the gap between the functional requirements of applications and the underlying operating systems, network protocol stacks, and databases. This paper presents an overview of patterns, frameworks, and middleware, describes how these technologies complement each other to enhance reuse and productivity, and then illustrates how they have been applied successfully in practice to improve the reusability and quality of complex software systems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201256,no,no,1486912014.277367
An approach to preserving sufficient correctness in open resource coalitions,"Most software that most people use most of the time needs only moderate assurance of fitness for its intended purpose. Unlike high-assurance software, where the severe consequences of failure justify substantial investment in validation, everyday software is used in settings in which occasional degraded service or even failure is tolerable. Unlike high-assurance software, which has been the subject of extensive scrutiny, everyday software has received only meager attention concerning how good it must be, how to decide whether a system is sufficiently correct, or how to detect and remedy abnormalities. The need for such techniques is particularly strong for software that takes the form of open resource coalitions - loosely-coupled aggregations of independent distributed resources. We discuss the problem of determining fitness for purpose, introduce a model for detecting abnormal behavior, and describe some of the ways to deal with abnormalities when they are detected",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=891137,no,no,1486912014.277366
Shared Data Analysis for Multi-Tasking Real-Time System Testing,"Memory corruption due to program faults is one of the most common failures in computer software. For software running in a sequential manner and for multi-tasking software with synchronized data accesses, it has been shown that program faults causing memory corruption can be detected by analyzing the relations between defines and uses of variables (DU coverage-based testing). However, using such methods in testing for memory corruption where globally shared data is accessed through asynchronous events will not be sufficient since they lack the possibility to analyse the cases where preemption of tasks may lead to interleaving failures. In this paper, we propose the use of a system level shared variable DU analysis of multi-tasking realtime software. By analyzing the temporal attributes of each access to globally shared data, our method handles asynchronous data accesses. When used in system-level testing, the result from the analysis can discover failures such as ordering, synchronization and interleaving failures. The result can also serve a as measure for coverage and complexity in data dependency at system level.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297324,no,no,1486912014.277364
A design tool for large scale fault-tolerant software systems,"In order to assist software designers in the application of fault-tolerance techniques to large scale software systems, a computer-aided software design tool has been proposed and implemented that assess the criticality of the software modules contained in the system. This information assists designers in identifying weaknesses in large systems that can lead to system failures. Through analysis and modeling techniques based in graph theory, modules are assessed and rated as to the criticality of their position in the software system. Graphical representation at two levels facilitates the use of cut set analysis, which is our main focus. While the task of finding all cut sets in any graph is NP-complete, the tool intelligently applies cut set analysis by limiting the problem to provide only the information needed for meaningful analysis. In this paper, we examine the methodology and algorithms used in the implementation of this tool and consider future refinements. Although further testing is needed to assess performance on increasingly complex systems, preliminary results look promising. Given the growing demand for reliable software and the complexities involved in the design of these systems, further research in this area is indicated.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285457,no,no,1486912014.277363
Prediction models for software fault correction effort,"We have developed a model to explain and predict the effort associated with changes made to software to correct faults while it is undergoing development. Since the effort data available for this study is ordinal in nature, ordinal response models are used to explain the effort in terms of measures of fault locality and the characteristics of the software components being changed. The calibrated ordinal response model is then applied to two projects not used in the calibration to examine predictive validity",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914975,no,no,1486912014.277361
Logic circuit diagnosis by using neural networks,"This paper presents a new method of logic diagnosis for combinatorial logic circuits. First, for each type of circuit gates, an equivalent neural network gate is constructed. Then, by replacing circuit gate elements with corresponding neural network gates, an equivalent neural network circuit is constructed to the fault-free sample circuit. The testing procedure is to feed random patterns to both the neural network circuit and the fault-prone test circuit at the same time, and comparing, analyzing both outputs, the former circuit generates diagnostic data for the test circuit. Thus, the neural network circuit behaves like a diagnostic engine, and needs basically no preparation of special test patterns nor fault dictionary before diagnosing",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=924594,no,no,1486912014.277359
Prediction of software faults using fuzzy nonlinear regression modeling,"Software quality models can predict the risk of faults in modules early enough for cost-effective prevention of problems. This paper introduces the fuzzy nonlinear regression (FNR) modeling technique as a method for predicting fault ranges in software modules. FNR modeling differs from classical linear regression in that the output of an FNR model is a fuzzy number. Predicting the exact number of faults in each program module is often not necessary. The FNR model can predict the interval that the number of faults of each module falls into with a certain probability. A case study of a full-scale industrial software system was used to illustrate the usefulness of FNR modeling. This case study included four historical software releases. The first release's data were used to build the FNR model, while the remaining three releases' data were used to evaluate the model. We found that FNR modeling gives useful results",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895473,no,no,1486912013.603256
A defect estimation approach for sequential inspection using a modified capture-recapture model,"Defect prediction is an important process in the evaluation of software quality. To accurately predict the rate of software defects can not only facilitate software review decisions, but can also improve software quality. In this paper, we have provided a defect estimation approach, which uses defective data from sequential inspections to increase the accuracy of estimating defects. To demonstrate potential improvements, the results of our approach were compared to those of two other popular estimation approaches, the capture-recapture model and the re-inspection model. By using the proposed approach, software organizations may increase the accuracy of their defect predictions and reduce the effort of subsequent inspections.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509995,no,no,1486912013.603254
Estimating Dependability of Parallel FFT Application using Fault Injection,This paper discusses estimation of dependability of a parallel FFT application. The application uses FFTW library. Fault susceptibility is assessed using software implemented fault injection. The fault injection campaign and the experiment results are presented. The response classes to injected faults are analyzed. The accuracy of evaluated data is verified experimentally.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376763,no,no,1486912013.603253
Definition and validation of design metrics for distributed applications,"As distributed technologies become more widely used, the need for assessing the quality of distributed applications correspondingly increases. Despite the rich body of research and practice in developing quality measures for centralised applications, there has been little emphasis on measures for distributed software. The need to understand the complex structure and behaviour of distributed applications suggests a shift in interest from traditional centralised measures to the distributed arena. We tackles the problem of evaluating quality attributes of distributed applications using software measures. Firstly, we present a measures suite to quantify internal attributes of design at an early development phase, embracing structural and behavioural aspects. The proposed measures are obtained from formal models derived from intuitive models of the problem domain. Secondly, since theoretical validation of software measures provides supporting evidence as to whether a measure really captures the internal attributes they purport to measure, we consider this validation as a necessary step before empirical validation takes place. Therefore, these measures are here theoretically validated following a framework proposed in the literature.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232461,no,no,1486912013.603252
Stool detection in colonoscopy videos,"Colonoscopy is the accepted screening method for detection of colorectal cancer or its precursor lesions, colorectal polyps. Indeed, colonoscopy has contributed to a decline in the number of colorectal cancer related deaths. However, not all cancers or large polyps are detected at the time of colonoscopy, and methods to investigate why this occurs are needed. One of the main factors affecting the diagnostic accuracy of colonoscopy is the quality of bowel preparation. The quality of bowel cleansing is generally assessed by the quantity of solid or liquid stool in the lumen. Despite a large body of published data on methods that could optimize cleansing, a substantial level of inadequate cleansing occurs in 10% to 75% of patients in randomized controlled trials. In this paper, a machine learning approach to the detection of stool in images of digitized colonoscopy video files is presented. The method involves the classification based on color features using a support vector machine (SVM) classifier. Our experiments show that the proposed stool image classification method is very accurate.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4649835,no,no,1486912013.60325
Design by Contract to Improve Software Vigilance,"Design by contract is a lightweight technique for embedding elements of formal specification (such as invariants, pre and postconditions) into an object-oriented design. When contracts are made executable, they can play the role of embedded, online oracles. Executable contracts allow components to be responsive to erroneous states and, thus, may help in detecting and locating faults. In this paper, we define vigilance as the degree to which a program is able to detect an erroneous state at runtime. Diagnosability represents the effort needed to locate a fault once it has been detected. In order to estimate the benefit of using design by contract, we formalize both notions of vigilance and diagnosability as software quality measures. The main steps of measure elaboration are given, from informal definitions of the factors to be measured to the mathematical model of the measures. As is the standard in this domain, the parameters are then fixed through actual measures, based on a mutation analysis in our case. Several measures are presented that reveal and estimate the contribution of contracts to the overall quality of a system in terms of vigilance and diagnosability",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703388,no,no,1486912013.603249
A Bayesian framework-based end-to-end packet loss prediction in IP networks,"Channel modelling in a network path is of major importance in designing delay sensitive applications. It is often not possible for these applications to retransmit packets due to delay constraints and they must therefore be resilient to packet losses. In this paper, we first establish an association between traffic delays and the queue size at a network gateway. A novel method for predicting packet losses is then proposed that is based on the correlation between the packet losses and the variations in the end-to-end time delay observed during transmission. We show that this makes it possible to predict packet losses before they occur. The transmission of multimedia streams can then be dynamically adjusted to account for the predicted losses. As a result, better error-resilience can be provided for multimedia streams transmitting through a dynamic network channel. This means that they can provide an improved quality of transmission under the same network budget constraint. Experiments have been performed and preliminary results have shown that the method can provide a much smoother and more reliable transmission of data.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376638,no,no,1486912013.603248
"Detection of forestland degradation using Landsat TM data in panda's habitat, Sichuan, China","In the 1990s forestland in the panda's habitat, Southwest China Mountains, underwent rapid degradation since the natural forest was converted into agricultural land. Remote sensing technology has not only provided a vivid representation of the forestland's surface but also become an efficient source of thematic maps such as the deforestation in this area. Landsat-5 TM data in 1994 and Landsat-7 TM data in 2002 are available for detecting the forestland degradation in the study area. The foggy, cloudy and snowy weather and mountainous landscape make it difficult to acquire remotely sensed data with high quality in the panda's habitat. Supervised classification is performed in the image process and a maximum-likelihood classification (MLC) is applied using the spectral signatures from the training sites. According to the topographical and meteorological conditions, different training sites are created such as forest-forest, river valley, forest, crop, town, water, snow, cloud, shadow and non-forest. As the result, forestland degradation map provides much information for forest degradation. Classification accuracy assessment is carried out by ERDAS software and the overall classification accuracy is up to 82.81%.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1369848,no,no,1486912013.603246
The Evaluation of Reliability Based on the Software Architecture in Neural Networks,"Software reliability is one of the key quality attributes. To avoid reworks after developing software, mentioned attribute of software must be evaluated correctly. Therefore, estimation and prediction of reliability during system development are very important. In this article a method to predict the failure probability in whole system by the neural networks has been presented. The model of this neural network varies based on the architectural style of software. An evaluation model has been implemented for one architectural style of software as a case study.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668082,no,no,1486912013.603244
Availability requirement for a fault-management server in high-availability communication systems,"This paper investigates the availability requirement for the fault management server in high-availability communication systems. This study shows that the availability of the fault management server does not need to be 99.999% in order to guarantee a 99.999% system availability, as long as the fail-safe ratio (the probability that the failure of the fault management server does not bring down the system) and the fault coverage ratio (probability that the failure in the system can be detected and recovered by the fault management server) are sufficiently high. Tradeoffs can be made among the availability of the fault management server, the fail-safe ratio, and the fault coverage ratio to optimize system availability. A cost-effective design for the fault management server is proposed.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1211116,no,no,1486912013.603241
Predicting the SEU error rate through fault injection for a complex microprocessor,"This paper deals with the prediction of SEU error rate for an application running on a complex processor. Both, radiation ground testing and fault injection, were performed while the selected processor, a Power PC 7448, executed a software issued from a real space application. The predicted error rate shows that generally used strategies, based on static cross-section, significantly overestimate the application error rate.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677290,no,no,1486912012.933194
Using Software Dependencies and Churn Metrics to Predict Field Failures: An Empirical Case Study,"Commercial software development is a complex task that requires a thorough understanding of the architecture of the software system. We analyze the Windows Server 2003 operating system in order to assess the relationship between its software dependencies, churn measures and post-release failures. Our analysis indicates the ability of software dependencies and churn measures to be efficient predictors of post-release failures. Further, we investigate the relationship between the software dependencies and churn measures and their ability to assess failure-proneness probabilities at statistically significant levels.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343764,no,no,1486912012.933192
A Fast Audio Digital Watermark Method Based on Counter-Propagation Neural Networks,"In this thesis, we present a novel audio digital watermark method based on counter-propagation Neural Networks. After dealing with the audio by discrete wavelet transform, we select the important coefficients which are ready to be trained in the neural networks. By making use of the capabilities of memorization and fault tolerance in CPN, watermark is memorized in the nerve cells of CPN. In addition, we adopt a kind of architecture with a adaptive number of parallel CPN to treat with each audio frame and the corresponding watermark bit. Comparing with other traditional methods by using CPN, it was largely improve the efficiency for watermark embedding and correctness for extracting, namely the speed of whole algorithm. The extensive experimental results show that, we can detect the watermark exactly under most of attacks. This method efficaciously tradeoff both the robustness and inaudibility of the audio digital watermark.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722411,no,no,1486912012.933191
Finding predictors of field defects for open source software systems in commonly available data sources: a case study of OpenBSD,"Open source software systems are important components of many business software applications. Field defect predictions for open source software systems may allow organizations to make informed decisions regarding open source software components. In this paper, we remotely measure and analyze predictors (metrics available before release) mined from established data sources (the code repository and the request tracking system) as well as a novel source of data (mailing list archives) for nine releases of OpenBSD. First, we attempt to predict field defects by extending a software reliability model fitted to development defects. We find this approach to be infeasible, which motivates examining metrics-based field defect prediction. Then, we evaluate 139 predictors using established statistical methods: Kendall's rank correlation, Pearson's rank correlation, and forward AIC model selection. The metrics we collect include product metrics, development metrics, deployment and usage metrics, and software and hardware configurations metrics. We find the number of messages to the technical discussion mailing list during the development period (a deployment and usage metric captured from mailing list archives) to be the best predictor of field defects. Our work identifies predictors of field defects in commonly available data sources for open source software systems and is a step towards metrics-based field defect prediction for quantitatively-based decision making regarding open source software components",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509310,no,no,1486912012.933189
Software Effort Estimation using Machine Learning Techniques with Robust Confidence Intervals,"The precision and reliability of the estimation of the effort of software projects is very important for the competitiveness of software companies. Good estimates play a very important role in the management of software projects. Most methods proposed for effort estimation, including methods based on machine learning, provide only an estimate of the effort for a novel project. In this paper we introduce a method based on machine learning which gives the estimation of the effort together with a confidence interval for it. In our method, we propose to employ robust confidence intervals, which do not depend on the form of probability distribution of the errors in the training set. We report on a number of experiments using two datasets aimed to compare machine learning techniques for software effort estimation and to show that robust confidence intervals can be successfully built.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344078,no,no,1486912012.933188
An intelligent early warning system for software quality improvement and project management,"One of the main reasons behind unfruitful software development projects is that it is often too late to correct the problems by the time they are detected. It clearly indicates the need for early warning about the potential risks. In this paper, we discuss an intelligent software early warning system based on fuzzy logic using an integrated set of software metrics. It helps to assess risks associated with being behind schedule, over budget, and poor quality in software development and maintenance from multiple perspectives. It handles incomplete, inaccurate, and imprecise information, and resolve conflicts in an uncertain environment in its software risk assessment using fuzzy linguistic variables, fuzzy sets, and fuzzy inference rules. Process, product, and organizational metrics are collected or computed based on solid software models. The intelligent risk assessment process consists of the following steps: fuzzification of software metrics, rule firing, derivation and aggregation of resulted risk fuzzy sets, and defuzzification of linguistic risk variables.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250167,no,no,1486912012.933187
IDEM: an Internet delay emulator approach for assessing VoIP quality,"Network emulations are imitations of real-time network behavior that help in testing and assessing protocols, and other network related applications in a controlled hardware and software environment. Most of the emulators existing today are hardware implemented emulators. There is a rising demand to emulate the network behavior using a software tool. Our Internet delay emulator (IDEM) is a software tool that captures the network details and reproduces an environment useful for research oriented projects. IDEM is based on bouncers that are distributed over the Internet. The concepts of firewall routing are used in designing IDEM. IDEM supports both TCP and UDP applications. Rigorous testing shows that actual delay in data sent is accurately modeled by IDEM. Advantages of IDEM especially for delay sensitive applications like VoIP are discussed.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515508,no,no,1486912012.933185
Fair QoS resource management and non-linear prediction of 3D rendering applications,"Resource management in a grid has to able to guarantee commercial or industrial applications a personalized quality of service (QoS). To implement, however, an efficient resource allocation scheme, prediction of task workload is required. In this paper, we present an efficient algorithm for predicting the workload of 3D rendering tasks based on constructive neural network architecture. We also consider the QoS scheduling problem whose target is to determine when and on which resource a given job should be executed. We propose an algorithm for QoS scheduling, which allocates the resources in a fair way, and we compare it to other scheduling schemes such as the earliest deadline first and the first come first serve policies.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1328890,no,no,1486912012.933183
Predicting Defect Content and Quality Assurance Effectiveness by Combining Expert Judgment and Defect Data - A Case Study,"Planning quality assurance (QA) activities in a systematic way and controlling their execution are challenging tasks for companies that develop software or software-intensive systems. Both require estimation capabilities regarding the effectiveness of the applied QA techniques and the defect content of the checked artifacts. Existing approaches for these purposes need extensive measurement data from his-torical projects. Due to the fact that many companies do not collect enough data for applying these approaches (es-pecially for the early project lifecycle), they typically base their QA planning and controlling solely on expert opinion. This article presents a hybrid method that combines commonly available measurement data and context-specific expert knowledge. To evaluate the methodpsilas applicability and usefulness, we conducted a case study in the context of independent verification and validation activities for critical software in the space domain. A hybrid defect content and effectiveness model was developed for the software requirements analysis phase and evaluated with available legacy data. One major result is that the hybrid model provides improved estimation accuracy when compared to applicable models based solely on data. The mean magni-tude of relative error (MMRE) determined by cross-validation is 29.6% compared to 76.5% obtained by the most accurate data-based model.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700306,no,no,1486912012.933181
Type Highlighting: A Client-Driven Visual Approach for Class Hierarchies Reengineering,"Polymorphism and class hierarchies are key to increasing the extensibility of an object-oriented program but also raise challenges for program comprehension. Despite many advances in understanding and restructuring class hierarchies, there is no direct support to analyze and understand the design decisions that drive their polymorphic usage. In this paper we introduce a metric-based visual approach to capture the extent to which the clients of a hierarchy polymorphically manipulate that hierarchy. A visual pattern vocabulary is also presented in order to facilitate the communication between analysts. Initial evaluation shows that our techniques aid program comprehension by effectively visualizing large quantities of information, and can help detect several design problems.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4637553,no,no,1486912012.933179
How to produce better quality test software,"LabWindows/CVI is a popular C compiler for writing automated test equipment (ATE) test software. Since C was designed as a portable assembly language, it uses many low-level machine operations that tend to be error prone, even for the professional programmer. Test equipment engineers also tend to underestimate the effort required to write high-quality software. Quality software has very few defects and is easy to write and maintain. The examples used in this article are for the C programming language, but the principles also apply to most other programming languages. Most of the tools mentioned work with both C and C++ software.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1502445,no,no,1486912012.265907
Using Sequence Diagrams to Detect Communication Problems between Systems,"Many software systems are evolving complex system of systems (SoS) for which inter-system communication is both mission-critical and error-prone. Such communication problems ideally would be detected before deployment. In a NASA-supported Software Assurance Research Program (SARP) project, we are researching a new approach addressing such problems. In this paper, we show that problems in the communication between two systems can be detected by using sequence diagrams to model the planned communication and by comparing the planned sequence to the actual sequence. We identify different kinds of problems that can be addressed by modeling the planned sequence using different level of abstractions.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526571,no,no,1486912012.265906
Covering arrays for efficient fault characterization in complex configuration spaces,"Many modern software systems are designed to be highly configurable so they can run on and be optimized for a wide variety of platforms and usage scenarios. Testing such systems is difficult because, in effect, you are testing a multitude of systems, not just one. Moreover, bugs can and do appear in some configurations, but not in others. Our research focuses on a subset of these bugs that are """"option-related""""-those that manifest with high probability only when specific configuration options take on specific settings. Our goal is not only to detect these bugs, but also to automatically characterize the configuration subspaces (i.e., the options and their settings) in which they manifest. To improve efficiency, our process tests only a sample of the configuration space, which we obtain from mathematical objects called covering arrays. This paper compares two different kinds of covering arrays for this purpose and assesses the effect of sampling strategy on fault characterization accuracy. Our results strongly suggest that sampling via covering arrays allows us to characterize option-related failures nearly as well as if we had tested exhaustively, but at a much lower cost. We also provide guidelines for using our approach in practice.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1583600,no,no,1486912012.265904
An analysis of fault detection latency bounds of the SNS scheme incorporated into an Ethernet based middleware system,"The supervisor-based network surveillance (SNS) scheme is a semi-centralized network surveillance scheme for detecting the health status of computing components in a distributed real-time (RT) system. An implementation of the SNS scheme in a middleware architecture, named ROAFTS (real-time object-oriented adaptive fault-tolerance support), has been underway in the authors' lab. ROAFTS is a middleware subsystem which is layered above a COTS (commercial-off-the-shelf) operating system (OS), such as Windows XP or UNIX, and functions as the core of a reliable RT execution engine for fault-tolerant (FT) distributed RT applications. The applications supported by ROAFTS are structured as a network of RT objects, named time-triggered message-triggered objects (TMOs). The structure of the prototype implementation of the SNS scheme is discussed first, then a rigorous analysis of the time bounds for fault detection and recovery is provided.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180192,no,no,1486912012.265903
A novel method for DC system grounding fault monitoring on-line and its realization,"On the basis of comparison and analysis of the present grounding fault monitoring methods such as method of AC injection, method of DC leakage and so on, this paper points out their shortcomings in practical applications. A novel method named method of different frequency signals for detecting grounding fault of DC system is advanced, which can overcome the bad influence of distributed capacitor between the ground and branches. Finally a new kind of detector based on the proposed method is introduced. The detector, with C8051F041 as kernel, adopting method of different frequency signals, realizes on-line grounding fault monitoring exactly. The principles, hardware and software design are introduced in detail. The experimental results and practical operations show that the detector has the advantages of high-precision, better anti-interference, high degree of automation, low cost, etc.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4636452,no,no,1486912012.265902
Discrete wavelet transform and probabilistic neural network based algorithm for classification of fault on transmission systems,"This paper presents the development of an algorithm based on discrete wavelet transform (DWT) and probabilistic neural network (PNN) for classifying the power system faults. The proposed technique consists of a preprocessing unit based on discrete wavelet transform in combination with PNN. The DWT acts as extractor of distinctive features in the input current signal, which are collected at source end. The information is then fed into PNN for classifying the faults. It can be used for off-line process using the data stored in the digital recording apparatus. Extensive simulation studies carried out using MATLAB show that the proposed algorithm not only provides an accepted degree of accuracy in fault classification under different fault conditions but it is also reliable, fast and computationally efficient tool.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4768827,no,no,1486912012.2659
Using simulation for assessing the real impact of test-coverage on defect-coverage,"The use of test-coverage measures (e.g., block-coverage) to control the software test process has become an increasingly common practice. This is justified by the assumption that higher test-coverage helps achieve higher defect-coverage and therefore improves software quality. In practice, data often show that defect-coverage and test-coverage grow over time, as additional testing is performed. However, it is unclear whether this phenomenon of concurrent growth can be attributed to a causal dependency, or if it is coincidental, simply due to the cumulative nature of both measures. Answering such a question is important as it determines whether a given test-coverage measure should be monitored for quality control and used to drive testing. Although it is no general answer to this problem, a procedure is proposed to investigate whether any test-coverage criterion has a genuine additional impact on defect-coverage when compared to the impact of just running additional test cases. This procedure applies in typical testing conditions where the software is tested once, according to a given strategy, coverage measures are collected as well as defect data. This procedure is tested on published data, and the results are compared with the original findings. The study outcomes do not support the assumption of a causal dependency between test-coverage and defect-coverage, a result for which several plausible explanations are provided",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=855537,no,no,1486912012.265899
Empirical studies of a prediction model for regression test selection,"Regression testing is an important activity that can account for a large proportion of the cost of software maintenance. One approach to reducing the cost of regression testing is to employ a selective regression testing technique that: chooses a subset of a test suite that was used to test the software before the modifications; then uses this subset to test the modified software. Selective regression testing techniques reduce the cost of regression testing if the cost of selecting the subset from the test suite together with the cost of running the selected subset of test cases is less than the cost of rerunning the entire test suite. Rosenblum and Weyuker (1997) proposed coverage-based predictors for use in predicting the effectiveness of regression test selection strategies. Using the regression testing cost model of Leung and White (1989; 1990), Rosenblum and Weyuker demonstrated the applicability of these predictors by performing a case study involving 31 versions of the KornShell. To further investigate the applicability of the Rosenblum-Weyuker (RW) predictor, additional empirical studies have been performed. The RW predictor was applied to a number of subjects, using two different selective regression testing tools, Deja vu and TestTube. These studies support two conclusions. First, they show that there is some variability in the success with which the predictors work and second, they suggest that these results can be improved by incorporating information about the distribution of modifications. It is shown how the RW prediction model can be improved to provide such an accounting",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910860,no,no,1486912012.265897
Fault location using wavelet energy spectrum analysis of traveling waves,"Power grid faults generate traveling wave signals at the fault point. The signals transmit to both ends of the faulted transmission line, and to the whole power grid. The traveling wave signals have many components with different frequencies and all the components have fault characteristics. The signals can be employed in locating accurately the fault, and the location method cannot be influenced by current transformer saturation and low frequency oscillation. The frequency band component with energy concentrated in the detected traveling wave is extracted by wavelet energy spectrum analysis. The arrival time of the component is recorded with wavelet analysis in the time domain. The propagation velocity of the component is calculated by the last recorded traveling wave arrived time at both ends of the tested transmission line, which is generated by an outside disturbance. The fault location scheme is simulated with ATP software. Results show that the accuracy of the method is little affected by fault positions, fault types and grounding resistances. The fault location error is less than 100 m.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4510194,no,no,1486912012.265895
A memory-based reasoning approach for assessing software quality,"Several methods have been explored for assuring the reliability of mission critical systems (MCS), but no single method has proved to be completely effective. This paper presents an approach for quantifying the confidence in the probability that a program is free of specific classes of defects. The method uses memory-based reasoning techniques to admit a variety of data from a variety of projects for the purpose of assessing new systems. Once a sufficient amount of information has been collected, the statistical results can be applied to programs that are not in the analysis set to predict their reliabilities and guide the testing process. The approach is applied to the analysis of Y2K defects based on defect data generated using fault-injection simulation",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960603,no,no,1486912012.265893
Documentation as a software process capability indicator,"Summary form only given. In a small software organization, a close and intense relationship with its customers is often a substitute for documentation along the software processes. Nevertheless, according to the quality standards, the inadequacy of the required documentation will retain the assessed capability of software processes on the lowest level. This article describes the interconnections between software process documentation and the maturity of the organization. The data is collected from the SPICE assessment results of small and medium sized software organizations in Finland. The aim of the article is to visualise the necessity of documentation throughout the software engineering processes in order to achieve a higher capability level. In addition we point out that processes with insufficient documentation decrease the chance to improve the quality of the processes, as it is impossible to track and analyse them",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952347,no,no,1486912011.598393
ConfErr: A tool for assessing resilience to human configuration errors,"We present ConfErr, a tool for testing and quantifying the resilience of software systems to human-induced configuration errors. ConfErr uses human error models rooted in psychology and linguistics to generate realistic configuration mistakes; it then injects these mistakes and measures their effects, producing a resilience profile of the system under test. The resilience profile, capturing succinctly how sensitive the target software is to different classes of configuration errors, can be used for improving the software or to compare systems to each other. ConfErr is highly portable, because all mutations are performed on abstract representations of the configuration files. Using ConfErr, we found several serious flaws in the MySQL and Postgres databases, Apache web server, and BIND and djbdns name servers; we were also able to directly compare the resilience of functionally-equivalent systems, such as MySQL and Postgres.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630084,no,no,1486912011.598391
Perspectives on Redundancy: Applications to Software Certification,"Redundancy is a feature of systems that arises by design or as an accidental byproduct of design, and can be used to detect, diagnose or correct errors that occur in systems operations. While it is usually investigated in the context of fault tolerance, one can argue that it is in fact an intrinsic feature of a system that can be analyzed on its own without reference to any fault tolerance capability. In this paper, we submit three alternative views of redundancy, which we propose to analyze to gain a better understanding of redundancy; we also explore means to use this understanding to enhance the design of fault tolerant systems.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1385898,no,no,1486912011.59839
The effectiveness of software development technical reviews: a behaviorally motivated program of research,"Software engineers use a number of different types of software development technical review (SDTR) for the purpose of detecting defects in software products. This paper applies the behavioral theory of group performance to explain the outcomes of software reviews. A program of empirical research is developed, including propositions to both explain review performance and identify ways of improving review performance based on the specific strengths of individuals and groups. Its contributions are to clarify our understanding of what drives defect detection performance in SDTRs and to set an agenda for future research. In identifying individuals' task expertise as the primary driver of review performance, the research program suggests specific points of leverage for substantially improving review performance. It points to the importance of understanding software reading expertise and implies the need for a reconsideration of existing approaches to managing reviews",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=825763,no,no,1486912011.598388
Combining software quality predictive models: an evolutionary approach,"During the last ten years, a large number of quality models have been proposed in the literature. In general, the goal of these models is to predict a quality factor starting from a set of direct measures. The lack of data behind these models makes it hard to generalize, cross-validate, and reuse existing models. As a consequence, for a company, selecting an appropriate quality model is a difficult, non-trivial decision. In this paper, we propose a general approach and a particular solution to this problem. The main idea is to combine and adapt existing models (experts) in such a way that the combined model works well on the particular system or in the particular type of organization. In our particular solution, the experts are assumed to be decision tree or rule-based classifiers and the combination is done by a genetic algorithm. The result is a white-box model: for each software component, not only does the model give a prediction of the software quality factor, it also provides the expert that was used to obtain the prediction. Test results indicate that the proposed model performs significantly better than individual experts in the pool.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167795,no,no,1486912011.598387
CSMR 2008 - Workshop on Software Quality and Maintainability (SQM 2008),"Software is playing a crucial role in modern societies. Not only do people rely on it for their daily operations or business, but for their lives as well. For this reason correct and consistent behaviour of software systems is a fundamental part of end user expectations. Additionally, businesses require cost-effective production, maintenance, and operation of their systems. Thus, the demand for software quality is increasing and is setting it as a differentiator for the success or failure of a software product. In fact, high quality software is becoming not just a competitive advantage but a necessary factor for companies to be successful. The main question that arises now is how quality is measured. What, where and when we assess and assure quality, are still open issues. Many views have been expressed about software quality attributes, including maintainability, evolvability, portability, robustness, reliability, usability, and efficiency. These have been formulated in standards such as ISO/IEC-9126 and CMM. However, the debate about quality and maintainability between software producers, vendors and users is ongoing, while organizations need the ability to evaluate from multiple angles the software systems that they use or develop. So, is """"software quality in the eye of the beholder""""? This workshop session aims at feeding into this debate by establishing what the state of the practice and the way forward is.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493346,no,no,1486912011.598386
An experimental evaluation of correlated network partitions in the Coda distributed file system,"Experimental evaluation is an important way to assess distributed systems, and fault injection is the dominant technique in this area for the evaluation of a system's dependability. For distributed systems, network failure is an important fault model. Physical network failures often have far-reaching effects, giving rise to multiple correlated failures as seen by higher-level protocols. This paper presents an experimental evaluation, using the Loki fault injector, which provides insight into the impact that correlated network partitions have on the Coda distributed file system. In this evaluation, Loki created a network partition between two Coda file servers, during which updates were made at each server to the same replicated data volume. Upon repair of the partition, a client requested directory resolution to converge the diverging replicas. At various stages of the resolution, Loki invoked a second correlated network partition, thus allowing us to evaluate its impact on the system's correctness, performance, and availability.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238077,no,no,1486912011.598384
End-to-end defect modeling,"In this context, computer models can help us predict outcomes and anticipate with confidence. We can now use cause-effect modeling to drive software quality, moving our organization toward higher maturity levels. Despite missing good software quality models, many software projects successfully deliver software on time and with acceptable quality. Although researchers have devoted much attention to analyzing software projects' failures, we also need to understand why some are successful - within budget, of high quality, and on time-despite numerous challenges. Restricting software quality to defects, decisions made in successful projects must be based on some understanding of cause-effect relationships that drive defects at each stage of the process. To manage software quality by data, we need a model describing which factors drive defect introduction and removal in the life cycle, and how they do it. Once properly built and validated, a defect model enables successful anticipation. This is why it's important that the model include all variables influencing the process response to some degree.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1331312,no,no,1486912011.598383
An approach for estimation of software aging in a Web server,"A number of recent studies have reported the phenomenon of """"software aging"""", characterized by progressive performance degradation or a sudden hang/crash of a software system due to exhaustion of operating system resources, fragmentation and accumulation of errors. To counteract this phenomenon, a proactive technique called """"software rejuvenation"""" has been proposed. This essentially involves stopping the running software, cleaning its internal state and then restarting it. Software rejuvenation, being preventive in nature, begs the question as to when to schedule it. Periodic rejuvenation, while straightforward to implement, may not yield the best results. A better approach is based on actual measurement of system resource usage and activity that detects and estimates resource exhaustion times. Estimating the resource exhaustion times makes it possible for software rejuvenation to be initiated or better planned so that the system availability is maximized in the face of time-varying workload and system behavior. We propose a methodology based on time series analysis to detect and estimate resource exhaustion times due to software aging in a Web server while subjecting it to an artificial workload. We first collect and log data on several system resource usage and activity parameters on a Web server. Time-series ARMA models are then constructed from the data to detect aging and estimate resource exhaustion times. The results are then compared with previous measurement-based models and found to be more efficient and computationally less intensive. These models can be used to develop proactive management techniques like software rejuvenation which are triggered by actual measurements.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166929,no,no,1486912011.598381
"Adaptive and automated detection of service anomalies in transaction-oriented WANs: network analysis, algorithms, implementation, and deployment","Algorithms and software for proactive and adaptive detection of network/service anomalies (i.e., performance degradations) have been developed, implemented, deployed, and field-tested for transaction-oriented wide area networks (WANs). A real-time anomaly detection system called TRISTAN (transaction instantaneous anomaly notification) has been implemented, and is deployed in the commercially important AT&T transaction access services (TAS) network. TAS is a high volume, multiple service classes, hybrid telecom and data WAN that services transaction traffic in the U.S. and neighboring countries. TRISTAN adaptively and preactively detects network/service performance anomalies in multiple-service-class-based and transaction-oriented networks, where performances of service classes are mutually dependent and correlated, where environmental factors (e.g., nonmanaged or nonmonitored equipment within customer premises) can strongly impact network and service performances. Specifically, TRISTAN implements algorithms that: 1) sample and convert raw transaction records to service-class based performance data in which potential network anomalies are highlighted; 2) automatically construct adaptive and service-class-based performance thresholds from historical transaction records for detecting network and service anomalies; and 3) perform real-time network/service anomaly detection. TRISTAN is demonstrated to be capable of proactively detecting network/service anomalies, which easily elude detection by the traditional alarm-based network monitoring systems.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842990,no,no,1486912011.598378
Web site complexity metrics for measuring navigability,"In years, navigability has become the pivot of Web site designs. Existing works fall into two categories. The first is to evaluate and assess a Web site's navigability against a set of criteria or check list. The second is to analyse usage data of the Web site, such as the server log files. This work investigates a metric approach to Web site navigability measurement. In comparison with existing assessment and analysis methods, navigability metrics have the advantages of objectiveness and the possibility of using automated tools to evaluate large-scale Web sites. This work proposes a number of metrics for Web site navigability measurement based on measuring Web site structural complexity. We validate these metrics against Weyuker's software complexity axioms, and report the results of empirical studies of the metrics.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357958,no,no,1486912010.931003
Diagram-based CBA using DATsys and CourseMaster,"Supporting the assessment of an ever-increasing number of students is an error-prone and resource intensive process. Computer based assessment (CBA) software aids educators by automating aspects of the assessment of student work. Using CBA benefits pedagogically and practically both students and educators. The Learning Technology Group at the University of Nottingham has been actively researching, developing and using software to automatically assess programming coursework for 14 years. Two of the systems developed, Ceilidh and its successor CourseMaster, are being used by an increasing number of academic institutions. Research has resulted in a system for supporting the full lifecycle of free-response CBA that has diagram-based solutions. The system, DATsys, is an authoring environment for developing diagram-based CBA. It has been designed to support the authoring of coursework for most types of diagram notations. Exercises have been developed and tested for circuit diagrams, flowcharts and class diagrams. Future research plans are for authoring exercises in many more diagram notations.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1185893,no,no,1486912010.931001
Assessing the Quality Impact of Design Inspections,"Inspections are widely used and studies have found them to be effective in uncovering defects. However, there is less data available regarding the impact of inspections on different defect types and almost no data quantifying the link between inspections and desired end product qualities. This paper addresses this issue by investigating whether design inspection checklists can be tailored so as to effectively target certain defect types without impairing the overall defect detection rate. The results show that the design inspection approach used here does uncover useful design quality issues and that the checklists can be effectively tailored for some types of defects.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343782,no,no,1486912010.931
Signature-based workload estimation for mobile 3D graphics,"Until recently, most 3D graphics applications had been regarded as too computationally intensive for devices other than desktop computers and gaming consoles. This notion is rapidly changing due to improving screen resolutions and computing capabilities of mass-market handheld devices such as cellular phones and PDAs. As the mobile 3D gaming industry is poised to expand, significant innovations are required to provide users with high-quality 3D experience under limited processing, memory and energy budgets that are characteristic of the mobile domain. Energy saving schemes such as dynamic voltage and frequency scaling (DVFS), as well as system-level power and performance optimization methods for mobile devices require accurate and fast workload prediction. In this paper, we address the problem of workload prediction for mobile 3D graphics. We propose and describe a signature-based estimation technique for predicting 3D graphics workloads. By analyzing a gaming benchmark, we show that monitoring specific parameters of the 3D pipeline provides better prediction accuracy over conventional approaches. We describe how signatures capture such parameters concisely to make accurate workload predictions. Signature-based prediction is computationally efficient because first, signatures are compact, and second, they do not require elaborate model evaluations. Thus, they are amenable to efficient, real-time prediction. A fundamental difference between signatures and standard history-based predictors is that signatures capture previous outcomes as well as the cause that led to the outcome, and use both to predict future outcomes. We illustrate the utility of signature-based workload estimation technique by using it as a basis for DVFS in 3D graphics pipelines",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688866,no,no,1486912010.930999
Anomaly Detection Support Vector Machine and Its Application to Fault Diagnosis,"We address the issue of classification problems in the following situation: test data include data belonging to unlearned classes. To address this issue, most previous works have taken two-stage strategies where unclear data are detected using an anomaly detection algorithm in the first stage while the rest of data are classified into learned classes using a classification algorithm in the second stage. In this study, we propose anomaly detection support vector machine (ADSVM) which unifies classification and anomaly detection. ADSVM is unique in comparison with the previous work in that it addresses the two problems simultaneously. We also propose a multiclass extension of ADSVM that uses a pairwise voting strategy. We empirically present that ADSVM outperforms two-stage algorithms in application to an real automobile fault dataset, as well as to UCI benchmark datasets.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781181,no,no,1486912010.930997
Software-based adaptive and concurrent self-testing in programmable network interfaces,"Emerging network technologies have complex network interfaces that have renewed concerns about network reliability. In this paper, we present an effective low-overhead failure detection technique, which is based on a software watchdog timer that detects network processor hangs and a self-testing scheme that detects interface failures other than processor hangs. The proposed adaptive and concurrent self-testing scheme achieves failure detection by periodically directing the control flow to go through only active software modules in order to detect errors that affect instructions in the local memory of the network interface. The paper shows how this technique can be made to minimize the performance impact on the host system and be completely transparent to the user",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1655700,no,no,1486912010.930996
An empirical study on the design effort of Web applications,"We study the effort needed for designing Web applications from an empirical point of view. The design phase forms an important part of the overall effort needed to develop a Web application, since the use of tools can help automate the implementation phase. We carried out an empirical study with students of an advanced university class that used W2000 as a Web application design technique. Our first goal was to compare the relative importance of each design activity. Second, we tried to assess the accuracy of a priori design effort predictions and the influence of factors on the effort needed for each design activity. Third, we also studied the quality of the designs obtained.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181670,no,no,1486912010.930995
A Scalable Parallel Deduplication Algorithm,"The identification of replicas in a database is fundamental to improve the quality of the information. Deduplication is the task of identifying replicas in a database that refer to the same real world entity. This process is not always trivial, because data may be corrupted during their gathering, storing or even manipulation. Problems such as misspelled names, data truncation, data input in a wrong format, lack of conventions (like how to abbreviate a name), missing data or even fraud may lead to the insertion of replicas in a database. The deduplication process may be very hard, if not impossible, to be performed manually, since actual databases may have hundreds of millions of records. In this paper, we present our parallel deduplication algorithm, called FER- APARDA. By using probabilistic record linkage, we were able to successfully detect replicas in synthetic datasets with more than 1 million records in about 7 minutes using a 20- computer cluster, achieving an almost linear speedup. We believe that our results do not have similar in the literature when it comes to the size of the data set and the processing time.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384045,no,no,1486912010.930993
Design and implementation of a fault diagnosis system for transmission and subtransmission networks,"This paper proposes a new intelligent diagnostic system for on-line fault diagnosis of power systems using information of relays and circuit breakers. This diagnostic system consists of three parts: an interfacing hardware, a navigation software and an intelligent core. The interfacing hardware samples the protective elements of the power system. By means of this data, the intelligent core detects occurrence of fault and determines the fault features, such as type and location of fault. The navigation software manages the diagnostic system. The software controls the interfacing hardware and provides required data of the intelligent core. Moreover, this software is user interface of the fault diagnostic system. The proposed approach has been examined on a practical power system (Semnan Regional Electric Company) with real and simulated events. Obtained results confirm the validity of the developed approach.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1335361,no,no,1486912010.930992
Design of a video system to detect and sort the faults of cigarette package,A hardware flatform detecting and sorting the faults of cigarette package is designed by using the technology of machine vision. Algorithm and systemic software are also designed according to its characteristics. With stability and accuracy it is of some guiding value to the high speed on-line package detection.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4744018,no,no,1486912010.930989
"High quality statecharts through tailored, perspective-based inspections","In the embedded systems domain, statecharts have become an important technique to describe the dynamic behavior of a software system. In addition, statecharts are an important element of object-oriented design documents and are thus widely used in practice. However, not much is known about how to inspect them. Since their invention by Pagan in 1976, inspections proved to be an essential quality assurance technique in software engineering. Traditionally, inspections were used to detect defects in code documents, and later in requirements documents. We define a defect taxonomy for statecharts. Using this taxonomy, we present an inspection approach for inspecting statecharts, which combines existing inspection techniques with several new perspective-based scenarios. Moreover, we address the problems of inspecting large documents by using prioritized use cases in combination with perspective-based reading.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231608,no,no,1486912010.268944
Uncertainty in the output of artificial neural networks,"Analysis of the performance of artificial neural networks (ANNs) is usually based on aggregate results on a population of cases. In this paper, we analyze ANN output corresponding to the individual case. We show variability in the outputs of multiple ANNs that are trained and """"optimized"""" from a common set of training cases. We predict this variability from a theoretical standpoint on the basis that multiple ANNs can be optimized to achieve similar overall performance on a population of cases, but produce different outputs for the same individual case because the ANNs use different weights. We use simulations to show that the average standard deviation in the ANN output can be two orders of magnitude higher than the standard deviation in the ANN overall performance measured by the A<sub>z</sub> value. We further show this variability using an example in mammography where the ANNs are used to classify clustered microcalcifications as malignant or benign based on image features extracted from mammograms. This variability in the ANN output is generally not recognized because a trained individual ANN becomes a deterministic model. Recognition of this variability and the deterministic view of the ANN present a fundamental contradiction. The implication of this variability to the classification task warrants additional study.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1216214,no,no,1486912010.268942
Defect-based reliability analysis for mission-critical software,"Most software reliability methods have been developed to predict the reliability of a program using only data gathered during the resting and validation of a specific program. Hence, the confidence that can be attained in the reliability estimate is limited since practical resource constraints can result in a statistically small sample set. One exception is the Orthogonal Defect Classification (ODC) method, which uses data gathered from several projects to track the reliability of a new program, Combining ODC with root-cause analysis can be useful in many applications where it is important to know the reliability of a program for a specific type of a fault. By focusing on specific classes of defects, it becomes possible to (a) construct a detailed model of the defect and (b) use data from a large number of programs. In this paper, we develop one such approach and demonstrate its application to modeling Y2K defects",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884761,no,no,1486912010.268941
Reliability Analysis of Self-Healing Network using Discrete-Event Simulation,"The number of processors embedded on high performance computing platforms is continuously increasing to accommodate user desire to solve larger and more complex problems. However, as the number of components increases, so does the probability of failure. Thus, both scalable and fault-tolerance of software are important issues in this field. To ensure reliability of the software especially under the failure circumstance, the reliability analysis is needed. The discrete-event simulation technique offers an attractive a ternative to traditional Markovian-based analytical models, which often have an intractably large state space. In this paper, we analyze reliability of a self-healing network developed for parallel runtime environments using discrete-event simulation. The network is designed to support transmission of messages across multiple nodes and at the same time, to protect against node and process failures. Results demonstrate the flexibility of a discrete-event simulation approach for studying the network behavior under failure conditions and various protocol parameters, message types, and routing algorithms.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4215409,no,no,1486912010.268939
Adequate and Precise Evaluation of Quality Models in Software Engineering Studies,"Many statistical techniques have been proposed and introduced to predict fault-proneness of program modules in software engineering. Choosing the """"best"""" candidate among many available models involves performance assessment and detailed comparison. But these comparisons are not simple due to varying performance measures and the related verification and validation cost implications. Therefore, a methodology for precise definition and evaluation of the predictive models is still needed. We believe the procedure we outline here, if followed, has a potential to enhance the statistical validity of future experiments.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273257,no,no,1486912010.268938
A framework for distributed fault management using intelligent software agents,"This paper proposes a framework for distributed management of network faults by software agents. Intelligent network agents with advanced reasoning capabilities address many of the issues for the distribution of processing and control in network management. The agents detect, correlate and selectively seek to derive a clear explanation of alarms generated in their domain. The causal relationship between faults and their effects is presented as a Bayesian network. As evidence (alarms) is gathered, the probability of the presence of any particular fault is strengthened or weakened. Agents having a narrower view of the network forward their findings to another with a much broader view of the network. Depending on the network's degree of automation, the agent can carry out local recovery actions. A prototype reflecting the ideas discussed in this paper is under implementation.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1226015,no,no,1486912010.268937
Managing a relational database with intelligent agents,"A prototype relational database system was developed that has indexing capability, which threads into data acquisition and analysis programs used by a wide range of researchers. To streamline the user interface and table design, free-formatted table entries were used as descriptors for experiments. This approach potentially could increase data entry errors, compromising system index and retrieval capabilities. A methodology of integrating intelligent agents with the relational database was developed to cleanse and improve the data quality for search and retrieval. An intelligent agent was designed using JACK<sup>TM</sup> (Agent Oriented Software Group) and integrated with an Oracle-based relational database. The system was tested by triggering agent corrective measures and was found to improve the quality of the data entries. Wider testing protocols and metrics for assessing its performance are subjects for future studies. This methodology for designing intelligent-based database systems should be useful in developing robust large-scale database systems.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1428468,no,no,1486912010.268935
Modeling clones evolution through time series,"The actual effort to evolve and maintain a software system is likely to vary depending on the amount of clones (i.e., duplicated or slightly different code fragments) present in the system. This paper presents a method for monitoring and predicting clones evolution across subsequent versions of a software system. Clones are firstly identified using a metric-based approach, then they are modeled in terms of time series identifying a predictive model. The proposed method has been validated with an experimental activity performed on 27 subsequent versions of mSQL, a medium-size software system written in C. The time span period of the analyzed mSQL releases covers four years, from May 1995 (mSQL 1.0.6) to May 1999 (mSQL 2. 0. 10). For any given software release, the identified models was able to predict the clone percentage of the subsequent release with an average error below 4 %. A higher prediction error was observed only in correspondence of major system redesign",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972740,no,no,1486912010.268934
Applications of fuzzy-logic-wavelet-based techniques for transformers inrush currents identification and power systems faults classification,"The advent of wavelet transforms (WTs) and fuzzy-inference mechanisms (FIMs) with the ability of the first to focus on system transients using short data windows and of the second to map complex and nonlinear power system configurations provide an excellent tool for high speed digital relaying. This work presents a new approach to real-time fault classification in power transmission systems, and identification of power transformers magnetising inrush currents using fuzzy-logic-based multicriteria approach Omar A.S. Youssef [2004, 2003] with a wavelet-based preprocessor stage Omar A.S. Youssef [2003, 2001]. Three inputs, which are functions of the three line currents, are utilised to detect fault types such as LG, LL, LLG as well as magnetising inrush currents. The technique is based on utilising the low-frequency components generated during fault conditions on the power system and/or magnetising inrush currents. These components are extracted using an online wavelet-based preprocessor stage with data window of 16 samples (based on 1.0 kHz sampling rate and 50 Hz power frequency). Generated data from the simulation of an 330 /33Y kV, step-down transformer connected to a 330 kV model power system using EMTP software were used by the MATLAB program to test the performance of the technique as to its speed of response, computational burden and reliability. Results are shown and they indicate that this approach can be used as an effective tool for high-speed digital relaying, and that computational burden is much simpler than the recently postulated fault classification.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1397423,no,no,1486912010.268932
Teraflops supercomputer: architecture and validation of the fault tolerance mechanisms,"Intel Corporation developed the Teraflops supercomputer for the US Department of Energy (DOE) as part of the Accelerated Strategic Computing Initiative (ASCI). This is the most powerful computing machine available today, performing over two trillion floating point operations per second with the aid of more than 9,000 Intel processors. The Teraflops machine employs complex hardware and software fault/error handling mechanisms for complying with DOE's reliability requirements. This paper gives a brief description of the system architecture and presents the validation of the fault tolerance mechanisms. Physical fault injection at the IC pin level was used for validation purposes. An original approach was developed for assessing signal sensitivity to transient faults and the effectiveness of the fault/error handling mechanisms. Dependency between fault/error detection coverage and fault duration was also determined. Fault injection experiments unveiled several malfunctions at the hardware, firmware, and software levels. The supercomputer performed according to the DOE requirements after corrective actions were implemented. The fault injection approach presented in this paper can be used for validation of any fault-tolerant or highly available computing system",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=869320,no,no,1486912010.268929
Testing of User-Configurable Software Systems Using Firewalls,"User-configurable software systems present many challenges to software testers. These systems are created to address a large number of possible uses, each of which is based on a specific configuration. As configurations are made up of groups of configurable elements and settings, a huge number of possible combinations exist. Since it is infeasible to test all configurations before release, many latent defects remain in the software once deployed. An incremental testing process is presented to address this problem, including examples of how it can be used with various user-configurable systems in the field. The proposed solution is evaluated with a set of empirical studies conducted on two separate ABB software systems using real customer configurations and changes. The three case studies analyzed failures reported by many different customers around the world and show that this incremental testing process is effective at detecting latent defects exposed by customer configuration changes in user-configurable systems.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700322,no,no,1486912009.605319
A Comprehensive Empirical Study of Count Models for Software Fault Prediction,"Count models, such as the Poisson regression model, and the negative binomial regression model, can be used to obtain software fault predictions. With the aid of such predictions, the development team can improve the quality of operational software. The zero-inflated, and hurdle count models may be more appropriate when, for a given software system, the number of modules with faults are very few. Related literature lacks quantitative guidance regarding the application of count models for software quality prediction. This study presents a comprehensive empirical investigation of eight count models in the context of software fault prediction. It includes comparative hypothesis testing, model selection, and performance evaluation for the count models with respect to different criteria. The case study presented is that of a full-scale industrial software system. It is observed that the information obtained from hypothesis testing, and model selection techniques was not consistent with the predictive performances of the count models. Moreover, the comparative analysis based on one criterion did not match that of another criterion. However, with respect to a given criterion, the performance of a count model is consistent for both the fit, and test data sets. This ensures that, if a fitted model is considered good based on a given criterion, then the model will yield a good prediction based on the same criterion. The relative performances of the eight models are evaluated based on a one-way anova model, and Tukey's multiple comparison technique. The comparative study is useful in selecting the best count model for estimating the quality of a given software system",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4220784,no,no,1486912009.605318
A Critical Analysis of Empirical Research in Software Testing,"In the foreseeable future, software testing will remain one of the best tools we have at our disposal to ensure software dependability. Empirical studies are crucial to software testing research in order to compare and improve software testing techniques and practices. In fact, there is no other way to assess the cost-effectiveness of testing techniques, since all of them are, to various extents, based on heuristics and simplifying assumptions. However, when empirically studying the cost and fault- detection rates of a testing technique, a number of validity issues arise. Further, there are many ways in which empirical studies can be performed, ranging from simulations to controlled experiments with human subjects. What are the strengths and drawbacks of the various approaches? What is the best option under which circumstances? This paper presents a critical analysis of empirical research in software testing and will attempt to highlight and clarify the issues above in a structured and practical manner.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343726,no,no,1486912009.605316
Using Statistical Models to Predict Software Regressions,"Incorrect changes made to the stable parts of a software system can cause failures - software regressions. Early detection of faulty code changes can be beneficial for the quality of a software system when these errors can be fixed before the system is released. In this paper, a statistical model for predicting software regressions is proposed. The model predicts risk of regression for a code change by using software metrics: type and size of the change, number of affected components, dependency metrics, developerpsilas experience and code metrics of the affected components. Prediction results could be used to prioritize testing of changes: the higher is the risk of regression for the change, the more thorough testing it should receive.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700331,no,no,1486912009.605315
A Comparative Evaluation of Using Genetic Programming for Predicting Fault Count Data,"There have been a number of software reliability growth models (SRGMs) proposed in literature. Due to several reasons, such as violation of models' assumptions and complexity of models, the practitioners face difficulties in knowing which models to apply in practice. This paper presents a comparative evaluation of traditional models and use of genetic programming (GP) for modeling software reliability growth based on weekly fault count data of three different industrial projects. The motivation of using a GP approach is its ability to evolve a model based entirely on prior data without the need of making underlying assumptions. The results show the strengths of using GP for predicting fault count data.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668139,no,no,1486912009.605314
A cognitive complexity metric based on category learning,"Software development is driven by software comprehension. Controlling a software development process is dependent on controlling software comprehension. Measures of factors that influence software comprehension are required in order to achieve control. The use of high-level languages results in many different kinds of lines of code that require different levels of comprehension effort. As the reader learns the set of arrangements of operators, attributes and labels particular to an application, comprehension is eased as familiar arrangements are repeated. Elements of cognition that describe the mechanics of comprehension serve as a guide to assessing comprehension demands in the understanding of programs written in high level languages. A new metric, kinds of lines of code identifier density is introduced and a case study demonstrates its application and importance. Related work is discussed.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1225966,no,no,1486912009.605312
An empirical study on reliability modeling for diverse software systems,"Reliability and fault correlation are two main concerns for design diversity, yet empirical data are limited in investigating these two. In previous work, we conducted a software project with real-world application for investigation on software testing and fault tolerance for design diversity. Mutants were generated by injecting one single real fault recorded in the software development phase to the final versions. In this paper, we perform more analysis and experiments on these mutants to evaluate and investigate the reliability features in diverse software systems. We apply our project data on two different reliability models and estimate the reliability bounds for evaluation purpose. We also parameterize fault correlations to predict the reliability of various combinations of versions, and compare three different fault-tolerant software architectures.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383112,no,no,1486912009.605311
Visualization for Software Evolution Based on Logical Coupling and Module Coupling,"In large scale software projects, developers make much software during long term. The source codes of software are frequently revised in the projects. The source codes evolve to become complex. Measurements of software complexity have been proposed, such as module coupling and logical coupling. In the case of the module coupling, even if developers copy pieces of source codes to a new module, the module coupling can not detect relationship between the pieces of the source codes although the pieces of the two modules have strong coupling. On the other hand, in the logical coupling, if two modules are accidentally revised at same time by a same developer, the logical coupling will judge strong coupling between the two modules although the modules have no relation. Therefore, we proposed a visualization technique and software complexity metrics for software evolution. A basic idea is that modules including strong module coupling should have strong logical coupling. If a gap between a set of modules including strong module couplings and a set of modules including strong logical couplings is large, the software complexity will be large. In addition, our visualization technique helps developers understand changes of software complexity. As a result of experiments in open source projects, we confirmed that the proposed metrics and visualization technique were able to detect high risky project with many bugs.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425857,no,no,1486912009.605309
The framework of a web-enabled defect tracking system,"This paper presents an evaluation and investigation of issues to implement a defect management system; a tool used to understand and predict software product quality and software process efficiency. The scope is to simplify the process of defect tracking through a web-enabled application. The system will enable project management, development, quality assurance and software engineer to track and manage problem specifically defects in the context of software project. A collaborative function is essential as this will enable users to communicate in real time mode. This system makes key defect tracking coordination and information available disregards the geographical and time factor.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1292958,no,no,1486912009.605307
NFTAPE: a framework for assessing dependability in distributed systems with lightweight fault injectors,"Many fault injection tools are available for dependability assessment. Although these tools are good at injecting a single fault model into a single system, they suffer from two main limitations for use in distributed systems: (1) no single tool is sufficient for injecting all necessary fault models; (2) it is difficult to port these tools to new systems. NFTAPE, a tool for composing automated fault injection experiments from available lightweight fault injectors, triggers, monitors, and other components, helps to solve these problems. We have conducted experiments using NFTAPE with several types of lightweight fault injectors, including driver-based, debugger-based, target-specific, simulation-based, hardware-based, and performance-fault injections. Two example experiments are described in this paper. The first uses a hardware fault injector with a Myrinet LAN; the other uses a Software Implemented Fault Injection (SWIFI) fault injector to target a space-imaging application",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839467,no,no,1486912009.605305
NAFIPS 2004. 2004 Annual Meeting of the North American Fuzzy Information Processing Society (IEEE Cat. No.04TH8736),The following topics are dealt with: Web intelligence and world knowledge; reverse engineering software architecture using rough clusters; fuzzy logic to assist the planning in adolescent idiopathic scoliosis instrumentation surgery; fuzzy modeling estimation of mercury by wetland components; fuzzy logic aircraft environment controller; granular jointree probability propagation; data granulation and formal concept analysis; robust fuzzy clustering of relational data; genetic fuzzy decision agent based on personal ontology for meeting scheduling support system; soft computing agents for e-health: a prototype glaucoma monitoring; soft semantic Web services agent; interpolated linguistic terms; a fuzzy based method for classifying semantically equivalent spatial data sets in spatial database queries; indexing mechanisms to query FMBRs; A parallel fuzzy C-mean algorithm for image segmentation; fuzzy sliding mode control for a singularly perturbed systems; robust tuning for disturbance rejection of PID Controller using evolutionary algorithm; hierarchical genetic algorithms for fuzzy system optimization in intelligent control; rule extraction from a trained neural network for image keywords extraction; license plate recognition system using hybrid neural networks; pattern recognition using the fuzzy Sugeno integral for response integration in modular neural networks; ANFIS Based fault diagnosis for voltage-fed PWM motor drive systems; urban land development based on possibility theory; a fuzzy approach to segmenting the breast region in mammograms.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1337342,no,no,1486912008.947411
Quantitative studies in software release planning under risk and resource constraints,"Delivering software in an incremental fashion implicitly reduces many of the risks associated with delivering large software projects. However, adopting a process, where requirements are delivered in releases means decisions have to be made on which requirements should be delivered in which release. This paper describes a method called EVOLVE+, based on a genetic algorithm and aimed at the evolutionary planning of incremental software development. The method is initially evaluated using a sample project. The evaluation involves an investigation of the tradeoff relationship between risk and the overall benefit. The link to empirical research is two-fold: firstly, our model is based on interaction with industry and randomly generated data for effort and risk of requirements. The results achieved this way are the first step for a more comprehensive evaluation using real-world data. Secondly, we try to approach uncertainty of data by additional computational effort providing more insight into the problem solutions: (i) effort estimates are considered to be stochastic variables following a given probability function; (ii) instead of offering just one solution, the L-best (L > 1) solutions are determined. This provides support in finding the most appropriate solution, reflecting implicit preferences and constraints of the actual decision-maker. Stability intervals are given to indicate the validity of solutions and to allow the problem parameters to be changed without adversely affecting the optimality of the solution.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237987,no,no,1486912008.94741
Fast Mode Decision for H.264/AVG using Mode and RD Cost Prediction,"In an H.264/AVC encoder, each macroblock can be coded in one of a large number of coding modes, which requires a huge computational effort. In this paper, we present a new method to speed up the mode decision process using RD cost prediction in addition to mode prediction. In general, video coding exploits spatial and temporal redundancies between video blocks, in particular temporal redundancy is a crucial key to compress a video sequence with little loss of image quality. The proposed method determines the best coding mode of a given macroblock by predicting the mode and its rate-distortion (RD) cost from neighboring MBs in time and space. Compared to the H.264/AVC reference software, the simulation results show that the proposed method can save about 60% of the number of RD cost computations resulting in up to 57% total encoding time reduction with up to 3.5% bit rate increase at the same PSNR.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4156466,no,no,1486912008.947408
Using software tools and metrics to produce better quality test software,"Automatic test equipment (ATE) software is often written by test equipment engineers without professional software training. This may lead to poor designs and an excessive number of defects. The Naval Surface Warfare Center (NSWC), Corona Division, as the US Navy's recognized authority on test equipment assessment, has reviewed a large number of test software programs. As an aid in the review process, various software tools have been used such as PC-lint<sup>TM</sup> or Understand for C++<sup>TM</sup>. This paper focus on software tools for C compilers since C is the most error prone language in use today. The McCabe cyclomatic complexity metric and the Halstead complexity measures are just two of the ways to measure """"software quality"""". Applying the best practices of industry including coding standards, software tools, configuration management and other practices produce better quality code in less time. Good quality code would also be easier to write, understand, maintain and upgrade.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436853,no,no,1486912008.947407
A New Topology of Fault-current Limiter and its control strategy,"In this paper a new type of fault current limiter based on DC reactor with using superconductor are presented. In normal operation condition the limiter has no obvious effect on loads. When fault happens, the bypass AC reactor and series resistor will insert the fault line automatically to limit the short circuit current, when the control circuit detects a short circuit fault, the solid state bridge in fault line works as an inverter and is closed as soon as possible. Subsequently the fault current is fully limited by the bypass AC reactor and series resistor. The magnitude of L<sub>ac</sub> and r<sub>ac</sub> must be equal with protected load. By using the electro-magnetic transients in DC systems which are the simulator of electric networks (EMTDC) software we carried out analysis of the voltage and current waveforms for fault conditions. Waveforms are considered in calculating the voltage drop at substation during the fault. The analysis used in selecting an appropriate inductance value for designing",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4108772,no,no,1486912008.947405
Design and Analyses on Permanent Magnet Actuator for Mining Vacuum Circuit Breaker,"A novel permanent magnet actuator (PMA) for mining vacuum circuit breaker (VCB) is presented in this paper. Which is monostable, has two coils and able to break the VCB when the fault of low-voltage happened. And can detect the voltage variation in main circuit at each instant. When the fault of low-voltage happened, it can automatically break without additional detection and control of apparatus. Moreover, the different states and parameters of breaking and closing courses have been numerical computed and analyzed by adopting ANSOFT software. Based on the simulation results, the prototype is manufactured and assembled in the mining VCB. The feasibility and validity of the proposed PMA have been proved by testing results",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4194931,no,no,1486912008.947404
Multiple information fusion of aluminum alloy resistance spot welding based on principal component analysis,"The monitoring of aluminum alloy resistance spot welding (RSW) is realized by distributed multiple sensor synchronous collection system and the data processing software is also developed by using LABVIEW graphical language. Statistical analysis has been applied to investigate the relationship between the extracted features and the RSW quality. The results show that the expulsion in spot welding is related to the notching curve of voltage and electrode displacement signal. Moreover, there is a correlation between the high frequency impulse amplitude and duration of the electrode force signal and the expulsion strength, and three features simultaneously or separately occur according to the expulsion strength in spot welding. Resistance spot welding quality can be assessed by nine features of high Signal-to-Noise ratio, and these may be the base of on-line quality classification of aluminum alloy spot welding in future. Furthermore, using principal component analysis (PCA) may implement the information fusion and data compression. The percentage of spot welding quality classification accuracy can reach 98%.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4697439,no,no,1486912008.947403
Impact of fault management server and its failure-related parameters on high-availability communication systems,"In this paper, we investigate the impact of a fault management server and its failure-related parameters on high-availability communication systems. The key point is that, to achieve high overall availability of a communication system, the availability of the fault management server itself is not as important as its fail-safe ratio and fault coverage. In other words, in building fault management servers, more attention should be paid to improving the server's ability of detecting faults in functional units and its own isolation under failure from the functional units. Tradeoffs can be made between the availability of the fault management server, the fail-safe ratio and the fault coverage ratio to optimize system availability. A cost-effective design for the fault management server is proposed in this paper.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1029013,no,no,1486912008.947401
Invited Talk: The Role of Empiricism in Improving the Reliability of Future Software,"This talk first bemoan the general absence of empiricism in the evolution of software system building and then go on to show the results of some experiments in attempting to understand how defects appear in software, what factors affect their appearance and their relationship to testing generally. It challenge a few cherished beliefs on the way and demonstrate in no particular order at least the following: 1) the equilibrium state of a software system appears to conserve defect; 2) there is strong evidence in quasi-equilibrated systems for xlogx growth in defects where x is a measure of the lines of code; 3) component sizes in OO and non-OO software systems appear to be scale-free, (this is intimately related to the first two bullet points); 4) software measurements, (also known rather inaccurately as metrics) are effectively useless in determining the defect behaviour of a software system; 5) most such measurements, (including the ubiquitous cyclomatic complexity) are almost as highly correlated with lines of code as the relationship between temperature in degrees Fahrenheit and degrees Centigrade measured with a slightly noisy thermometer. In other words, lines of code are just about as good as anything else when estimating defects; 6) 'gotos considered irrelevant'. The goto statement has no obvious relationship with defects even when studied over very long periods. It probably never did; 7) checklists in code inspections appear to make no significant difference to the efficiency of the inspection; and 8) when you find a defect, there is an increasing probability of finding another in the same component. This strategy is effective up to a surprisingly large number of defects in youthful systems but not at all in elderly systems.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670293,no,no,1486912008.9474
Assessing Fault Sensitivity in MPI Applications,"Today, clusters built from commodity PCs dominate high-performance computing, with systems containing thousands of processors now being deployed. As node counts for multi-teraflop systems grow to thousands and with proposed petaflop system likely to contain tens of thousands of nodes, the standard assumption that system hardware and software are fully reliable becomes much less credible. Concomitantly, understanding application sensitivity to system failures is critical to establishing confidence in the outputs of large-scale applications. Using software fault injection, we simulated single bit memory errors, register file upsets and MPI message payload corruption and measured the behavioral responses for a suite of MPI applications. These experiments showed that most applications are very sensitive to even single errors. Perhaps most worrisome, the errors were often undetected, yielding erroneous output with no user indicators. Encouragingly, even minimal internal application error checking and program assertions can detect some of the faults we injected.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1392967,no,no,1486912008.947397
Classification of Electrical Disturbances in Real Time Using Neural Networks,"Power-quality (PQ) monitoring is an essential service that many utilities perform for their industrial and larger commercial customers. Detecting and classifying the different electrical disturbances which can cause PQ problems is a difficult task that requires a high level of engineering knowledge. This paper presents a novel system based on neural networks for the classification of electrical disturbances in real time. In addition, an electrical pattern generator has been developed in order to generate common disturbances which can be found in the electrical grid. The classifier obtained excellent results (for both test patterns and field tests) thanks in part to the use of this generator as a training tool for the neural networks. The neural system is integrated on a software tool for a PC with hardware connected for signal acquisition. The tool makes it possible to monitor the acquired signal and the disturbances detected by the system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4265702,no,no,1486912008.289521
Wire length prediction based clustering and its application in placement,"In this paper, we introduce a metric to evaluate proximity of connected elements in a netlist. Compared to connectivity by S. Hauck and G. Borriello (1997) and edge separability by J. Cong and S.K. Lim (2000), our metric is capable of predicting short connections more accurately. We show that the proposed metric can also predict relative wire length in multipin nets. We develop a fine-granularity clustering algorithm based on the new metric and embed it into the Fast Placer Implementation (FPI) framework by B. Hu and M. Marek-Sadowska (2003). Experimental results show that the new clustering algorithm produces better global placement results than the net absorption of Hu and M. Marek-Sadowska (2003) algorithm, connectivity of S. Hauck and G. Borriello (1997), and edge separability of J. Cong and S.K. Lim (2000) based algorithms. With the new clustering algorithm, FPI achieves up to 50% speedup compared to the latest version of Capo8.5 in http://vlsicad.ucsd.edu/Resources/SoftwareLinks/PDtools/, without placement quality losses.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1219128,no,no,1486912008.28952
Predicting class testability using object-oriented metrics,"We investigate factors of the testability of object-oriented software systems. The starting point is given by a study of the literature to obtain both an initial model of testability and existing OO metrics related to testability. Subsequently, these metrics are evaluated by means of two case studies of large Java systems for which JUnit test cases exist. The goal of This work is to define and evaluate a set of metrics that can be used to assess the testability of the classes of a Java system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1386167,no,no,1486912008.289518
Analyzing Java software by combining metrics and program visualization,"Shimba, a prototype reverse engineering environment, has been built to support the understanding of Java software. Shimba uses Rigi and SCED to analyze, visualize, and explore the static and dynamic aspects, respectively, of the subject system. The static software artifacts and their dependencies are extracted from Java byte code and viewed as directed graphs using the Rigi reverse engineering environment. The static dependency graphs of a subject system can be annotated with attributes, such as software quality measures, and then be analyzed and visualised using scripts through the end user programmable interface. Shimba has recently been extended with the Chidamber and Kemerer suite of object oriented metrics. The metrics measure properties of the classes, the inheritance hierarchy, and the interaction among classes of a subject system. Since Shimba is primarily intended for the analysis and exploration of Java software, the metrics have been tailored to measure properties of software systems using a reverse engineering environment. The static dependency graphs of the system under investigation are decorated with measures obtained by applying the object oriented metrics to selected software components. Shimba provides tools to examine these measures, to find software artifacts that have values that are in a given range, and to detect correlations among different measures. The object oriented analysis of the subject Java system can be investigated further by exporting the measures to a spreadsheet",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=827328,no,no,1486912008.289517
Correlations between Internal Software Metrics and Software Dependability in a Large Population of Small C/C++ Programs,"Software metrics are often supposed to give valuable information for the development of software. In this paper we focus on several common internal metrics: Lines of Code, number of comments, Halstead Volume and McCabe's Cyclomatic Complexity. We try to find relations between these internal software metrics and metrics of software dependability: Probability of Failure on Demand and number of defects. The research is done using 59 specifications from a programming competition---The Online Judge--on the internet. Each specification provides us between 111 and 11,495programs for our analysis; the total number of programs used is 71,917. We excluded those programs that consist of a look-up table. The results for the Online Judge programs are: (1) there is a very strong correlation between Lines of Code and Hal- stead Volume; (2) there is an even stronger correlation between Lines of Code and McCabe's Cyclomatic Complexity; (3) none of the internal software metrics makes it possible to discern correct programs from incorrect ones; (4) given a specification, there is no correlation between any of the internal software metrics and the software dependability metrics.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402211,no,no,1486912008.289515
TPS precision & accuracy,"In this paper we discuss the benefits of a Test Program Set (TPS) that can precisely isolate down to the absolute lowest component of a UUT (Unit Under Test) versus a TPS's ability to accurately detect AND isolate a fault to some higher level of the UUT. We touch on the idea of not isolating down to this absolute level just because we can, but only when we need to as an optimum characteristic of the support system. As in all things, the competing ideals of theory, need and desire slam up against the boundaries of reality. Essentially, we try to view the environmental realities that a system must deploy in. The cost of complete, precise diagnostics and fault isolation is high. In this paper we discuss various trade-offs that developers and decision makers make to design, develop and deliver a TPS that supports the user in the most optimal and cost effective way",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949434,no,no,1486912008.289514
An extension of Integrated Services with active networking for providing quality of service in networks with long-range dependent traffic,"Although today's network capacity is increasing exponentially, new applications are demanding higher and higher bandwidth. The available bandwidth always seems to be less than the new applications require. This tendency results in congested networks and packet losses, and we can expect this to continue into the foreseeable future. Congestion can be caused by several factors. The most dangerous cause of congestion is the burstiness of the network traffic. Recent results make it evident that high-speed network traffic is more bursty, and its variability cannot be predicted, as was assumed previously. It has been shown that network traffic has similar statistical properties on many time scales. Traffic that is bursty on many or all time scales can be described statistically using the notion of long-range dependency. Long-range-dependent traffic has observable bursts on all time scales. Factors such as traffic burstiness make providing quality of service (QoS) in high-speed networks increasingly important. QoS implies mechanisms to avoid congestion by allocating network resources optimally, rather than continually increasing network capacities. The objective of our paper is to present an extension of a QoS mechanism called Integrated Services (IntServ) with active networking in networks with long-range-dependent traffic",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969780,no,no,1486912008.289512
Anger management [emotion recognition],"Aside from monitoring calls for quality assurance purposes, many corporate call centers require call monitoring for anger management. By identifying angry emotions in calls, managers can take appropriate action against call agents who may have behaved improperly. NICE Systems Inc., a supplier of call monitoring systems, has developed an emotion-sensitive software that is able to detect angry emotions during phone conversations using the changes in a voice's pitch. The software engine will go over the data signal, and, second by second, run the algorithm. If emotion is detected, a report is generated that includes the level of certainty that the call included angry emotions. As the software improves and the hardware gets faster, ever more calls will be scanned in ever more sophisticated ways.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1413722,no,no,1486912008.289511
Predicting testability of program modules using a neural network,"J.M. Voas (1992) defines testability as the probability that a test case will fail if the program has a fault. It is defined in the context of an oracle for the test, and a distribution of test cases, usually emulating operations. Because testability is a dynamic attribute of software, it is very computation-intensive to measure directly. The paper presents a case study of real time avionics software to predict the testability of each module from static measurements of source code. The static software metrics take much less computation than direct measurement of testability. Thus, a model based on inexpensive measurements could be an economical way to take advantage of testability attributes during software development. We found that neural networks are a promising technique for building such predictive models, because they are able to model nonlinearities in relationships. Our goal is to predict a quantity between zero and one whose distribution is highly skewed toward zero. This is very difficult for standard statistical techniques. In other words, high testability modules present a challenging prediction problem that is appropriate for neural networks",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888032,no,no,1486912008.289509
Fault prediction of boilers with fuzzy mathematics and RBF neural network,"How to predict potential faults of a boiler in an efficient and scientific way is very important. A lot of comprehensive research has been done, and promising results have been obtained, especially regarding the application of intelligent software. Still there are a lot of problems to be studied. It combines fuzzy mathematics with. RBF neural network in an intuition and natural way. Thus a new method is proposed for the prediction of the potential faults of a coal-fired boiler. The new method traces the development trend of related operation and state variables. The new method has been tested on a simulation machine. And its predicted results were compared with those of traditional statistical results. It is found that the new method has a good performance.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1495278,no,no,1486912008.289507
Assessing the Quality of Software Requirements Specifications,"Software requirements specifications (SRS) are hard to compare due to the uniqueness of the projects they were created in. In practice this means that it is not possible to objectively determine if a projects SRS fails to reach a certain quality threshold. Therefore, a commonly agreed-on quality model is needed. Additionally, a large set of empirical data is needed to establish a correlation between project success and quality levels. As there is no such quality model, we had to define our own based on the goal-question-metric (GQM) method. Based on this we analyzed more than 40 software projects (student projects in undergraduate software engineering classes), in order to contribute to the empirical part. This paper contributes in three areas: Firstly, we outline our GQM plan and our set of metrics. They were derived from widespread literature, and hence could lead to a discussion of how to measure requirements quality. Practitioners and researchers can profit from our experience, when measuring the quality of their requirements. Secondly, we present our findings. We hope that others find these valuable when comparing them to their own results. Finally,we show that the results of our quality assessment correlate to project success. Thus, we give an empirical indication for the correlation of requirements engineering and project success.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4685703,no,no,1486912007.635256
A quality control method for nuclear instrumentation and control systems based on software safety prediction,"In the case of safety-related applications like nuclear instrumentation and control (NI&C), safety-oriented quality control is required. The objective of this paper is to present a software safety classification method as a safety-oriented quality control tool. Based on this method, we predict the risk (and thus safety) of software items that are at the core of NI&C systems. Then we classify the software items according to the degree of the risk. The method can be used earlier than at the detailed design phase. Furthermore, the method can also be used in all the development phases without major changes. The proposed method seeks to utilize the measures that can be obtained from the safety analysis and requirements analysis. Using the measures proved to be desirable in a few aspects. The authors have introduced fuzzy approximate reasoning to the classification method because experts' knowledge covers the vague frontiers between good quality and bad quality with linguistic uncertainty and fuzziness. Fuzzy Colored Petri Net (FCPN) is introduced in order to offer a formal framework for the classification method and facilitate the knowledge representation, modification, or verification. Through the proposed quality control method, high-quality NI&C systems can be developed effectively and used safely",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=846274,no,no,1486912007.635254
Considering Fault Correction Lag in Software Reliability Modeling,"The fault correction process is very important in software testing, and it has been considered into some software reliability growth models (SRGMs). In these models, the time-delay functions are often used to describe the dependency of the fault detection and correction processes. In this paper, a more direct variable """"correction lag"""", which is defined as the difference between the detected and corrected fault numbers, is addressed to characterize the dependency of the two processes. We investigate the correction lag and find that it appears Bell-shaped. Therefore, we adopt the Gamma function to describe the correction lag. Based on this function, a new SRGM which includes the fault correction process is proposed. And the experimental results show that the new model gives better fit and prediction than other models.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725319,no,no,1486912007.635253
The influence of organizational structure on software quality,"Often software systems are developed by organizations consisting of many teams of individuals working together. Brooks states in the Mythical Man Month book that product quality is strongly affected by organization structure. Unfortunately there has been little empirical evidence to date to substantiate this assertion. In this paper we present a metric scheme to quantify organizational complexity, in relation to the product development process to identify if the metrics impact failure-proneness. In our case study, the organizational metrics when applied to data from Windows Vista were statistically significant predictors of failure-proneness. The precision and recall measures for identifying failure-prone binaries, using the organizational metrics, was significantly higher than using traditional metrics like churn, complexity, coverage, dependencies, and pre-release bug measures that have been used to date to predict failure-proneness. Our results provide empirical evidence that the organizational metrics are related to, and are effective predictors of failure-proneness.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814163,no,no,1486912007.635251
A Heuristic Approach for Predicting Fault Locations in Distribution Power Systems,"The first step in restoring systems after a fault is detected, is determining the fault location. The large number of candidate locations for the fault makes this a complex process. Knowledge based methods have the capability to accomplish this quickly and reliably. In this paper, a heuristic approach has been used to predict potential fault locations. A software tool implements the heuristic rules and a genetic algorithm based search. The implementation and evaluation results of this tool have been presented.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4201307,no,no,1486912007.63525
Detection and repair of software errors in hierarchical sensor networks,"Sensor networks are being increasingly deployed for collecting critical data in various applications. Once deployed, a sensor network may experience faults at the individual node level or at an aggregate network level due to design errors in the protocol, implementation errors, or deployment conditions that are significantly different from the target environment. In many applications, the deployed system may fail to collect data in an accurate, complete, and timely manner due to such errors. If the network produces incorrect data, the resulting decisions on the data may be incorrect, and negatively impact the application. Hence, it is important to detect and diagnose these faults through run-time observation. Existing technologies face difficulty with wireless sensor networks due to the large scale of the networks, the resource constraints of bandwidth and energy on the sensing nodes, and the unreliability of the observation channels for recording the behavior. This paper presents a semi-automatic approach named H-SEND (hierarchical sensor network debugging) to observe the health of a sensor network and to remotely repair errors by reprogramming through the wireless network. In H-SEND, a programmer specifies correctness properties of the protocol (""""invariants""""). These invariants are associated with conditions (the """"observed variables"""") of individual nodes or the network. The compiler automatically inserts checking code to ensure that the observed variables satisfy the invariants. The checking can be done locally or remotely, depending on the nature of the invariant. In the latter case, messages are generated automatically. If an error is detected at run-time, the logs of the observed variables are examined to analyze and correct the error. After errors are corrected, new programs or patches can be uploaded to the nodes through the wireless network. We construct a prototype to demonstrate the benefit of run-time detection and correction",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1636206,no,no,1486912007.635248
Hardware and software implementation of a travelling wave based protection relay,"Fault generated transient signals can undoubtedly be employed to achieve ultra-high-speeds in transmission line protection. Even though such transient based schemes can quickly detect a faulty state on the power system, they face several challenges. Firstly, they have inherent reliability problems and secondly, it is difficult to implement them as a real-time product due to their excessive demand for high speed signal acquisition and processing. This paper examines the theoretical aspects and design procedure of a reliable, high speed protection scheme based on fault generated travelling wave information.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1489216,no,no,1486912007.635247
A fault-tolerant approach to network security,"Summary form only given. The increasing use of the Internet, especially for internal and business-to-business applications has resulted in the need for increased security for all networked systems to avoid unauthorized access and use. A failure of network security can effectively close the business, its availability is vital to operations. Vital functions such as firewalls and VPNs must remain in operation without loss of time for fallover, without loss of data and must be able to be placed even at remote locations where support personnel may not be readily available. Network firewalls are the first, and often are the only, line of defense against an attack. However, the firewall can be a double-edged sword. In operation, the firewall protects the network from everything from Denial of Service attacks to the entry of known viruses and unauthorized intrusion. If the firewall falls, there are generally only two options: Leave the network open to all or shut down access by anyone. The default condition is to close everything off, but this can be as disastrous as leaving the network open. Due to the importance of the firewall, most leading firewall software provides some method of establishing a form of fail-over redundancy for high availability. Yet in most cases this means some form of clustering using a secondary system as a backup with specialty software to detect and respond to a failure of the primary firewall. Such a clustered approach introduces additional complexity when establishing and configuring the firewall and additional complexity when upgrading. It also adds dramatically to the cost, not only in the hardware for the firewall, but in additional software copies and in the expertise for clustering support software required to establish and maintain the cluster. The approach we will discuss examines the creation of network security based on a hardware approach to fault tolerance. This approach will dramatically reduce the system complexity, simultaneously eliminating the need for special clustering software and special expertise for configuring the system for the kind of continuous availability that is the objective of the network security application. In addition, because the hardware approach is something that is designed in from the inception of the system, there are additional advantages. The fault tolerance is not an afterthought, but rather the purpose of the hardware, meaning that the system can be made to function very smoothly with very little administration. Failure of a part of the system is seamlessly recovered by the redundant elements, without loss of data in memory or loss of state for the system. In sum, this paper discusses the ability to create network security that reaches the standard of being continuously available, what is often referred to as the """"Holy Grail of reliability,"""" 99.999% uptime",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962536,no,no,1486912007.635245
BP-Neural Network Used to Choose the Style of Graphic-Interface of Application Program,"With the interface from words to graphic, the quality of application program is more and more depend on the degree which the graphic interface feat the taste of the users. So if we can predict the users' taste of the style of application program, our work must be more popular. Now with the help of BP-neural network we can do it because of its strong capacity of prediction.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4770047,no,no,1486912007.635244
Predicting software escalations with maximum ROI,"Enterprise software vendors often have to release software products before all reported defects are corrected, and a small number of these reported defects will be escalated by customers whose businesses are seriously impacted. Escalated defects must be quickly resolved at a high cost by the software vendors. The total costs can be even greater, including loss of reputation, satisfaction, loyalty, and repeat revenue. In this paper, we develop an Escalation Prediction (EP) system to mine historic defect report data and predict the escalation risk of current defect reports for maximum ROI (Return On Investment). More specifically, we first describe a simple and general framework to convert the maximum ROI problem to cost-sensitive learning. We then apply and compare several best-known cost-sensitive learning approaches for EP. The EP system has produced promising results, and has been deployed in the product group of an enterprise software vendor. Conclusions drawn from this study also provide guidelines for mining imbalanced datasets and cost-sensitive learning.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1565765,no,no,1486912007.635241
Fault Tolerant ICAP Controller for High-Reliable Internal Scrubbing,"High reliable reconfigurable applications today require system platforms that can easily and quickly detect and correct single event upsets. This capability, however, can be costly for FPGAs. This paper demonstrates a technique for detecting and repairing SEUs within the configuration memory of a Xilinx Virtex-4 FPGA using the ICAP interface. The internal configuration access port (ICAP) provides a port internal to the FPGA for configuring the FPGA device. An application note demonstrates how this port can be used for both error injection and scrubbing (L. Jones, 2007). We have extended this work to create a fault tolerant ICAP scrubber by triplicating the internal ICAP circuit using TMR and block memory scrubbing. This paper will describe the costs, benefits, and reliability of this fault-tolerant ICAP controller.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526471,no,no,1486912006.982712
Interval Quality: Relating Customer-Perceived Quality to Process Quality,"We investigate relationships among software quality measures commonly used to assess the value of a technology, and several aspects of customer perceived quality measured by interval quality (IQ): a novel measure of the probability that a customer will observe a failure within a certain interval after software release. We integrate information from development and customer support systems to compare defect density measures and IQ for six releases of a major telecommunications system. We find a surprising negative relationship between the traditional defect density and IQ. The four years of use in several large telecommunication products demonstrates how a software organization can control customer perceived quality not just during development and verification, but also during deployment by changing the release rate strategy and by increasing the resources to correct field problems rapidly. Such adaptive behavior can compensate for the variations in defect density between major and minor releases.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814186,no,no,1486912006.982711
Keynote: Hierarchical Fault Detection in Embedded Control Software,"We propose a two-tiered hierarchical approach for detecting faults in embedded control software during their runtime operation: The observed behavior is monitored against the appropriate specifications at two different levels, namely, the software level and the controlled-system level. (The additional controlled- system level monitoring safeguards against any possible incompleteness at the software level monitoring.) A software fault is immediately detected when an observed behavior is rejected by a software level monitor. In contrast, when a system level monitor rejects an observed behavior it indicates a system level failure, and an additional isolation step is required to conclude whether a software fault occurred. This is done by tracking the executed behavior in the system model comprising of the models for the software and those for the nonfaulty hardware components: An acceptance by such a model indicates the presence of a software fault. The design of both the software-level and system-level monitors is modular and hence scalable (there exists one monitor for each property), and further the monitors are constructed directly from the property specifications and do not require any software or system model. Such models are required only for the fault isolation step when the detection occurs at the system level. We use input-output extended finite automata (I/O- EFA) for software as well as system level modeling, and also for modeling the property monitors. Note since the control changes only at the discrete times when the system/environment states are sampled, the controlled- system has a discrete-time hybrid dynamics which can be modeled as an I/O-EFA.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591674,no,no,1486912006.982709
Empirical validation of object-oriented metrics on open source software for fault prediction,"Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542070,no,no,1486912006.982708
Fault Detection In PCB Using Homotopic Morphological Operator,"Homotopic morphological image processing software solution is developed for detection of hair cracks in PCB, which cannot be seen by naked eye. The proposed software solution is implemented using basic morphological operations like dilation, erosion, opening, closing, hit-miss transform, thinning, thickening skeletonizing and pruning. The software solution also performs image enhancement operations using mathematical morphology. The proposed software solution can be extended to detect thin bone fractures and real time automatic PCB fault detection",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4086266,no,no,1486912006.982707
The improvement on the measuring precision of detecting fault composite insulators by using electric field mapping,"According to the statistics of the fault composite insulators, most of the defects take place at the high voltage (HV) end of the insulators. At present, the minimum defect length detected possibly at HV end of the insulators is about 7 cm by using electric field mapping device, which could not indicate the defects located between the last shed and the HV electrode. Therefore, it is important to improve the measuring precision that is suitable for indicating the defects less than 7 cm in inspecting the fault composite insulators based on electric field mapping device. In order to enhance the measuring precision of the device, we analyzed the electric field distribution along with an insulator by using the commercial software ANSYS. We found that a 5 cm defect can be found if we collect two to three electric field data between the two sheds. Therefore, we added a photoelectric cell array to trigger the device for collecting more data between the two sheds. The tests were conducted in our laboratory by using our new device. The results from our experiments show that the sensitivity of detecting the defects is increased and our new device can indicate the defects less than 5 cm at the HV end without grading rings.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1560677,no,no,1486912006.982705
Data Quality Monitoring Framework for the ATLAS Experiment at the LHC,"Data quality monitoring (DQM) is an important and integral part of the data taking process of HEP experiments. DQM involves automated analysis of monitoring data through user-defined algorithms and relaying the summary of the analysis results while data is being processed. When DQM occurs in the online environment, it provides the shifter with current run information that can be used to overcome problems early on. During the offline reconstruction, more complex analysis of physics quantities is performed by DQM, and the results are used to assess the quality of the reconstructed data. The ATLAS data quality monitoring framework (DQMF) is a distributed software system providing DQM functionality in the online environment. The DQMF has a scalable architecture achieved by distributing execution of the analysis algorithms over a configurable number of DQMF agents running on different nodes connected over the network. The core part of the DQMF is designed to only have dependence on software that is common between online and offline (such as ROOT) and therefore is used in the offline framework as well. This paper describes the main requirements, the architectural design, and the implementation of the DQMF.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4382779,no,no,1486912006.982704
Identifying noise in an attribute of interest,"One of the most significant issues facing the data mining community is that of low-quality data. Real-world datasets are often inundated with various types of data integrity issues, particularly noisy data. In response to the difficulties created by low-quality data, we propose a novel technique to detect noisy instances relative to an attribute of interest (AOI). Any attribute in the dataset can be defined by the user as the attribute of interest. A noise ranking of instances relative to the chosen attribute is output. This approach can be iterated for any number of user-specified attributes of interest. The case study described in this work demonstrates how our technique may be used to detect class noise, which occurs when errors are present in the class or dependent variable. In this scenario the class is declared to be the attribute of interest and an instance noise ranking relative to the class is provided. Our technique is compared to the well-known ensemble and classification filters which have been previously proposed for class noise detection. The results of this study demonstrate the effectiveness of our approach and show that our procedure is a useful tool for improving data quality.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607431,no,no,1486912006.982702
A pragmatic approach to managing APC FDC in high volume logic production,"At Infineon Technologies APC fault detection is now implemented in many process areas in its high volume fabs. With the APC Software """"APC-Trend"""" process engineers and maintenance can detect and classify anomalies in machine and process parameters and supervise them on the basis of an automated alarming system. An overview of the current usage of APC FDC at Infineon is given.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194511,no,no,1486912006.9827
Assessment of a New High-Performance Small-Animal X-Ray Tomograph,"We have developed a new X-ray cone-beam tomograph for in vivo small-animal imaging using a flat panel detector (CMOS technology with a microcolumnar CsI scintillator plate) and a microfocus X-ray source. The geometrical configuration was designed to achieve a spatial resolution of about 12 lpmm with a field of view appropriate for laboratory rodents. In order to achieve high performance with regard to per-animal screening time and cost, the acquisition software takes advantage of the highest frame rate of the detector and performs on-the-fly corrections on the detector raw data. These corrections include geometrical misalignments, sensor non-uniformities, and defective elements. The resulting image is then converted to attenuation values. We measured detector modulation transfer function (MTF), detector stability, system resolution, quality of the reconstructed tomographic images and radiated dose. The system resolution was measured following the standard test method ASTM E 1695 -95. For image quality evaluation, we assessed signal-to-noise ratio (SNR) and contrast-to-noise ratio (CNR) as a function of the radiated dose. Dose studies for different imaging protocols were performed by introducing TLD dosimeters in representative organs of euthanized laboratory rats. Noise figure, measured as standard deviation, was 50 HU for a dose of 10 cGy. Effective dose with standard research protocols is below 200 mGy, confirming that the system is appropriate for in vivo imaging. Maximum spatial resolution achieved was better than 50 micron. Our experimental results obtained with image quality phantoms as well as with in-vivo studies show that the proposed configuration based on a CMOS flat panel detector and a small micro-focus X-ray tube leads to a compact design that provides good image quality and low radiated dose, and it could be used as an add-on for existing PET or SPECT scanners.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4545226,no,no,1486912006.982698
Applying fault correction profiles,"In general, software reliability models have focused on modeling and predicting the failure detection process and have not given equal priority to modeling the fault correction process. However, it is important to address the fault correction process in order to identify the need for process improvements. Process improvements, in turn, will contribute to achieving software reliability goals. We introduce the concept of a fault correction profile - a set of functions that predict fault correction events as a function of failure detection events. The fault correction profile identifies the need for process improvements and provides information for developing fault correction strategies. Related to the fault correction profile is the goal fault correction profile. This profile represents the fault correction goal against which the achieved fault correction profile can be compared. This comparison motivates the concept of fault correction process instability, and the attributes of instability. Applying these concepts to the NASA Goddard Space Flight Center fault correction process and its data, we demonstrate that the need for process improvement can be identified, and that improvements in process would contribute to meeting product reliability goals.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1270742,no,no,1486912006.332248
Geotechnical application of borehole GPR - a case history,"Borehole GPR measurements were performed to complement the site characterization of a planned expansion of a cement plant. This expansion includes a mill and a reclaim facility adjacent to the present buildings, the whole site being located in a karstic environment. Twenty-one geotechnical exploration borings revealed that the depth to bedrock is very irregular (between 1.5 m and 18 m) and that the rocks likely have vertical alteration channels that extend many meters below the rock surface. The purpose of the GPR survey was to reveal the presence of potential cavities and to better determine the required vertical extent of the caissons of the foundations. In general, the subsurface conditions consist of a top fill layer, which is electrically conductive, a residual clay layer and a limestone bedrock. Very poor EM penetration prevented surface measurements. Hence, 100 MHz borehole antennas were used to perform single-hole reflection and cross-hole transmission measurements. Sixteen geotechnical holes were visited during the survey. All holes were surveyed in reflection mode. Nineteen tomographic panels were scanned. Velocity tomogram were obtained for all the data. Attenuation tomography was performed in fewer occasions, due to higher uncertainty in the quality of the amplitude data. Resistivity probability maps were drawn when both velocity and attenuation data were obtained. The velocity tomography calculations were constrained using velocity profiles along the borings. These profiles were obtained by inversion of the single-hole first arrival data. The velocity tomography results show that globally the area can be separated in two zones, one with an average velocity around 0.1 m/ns and one with a slower 0.09 m/ns average velocity. In addition, low velocity anomalies attributable to weathered zones appear in the tomogram. In conclusion, the GPR results revealed no cavities. Sensitive zones were located, which helped the planning and the budgeting of the caissons - onstruction.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1343415,no,no,1486912006.332247
Enhanced DO-RE-ME based defect level prediction using defect site aggregation-MPG-D,"Predicting the final value of the defective part level after the application of a set of test vectors is not a simple problem. In order for the defective part level to decrease, both the excitation and observation of defects must occur. This research shows that the probability of exciting an as yet undetected defect does indeed decrease exponentially as the number of observations increases. In addition, a new defective part level model is proposed which accurately predicts the final defective part level (even at high fault coverages) for several benchmark circuits and which continues to provide good predictions even as changes are made an the set of test patterns applied",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894304,no,no,1486912006.332246
Effect of the Delay Time in Fixing a Fault on Software Error Models,"In this paper, we propose a new model that incorporates both the fault-detection process and the fault-correction process. In addition, the fault- correction process is modeled as a delayed fault- detection process. Significant improvements on the conventional software reliability growth models (SRGMs) to better describe the actual software development have been achieved by eliminating an unrealistic assumption that detected errors are immediately corrected. This can especially be seen when some latent software errors are hard to detect and they even exist in the software product for a long time after they are detected. Therefore, the time delayed by the correction process is not negligible. The objective here is to remove this assumption in order to make the SRGMs more realistic and accurate. Finally, two real data sets have been performed, and the results show that the proposed new model performs much better in estimating the number of initial faults.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291198,no,no,1486912006.332244
Design of a software distributed shared memory system using an MPI communication layer,"We designed and implemented a software distributed shared memory (DSM) system, SCASH-MPI, by using MPI as the communication layer of the SCASH DSM. With MPI as the communication layer, we could use high-speed networks with several clusters and high portability. Furthermore, SCASH-MPI can use high-speed networks with MPI, which is the most commonly available communication library. On the other hand, existing software DSM systems usually use a dedicated communication layer, TCP, or UDP-Ethernet. SCASH-MPI avoids the need for a large amount of pin-down memory for shared memory use that has limited the applications of the original SCASH. In SCASH-MPI, a thread is created to support remote memory communication using MPI. An experiment on a 4-node Itanium cluster showed that the Laplace Solver benchmark using SCASH-MPI achieves a performance comparable to the original SCASH. Performance degradation is only 6.3% in the NPB BT benchmark Class B test. In SCASH-MPI, page transfer does not start until a page fault is detected. To hide the latency of page transmission, we implemented a prefetch function. The latency in BT Class B was reduced by 64% when the prefetch function was used.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575830,no,no,1486912006.332243
Static analysis of XML transformations in Java,"XML documents generated dynamically by programs are typically represented as text strings or DOM trees. This is a low-level approach for several reasons: 1) traversing and modifying such structures can be tedious and error prone, 2) although schema languages, e.g., DTD, allow classes of XML documents to be defined, there are generally no automatic mechanisms for statically checking that a program transforms from one class to another as intended. We introduce XACT, a high-level approach for Java using XML templates as a first-class data type with operations for manipulating XML values based on XPath. In addition to an efficient runtime representation, the data type permits static type checking using DTD schemas as types. By specifying schemes for the input and output of a program, our analysis algorithm will statically verify that valid input data is always transformed into valid output data and that the operations are used consistently.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1271173,no,no,1486912006.332241
Vision-model-based impairment metric to evaluate blocking artifacts in digital video,"In this paper investigations are conducted to simplify and refine a vision-model-based video quality metric without compromising its prediction accuracy. Unlike other vision-model-based quality metrics, the proposed metric is parameterized using subjective quality assessment data recently provided by the Video Quality Experts Group. The quality metric is able to generate a perceptual distortion map for each and every video frame. A perceptual blocking distortion metric (PBDM) is introduced which utilizes this simplified quality metric. The PBDM is formulated based on the observation that blocking artifacts are noticeable only in certain regions of a picture. A method to segment blocking dominant regions is devised, and perceptual distortions in these regions are summed up to form an objective measure of blocking artifacts. Subjective and objective tests are conducted and the performance of the PBDM is assessed by a number of measures such as the Spearman rank-order correlation, the Pearson correlation, and the average absolute error The results show a strong correlation between the objective blocking ratings and the mean opinion scores on blocking artifacts",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=982412,no,no,1486912006.332239
Force Feature Spaces for Visualization and Classification,"Distance-preserving dimension reduction techniques can fail to separate elements of different classes when the neighborhood structure does not carry sufficient class information. We introduce a new visual technique, K-epsilon diagrams, to analyze dataset topological structure and to assess whether intra-class and inter-class neighborhoods can be distinguished. We propose a force feature space data transform that emphasizes similarities between same-class points and enhances class separability. We show that the force feature space transform combined with distance-preserving dimension reduction produces better visualizations than dimension reduction alone. When used for classification, force feature spaces improve performance of K-nearest neighbor classifiers. Furthermore, the quality of force feature space transformations can be assessed using K-epsilon diagrams.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725009,no,no,1486912006.332238
An approach of fault detection based on multi-mode,"Conventional multi-scale principal component analysis (MSPCA) only detects fault, but it canpsilat detect fault types. For these problems, a method of fault detection based on multi-mode that incorporates MSPCA into adaptive resonance (ART) neural network is presented. Firstly, this method presents a wavelet transform for samples data, and principal component analysis can be used to analyze data at each scale. Then ART is used to classify reconstruction data. It can detect fault effectively, and ART2 can classify fault using wavelet denoising easily, it separates the fault successfully in the system. At last, it develops multi-mode fault detection in autocorrelation system application through computer simulation experiment. The theory and simulation experiments shows that this method is of wide application prospect.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4605762,no,no,1486912006.332236
Fault correction profiles,"In general, software reliability models have focused on modeling and predicting the failure detection process and have not given equal priority to modeling the fault correction process. However, it is important to address the fault correction process in order to identify the need for process improvements. Process improvements, in turn, will contribute to achieving software reliability goals. We introduce the concept of a fault correction profile """" a set of functions that predict fault correction events as a function of failure detection events. The fault correction profile identifies the need for process improvements and provides information for developing fault correction strategies. Related to the fault correction profile is the goal fault correction profile. This profile represents the fault correction goal against which the achieved fault correction profile can be compared. This comparison motivates the concept of fault correction process instability, and the attributes of instability. Applying these concepts to the NASA Goddard Space Flight Center fault correction process and its data, we demonstrate that the need for process improvement can be identified, and that improvements in process would contribute to meeting product reliability goals.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251048,no,no,1486912006.332234
A ratio sensitive image quality metric,"Many applications require a fast quality measure for digitally coded images, such as the mobile multimedia communication. Although the widely used peak signal-noise ratio (PSNR) has low computation cost, it fails to predict structured errors that dominate in digital images. On the other hand, human visual system (HVS) based metrics can improve the prediction accuracy. However, they are computationally complex and time consuming. This paper proposes a very simple image quality measure defined by a ratio based mathematical formulation that attempts to simulate the scaling of the human visual sensation to brightness. The current results demonstrate that the novel image quality metric predicts the subjective ratings better and has lower computation cost than the PSNR. As an alternative to the PSNR, the proposed metric can be used for on-line or real-time picture quality assessment applications.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1439076,no,no,1486912005.678646
Sentient Networks: A New Dimension in Network Capability,"As computer networks evolve to the next generation, they offer flexible infrastructures for communication as well as high vulnerability. In order to sustain the reliability, to prevent misuse, and to provide radically new services, these networks need new capabilities in non-traditional domains of operation such as perception and consciousness. Therefore, we propose Sentient Networks, as an early approach towards realizing them. There are several applications for such networking capability. For one, sentient networks can enable packet detection in a blackbox manner, detect encryption of data transferred, detecting and classifying data types such as voice, video, and data. Some of the applications of such capabilities, for example, include the use of emotion content of the voice packets through a network and classifying these packets into normal, moderately panicked, and extremely panicked. Such classifications can be used to provide better QoS schemes for panicked callers. We obtained a packet detection accuracy of about 65-70% for data packets and about 98% accuracy for detecting encrypted TCP packets. The emotion-aware QoS provision mechanism provided approximately 60% improvement in delay performance for voice packets generated by panicked sources.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221027,no,no,1486912005.678644
Neural network modeling of distribution transformers with internal short circuit winding faults,"To detect and diagnose a transformer internal fault an efficient transformer model is required to characterize the faults for further research. This paper discusses the application of neural network (NN) techniques in the modeling of a distribution transformer with internal short-circuit winding faults. A transformer model can be viewed as a functional approximator constructing an input-output mapping between some specific variables and the terminal behaviors of the transformer. The complex approximating task was implemented using six small simple neural networks. Each small neural network model takes fault specification and energized voltage as the inputs and the output voltage or terminal currents as the outputs. Two kinds of neural networks, back-propagation feedforward network (BPFN) and radial basis function network (RBFN), were investigated to model the faults in distribution transformers. The NN models were trained offline using training sets generated by finite element analysis (FEA) models and field experiments. The FEA models were implemented using a commercial finite element analysis software package. The comparison between some simulation cases and corresponding experimental results shows that the well-trained, neural networks can accurately simulate the terminal behaviors of distribution transformers with internal short circuit faults",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=932333,no,no,1486912005.678643
A Classification-Based Fault Detection and Isolation Scheme for the Ion Implanter,"We propose a classification-based fault detection and isolation scheme for the ion implanter. The proposed scheme consists of two parts: 1) the classification part and 2) the fault detection and isolation part. In the classification part, we propose a hybrid classification tree (HCT) with learning capability to classify the recipe of a working wafer in the ion implanter, and a k-fold cross-validation error is treated as the accuracy of the classification result. In the fault detection and isolation part, we propose a warning signal generation criteria based on the classification accuracy to detect and fault isolation scheme based on the HCT to isolate the actual fault of an ion implanter. We have compared the proposed classifier with the existing classification software and tested the validity of the proposed fault detection and isolation scheme for real cases to obtain successful results",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4012104,no,no,1486912005.678641
Automatic document metadata extraction using support vector machines,"Automatic metadata generation provides scalability and usability for digital libraries and their collections. Machine learning methods offer robust and adaptable automatic metadata extraction. We describe a support vector machine classification-based method for metadata extraction from header part of research papers and show that it outperforms other machine learning methods on the same task. The method first classifies each line of the header into one or more of 15 classes. An iterative convergence procedure is then used to improve the line classification by using the predicted class labels of its neighbor lines in the previous round. Further metadata extraction is done by seeking the best chunk boundaries of each line. We found that discovery and use of the structural patterns of the data and domain based word clustering can improve the metadata extraction performance. An appropriate feature normalization also greatly improves the classification performance. Our metadata extraction method was originally designed to improve the metadata extraction quality of the digital libraries Citeseer [S. Lawrence et al., (1999)] and EbizSearch [Y. Petinot et al., (2003)]. We believe it can be generalized to other digital libraries.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1204842,no,no,1486912005.67864
A neural network based fault detection and identification scheme for pneumatic process control valves,"This paper outlines a method for detection and identification of actuator faults in a pneumatic process control valve using a neural network. First, the valve signature and dynamic error band tests, used by specialists to determine valve performance parameters, are carried out for a number of faulty operating conditions. A commercially available software package is used to carry out the diagnostic tests, thus eliminating the need for additional instrumentation of the valve. Next, the experimentally determined valve performance parameters are used to train a multilayer feedforward network to successfully detect and identify incorrect supply pressure, actuator vent blockage, and diaphragm leakage faults",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969794,no,no,1486912005.678639
Fine-Grained Software Metrics in Practice,"Modularity is one of the key features of the Object- Oriented (00) paradigm. Low coupling and high cohesion help to achieve good modularity. Inheritance is one of the core concepts of the 00 paradigm which facilitates modularity. Previous research has shown that the use of the friend construct as a coupling mechanism in C+ + software is extensive. However, measures of the friend construct are scarse in comparison with measures of inheritance. In addition, these existing measures are coarse-grained, in spite of the widespread use of the friend mechanism. In this paper, a set of software metrics are proposed that measure the actual use of the friend construct, inheritance and other forms of coupling. These metrics are based on the interactions for which each coupling mechanism is necessary and sufficient. Previous work only considered the declaration of a relationship between classes. The software metrics introduced are empirically assessed using the LEDA software system. Our results indicate that the friend mechanism is used to a very limited extent to access hidden methods in classes. However, access to hidden attributes is more common.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343757,no,no,1486912005.678637
Low Complexity Scalable Video Coding,"In this paper, we consider scalable video coding (SVC) which has higher complexity than H.264/AVC since it has spatial, temporal and quality scalability in addition to H.264/AVC functionality. Furthermore, inter-layer prediction and layered coding for spatial scalability make motion estimation and mode decision more complex. Therefore, we propose low complexity SVC schemes by using current developing SVC standard. It is archived by prediction method such as skip, direct, inter-layer MV prediction with fast mode and motion vector (MV) estimation at enhancement layer. In order to increase the performance of inter-layer MV prediction, combined MV interpolation is applied with adjustment of prediction direction. Additionally, fast mode and MV estimation are proposed from structural properties of motion-compensated temporal filtering (MCTF) to elaborate predicted macro block (MB) mode and MV. From the experimental results, proposed method has comparable performance to reference software model with significant lower complexity.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4176730,no,no,1486912005.678635
The Quadrics network (QsNet): high-performance clustering technology,"The Quadrics interconnection network (QsNet) contributes two novel innovations to the field of high-performance interconnects: (I) integration of the virtual-address spaces of individual nodes into a single, global, virtual-address space and (2) network fault tolerance via link-level and end-to-end protocols that can detect faults and automatically re-transmit packets. QsNet achieves these feats by extending the native operating system in the nodes with a network operating system and specialized hardware support in the network interface. As these and other important features of QsNet can be found in the InfiniBand specification, QsNet can be viewed as a precursor to InfiniBand. In this paper, we present an initial performance evaluation of QsNet. We first describe the main hardware and software features of QsNet, followed by the results of benchmarks that we ran on our experimental, Intel-based, Linux cluster built around QsNet. Our initial analysis indicates that QsNet performs remarkably well, e.g., user-level latency under 2 s and bandwidth over 300 MB/s",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=946704,no,no,1486912005.678634
Myrinet networks: a performance study,"As network computing become commonplace, the interconnection networks and the communication system software become critical in achieving high performance. Thus, it is essential to systematically assess the features and performance of the new networks. Recently, Myricom has introduced a two-port """"E-card"""" Myrinet/PCl-X interface. In this paper, we present the basic performance of its GM2.I messaging layer, as well as a set of microbenchmarks designed to assess the quality of MPI implementation on top of GM. These microbenchmarks measure the latency, bandwidth, intra-node performance, computation/communication overlap, parameters of the LogP model, buffer reuse impact, different traffic patterns, and collective communications. We have discovered that the MPI basic performance is close to those offered at the GM. We find that the host overhead is very small in our system. The Myrinet network is shown to be sensitive to the buffer reuse patterns. However, it provides opportunities for overlapping computation with communication. The Myrinet network is able to deliver up to 2000MB/s bandwidth for the permutation patterns.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347794,no,no,1486912005.678631
Data Quality Monitoring Framework for the ATLAS Experiment at the LHC,"Data quality monitoring (DQM) is an integral part of the data taking process of HEP experiments. DQM involves automated analysis of monitoring data through user-defined algorithms and relaying the summary of the analysis results to the shift personnel while data is being processed. In the online environment, DQM provides the shifter with current run information that can be used to overcome problems early on. During the offline reconstruction, more complex analysis of physics quantities is performed by DQM, and the results are used to assess the quality of the reconstructed data. The ATLAS data quality monitoring framework (DQMF) is a distributed software system providing DQM functionality in the online environment. The DQMF has a scalable architecture achieved by distributing the execution of the analysis algorithms over a configurable number of DQMF agents running on different nodes connected over the network. The core part of the DQMF is designed to have dependence only on software that is common between online and offline (such as ROOT) and therefore the same framework can be used in both environments. This paper describes the main requirements, the architectural design, and the implementation of the DQMF.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4448489,no,no,1486912005.029451
P-RnaPredict-a parallel evolutionary algorithm for RNA folding: effects of pseudorandom number quality,"This paper presents a fully parallel version of RnaPredict, a genetic algorithm (GA) for RNA secondary structure prediction. The research presented here builds on previous work and examines the impact of three different pseudorandom number generators (PRNGs) on the GA's performance. The three generators tested are the C standard library PRNG RAND, a parallelized multiplicative congruential generator (MCG), and a parallelized Mersenne Twister (MT). A fully parallel version of RnaPredict using the Message Passing Interface (MPI) was implemented on a 128-node Beowulf cluster. The PRNG comparison tests were performed with known structures whose sequences are 118, 122, 468, 543, and 556 nucleotides in length. The effects of the PRNGs are investigated and the predicted structures are compared to known structures. Results indicate that P-RnaPredict demonstrated good prediction accuracy, particularly so for shorter sequences.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1501839,no,no,1486912005.02945
Investigating the Efficacy of Nonlinear Dimensionality Reduction Schemes in Classifying Gene and Protein Expression Studies,"The recent explosion in procurement and availability of high-dimensional gene and protein expression profile data sets for cancer diagnostics has necessitated the development of sophisticated machine learning tools with which to analyze them. While some investigators are focused on identifying informative genes and proteins that play a role in specific diseases, other researchers have attempted instead to use patients based on their expression profiles to prognosticate disease status. A major limitation in the ability to accurately classify these high-dimensional data sets stems from the """"curse of dimensionality,"""" occurring in situations where the number of genes or peptides significantly exceeds the total number of patient samples. Previous attempts at dealing with this issue have mostly centered on the use of a dimensionality reduction (DR) scheme, principal component analysis (PCA), to obtain a low-dimensional projection of the high-dimensional data. However, linear PCA and other linear DR methods, which rely on euclidean distances to estimate object similarity, do not account for the inherent underlying nonlinear structure associated with most biomedical data. While some researchers have begun to explore nonlinear DR methods for computer vision problems such as face detection and recognition, to the best of our knowledge, few such attempts have been made for classification and visualization of high-dimensional biomedical data. The motivation behind this work is to identify the appropriate DR methods for analysis of high-dimensional gene and protein expression studies. Toward this end, we empirically and rigorously compare three nonlinear (Isomap, Locally Linear Embedding, and Laplacian Eigenmaps) and three linear DR schemes (PCA, Linear Discriminant Analysis, and Multidimensional Scaling) with the intent of determining a reduced subspace representation in which the individual object classes are more easily discriminable. Owing to the inherent nonlinear structure- - of gene and protein expression studies, our claim is that the nonlinear DR methods provide a more truthful low-dimensional representation of the data compared to the linear DR schemes. Evaluation of the DR schemes was done by 1) assessing the discriminability of two supervised classifiers (Support Vector Machine and C4.5 Decision Trees) in the different low- dimensional data embeddings and 2) five cluster validity measures to evaluate the size, distance, and tightness of object aggregates in the low-dimensional space. For each of the seven evaluation measures considered, statistically significant improvement in the quality of the embeddings across 10 cancer data sets via the use of three nonlinear DR schemes over three linear DR techniques was observed. Similar trends were observed when linear and nonlinear DR was applied to the high-dimensional data following feature pruning to isolate the most informative features. Qualitative evaluation of the low-dimensional data embedding obtained via the six DR methods further suggests that the nonlinear schemes are better able to identify potential novel classes (e.g., cancer subtypes) within the data.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4492764,no,no,1486912005.029448
Software tools for PRA,"Probabilistic Risk Assessment (PRA) is performed to assess the probability of failure or success of a system's operation. Results provided by the risk assessment methodology are used to make decisions concerning choice of improvements to the design. PRA has been applied or recommended to NASA space applications to identify and mitigate their risks. The complexity of these tasks and varied information sources required for space applications makes solving them manually infeasible. Software tools are mandated. To date, numerous software tools have been developed and claimed as PRA solutions. It is always a concern which software best fits a particular PRA. The authors conducted a limited scope PRA on a NASA application using four different Reliability/PRA software tools (Relex, QRAS, SAPHIRE, GoldSim), which were readily available. The strength and weakness for each tool are identified and discussed. Recommendations on how to improve each tool to better satisfy NASA PRA needs are discussed.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4925775,no,no,1486912005.029447
A Novel Embedded Accelerator for Online Detection of Shrew DDoS Attacks,"As one type of stealthy and hard-to-detect attack, low-rate TCP-targeted DDoS attack can seriously throttle the throughput of normal TCP flows for a long time without being noticed. The Power Spectral Density (PSD) analysis in frequency domain can detect this type of attack accurately. However, computational complexity of PSD analysis makes it impossible for software implementation at high speed network. Taking advantages of powerful computing capability and software-like flexibility, an embedded accelerator using FPGA for PSD analysis has been proposed. Optimized design in autocorrelation calculation algorithm and DFT processing distinguishes our scheme more meaningful for high speed real-time processing with limited resources. Simulation verifies that even working at very low system clock frequency, our design can still provide quality-service for malicious detection in multi-gigabyte rate network.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579615,no,no,1486912005.029446
"Fault Detection, Isolation, and Localization in Embedded Control Software","Embedded control software reacts to plant and environment conditions in order to enforce a desired functionality, and exhibit hybrid dynamics: control-loops together with switching logic. Control software can contain errors (faults), and fault-tolerance methods must be developed to enhance system safety and reliability. We present an approach for fault detection and isolation that is key to achieving fault-tolerance. Detection approach is hierarchical involving monitoring both the control software, and the controlled-system. The latter is necessary to safeguard against any incompleteness of software level properties. A model of the system being monitored is not required, and further the approach is modular and hence scalable. When fault is detected at the system level, an isolation of a software fault is achieved by using residue methods to rule out any hardware (plant) fault. We also proposed a method to localize a software fault (to those lines of code that contain the fault). The talk will be illustrated through a servo control application.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4635929,no,no,1486912005.029444
Outlier Detection in Wireless Sensor Networks using Bayesian Belief Networks,"Data reliability is an important issue from the user's perspective, in the context of streamed data in wireless sensor networks (WSN). Reliability is affected by the harsh environmental conditions, interferences in wireless medium and usage of low quality sensors. Due to these conditions, the data generated by the sensors may get corrupted resulting in outliers and missing values. Deciding whether an observation is an outlier or not depends on the behavior of the neighbors' readings as well as the readings of the sensor itself. This can be done by capturing the spatio-temporal correlations that exists among the observations of the sensor nodes. By using naive Bayesian networks for classification, we can estimate whether an observation belongs to a class or not. If it falls beyond the range of the class, then it can be detected as an outlier. However naive Bayesian networks do not consider the conditional dependencies among the observations of sensor attributes. So, we propose an outlier detection scheme based on Bayesian belief networks, which captures the conditional dependencies among the observations of the attributes to detect the outliers in the sensor streamed data. Applicability of this scheme as a plug-in to the component oriented middleware for sensor networks (COMiS) of our early research work is also presented",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665221,no,no,1486912005.029443
Video transcoding using network processors to support dynamically adaptive video multicast,"Heterogeneity of networks and end systems poses challenges for multicast based collaborative applications. In traditional multicasting, the sender transmits video at the same rate to all receivers independent of their network connection, end system equipment, and users' preferences. This wastes resources and may also result in some receivers having their quality expectations unsatisfied. This problem can be addressed, near the network edge, by applying dynamic, in-network transcoding of video streams. In this paper, we design, implement, and assess a network processor (NP) based video transcoding system using the Intel IXP1200. Experiments suggest that our system can adapt the video rate of MPEG-1 streams to a desired level on a per packet basis for moderate traffic levels.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1620234,no,no,1486912005.029441
Predictive modeling and control of DMAS,"Predicting the behavior of distributed multi-agent systems (DMAS) is a blown, extremely challenging problem. In general, we are not able to make reliable quantitative predictions of the behavior that a given DMAS exhibits, even in a blown environment, due to the complex emergent effects in those systems, which often reflect chaotic interactions. Such predictability is nonetheless crucial for reliable, controlled development and deployment of such systems. We need to be able to control the behaviors of such systems, and want to optimize configurations to achieve acceptable and reliable returns of quality-of-service for an investment of resources. We describe here an approach to developing reliable predictive models for a particular class of DMAS. We have succeeded in developing such models for this class of applications and in achieving controlled behaviors and optimized configurations based on these predictive models. We discuss our approach, and results and plans for applying this approach to broader classes of applications.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1368421,no,no,1486912005.029439
A practical classification-rule for software-quality models,"A practical classification rule for a SQ (software quality) model considers the needs of the project to use a model to guide targeting software RE (reliability enhancement) efforts, such as extra reviews early in development. Such a rule is often more useful than alternative rules. This paper discusses several classification rules for SQ models, and recommends a generalized classification rule, where the effectiveness and efficiency of the model for guiding software RE efforts can be explicitly considered. This is the first application of this rule to SQ modeling that we know of. Two case studies illustrate application of the generalized classification rule. A telecommunication-system case-study models membership in the class of fault-prone modules as a function of the number of interfaces to other modules. A military-system case-study models membership in the class of fault-prone modules as a function of a set of process metrics that depict the development history of a module. These case studies are examples where balanced misclassification rates resulted in more useful and practical SQ models than other classification rules",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=877340,no,no,1486912005.029436
Assessing software implemented fault detection and fault tolerance mechanisms,The problems of hardware fault detection and correction using software techniques are analysed. They relate to our experience with a large class of applications. The effectiveness of these techniques is studied using a special fault injector with various statistical tools. A new error-handling scheme is proposed.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250857,no,no,1486912004.383746
Automated identification of single nucleotide polymorphisms from sequencing data,"Single nucleotide polymorphisms (SNPs) provide abundant information about genetic variation. Large scale discovery of high frequency SNPs is being undertaken using various methods. However, the publicly available SNP data are not always accurate, and therefore should be verified. If only a particular gene locus is concerned, locus-specific polymerase chain reaction amplification may be useful. Problem of this method is that the secondary peak has to be measured. We have analyzed trace data from conventional sequencing equipment and found an applicable rule to discern SNPs from noise. We have developed software that integrates this function to automatically identify SNPs. The software works accurately for high quality sequences and also can detect SNPs in low quality sequences. Further, it can determine allele frequency, display this information as a bar graph and assign corresponding nucleotide combinations. It is very useful for identifying de novo SNPs in a DNA fragment of interest.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1039332,no,no,1486912004.383745
Cohesion Metrics for Predicting Maintainability of Service-Oriented Software,"Although service-oriented computing (SOC) is a promising paradigm for developing enterprise software systems, existing research mostly assumes the existence of black box services with little attention given to the structural characteristics of the implementing software, potentially resulting in poor system maintainability. Whilst there has been some preliminary work examining coupling in a service-oriented context, there has to date been no such work on the structural property of cohesion. Consequently, this paper extends existing notions of cohesion in OO and procedural design in order to account for the unique characteristics of SOC, allowing the derivation of assumptions linking cohesion to the maintainability of service-oriented software. From these assumptions, a set of metrics are derived to quantify the degree of cohesion of service oriented design constructs. Such design level metrics are valuable because they allow the prediction of maintainability early in the SDLC.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385516,no,no,1486912004.383743
Predict Malfunction-Prone Modules for Embedded System Using Software Metrics,"High software dependability is significant for many software systems, especially in embedded system. Dependability is usually measured from the user's viewpoint in terms of time between failures, according to an operational profile. A software malfunction is defined as a defect in an executable software product that may cause a failure. Thus, malfunctions are attributed to the software modules that cause failures. Developers tend to focus on malfunctions, because they are closely related to the amount of rework necessary to prevent future failures. This paper defined a software module malfunction-prone by class cohesion metrics when there is a high risk that malfunctions will be discovered during operations. Also proposed a novel cohesion measure method for derived classes in embedded system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4350736,no,no,1486912004.383742
Medical software control quality using the 3D Mojette projector,"The goal of this paper is to provide a tool that allows to assess a set of 2D projection data from a 3D object which can be either real or synthetic data. However, the generated 2D projection set is not sampled onto a classic orthogonal grid but uses a regular grid depending on the discrete angle of projection and the 3D orthogonal grid. This allows a representation of the set of projections that can easily be described in spline spaces. The subsequent projections set is used after an interpolation scheme to compare (in the projection space) the adequation between the original dataset and the obtained reconstruction. These measures are performed from a 3D multiresolution stack in the case of 3D PET projector. Its direct use for the digital radiography reprojection control quality assessment is finally exposed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1398668,no,no,1486912004.383741
An Innovative Transient-Based Protection Scheme for MV Distribution Networks with Distributed Generation,This paper presents a new transient based scheme to detect single phase to ground faults (or grounded faults in general) in distribution systems with high penetration of distributed generation. This algorithm combines the fault direction based approach and the distance estimation based approach in order to determine the faulted section. The wavelet coefficients of the transient fault currents measured at the interconnection points of the network are used to determine the direction of fault currents and to estimate the distance of the fault. The simulations have been carried out by using DigSilent software package and the results have been processed in MATLAB using the Wavelet Toolbox.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4496999,no,no,1486912004.383739
Reliable Effects Screening: A Distributed Continuous Quality Assurance Process for Monitoring Performance Degradation in Evolving Software Systems,"Developers of highly configurable performance-intensive software systems often use in-house performance-oriented """"regression testing"""" to ensure that their modifications do not adversely affect their software's performance across its large configuration space. Unfortunately, time and resource constraints can limit in-house testing to a relatively small number of possible configurations, followed by unreliable extrapolation from these results to the entire configuration space. As a result, many performance bottlenecks escape detection until systems are fielded. In our earlier work, we improved the situation outlined above by developing an initial quality assurance process called """"main effects screening"""". This process 1) executes formally designed experiments to identify an appropriate subset of configurations on which to base the performance-oriented regression testing, 2) executes benchmarks on this subset whenever the software changes, and 3) provides tool support for executing these actions on in-the-field and in-house computing resources. Our initial process had several limitations, however, since it was manually configured (which was tedious and error-prone) and relied on strong and untested assumptions for its accuracy (which made its use unacceptably risky in practice). This paper presents a new quality assurance process called """"reliable effects screening"""" that provides three significant improvements to our earlier work. First, it allows developers to economically verify key assumptions during process execution. Second, it integrates several model-driven engineering tools to make process configuration and execution much easier and less error prone. Third, we evaluate this process via several feasibility studies of three large, widely used performance-intensive software frameworks. Our results indicate that reliable effects screening can detect performance degradation in large-scale systems more reliably and with significantly less resources than conventional t- echniques",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052587,no,no,1486912004.383738
Analogy based prediction of work item flow in software projects: a case study,"A software development project coordinates work by using work items that represent customer, tester and developer found defects, enhancements, and new features. We set out to facilitate software project planning by modeling the flow of such work items and using information on historic projects to predict the work flow of an ongoing project. The history of the work items is extracted from problem tracking or configuration management databases. The Web-based prediction tool allows project managers to select relevant past projects and adjust the prediction based on staffing, type, and schedule of the ongoing project. We present the workflow model, and briefly describe project prediction of a large software project for customer relationship management (CRM).",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237970,no,no,1486912004.383736
Proportional Intensity-Based Software Reliability Modeling with Time-Dependent Metrics,"The black-box approach based on stochastic software reliability models is a simple methodology with only software fault data in order to describe the temporal behavior of fault-detection processes, but fails to incorporate some significant development metrics data observed in the development process. In this paper we develop proportional intensity-based software reliability models with time-dependent metrics, and propose a statistical framework to assess the software reliability with the time-dependent covariate as well as the software fault data. The resulting models are similar to the usual proportional hazard model, but possess somewhat different covariate structure from the existing one. We compare these metrics-based software reliability models with some typical non-homogeneous Poisson process models, which are the special cases of our models, and evaluate quantitatively the goodness-of-fit from the viewpoint of information criteria. As an important result, the accuracy on reliability assessment strongly depends on the kind of software metrics used for analysis and can be improved by incorporating the time-dependent metrics data in modeling",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020098,no,no,1486912004.383735
Software Project Management Using Decision Networks,"The Bayesian networks support resource allocation in software project and also help in analyzing trade-offs among resources. The model predicts the probability distribution of every variable given incomplete data. Even though the Bayesian networks conveniently facilitate scenario-based analysis, they do not support finding an optimal solution in multi-criteria decision making. This paper proposes extending the Bayesian networks into the decision networks to optimize an organizational target and to handle the multi-criteria environment of software project management. Specifically, the decision networks are used to find an optimal set of software activities under constraints of software cost and quality. The preliminary results demonstrate that the Bayesian networks can be easily extended into the decision networks, which then allow for optimization. The proposed methodology provides a flexible process for utilizing the encoded knowledge within the Bayesian networks to facilitate decision making, which could be applicable in other domains of problems",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021822,no,no,1486912004.383732
Assessing reliability risk using fault correction profiles,"Building on the concept of the fault correction profile - a set of functions that predict fault correction events as a function of failure detection events - introduced in previous research, we define and apply reliability risk metrics that are derived from the fault correction profile. These metrics assess the threat to reliability of an unstable fault correction process. The fault correction profile identifies the need for process improvements and provides information for developing fault correction strategies. Applying these metrics to the NASA Goddard Space Flight Center fault correction process and its data, we demonstrate that reliability risk can be measured and used to identify the need for process improvement.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281738,no,no,1486912003.737346
Towards validated network configurations with NCGuard,"Today, most IP networks are still configured manually on a router-by-router basis. This is error-prone and often leads to misconfiguration. In this paper, we describe the Network Configuration Safeguard (NCGuard), a tool that allows the network architect to apply a safer methodology. The first step is to define his design rules. Based on a survey of the networking literature, we classify the most common types of rules in three main patterns: presence, uniqueness and symmetry and provide several examples. The second step is to write a high-level representation of his network. The third step is to validate the network representation and generate the configuration of each router. This last step is performed automatically by our prototype. Finally, we describe our prototype and apply it to the Abilene network.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4660329,no,no,1486912003.737344
Fast Recovery and QoS Assurance in the Presence of Network Faults for Mission-Critical Applications in Hostile Environments,"In a hostile military environment, systems must be able to detect and react to catastrophes in a timely manner in order to provide assurance that critical tasks will continue to meet their timeliness requirements. Our research focuses on achieving network quality of service (QoS) assurance using a Bandwidth Broker in the presence of network faults in layer-3 networks. Passive discovery techniques using the link-state information from routers provide for rapid path discovery which, in turn, leads to fast failure impact analysis and QoS restoration. In addition to network fault tolerance, the Bandwidth Broker must be fault tolerant and must be able to recover quickly. This is accomplished using a modified commercially available and open-source in- memory database cluster technology.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296863,no,no,1486912003.737343
Telephony-based voice pathology assessment using automated speech analysis,"A system for remotely detecting vocal fold pathologies using telephone-quality speech is presented. The system uses a linear classifier, processing measurements of pitch perturbation, amplitude perturbation and harmonic-to-noise ratio derived from digitized speech recordings. Voice recordings from the Disordered Voice Database Model 4337 system were used to develop and validate the system. Results show that while a sustained phonation, recorded in a controlled environment, can be classified as normal or pathologic with accuracy of 89.1%, telephone-quality speech can be classified as normal or pathologic with an accuracy of 74.2%, using the same scheme. Amplitude perturbation features prove most robust for telephone-quality speech. The pathologic recordings were then subcategorized into four groups, comprising normal, neuromuscular pathologic, physical pathologic and mixed (neuromuscular with physical) pathologic. A separate classifier was developed for classifying the normal group from each pathologic subcategory. Results show that neuromuscular disorders could be detected remotely with an accuracy of 87%, physical abnormalities with an accuracy of 78% and mixed pathology voice with an accuracy of 61%. This study highlights the real possibility for remote detection and diagnosis of voice pathology.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1597497,no,no,1486912003.737342
Performance Prediction Model for Service Oriented Applications,"Software architecture plays a significant role in determining the quality of a software system. It exposes important system properties for consideration and analysis. Performance related properties are frequently of interest in determining the acceptability of a given software design. This paper focuses mainly on developing an architectural model for applications that use service oriented architecture (SOA). This enables predicting the performance of the application even before it is completely developed. The performance characteristics of the components and connectors are modeled using queuing network model. This approach facilitates the performance prediction of service oriented applications. Further, it also helps in identification of various bottlenecks. A prototype service oriented application has been implemented and the actual performance is measured. This is compared against the predicted performance in order to analyze the accuracy of the prediction.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4637815,no,no,1486912003.73734
Adaptation of high level behavioral models for stuck-at coverage analysis,"There has been increasing effort in the years for defining test strategies at the behavioral level. Due to the lack of suitable coverage metrics and tools to assess the quality of a testbench, these strategies have not been able to play an important role in stuck-at fault simulation. The work we are presenting here proposes a new coverage metric that employs back-annotation of post-synthesis design properties into pre-synthesis simulation models to estimate the stuck-at fault coverage of a testbench. The effectiveness of this new metric is evaluated for several example circuits. The results show that the new metric provides a good evaluation of high level testbenches for detection of stuck-at faults.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4540254,no,no,1486912003.737339
Static analysis tools as early indicators of pre-release defect density,"During software development it is helpful to obtain early estimates of the defect density of software components. Such estimates identify fault-prone areas of code requiring further testing. We present an empirical approach for the early prediction of pre-release defect density based on the defects found using static analysis tools. The defects identified by two different static analysis tools are used to fit and predict the actual pre-release defect density for Windows Server 2003. We show that there exists a strong positive correlation between the static analysis defect density and the pre-release defect density determined by testing. Further, the predicted pre-release defect density and the actual pre-release defect density are strongly correlated at a high degree of statistical significance. Discriminant analysis shows that the results of static analysis tools can be used to separate high and low quality components with an overall classification rate of 82.91%.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553604,no,no,1486912003.737337
Exploring the relationship between experience and group performance in software review,"The aim is to examine the important relationships between experience, task training and software review performance. One hundred and ninety-two volunteer university students were randomly assigned into 48 four-member groups. Subjects were required to detect defects from a design document. The main findings include (1) role experience has a positive effect on software review performance; (2) working experience in the software industry has a positive effect on software review performance; (3) task training has no significant effect on software review performance; (4) role experience has no significant effect on task training; (5) working experience in the software industry has a significant effect on task training.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254405,no,no,1486912003.737336
Reliability and Sensitivity Analysis of Embedded Systems with Modular Dynamic Fault Trees,"Fault trees theories have been used in years because they can easily provide a concise representation of failure behavior of general non-repairable fault-tolerant systems. But the defect of traditional fault trees is lack of accuracy when modeling dynamic failure behavior of certain systems with fault-recovery process. A solution to this problem is called behavioral decomposition. A system will be divided into several dynamic or static modules, and each module can be further analyzed using BDD or Markov chains separately. In this paper, we will show a decomposition scheme that independent subtrees of a dynamic module are detected and solved hierarchically for saving computation time of solving Markov chains without losing unacceptable accuracy when assessing components sensitivities. In the end, we present our analyzing software toolkit that implements our enhanced methodology.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4084932,no,no,1486912003.737334
Exception handling in workflow management systems,"Fault tolerance is a key requirement in process support systems (PSS), a class of distributed computing middleware encompassing applications such as workflow management systems and process centered software engineering environments. A PSS controls the flow of work between programs and users in networked environments based on a metaprogram (the process). The resulting applications are characterized by a high degree of distribution and a high degree of heterogeneity (properties that make fault tolerance both highly desirable and difficult to achieve). We present a solution for implementing more reliable processes by using exception handling, as it is used in programming languages, and atomicity, as it is known from the transaction concept in database management systems. We describe the mechanism incorporating both transactions and exceptions and present a validation technique allowing to assess the correctness of process specifications",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879818,no,no,1486912003.737331
An integrated failure detection and fault correction model,"In general, software reliability models have focused an modeling and predicting failure occurrence and have not given equal priority to modeling the fault correction process. However, there is a need for fault correction prediction, because there are important applications that fault correction modeling and prediction support. These are the following: predicting whether reliability goals have been achieved, developing stopping rules for testing, formulating test strategies, and rationally allocating test resources. Because these factors are related, we integrate them in our model. Our modeling approach involves relating fault correction to failure prediction, with a time delay estimated from a fault correction queuing model.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167772,no,no,1486912003.092997
Object oriented metrics useful in the prediction of class testing complexity,"Adequate metrics of object-oriented software enable one to determine the complexity of a system and estimate the effort needed for testing already in the early stage of system development. The metrics values enable to locate parts of the design that could be error prone. Changes in these parts could significantly, improve the quality of the final product and decrease testing complexity. Unfortunately only few of the existing Computer Aided Software Engineering tools (CASE) calculate object metrics. In this paper methods allowing proper calculation of class metrics for some commercial CASE tool have been developed. New metric, calculable on the basis of information kept in CASE repository and useful in the estimation of testing effort have also been proposed. The evaluation of all discussed metrics does not depend on object design method and on the implementation language",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952447,no,no,1486912003.092996
Predictors of customer perceived software quality,"Predicting software quality as perceived by a customer may allow an organization to adjust deployment to meet the quality expectations of its customers, to allocate the appropriate amount of maintenance resources, and to direct quality improvement efforts to maximize the return on investment. However, customer perceived quality may be affected not simply by the software content and the development process, but also by a number of other factors including deployment issues, amount of usage, software platform, and hardware configurations. We predict customer perceived quality as measured by various service interactions, including software defect reports, requests for assistance, and field technician dispatches using the afore mentioned and other factors for a large telecommunications software system. We employ the non-intrusive data gathering technique of using existing data captured in automated project monitoring and tracking systems as well as customer support and tracking systems. We find that the effects of deployment schedule, hardware configurations, and software platform can increase the probability of observing a software failure by more than 20 times. Furthermore, we find that the factors affect all quality measures in a similar fashion. Our approach can be applied at other organizations, and we suggest methods to independently validate and replicate our results.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553565,no,no,1486912003.092994
Adaptive OSEK Network Management for in-vehicle network fault detection,"Rapid growth in the deployment of networked electronic control units (ECUs) and enhanced software features within automotive vehicles has occurred over the past two decades. This inevitably results in difficulties and complexity in in-vehicle network fault diagnostics. To overcome these problems, a framework for on-board in-vehicle network diagnostics has been proposed and its concept has previously been demonstrated through experiments. This paper presents a further implementation of network fault detection within the framework. Adaptive OSEK Network Management, a new technique for detecting network level faults, is presented. It is demonstrated in this paper that this technique provides more accurate fault detection and the capability to cover more fault scenarios.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4456405,no,no,1486912003.092993
Fault analysis study using modeling and simulation tools for distribution power systems,"This article describes a fault analysis study using some of the best available simulation and modeling tools for electrical distribution power systems. Several software tools were identified and assessed in L. Nastac, et al (2005). The fault analysis was conducted with the assessed software tools using the recorded fault data from a real circuit system. The recorded fault data including the topology and the line data with more than 1000 elements were provided by Detroit Edison (DTE) Energy for validation purposes. The effects of pro-fault loading and arcing impedance on the predicted fault current values were also investigated. Then, to ensure that the validated software tools are indeed capable of analyzing circuits with DCs, fault management and relay protection problems were developed and solved using a modified IEEE 34-bus feeder with addition of DCs.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1560529,no,no,1486912003.092992
Proactive detection of software aging mechanisms in performance critical computers,"Software aging is a phenomenon, usually caused by resource contention, that can cause mission critical and business critical computer systems to hang, panic, or suffer performance degradation. If the incipience or onset of software aging mechanisms can be reliably detected in advance of performance degradation, corrective actions can be taken to prevent system hangs, or dynamic failover events can be triggered in fault tolerant systems. In the 1990 's the U.S. Dept. of Energy and NASA funded development of an advanced statistical pattern recognition method called the multivariate state estimation technique (MSET) for proactive online detection of dynamic sensor and signal anomalies in nuclear power plants and Space Shuttle Main Engine telemetry data. The present investigation was undertaken to investigate the feasibility and practicability of applying MSET for realtime proactive detection of software aging mechanisms in complex, multiCPU servers. The procedure uses MSET for model based parameter estimation in conjunction with statistical fault detection and Bayesian fault decision processing. A realtime software telemetry harness was designed to continuously sample over 50 performance metrics related to computer system load, throughput, queue lengths, and transaction latencies. A series of fault injection experiments was conducted using a """"memory leak"""" injector tool with controllable parasitic resource consumption rates. MSET was able to reliably detect the onset of resource contention problems with high sensitivity and excellent false-alarm avoidance. Spin-off applications of this NASA-funded innovation for business critical eCommerce servers are described.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199445,no,no,1486912003.09299
Using Stealth Mixins to Achieve Modularity,"Organising a complex, interactive application into separate modules is a significant challenge. We would like to be able to evolve modules independently, and to add new modules into the system (or remove optional ones) without requiring major revisions to existing code. Solutions that rely on pre-planning when writing core modules are clumsy and error-prone, since programmers may omit to include all the required """"hooks,"""" unused hooks incur a runtime overhead, and any unanticipated extensions may still require significant code changes. Unfortunately, most languages do not provide adequate mechanisms for supporting the separation of modules. In this paper, we review partial solutions for typical object-oriented languages such as Java, and then present """"stealth mixins"""", a much more satisfactory solution that can be built on top of Common Lisp. The Common Lisp solution was developed for Gsharp, a sophisticated graphical music score editor, and the technique has been used extensively throughout the program. We use Gsharp as a running example throughout this paper. However, the ideas are applicable to a wide range of applications.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4159664,no,no,1486912003.092989
Assessing the Object-level behavioral complexity in Object Relational Databases,"Object Relational Database Management Systems model set of interrelated objects using references and collection attributes. The static metrics capture the internal quality of the database schema at the class -level during design time. Complex databases like ORDB exhibit dynamism during runtime and hence require performance-level monitoring. This is achieved by measuring the access and invocations of the objects during runtime, thus assessing the behavior of the objects. Runtime coupling and cohesion metrics are deemed as attributes of measuring the Object-level behavioral complexity. In this work, we evaluate the runtime coupling and cohesion metrics and assess their influence in measuring the behavioral complexity of the objects in ORDB. Further, these internal measures of object behavior are externalized in measuring the performance of the database in entirety. Experiments on sample ORDB schemas are conducted using statistical analysis and correlation clustering techniques to assess the behavior of the objects in real time. The results indicate the significance of the object behavior in influencing the database performance. The scope of this work and the future works in extending this research form the concluding note.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384084,no,no,1486912003.092988
A DSP-based FFT-analyzer for the fault diagnosis of rotating machine based on vibration analysis,"A DSP-based measurement system dedicated to the vibration analysis on rotating machines was designed and realized. Vibration signals are on-line acquired and processed to obtain a continuous monitoring of the machine status. In case of a fault, the system is capable of isolating the fault with a high reliability. The paper describes in detail the approach followed to built up fault and non-fault models together with the chosen hardware and software solutions. A number of tests carried out on small-size three-phase asynchronous motors highlight the excellent promptness in detecting faults, low false alarm rate, and very good diagnostic performance.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1177930,no,no,1486912003.092986
Fault location using wavelet packets,"A technique using wavelet packets is presented for accurate fault location on power lines with branches. It relies on detecting fault-generated transient traveling waves and identifies some waves reflected back from discontinuities and the fault point. Wavelet packets analysis is used to decompose and reconstruct high-frequency fault signals. An eigenvector matrix consists of local energies of high-frequency content of the fault signal; the faulty section is determined by comparing local energies in the eigenvector matrix with a given threshold. With the faulty section determined, the time ranges of two reflected waves related with fault point would be found out, then the fault point is located. The paper shows the theoretical development of the algorithm, and together with the results obtained using EMTP simulation software modeling, a simple 10 kV overhead line circuit.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1047251,no,no,1486912003.092983
Predicting software stability using case-based reasoning,"Predicting stability in object-oriented (OO) software, i.e., the ease with which a software item can evolve while preserving its design, is a key feature for software maintenance. We present a novel approach which relies on the case-based reasoning (CBR) paradigm. Thus, to predict the chances of an OO software item breaking downward compatibility, our method uses knowledge of past evolution extracted from different software versions. A comparison of our similarity-based approach to a classical inductive method such as decision trees, is presented which includes various tests on large datasets from existing software.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115033,no,no,1486912002.450678
Extended fault modeling used in the space shuttle PRA,"A probabilistic risk assessment (PRA) has been completed for the space shuttle with NASA sponsorship and involvement. This current space shuttle PRA is an advancement over past PRAs conducted for the space shuttle in the technical approaches utilized and in the direct involvement of the NASA centers and prime contractors. One of the technical advancements is the extended fault modeling techniques used. A significant portion of the data collected by NASA for the space shuttle consists of faults, which are not yet failures but have the potential of becoming failures if not corrected. This fault data consists of leaks, cracks, material anomalies, and debonding faults. Detailed, quantitative fault models were developed for the space shuttle PRA which involved assessing the severity of the fault, detection effectiveness, recurrence control effectiveness, and mission-initiation potential. Each of these attributes was transformed into a quantitative weight to provide a systematic estimate of the probability of the fault becoming a failure in a mission. Using the methodology developed, mission failure probabilities were estimated from collected fault data. The methodology is an application of counter-factual theory and defect modeling which produces consistent estimates of failure rates from fault rates. Software was developed to analyze all the relevant fault data collected for given types of faults in given systems. The software allowed the PRA to be linked to NASA's fault databases. This also allows the PRA to be updated as new fault data is collected. This fault modeling and its implementation with FRAS was an important part of the space shuttle PRA.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285479,no,no,1486912002.450677
Combining Software Quality Analysis with Dynamic Event/Fault Trees for High Assurance Systems Engineering,"We present a novel approach for probabilistic risk assessment (PRA) of systems which require high assurance that they will function as intended. Our approach uses a new model i.e., a dynamic event/fault tree (DEFT) as a graphical and logical method to reason about and identify dependencies between system components, software components, failure events and system outcome modes. The method also explicitly includes software in the analysis and quantifies the contribution of the software components to overall system risk/ reliability. The latter is performed via software quality analysis (SQA) where we use a Bayesian network (BN) model that includes diverse sources of evidence about fault introduction into software; specifically, information from the software development process and product metrics. We illustrate our approach by applying it to the propulsion system of the miniature autonomous extravehicular robotic camera (mini-AERCam). The software component considered for the analysis is the related guidance, navigation and control (GN&C) component. The results of SQA indicate a close correspondence between the BN model estimates and the developer estimates of software defect content. These results are then used in an existing theory of worst-case reliability to quantify the basic event probability of the software component in the DEFT.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404747,no,no,1486912002.450675
Walking the talk: building quality into the software quality management tool,"The market for products whose objective is to improve software project management and software quality management is expected to be large and growing. In this paper, we present data from a software project whose mission is to build a commercial enterprise software project and quality management tool. The data we present include: planned and actual schedule, earned value, size, productivity, and defect removal by phase. We present process quality index and percent defect free modules as useful measures to predict post development defects in the product. We conclude that vendors of software project and quality management tools must walk the talk by utilizing disciplined techniques for managing the project and building quality into their products with known quality methods. The Team Software Process is a proven framework for both software project management and software quality management.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319087,no,no,1486912002.450674
Genetic programming model for software quality classification,"We apply genetic programming techniques to build a software quality classification model based on the metrics of software modules. The model we built attempts to distinguish the fault-prone modules from non-fault-prone modules using genetic programming (GP). These GP experiments were conducted with a random subset selection for GP in order to avoid overfitting. We then use the whole fit data set as the validation data set to select the best model. We demonstrate through two case studies that the GP technique can achieve good results. Also, we compared GP modeling with logistic regression modeling to verify the usefulness of GP",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966814,no,no,1486912002.450672
Predicting defects using network analysis on dependency graphs,"In software development, resources for quality assurance are limited by time and by cost. In order to allocate resources effectively, managers need to rely on their experience backed by code complexity metrics. But often dependencies exist between various pieces of code over which managers may have little knowledge. These dependencies can be construed as a low level graph of the entire system. In this paper, we propose to use network analysis on these dependency graphs. This allows managers to identify central program units that are more likely to face defects. In our evaluation on Windows Server 2003, we found that the recall for models built from network measures is by 10% points higher than for models built from complexity metrics. In addition, network measures could identify 60% of the binaries that the Windows developers considered as critical-twice as many as identified by complexity metrics.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814164,no,no,1486912002.450671
Metrics-Based Software Reliability Models Using Non-homogeneous Poisson Processes,"The traditional software reliability models aim to describe the temporal behavior of software fault-detection processes with only the fault data, but fail to incorporate some significant test-metrics data observed in software testing. In this paper we develop a useful modeling framework to assess the quantitative software reliability with time-dependent covariate as well as software-fault data. The basic ideas employed here are to introduce the discrete proportional hazard model on a cumulative Bernoulli trial process, and to represent a generalized fault-detection processes having time-dependent covariate structure. The resulting stochastic models are regarded as combinations of the proportional hazard models and the familiar non-homogeneous Poisson processes. We compare these metrics-based software reliability models with some typical non-homogeneous Poisson process models, and evaluate quantitatively both goodness-of-fit and predictive performances from the viewpoint of information criteria. As an important result, the accuracy on reliability assessment strongly depends on the kind of software metrics used for analysis and can be improved by incorporating time-dependent metrics data in modeling",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021971,no,no,1486912002.450669
Release date prediction for telecommunication software using Bayesian Belief Networks,"Many techniques are used for cost, quality and schedule estimation in the context of software risk management. Application of Bayesian Belief Networks (BBN) in this area permits process metrics and product metrics (static code metrics) to be considered in a causal way (i.e. each variable within the model has a cause-effect relationship with other variables) and, in addition, current observations can be used to update estimates based on historical data. However, the real situation that researchers face is that process data is often inadequately, or inappropriately, collected and organized by the development organization. In this paper, we explore if BBN could be used to predict appropriate release dates for a new set of products from a telecommunication company based on static code metrics data and limited process information collected from a earlier set of the same products. Two models are evaluated with different methods involved to analyze the available metrics data.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1013033,no,no,1486912002.450668
A computational framework for supporting software inspections,"Software inspections improve software quality by the analysis of software artifacts, detecting their defects for removal before these artifacts are delivered to the following software life cycle activities. Some knowledge regarding software inspections have been acquired by empirical studies. However, we found no indication that computational support for the whole software inspection process using appropriately such knowledge is available. This paper describes a computational framework whose requirements set was derived from knowledge acquired by empirical studies to support software inspections. To evaluate the feasibility of such framework, two studies have been accomplished: one case study, which has shown the feasibility of using the framework to support inspections, and an experimental study that evaluated the supported software inspection planning activity. Preliminary results of this experimental study suggested that unexperienced subjects are able to plan inspections with higher defect detection effectiveness, and in less time, when using this computational framework.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342723,no,no,1486912002.450666
A Two-Step Model for Defect Density Estimation,"Identifying and locating defects in software projects is a difficult task. Further, estimating the density of defects is more difficult. Measuring software in a continuous and disciplined manner brings many advantages such as accurate estimation of project costs and schedules, and improving product and process qualities. Detailed analysis of software metric data gives significant clues about the locations and magnitude of possible defects in a program. The aim of this research is to establish an improved method for predicting software quality via identifying the defect density of fault prone modules using machine-learning techniques. We constructed a two-step model that predicts defect density by taking module metric data into consideration. Our proposed model utilizes classification and regression type learning methods consecutively. The results of the experiments on public data sets show that the two-step model enhances the overall performance measures as compared to applying only regression methods.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4301095,no,no,1486912002.450663
Use of relative code churn measures to predict system defect density,"Software systems evolve over time due to changes in requirements, optimization of code, fixes for security and reliability bugs etc. Code churn, which measures the changes made to a component over a period of time, quantifies the extent of this change. We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn. Using statistical regression models, we show that while absolute measures of code chum are poor predictors of defect density, our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore, our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553571,no,no,1486912001.810322
Towards supporting evolution of service-oriented architectures through quality impact prediction,"The difficulty in evolving service-oriented architectures with extra-functional requirements seriously hinders the spread of this paradigm in critical application domains. This work tries to offset this disadvantage by introducing a design-time quality impact prediction and trade-off analysis method, which allows software engineers to predict the extra-functional consequences of alternative design decisions and select the optimal architecture without costly prototyping.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4686297,no,no,1486912001.81032
CVS release history data for detecting logical couplings,"The dependencies and interrelations between classes and modules affect the maintainability of object-oriented systems. It is therefore important to capture weaknesses of the software architecture to make necessary corrections. We describe a method for software evolution analysis. It consists of three complementary steps, which form an integrated approach for the reasoning about software structures based on historical data: 1) the quantitative analysis uses version information for the assessment of growth and change behavior; 2) the change sequence analysis identifies common change patterns across all system parts; and 3) the relation analysis compares classes based on CVS release history data and reveals the dependencies within the evolution of particular entities. We focus on the relation analysis and discuss its results; it has been validated based on empirical data collected from a concurrent versions system (CVS) covering 28 months of a picture archiving and communication system (PACS). Our software evolution analysis approach enabled us to detect shortcomings of PACS such as architectural weaknesses, poorly designed inheritance hierarchies, or blurred interfaces of modules.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231205,no,no,1486912001.810319
Experiences with evaluating network QoS for IP telephony,"Successful deployment of networked multimedia applications such as IP telephony depends on the performance of the underlying data network. QoS requirements of these applications are different from those of traditional data applications. For example, while IP telephony is very sensitive to delay and jitter, traditional data applications are more tolerant of these performance metrics. Consequently, assessing a network to determine whether it can accommodate the stringent QoS requirements of IP telephony becomes critical. We describe a technique for evaluating a network for IP telephony readiness. Our technique relies on the data collection and analysis support of our prototype tool, ExamiNetTM. It automatically discovers the topology of a given network and collects and integrates network device performance and voice quality metrics. We report the results of assessing the IP telephony readiness of a real network of 31 network devices (routers/switches) and 23 hosts via ExamiNetTM. Our evaluation identified links in the network that were over utilized to the point at which they could not handle IP telephony.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1006594,no,no,1486912001.810317
A Value-Added Predictive Defect Type Distribution Model Based on Project Characteristics,"In software project management, there are three major factors to predict and control; size, effort, and quality. Much software engineering work has focused on these. When it comes to software quality, there are various possible quality characteristics of software, but in practice, quality management frequently revolves around defects, and delivered defect density has become the current de facto industry standard. Thus, research related to software quality has been focused on modeling residual defects in software in order to estimate software reliability. Currently, software engineering literature still does not have a complete defect prediction for a software product although much work has been performed to predict software quality. On the other side, the number of defects alone cannot be sufficient information to provide the basis for planning quality assurance activities and assessing them during execution. That is, for project management to be improved, we need to predict other possible information about software quality such as in-process defects, their types, and so on. In this paper, we propose a new approach for predicting the distribution of defects and their types based on project characteristics in the early phase. For this approach, the model for prediction was established using the curve-fitting method and regression analysis. The maximum likelihood estimation (MLE) was used in fitting the Weibull probability density function to the actual defect data, and regression analysis was used in identifying the relationship between the project characteristics and the Weibull parameters. The research model was validated by cross-validation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529863,no,no,1486912001.810314
QoS-aware service composition in large scale multi-domain networks,"Next generation networks are envisioned to support dynamic and customizable service compositions at Internet scale. To facilitate the communication between distributed software components, on-demand and QoS-aware network service composition across large scale networks emerges as a key research challenge. This paper presents a fast QoS-aware service composition algorithm for selecting a set of interconnected domains with specific service classes. We further show how such algorithm can be used to support network adaptation and service mobility. In simulation studies performed on large scale networks, the algorithm exhibits very high probability of finding the optimal solution within short execution time. In addition, we present a distributed service composition framework utilizing this algorithm.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1440810,no,no,1486912001.810313
HyperMIP: Hypervisor Controlled Mobile IP for Virtual Machine Live Migration across Networks,"Live migration provides transparent load-balancing and fault-tolerant mechanism for applications. When a Virtual Machine migrates among hosts residing in two networks, the network attachment point of the Virtual Machine is also changed, thus the Virtual Machine will suffer from IP mobility problem after migration. This paper proposes an approach called Hypervisor controlled Mobile IP to support live migration of Virtual Machine across networks, which enables virtual machine live migration over distributed computing resources. Since Hypervisor is capable of predicting exact time and destination host of Virtual Machine migration, our approach not only can improve migration performance but also reduce the network restoration latency. Some comprehensive experiments have been conducted and the results show that the HyperMIP brings negligible overhead to network performance of Virtual Machines. The network restoration time of HyperMIP supported migration is about only 3 second. HyperMIP is a promising essential component to provide reliability and fault tolerant for network application running in Virtual Machine.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4708866,no,no,1486912001.810311
The Effects of Over and Under Sampling on Fault-prone Module Detection,"The goal of this paper is to improve the prediction performance of fault-prone module prediction models (fault-proneness models) by employing over/under sampling methods, which are preprocessing procedures for a fit dataset. The sampling methods are expected to improve prediction performance when the fit dataset is unbalanced, i.e. there exists a large difference between the number of fault-prone modules and not-fault-prone modules. So far, there has been no research reporting the effects of applying sampling methods to fault-proneness models. In this paper, we experimentally evaluated the effects of four sampling methods (random over sampling, synthetic minority over sampling, random under sampling and one-sided selection) applied to four fault-proneness models (linear discriminant analysis, logistic regression analysis, neural network and classification tree) by using two module sets of industry legacy software. All four sampling methods improved the prediction performance of the linear and logistic models, while neural network and classification tree models did not benefit from the sampling methods. The improvements of Fl-values in linear and logistic models were 0.078 at minimum, 0.224 at maximum and 0.121 at the mean.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343747,no,no,1486912001.810309
iOptimize: A software capability for analyzing and optimizing connection-oriented data networks in real time,"This paper describes a service called iOptimize that analyzes and optimizes service providers' connection-oriented data networks. In these networks, online connection routing is used to set up connections quickly, but the simple path selection scheme and the limited information available for online routing can cause network capacity to be used inefficiently. This may result in the loss or disruption of user data transmissions, leading to a degraded quality of service (QoS) in a network capable of supporting much higher throughput. The objective of iOptimize is to detect and analyze such inefficiencies and to offset them by the occasional rerouting of a selected set of connections, while causing little or no disruption of the existing data services and network operations. By helping service providers support data transport services at the QoS levels desired, iOptimize lets them make the most of their existing network resources and helps them defer capital expenditure. Moreover, by tuning their networks to perform at optimal efficiency, it improves end users' experiences, resulting in lower churn/turnover and reduced maintenance costs.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6768575,no,no,1486912001.810306
Improvement of frequency resolution for three-phase induction machine fault diagnosis,"This paper deals with the use of the zoom FFT algorithm (ZFFTA) for the electrical fault diagnosis of squirrel-cage three-phase induction machines with a special interest in broken rotor bar situation. The machine stator current can be analysed to observe the side-band harmonics around the fundamental frequency. In this case, it is necessary to take a very long data sequence to get high frequency resolution. This is not always possible due to the hardware and software limitations. The proposed algorithm can be considered for solving high frequency resolution problem without increasing the initial data acquisition size. The ZFFTA is applied to detect incipient rotor fault in a three-phase squirrel-cage induction machine by using both stator current and stray flux sensors.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1518286,no,no,1486912001.208283
Predicting fault-proneness using OO metrics. An industrial case study,"Software quality is an important external software attribute that is difficult to measure objectively. In this case study, we empirically validate a set of object-oriented metrics in terms of their usefulness in predicting fault-proneness, an important software quality indicator We use a set of ten software product metrics that relate to the following software attributes: the size of the software, coupling, cohesion, inheritance, and reuse. Eight hypotheses on the correlations of the metrics with fault-proneness are given. These hypotheses are empirically tested in a case study, in which the client side of a large network service management system is studied. The subject system is written in Java and it consists of 123 classes. The validation is carried out using two data analysis techniques: regression analysis and discriminant analysis",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995794,no,no,1486912001.208282
A Software-Based Error Detection Technique Using Encoded Signatures,"In this paper, a software-based control flow checking technique called SWTES (software-based error detection technique using encoded signatures) is presented and evaluated. This technique is processor independent and can be applied to any kind of processors and microcontrollers. To implement this technique, the program is partitioned to a set of blocks and the encoded signatures are assigned during the compile time. In the run-time, the signatures are compared with the expected ones by a monitoring routine. The proposed technique is experimentally evaluated on an ATMEL MCS51 microcontroller using software implemented fault injection (SWIFI). The results show that this technique detects about 90% of the injected errors. The memory overhead is about 135% on average, and the performance overhead varies between 11% and 191% depending on the workload used",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030951,no,no,1486912001.20828
Combined use of intelligent partial discharge analysis in evaluating high voltage dielectric condition,"This paper describes the results of synthesised high voltage impulse tests, conducted on surrogate dielectric samples. The tests conducted under laboratory conditions, were performed using contoured electrodes submersed under technical grade insulating oil. An escalating level of artificial degradation within surrogate samples was assessed, this correlated against magnitude and frequency of events. Withstand of partial discharge activity up to a point of insulation breakdown was observed using a conventional elliptical display partial discharge detector. Measurements of PD activity were simultaneously captured by virtual scope relaying data array captures to a desktop computer. The captured data arrays were duly processed by an artificial neural network program, the net result of which indicated harmony between human-guided opinion and the software aptitude. This paper describes work currently being undertaken for the identification and diagnosis of faults in high voltage dielectrics in furthering development of AI techniques",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=971345,no,no,1486912001.208279
Predicting C++ program quality by using Bayesian belief networks,"There have been many attempts to build models for predicting the software quality. Such models are used to measure the quality of software systems. The key variables in these models are either size or complexity metrics. There are, however, serious statistical and theoretical difficulties with these approaches. By using Bayesian belief network, we can overcome some of the more serious problems by taking more quality factors, which have direct or indirect impact on the software quality. In this paper, we have suggested a model to predicting the computer program quality by using Bayesian belief network. We found that the implementation of all quality factors were not feasible. Therefore, we have selected 14 quality factors to be implemented on an average size of two C++ programs. The selection criteria were based on the reviewer's opinions. Each node on the given Bayesian believe network represents one quality factor. We have drawn the BBN for the two C++ programs considering 14 nodes. The BBN has been constructed. The model has been executed and the results have been discussed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307903,no,no,1486912001.208277
A study in dynamic neural control of semiconductor fabrication processes,"This paper describes a generic dynamic control system designed for use in semiconductor fabrication process control. The controller is designed for any batch silicon wafer process that is run on equipment having a high number of variables that are under operator control. These controlled variables include both equipment state variables such as power, temperature, etc., and the repair, replacement, or maintenance of equipment parts, which cause parameter drift of the machine over time. The controller consists of three principal components: 1) an automatically updating database, 2) a neural-network prediction model for the prediction of process quality based on both equipment state variables and parts usage, and 3) an optimization algorithm designed to determine the optimal change of controllable inputs that yield a reduced operation cost, in-control solution. The optimizer suggests a set of least cost and least effort alternatives for the equipment engineer or operator. The controller is a PC-driven software solution that resides outside the equipment and does not mandate implementation of recommendations in order to function correctly. The neural model base continues to learn and improve over time. An example of the dynamic process control tool performance is presented retrospectively for a plasma etch system. In this study, the neural networks exhibited overall accuracy to within 20% of the observed values of .986, .938, and .87 for the output quality variables of etch rate, standard deviation, and selectivity, respectively, based on a total sample size of 148 records. The control unit was able to accurately detect the need for parts replacements and wet clean operations in 34 of 40 operations. The controller suggested chamber state variable changes which either improved performance of the output quality variables or adjusted the input variable to a lower cost level without impairment of output quality",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857946,no,no,1486912001.208275
Air quality data remediation by means of ANN,"We present an application of neural networks to air quality time series remediation. The focus has been set on photochemical pollutants, and particularly on ozone, considering statistical correlations between precursors and secondary pollutants. After a preliminary study of the phenomenon, we tried to adapt a predictive MLP (multi layer perceptron) network to fulfill data gaps. The selected input was, along with ozone series, ozone precursors (NO<sub>x</sub>) and meteorological variables (solar radiation, wind velocity and temperature). We then proceeded in selecting the most representative periods for the ozone cycle. We ran all tests for a 80-hours validation set (the most representative gap width in our data base) and an accuracy analysis with respect to gap width as been performed too. In order to maximize the process automation, a software tool has been implemented in the MatlabTM environment. The ANN validation showed generally good results but a considerable instability in data prediction has been found out. The re-introduction of predicted data as input of following simulations generates an uncontrolled error propagation scarcely highlighted by the error autocorrelation analysis usually performed.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1198163,no,no,1486912001.208274
Evaluating software degradation through entropy,"Software systems are affected by degradation as an effect of continuous change. Since late interventions are too much onerous, software degradation should be detected early in the software lifetime. Software degradation is currently detected by using many different complexity metrics, but their use to monitor maintenance activities is costly. These metrics are difficult to interpret, because each emphasizes a particular aspect of degradation and the aspects shown by different metrics are not orthogonal. The purpose of our research is to measure the entropy of a software system to assess its degradation. In this paper, we partially validate the entropy class of metrics by a case study, replicated on successive releases of a set of software systems. The validity is shown through direct measures of software quality, such as the number of detected defects, the maintenance effort and the number of slipped defects",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915530,no,no,1486912001.208273
Cost-effective graceful degradation in speculative processor subsystems: the branch prediction case,"We analyze the effect of errors in branch predictors, a representative example of speculative processor subsystems, to motivate the necessity for fault tolerance in such subsystems. We also describe the design of fault tolerant branch predictors using general fault tolerance techniques. We then propose a fault-tolerant implementation that utilizes the finite state machine (FSM) structure of the pattern history table (PHT) and the set of potential faulty states to predict the branch direction, yet without strictly identifying the correct state. The proposed solution provides virtually the same prediction accuracy as general fault tolerant techniques, while significantly reducing the incurred hardware overhead.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240894,no,no,1486912001.208271
Derivation of Fault Tolerance Measures of Self-Stabilizing Algorithms by Simulation,"Fault tolerance measures can be used to distinguish between different self-stabilizing solutions to the same problem. However, derivation of these measures via analysis suffers from limitations with respect to scalability of and applicability to a wide class of self-stabilizing distributed algorithms. We describe a simulation framework to derive fault tolerance measures for self-stabilizing algorithms which can deal with the complete class of self-stabilizing algorithms. We show the advantages of the simulation framework in contrast to the analytical approach not only by means of accuracy of results, range of applicable scenarios and performance, but also for investigation of the influence of schedulers on a meta level and the possibility to simulate large scale systems featuring dynamic fault probabilities.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4494419,no,no,1486912000.577515
An application of fuzzy clustering to software quality prediction,"The ever increasing demand for high software reliability requires more robust modeling techniques for software quality prediction. The paper presents a modeling technique that integrates fuzzy subtractive clustering with module-order modeling for software quality prediction. First fuzzy subtractive clustering is used to predict the number of faults, then module-order modeling is used to predict whether modules are fault-prone or not. Note that multiple linear regression is a special case of fuzzy subtractive clustering. We conducted a case study of a large legacy telecommunication system to predict whether each module will be considered fault-prone. The case study found that using fuzzy subtractive clustering and module-order modeling, one can classify modules which will likely have faults discovered by customers with useful accuracy prior to release",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888052,no,no,1486912000.577514
Predicting maintainability with object-oriented metrics -an empirical comparison,"<div style=""""font-variant: small-caps; font-size: .9em;"""">First Page of the Article</div><img class=""""img-abs-container"""" style=""""width: 95%; border: 1px solid #808080;"""" src=""""/xploreAssets/images/absImages/01287246.png"""" border=""""0"""">",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1287246,no,no,1486912000.577513
Measurement and quality in object-oriented design,"In order to support the maintenance of object-oriented software systems, the quality of their design must be evaluated using adequate quantification means. In spite of the current extensive use of metrics, if used in isolation, metrics are oftentimes too fine grained to quantify comprehensively an investigated aspect of the design. To help the software engineer detect and localize design problems, the novel detection strategy mechanism is defined so that deviations from good-design principles and heuristics are quantised inform of metrics-based rules. Using detection strategies an engineer can directly localize classes or methods affected by a particular design flaw (e.g. God Class), rather than having to infer the real design problem from a large set of abnormal metric values. In order to reach the ultimate goal of bridging the gap between qualitative and quantitative statements about design, the dissertation proposes a novel type of quality model, called factor-strategy. In contrast to traditional quality models that express the goodness of design in terms of a set of metrics, this novel model relates explicitly the quality of a design to its conformance with a set of essential principles, rules and heuristics, which are quantified using detection strategies.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510177,no,no,1486912000.577511
Quality vs. quantity: comparing evaluation methods in a usability-focused software architecture modification task,"A controlled experiment was performed to assess the usefulness of portions of a usability-supporting architectural pattern (USAP) in modifying the design of software architectures to support a specific usability concern. Results showed that participants using a complete USAP produced modified designs of significantly higher quality than participants using only a usability scenario. Comparison of solution quality ratings with a quantitative measure of responsibilities considered in the solution showed positive correlation between the measures. Implications for software development are that usability concerns can be included at architecture design time, and that USAPs can significantly help software architects to produce better designs to address usability concerns. Implications for empirical software engineering are that validated quantitative measures of software architecture quality may potentially be substituted for costly and often elusive expert assessment.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541823,no,no,1486912000.57751
Modeling of cable fault system,"Modeling is the essential part of implementing the prediction and location of three-phase cable fault. To predict and locate cable fault, a model of three-phase cable fault system is constructed based on a great deal of measured validation data by choosing BP neural network that has nonlinear characteristic and using the unproved BP algorithm, Levenberg-Marquardt data-optimized method. It is shown by the simulation using MATLAB software that the parameters of the model converge rapidly, and the simulated output of the neural network model and the measured output of cable fault system are approximately equal, and the mean value of the relatively predictive error of the fault distance is smaller than 0.3%, so that the model quality is reliable.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1378603,no,no,1486912000.577508
Application of ANN to power system fault analysis,"This paper presents the computer architecture development using Artificial Neural Network (ANN) as an approach for predicting fault in a large interconnected transmission system. Transmission line faults can be classified using the bus voltage and line fault current. Monitoring the performance of these two factors are very useful for power system protection devices. The ANN is designed to be incorporated with a matrix based software tool MATLAB Version 6.0, which deals with fault diagnosis in power system. In MATLAB software modules, the balanced and unbalanced fault can be simulated. The data generated from this software are to be used as training and testing sets in the Neural Ware Simulator.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1033109,no,no,1486912000.577507
Quality metrics of object oriented design for software development and re-development,"The quality of software has an important bearing on the financial and safety aspects in our daily life. Assessing quality of software at the design level will provide ease and higher accuracy for users. However, there is a great gap between the rapid adoption of Object Oriented (OO) techniques and the slow speed of developing corresponding object oriented metric measures, especially object oriented design measures. To tackle this issue, we look into measuring the quality of Object Oriented designs during both software development and re-development processes. A set of OO design metrics has been derived from the existing work found in literature and been further extended. The paper also presents software tools for assisting software re-engineering using the metric measures developed; this has been illustrated with an example of the experiments conducted during our research, finally concluded with the lessons learned and intended further work",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883786,no,no,1486912000.577505
Thresholds for object-oriented measures,"A practical application of object oriented measures is to predict which classes are likely to contain a fault. This is contended to be meaningful because object oriented measures are believed to be indicators of psychological complexity, and classes that are more complex are likely to be faulty. Recently, a cognitive theory was proposed suggesting that there are threshold effects for many object oriented measures. This means that object oriented classes are easy to understand as long as their complexity is below a threshold. Above that threshold their understandability decreases rapidly, leading to an increased probability of a fault. This occurs, according to the theory, due to an overflow of short-term human memory. If this theory is confirmed, then it would provide a mechanism that would explain the introduction of faults into object oriented systems, and would also provide some practical guidance on how to design object oriented programs. The authors empirically test this theory on two C++ telecommunications systems. They test for threshold effects in a subset of the Chidamber and Kemerer (CK) suite of measures (S. Chidamber and C. Kemerer, 1994). The dependent variable was the incidence of faults that lead to field failures. The results indicate that there are no threshold effects for any of the measures studied. This means that there is no value for the studied CK measures where the fault-proneness changes from being steady to rapidly increasing. The results are consistent across the two systems. Therefore, we can provide no support to the posited cognitive theory",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885858,no,no,1486912000.577504
Application of neural network for predicting software development faults using object-oriented design metrics,"In this paper, we present the application of neural network for predicting software development faults including object-oriented faults. Object-oriented metrics can be used in quality estimation. In practice, quality estimation means either estimating reliability or maintainability. In the context of object-oriented metrics work, reliability is typically measured as the number of defects. Object-oriented design metrics are used as the independent variables and the number of faults is used as dependent variable in our study. Software metrics used include those concerning inheritance measures, complexity measures, coupling measures and object memory allocation measures. We also test the goodness of fit of neural network model by comparing the prediction result for software faults with multiple regression model. Our study is conducted on three industrial real-time systems that contain a number of natural faults that has been reported for three years (Mei-Huei Tang et al., 1999).",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201906,no,no,1486912000.577501
Empirical Validation of Three Software Metrics Suites to Predict Fault-Proneness of Object-Oriented Classes Developed Using Highly Iterative or Agile Software Development Processes,"Empirical validation of software metrics suites to predict fault proneness in object-oriented (OO) components is essential to ensure their practical use in industrial settings. In this paper, we empirically validate three OO metrics suites for their ability to predict software quality in terms of fault-proneness: the Chidamber and Kemerer (CK) metrics, Abreu's Metrics for Object-Oriented Design (MOOD), and Bansiya and Davis' Quality Metrics for Object-Oriented Design (QMOOD). Some CK class metrics have previously been shown to be good predictors of initial OO software quality. However, the other two suites have not been heavily validated except by their original proposers. Here, we explore the ability of these three metrics suites to predict fault-prone classes using defect data for six versions of Rhino, an open-source implementation of JavaScript written in Java. We conclude that the CK and QMOOD suites contain similar components and produce statistical models that are effective in detecting error-prone classes. We also conclude that the class components in the MOOD metrics suite are not good class fault-proneness predictors. Analyzing multivariate binary logistic regression models across six Rhino versions indicates these models may be useful in assessing quality in OO classes produced using modern highly iterative or agile software development processes.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4181709,no,no,1486911999.988891
Reduction of False Positives in Polyp Detection Using Weighted Support Vector Machines,"Colorectal cancer is the third highest cause of cancer deaths in US (2007). Early detection and treatment of colon cancer can significantly improve patient prognosis. Manual identification of polyps by radiologists using CT colonography can be labour intensive due to the increasing size of datasets and is error prone due to the complexity of the anatomical structures. There has been increasing interest in computer aided detection (CAD) systems for detecting polyps using CT colonography. For a typical CAD system two major steps can be identified. In the first step image processing techniques are used to detect potential polyp candidates. Many non-polyps are inevitably found in this process. The second step attempts to discount the non-polyp candidates while maintaining true polyps. In practice this is a challenging task as training data is heavily imbalanced, that is, non-polyps dominate the data. This paper describes how the weighted support vector machine (weighted-SVM) can be used to tackle the problem effectively. The weighted-SVM generalises the traditional SVM by applying different penalties to different classes. This trains the classifier to give favour to the most weighted class (in this case true polyps). In this paper the method was applied to data obtained from the intermediate results from a CAD system, originally applied to 209 cases. The results show that the weighted-SVM can play an important role in CAD algorithms for colorectal polyps.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4353322,no,no,1486911999.98889
Unsupervised learning for expert-based software quality estimation,"Current software quality estimation models often involve using supervised learning methods to train a software quality classifier or a software fault prediction model. In such models, the dependent variable is a software quality measurement indicating the quality of a software module by either a risk-based class membership (e.g., whether it is fault-prone or not fault-prone) or the number of faults. In reality, such a measurement may be inaccurate, or even unavailable. In such situations, this paper advocates the use of unsupervised learning (i.e., clustering) techniques to build a software quality estimation system, with the help of a software engineering human expert. The system first clusters hundreds of software modules into a small number of coherent groups and presents the representative of each group to a software quality expert, who labels each cluster as either fault-prone or not fault-prone based on his domain knowledge as well as some data statistics (without any knowledge of the dependent variable, i.e., the software quality measurement). Our preliminary empirical results show promising potentials of this methodology in both predicting software quality and detecting potential noise in a software measurement and quality dataset.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281739,no,no,1486911999.988889
Index,"All organizations today confront data quality problems, both systemic and structural. Neither ad hoc approaches nor fixes at the systems level--installing the latest software or developing an expensive data warehouse--solve the basic problem of bad data quality practices. Journey to Data Quality offers a roadmap that can be used by practitioners, executives, and students for planning and implementing a viable data and information quality management program. This practical guide, based on rigorous research and informed by real-world examples, describes the challenges of data management and provides the principles, strategies, tools, and techniques necessary to meet them.The authors, all leaders in the data quality field for many years, discuss how to make the economic case for data quality and the importance of getting an organization's leaders on board. They outline different approaches for assessing data, both subjectively (by users) and objectively (using sampling and other techniques). They describe real problems and solutions, including efforts to find the root causes of data quality problems at a healthcare organization and data quality initiatives taken by a large teaching hospital. They address setting company policy on data quality and, finally, they consider future challenges on the journey to data quality.",2006,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6290110.pdf&bkn=6267297&pdfType=chapter,no,no,1486911999.988887
References,"All organizations today confront data quality problems, both systemic and structural. Neither ad hoc approaches nor fixes at the systems level--installing the latest software or developing an expensive data warehouse--solve the basic problem of bad data quality practices. Journey to Data Quality offers a roadmap that can be used by practitioners, executives, and students for planning and implementing a viable data and information quality management program. This practical guide, based on rigorous research and informed by real-world examples, describes the challenges of data management and provides the principles, strategies, tools, and techniques necessary to meet them.The authors, all leaders in the data quality field for many years, discuss how to make the economic case for data quality and the importance of getting an organization's leaders on board. They outline different approaches for assessing data, both subjectively (by users) and objectively (using sampling and other techniques). They describe real problems and solutions, including efforts to find the root causes of data quality problems at a healthcare organization and data quality initiatives taken by a large teaching hospital. They address setting company policy on data quality and, finally, they consider future challenges on the journey to data quality.",2006,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6290109.pdf&bkn=6267297&pdfType=chapter,no,no,1486911999.988886
Empirical validation of class diagram metrics,"As a key early artefact in the development of OO software, the quality of class diagrams is crucial for all later design work and could be a major determinant for the quality of the software product that is finally delivered. Quantitative measurement instruments are useful to assess class diagram quality in an objective way, thus avoiding bias in the quality evaluation process. This paper presents a set of metrics - based on UML relationships $which measure UML class diagram structural complexity following the idea that it is related to the maintainability of such diagrams. Also summarized are two controlled experiments carried out in order to gather empirical evidence in this sense. As a result of all the experimental work, we can conclude that most of the metrics we proposed (NAssoc, NAgg, NaggH, MaxHAgg, NGen, NgenH and MaxDIT) are good indicators of class diagram maintainability. We cannot, however, draw such firm conclusions regarding the NDep metric.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166940,no,no,1486911999.988885
A tool for reliability and availability prediction,"As the number of embedded systems in everyday usage has been increasing at an enormous rate recently, the demand for reliable and readily available systems is continuously growing. Problems in reliability and availability are typically detected after system implementation, when fault correction and modifications are difficult and expensive to implement. This is why a new method has been developed for predicting reliability and availability already from architectural models. However, to be beneficial in system development, the prediction method should enable quick, easy and repeatable quality analysis. Therefore, the different procedures and features of the method require tool support. Our contribution is the tool that supports reliability and availability prediction at the architectural level. The tool enables a representation of these two quality attributes in the architecture, and assists in analyzing system reliability and availability using architectural models. The tool has been tested and validated by using it for reliability and availability prediction in a case example.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517768,no,no,1486911999.988882
Empirical evaluation of orthogonality of class mutation operators,"Mutation testing is a fault-based testing technique which provides strong quality assurance. Mutation testing has a very long history for the procedural programs at unit-level testing, but the research on mutation testing of object-oriented programs is still immature. Recently, class mutation operators are proposed to detect object-oriented specific faults. However, any analysis has not been conducted on the class mutation operators. In this paper, we evaluate the orthogonality of the class mutation operators by some experiment. The experimental results show the high possibility that each class mutation operator has fault-revealing power that is not achieved by other mutation operators, i.e. orthogonal. Also, the results show that the number of mutants from the class mutation operators is small so that the cost is not so high as procedural programs.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371955,no,no,1486911999.98888
Fault Detection of Bloom Filters for Defect Maps,"Bloom filters can be used as a data structure for defect maps in nanoscale memory. Unlike most other applications of Bloom filters, both false positive and false negative induced by a fault cause a fatal error in the memory system. In this paper, we present a technique for detecting faults in Bloom filters for defect maps. Spare hashing units and a simple coding technique for bit vectors are employed to detect faults during normal operation. Parallel write/read is also proposed to detect faults with high probability even without spare hashing units.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641177,no,no,1486911999.378454
Two-dimensional TMR with partial majority selection and forwarding,"TMR (triple modular redundancy) is one of the most common forms of fault tolerance approaches, which is based on majority voting. Here, after each checkpoint interval, three modules are compared. If the results of at least two modules are same, the system is considered to be fault-free. In this paper, the authors propose a 2-dimensional TMR scheme (TMR-2D) and partial majority selection and forwarding scheme (PMSF) scheme which use several small sub-checkpoints instead of one large checkpoint at the voting time. With very small amount of overhead, the proposed scheme avoids many rollbacks even though the results of all three modules are different. As a result, the rollback probability and average task execution time are significantly reduced compared to the existing: schemes. The availability is also greatly improved. The proposed scheme will be effective for general fault-tolerant systems, especially for time critical systems",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931838,no,no,1486911999.378451
Fault-tolerant data delivery for multicast overlay networks,"Overlay networks represent an emerging technology for rapid deployment of novel network services and applications. However, since public overlay networks are built out of loosely coupled end-hosts, individual nodes are less trustworthy than Internet routers in carrying out the data forwarding function. Here we describe a set of mechanisms designed to detect and repair errors in the data stream. Utilizing the highly redundant connectivity in overlay networks, our design splits each data stream to multiple sub-streams which are delivered over disjoint paths. Each sub-stream carries additional information that enables receivers to detect damaged or lost packets. Furthermore, each node can verify the validity of data by periodically exchanging Bloom filters, the digests of recently received packets, with other nodes in the overlay. We have evaluated our design through both simulations and experiments over a network testbed. The results show that most nodes can effectively detect corrupted data streams even in the presence of multiple tampering nodes.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281635,no,no,1486911999.378449
An efficient defect estimation method for software defect curves,Software defect curves describe the behavior of the estimate of the number of remaining software defects as software testing proceeds. They are of two possible patterns: single-trapezoidal-like curves or multiple-trapezoidal-like curves. In this paper we present some necessary and/or sufficient conditions for software defect curves of the Goel-Okumoto NHPP model. These conditions can be used to predict the effect of the detection and removal of a software defect on the variations of the estimates of the number of remaining defects. A field software reliability dataset is used to justify the trapezoidal shape of software defect curves and our theoretical analyses. The results presented in this paper may provide useful feedback information for assessing software testing progress and have potentials in the emerging area of software cybernetics that explores the interplay between software and control.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245391,no,no,1486911999.378448
A methodology for detecting performance faults in microprocessors via performance monitoring hardware,"Speculative execution of instructions boosts performance in modern microprocessors. Control and data flow dependencies are overcome through speculation mechanisms, such as branch prediction or data value prediction. Because of their inherent self-correcting nature, the presence of defects in speculative execution units does not affect their functionality (and escapes traditional functional testing approaches) but impose severe performance degradation. In this paper, we investigate the effects of performance faults in speculative execution units and propose a generic, software-based test methodology, which utilizes available processor resources: hardware performance monitors and processor exceptions, to detect these faults in a systematic way. We demonstrate the methodology on a publicly available fully pipelined RISC processor that has been enhanced with the most common speculative execution unit, the branch prediction unit. Two popular schemes of predictors built around a Branch Target Buffer have been studied and experimental results show significant improvements on both cases fault coverage of the branch prediction units increased from 80% to 97%. Detailed experiments for the application of a functional self-testing methodology on a complete RISC processor incorporating both a full pipeline structure and a branch prediction unit have not been previously given in the literature.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437646,no,no,1486911999.378447
Research abstract for semantic anomaly detection in dynamic data feeds with incomplete specifications,"Everyday software must be dependable enough for its intended use. Because this software is not usually mission-critical, it may be cost-effective to detect improper behavior and notify the user or take remedial action. Detecting improper behavior requires a model of proper behavior. Unfortunately, specifications of everyday software are often incomplete and imprecise. The situation is exacerbated when the software incorporates third-party elements such as commercial-off-the-shelf software components, databases, or dynamic data feeds from online data sources. We want to make the use of dynamic data feeds more dependable. We are specifically interested in semantic problems with these feeds-cases in which the data feed is responsive, it delivers well-formed results, but the results are inconsistent, out of range, incorrect, or otherwise unreasonable. We focus on a particular facet of dependability: availability or readiness for usage, and change the fault model from the traditional """"fail-silent"""" (crash failures) to """"semantic"""". We investigate anomaly detection as a step towards increasing the semantic availability of dynamic data feeds.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008060,no,no,1486911999.378445
Complexity signatures for system health monitoring,"The ability to assess risk in complex systems is one of the fundamental challenges facing the aerospace industry in general, and NASA in particular. First, such an ability allows for quantifiable trade-offs during the design stage of a mission. Second, it allows the monitoring of die health of the system while in operation. Because many of the difficulties in complex systems arise from the interactions among the subsystems, system health monitoring cannot solely focus on the health of those subsystems. Instead system level signatures that encapsulate the complex system interactions are needed. In this work, we present the entropy-scale (ES) and entropy-resolution (ER) system-level signatures that are both computationally tractable and encapsulate many of the salient characteristics of a system. These signatures are based on the change of entropy as a system is observed across different resolutions and scales. We demonstrate the use of the ES and ER signatures on artificial data streams and simple dynamical systems and show that they allow the unambiguous clustering of many types of systems, and therefore are good indicators of system health. We then show how these signatures can be applied to graphical data as well as data strings by using a simple """"graph-walking"""" method. This method extracts a data stream from a graphical system representation (e.g., fault tree, software call graph) that conserves the properties of the graph. Finally we apply these signatures to analysis of software packages, and show that they provide significantly better correlation with risk markers than many standard metrics. These results indicate that proper system level signatures, coupled with detailed component-level analysis enable the automatic detection of potentially hazardous subsystem interactions in complex systems before they lead to system deterioration or failures",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559687,no,no,1486911999.378444
Statistical analysis of time series data on the number of faults detected by software testing,"According to a progress of the software process improvement, the time series data on the number of faults detected by the software testing are collected extensively. In this paper, we perform statistical analyses of relationships between the time series data and the field quality of software products. At first, we apply the rank correlation coefficient  to the time series data collected from actual software testing in a certain company, and classify these data into four types of trends: strict increasing, almost increasing, almost decreasing, and strict decreasing. We then investigate, for each type of trend, the field quality of software products developed by the corresponding software projects. As a result of statistical analyses, we showed that software projects having trend of almost or strict decreasing in the number of faults detected by the software testing could produce the software products with high quality.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181723,no,no,1486911999.378442
An interaction testing technique between hardware and software in embedded systems,"An embedded system is an electronically controlled system combining hardware and software. Many systems used in real life such as power plants, medical instrument systems and home appliances are embedded. However, studies related to embedded system testing are insufficient. In embedded systems, it is necessary to develop a test technique to detect faults in interaction between hardware and software. We propose a test data selection technique using fault injection for the interaction between hardware and software. The proposed test data selection technique first simulates behavior of a software program from requirements specification. Hardware faults, after being converted to software faults, are then injected into the simulated program. We finally select effective test data to detect faults caused by the interactions between hardware and software. We apply our technique to a digital plant protection system and evaluate the effectiveness of selected test data through experiments.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183017,no,no,1486911999.378439
How to predict software defect density during proposal phase,"The author has developed a method to predict defect density based on empirical data. The author has evaluated the software development practices of 45 software organizations. Of those, 17 had complete actual observed defect density to correspond to the observed development practices. The author presents the correlation between these practices and defect density in this paper. This correlation can and is used to: (a) predict defect density as early as the proposal phase, (b) evaluate proposals from subcontractors, (c) perform tradeoffs so as to minimize software defect density. It is found that as practices improve, defect density decreases. Contrary to what many software engineers claim, the average probability of a late delivery is less on average for organizations with better practices. Furthermore, the margin of error in the event that a schedule is missed was smaller on average for organizations with better practices. It is also interesting that the average number of corrective action releases required is also smaller for the organizations with the best practices. This means less downtime for customers. It is not surprising that the average SEI CMM level is higher for the organizations with the better practices",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894894,no,no,1486911998.774862
Scale Free in Software Metrics,"Software has become a complex piece of work by the collective efforts of many. And it is often hard to predict what the final outcome will be. This transition poses new challenge to the software engineering (SE) community. By employing methods from the study of complex network, we investigate the object oriented (OO) software metrics from a different perspective. We incorporate the weighted methods per class (WMC) metric into our definition of the weighted OO software coupling network as the node weight. Empirical results from four open source OO software demonstrate power law distribution of weight and a clear correlation between the weight and the out degree. According to its definition, it suggests uneven distribution of function among classes and a close correlation between the functionality of a class and the number of classes it depending on. Further experiment shows similar distribution also exists between average LCOM and WMC as well as out degree. These discoveries will help uncover the underlying mechanisms of software evolution and will be useful for SE to cope with the emerged complexity in software as well as efficient test cases design",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020082,no,no,1486911998.774861
Predicting Qualitative Assessments Using Fuzzy Aggregation,"Given the complexity and sophistication of many contemporary software systems, it is often difficult to gauge the effectiveness, maintainability, extensibility, and efficiency of their underlying software components. A strategy to evaluate the qualitative attributes of a system's components is to use software metrics as quantitative predictors. We present a fusion strategy that combines the predicted qualitative assessments from multiple classifiers with the anticipated outcome that the aggregated predictions are superior to any individual classifier prediction. Multiple linear classifiers are presented with different, randomly selected, subsets of software metrics. In this study, the software components are from a sophisticated biomedical data analysis system, while the external reference test is a thorough assessment of both complexity and maintainability, by a software architect, of each system component. The fuzzy integration results are compared against the best individual classifier operating on a software metric subset",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4216813,no,no,1486911998.774859
Cost Curve Evaluation of Fault Prediction Models,"Prediction of fault prone software components is one of the most researched problems in software engineering. Many statistical techniques have been proposed but there is no consensus on the methodology to select the """"best model"""" for the specific project. In this paper, we introduce and discuss the merits of cost curve analysis of fault prediction models. Cost curves allow software quality engineers to introduce project-specific cost of module misclassification into model evaluation. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Through the analysis of sixteen projects from public repositories, we observe that software quality does not necessarily benefit from the prediction of fault prone components. The inclusion of misclassification cost in model evaluation may indicate that even the """"best"""" models achieve performance no better than trivial classification. Our results support a recommendation to adopt cost curves as one of the standard methods for software quality model performance evaluation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700324,no,no,1486911998.774858
Developing fault predictors for evolving software systems,"Over the past several years, we have been developing methods of predicting the fault content of software systems based on measured characteristics of their structural evolution. In previous work, we have shown there is a significant linear relationship between code churn, a synthesized metric, and the rate at which faults are inserted into the system in terms of number of faults per unit change in code churn. We have begun a new investigation of this relationship with a flight software technology development effort at the jet propulsion laboratory (JPL) and have progressed in resolving the limitations of the earlier work in two distinct steps. First, we have developed a standard for the enumeration of faults. Second, we have developed a practical framework for automating the measurement of these faults. we analyze the measurements of structural evolution and fault counts obtained from the JPL flight software technology development effort. Our results indicate that the measures of structural attributes of the evolving software system are suitable for forming predictors of the number of faults inserted into software modules during their development. The new fault standard also ensures that the model so developed has greater predictive validity.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232479,no,no,1486911998.774856
Improving tree-based models of software quality with principal components analysis,"Software quality classification models can predict which modules are to be considered fault-prone, and which are not, based on software product metrics, process metrics and execution metrics. Such predictions can be used to target improvement efforts to those modules that need them the most. Classification-tree modeling is a robust technique for building such software quality models. However, the model structure may be unstable, and accuracy may suffer when the predictors are highly correlated. This paper presents an empirical case study of four releases of a very large telecommunications system, which shows that the tree-based models can be improved by transforming the predictors with principal components analysis, so that the transformed predictors are not correlated. The case study used the regression-tree algorithm in the S-Plus package and then applied a general decision rule to classify the modules",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885872,no,no,1486911998.774855
Modeling fault-prone modules of subsystems,"Software developers are very interested in targeting software enhancement activities prior to release, so that reworking of faulty modules can be avoided. Credible predictions of which modules are likely to have faults discovered by customers can be the basis for selecting modules for enhancement. Many case studies in the literature build models to predict which modules will be fault-prone without regard to the subsystems defined by the system's functional architecture. Our hypothesis is this: models that are specially built for subsystems will be more accurate than a system-wide model applied to each subsystem's modules. In other words, the subsystem that a module belongs to can be valuable information in software quality modeling. This paper presents an empirical case study which compared software quality models of an entire system to models of a major functional subsystem. The study, modeled a very large telecommunications system with classification trees built by the CART (classification and regression trees) algorithm. For predicting subsystem quality, we found that a model built with training data on the subsystem alone was more accurate than a similar model built with training data on the entire system. We concluded that the characteristics of the subsystem's modules were not similar to those of the system as a whole, and thus, information on subsystems can be valuable",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885877,no,no,1486911998.774853
On the repeatability of metric models and metrics across software builds,"We have developed various software metrics models over the years: Boolean discriminate functions (BDFs); the Kolmogorov-Smirnov distance; derivative calculations for assessing achievable quality; a stopping rule; point and confidence interval estimates of quality; relative critical value deviation metrics; and nonlinear regression functions. We would like these models and metrics to be repeatable across the n builds of a software system. The advantage of repeatability is that models and metrics only need to be developed and validated once on build, and then applied n-1 times without modification to subsequent builds, with considerable savings in analysis and computational effort. In practical terms, this approach involves using the same model parameters that were validated and applying them unchanged on subsequent builds. The disadvantage is that the quality and metrics data of builds 2, ..., n, which varies across builds, is not utilized. We make a comparison of this approach with one that involves validating models and metrics on each build i and applying them only on build i+1, and then repeating the process. The advantage of this approach is that all available data are used in the models and analysis but at considerable cost in effort. We report on experiments involving large sets of discrepancy reports and metrics data on the Space Shuttle flight software, where we compare the predictive accuracy and effort of the two approaches for BDFs, critical values, derivative quality and inspection calculations, and the stopping rule",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885875,no,no,1486911998.774852
"Using product, process, and execution metrics to predict fault-prone software modules with classification trees","Software-quality classification models can make predictions to guide improvement efforts to those modules that need it the most. Based on software metrics, a model can predict which modules will be considered fault-prone, or not. We consider a module fault-prone if any faults were discovered by customers. Useful predictions are contingent on the availability of candidate predictors that are actually related to faults discovered by customers. With a diverse set of candidate predictors in hand, classification-tree modeling is a robust technique for building such software quality models. This paper presents an empirical case study of four releases of a very large telecommunications system. The case study used the regression-tree algorithm in the S-Plus package and then applied our general decision rule to classify modules. Results showed that in addition to product metrics, process metrics and execution metrics were significant predictors of faults discovered by customers",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895475,no,no,1486911998.774851
Classification-tree models of software-quality over multiple releases,"This paper presents an empirical study that evaluates software-quality models over several releases, to address the question, How long will a model yield useful predictions? The classification and regression trees (CART) algorithm is introduced, CART can achieve a preferred balance between the two types of misclassification rates. This is desirable because misclassification of fault-prone modules often has much more severe consequences than misclassification of those that are not fault-prone. The case-study developed 2 classification-tree models based on 4 consecutive releases of a very large legacy telecommunication system. Forty-two software product, process and execution metrics were candidate predictors. Model 1 used measurements of the first release as the training data set; this model had 11 important predictors. Model 2 used measurements of the second release as the training data set; this model had 15 important predictors. Measurements of subsequent releases were evaluation data sets. Analysis of the models' predictors yielded insights into various software development practices. Both models had accuracy that would be useful to developers. One might suppose that software-quality models lose their value very quickly over successive releases due to evolution of the product and the underlying development processes. The authors found the models remained useful over all the releases studied",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=855532,no,no,1486911998.774848
Identifying effective software metrics using genetic algorithms,"Various software metrics may be used to quantify object-oriented source code characteristics in order to assess the quality of the software. This type of software quality assessment may be viewed as a problem of classification: given a set of objects with known features (software metrics) and group labels (quality rankings), design a classifier that can predict the quality rankings of new objects using only the software metrics. We have obtained a variety of software measures for a Java application used for biomedical data analysis. A system architect has ranked the quality of the objects as low, medium-low, medium or high with respect to maintainability. A commercial program was used to parse the source code identifying 16 metrics. A genetic algorithm (GA) was implemented to determine which subset of the various software metrics gave the best match to the quality ranking specified by the expert. By selecting the optimum metrics for determining object quality, GA-based feature selection offers an insight into which software characteristics developers should try to optimize.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1226139,no,no,1486911998.189487
Towards Portable Metrics-based Models for Software Maintenance Problems,"The usage of software metrics for various purposes has become a hot research topic in academia and industry (e.g. detecting design patterns and bad smells, studying change-proneness, quality and maintainability, predicting faults). Most of these topics have one thing in common: they are all using some kind of metrics-based models to achieve their goal. Unfortunately, only few researchers have tested these models on unknown software systems so far. This paper tackles the question, which metrics are suitable for preparing portable models (which can be efficiently applied to unknown software systems). We have assessed several metrics on four large software systems and we found that the well-known RFC and WMC metrics differentiate the analyzed systems fairly well. Consequently, these metrics cannot be used to build portable models, while the CBO, LCOM and LOC metrics behave similarly on all systems, so they seem to be suitable for this purpose",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021377,no,no,1486911998.189486
Considering the Dependency of Fault Detection and Correction in Software Reliability Modeling,"Most existing software reliability growth models (SRGMs) focused on the fault detection process, while the fault correction process was ignored by assuming that the detected faults can be removed immediately and perfectly. However, these assumptions are not realistic. The fault correction process is a critical part in software testing. In this paper, we studied the dependency of the fault detection and correction processes in view of the number of faults. The ratio of corrected fault number to detected fault number is used to describe the dependency of the two processes, which appears S-shaped. Therefore, we adopt the logistical function to represent the ratio function. Based on this function, both fault correction and detection processes are modeled. The proposed models are evaluated by a data set of software testing. The experimental results show that the new models fit the data set of fault detection and correction processes very well.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722140,no,no,1486911998.189484
Predicting Defects for Eclipse,"We have mapped defects from the bug database of eclipse (one of the largest open-source projects) to source code locations. The resulting data set lists the number of pre- and post-release defects for every package and file in the eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data with common complexity metrics. All data is publicly available and can serve as a benchmark for defect prediction models.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273265,no,no,1486911998.189483
An analogy-based approach for predicting design stability of Java classes,"Predicting stability in object-oriented (OO) software, i.e., the ease with which a software item evolves while preserving its design, is a key feature for software maintenance. In fact, a well designed OO software must be able to evolve without violating the compatibility among versions, provided that no major requirement reshuffling occurs. Stability, like most quality factors, is a complex phenomenon and its prediction is a real challenge. We present an approach, which relies on the case-based reasoning (CBR) paradigm and thus overcomes the handicap of insufficient theoretical knowledge on stability. The approach explores structural similarities between classes, expressed as software metrics, to guess their chances of becoming unstable. In addition, our stability model binds its value to the impact of changing requirements, i.e., the degree of class responsibilities increase between versions, quantified as the stress factor. As a result, the prediction mechanism favours the stability values for classes having strong structural analogies with a given test class as well as a similar stress impact. Our predictive model is applied on a testbed made up of the classes from four major version of the Java API.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232472,no,no,1486911998.189481
Data quality monitor of the muon spectrometer tracking detectors of the ATLAS experiment at the Large Hadron Collider: First experience with cosmic rays,"The Muon Spectrometer of the ATLAS experiment at the CERN Large Hadron Collider is completely installed and many data have been collected with cosmic rays in different trigger configurations. In the barrel part of the spectrometer, cosmic ray muons are triggered with Resistive Plate Chambers, RPC, and tracks are obtained joining segments reconstructed in three measurement stations equipped with arrays of high-pressure drift tubes, MDT. The data are used to validate the software tools for the data extraction, to assess the quality of the drift tubes response and to test the performance of the tracking programs. We present a first survey of the MDT data quality based on large samples of cosmic ray data selected by the second level processors for the calibration stream. This data stream was set up to provide high statistics needed for the continuous monitor and calibration of the drift tubes response. Track segments in each measurement station are used to define quality criteria and to assess the overall performance of the MDT detectors. Though these data were taken in not optimized conditions, when the gas temperature and pressure was not stabilized, the analysis of track segments shows that the MDT detector system works properly and indicates that the efficiency and space resolution are in line with the results obtained with previous tests with a high energy muon beam.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774959,no,no,1486911998.18948
Resource Availability Prediction in Fine-Grained Cycle Sharing Systems,"Fine-grained cycle sharing (FGCS) systems aim at utilizing the large amount of computational resources available on the Internet. In FGCS, host computers allow guest jobs to utilize the CPU cycles if the jobs do not significantly impact the local users of a host. A characteristic of such resources is that they are generally provided voluntarily and their availability fluctuates highly. Guest jobs may fail because of unexpected resource unavailability. To provide fault tolerance to guest jobs without adding significant computational overhead, it requires to predict future resource availability. This paper presents a method for resource availability prediction in FGCS systems. It applies a semi-Markov Process and is based on a novel resource availability model, combining generic hardware-software failures with domain-specific resource behavior in FGCS. We describe the prediction framework and its implementation in a production FGCS system named iShare. Through the experiments on an iShare testbed, we demonstrate that the prediction achieves accuracy above 86% on average and outperforms linear time series models, while the computational cost is negligible. Our experimental results also show that the prediction is robust in the presence of irregular resource unavailability",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652140,no,no,1486911998.189478
A multiobjective module-order model for software quality enhancement,"The knowledge, prior to system operations, of which program modules are problematic is valuable to a software quality assurance team, especially when there is a constraint on software quality enhancement resources. A cost-effective approach for allocating such resources is to obtain a prediction in the form of a quality-based ranking of program modules. Subsequently, a module-order model (MOM) is used to gauge the performance of the predicted rankings. From a practical software engineering point of view, multiple software quality objectives may be desired by a MOM for the system under consideration: e.g., the desired rankings may be such that 100% of the faults should be detected if the top 50% of modules with highest number of faults are subjected to quality improvements. Moreover, the management team for the same system may also desire that 80% of the faults should be accounted if the top 20% of the modules are targeted for improvement. Existing work related to MOM(s) use a quantitative prediction model to obtain the predicted rankings of program modules, implying that only the fault prediction error measures such as the average, relative, or mean square errors are minimized. Such an approach does not provide a direct insight into the performance behavior of a MOM. For a given percentage of modules enhanced, the performance of a MOM is gauged by how many faults are accounted for by the predicted ranking as compared with the perfect ranking. We propose an approach for calibrating a multiobjective MOM using genetic programming. Other estimation techniques, e.g., multiple linear regression and neural networks cannot achieve multiobjective optimization for MOM(s). The proposed methodology facilitates the simultaneous optimization of multiple performance objectives for a MOM. Case studies of two industrial software systems are presented, the empirical results of which demonstrate a new promise for goal-oriented software quality modeling.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1369249,no,no,1486911998.189477
Behavioral Dependency Measurement for Change-Proneness Prediction in UML 2.0 Design Models,"During the development and maintenance of object-oriented (OO) software, the information on the classes which are more prone to be changed is very useful. Developers and maintainers can make a more flexible software by modifying the part of classes which are sensitive to changes. Traditionally, most change-proneness prediction has been studied based on source codes. However, change-proneness prediction in the early phase of software development can provide an easier way for developing a stable software by modifying the current design or choosing alternative designs before implementation. To address this need, we present a systematic method for calculating the behavioral dependency measure (BDM) which helps to predict change-proneness in UML 2.0 models. The proposed measure has been evaluated on a multi-version medium size open-source project namely JFreeChart. The obtained results show that the BDM is an useful indicator and can be complementary to existing OO metrics for change-proneness prediction.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591537,no,no,1486911998.189474
Building a genetically engineerable evolvable program (GEEP) using breadth-based explicit knowledge for predicting software defects,"There has been extensive research in the area of data mining over the last decade, but relatively little research in algorithmic mining. Some researchers shun the idea of incorporating explicit knowledge with a Genetic Program environment. At best, very domain specific knowledge is hard wired into the GP modeling process. This work proposes a new approach called the Genetically Engineerable Evolvable Program (GEEP). In this approach, explicit knowledge is made available to the GP. It is considered breadth-based, in that all pieces of knowledge are independent of each other. Several experiments are performed on a NASA-based data set using established equations from other researchers in order to predict software defects. All results are statistically validated.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1336240,no,no,1486911997.657983
Modeling software quality: the Software Measurement Analysis and Reliability Toolkit,"The paper presents the Software Measurement Analysis and Reliability Toolkit (SMART) which is a research tool for software quality modeling using case based reasoning (CBR) and other modeling techniques. Modern software systems must have high reliability. Software quality models are tools for guiding reliability enhancement activities to high risk modules for maximum effectiveness and efficiency. A software quality model predicts a quality factor, such as the number of faults in a module, early in the life cycle in time for effective action. Software product and process metrics can be the basis for such fault predictions. Moreover, classification models can identify fault prone modules. CBR is an attractive modeling method based on automated reasoning processes. However, to our knowledge, few CBR systems for software quality modeling have been developed. SMART addresses this area. There are currently three types of models supported by SMART: classification based on CBR, CBR classification extended with cluster analysis, and module-order models, which predict the rank-order of modules according to a quality factor. An empirical case study of a military command, control, and communications applied SMART at the end of coding. The models built by SMART had a level of accuracy that could be very useful to software developers",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889846,no,no,1486911997.657982
Quality Assessment of Enterprise Software Systems,"In the last years, as object-oriented software systems became more and more complex, the need of having tools that help us to understand and to assess the quality of their design has increased significantly. This applies also to enterprise applications, a novel category of software systems. Unfortunately, the existing techniques for design's understanding and quality assessment of object-oriented systems are not sufficient and sometimes not suitable when applied on enterprise applications. In the current Ph.D. we propose a new approach which increases the level of understanding and the accuracy assessment of the design of enterprise software systems",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4024007,no,no,1486911997.65798
Improving Accuracy of Multiple Regression Analysis for Effort Prediction Model,"In this paper, we outline the effort prediction model and the evaluation experiment. In addition we explore the parameters in the model. The model predicts effort of embedded software developments via multiple regression analysis using the collaborative filtering. Because companies, recently, focus on methods to predict effort of projects, which prevent project failures such as exceeding deadline and cost, due to more complex embedded software, which brings the evolution of the performance and function enhancement. In the model, we have fixed two parameters named k and ampmax, which would influence the accuracy of predicting effort. Hence, we investigate a tendency of them in the model and find the optimum value",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1651969,no,no,1486911997.657979
Discrimination of software quality in a biomedical data analysis system,"Object-oriented visualization-based software systems for biomedical data analysis must deal with complex and voluminous datasets within a flexible yet intuitive graphical user interface. In a research environment, the development of such systems are difficult to manage due to rapidly changing requirements, incorporation of newly developed algorithms, and the needs imposed by a diverse user base. One issue that research supervisors must contend with is an assessment of the quality of the system's software objects with respect to their extensibility, reusability, clarity, and efficiency. Objects from a biomedical data analysis system were independently analyzed by two software architects and ranked according to their quality. Quantitative software features were also compiled at varying levels of granularity. The discriminatory power of these software metrics is discussed and their effectiveness in assessing and predicting software object quality is described",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=943808,no,no,1486911997.657977
DCPD acquisition and analysis for HV storage capacitor based on Matlab,"High-voltage storage capacitor is the key device in weapon system, high reliability is required. In the process of storage and application, effective measurements are needed to ensure the insulation capability of capacitors. Usually, partial discharge (PD) is used to detect the insulation status of capacitors. Under DC there does not exist fundamental parameter Phi , so a new parameter delta (t) is introduced .Adopting data acquisition and analysis system with single trigger based on Matlab software, PD of four typical defect models under DC condition is obtained. Data of DCPD signals was analyzed through q, n, delta (t) distribution figures, from which obvious differences can be obtained. All results can provide data foundation for pattern recognition of high-voltage storage capacitors.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4451532,no,no,1486911997.657975
An investigation of the applicability of design of experiments to software testing,"Approaches to software testing based on methods from the field of design of experiments have been advocated as a means of providing high coverage at relatively low cost. Tools to generate all pairs, or higher n-degree combinations, of input values have been developed and demonstrated in a few applications, but little empirical evidence is available to aid developers in evaluating the effectiveness of these tools for particular problems. We investigate error reports from two large open-source software projects, a browser and Web server, to provide preliminary answers to three questions: Is there a point of diminishing returns at which generating all n-degree combinations is nearly as effective as all n+1-degree combinations? What is the appropriate value of n for particular classes of software? Does this value differ for different types of software, and by how much? Our findings suggest that more than 95% of errors in the software studied would be detected by test cases that cover all 4-way combinations of values, and that the browser and server software were similar in the percentage of errors detectable by combinations of degree 2 through 6.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199454,no,no,1486911997.657974
Performance analysis of software rejuvenation,"Cluster-based systems, a combination of interconnected individual computers, have become a popular solution to build the scalable and highly available Web servers. In order to reduce system outages due to aging phenomenon, software rejuvenation, a proactive fault-tolerance strategy has been introduced into cluster systems. Compared with clusters of a flat architecture, in which all the nodes share the same functions, we model and analyze the dispatcher-worker based cluster systems, which employ prediction-based rejuvenation both on the dispatcher and the worker pool. To evaluate the effects of rejuvenation, stochastic reward net models are constructed and solved by SPNP (stochastic Petri net package). Numerical results show that prediction-based software rejuvenation can significantly increase system availability and reduce the expected job loss probability.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1236365,no,no,1486911997.657973
"Hydratools, a MATLAB based data processing package for Sontek Hydra data","The U.S. Geological Survey (USGS) has developed a set of MATLAB tools to process and convert data collected by Sontek Hydra instruments to netCDF, which is a format used by the USGS to process and archive oceanographic time-series data. The USGS makes high-resolution current measurements within 1.5 meters of the bottom. These data are used in combination with other instrument data from sediment transport studies to develop sediment transport models. Instrument manufacturers provide software which outputs unique binary data formats. Multiple data formats are cumbersome. The USGS solution is to translate data streams into a common data format: netCDF. The Hydratools toolbox is written to create netCDF format files following EPIC conventions, complete with embedded metadata. Data are accepted from both the ADV and the PCADP. The toolbox will detect and remove bad data, substitute other sources of heading and tilt measurements if necessary, apply ambiguity corrections, calculate statistics, return information about data quality, and organize metadata. Standardized processing and archiving makes these data more easily and routinely accessible locally and over the Internet. In addition, documentation of the techniques used in the toolbox provides a baseline reference for others utilizing the data.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1506360,no,no,1486911997.657971
Detecting Fault Modules Applying Feature Selection to Classifiers,"At present, automated data collection tools allow us to collect large amounts of information, not without associated problems. This paper, we apply feature selection to several software engineering databases selecting attributes with the final aim that project managers can have a better global vision of the data they manage. In this paper, we make use of attribute selection techniques in different datasets publicly available (PROMISE repository), and different data mining algorithms for classification to defect faulty modules. The results show that in general, smaller datasets with less attributes maintain or improve the prediction capability with less attributes than the original datasets.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296696,no,no,1486911997.657969
Reversible Data Hiding Based on Histogram Shifting of Prediction Errors,"In this paper, a reversible data hiding scheme based on histogram-shifting of prediction errors (HSPE) is proposed. Two-stage structures, the prediction stage and the error modification stage, are employed in our scheme. In the prediction stage, value of each pixel is predicted, and the error of the predicted value is obtained. In the error modification stage, histogram-shifting technique is used to prepare vacant positions for embedding data. The peak signal-to-noise ratio (PSNR) of the stego image produced by HSPE is guaranteed to be above 48 dB, while the embedding capacity is, in average, 4.74 times higher than that of the well known Ni et al.psilas technique with the same PSNR. Besides, the stego image quality produced by HSPE gains 7.99 dB higher than that of Ni et al.psilas method under the same embedding capacity. Experimental results indicate that the proposed data hiding scheme outperforms the prior works not only in terms of larger payload, but also in terms of stego image quality.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731936,no,no,1486911997.167197
Generalizing fault contents from a few classes,"The challenges in fault prediction today are to get a prediction as early as possible, at as low a cost as possible, needing as little data as possible and preferably in such a language that your average developer can understand where it came from. This paper presents a fault sampling method where a summary of a few, easily available metrics is used together with the results of a few sampled classes to generalize the fault content to an entire system. The method is tested on a large software system written in Java, that currently consists of around 2000 classes and 300,000 lines of code. The evaluation shows that the fault generalization method is good at predicting fault-prone clusters and that it is possible to generalize the values of a few representative classes.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343748,no,no,1486911997.167196
Parsimonious classifiers for software quality assessment,"Modeling to predict fault-proneness of software modules is an important area of research in software engineering. Most such models employ a large number of basic and derived metrics as predictors. This paper presents modeling results based on only two metrics, lines of code and cyclomatic complexity, using radial basis functions with Gaussian kernels as classifiers. Results from two NASA systems are presented and analyzed.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404779,no,no,1486911997.167195
Fault Tree Based Prediction of Software Systems Safety,"In our modern world, software controls much of the hardware (equipment, electronics, and instruments) around us. Sometimes hardware failure can lead to a loss of human life. When software controls, operates, or interacts with such hardware, software safety becomes a vital concern. To assure the safety of software controlling system, prediction of software safety should be done at the beginning of systempsilas design. The paper focused on safety prediction using the key node property of fault trees. This metric use parameter """"s"""" related to the fault tree to predict the safety of the software control systems. This metric allow designers to measure and the safety of software systems early in the design process. An applied example is shown in the paper.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722141,no,no,1486911997.167193
Fault Detection and Analysis of Control Software for a Mobile Robot,"In certain circumstances mobile robots are unreachable from human being, for example Mars exploration rover. So robots should detect and handle faults of control software themselves. This paper is intended to detect faults of control software by computers. Support vector machine (SVM) based classification is applied to fault diagnostics of control software for a mobile robot. Both training and testing data are sampled by simulating several fault software strategies and recording the operation parameters of the robot. The correct classification percentages for different situations are discussed",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021552,no,no,1486911997.167192
A DSP-based FFT-analyzer for the fault diagnosis of rotating machine based on vibration analysis,"A DSP-based measurement system dedicated to the vibration analysis on rotating machines was designed and realized. Vibration signals are on-line acquired and processed to obtain a continuous monitoring of the machine status. In case of fault, the system is capable of isolating the fault with a high reliability. The paper describes in detail the approach followed to built up fault and unfault models together with the chosen hardware and software solutions. A number of tests carried out on small-size three-phase asynchronous motors highlights high promptness in detecting faults, low false alarm rate, and very good diagnostic performance",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=928883,no,no,1486911997.16719
Induction machine condition monitoring with higher order spectra,"This paper describes a novel method of detecting and unambiguously diagnosing the type and magnitude of three induction machine fault conditions from the single sensor measurement of the radial electromagnetic machine vibration. The detection mechanism is based on the hypothesis that the induction machine can be considered as a simple system, and that the action of the fault conditions are to alter the output of the system in a characteristic and predictable fashion. Further, the change in output and fault condition can be correlated allowing explicit fault identification. Using this technique, there is no requirement for a priori data describing machine fault conditions, the method is equally applicable to both sinusoidally and inverter-fed induction machines and is generally invariant of both the induction machine load and speed. The detection mechanisms are rigorously examined theoretically and experimentally, and it is shown that a robust and reliable induction machine condition-monitoring system has been produced. Further, this technique is developed into a software-based automated commercially applicable system",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873211,no,no,1486911997.167189
Identifying comprehension bottlenecks using program slicing and cognitive complexity metrics,Achieving and maintaining high software quality is most dependent on how easily the software engineer least familiar with the system can understand the system's code. Understanding attributes of cognitive processes can lead to new software metrics that allow the prediction of human performance in software development and for assessing and improving the understandability of text and code. In this research we present novel metrics based on current understanding of short-term memory performance to predict the location of high frequencies of errors and to evaluate the quality of a software system. We further enhance these metrics by applying static and dynamic program slicing to provide programmers with additional guidance during software inspection and maintenance efforts.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199195,no,no,1486911997.167187
Emulation of Software Faults: A Field Data Study and a Practical Approach,"The injection of faults has been widely used to evaluate fault tolerance mechanisms and to assess the impact of faults in computer systems. However, the injection of software faults is not as well understood as other classes of faults (e.g., hardware faults). In this paper, we analyze how software faults can be injected (emulated) in a source-code independent manner. We specifically address important emulation requirements such as fault representativeness and emulation accuracy. We start with the analysis of an extensive collection of real software faults. We observed that a large percentage of faults falls into well-defined classes and can be characterized in a very precise way, allowing accurate emulation of software faults through a small set of emulation operators. A new software fault injection technique (G-SWFIT) based on emulation operators derived from the field study is proposed. This technique consists of finding key programming structures at the machine code-level where high-level software faults can be emulated. The fault-emulation accuracy of this technique is shown. This work also includes a study on the key aspects that may impact the technique accuracy. The portability of the technique is also discussed and it is shown that a high degree of portability can be achieved",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015509,no,no,1486911997.167185
Predicting Defective Software Components from Code Complexity Measures,"The ability to predict defective modules can help us allocate limited quality assurance resources effectively and efficiently. In this paper, we propose a complexity- based method for predicting defect-prone components. Our method takes three code-level complexity measures as input, namely Lines of Code, McCabe's Cyclomatic Complexity and Halstead's Volume, and classifies components as either defective or non-defective. We perform an extensive study of twelve classification models using the public NASA datasets. Cross-validation results show that our method can achieve good prediction accuracy. This study confirms that static code complexity measures can be useful indicators of component quality.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459644,no,no,1486911997.167183
Software quality prediction techniques: A comparative analysis,"There are many software quality prediction techniques available in literature to predict software quality. However, literature lacks a comprehensive study to evaluate and compare various prediction methodologies so that quality professionals may select an appropriate predictor. To find a technique which performs better in general is an undecidable problem because behavior of a predictor also depends on many other specific factors like problem domain, nature of dataset, uncertainty in the available data etc. We have conducted an empirical survey of various software quality prediction techniques and compared their performance in terms of various evaluation metrics. In this paper, we have presented comparison of 30 techniques on two standard datasets.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777508,no,no,1486911996.707623
A study on fault-proneness detection of object-oriented systems,"Fault-proneness detection in object-oriented systems is an interesting area for software companies and researchers. Several hundred metrics have been defined with the aim of measuring the different aspects of object-oriented systems. Only a few of them have been validated for fault detection, and several interesting works with this view have been considered. This paper reports a research study starting from the analysis of more than 200 different object-oriented metrics extracted from the literature with the aim of identifying suitable models for the detection of the fault-proneness of classes. Such a large number of metrics allows the extraction of a subset of them in order to obtain models that can be adopted for fault-proneness detection. To this end, the whole set of metrics has been classified on the basis of the measured aspect in order to reduce them to a manageable number; then, statistical techniques were employed to produce a hybrid model comprised of 12 metrics. The work has focused on identifying models that can detect as many faulty classes as possible and, at the same time, that are based on a manageably small set of metrics. A compromise between these aspects and the classification correctness of faulty and non-faulty classes was the main challenge of the research. As a result, two models for fault-proneness class detection have been obtained and validated",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914976,no,no,1486911996.707621
A Novel Hybrid Approach of KPCA and SVM for Crop Quality Classification,"Quality evaluation and classification is very important for crop market price determination. A lot of methods have been applied in the field of quality classification including principal component analysis (PCA) and artificial neural network (ANN) etc. The use of ANN has been shown to be a cost-effective technique. But their training is featured with some drawbacks such as small sample effect, black box effect and prone to overfitting. This paper proposes a novel hybrid approach of kernel principal component analysis (KPCA) with support vector machine (SVM) for developing the accuracy of quality classification. The tobacco quality data is evaluated in the experiment. Traditional PCA-SVM, SVM and ANN are investigated as comparison basis. The experimental results show that the proposed approach can achieve better performance in crop quality classification.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722724,no,no,1486911996.707619
Modelling the fault correction process,"In general, software reliability models have focused on modeling and predicting failure occurrence and have not given equal priority to modeling the fault correction process. However, there is a need for fault correction prediction, because there are important applications that fault correction modeling and prediction support. These are the following: predicting whether reliability goals have been achieved, developing stopping rules for testing, formulating test strategies, and rationally allocating test resources. Because these factors are related, we integrate them in our model. Our modeling approach involves relating fault correction to failure prediction, with a time delay between failure detection and fault correction, represented by a random variable whose distribution parameters are estimated from observed data.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989472,no,no,1486911996.707618
Assessing the applicability of fault-proneness models across object-oriented software projects,"A number of papers have investigated the relationships between design metrics and the detection of faults in object-oriented software. Several of these studies have shown that such models can be accurate in predicting faulty classes within one particular software product. In practice, however, prediction models are built on certain products to be used on subsequent software development projects. How accurate can these models be, considering the inevitable differences that may exist across projects and systems? Organizations typically learn and change. From a more general standpoint, can we obtain any evidence that such models are economically viable tools to focus validation and verification effort? This paper attempts to answer these questions by devising a general but tailorable cost-benefit model and by using fault and design data collected on two mid-size Java systems developed in the same environment. Another contribution of the paper is the use of a novel exploratory analysis technique - MARS (multivariate adaptive regression splines) to build such fault-proneness models, whose functional form is a-priori unknown. The results indicate that a model built on one system can be accurately used to rank classes within another system according to their fault proneness. The downside, however, is that, because of system differences, the predicted fault probabilities are not representative of the system predicted. However, our cost-benefit model demonstrates that the MARS fault-proneness model is potentially viable, from an economical standpoint. The linear model is not nearly as good, thus suggesting a more complex model is required.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1019484,no,no,1486911996.707616
Towards the Optimization of Automatic Detection of Design Flaws in Object-Oriented Software Systems,"In order to increase the maintainability and the flexibility of a software, its design and implementation quality must be properly assessed. For this purpose a large number of metrics and several higher-level mechanisms based on metrics are defined in literature. But the accuracy of these quantification means is heavily dependent on the proper selection of threshold values, which is oftentimes totally empirical and unreliable. In this paper we present a novel method for establishing proper threshold values for metrics-based rules used to detect design flaws in object-oriented systems. The method, metaphorically called """"tuning machine"""", is based on inferring the threshold values based on a set of reference examples, manually classified in """"flawed"""" respectively """"healthy"""" design entities (e.g., classes, methods). More precisely, the """"tuning machine"""" searches, based on a genetic algorithm, for those thresholds which maximize the number of correctly classified entities. The paper also defines a repeatable process for collecting examples, and discusses the encouraging and intriguing results while applying the approach on two concrete metrics-based rules that quantify two well-known design flaws i.e., """"God Class"""" and """"Data Class"""".",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402118,no,no,1486911996.707614
Application of Random Forest in Predicting Fault-Prone Classes,"There are available metrics for predicting fault prone classes, which may help software organizations for planning and performing testing activities. This may be possible due to proper allocation of resources on fault prone parts of the design and code of the software. Hence, importance and usefulness of such metrics is understandable, but empirical validation of these metrics is always a great challenge. Random forest (RF) algorithm has been successfully applied for solving regression and classification problems in many applications. This paper evaluates the capability of RF algorithm in predicting fault prone software classes using open source software. The results indicate that the prediction performance of random forest is good. However, similar types of studies are required to be carried out in order to establish the acceptability of the RF model.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736919,no,no,1486911996.707613
Predicting Fault Proneness of Classes Trough a Multiobjective Particle Swarm Optimization Algorithm,"Software testing is a fundamental software engineering activity for quality assurance that is also traditionally very expensive. To reduce efforts of testing strategies, some design metrics have been used to predict the fault-proneness of a software class or module. Recent works have explored the use of machine learning (ML) techniques for fault prediction. However most used ML techniques can not deal with unbalanced data and their results usually have a difficult interpretation. Because of this, this paper introduces a multi-objective particle swarm optimization (MOPSO) algorithm for fault prediction. It allows the creation of classifiers composed by rules with specific properties by exploring Pareto dominance concepts. These rules are more intuitive and easier to understand because they can be interpreted independently one of each other. Furthermore, an experiment using the approach is presented and the results are compared to the other techniques explored in the area.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669800,no,no,1486911996.707611
Comparing fault-proneness estimation models,"Over the last, years, software quality has become one of the most important requirements in the development of systems. Fault-proneness estimation could play a key role in quality control of software products. In this area, much effort has been spent in defining metrics and identifying models for system assessment. Using this metrics to assess which parts of the system are more fault-proneness is of primary importance. This paper reports a research study begun with the analysis of more than 100 metrics and aimed at producing suitable models for fault-proneness estimation and prediction of software modules/files. The objective has been to find a compromise between the fault-proneness estimation rate and the size of the estimation model in terms of number of metrics used in the model itself. To this end, two different methodologies have been used, compared, and some synergies exploited. The methodologies were the logistic regression and the discriminant analyses. The corresponding models produced for fault-proneness estimation and prediction have been based on metrics addressing different aspects of computer programming. The comparison has produced satisfactory results in terms of fault-proneness prediction. The produced models have been cross validated by using data sets derived from source codes provided by two application scenarios.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467901,no,no,1486911996.707609
The quality of design team factors on software effort estimation,"Over the past ten couple of years, there is a variety of effort models proposed by academicians and practitioners at early stage of software development life cycle. Some addressed that efforts could be predicted using lines of codes (LOC) and COCOMO, others emphasized that it could be made using function point analysis (FPA) or others. The study seeks to develop a model that estimates software effort by studying and analyzing small and medium scale application software. To develop such a model, 50 completed software projects are collected from a software company. With the sample data, design team factors are identified and extracted. By applying them to simple regression analyses, a prediction of software of effort estimates with accuracy of MMRE=9% was constructed. The results give several benefits. First, the estimation problems are minimized due to the simple procedure used in identifying those factors. Second, the predicted software projects are only limited to a specific environment rather than being based upon industry environment. We believe the accuracy of effort estimates can be improved. According to the results analyzed, the work shows that it is possible to build up simple and useful prediction model based on data extracted at the early stage of software development life cycle. We hope this model can provide valuable ideas and suggestions for project designers for planning and controlling software projects in near future",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4125542,no,no,1486911996.271845
Advanced Fault Analysis Software System (or AFAS) for Distribution Power Systems,"An advanced fault analysis software system (or AFAS) is currently being developed at Concurrent Technologies Corporation (CTC) to automatically detect and locate low and high impedance, momentary and permanent faults in distribution power systems. Microsoft Visual Studio is used to integrate advanced software packages and analysis tools (including DEW, AEMPFAST, PSCAD, and CTC's DSFL) under the AFAS platform. AFAS is an intelligent, operational, decision-support fault analysis tool that utilizes PSCAD to simulate fault transients of distribution systems to improve fault location accuracy of DSFL tool and to enhance DSFL capabilities for predicting low and high impedance, momentary and permanent faults. The implementation and evaluation results of this software tool have been presented.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402372,no,no,1486911996.271844
Experimental Study of Discriminant Method with Application to Fault-Prone Module Detection,"Some techniques have been applied to improving software quality by classifying the software modules into fault-prone or non fault-prone categories. This can help developers focus on some high risk fault-prone modules. In this paper, a distribution-based Bayesian quadratic discriminant analysis (D-BQDA) technique is experimental investigated to identify software fault-prone modules. Experiments with software metrics data from two real projects indicate that this technique can classify software modules into a proper class with a lower misclassification rate and a higher efficiency.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724642,no,no,1486911996.271841
Analyzing software measurement data with clustering techniques,"For software quality estimation, software development practitioners typically construct quality-classification or fault prediction models using software metrics and fault data from a previous system release or a similar software project. Engineers then use these models to predict the fault proneness of software modules in development. Software quality estimation using supervised-learning approaches is difficult without software fault measurement data from similar projects or earlier system releases. Cluster analysis with expert input is a viable unsupervised-learning solution for predicting software modules' fault proneness and potential noisy modules. Data analysts and software engineering experts can collaborate more closely to construct and collect more informative software metrics.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1274907,no,no,1486911996.27184
Utilizing Computational Intelligence in Estimating Software Readiness,"Defect tracking using computational intelligence methods is used to predict software readiness in this study. By comparing predicted number of faults and number of faults discovered in testing, software managers can decide whether the software are ready to be released or not. Our predictive models can predict: (i) the number of faults (defects), (ii) the amount of code changes required to correct a fault and (iii) the amount of time (in minutes) to make the changes in respective object classes using software metrics as independent variables. The use of neural network model with a genetic training strategy is introduced to improve prediction results for estimating software readiness in this study. Existing object-oriented metrics and complexity software metrics are used in the Business Tier neural network based prediction model. New sets of metrics have been defined for the Presentation Logic Tier and Data Access Tier.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1716506,no,no,1486911996.271838
A maintainability model for industrial software systems using design level metrics,"Software maintenance is a time consuming and expensive phase of a software product's life-cycle. The paper investigates the use of software design metrics to statistically estimate the maintainability of large software systems, and to identify error prone modules. A methodology for assessing, evaluating and, selecting software metrics for predicting software maintainability is presented. In addition, a linear prediction model based on a minimal set of design level software metrics is proposed. The model is evaluated by applying it to industrial software systems",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=891476,no,no,1486911996.271835
Quantitative analysis of faults and failures in a complex software system,"The authors describe a number of results from a quantitative study of faults and failures in two releases of a major commercial software system. They tested a range of basic software engineering hypotheses relating to: the Pareto principle of distribution of faults and failures; the use of early fault data to predict later fault and failure data; metrics for fault prediction; and benchmarking fault data. For example, we found strong evidence that a small number of modules contain most of the faults discovered in prerelease testing and that a very small number of modules contain most of the faults discovered in operation. We found no evidence to support previous claims relating module size to fault density nor did we find evidence that popular complexity metrics are good predictors of either fault-prone or failure-prone modules. We confirmed that the number of faults discovered in prerelease testing is an order of magnitude greater than the number discovered in 12 months of operational use. The most important result was strong evidence of a counter-intuitive relationship between pre- and postrelease faults; those modules which are the most fault-prone prerelease are among the least fault-prone postrelease, while conversely, the modules which are most fault-prone postrelease are among the least fault-prone prerelease. This observation has serious ramifications for the commonly used fault density measure. Our results provide data-points in building up an empirical picture of the software development process",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879815,no,no,1486911996.271833
"Tail-Splitting"""" to Predict Failing Software Modules - with a Case Study on an Operating Systems Product","Tail-splitting"""" is a new technique to identify defect prone modules by enhancing the focus of the Pareto distribution by a development process factor. The simple yet powerful influence of a varying tail membership as a function of development process phases is captured by the tail-split-string which tags each module. The case studies on an operating systems product demonstrate that the tail-split-string identifies a small set of modules with a high probability of field failure. The tail-boundary in the algorithm provides for a natural tuning parameter to control the size of the identified set to suit the resources available for rework. Release managers have found that the method is particularly useful to sift modules, with low false positive, for late stage rework",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021984,no,no,1486911995.889226
Impact of rate control on the capacity of an Iub link: single service case,"Universal Mobile Telecommunications System (UMTS) networks are capable of serving packet-switched data applications at bit rates as high as 384 kbps. This paper studies the capacity and utilization of the downlink of the Iub interface, which lies between the radio network controller (RNC) and the base station (NodeB) in the UMTS network. The 3GPP standards define a Node B """"receive window"""" within which a frame should arrive for it to be processed and transmitted to the UE in time. If the frame arrives too late, it will be discarded. Such frame discard event results in some loss in voice/data quality. Via simulations, we evaluate the link capacity for web-browsing traffic at 64 kbps, 128 kbps and 384 kbps, with a frame discard probability target of 0.5%. Our results indicate that the Iub link utilization is very poor due to the highly bursty nature of data traffic. In order to alleviate this problem, we introduce a rate control (RC) scheme where the peak user data rate is temporarily lowered during times of high congestion. This lowering of data rate is done through appropriate selection of the transport block size within the transport format set. As a result of such rate control, the capacity of the Iub link improves.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1204485,no,no,1486911995.889225
Tests and tolerances for high-performance software-implemehted fault detection,"We describe and test a software approach to fault detection in common numerical algorithms. Such result checking or algorithm-based fault tolerance (ABFT) methods may be used, for example, to overcome single-event upsets in computational hardware or to detect errors in complex, high-efficiency implementations of the algorithms. Following earlier work, we use checksum methods to validate results returned by a numerical subroutine operating subject to unpredictable errors in data. We consider common matrix and Fourier algorithms which return results satisfying a necessary condition having a linear form; the checksum tests compliance with this condition. We discuss the theory and practice of setting numerical tolerances to separate errors caused by a fault from those inherent in finite-precision floating-point calculations. We concentrate on comprehensively defining and evaluating tests having various accuracy/computational burden tradeoffs, and we emphasize average-case algorithm behavior rather than using worst-case upper, bounds on error.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1197125,no,no,1486911995.889223
Improving Effort Estimation Accuracy by Weighted Grey Relational Analysis During Software Development,"Grey relational analysis (GRA), a similarity-based method, presents acceptable prediction performance in software effort estimation. However, we found that conventional GRA methods only consider non-weighted conditions while predicting effort. Essentially, each feature of a project may have a different degree of relevance in the process of comparing similarity. In this paper, we propose six weighted methods, namely, non-weight, distance-based weight, correlative weight, linear weight, nonlinear weight, and maximal weight, to be integrated into GRA. Three public datasets are used to evaluate the accuracy of the weighted GRA methods. Experimental results show that the weighted GRA performs better precision than the non-weighted GRA. Specifically, the linearly weighted GRA greatly improves accuracy compared with the other weighted methods. To sum up, the weighted GRA not only can improve the accuracy of prediction but is an alternative method to be applied to software development life cycle.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425897,no,no,1486911995.889222
A case for exploiting self-similarity of network traffic in TCP congestion control,"Analytical and empirical studies have shown that self-similar traffic can have a detrimental impact on network performance including amplified queuing delay and packet loss ratio. On the flip side, the ubiquity of scale-invariant burstiness observed across diverse networking contexts can be exploited to design better resource control algorithms. We explore the issue of exploiting the self-similar characteristics of network traffic in TCP congestion control. We show that the correlation structure present in long-range dependent traffic can be detected on-line and used to predict future traffic. We then devise an novel scheme, called TCP with traffic prediction (TCP-TP), that exploits the prediction result to infer, in the context of AIMD (additive increase, multiplicative decrease) steady-state dynamics, the optimal operational point for a TCP connection. Through analytical reasoning, we show that the impact of prediction errors on fairness is minimal. We also conduct ns-2 simulation and FreeBSD 4.1-based implementation studies to validate the design and to demonstrate the performance improvement in terms of packet loss ratio and throughput attained by connections.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181384,no,no,1486911995.889217
Prediction of Self-Similar Traffic and its Application in Network Bandwidth Allocation,"In this paper, traffic prediction models based on chaos theory are studied and compared with FARIMA (fractional autoregressive integrated moving average) predictors by means of the adopted measurements of predictability. The traffic prediction results are applied in the bandwidth allocation of a mesh network, and the OPNET simulation platform is developed in order to compare their effects. The adopted predictability measurements are inadequate because although the chaotic predictor based on the Lyapunov exponent with worse values of the measurements can timely predict the burstiness of self- similar traffic, the FARIMA predictor forecasts the burstiness with a time-delay. The DAMA (dynamic assignment multiaccess) bandwidth allocation strategy combined with the chaotic predictor can provide better QoS performance.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4340270,no,no,1486911995.889216
Neural network based BER prediction for 802.16e channel,"The prediction of bit error rate (BER) in IEEE 802.16e mobile wireless MAN network is investigated here. The state of the channel is estimated on symbol by symbol basis on a realistic fading environment. The state of a channel is modeled as nonlinear and temporal system. Neural network method is the best system to predict and analyze the behaviors of such nonlinear and temporal system. In this context, BER prediction by k symbol ahead is investigated by two different recurrent neural network architectures such as recurrent radial basis function (RRBF) network and echo state network (ESN). The predicted BER will match very well with the simulation results.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4446119,no,no,1486911995.889212
Hot-spot prediction and alleviation in distributed stream processing applications,"Many emerging distributed applications require the real-time processing of large amounts of data that are being updated continuously. Distributed stream processing systems offer a scalable and efficient means of in-network processing of such data streams. However, the large scale and the distributed nature of such systems, as well as the fluctuation of their load render it difficult to ensure that distributed stream processing applications meet their Quality of Service demands. We describe a decentralized framework for proactively predicting and alleviating hot-spots in distributed stream processing applications in real-time. We base our hot-spot prediction techniques on statistical forecasting methods, while for hot-spot alleviation we employ a non-disruptive component migration protocol. The experimental evaluation of our techniques, implemented in our Synergy distributed stream processing middleware over PlanetLab, using a real stream processing application operating on real streaming data, demonstrates high prediction accuracy and substantial performance benefits.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630103,no,no,1486911995.542644
Module-order modeling using an evolutionary multi-objective optimization approach,"The problem of quality assurance is important for software systems. The extent to which software reliability improvements can be achieved is often dictated by the amount of resources available for the same. A prediction for risk-based rankings of software modules can assist in the cost-effective delegation of the limited resources. A module-order model (MOM) is used to gauge the performance of the predicted rankings. Depending on the software system under consideration, multiple software quality objectives may be desired for a MOM; e.g., the desired rankings may be such that if 20% of modules were targeted for reliability enhancements then 80% of the faults would be detected. In addition, it may also be desired that if 50% of modules were targeted then 100% of the faults would be detected. Existing works related to MOM(s) have used an underlying prediction model to obtain the rankings, implying that only the average, relative, or mean square errors are minimized. Such an approach does not provide an insight into the behavior of a MOM, the performance of which focusses on how many faults are accounted for by the given percentage of modules enhanced. We propose a methodology for building MOM (s) by implementing a multiobjective optimization with genetic programming. It facilitates the simultaneous optimization of multiple performance objectives for a MOM. Other prediction techniques, e.g., multiple linear regression and neural networks, cannot achieve multiobjective optimization for MOM(s). A case study of a high-assurance telecommunications software system is presented. The observed results show a new promise in the modeling of goal-oriented software quality estimation models.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357900,no,no,1486911995.542643
Data Analysis and Confidence based on SVM Density Estimation,"Data-driven models are frequently used in industry to predict various characteristics of processes. In order to build robust model, the quality of the data needs to be analysed. These models are also required to associate a level of confidence with their predictions. In a high-dimensional setting it is important to incorporate data density information when analyzing the quality of the data and the determining the confidence in a prediction. The SVM density estimation together with results from the typicalness framework form a powerful tool that is effective for industrial applications.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1716330,no,no,1486911995.542641
Automated Information Aggregation for Scaling Scale-Resistant Services,"Machine learning provides techniques to monitor system behavior and predict failures from sensor data. However, such algorithms are """"scale resistant"""" $high computational complexity and not parallelizable. The problem then becomes identifying and delivering the relevant subset of the vast amount of sensor data to each monitoring node, despite the lack of explicit """"relevance"""" labels. The simplest solution is to deliver only the """"closest"""" data items under some distance metric. We demonstrate a better approach using a more sophisticated architecture: a scalable data aggregation and dissemination overlay network uses an influence metric reflecting the relative influence of one node's data on another, to efficiently deliver a mix of raw and aggregated data to the monitoring components, enabling the application of machine learning tools on real-world problems. We term our architecture level of detail after an analogous computer graphics technique",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019558,no,no,1486911995.54264
A Bayesian approach for software quality prediction,"Many statistical algorithms have been proposed for software quality prediction of fault-prone and non fault-prone program modules. The main goal of these algorithms is the improvement of software development processes. In this paper, we introduce a new software prediction algorithm. Our approach is purely Bayesian and is based on finite Dirichlet mixture models. The implementation of the Bayesian approach is done through the use of the Gibbs sampler. Experimental results are presented using simulated data, and a real application for software modules classification is also included.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670508,no,no,1486911995.542638
Computationally efficient algorithms for predicting the file size of JPEG images subject to changes of quality factor and scaling,"To enable the delivery of multimedia content to mobile devices with limited capabilities, high volume transcoding servers must rely on efficient adaptation algorithms. Our objective in addressing the case of JPEG image adaptation was to find computationally efficient algorithms to accurately predict the compressed file size of images subject to simultaneous changes in quality factor (QF) and resolution. In this paper, we present two new prediction algorithms which use only information readily available from the file header. The first algorithm, QF Scaling-Aware Prediction, predicts file size based on the QF of the original picture, as well as a target QF and scaling. The second algorithm, Clustered QF Scaling-Aware Prediction, also takes into account the resolution of the original picture for improved prediction accuracy. As both algorithms rely on machine-learning strategies, a large corpus of representative JPEG images was assembled. We show that both prediction algorithms lead to acceptably small relative prediction errors in adaptation scenarios of interest.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4563280,no,no,1486911995.542637
Quality Assessment Based on Attribute Series of Software Evolution,"Defect density and defect prediction are essential for efficient resource allocation in software evolution. In an empirical study we applied data mining techniques for value series based on evolution attributes such as number of authors, commit messages, lines of code, bug fix count, etc. Daily data points of these evolution attributes were captured over a period of two months to predict the defects in the subsequent two months in a project. For that, we developed models utilizing genetic programming and linear regression to accurately predict software defects. In our study, we investigated the data of three independent projects, two open source and one commercial software system. The results show that by utilizing series of these attributes we obtain models with high correlation coefficients (between 0.716 and 0.946). Further, we argue that prediction models based on series of a single variable are sometimes superior to the model including all attributes: in contrast to other studies that resulted in size or complexity measures as predictors, we have identified the number of authors and the number of commit messages to versioning systems as excellent predictors of defect densities.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4400154,no,no,1486911995.542635
A WCET-oriented static branch prediction scheme for real time systems,"Branch prediction mechanisms are becoming commonplace within current generation processors. Dynamic branch predictors, albeit able to predict branches quite accurately in average, are becoming increasingly complex. Thus, determining their worst-case behavior, which is highly recommended for real-time applications, is getting increasingly difficult and error-prone, and may even be soon impossible for the most complex branch predictors. In contrast, static branch predictors are inherently predictable, to the detriment of a lower prediction accuracy. In this paper, we propose a WCET-oriented static branch prediction scheme. Unlike related work on compiler-directed static branch prediction, our scheme does not address program average-case performance (i.e. average-case branch misprediction rate) but addresses worst-case program performance instead (i.e. branch mispredictions which impact programs WCET estimates). Experimental results on a PowerPC 7451 architecture show that the estimated WCET can be decreased by up to 21 % (with an average improvement of 15%) as compared with the method where all branches are conservatively considered mispredicted. Our scheme, although applicable to any processor with support for static branch prediction, is specially suited to processors with complex dynamic predictors, for which safe and tight WCET estimate methods do not exist.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508444,no,no,1486911995.542632
Using machine learning for estimating the defect content after an inspection,"We view the problem of estimating the defect content of a document after an inspection as a machine learning problem: The goal is to learn from empirical data the relationship between certain observable features of an inspection (such as the total number of different defects detected) and the number of defects actually contained in the document. We show that some features can carry significant nonlinear information about the defect content. Therefore, we use a nonlinear regression technique, neural networks, to solve the learning problem. To select the best among all neural networks trained on a given data set, one usually reserves part of the data set for later cross-validation; in contrast, we use a technique which leaves the full data set for training. This is an advantage when the data set is small. We validate our approach on a known empirical inspection data set. For that benchmark, our novel approach clearly outperforms both linear regression and the current standard methods in software engineering for estimating the defect content, such as capture-recapture. The validation also shows that our machine learning approach can be successful even when the empirical inspection data set is small.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265733,no,no,1486911995.542629
Main effects screening: a distributed continuous quality assurance process for monitoring performance degradation in evolving software systems,"Developers of highly configurable performance-intensive software systems often use a type of in-house performance-oriented """"regression testing"""" to ensure that their modifications have not adversely affected their software's performance across its large configuration space. Unfortunately, time and resource constraints often limit developers to in-house testing of a small number of configurations and unreliable extrapolation from these results to the entire configuration space, which allows many performance bottlenecks and sources of QoS degradation to escape detection until systems are fielded. To improve performance assessment of evolving systems across large configuration spaces, we have developed a distributed continuous quality assurance (DCQA) process called main effects screening that uses in-the-field resources to execute formally designed experiments to help reduce the configuration space, thereby allowing developers to perform more targeted in-house QA. We have evaluated this process via several feasibility studies on several large, widely-used performance-intensive software systems. Our results indicate that main effects screening can detect key sources of performance degradation in large-scale systems with significantly less effort than conventional techniques.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553572,no,no,1486911995.231244
Software Fault Prediction using Language Processing,"Accurate prediction of faulty modules reduces the cost of software development and evolution. Two case studies with a language-processing based fault prediction measure are presented. The measure, refereed to as a QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgements of software quality. The two case studies consider the measure's application to fault prediction using two programs (one open source, one proprietary). Linear mixed-effects regression models are used to identify relationships between defects and QALP score. Results, while complex, show that little correlation exists in the first case study, while statistically significant correlations exists in the second. In this second study the QALP score is helpful in predicting faults in modules (files) with its usefulness growing as module size increases.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344105,no,no,1486911995.231243
A comprehensive evaluation of capture-recapture models for estimating software defect content,"An important requirement to control the inspection of software artifacts is to be able to decide, based on more objective information, whether the inspection can stop or whether it should continue to achieve a suitable level of artifact quality. A prediction of the number of remaining defects in an inspected artifact can be used for decision making. Several studies in software engineering have considered capture-recapture models to make a prediction. However, few studies compare the actual number of remaining defects to the one predicted by a capture-recapture model on real software engineering artifacts. The authors focus on traditional inspections and estimate, based on actual inspections data, the degree of accuracy of relevant state-of-the-art capture-recapture models for which statistical estimators exist. In order to assess their robustness, we look at the impact of the number of inspectors and the number of actual defects on the estimators' accuracy based on actual inspection data. Our results show that models are strongly affected by the number of inspectors, and therefore one must consider this factor before using capture-recapture models. When the number of inspectors is too small, no model is sufficiently accurate and underestimation may be substantial. In addition, some models perform better than others in a large number of conditions and plausible reasons are discussed. Based on our analyses, we recommend using a model taking into account that defects have different probabilities of being detected and the corresponding Jackknife Estimator. Furthermore, we calibrate the prediction models based on their relative error, as previously computed on other inspections. We identified theoretical limitations to this approach which were then confirmed by the data",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852741,no,no,1486911995.231241
Cost-sensitive boosting in software quality modeling,"Early prediction of the quality of software modules prior to software testing and operations can yield great benefits to the software development teams, especially those of high-assurance and mission-critical systems. Such an estimation allows effective use of the testing resources to improve the modules of the software system that need it most and achieve high reliability. To achieve high reliability, by the means of predictive methods, several tools are available. Software classification models provide a prediction of the class of a module, i.e., fault-prone or not fault-prone. Recent advances in the data mining field allow to improve individual classifiers (models) by using the combined decision from multiple classifiers. This paper presents a couple of algorithms using the concept of combined classification. The algorithms provided useful models for software quality modeling. A comprehensive comparative evaluation of the boosting and cost-boosting algorithms is presented. We demonstrate how the use of boosting algorithms (original and cost-sensitive) meets many of the specific requirements for software quality modeling. C4.5 decision trees and decision stumps were used to evaluate these algorithms with two large-scale case studies of industrial software systems.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173102,no,no,1486911995.23124
Testing and defect tolerance: a Rent's rule based analysis and implications on nanoelectronics,"Defect tolerant architectures will be essential for building economical gigascale nanoelectronic computing systems to permit functionality in the presence of a significant number of defects. The central idea underlying a defect tolerant configurable system is to build the system out of partially perfect components, detect the defects and configure the available good resources using software. In this paper we discuss implications of defect tolerance on power area, delay and other relevant parameters for computing architectures. We present a Rent's rule based abstraction of testing for VLSI systems and evaluate the redundancy requirements for observability. It is shown that for a very high interconnect defect density, a prohibitively large number of redundant components are necessary for observability and this has adverse affect on the system performance. Through a unified framework based on a priori wire length estimation and Rent's rule we illustrate the hidden cost of supporting such an architecture.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347850,no,no,1486911995.231239
Software quality classification modeling using the SPRINT decision tree algorithm,"Predicting the quality of system modules prior to software testing and operations can benefit the software development team. Such a timely reliability estimation can be used to direct cost-effective quality improvement efforts to the high-risk modules. Tree-based software quality classification models based on software metrics are used to predict whether a software module is fault-prone or not fault-prone. They are white box quality estimation models with good accuracy, and are simple and easy to interpret. This paper presents an in-depth study of calibrating classification trees for software quality estimation using the SPRINT decision tree algorithm. Many classification algorithms have memory limitations including the requirement that data sets be memory resident. SPRINT removes all of these limitations and provides a fast and scalable analysis. It is an extension of a commonly used decision tree algorithm, CART, and provides a unique tree-pruning technique based on the minimum description length (MDL) principle. Combining the MDL pruning technique and the modified classification algorithm, SPRINT yields classification trees with useful prediction accuracy. The case study used comprises of software metrics and fault data collected over four releases from a very large telecommunications system. It is observed that classification trees built by SPRINT are more balanced and demonstrate better stability in comparison to those built by CART.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180826,no,no,1486911995.231237
Controlling overfitting in software quality models: experiments with regression trees and classification,"In these days of faster, cheaper, better release cycles, software developers must focus enhancement efforts on those modules that need improvement the most. Predictions of which modules are likely to have faults during operations is an important tool to guide such improvement efforts during maintenance. Tree-based models are attractive because they readily model nonmonotonic relationships between a response variable and its predictors. However, tree-based models are vulnerable to overfitting, where the model reflects the structure of the training data set too closely. Even though a model appears to be accurate on training data, if overfitted it may be much less accurate when applied to a current data set. To account for the severe consequences of misclassifying fault-prone modules, our measure of overfitting is based on the expected costs of misclassification, rather than the total number of misclassifications. In this paper, we apply a regression-tree algorithm in the S-Plus system to the classification of software modules by the application of our classification rule that accounts for the preferred balance between misclassification rates. We conducted a case study of a very large legacy telecommunications system, and investigated two parameters of the regression-tree algorithm. We found that minimum deviance was strongly related to overfitting and can be used to control it, but the effect of minimum node size on overfitting is ambiguous",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915528,no,no,1486911995.231236
On reliability modeling and analysis of highly-reliable large systems,"Modem systems are getting more and more complex, incorporating new technology to meet customer's high expectations. Hardware, software and data communications are integrated to make systems function properly. One example is a critical power system, which provides electrical power to a data center or to semiconductor manufacturing equipment. Reliability techniques of fault-tolerance, true redundancy, multiple grid connections, concurrent maintenance and so forth are applied in design to provide high system reliability. The techniques make the system larger and more complicated in configuration and behavior. Application of modeling tools and analysis methods to such highly reliable, large, complex and repairable systems is discussed in this paper, based on the experience of assessing critical power systems. The use of reliability block diagram plus simulation is recommended as one of the best engineering practices in planning for such large complex repairable systems",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981685,no,no,1486911995.231235
Exploring visualization of complex telecommunications systems network data,"High-speed broadband telecommunication systems are built with extensive redundancy and complex management systems to ensure robustness. The presence of a fault may be detected by the offending component, its parent component or by other components. This can potentially result in a net effect of a large number of alarm events being raised. There can be a considerable amount of alarm depending on the size and configuration of the network. Data visualization can reduce mountains of data to visually insightful representations, which can aid decision-making and identification of faults. The paper explores data visualizations to provide a context to assist in the identification of such faults.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1175708,no,no,1486911995.231233
Empirical case studies of combining software quality classification models,"The increased reliance on computer systems in the modern world has created a need for engineering reliability control of computer systems to the highest possible standards. This is especially crucial in high-assurance and mission critical systems. Software quality classification models are one of the important tools in achieving high reliability. They can be used to calibrate software metrics-based models to detect fault-prone software modules. Timely use of such models can greatly aid in detecting faults early in the life cycle of the software product. Individual classifiers (models) may be improved by using the combined decision from multiple classifiers. Several algorithms implement this concept and have been investigated. These combined learners provide the software quality modeling community with accurate, robust, and goal oriented models. This paper presents a comprehensive comparative evaluation of three combined learners, Bagging, Boosting, and Logit-Boost. We evaluated these methods with a strong and a weak learner, i.e., C4.5 and Decision Stumps, respectively. Two large-scale case studies of industrial software systems are used in our empirical investigations.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319084,no,no,1486911995.23123
Using Developer Information as a Factor for Fault Prediction,"We have been investigating different prediction models to identify which files of a large multi-release industrial software system are most likely to contain the largest numbers of faults in the next release. To make predictions we considered a number of different file characteristics and change information about the files, and have built fully- automatable models that do not require that the user have any statistical expertise. We now consider the effect of adding developer information as a prediction factor and assess the extent to which this affects the quality of the predictions.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273264,no,no,1486911994.937313
QoS Management of Real-Time Data Stream Queries in Distributed Environments,"Many emerging applications operate on continuous unbounded data streams and need real-time data services. Providing deadline guarantees for queries over dynamic data streams is a challenging problem due to bursty stream rates and time-varying contents. This paper presents a prediction-based QoS management scheme for real-time data stream query processing in distributed environments. The prediction-based QoS management scheme features query workload estimators, which predict the query workload using execution time profiling and input data sampling. In this paper, we apply the prediction-based technique to select the proper propagation schemes for data streams and intermediate query results in distributed environments. The performance study demonstrates that the proposed solution tolerates dramatic workload fluctuations and saves significant amounts of CPU time and network bandwidth with little overhead",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4208850,no,no,1486911994.937312
Influence of adaptive data layouts on performance in dynamically changing storage environments,"For most of today's IT environments, the tremendous need for storage capacity in combination with a required minimum I/O performance has become highly critical. In dynamically growing environments, a storage management solution's underlying data distribution scheme has great impact to the overall system I/O performance. The evaluation of a number of open system storage visualization solutions and volume managers has shown that all of them lack the ability to automatically adapt to changing access patterns and storage infrastructures; many of them require an error prone manual re-layout of the data blocks, or rely on a very time consuming re-striping of all available data. This paper evaluates the performance of conventional data distribution approaches compared to the adaptive virtualization solution V:DRIVE in dynamically changing storage environments. Changes of the storage infrastructure are normally not considered in benchmark results, but can have a significant impact on storage performance. Using synthetic benchmarks, V:DRIVE is compared in such changing environments with the non-adaptive Linux logical volume manager (LVM). The performance results of our tests clearly outline the necessity of adaptive data distribution schemes.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1613268,no,no,1486911994.93731
A Multivariate Analysis of Static Code Attributes for Defect Prediction,"Defect prediction is important in order to reduce test times by allocating valuable test resources effectively. In this work, we propose a model using multivariate approaches in conjunction with Bayesian methods for defect predictions. The motivation behind using a multivariate approach is to overcome the independence assumption of univariate approaches about software attributes. Using Bayesian methods gives practitioners an idea about the defectiveness of software modules in a probabilistic framework rather than the hard classification methods such as decision trees. Furthermore the software attributes used in this work are chosen among the static code attributes that can easily be extracted from source code, which prevents human errors or subjectivity. These attributes are preprocessed with feature selection techniques to select the most relevant attributes for prediction. Finally we compared our proposed model with the best results reported so far on public datasets and we conclude that using multivariate approaches can perform better.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385500,no,no,1486911994.937309
An empirical study of the impact of count models predictions on module-order models,"Software quality prediction models are used to achieve high software reliability. A module-order model (MOM) uses an underlying quantitative prediction model to predict this rank-order. This paper compares performances of module-order models of two different count models which are used as the underlying prediction models. They are the Poisson regression model and the zero-inflated Poisson regression model. It is demonstrated that improving a count model for prediction does not ensure a better MOM performance. A case study of a full-scale industrial software system is used to compare performances of module-order models of the two count models. It was observed that improving prediction of the Poisson count model by using zero-inflated Poisson regression did not yield module-order models with better performance. Thus, it was concluded that the degree of prediction accuracy of the underlying model did not influence the results of the subsequent module-order model. Module-order modeling is proven to be a robust and effective method even though both underlying prediction may sometimes lack acceptable prediction accuracy.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011335,no,no,1486911994.937308
Fault Prediction using Early Lifecycle Data,"The prediction of fault-prone modules in a software project has been the topic of many studies. In this paper, we investigate whether metrics available early in the development lifecycle can be used to identify fault-prone software modules. More precisely, we build predictive models using the metrics that characterize textual requirements. We compare the performance of requirements-based models against the performance of code-based models and models that combine requirement and code metrics. Using a range of modeling techniques and the data from three NASA projects, our study indicates that the early lifecycle metrics can play an important role in project management, either by pointing to the need for increased quality monitoring during the development or by using the models to assign verification and validation activities.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402215,no,no,1486911994.937305
Prediction-based Haptic Data Reduction and Compression in Tele-Mentoring Systems,"In this paper, a novel haptic data reduction and compression technique to reduce haptic data traffic in networked haptic tele-mentoring systems is presented. The suggested method follows a two-step procedure: (1) haptic data packets are not transmitted when they can be predicted within a predefined tolerable error; otherwise, (2) data packets are compressed prior to transmission. The prediction technique relies on the least-squares method. Knowledge from human haptic perception is incorporated into the architecture to assess the perceptual quality of the prediction results. Packet-payload compression is performed using uniform quantization and adaptive Golomb-Rice codes. The preliminary experimental results demonstrate the algorithm's effectiveness as great haptic data reduction and compression is achieved, while preserving the overall quality of the tele-mentoring environment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4547342,no,no,1486911994.937303
"Comments on """"Data Mining Static Code Attributes to Learn Defect Predictors""""","In this correspondence, we point out a discrepancy in a recent paper, """"data mining static code attributes to learn defect predictors,"""" that was published in this journal. Because of the small percentage of defective modules, using probability of detection (pd) and probability of false alarm (pf) as accuracy measures may lead to impractical prediction models.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288196,no,no,1486911994.937299
Predicting failures of computer systems: a case study for a telecommunication system,"The goal of online failure prediction is to forecast imminent failures while the system is running. This paper compares similar events prediction (SEP) with two other well-known techniques for online failure prediction: a straightforward method that is based on a reliability model and dispersion frame technique (DFT). SEP is based on recognition of failure-prone patterns utilizing a semi-Markov chain in combination with clustering. We applied the approaches to real data of a commercial telecommunication system. Results are presented in terms of precision, recall, F-measure and accumulated runtime-cost. The results suggest a significantly improved forecasting performance.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639672,no,no,1486911994.703117
An empirical investigation of an object-oriented software system,"The paper describes an empirical investigation into an industrial object oriented (OO) system comprised of 133000 lines of C++. The system was a subsystem of a telecommunications product and was developed using the Shlaer-Mellor method (S. Shlaer and S.J. Mellor, 1988; 1992). From this study, we found that there was little use of OO constructs such as inheritance, and therefore polymorphism. It was also found that there was a significant difference in the defect densities between those classes that participated in inheritance structures and those that did not, with the former being approximately three times more defect-prone. We were able to construct useful prediction systems for size and number of defects based upon simple counts such as the number of states and events per class. Although these prediction systems are only likely to have local significance, there is a more general principle that software developers can consider building their own local prediction systems. Moreover, we believe this is possible, even in the absence of the suites of metrics that have been advocated by researchers into OO technology. As a consequence, measurement technology may be accessible to a wider group of potential users",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879814,no,no,1486911994.703116
Software Defects Prediction using Operating Characteristic Curves,We present a software defect prediction model using operating characteristic curves. The main idea behind our proposed technique is to use geometric insight in helping construct an efficient and fast prediction method to accurately predict the. cumulative number of failures at any given stage during the software development process. Our predictive approach uses the number of detected faults instead of the software failure-occurrence time in the testing phase. Experimental results illustrate the effectiveness and the much improved performance of the proposed method in comparison with the Bayesian prediction approaches.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296704,no,no,1486911994.703114
Execution-time Prediction for Dynamic Streaming Applications with Task-level Parallelism,"Programmable multiprocessor systems-on-chip are becoming the preferred implementation platform for embedded streaming applications. This enables using more software components, which leads to large and frequent dynamic variations of data-dependent execution times. In this context, accurate and conservative prediction of execution times helps in maintaining good audio/video quality and reducing energy consumption by dynamic evaluation of the amount of on-chip resources needed by applications. To be effective, multiprocessor systems have to employ the available parallelism. The combination of task-level parallelism and task delay variations makes predicting execution times a very hard problem. So far, under these conditions, no appropriate techniques exist for the conservative prediction of execution times with the required accuracy. In this paper, we present a novel technique for this problem, exploiting the concept of scenario-based prediction, and taking into account the transient and periodic behavior of scenarios and the effect of scenario transitions. In our MPEG-4 shape-decoder case study, we observe no more than 11% average overestimation.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341473,no,no,1486911994.703113
Software measurement data analysis using memory-based reasoning,"The goal of accurate software measurement data analysis is to increase the understanding and improvement of software development process together with increased product quality and reliability. Several techniques have been proposed to enhance the reliability prediction of software systems using the stored measurement data, but no single method has proved to be completely effective. One of the critical parameters for software prediction systems is the size of the measurement data set, with large data sets providing better reliability estimates. In this paper, we propose a software defect classification method that allows defect data from multiple projects and multiple independent vendors to be combined together to obtain large data sets. We also show that once a sufficient amount of information has been collected, the memory-based reasoning technique can be applied to projects that are not in the analysis set to predict their reliabilities and guide their testing process. Finally, the result of applying this approach to the analysis of defect data generated from fault-injection simulation is presented.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180813,no,no,1486911994.70311
Prediction-Table Based Fault-Tolerant Real-Time Scheduling Algorithm,"In order to predict accurately whether primary versions of real-time tasks is executable in software fault-tolerant module, a new algorithm, PTBA, prediction-table based algorithm, is presented. PTBA uses prediction-table to predict whether a host primary can meet its pre-deadline. Prediction-table contains the pre-assignment information of tasks between the current time and the alternates' notification time. If the prediction result shows that host primary has not enough time to execute, it will be aborted. Otherwise, prediction-table is referenced to schedule tasks with low overhead. The novelty of PTBA is that it schedules primaries according to their corresponding alternates' notification time and has no extra scheduling overhead in prediction-table mode. Simulation results show that PTBA allows more execution time for primaries and wastes less processor time than the well-known similar algorithms. PTBA is appropriate to the situation where the periods of tasks are short and software fault probability is low",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032167,no,no,1486911994.703108
The accuracy of fault prediction in modified code - statistical model vs. expert estimation,"Fault prediction models still seem to be more popular in academia than in industry. In industry, expert estimations of fault proneness are the most popular methods of deciding where to focus the fault detection efforts. In this paper, we present a study in which we empirically evaluate the accuracy of fault prediction offered by statistical models as compared to expert estimations. The study is industry based. It involves a large telecommunication system and experts that were involved in the development of this system. Expert estimations are compared to simple prediction models built on another large system, also from the telecommunication domain. We show that the statistical methods clearly outperform the expert estimations. As the main reason for the superiority of the statistical models we see their ability to cope with large datasets, which results in their ability to perform reliable predictions for larger number of components in the system, as well as the ability to perform prediction at a more fine-grain level, e.g., at the class instead of at the component level",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607383,no,no,1486911994.703107
Experience from replicating empirical studies on prediction models,"When conducting empirical studies, replications are important contributors to investigating the generality of the studies. By replicating a study in another context, we investigate what impact the specific environment has, related to the effect of the studied object. In this paper, we define different levels of replication to characterise the similarities and differences between an original study and a replication, with particular focus on prediction models for the identification of fault-prone software components. Further, we derive a set of issues and concerns which are important in order to enable replication of an empirical study and to enable practitioners to use the results. To illustrate the importance of the issues raised, a replication case study is presented in the domain of prediction models for fault-prone software components. It is concluded that the results are very divergent, depending on how different parameters are chosen, which demonstrates the need for well-documented empirical studies to enable replication and use",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011340,no,no,1486911994.703102
Forecasting field defect rates using a combined time-based and metrics-based approach: a case study of OpenBSD,"Open source software systems are critical infrastructure for many applications; however, little has been precisely measured about their quality. Forecasting the field defect-occurrence rate over the entire lifespan of a release before deployment for open source software systems may enable informed decision-making. In this paper, we present an empirical case study often releases of OpenBSD. We use the novel approach of predicting model parameters of software reliability growth models (SRGMs) using metrics-based modeling methods. We consider three SRGMs, seven metrics-based prediction methods, and two different sets of predictors. Our results show that accurate field defect-occurrence rate forecasts are possible for OpenBSD, as measured by the Theil forecasting statistic. We identify the SRGM that produces the most accurate forecasts and subjectively determine the preferred metrics-based prediction method and set of predictors. Our findings are steps towards managing the risks associated with field defects",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544734,no,no,1486911994.486944
An Agglomerative Clustering Methodology For Data Imputation,"The prediction of accurate effort estimates from software project data sets still remains to be a challenging problem. Major amounts of data are frequently found missing in these data sets that are utilized to build effort/cost/time prediction models. Current techniques used in the industry ignore all the missing data and provide estimates based on the remaining complete information. Thus, the very estimates are error prone. In this paper, we investigate the design and application of a hybrid methodology on six real-time software project data sets in order to better the prediction accuracies of the estimates. We perform useful experimental analyses and evaluate the impact of the methodology. Finally, we discuss the findings and elaborate the appropriateness of the methodology",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611567,no,no,1486911994.486942
An integrated methodology to improve classification accuracy of remote sensing data,"We investigated and improved the accuracy of supervised classification by eliminating electromagnetic radiation scattering effect of aerosol particles from cloud-free Landsat TM data. The scattering effect was eliminated by deriving a mathematical model including the amount of the scattered radiation per pixel area and aerosol size distribution, which was derived using randomly collected training sets. An algorithm in C<sup>++</sup> has been developed with iterations to derive the aerosol size distribution and to remove the effect of aerosols scattering in addition to the use of IRDAS software (commercial software). To assess the accuracy of the supervised classification, results of remote sensing data were compared with Global Positioning System (GPS) ground truth reference data in error matrices (output results of classification). The results of the corrected images show great improvement of image quality and classification accuracy. The misclassified off-diagonal pixels were minimized in the accuracy assessment error matrices. Therefore it fulfills the criteria of accuracy improvement. The overall accuracy of the supervised classification is improved (between 18% and 27%). The Z-score shows significant difference between the corrected data and the raw data (between 4.0 and 11.91) by employing KHAT statistics evaluation.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1294374,no,no,1486911994.486941
A New Method to Predict Software Defect Based on Rough Sets,"High quality software should have as few defects as possible. Many modeling techniques have been proposed and applied for software quality prediction. Software projects vary in size and complexity, programming languages, development processes, etc. We research the correlation of software metrics focusing on the data sets of software defect prediction. A rough set model is presented to reduce the attributes of data sets of software defect prediction in this paper. Experiment shows its splendid performance.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4683186,no,no,1486911994.486939
Robust prediction of fault-proneness by random forests,"Accurate prediction of fault prone modules (a module is equivalent to a C function or a C+ + method) in software development process enables effective detection and identification of defects. Such prediction models are especially beneficial for large-scale systems, where verification experts need to focus their attention and resources to problem areas in the system under development. This paper presents a novel methodology for predicting fault prone modules, based on random forests. Random forests are an extension of decision tree learning. Instead of generating one decision tree, this methodology generates hundreds or even thousands of trees using subsets of the training data. Classification decision is obtained by voting. We applied random forests in five case studies based on NASA data sets. The prediction accuracy of the proposed methodology is generally higher than that achieved by logistic regression, discriminant analysis and the algorithms in two machine learning software packages, WEKA [I. H. Witten et al. (1999)] and See5. The difference in the performance of the proposed methodology over other methods is statistically significant. Further, the classification accuracy of random forests is more significant over other methods in larger data sets.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383136,no,no,1486911994.486938
Theoretical Maximum Prediction Accuracy for Analogy-Based Software Cost Estimation,"Software cost estimation is an important area of research in software engineering. Various cost estimation model evaluation criteria (such as MMRE, MdMRE etc.) have been developed for comparing prediction accuracy among cost estimation models. All of these metrics capture the residual difference between the predicted value and the actual value in the dataset, but ignore the importance of the dataset quality. What is more, they implicitly assume the prediction model to be able to predict with up to 100% accuracy at its maximum for a given dataset. Given that these prediction models only provide an estimate based on observed historical data, absolute accuracy cannot be possibly achieved. It is therefore important to realize the theoretical maximum prediction accuracy (TMPA) for the given model with a given dataset. In this paper, we first discuss the practical importance of this notion, and propose a novel method for the determination of TMPA in the application of analogy-based software cost estimation. Specifically, we determine the TMPA of analogy using a unique dynamic K-NN approach to simulate and optimize the prediction system. The results of an empirical experiment show that our method is practical and important for researchers seeking to develop improved prediction models, because it offers an alternative for practical comparison between different prediction models.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724583,no,no,1486911994.486935
Defect Data Analysis Based on Extended Association Rule Mining,"This paper describes an empirical study to reveal rules associated with defect correction effort. We defined defect correction effort as a quantitative (ratio scale) variable, and extended conventional (nominal scale based) association rule mining to directly handle such quantitative variables. An extended rule describes the statistical characteristic of a ratio or interval scale variable in the consequent part of the rule by its mean value and standard deviation so that conditions producing distinctive statistics can be discovered As an analysis target, we collected various attributes of about 1,200 defects found in a typical medium-scale, multi-vendor (distance development) information system development project in Japan. Our findings based on extracted rules include: (l)Defects detected in coding/unit testing were easily corrected (less than 7% of mean effort) when they are related to data output or validation of input data. (2)Nevertheless, they sometimes required much more effort (lift of standard deviation was 5.845) in case of low reproducibility, (i)Defects introduced in coding/unit testing often required large correction effort (mean was 12.596 staff-hours and standard deviation was 25.716) when they were related to data handing. From these findings, we confirmed that we need to pay attention to types of defects having large mean effort as well as those having large standard deviation of effort since such defects sometimes cause excess effort.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228640,no,no,1486911994.486934
Software Defect Prediction Using Call Graph Based Ranking (CGBR) Framework,"Recent research on static code attribute (SCA) based defect prediction suggests that a performance ceiling has been achieved and this barrier can be exceeded by increasing the information content in data. In this research we propose static call graph based ranking (CGBR) framework, which can be applied to any defect prediction model based on SCA. In this framework, we model both intra module properties and inter module relations. Our results show that defect predictors using CGBR framework can detect the same number of defective modules, while yielding significantly lower false alarm rates. On industrial public data, we also show that using CGBR framework can improve testing efforts by 23%.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725722,no,no,1486911994.486932
A Constructive RBF Neural Network for Estimating the Probability of Defects in Software Modules,"Much of the current research in software defect prediction focuses on building classifiers to predict only whether a software module is fault-prone or not. Using these techniques, the effort to test the software is directed at modules that are labelled as fault-prone by the classifier. This paper introduces a novel algorithm based on constructive RBF neural networks aimed at predicting the probability of errors in fault-prone modules; it is called RBF-DDA with Probabilistic Outputs and is an extension of RBF-DDA neural networks. The advantage of our method is that we can inform the test team of the probability of defect in a module, instead of indicating only if the module is fault-prone or not. Experiments carried out with static code measures from well-known software defect datasets from NASA show the effectiveness of the proposed method. We also compared the performance of the proposed method in software defect prediction with kNN and two of its variants, the S-POC-NN and R-POC-NN. The experimental results showed that the proposed method outperforms both S-POC-NN and R-POC-NN and that it is equivalent to kNN in terms of performance with the advantage of producing less complex classifiers.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4371415,no,no,1486911994.301971
Tool-based configuration of real-time CORBA middleware for embedded systems,"Real-time CORBA is a middleware standard that has demonstrated successes in developing distributed, realtime, and embedded (DRE) systems. Customizing real-time CORBA for an application can considerably reduce the size of the middleware and improve its performance. However, customizing middleware is an error-prone task and requires deep knowledge of the CORBA standard as well as the middleware design. This paper presents ZEN-kit, a graphical tool for customizing RTZen (an RTSJ-based implementation of real-time CORBA). This customization is achieved through modularizing the middleware so that features may be inserted or removed based on the DRE application requirements. This paper presents three main contributions: 1) it describes how real-time CORBA features can be modularized and configured in RTZen using components and aspects, 2) it provides a configuration strategy to customize real-time middleware to achieve low-footprint ORBs, and 3) it presents ZEN-kit, a graphical tool for composing customized real-time middleware.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420990,no,no,1486911994.301968
Multiparadigm scheduling for distributed real-time embedded computing,"Increasingly complex requirements, coupled with tighter economic and organizational constraints, are making it hard to build complex distributed real-time embedded (DRE) systems entirely from scratch. Therefore, the proportion of DRE systems made up of commercial-off-the-shelf (COTS) hardware and software is increasing significantly. There are relatively few systematic empirical studies, however, that illustrate how suitable COTS-based hardware and software have become for mission-critical DRE systems. This paper provides the following contributions to the study of real-time quality-of-service (QoS) assurance and performance in COTS-based DRE systems: it presents evidence that flexible configuration of COTS middleware mechanisms, and the operating system (OS) settings they use, allows DRE systems to meet critical QoS requirements over a wider range of load and jitter conditions than statically configured systems; it shows that in addition to making critical QoS assurances, noncritical QoS performance can be improved through flexible support for alternative scheduling strategies; and it presents an empirical study of three canonical scheduling strategies; specifically the conditions that predict success of a strategy for a production-quality DRE avionics mission computing system. Our results show that applying a flexible scheduling framework to COTS hardware, OSs, and middleware improves real-time QoS assurance and performance for mission-critical DRE systems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173210,no,no,1486911994.301967
Using regression trees to classify fault-prone software modules,"Software faults are defects in software modules that might cause failures. Software developers tend to focus on faults, because they are closely related to the amount of rework necessary to prevent future operational software failures. The goal of this paper is to predict which modules are fault-prone and to do it early enough in the life cycle to be useful to developers. A regression tree is an algorithm represented by an abstract tree, where the response variable is a real quantity. Software modules are classified as fault-prone or not, by comparing the predicted value to a threshold. A classification rule is proposed that allows one to choose a preferred balance between the two types of misclassification rates. A case study of a very large telecommunications systems considered software modules to be fault-prone, if any faults were discovered by customers. Our research shows that classifying fault-prone modules with regression trees and the using the classification rule in this paper, resulted in predictions with satisfactory accuracy and robustness.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044344,no,no,1486911994.301965
Comparative Study of Various Artificial Intelligence Techniques to Predict Software Quality,"Software quality prediction models are used to identify software modules that may cause potential quality problems. These models are based on various metrics available during the early stages of software development life cycle like product size, software complexity, coupling and cohesion. In this survey paper, we have compared and discussed some software quality prediction approaches based on Bayesian belief network, neural networks, fuzzy logic, support vector machine, expectation maximum likelihood algorithm and case-based reasoning. This study gives better comparative insight about these approaches, and helps to select an approach based on available resources and desired level of quality.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4196400,no,no,1486911994.301964
A novel out-of-band signaling mechanism for enhanced real-time support in tactical ad hoc wireless networks,"Ad hoc wireless networks have been an increasingly important area of research in the recent past. One issue of great interest in this area is the provision of real-time support. While existing military applications use reservation-based approaches to provide real-time bandwidth guarantees, these schemes are adversely affected by node mobility. We propose an enhanced real-time support scheme that uses a novel out-of-band signaling mechanism to predict future mobility patterns and take corrective action when needed. We also propose an architecture to support differentiated service classes. This helps mobility affected nodes to take proactive measures so as to offer better real-time services to bandwidth critical applications. Through extensive simulations, we show that the use of the out-of-band signaling leads to better real-time support overall, and better response to the more critical classes as needed. We also provide a theoretical analysis for estimating the probability of disruption of a real-time call.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317249,no,no,1486911994.301962
Utility of popular software defect models,Numerical models can be used to track and predict the number of defects in developmental and operational software. This paper introduces techniques to critically assess the effectiveness of software defect reduction efforts as the data is being gathered. This can be achieved by observing the fundamental shape of the cumulative defect discovery curves and by judging how quickly the various defect models converge to common predictions of long term software performance,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981659,no,no,1486911994.301961
Building effective defect-prediction models in practice,"Defective software modules cause software failures, increase development and maintenance costs, and decrease customer satisfaction. Effective defect prediction models can help developers focus quality assurance activities on defect-prone modules and thus improve software quality by using resources more efficiently. These models often use static measures obtained from source code, mainly size, coupling, cohesion, inheritance, and complexity measures, which have been associated with risk factors, such as defects and changes.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524911,no,no,1486911994.301959
Mining Change Patterns in AspectJ Software Evolution,"Understanding software change patterns during evolution is important for researchers concerned with alleviating change impacts. It can provide insight to understand the software evolution, predict future changes, and develop new refactoring algorithms. However, most of the current research focus on the procedural programs like C, or object-oriented programs like Java; seldom effort has been made for aspect-oriented software. In this paper, we propose an approach for mining change patterns in AspectJ software evolution. Our approach first decomposes the software changes into a set of atomic change representations, then employs the apriori data mining algorithm to generate the most frequent itemsets. The patterns we found reveal multiple properties of software changes, including their kind, frequency, and correlation with other changes. In our empirical evaluation on several non-trivial AspectJ benchmarks, we demonstrate that those change patterns can be used as measurement aid and fault predication for AspectJ software evolution analysis.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722012,no,no,1486911994.171344
Estimating Software Quality with Advanced Data Mining Techniques,"Current software quality estimation models often involve the use of supervised learning methods for building a software fault prediction models. In such models, dependent variable usually represents a software quality measurement indicating the quality of a module by risk-basked class membership, or the number of faults. Independent variables include various software metrics as McCabe, Error Count, Halstead, Line of Code, etc... In this paper we present the use of advanced tool for data mining called Multimethod on the case of building software fault prediction model. Multimethod combines different aspects of supervised learning methods in dynamical environment and therefore can improve accuracy of generated prediction model. We demonstrate the use Multimethod tool on the real data from the Metrics Data Project Data (MDP) Repository. Our preliminary empirical results show promising potentials of this approach in predicting software quality in a software measurement and quality dataset.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031804,no,no,1486911994.171343
Attribute Selection in Software Engineering Datasets for Detecting Fault Modules,"Decision making has been traditionally based on managers experience. At present, there is a number of software engineering (SE) repositories, and furthermore, automated data collection tools allow managers to collect large amounts of information, not without associated problems. On the one hand, such a large amount of information can overload project managers. On the other hand, problems found in generic project databases, where the data is collected from different organizations, is the large disparity of its instances. In this paper, we characterize several software engineering databases selecting attributes with the final aim that project managers can have a better global vision of the data they manage. In this paper, we make use of different data mining algorithms to select attributes from the different datasets publicly available (PROMISE repository), and then, use different classifiers to defect faulty modules. The results show that in general, the smaller datasets maintain the prediction capability with a lower number of attributes than the original datasets.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4301106,no,no,1486911994.171342
Software measurement: uncertainty and causal modeling,"Software measurement can play an important risk management role during product development. For example, metrics incorporated into predictive models can give advance warning of potential risks. The authors show how to use Bayesian networks, a graphical modeling technique, to predict software defects and-perform """"what if"""" scenarios.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1020298,no,no,1486911994.17134
A little knowledge about software,"Software engineering is still a young discipline. Software development group managers must keep their groups current with this dynamic body of knowledge as it evolves. There are two basic approaches: require staff to have both application expertise and software expertise, or create a software cell. The latter approach runs the risk of two communities not communicating well, although it might make staying abreast of changes in software engineering easier. The first approach should work better than it does today if some new educational patterns are put in place. For example, we could start treating software more like mathematics, introducing more software courses into undergraduate programs in other disciplines. Managers must also focus on the best way to develop software expertise for existing staff. Staff returning to school for a master's in software engineering can acquire a broad understanding of the field, but at a substantial cost in both time and effort. Short courses call help to fill this gap, but most short courses are skill based, whereas a deeper kind of learning is needed. As the first step, however, managers must assess software's impact on their bottom line deliverables. It might surprise them how much they depend on software expertise to deliver their products.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1270761,no,no,1486911994.171339
Top 10 list [software development],"Software's complexity and accelerated development schedules make avoiding defects difficult. We have found, however, that researchers have established objective and quantitative data, relationships, and predictive models that help software developers avoid predictable pitfalls and improve their ability to predict and control efficient software projects. The article presents 10 techniques that can help reduce the flaws in your code",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962984,no,no,1486911994.171337
An Empirical Study of the Classification Performance of Learners on Imbalanced and Noisy Software Quality Data,"In the domain of software quality classification, data mining techniques are used to construct models (learners) for identifying software modules that are most likely to be fault-prone. The performance of these models, however, can be negatively affected by class imbalance and noise. Data sampling techniques have been proposed to alleviate the problem of class imbalance, but the impact of data quality on these techniques has not been adequately addressed. We examine the combined effects of noise and imbalance on classification performance when seven commonly-used sampling techniques are applied to software quality measurement data. Our results show that some sampling techniques are more robust in the presence of noise than others. Further, sampling techniques are affected by noise differently given different levels of imbalance.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296694,no,no,1486911994.171334
Software quality prediction using Affinity Propagation algorithm,"Software metrics are collected at various phases of the software development process. These metrics contain the information of the software and can be used to predict software quality in the early stage of software life cycle. Intelligent computing techniques such as data mining can be applied in the study of software quality by analyzing software metrics. Clustering analysis, which can be considered as one of the data mining techniques, is adopted to build the software quality prediction models in the early period of software testing. In this paper, a new clustering method called Affinity Propagation is investigated for the analysis of two software metric datasets extracted from real-world software projects. Meanwhile, K-Means clustering method is also applied for comparison. The numerical experiment results show that the Affinity Propagation algorithm can be applied well in software quality prediction in the very early stage, and it is more effective on reducing Type II error.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4634056,no,no,1486911994.17133
Using Boosting Techniques to Improve Software Reliability Models Based on Genetic Programming,"Software reliability models are used to estimate the probability of a software fails along the time. They are fundamental to plan test activities and to ensure the quality of the software being developed. Two kind of models are generally used: time or test coverage based models. In our previous work, we successfully explored genetic programming (GP) to derive reliability models. However, nowadays boosting techniques (BT) have been successfully applied with other machine learning techniques, including GP. BT merge several hypotheses of the training set to get better results. With the goal of improving the GP software reliability models, this work explores the combination GP and BT. The results show advantages in the use of the proposed approach",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031955,no,no,1486911994.063838
Reducing overfitting in genetic programming models for software quality classification,"A high-assurance system is largely dependent on the quality of its underlying software. Software quality models can provide timely estimations of software quality, allowing the detection and correction of faults prior to operations. A software metrics-based quality prediction model may depict overfitting, which occurs when a prediction model has good accuracy on the training data but relatively poor accuracy on the test data. We present an approach to address the overfitting problem in the context of software quality classification models based on genetic programming (GP). The problem has not been addressed in depth for GP-based models. The presence of overfitting in a software quality classification model affects its practical usefulness, because management is interested in good performance of the model when applied to unseen software modules, i.e., generalization performance. In the process of building GP-based software quality classification models for a high-assurance telecommunications system, we observed that the GP models were prone to overfitting. We utilize a random sampling technique to reduce overfitting in our GP models. The approach has been found by many researchers as an effective method for reducing the time of a GP run. However, in our study we utilize random to reduce overfitting with the aim of improving the generalization capability of our GP models.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281730,no,no,1486911994.063836
An empirical evaluation of fault-proneness models,"Planning and allocating resources for testing is difficult and it is usually done on an empirical basis, often leading to unsatisfactory results. The possibility of early estimation of the potential faultiness of software could be of great help for planning and executing testing activities. Most research concentrates on the study of different techniques for computing multivariate models and evaluating their statistical validity, but we still lack experimental data about the validity of such models across different software applications. The paper reports on an empirical study of the validity of multivariate models for predicting software fault-proneness across different applications. It shows that suitably selected multivariate models can predict fault-proneness of modules of different software packages.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007972,no,no,1486911994.063835
Prediction of software reliability: a comparison between regression and neural network non-parametric models,"In this paper, neural networks have been proposed as an alternative technique to build software reliability growth models. A feedforward neural network was used to predict the number of faults initially resident in a program at the beginning of a test/debug process. To evaluate the predictive capability of the developed model, data sets from various projects were used. A comparison between regression parametric models and neural network models is provided",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=934046,no,no,1486911994.063833
Software reliability models with time-dependent hazard function based on Bayesian approach,"In this paper, two models predicting mean time until next failure based on Bayesian approach are presented. Times between failures follow Weibull distributions with stochastically decreasing ordering on the hazard functions of successive failure time intervals, reflecting the tester's intent to improve the software quality with each corrective action. We apply the proposed models to actual software failure data and show they give better results under sum of square errors criteria as compared to previous Bayesian models and other existing times between failures models. Finally, we utilize likelihood ratios criterion to compare new model's predictive performance",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=823478,no,no,1486911994.063832
Improving usefulness of software quality classification models based on Boolean discriminant functions,"BDF (Boolean discriminant functions) are an attractive technique for software quality estimation. Software quality classification models based on BDF provide stringent rules for classifying not fault-prone modules (nfp), thereby predicting a large number of modules as fp. Such models are practically not useful from software quality assurance and software management points of view. This is because, given the large number of modules predicted as fp, project management will face a difficult task of deploying, cost-effectively, the always-limited reliability improvement resources to all the fp modules. This paper proposes the use of generalized Boolean discriminant functions (GBDF) as a solution for improving the practical and managerial usefulness of classification models based on BDF. In addition, the use of GBDF avoids the need to build complex hybrid classification models in order to improve usefulness of models based on BDF. A case study of a full-scale industrial software system is presented to illustrate the promising results obtained from using the proposed classification technique using GBDF.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173256,no,no,1486911994.063831
Data Mining Techniques for Building Fault-proneness Models in Telecom Java Software,"This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and inspections and would like to be able to devote extra resources to faulty system parts. The main research focus of this paper is two-fold: (1) use and compare many data mining and machine learning techniques to build fault-proneness models based mostly on source code measures and change/fault history data, and (2) demonstrate that the usual classification evaluation criteria based on confusion matrices may not be fully appropriate to compare and evaluate models.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402213,no,no,1486911994.063829
Guest Editor's Introduction: The Promise of Public Software Engineering Data Repositories,"Scientific discovery related to software is based on a centuries-old paradigm common to all fields of science: setting up hypotheses and testing them through experiments. Repeatedly confirmed hypotheses become models that can describe and predict real-world phenomena. The best-known models in software engineering describe relationships between development processes, cost and schedule, defects, and numerous software """"-ilities"""" such as reliability, maintainability, and availability. But, compared to other disciplines, the science of software is relatively new. It's not surprising that most software models have proponents and opponents among software engineers. This introduction to the special issue discusses the power of modeling, the promise of data repositories, and the workshop devoted to this topic.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524910,no,no,1486911994.063827
The application of capture-recapture log-linear models to software inspections data,"Re-inspection has been deployed in industry to improve the quality of software inspections. The number of remaining defects after inspection is an important factor affecting whether to re-inspect the document or not. Models based on capture-recapture (CR) sampling techniques have been proposed to estimate the number of defects remaining in the document after inspection. Several publications have studied the robustness of some of these models using software engineering data. Unfortunately, most of the existing studies did not examine the log linear models with respect software inspection data. In order o explore the performance of the log linear models, we evaluated their performance for three person inspection teams. Furthermore, we evaluated the models using an inspection data set that was previously used to asses different CR models. Generally speaking, the study provided very promising results. According to our results, the log linear models proved to be more robust that all CR based models previously assessed for three-person inspections.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237980,no,no,1486911994.063826
Count Models for Software Quality Estimation,"Identifying which software modules, during the software development process, are likely to be faulty is an effective technique for improving software quality. Such an approach allows a more focused software quality & reliability enhancement endeavor. The development team may also like to know the number of faults that are likely to exist in a given program module, i.e., a quantitative quality prediction. However, classification techniques such as the logistic regression model (lrm) cannot be used to predict the number of faults. In contrast, count models such as the Poisson regression model (prm), and the zero-inflated Poisson (zip) regression model can be used to obtain both a qualitative classification, and a quantitative prediction for software quality. In the case of the classification models, a classification rule based on our previously developed generalized classification rule is used. In the context of count models, this study is the first to propose a generalized classification rule. Case studies of two industrial software systems are examined, and for each we developed two count models, (prm, and zip), and a classification model (lrm). Evaluating the predictive capabilities of the models, we concluded that the prm, and the zip models have similar classification accuracies as the lrm. The count models are also used to predict the number of faults for the two case studies. The zip model yielded better fault prediction accuracy than the prm. As compared to other quantitative prediction models for software quality, such as multiple linear regression (mlr), the prm, and zip models have a unique property of yielding the probability that a given number of faults will occur in any module",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4220787,no,no,1486911994.063823
Fault Detection Probability Analysis for Coverage-Based Test Suite Reduction,"Test suite reduction seeks to reduce the number of test cases in a test suite while retaining a high percentage of the original suite's fault detection effectiveness. Most approaches to this problem are based on eliminating test cases that are redundant relative to some coverage criterion. The effectiveness of applying various coverage criteria in test suite reduction is traditionally based on empirical comparison of two metrics derived from the full and reduced test suites and information about a set of known faults: (1) percentage size reduction and (2) percentage fault detection reduction, neither of which quantitatively takes test coverage data into account. Consequently, no existing measure expresses the likelihood of various coverage criteria to force coverage-based reduction to retain test cases that expose specific faults. In this paper, we develop and empirically evaluate, using a number of different coverage criteria, a new metric based on the """"average expected probability of finding a fault"""" in a reduced test suite. Our results indicate that the average probability of detecting each fault shows promise for identifying coverage criteria that work well for test suite reduction.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362646,no,no,1486911993.931699
SCARS: Scalable Self-Configurable Architecture for Reusable Space Systems,"Creating an environment of ldquono doubtrdquo for mission success is essential to most critical embedded applications. With reconfigurable devices such as field programmable gate arrays (FPGAs), designers are provided with a seductive tool to use as a basis for sophisticated but highly reliable platforms. We propose a two-level self-healing methodology for increasing the probability of success in critical missions. Our proposed system first undertakes healing at node-level. Failing to rectify system at node-level, network-level healing is undertaken. We have designed a system based on Xilinx Virtex-5 FPGAs and Cirronet DM2200 wireless mesh nodes to demonstrate autonomous wireless healing capability among networked node devices.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4584275,no,no,1486911993.931698
Numerical software quality control in object oriented development,"This paper proposes new method to predict the number of the remaining bugs at the delivery inspection applied to every iteration of OOD, object oriented development. Our method consists of two parts. The first one estimates the number of the remaining bugs by applying the Gompertz curve. The second one uses the interval estimation called OOQP, object oriented quality probe. The basic idea of OOQP is to randomly extract a relatively small number of test cases, usually 10 to 20% of the entire test cases, and to execute them in the actual operation environment. From the test result of OOQP, we can efficiently predict the number of the remaining bugs by the interval estimation. The premier problem of OOQP is that OOD is imposed to use the system design specification document whose contents, like UML, tend to be ambiguous. Our estimation method works well at a matrix-typed organization where a QA team and a development team collaboratively work together to improve the software quality.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1551146,no,no,1486911993.931696
Towards self-configuring networks,"Current networks require ad-hoc operating procedures by expert administrators to handle changes. These configuration management operations are costly and error prone. Active networks involve particularly fast dynamics of change that cannot depend on operators and must be automated. This paper describes an architecture called NESTOR that seeks to replace labor-intensive configuration management with one that is automated and software-intensive. Network element configuration state is represented in a unified object-relationship model. Management is automated via policy rules that control change propagation across model objects. Configuration constraints assure the consistency of model transactions. Model objects are stored in a distributed repository supporting atomicity and recovery of configuration change transactions. Element adapters are responsible for populating the repository with configuration objects, and for pushing committed changes to the underlying network elements. NESTOR has been implemented in two complementary versions and is now being applied to automate several configuration management scenarios of increasing complexity, with encouraging results",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003489,no,no,1486911993.931695
Object-Relational Database Metrics Formalization,"In this paper the formalization of a set of metrics for assessing the complexity of ORBDs is presented. An ontology for the SQL:2003 standard was produced, as a framework for representing the SQL schema definitions. It was represented using UML. The metrics were defined with OCL, upon the SQL:2003 ontology",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032266,no,no,1486911993.931693
A different view of fault prediction,"We investigated a different mode of using the prediction model to identify the files associated with a fixed percentage of the faults. The tester could ask the tool to identify which files are likely to contain the bulks of faults, with the tester selecting any desired percentage of faults. Again the tool would return a list ordered in decreasing order of the predicted numbers of faults in the files the model expects to be most problematic. If the number of files identified is too large, the tester could reselect a smaller percentage of faults. This would make the number of files requiring particular scrutiny manageable. We expect both modes to be valuable to professional software testers and developers.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508064,no,no,1486911993.931692
Performance evaluation of maximal ratio combining diversity over the Weibull fading channel in presence of co-channel interference,"The Weibull distribution has recently attracted much attention among the radio community as a statistical model that better describes the fading phenomenon on wireless channels. In this paper, we consider a multiple access system in which each of the desired signal as well as the co-channel interferers are subject to Weibull fading. We analyze the performance of the L-branch maximal ratio combining (MRC) receiver in terms of the outage probability in such scenario in the two cases where the diversity branches are assumed to be independent or correlated. The analysis is also applicable to the cases where the diversity branches and/or the interferers fading amplitudes are non-identically distributed. Due to the difficulty of handling the resulting outage probability expressions numerically using the currently available mathematical software packages, we alternatively propose using Pade approximation (PA) to make the results numerically tractable. We provide numerical results for different number of interferers, different number of diversity branches as well as different degrees of correlation and power unbalancing between diversity branches. All our numerical results are verified by means of Monte-Carlo simulations and excellent agreement between the two sets is noticed",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1696510,no,no,1486911993.931691
Assessing Value of SW Requirements,"Understanding software requirements and customer needs is vital for all SW companies around the world. Lately clearly more attention has been focused also on the costs, cost-effectiveness, productivity and value of software development and products. This study outlines concepts, principles and process of implementing a value assessment for SW requirements. The main purpose of this study is to collect experiences whether the value assessment for product requirements is useful for companies, works in practice, and what are the strengths and weaknesses of using it. This is done by implementing value assessment in a case company step by step to see which phases possibly work and which phases possibly do not work. The practical industrial case shows that proposed value assessment for product requirements is useful and supports companies trying to find value in their products.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483243,no,no,1486911993.93169
Concept analysis for module restructuring,"Low coupling between modules and high cohesion inside each module are the key features of good software design. This paper proposes a new approach to using concept analysis for module restructuring, based on the computation of extended concept subpartitions. Alternative modularizations, characterized by high cohesion around the internal structures that are being manipulated, can be determined by such a method. To assess the quality of the restructured modules, the trade-off between encapsulation violations and decomposition is considered, and proper measures for both factors are defined. Furthermore, the cost of restructuring is evaluated through a measure of distance between the original and the new modularizations. Concept subpartitions were determined for a test suite of 20 programs of variable size: 10 public-domain and 10 industrial applications. The trade-off between encapsulation and decomposition was measured on the resulting module candidates, together with an estimate of the cost of restructuring. Moreover, the ability of concept analysis to determine meaningful modularizations was assessed in two ways. First, programs without encapsulation violations were used as oracles, assuming the absence of violations as an indicator of careful decomposition. Second, the suggested restructuring interventions were actually implemented in some case studies to evaluate the feasibility of restructuring and to deeply investigate the code organization before and after the intervention. Concept analysis was experienced to be a powerful tool supporting module restructuring",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=917524,no,no,1486911993.931688
An approximate muscle guided global optimization algorithm for the Three-Index Assignment Problem,"The three-index assignment problem (AP3) is a famous NP-hard problem with wide applications. Since itpsilas intractable, many heuristics have been proposed to obtain near optimal solutions in reasonable time. In this paper, a new meta-heuristic was proposed for solving the AP3. Firstly, we introduced the conception of muscle (the union of optimal solutions) and proved that it is intractable to obtain the muscle under the assumption that P ne NP. Moreover, we showed that the whole muscle can be approximated by the union of local optimal solutions. Therefore, the approximate muscle guided global optimization (AMGO) is proposed to solve the AP3. AMGO employs a global optimization strategy to search in a search space reduced by the approximate muscle, which is constructed by a multi-restart scheme. During the global optimization procedure, the running time can be dramatically saved by detecting feasible solutions and extracting poor partial solutions. Extensive experimental results on the standard AP3 benchmark indicated that the new algorithm outperforms the state-of-the-art heuristics in terms of solution quality. Work of this paper not only provides a new meta-heuristic for NP-hard problems, but shows that global optimization can provide promising results in reasonable time, by restricting it to a fairly reduced search space.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631119,no,no,1486911993.928292
Fault-Prone Filtering: Detection of Fault-Prone Modules Using Spam Filtering Technique,"The fault-prone module detection in source code is of importance for assurance of software quality. Most of previous conventional fault-prone detection approaches have been based on using software metrics. Such approaches, however, have difficulties in collecting the metrics and constructing mathematical models based on the metrics. In order to mitigate such difficulties, we propose a novel approach for detecting fault-prone modules using a spam filtering technique. Because of the increase of needs for spam e-mail detection, the spam filtering technique has been progressed as a convenient and effective technique for text mining. In our approach, fault-prone modules are detected in a way that the source code modules are considered as text files and are applied to the spam filter directly. In order to show the usefulness of our approach, we conducted an experiment using source code repository of a Java based open source development. The result of experiment shows that our approach can classify more than 70% of software modules correctly.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343765,no,no,1486911993.928291
An Applied Study of Destructive Measurement System Analysis,"Measurement system analysis (MSA) is used to assess the ability of a measurement system to detect meaningful differences in process variables. For destructive measurement system, there are two methods to design and analyze it based on the homogenous batch size, including nested MSA and crossed MSA. At the same time, the P/T ratio (""""tolerance method"""") is modified to measure the suitability of destructive measurement system to make pass/fail decisions to a specification. Finally, rip off force testing system of chargers is assessed by crossed MSA and modified P/T ratio. Some suggestions for destructive MSA are also presented.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4318364,no,no,1486911993.92829
Quality metric for approximating subjective evaluation of 3-D objects,"Many factors, such as the number of vertices and the resolution of texture, can affect the display quality of three-dimensional (3-D) objects. When the resources of a graphics system are not sufficient to render the ideal image, degradation is inevitable. It is, therefore, important to study how individual factors will affect the overall quality, and how the degradation can be controlled given limited resources. In this paper, the essential factors determining the display quality are reviewed. We then integrate two important ones, resolution of texture and resolution of wireframe, and use them in our model as a perceptual metric. We assess this metric using statistical data collected from a 3-D quality evaluation experiment. The statistical model and the methodology to assess the display quality metric are discussed. A preliminary study of the reliability of the estimates is also described. The contribution of this paper lies in: 1) determining the relative importance of wireframe versus texture resolution in perceptual quality evaluation and 2) proposing an experimental strategy for verifying and fitting a quantitative model that estimates 3-D perceptual quality. The proposed quantitative method is found to fit closely to subjective ratings by human observers based on preliminary experimental results.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407900,no,no,1486911993.928289
Development of a virtual environment for fault diagnosis in rotary machinery,"Component fault analysis is a very widely researched area and requires a great deal of knowledge and expertise to establish a consistent and accurate tool for analysis. This paper will discuss a virtual diagnostic tool for fault detection of rotary machinery. The diagnostic tool has been developed using FMCELL software, which provides a 3D graphical visualization environment to modeling rotary machinery with virtual data acquisition capabilities. The developed diagnostic tool provides a virtual testbed with suitable graphical user interfaces for rapid diagnostic fault analysis of machinery. In this paper, we will discuss details of this newly developed virtual diagnostic model using FMCELL software and present our approach for diagnostics of a mechanical bearing test bed (TSU-BTB). Furthermore, we will provide some examples of how the virtual diagnostic environment can be used for performing machinery fault diagnostics. Using a frequency pattern matching superimposing technique, the model is proven to be able to detect primary faults in machines with fair accuracy and reliability",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918499,no,no,1486911993.928288
Diagnosis of Embedded Software Using Program Spectra,"Automated diagnosis of errors detected during software testing can improve the efficiency of the debugging process, and can thus help to make software more reliable. In this paper we discuss the application of a specific automated debugging technique, namely software fault localization through the analysis of program spectra, in the area of embedded software in high-volume consumer electronics products. We discuss why the technique is particularly well suited for this application domain, and through experiments on an industrial test case we demonstrate that it can lead to highly accurate diagnoses of realistic errors",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148936,no,no,1486911993.928286
NESTOR: an architecture for network self-management and organization,"Configuration management presently requires complex labor-intensive processes by experts. A single configuration task-installing/reconfiguring a system, or provisioning a service-typically involves a large number of activities fragmented among multiple network elements, each with its own proprietary configuration management instrumentation and tools. A change may cause configuration inconsistencies resulting in failures or inefficiencies; undoing changes to recover an operational state is often very difficult or even practically impossible. Therefore, configuration management is very costly, error prone, and often results in unpredictable failures and costly recovery. NESTOR seeks to replace labor-intensive configuration management with one that is automated and software-intensive. Configuration management is automated by policy scripts that access and manipulate respective network elements via a resource directory server (RDS). RDS provides a uniform object-relationship model of network resources and represents consistency in terms of constraints; it supports atomicity and recovery of configuration change transactions, and mechanisms to assure consistency through changes. RDS pushes configuration changes to network elements using a layer of adapters that translate operations on its object-relationship model to actions on respective elements. NESTOR has been implemented in two complementary versions and is now being applied to automate several configuration management scenarios of increasing complexity, with encouraging results.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842991,no,no,1486911993.928285
Unraveling ancient mysteries: reimagining the past using evolutionary computation in a complex gaming environment,"In this paper, we use principles from game theory, computer gaming, and evolutionary computation to produce a framework for investigating one of the great mysteries of the ancient Americas: why did the pre-Hispanic Pueblo (Anasazi) peoples leave large portions of their territories in the late A.D. 1200s? The gaming concept is overlaid on a large-scale agent-based simulation of the Anasazi. Agents in this game use a cultural algorithm framework to modify their finite-state automata (FSA) controllers following the work of Fogel (1966). In the game, there can be two kinds of active agents: scripted and unscripted. Unscripted agents attempt to maximize their survivability, whereas scripted agents can be used to test the impact that various pure and compound strategies for cooperation and defection have on the social structures produced by the overall system. The goal of our experiments here is to determine the extent to which cooperation and competition need to be present among the agent households in order to produce a population structure and spatial distribution similar to what has been observed archaeologically. We do this by embedding a """"trust in networks"""" game within the simulation. In this game, agents can choose from three pure strategies: defect, trust, and inspect. This game does not have a pure Nash equilibrium but instead has a mixed strategy Nash equilibrium such that a certain proportion of the population uses each at every time step, where the proportion relates to the quality of the signal used by the inspectors to predict defection. We use the cultural algorithm to help us determine what the mix of strategies might have been like in the prehistoric population. The simulation results indeed suggest a mixed strategy consisting of defectors, inspectors, and trustors was necessary to produce results compatible with the archaeological data. It is suggested that the presence of defectors derives from the unreliability of the signal which increases under drought conditions and produced increased stress on Anasazi communities and may have contributed to their departure.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1545945,no,no,1486911993.928284
A change impact dependency measure for predicting the maintainability of source code,"We first articulate the theoretic difficulties with the existing metrics designed for predicting software maintainability. To overcome the difficulties, we propose to measure a purely internal and objective attribute of code, namely change impact dependency, and show how it can be modeled to predict real change impact. The proposed base measure can be further elaborated for evaluating software maintainability.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342659,no,no,1486911993.928283
Automated video chain optimization,"Video processing algorithms found in complex video appliances such as television sets and set top boxes exhibit an interdependency that makes it is difficult to predict the picture quality of an end product before it is actually built. This quality is likely to improve when algorithm interaction is explicitly considered. Moreover, video algorithms tend to have many programmable parameters, which are traditionally tuned in manual fashion. Tuning these parameters automatically rather than manually is likely to speed up product development. We present a methodology that addresses these issues by means of a genetic algorithm that, driven by a novel objective image quality metric, finds high-quality configurations of the video processing chain of complex video products",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=964152,no,no,1486911993.928282
Schedules with minimized access latency for disseminating dependent information on multiple channels,"In wireless mobile environments, data broadcasting is an effective approach to disseminate information to mobile clients. In some applications, the access pattern of all the data to be downloaded can be represented by a DAG. In this paper, we consider the problem of efficiently generating the broadcast schedule on multiple channels when the data set has a DAG access pattern. We prove that it is NP-hard to find an optimal broadcast schedule which not only minimizes the latency but also satisfies the ancestor property which preserves the data dependency. We further rule out a condition for the input DAGs under which one can generate an optimal broadcast schedule in linear time and propose an algorithm, LBS, to generate the schedule under such a condition. For general DAGs, we provide three heuristics: the first one uses the overall access probability of each vertex; the second one considers the total access probability of a vertex's descendants; the third one combines the above two heuristics. We analyze these three heuristics and compare them through experiments. Our result shows that the third one can achieve a better broadcast schedule in terms of overall latency but costs more running time",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1636199,no,no,1486911993.92437
Wavelet transform approach to distance protection of transmission lines,"An application of wavelet transform to digital distance protection for transmission lines is presented in this paper. Fault simulation is carried out using the Power System Computer Aided Design program (PSCAD). The simulation results are used as an input to the proposed wavelet transform protection-relaying technique. The technique is based on decomposing the voltage and current signals at the relay location using wavelet filter banks (WFB). From the decomposed signals, faults can be detected and classified. Also the fundamental voltage and current phasors, which are needed to calculate the impedance to the fault point can be estimated. Results demonstrate that wavelets have high potential in distance relaying.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969995,no,no,1486911993.924369
Predicting risk of software changes,"Reducing the number of software failures is one of the most challenging problems of software production. We assume that software development proceeds as a series of changes and model the probability that a change to software will cause a failure. We use predictors based on the properties of a change itself. Such predictors include size in lines of code added, deleted, and unmodified; diffusion of the change and its component subchanges, as reflected in the number of files, modules, and subsystems touched, or changed; several measures of developer experience; and the type of change and its subchanges (fault fixes or new code). The model is built on historic information and is used to predict the risk of new changes. In this paper we apply the model to 5ESS software updates and find that change diffusion and developer experience are essential to predicting failures. The predictive model is implemented as a Web-based tool to allow timely prediction of change quality. The ability to predict the quality of change enables us to make appropriate decisions regarding inspection, testing, and delivery. Historic information on software changes is recorded in many commercial software projects, suggesting that our results can be easily and widely applied in practice.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6772130,no,no,1486911993.924368
A Design Quality Model for Service-Oriented Architecture,"Service-Oriented Architecture (SOA) is emerging as an effective solution to deal with rapid changes in the business environment. To handle fast-paced changes, organizations need to be able to assess the quality of its products prior to implementation. However, literature and industry has yet to explore the techniques for evaluating design quality of SOA artifacts. To address this need, this paper presents a hierarchical quality assessment model for early assessment of SOA system quality. By defining desirable quality attributes and tracing necessary metrics required to measure them, the approach establishes an assessment model for identification of metrics at different abstraction levels. Using the model, design problems can be detected and resolved before they work into the implemented system where they are more difficult to resolve. The model is validated against an empirical study on an existing SOA system to evaluate the quality impact from explicit and implicit changes to its requirements.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724572,no,no,1486911993.924366
Mining framework usage changes from instantiation code,"Framework evolution may break existing users, which need to be migrated to the new framework version. This is a tedious and error-prone process that benefits from automation. Existing approaches compare two versions of the framework code in order to find changes caused by refactorings. However, other kinds of changes exist, which are relevant for the migration. In this paper, we propose to mine framework usage change rules from already ported instantiations, the latter being applications build on top of the framework, or test cases maintained by the framework developers. Our evaluation shows that our approach finds usage changes not only caused by refactorings, but also by conceptual changes within the framework. Further, it copes well with some issues that plague tools focusing on finding refactorings such as deprecated program elements or multiple changes applied to a single program element.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814158,no,no,1486911993.924365
A Software Development Process for Small Projects,"The authors' development process integrates portions of an iterative, incremental process model with a quality assurance process that assesses each process phase's quality and a measurement process that collects data to guide process improvement. The process's goal is to produce the high quality and timely results required for today's market without imposing a large overhead on a small project.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6156718,no,no,1486911993.924364
A Classification of Architectural Reliability Models,"With the widespread use of software systems in the modern society, reliability of these systems have become as important as the functionality they provide. Building reliability into the software development process thus becomes critical for cost effective development and quality assurance. Existing reliability models (applied in post-implementation phases) may not be suitable to address reliability analysis at the software architecture level, as they often rely on implementation-level artifacts. In this paper, we present a framework for classifying reliability models based on their applicability to architectural artifacts, and assess several representative approaches based on the proposed classification. This study highlights several areas for future research.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270591,no,no,1486911993.924363
A New Method for Detection and Identification of Power Quality Disturbance,"In this paper, a new method for detection and identification of power quality disturbance is proposed: first, the original signals are de-noised by the wavelet transform; second, the beginning and ending time of the disturbance can be detected in the time domain, and a difference signal is formed; third, the type of power quality disturbance signals can be identified by the peak value and period; finally, parameters of disturbance signals can be calculated by fast Fourier transform during the disturbance time period. Based on this, software for detection and identification of power quality disturbance is developed. The application to a case study shows that this method is fast, sensitive, and practical for detection and identification of power quality disturbance",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4075971,no,no,1486911993.924362
Finite element analysis of internal winding faults in distribution transformers,"With the appearance of deregulation, distribution transformer predictive maintenance is becoming more important for utilities to prevent forced outages with the consequential costs. To detect and diagnose a transformer internal fault requires a transformer model to simulate these faults. This paper presents finite element analysis of internal winding faults in a distribution transformer. The transformer with a turn-to-earth fault or a turn-to-turn fault is modeled using coupled electromagnetic and structural finite elements. The terminal behaviors of the transformer are studied by an indirect coupling of the finite element method and circuit simulation. The procedure was realized using a commercially available software. The normal case and various faulty cases were simulated and the terminal behaviors of the transformer were studied and compared with field experimental results. The comparison results validate the finite element model to simulate internal faults in a distribution transformer.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=924821,no,no,1486911993.924361
QoS Explorer: A Tool for Exploring QoS in Composed Services,"This paper presents QoS explorer, an interactive tool we have developed which predicts quality of service (QoS) of a workflow from the QoS characteristics of its constituents, even when the relationships involved are complex. This facilitates design and instantiation of workflows to satisfy QoS constraints, as it enables the user to discover and focus effort on the aspects of a workflow which most affect their primary QoS concerns, thus improving efficiency of workflow development. Further, the underlying model we use is more sophisticated than those of similar recent work (Jaeger et al., 2005; Ardagna and Pernici, 2005; Menasce, 2004), and includes processing of entire statistical distributions and probabilistic states (instead of the simple numeric constants used elsewhere) to model such non-constant variables as execution time",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032096,no,no,1486911993.92436
Code optimization for code compression,"With the emergence of software delivery platforms such as Microsoft's .NET, the reduced size of transmitted binaries has become a very important system parameter, strongly affecting system performance. We present two novel pre-processing steps for code compression that explore program binaries' syntax and semantics to achieve superior compression ratios. The first preprocessing step involves heuristic partitioning of a program binary into streams with high auto-correlation. The second preprocessing step uses code optimization via instruction rescheduling in order to improve prediction probabilities for a given compression engine. We have developed three heuristics for instruction rescheduling that explore tradeoffs of the solution quality versus algorithm run-time. The pre-processing steps are integrated with the generic paradigm of prediction by partial matching (PPM) which is the basis of our compression codec. The compression algorithm is implemented for x86 binaries and tested on several large Microsoft applications. Binaries compressed using our compression codec are 18-24% smaller than those compressed using the best available off-the-shelf compressor.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1191555,no,no,1486911993.921001
A Hierarchy Management Framework for Automated Network Fault Identification,"An autonomous diagnosis approach of faulty links is proposed in this paper,. Given the information by which paths a designated network node with management responsibilities can communicate with certain other nodes, and can't communicate with another set of node, with the help of building diagnosis model and computing probability of link's failure the node with management responsibilities would like to identify as quickly as possible a ranked list of the most probable failed network links, and furthermore, accurately check out which links have failed by testing. Based on this approach, a hierarchy network management architecture is designed to deal with the fault diagnosis for a heterogeneous network environment. The simulation shows that this approach has the features of real-time, higher accuracy and autonomy, especially, it will occupy a few bit of bandwidth and even require no bandwidth.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4679266,no,no,1486911993.921
Useful cycles in probabilistic roadmap graphs,"Over the last decade, the probabilistic road map method (PRM) has become one of the dominant motion planning techniques. Due to its random nature, the resulting paths tend to be much longer than the optimal path despite the development of numerous smoothing techniques. Also, the path length varies a lot every time the algorithm is executed. We present a new technique that results in higher quality (shorter) paths with much less variation between the executions. The technique is based on adding useful cycles to the roadmap graph.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307190,no,no,1486911993.920999
A Method for an Accurate Early Prediction of Faults in Modified Classes,"In this paper we suggest and evaluate a method for predicting fault densities in modified classes early in the development process, i.e., before the modifications are implemented. We start by establishing methods that according to literature are considered the best for predicting fault densities of modified classes. We find that these methods can not be used until the system is implemented. We suggest our own methods, which are based on the same concept as the methods suggested in the literature, with the difference that our methods are applicable before the coding has started. We evaluate our methods using three large telecommunication systems produced by Ericsson. We find that our methods provide predictions that are of similar quality to the predictions based on metrics available after the code is implemented. Our predictions are, however, available much earlier in the development process. Therefore, they enable better planning of efficient fault prevention and fault detection activities",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021378,no,no,1486911993.920998
Model-based runtime analysis of distributed reactive systems,"Reactive distributed systems have pervaded everyday life and objects, but often lack measures to ensure adequate behaviour in the presence of unforeseen events or even errors at runtime. As interactions and dependencies within distributed systems increase, the problem of detecting failures which depend on the exact situation and environment conditions they occur in grows. As a result, not only the detection of failures is increasingly difficult, but also the differentiation between the symptoms of a fault, and the actual fault itself, i.e., the cause of a problem. In this paper, we present a novel and efficient approach for analysing reactive distributed systems at runtime, in that we provide a framework for detecting failures as well as identifying their causes. Our approach is based upon monitoring safety-properties, specified in the linear time temporal logic LTL (respectively, TLTL) to automatically generate monitor components which detect violations of these properties. Based on the results of the monitors, a dedicated diagnosis is then performed in order to identify explanations for the misbehaviour of a system. These may be used to store detailed log files, or to trigger recovery measures. Our framework is built modular, layered, and uses merely a minimal communication overhead - especially when compared to other, similar approaches. Further, we sketch first experimental results from our implementations, and describe how it can be used to build a variety of distributed systems using our techniques.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1615057,no,no,1486911993.920997
Symbolic Methods for VTB Model Development,"Summary form only given. Symbolic computation is not just helpful, it is very nearly a prerequisite for building models of complex objects that will be used in dynamic system simulators such as the virtual test bed (VTB). The reason that symbolic computation is so necessary is that, for reasons of compuatational speed, the code of any VTB model having natural coupling ports must directly express the Jacobian of the dynamic equations. Computing the often-large number of Jacobian terms by hand is extremely tedious, time consuming, and error-prone, and entirely unnecessary with today's symbolic math tools. VTB model developers currently rely on several different tools to accomplish model development. One of the tools, included directly in the VTB distribution package, comes in both runtime and development-time versions that allow a user to directly enter the math equations that describe the system dynamics. Then those equations are either interpreted and symbolically processed on-the-fly during system simulation, or they are interpreted and processed prior to generation of C code that will later be compiled into a model. The combination of these two approaches provides an incremental path for model development: the first approach supports rapidly prototyping of a model, and the second supports final development of full-featured models that may require some additional hand-tailoring of the C code. Additional model development tools that are being developed based on commercial symbolic math packages will also be described",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4075773,no,no,1486911993.920996
"Software, performance and resource utilisation metrics for context-aware mobile applications","As mobile applications become more pervasive, the need for assessing their quality, particularly in terms of efficiency (i.e., performance and resource utilisation), increases. Although there is a rich body of research and practice in developing metrics for traditional software, there has been little study on how these relate to mobile context-aware applications. Therefore, this paper defines and empirically evaluates metrics to capture software, resource utilisation and performance attributes, for the purpose of modelling their impact in context-aware mobile applications. To begin, a critical analysis of the problem domain identifies a number of specific software, resource utilisation and performance attributes. For each attribute, a concrete metric and technique of measurement is defined. A series of hypotheses are then proposed, and tested empirically using linear correlation analysis. The results support the hypotheses thus demonstrating the impact of software code attributes on the efficiency of mobile applications. As such, a more formal model in the form of mathematical equations is proposed in order to facilitate runtime decisions regarding the efficient placement of mobile objects in a context-aware mobile application framework. Finally, a preliminary empirical evaluation of the model is carried out using a typical application and an existing mobile application framework",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509290,no,no,1486911993.920994
Soft error sensitivity characterization for microprocessor dependability enhancement strategy,"This paper presents an empirical investigation on the soft error sensitivity (SES) of microprocessors, using the picoJava-II as an example, through software simulated fault injections in its RTL model. Soft errors are generated under a realistic fault model during program run-time. The SES of a processor logic block is defined as the probability that a soft error in the block causes the processor to behave erroneously or enter into an incorrect architectural state. The SES is measured at the functional block level. We have found that highly error-sensitive blocks are common for various workloads. At the same time soft errors in many other logic blocks rarely affect the computation integrity. Our results show that a reasonable prediction of the SES is possible by deduction from the processor's microarchitecture. We also demonstrate that the sensitivity-based integrity checking strategy can be an efficient way to improve fault coverage per unit redundancy.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028927,no,no,1486911993.920993
Predicting Subsystem Failures using Dependency Graph Complexities,"In any software project, developers need to be aware of existing dependencies and how they affect their system. We investigated the architecture and dependencies of Windows Server 2003 to show how to use the complexity of a subsystem's dependency graph to predict the number of failures at statistically significant levels. Such estimations can help to allocate software quality resources to the parts of a product that need it most, and as early as possible.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402214,no,no,1486911993.920992
Automatic Conflict Analysis and Resolution of Traffic Filtering Policy for Firewall and Security Gateway,"Firewalls and Security Gateways are core elements in network security infrastructure. As networks and services become more complex, managing access-list rules becomes an error-prone task. Conflicts in a policy can cause holes in security, and can often be hard to find while performing only visual or manual inspection. First, we have defined a methodology to systematically classify the severity of rule conflicts; secondly, we have proposed two different solutions to automatically resolve conflicts in a firewall. For one of them we found an algebraic proof of the existence of the solution and the convergence of the algorithm, and then we have made a software implementation to test it.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288891,no,no,1486911993.920991
Clinical Evaluation of Watermarked Medical Images,"Digital watermarking medical images provides security to the images. The purpose of this study was to see whether digitally watermarked images changed clinical diagnoses when assessed by radiologists. We embedded 256 bits watermark to various medical images in the region of non-interest (RONI) and 480K bits in both region of interest (ROI) and RONI. Our results showed that watermarking medical images did not alter clinical diagnoses. In addition, there was no difference in image quality when visually assessed by the medical radiologists. We therefore concluded that digital watermarking medical images were safe in terms of preserving image quality for clinical purposes",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463040,no,no,1486911993.917631
On Image Compression using Digital Curvelet Transform,"This paper describes a novel approach to digital image compression using a new mathematical transform: the curvelet transform. The transform has shown promising results over wavelet transform for 2D signals. Wavelets, though well suited to point singularities have limitations with orientation selectivity, and therefore, do not represent two-dimensional singularities (e.g. smooth curves) effectively. This paper employs the curvelet transform for image compression, exhibiting good approximation properties for smooth 2D functions. Curvelet improves wavelet by incorporating a directional component. The curvelet transform finds a direct discrete-space construction and is therefore computationally efficient. In this paper, we divided 2D spectrum into fine slices using iterated tree structured filter bank. Different amount of quantized curvelet coefficients were then selected for lossy compression and entropy encoding. A comparison with wavelet based compression was made for standard images like Lena, Barbara, etc. Curvelet transform has resulted in high quality image compression for natural images. Our implementation offers exact reconstruction, prone to perturbations, ease of implementation and low computational complexity. The algorithm works fairly well for grayscale and colored images",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4133497,no,no,1486911993.91763
Development of Growth Model-Based Decision Support System for Crop Management,"Growth model-based decision support system for crop management (GMDSSCM) was developed including process based models of 4 different crops, i.e. wheat, rice, rapeseed and cotton. This system aims in facilitating simulation and application of crop models for different purposes. Individual models each include six submodels for simulating phasic development, organ formation, biomass production, yield and quality formation, soil-crop water relations, and nutrient (N, P, K) balance. The implemented system can be used for evaluating individual and comprehensive management strategies based on the results of crop growth simulation under various environments and different genotypes. A Stand-alone version (GMDSSCMA) was established under the platforms VC++ and VB by adopting the characteristics of object-oriented and component-based software and with the effective integration and coupling of the growth-prediction and decision-making functions. A web-based system (GMDSSCMW) was further developed on a .net platform using C# language. These GMDSSCM systems have been used to predict dynamically crop growth and to make decisions regarding to management systems. This tool should be helpful for construction and application of informational and digital farming systems.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4548367,no,no,1486911993.917629
Information theoretic metrics for software architectures,"Because it codifies best practices, and because it supports various forms of software reuse, the discipline of software architecture is emerging as an important branch of software engineering research and practice. Because architectural-level decisions are prone to have a profound impact on finished software products, it is important to apprehend their quality attributes and to quantify them (as much as possible). In this paper, we discuss an information-theoretic approach to the definition and validation of architectural metrics, and illustrate our approach on a sample example",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960611,no,no,1486911993.917628
Improved Non-parametric Subtraction for Detection of Wafer Defect,"Automated defect inspection for wafer has been developed since 1990 's to replace defect detection by human eye for low-cost and high-quality. Defects are detected by comparing an inspected die with a reference die in application of wafer defect inspection. Referential methods compare with reference image by computing the intensity difference pixel by pixel between a reference image and an inspected image or measuring the similarity between two images using normalized cross correlation or eigen value. These methods are problematic for defect detection due to illumination change, noise and alignment error. To reduce the sensitivity of illumination change and noise, the new image subtraction called non-parametric subtraction was proposed. Non-parametric subtraction can solve problem about illumination change and noise, but sensitivity of alignment remains unsolved. This paper introduces new approach less sensitive to alignment using non-parametric subtraction for wafer defect inspection.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383738,no,no,1486911993.917627
Code and carrier divergence technique to detect ionosphere anomalies,"A single and dual frequency smoothing techniques implemented to detect ionosphere anomalies for GBAS system (ground based augmentation system) were discuss in this paper. As a dominant threat for using differential navigation satellites systems in landing applications an ionosphere storm is considered. To detect these occurrences the number of algorithms is developed. Some of them are addressed to meet the integrity requirements of CAT III landing and base on the multi frequency GPS techniques. Depending on the combination of frequency used during code and carrier phase measurements the smoothed pseudorange achieves a different level of accuracy. From this reason the most popular algorithms e.g. the divergence free and ionosphere free smoothing algorithms are analyzed and compared. In the article the works realized in the Institute of Radioelectronics connected with GBAS application are presented, too. The investigations were conducted by using actually GPS signals and signals from GNSS simulator. The self prepared software was used to analyze the results.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4585767,no,no,1486911993.917626
An expression's single fault model and the testing methods,"This paper proposes a single fault model for the faults of the expressions, including operator faults (operator reference fault: an operator is replaced by another, extra or missing operator for single operand), incorrect variable or constant, incorrect parentheses. These types of faults often exist in the software, but some fault classes are hard to detect using traditional testing methods. A general testing method is proposed to detect these types of faults. Furthermore the fault simulation method of the faults is presented which can accelerate the generation of test cases and minimize the testing cost greatly. Our empirical results indicate that our methods require a smaller number of test cases than random testing, while retaining fault-detection capabilities that are as good as, or better than the traditional testing methods.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250793,no,no,1486911993.917625
Software-implemented fault-tolerance and separate recovery strategies enhance maintainability [substation automation],"This paper describes a novel approach to software-implemented fault tolerance for distributed applications. This new approach can be used to enhance the flexibility and maintainability of the target applications in a cost-effective way. This is reached through a framework-approach including: (1) a library of fault tolerance functions; (2) a middleware application coordinating these functions; and (3) a language for the expression of nonfunctional services, including configuration, error recovery and fault injection. This framework-approach increases the availability and reliability of the application at a justifiable cost, also thanks to the re-usability of the components in different target systems. This framework-approach further increases the maintainability due to the separation of the functional behavior from the recovery strategies that are executed when an error is detected, because the modifications to functional and nonfunctional behavior are, to some extent, independent, and hence less complex to deal with. The resulting tool matches well, e.g., with current industrial requirements for embedded distributed systems, calling for adaptable and reusable software components. The """"integration of this approach in an automation system of a substation for electricity distribution"""" reports this experience. This case study shows in particular the ability of the configuration-and-recovery language ARIEL to allow adaptability to changes in the environment. This framework-approach is also useful in the context of distributed automation systems that are interconnected via a nondedicated network",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011520,no,no,1486911993.917624
A Model-based Approach to the Security Testing of Network Protocol Implementations,"Software is inherently buggy and those defects can lead to security breaches in applications. For more than a decade, buffer overflows have been the most common bugs found """"in the wild"""" and they often lead to critical security issues. Several techniques have been developed to defend against these types of security flaws, all with different rates of success. In this paper, we present a systematic approach for the automated testing of network protocol server implementations. The technique is based on established black-box testing methods (such as finite-state model-based testing and fault-injection) enhanced by the generation of intelligent, semantic-aware test cases that provide a more complete coverage of the code space. We also demonstrate the use of a model-based testing tool that can reliably detect vulnerabilities in server applications",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4116693,no,no,1486911993.917623
The Potemkin village and the art of deception,"Potemkin village is """"something that appears elaborate and impressive but in actual fact lacks substance"""". The software analogies for the Potemkin village are ripe for exploration. Software antipatterns address problem-solution pairs in which the typical solution is more harmful than the problem it solves. In essence, an antipattern presents an identifiable problem, the accompanying harmful effects of the typical solution, and a more appropriate solution called a refactoring. In this sense, the software version of a Potemkin village is an antipattern: Problem: deliver software with an impressive interface quickly. Solution: employ a ready-made architecture that provides an impressive interface quickly; spend as little time as possible on the back-end processing. Refactoring: do it right the first time. The author doesn't want to make the case too strongly that the building of Potemkin villages is a deliberate strategy of fraud that companies perpetrate. Is a weak piece of software covered by an elaborate GUI a deliberate fraud or simply poor design? You must assume the latter in the absence of proof. When faced with a Potemkin village or an emperor's new clothes situation, you must expose it immediately. Doing so is not easy when a high-quality GUI masks the shortcomings. You can, however, detect the situation through design reviews, and code inspections, reviews, or walkthroughs. Therefore, managers who oversee software projects (and customers who buy software) should require these reviews. Testing can sometimes uncover the situation, but it might be too late at this point. Test-driven design, on the other hand, can help avoid a Potemkin village.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407809,no,no,1486911993.917621
Path-based error coverage prediction,"Previous studies have shown that error detection coverage and other dependability measures estimated by fault injection experiments are affected by the workload. The workload is determined by the program executed during the experiments, and the input sequence to the program. In this paper, we present a promising analytical post-injection prediction technique, called path-based error coverage prediction, which reduces the effort of estimating error coverage for different input sequences. It predicts the error coverage for one input sequence based on fault injection results obtained for another input sequence. Although the accuracy of the prediction is low, path based error coverage prediction manages to correctly rank the input sequences with, respect to error detection coverage, provided that the difference in the actually coverage is significant. This technique may, drastically decrease the number of fault injection experiments, and thereby the time, needed to find the input sequence with the worst-case error coverage among a set of input sequences",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=937811,no,no,1486911993.914259
Predictive distribution reliability analysis considering post fault restoration and coordination failure,"Calculation of predicted distribution reliability indexes can be implemented using a distribution analysis model and the algorithms defined by Distribution System Reliability Handbook, EPRI Project 1356-1 Final Report. The calculation of predicted reliability indexes is fairly straightforward until post fault restoration and coordination failure are included. This paper presents the methods used to implement predictive reliability with consideration for post fault restoration and coordination failure into a distribution analysis software model",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=971384,no,no,1486911993.914258
Advanced Verification of Distributed WS-BPEL Business Processes Incorporating CSSA-based Data Flow Analysis,"The Business Process Execution Language for Web Services WS-BPEL provides an technology to aggregate encapsulated functionalities for defining high-value Web services. For a distributed application in a B2B interaction, the partners simply need to expose their provided functionality as BPEL processes and compose them. Verifying such distributed web service based systems has been a huge topic in the research community lately - cf. [4] for a good overview. However, in most of the work on analyzing properties of interacting Web Services, especially when backed by stateful implementations like WS-BPEL, the data flow present in the implementation is widely neglected, and the analysis focusses on control flow only. This might lead to false-positive analysis results when searching for design weaknesses and errors, e. g. analyzing the controllability [14] of a given BPEL process. In this paper, we present a method to extract dataflow information by constructing a CSSA representation and detecting data dependencies that effect communication behavior. Those discovered dependencies are used to construct a more precise formal model of the given BPEL process and hence to improve the quality of analysis results.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278643,no,no,1486911993.914257
Fault analysis of current-controlled PWM-inverter fed induction-motor drives,"In this paper, the fault-tolerance capability of IM-drive is studied. The discussion on the fault-tolerance of IM drives in the literature has mostly been on the conceptual level without any detailed analysis. Most of studies are only achieved experimentally. This paper provides an analytical tool to quickly analyze and predict the performance under fault conditions. Also, most of the presented results were machine specific and not general enough to be applicable as an evaluation tool. So, this paper will present a generalized method for predicting the post-fault performance of IM-drives after identifying the various faults that can occur. The fault analysis for IM in the motoring mode will be presented in this paper. The paper includes an analysis for different classifications of drive faults. The faults in an IM-drive -that will be studied- can be broadly classified as: machine fault, (i.e., one of stator windings is open or short, multiple phase open or short, bearings, and rotor bar is broken) and inverter-converter faults (i.e., phase switch open or short, multiple phase fault, and DC-link voltage drop). Briefly, a general-purpose software package for variety of IM-drive faults -is introduced. This package is very important in IM-fault diagnosis and detection using artificial intelligent techniques, wavelet and signal processing.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1218607,no,no,1486911993.914256
Dynamic Equilibrium Replica Location Algorithms in Data Grid,"Replica location is one of the key issues of data management in grid environment. Existing replica location services employ statistical location information to locate replica, lack initiative detecting mechanism for update, and equalize location information at large cost. In this paper, aimed at existing problems, combined with the characteristics of data gird, a dynamic equilibrium replica location algorithm (DERLS) was proposed. The idea underlying our DERLS algorithms consists of three parts: (1) Replica location service locates physical replica according to the density of pheromone by artificial ant spread. (2) A dynamic equilibrium technique is proposed to prevent positive feedback of basic ant algorithms. (3) Update strategy based on improved ant algorithms is used to detect new replica or resume fault replica. Experimental results show the availability and validity of the replica location in detail.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624479,no,no,1486911993.914255
Combinatorial Interaction Regression Testing: A Study of Test Case Generation and Prioritization,"Regression testing is an expensive part of the software maintenance process. Effective regression testing techniques select and order (or prioritize) test cases between successive releases of a program. However, selection and prioritization are dependent on the quality of the initial test suite. An effective and cost efficient test generation technique is combinatorial interaction testing, CIT, which systematically samples all t-way combinations of input parameters. Research on CIT, to date, has focused on single version software systems. There has been little work that empirically assesses the use of CIT test generation as the basis for selection or prioritization. In this paper we examine the effectiveness of CIT across multiple versions of two software subjects. Our results show that CIT performs well in finding seeded faults when compared with an exhaustive test set. We examine several CIT prioritization techniques and compare them with a re-generation/prioritization technique. We find that prioritized and re-generated/prioritized CIT test suites may find faults earlier than unordered CIT test suites, although the re-generated/prioritized test suites sometimes exhibit decreased fault detection.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362638,no,no,1486911993.914253
SHARP: a new real-time scheduling algorithm to improve security of parallel applications on heterogeneous clusters,"This paper addresses the problem of improving quality of security for real-time parallel applications on heterogeneous clusters. We propose a new security- and heterogeneity-driven scheduling algorithm (SHARP for short), which strives to maximize the probability that parallel applications are executed in time without any risk of being attacked. Because of high security overhead in existing clusters, an important step in scheduling is to guarantee jobs' security requirements while minimizing overall execution times. The SHARP algorithm accounts for security constraints in addition to different processing capabilities of each node in a cluster. We introduce two novel performance metrics, degree of security deficiency and risk-free probability, to quantitatively measure quality of security provided by a heterogeneous cluster. Both security and performance of SHARP are compared with two well-known scheduling algorithms. Extensive experimental studies using real-world traces confirm that the proposed SHARP algorithm significantly improves security and performance of parallel applications on heterogeneous clusters",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1629388,no,no,1486911993.914252
Optimal Admission Control for a Markovian Queue Under the Quality of Service Constraint,"We study an optimal admission of arriving customers to a Markovian finite-capacity queue, e.g. M/M/c/N queue, with several customer types. The system managers are paid for serving customers and penalized for rejecting them. The rewards and penalties depend on customer types. The goal is to maximize the average rewards per unit time subject to the constraint on the average penalties per unit time. We provide a solution to this problem through a Linear Programming transformation and characterize the structure of optimal policies based on Lagrangian optimization. For a feasible problem, we show the existence of a 1-randomized trunk reservation optimal policy with the acceptance thresholds for different customer types ordered according to a linear combination of the service rewards and rejection costs. I n addition, we prove that any 1-randomized optimal policy has this structure. In particular, we establish the structure of an optimal policy that maximizes the average rewards per unit time subject to the constraint on the blocking probability for one of the customer types or for a group of customer types pooled together, i.e., the QoS (Quality of Service) constraint. In the end, we also formulat the problem with multiple constraints and similar results hold.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1582409,no,no,1486911993.914251
Applying Novel Resampling Strategies To Software Defect Prediction,"Due to the tremendous complexity and sophistication of software, improving software reliability is an enormously difficult task. We study the software defect prediction problem, which focuses on predicting which modules will experience a failure during operation. Numerous studies have applied machine learning to software defect prediction; however, skewness in defect-prediction datasets usually undermines the learning algorithms. The resulting classifiers will often never predict the faulty minority class. This problem is well known in machine learning and is often referred to as learning from unbalanced datasets. We examine stratification, a widely used technique for learning unbalanced data that has received little attention in software defect prediction. Our experiments are focused on the SMOTE technique, which is a method of over-sampling minority-class examples. Our goal is to determine if SMOTE can improve recognition of defect-prone modules, and at what cost. Our experiments demonstrate that after SMOTE resampling, we have a more balanced classification. We found an improvement of at least 23% in the average geometric mean classification accuracy on four benchmark datasets.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271036,no,no,1486911993.91425
Challenges in Selecting COTS Component Guidelines,"Reuse of COTS (Commercial off the shelf components is a new development approach in software engineering. Developers get benefit from COTS-based development (CBD) environment to select suitable components, adopt and integrate into the system that to achieve better software, more quickly and lower cost. But the current environment didn't support the typical of CBD process in all processes [1]. Some environments are support in some steps or focus on how to develop the components and components interoperability that allow in the same environment. In this paper we propose a new environment tool called """"CS-COTS"""" are consist of (1) Sophisticate machine learning tools for generating rules in selection and integration. (2) Guideline for selecting methods category. (3) Predict a success rate to integrate methods fit into system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291078,no,no,1486911993.914249
"Towards """"Guardian Angels"""" and Improved Mobile User Experience","Today's mobile users expect high-quality experience which involves both high quality services as well as high service availability. It may take only a few bad service experiences such as dropped calls, unavailable navigation service, or delayed emails, to cause a mobile customer to consider switching service providers. Although great progress has been made in the radio communication and operations optimization as well as in the customer services areas, there are some hard technical problems yet to be solved to offer a personalized quality of service assurance across both the basic phone service and advanced applications. Our position as a major telecommunication software provider has given us great insight into these issues and in this paper we present the theoretical and architectural details of two interrelated approaches that could provide feasible means for improved quality of mobile user experience through intelligent device and network-resident software components. For the end user it will seem as though a """"guardian angel"""" is on her shoulder describing, predicting and explaining disruptive events.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4698120,no,no,1486911993.910886
A generic model and tool support for assessing and improving Web processes,"We discuss a generic quality framework, based on a generic model, for evaluating Web processes. The aim is to perform assessment and improvement of web processes by using techniques from empirical software engineering. A web development process can be broadly classified into two almost independent sub-processes: the authoring process (AUTH process) and the process of developing the infrastructure (INF process). The AUTH process concerns the creation and management of the contents of a set of nodes and the way they are linked to produce a web application, whereas the INF development process provides technological support and involves creation of databases, integration of the web application to legacy systems etc. In this paper, we instantiate our generic quality model to the AUTH process and present a measurement framework for this process. We also present a tool support to provide effective guidance to software personnel including developers, managers and quality assurance engineers.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011333,no,no,1486911993.910885
Context-based adaptive binary arithmetic coding in the H.264/AVC video compression standard,"Context-based adaptive binary arithmetic coding (CABAC) as a normative part of the new ITU-T/ISO/IEC standard H.264/AVC for video compression is presented. By combining an adaptive binary arithmetic coding technique with context modeling, a high degree of adaptation and redundancy reduction is achieved. The CABAC framework also includes a novel low-complexity method for binary arithmetic coding and probability estimation that is well suited for efficient hardware and software implementations. CABAC significantly outperforms the baseline entropy coding method of H.264/AVC for the typical area of envisaged target applications. For a set of test sequences representing typical material used in broadcast applications and for a range of acceptable video quality of about 30 to 38 dB, average bit-rate savings of 9%-14% are achieved.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1218195,no,no,1486911993.910884
On the Relation between External Software Quality and Static Code Analysis,"Only a few studies exist that try to investigate whether there is a significant correlation between external software quality and the data provided by static code analysis tools. A clarification on this issue could pave the way for more precise prediction models on the probability of defects based on the violation of programming rules. We therefore initiated a study where the defect data of selected versions of the open source development environment ldquoEclipse SDKrdquo is correlated with the data provided by the static code analysis tools PMD and FindBugs applied the source code of Eclipse. The results from this study are promising as especially some PMD rulesets show a good correlation with the defect data and could therefore serve as basis for measurement, control and prediction of software quality.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328373,no,no,1486911993.910883
Delay-Differentiated Gossiping in Delay Tolerant Networks,"Delay Tolerant Networks are increasingly being envisioned for a wide range of applications. Many of these applications need support for quality of service (QoS) differentiation from the network. This paper proposes a method for providing probabilistic delay assurances in DTNs. In particular, the paper presents a method called Delay- Differentiated Gossiping to assure a certain probability of meeting the packets' delay requirements while using as little network resources as possible. The idea is to adapt a set of forwarding probabilities and time-to-live parameters to control the usage of network resources based on how the delay requirements are being met. Empirical results evaluating the effectiveness of the proposed method are also included. The results show that there are simple ways of assuring the delay requirements while making effective use of the network resources.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4533655,no,no,1486911993.910882
Implementation and Applications of Wide-area monitoring systems,"This paper discusses the design and applications of wide area monitoring and control systems, which can complement classical protection systems and SCADA/EMS applications. System wide installed phasor measurement units send their measured data to a central computer, where snapshots of the dynamic system behavior are made available online. This new quality of system information opens up a wide range of new applications to assess and actively maintain system's stability in case of voltage, angle or frequency instability, thermal overload and oscillations. Recent developed algorithms and their design for these application areas are introduced. With practical examples the benefits in terms of system security are shown..",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275326,no,no,1486911993.910881
Envisioning the Next-Generation of Functional Testing Tools,"The functional test-driven development (FTDD) cycle moves functional test specification to the earliest part of the software development life cycle. Functional tests no longer merely assess quality; their purpose now is to drive quality. For some agile processes such as extreme programming, functional tests are the primary requirements specification artifact. When functional tests serve as both the system specification and the automated regression test safety net, they must remain viable for the production code's lifetime. A successful functional test-driven development strategy relies on effective fools across the application life cycle. This article reflects on FTDD teams' core tasks performed over the full application life cycle. It then envisions a conceptual functional testing framework and a concrete list of capabilities that satisfy these needs",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4163030,no,no,1486911993.91088
FPGA implementation of spiking neural networks - an initial step towards building tangible collaborative autonomous agents,"This work contains the results of an initial study into the FPGA implementation of a spiking neural network. This work was undertaken as a task in a project that aims to design and develop a new kind of tangible collaborative autonomous agent. The project intends to exploit/investigate methods for engineering emergent collective behaviour in large societies of actual miniature agents that can learn and evolve. Such multi-agent systems could be used to detect and collectively repair faults in a variety of applications where it is difficult for humans to gain access, such as fluidic environments found in critical components of material/industrial systems. The initial achievement of implementation of a spiking neural network on a FPGA hardware platform and results of a robotic wall following task are discussed by comparison with software driven robots and simulations.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1393322,no,no,1486911993.910879
Role of requirements engineering in software development process: an empirical study,"Requirements problems are widely acknowledged to reduce the quality of software. This work details an empirical study of requirements problems as identified by eleven Australian software companies. Our analysis aims to provide RE practitioners with some insight into designing appropriate RE processes in order to achieve better results. This research was a two-fold process; firstly, a requirements process maturity was assessed and secondly, the types and number of problems faced by different practitioners during their software project was documented. The results indicate that there is no significant difference in problems faced by companies with mature and immature RE process. These findings suggest that a holistic approach is required in order to achieve quality software and organizations should not solely concentrate on improving requirement process. Through our empirical study we have also analysed problems identified by different groups of practitioners and found that there are more differences than similarities in the problems across practitioner groups.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416759,no,no,1486911993.910877
Combinatorial designs in multiple faults localization for battlefield networks,We present an application of combinatorial designs and variance analysis to correlating events in the midst of multiple network faults. The network fault model is based on the probabilistic dependency graph that accounts for the uncertainty about the state of network elements. Orthogonal arrays help reduce the exponential number of failure configurations to a small subset on which further analysis is performed. The preliminary results show that statistical analysis can pinpoint the probable causes of the observed symptoms with high accuracy and significant level of confidence. An example demonstrates how multiple soft link failures are localized in MIL-STD 188-220's datalink layer to explain the end-to-end connectivity problems in the network layer This technique can be utilized for the networks operating in an unreliable environment such as wireless and/or military networks.,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=985975,no,no,1486911993.910876
Application of a Robust and Efficient ICP Algorithm for Fitting a Deformable 3D Human Torso Model to Noisy Data,"We investigate the use of an iterative closest point (ICP) algorithm in the alignment of a point distribution model (PDM) of 3D human female torsos to sample female torso data. An approximate k-d tree procedure for efficient ICP is tested to assess whether it improves the speed of the alignment process. The use of different error norms, namely L2 and L1, are compared to ascertain if either offers an advantage in terms of convergence and in the quality of the final fit when the sample data is clean, noisy or has some data missing. It is found that the performance of the ICP algorithm used is improved in both speed of convergence and accuracy of fit through the combined use of an approximate and exact k-d tree search procedure and with the minimisation of the L1 norm even when up to 50% of the data is noisy or up to 25% is missing. We demonstrate the use of this algorithm in providing, via a fitted torso PDM, smooth surfaces for noisy torso data and valid data points for torsos with missing data.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1587664,no,no,1486911993.907515
The Dangers of Failure Masking in Fault-Tolerant Software: Aspects of a Recent In-Flight Upset Event,"On 1 August 2005, a Boeing Company 777-200 aircraft, operating on an international passenger flight from Australia to Malaysia, was involved in a significant upset event while flying on autopilot. The Australian Transport Safety Bureau's investigation into the event discovered that """"an anomaly existed in the component software hierarchy that allowed inputs from a known faulty accelerometer to be processed by the air data inertial reference unit (ADIRU) and used by the primary flight computer, autopilot and other aircraft systems."""" This anomaly had existed in original ADIRU software, and had not been detected in the testing and certification process for the unit. This paper describes the software aspects of the incident in detail, and suggests possible implications concerning complex, safety- critical, fault-tolerant software.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4399910,no,no,1486911993.907514
On the effectiveness of mutation analysis as a black box testing technique,"The technique of mutation testing, in which the effectiveness of tests is determined by creating variants of a program in which statements are mutated, is well known.. Whilst of considerable theoretical interest, the technique requires costly tools and is computationally expensive. Very large numbers of `mutants' can be generated for even simple programs. More recently, it has been proposed that the concept be applied to specification based (black box) testing. The proposal is to generate test cases by systematically replacing data items relevant to a particular part of a specification with a data item relevant to another. If the specification is considered as generating a language that describes the set of valid inputs, then the mutation process is intended to generate syntactically valid and invalid statements. Irrespective of their 'correctness' in terms of the specification, these can then be used to test a program in the usual (black box) manner. For this approach to have practical value it must produce test cases that would not be generated by other popular black box test generation approaches. The paper reports a case study involving the application of mutation based black box testing to two programs of different types. Test cases were also generated using equivalence class testing. and boundary value testing approaches. The test cases from each method were examined to judge the overlap and to assess the value of the additional cases generated. It was found that less than 20% of the mutation test cases for a data-vetting program were generated by the other two methods, as against 75% for a statistical analysis program. The paper analyses these results and suggests classes of specifications for which mutation based test-case generation may be effective",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948492,no,no,1486911993.907513
Usability measures for software components,"The last decade marked the first real attempt to turn software development into engineering through the concepts of Component-Based Software Development (CBSD) and Commercial Off-The-Shelf (COTS) components, with the goal of creating high-quality parts that could be joined together to form a functioning system. One of the most critical processes in CBSD is the selection of the software components (from either in-house or external repositories) that fulfill some architectural and user-defined requirements. However, there is currently a lack of quality models and metrics that can help evaluate the quality characteristics of software components during this selection process. This paper presents a set of measures to assess the Usability of software components, and describes the method followed to obtain and validate them.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642462,no,no,1486911993.907512
Syntactic fault patterns in OO programs,"Although program faults are widely studied, there are many aspects of faults that we still do not understand, particularly about OO software. In addition to the simple fact that one important goal during testing is to cause failures and thereby detect faults, a full understanding of the characteristics of faults is crucial to several research areas. The power that inheritance and polymorphism brings to the expressiveness of programming languages also brings a number of new anomalies and fault types. In prior work we presented a fault model for the appearance and realization of OO faults that are specific to the use of inheritance and polymorphism. Many of these faults cannot appear unless certain syntactic patterns are used. The patterns are based on language constructs, such as overriding methods that directly define inherited state variables and non-inherited methods that call inherited methods. If one of these syntactic patterns is used, then we say the software contains an anomaly and possibly a fault. We describe the syntactic patterns for each OO fault type. These syntactic patterns can potentially be found with an automatic tool. Thus, faults can be uncovered and removed early in development.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181512,no,no,1486911993.90751
HUGE: an integrated system for human understandable granule extraction,"An integrated system for the extraction of interpretable information granules from data is presented. The system, called HUGE (human understandable granule extraction), is designed as a highly reusable software framework that can embody several techniques for information granulation within a homogeneous user interface. HUGE includes a set of tools that enable both qualitative and quantitative evaluation of the results of a granulation technique. Visualization tools allow the user to assess graphically the properties of extracted information granules. Evaluation tools give the user the possibility to estimate numerically the quality of the derived granules. The usefulness of HUGE in supporting the user in a granulation process is shown through a case study concerning the design of an interpretable fuzzy inference system for predicting automobile fuel consumption.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1548625,no,no,1486911993.907509
Plain end-to-end measurement for local area network voice transmission feasibility,"It is well known that company Intranets are growing into ubiquitous communications media for everything. As a consequence, network traffic is notoriously dynamic, and unpredictable. Unfortunately, local area networks were designed for scalability and robustness, not for sophisticated traffic monitoring. This paper introduces a performance measurement method based on widely used IP protocol elements, which allows measurement of network performance criteria to predict the voice transmission feasibility of a given local area network. The measurement does neither depend on special VoIP equipment, nor does it need network monitoring hardware. Rather it uses special payload samples to detect unloaded network conditions to receive reference values. These samples are followed by typical VoIP application payload to obtain real-world measurement conditions. The validation of our method was done within a local area network and showed convincing results",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948873,no,no,1486911993.907508
Scalable network assessment for IP telephony,Multimedia applications such as IP telephony are among the applications that demand strict quality of service (QoS) guarantees from the underlying data network. At the predeployment stage it is critical to assess whether the network can handle the QoS requirements of IP telephony and fix problems that may prevent a successful deployment. In this paper we describe a technique for efficiently assessing network readiness for IP telephony. Our technique relies on understanding link level QoS behavior in a network from an IP telephony perspective. We use network topology and end-to-end measurements collected from the network in locating the sources of performance problems that may prevent a successful IP telephony deployment. We present an empirical study conducted on a real network spanning three geographically separated sites of an enterprise network. The empirical results indicate that our approach efficiently and accurately pinpoints links in the network incurring the most significant delay.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1312762,no,no,1486911993.907507
Unknown non-self detection & robustness of distributed artificial immune system with normal model,"Biological immune system is typical distributed parallel system for processing biological information to defense the body against viruses and diseases. Inspired from nature, a distributed artificial immune system with the normal model is proposed for detecting unknown non-selfs such as worms and software faults. Traditional approaches are used to learn unknown features and types of the unknown non-selfs, but the learning problem can not be solved for human immune system in short time, neither that for the machines. A new detecting approach is proposed with the normal model of the system, and the selfs of the system are represented and detected at first. Depending on strictness and completeness of the normal model, the selfs are known and the process for detecting the selfs is much easier and more accurate than that for the non-selfs. Not only the artificial immune system can detect the non-selfs, but also the system can eliminate the non-selfs and repair the damaged parts of the system by itself. Minimization of the non-selfs and maximization of the selfs show robustness of the artificial immune system, and robustness of the distributed artificial immune system can be reduced according to each independent module.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4593134,no,no,1486911993.907506
The Application of Improved BP Neural Network Algorithm in Urban Air Quality Prediction: Evidence from China,"According to the limitations of traditional BP neural network algorithm, the method of adding momentum factor and changing learning rate is used to improve the traditional BP neural network algorithm and establish the new model of BP neural network which is applied to the urban air quality prediction. Practical application shows that improved BP neural network algorithm overcome the shortcomings like slow convergence speed, bad generation ability and easily falling into local minimum values. The model established for urban air quality prediction has characteristics of representative and predicting ability so that it has a broad application prospect in future urban air quality assessment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756756,no,no,1486911993.907505
POSAML: A Visual Modeling Framework for Middleware Provisioning,"Effective provisioning of next generation distributed applications hosted on diverse middleware platforms incurs significant challenges due to the applications' growing complexity and quality of service (QoS) requirements. An effective provisioning of the middleware platform includes a composition and configuration of the middleware services that meets the application QoS requirements under expected workloads. Traditional techniques for middleware provisioning tend to use non-intuitive, low-level and technology-specific approaches, which are tedious, error prone, non-reusable and not amenable to ease of QoS validation. Additionally, most often the configuration activities of the middleware platform tend to be decoupled from the QoS validation stages resulting in an iterative trial-and-error process between the two phases. This paper describes the design of a visual modeling language called POSAML (patterns oriented software architecture modeling language) and associated tools that provide an intuitive, higher level and unified framework for provisioning middleware platforms. POSAML provides visual modeling capabilities for middleware-independent provisioning while allowing automated middleware-specific QoS validation",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076969,no,no,1486911993.90404
Anomaly-based Fault Detection System in Distributed System,"One of the important design criteria for distributed systems and their applications is their reliability and robustness to hardware and software failures. The increase in complexity, inter connectedness, dependency and the asynchronous interactions between the components that include hardware resources (computers, servers, network devices), and software (application services, middleware, web services, etc.) makes the fault detection and tolerance a challenging research problem. In this paper, we present an innovative approach based on statistical and data mining techniques to detect faults (hardware or software) and also identify the source of the fault. In our approach, we monitor and analyze in realtime all the interactions between all the components of a distributed system. We used data mining and supervised learning techniques to obtain the rules that can accurately model the normal interactions among these components. Our anomaly analysis engine will immediately produce an alert whenever one or more of the interaction rules that capture normal operations is violated due to a software or hardware failure. We evaluate the effectiveness of our approach and its performance to detect software faults that we inject asynchronously, and compare the results for different noise level.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297016,no,no,1486911993.904039
Leveraging user-session data to support Web application testing,"Web applications are vital components of the global information infrastructure, and it is important to ensure their dependability. Many techniques and tools for validating Web applications have been created, but few of these have addressed the need to test Web application functionality and none have attempted to leverage data gathered in the operation of Web applications to assist with testing. In this paper, we present several techniques for using user session data gathered as users operate Web applications to help test those applications from a functional standpoint. We report results of an experiment comparing these new techniques to existing white-box techniques for creating test cases for Web applications, assessing both the adequacy of the generated test cases and their ability to detect faults on a point-of-sale Web application. Our results show that user session data can be used to produce test suites more effective overall than those produced by the white-box techniques considered; however, the faults detected by the two classes of techniques differ, suggesting that the techniques are complementary.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1423991,no,no,1486911993.904038
On feature interactions among Web services,"Web services promise to allow businesses to adapt rapidly to changes in the business environment, and the needs of different customers. However, the rapid introduction of new services paired with the dynamicity of the business environment also leads to undesirable interactions that negatively impact service quality and user satisfaction. In this paper, we propose an approach for modeling such undesirable interactions as feature interactions. Our approach for detecting interactions is based on goal-oriented analysis and scenario modeling. It allows us to reason about feature interactions in terms of goal conflicts, and feature deployment. Two case studies illustrate the approach. The paper concludes with a discussion, and an outlook on future research.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1314727,no,no,1486911993.904037
On detecting global predicates in distributed computations,"Monitoring of global predicates is a fundamental problem in asynchronous distributed systems. This problem arises in various contexts, such as design, testing and debugging, and fault tolerance of distributed programs. In this paper, we establish that the problem of determining whether there exists a consistent cut of a computation that satisfies a predicate in k-CNF (k&ges;2), in which no two clauses contain variables from the same process, is NP-complete in general. A polynomial-time algorithm to find the consistent cut, if it exists, that satisfies the predicate for special cases is provided. We also give algorithms (albeit exponential) that can be used to achieve an exponential reduction in time over existing techniques for solving the general version. Furthermore, we present an algorithm to determine whether there exists a consistent cut of a computation for which the sum x<sub>1</sub>+x<sub>2</sub>++x<sub>n</sub> exactly equals some constant k, where each x<sub>i</sub> is an integer variable on a process p<sub>i</sub> such that it is incremented or decremented by at most one at each step. As a corollary, any symmetric global predicate on Boolean variables, such as absence of simple majority and exclusive-OR of local predicates, can now be detected. Additionally, the problem is proved to be NP-complete if each x<sub>i </sub> can be changed by an arbitrary amount at each step. Our results solve the previously open problems in predicate detection proposed by V.K. Garg (1997) and bridge the wide gap between the known tractability and intractability results that have existed until now",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918927,no,no,1486911993.904036
Effective software-based self-test strategies for on-line periodic testing of embedded processors,"Software-based self-test (SBST) strategies are particularly useful for periodic testing of deeply embedded processors in low-cost embedded systems with respect to permanent and intermittent operational faults. Such strategies are well suited to embedded systems that do not require immediate detection of errors and cannot afford the well-known hardware, information, software, or time-redundancy mechanisms. We first identify the stringent characteristics of a SBST program to be suitable for on-line periodic testing. Also, we study the probability for a SBST program to detect permanent and intermittent faults during on-line periodic testing. Then, we introduce a new SBST methodology with a new classification and test-priority scheme for processor components. After that, we analyze the self-test routine code styles for the three more effective test pattern generation (TPG) strategies in order to select the most effective self-test routine for on-line periodic testing of a component under test. Finally, we demonstrate the effectiveness of the proposed SBST methodology for on-line periodic testing by presenting experimental results for two pipeline reduced instruction set computers reduced instruction set processors of different architecture.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1372664,no,no,1486911993.904035
The design of reliable devices for mission-critical applications,"Mission-critical applications require that any failure that may lead to erroneous behavior and computation is detected and signaled as soon as possible in order not to jeopardize the entire system. Totally self-checking (TSC) systems are designed to be able to autonomously detect faults when they occur during normal circuit operation. Based on the adopted TSC design strategy and the goal pursued during circuit realization (e.g., area minimization), the circuit, although TSC, may not promptly detect the fault depending on the actual number of input configurations that serve as test vectors for each fault in the network. If such a number is limited, although TSC it may be improbable that the fault is detected once it occurs, causing detection and aliasing problems. The paper presents a design methodology, based on a circuit re-design approach and an evaluation function, for improving a TSC circuit promptness in detecting faults' occurrence, a property we will refer to as TSC quality.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1246540,no,no,1486911993.904034
Effort measurement in student software engineering projects,"Teaching software engineering by means of student involvement in the team development of a product is the most effective way to teach the main issues of software engineering. Some of its difficulties are those of coordinating their work, measuring the time spent by the students (both in individual work and in meetings) and making sure that meeting time will not be excessive. Starting in the academic year 1998/1999, we assessed, improved and documented the development process for the student projects and found that measurement is one of the outstanding issues to be considered. Each week, the students report the time spent on the different project activities. We present and analyze the measurement results for our 16 student teams (each one with around 6 students). It is interesting to note that the time spent in meetings is usually too long, ranging from 46% in the requirements analysis phase to 21% in coding, mainly due to problems of coordination. Results from previous years are analyzed and presented to the following year's students for feedback. In the present year (2000), we have decreased the amount of time spent by the student doing group work, and improved the effectiveness and coordination of the teams",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897672,no,no,1486911993.904033
Comparing high-change modules and modules with the highest measurement values in two large-scale open-source products,"Identifying change-prone modules can enable software developers to take focused preventive actions that can reduce maintenance costs and improve quality. Some researchers observed a correlation between change proneness and structural measures, such as size, coupling, cohesion, and inheritance measures. However, the modules with the highest measurement values were not found to be the most troublesome modules by some of our colleagues in industry, which was confirmed by our previous study of six large-scale industrial products. To obtain additional evidence, we identified and compared high-change modules and modules with the highest measurement values in two large-scale open-source products, Mozilla and OpenOffice, and we characterized the relationship between them. Contrary to common intuition, we found through formal hypothesis testing that the top modules in change-count rankings and the modules with the highest measurement values were different. In addition, we observed that high-change modules had fairly high places in measurement rankings, but not the highest places. The accumulated findings from these two open-source products, together with our previous similar findings for six closed-source products, should provide practitioners with additional guidance in identifying the change-prone modules.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498769,no,no,1486911993.904032
Development of ANN-based virtual fault detector for Wheatstone bridge-oriented transducers,This paper reports on the development of a new artificial neural network-based virtual fault detector (VFD) for detection and identification of faults in DAS-connected Wheatstone bridge-oriented transducers of a computer-based measurement system. Experimental results show that the implemented VFD is convenient for fusing intelligence into such systems in a user-interactive manner. The performance of the proposed VFD is examined experimentally to detect seven frequently occurring faults automatically in such transducers. The presented technique used an artificial neural network-based two-class pattern classification network with hard-limit perceptrons to fulfill the function of an efficient residual generator component of the proposed VFD. The proposed soft residual generator detects and identifies various transducer faults in collaboration with a virtual instrument software-based inbuilt algorithm. An example application is also presented to demonstrate the use of implemented VFD practically for detecting and diagnosing faults in a pressure transducer having semiconductor strain gauges connected in a Wheatstone bridge configuration. The results obtained in the example application with this strategy are promising.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1504767,no,no,1486911993.90403
Mechatronic Software Testing,"The paper describes mechatronic software testing techniques. Testing is different from common testing and includes special features of mechatronic systems. It may be put into effect with the aim of improving quality, assessing reliability, checking and conforming correctness. Various adapted techniques may be employed for the purpose, such as, for example, the white box technique or the black box technique when correctness testing, endurance and stress testing in relability testing or the usage of ready programs for performance testing.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376049,no,no,1486911993.900672
A novel high-capability control-flow checking technique for RISC architectures,"Nowadays more and more small transistors make microprocessors more susceptible to transient faults, and then induce control-flow errors. Software-based signature monitoring is widely used for control-flow error detection. When previous signature monitoring techniques are applied to RISC architectures, there exist some branch-errors that they can not detect. This paper proposes a novel software-based signature monitoring technique: CFC-End (Control-Flow Checking in the End). One property of CFC-End is that it uses two global registers for storing the run-time signature alternately. Another property of CFC-End is that it compares the run-time signature with the assigned signature in the end of every basic block. CFC-End is better than previous techniques in the sense that it can detect any single branch-error when applied to RISC architectures. CFC-End has similar performance overhead in comparison with the RCF (Region based Control-Flow checking) technique, which has the highest capability of branch-error detection among previous techniques.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595567,no,no,1486911993.90067
Self-Adaptive Systems for Information Survivability: PMOP and AWDRAT,"Information systems form the backbones of the critical infrastructures of modern societies. Unfortunately, these systems are highly vulnerable to attacks that can result in enormous damage. This paper describes two related systems PMOP and AWDRAT that were developed during the DARPA Self Regenerative Systems program. PMOP defends against insider attacks while AWDRAT is intended to detect compromises to software systems. Both rely on self-monitoring, diagnosis and self-adaptation. We describe both systems and show the results of experiments with each.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274925,no,no,1486911993.900669
Early Software Reliability Prediction Using Cause-effect Graphing Analysis,"Early prediction of software reliability can help organizations make informed decisions about corrective actions. To provide such early prediction, we propose practical methods to: 1) systematically identify defects in a software requirements specification document using a technique derived from cause-effect graphing analysis (CEGA); 2) assess the impact of these defects on software reliability using a recursive algorithm based on binary decision diagram (BDD) technique. Using a numerical example, we show how predicting software reliability at the requirement analysis stage could be greatly facilitated by the use of the method presented in this paper. The acronyms used throughout this paper are alphabetically listed as follows: ACEG-actually implemented cause effect graph; BCEG-benchmark cause effect graph; BDD-binary decision diagram; CEGA-cause effect graphing analysis; PACS-personal access control system; SRS-software requirements specification document",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4126345,no,no,1486911993.900668
A Junction Tree Propagation Algorithm for Bayesian Networks with Second-Order Uncertainties,"Bayesian networks (BNs) have been widely used as a model for knowledge representation and probabilistic inferences. However, the single probability representation of conditional dependencies has been proven to be over-constrained in realistic applications. Many efforts have proposed to represent the dependencies using probability intervals instead of single probabilities. In this paper, we move one step further and adopt a probability distribution schema. This results in a higher order representation of uncertainties in a BN. We formulate probabilistic inferences in this context and then propose a mean/covariance propagation algorithm based on the well-known junction tree propagation for standard BNs. For algorithm validation, we develop a two-layered Markov likelihood weighting approach that handles high-order uncertainties and provides """"ground-truth"""" solutions to inferences, albeit very slowly. Our experiments show that the mean/covariance propagation algorithm can efficiently produce high-quality solutions that compare favorably to results obtained through painstaking sampling",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031931,no,no,1486911993.900667
Automated processing of raw DNA sequence data,"Present-day DNA sequencing techniques have evolved considerably from their early beginnings. A modern sequencing project is essentially an assembly-line environment and is therefore improved and accelerated by the degree to which slow and error-prone manual steps can be replaced by reliable and accurate automatic ones. For hardware, this typically means expanding the use of robotics, for example, to execute the multitude of micro-volume fluid transfers that occur for each of the samples processed in a project. Likewise, automated software replaces manual processing and analysis steps for samples wherever possible. In this article, we focus on one particular aspect of software: the automated handling of raw DNA data. Specifically, we discuss a number of critical software algorithms and components and how they have been woven into a framework for largely hands-off processing of Human Genome Project data at the Genome Sequencing Center. These data represent about 25% of the total public human sequencing project.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=940044,no,no,1486911993.900666
A Measurement Based Dynamic Policy for Switched Processing Systems,"Switched processing systems (SPS) represent a canonical model for many areas of applications of communication, computer and manufacturing systems. They are characterized by flexible, interdependent service capabilities and multiple classes of job traffic flows. Recently, increased attention has been paid to the issue of improving quality of service (QoS) performance in terms of delays and backlogs of the associated scheduling policies, rather than simply maximizing the system's throughput. In this study, we investigate a measurement based dynamic service allocation policy that significantly improves performance with respect to delay metrics. The proposed policy solves a linear program at selected points in time that are in turn determined by a monitoring strategy that detects 'significant' changes in the intensities of the input processes. The proposed strategy is illustrated on a small SPS subject to different types of input traffic.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288728,no,no,1486911993.900665
PCMOS-based Hardware Implementation of Bayesian Network,"Bayesian network (K.B. Korb and E. Nicholson, 2004) has received considerable attention in a great variety of research areas such as for artificial intelligence, bioinformatics, medicine, engineering, image processing, and various kinds of decision support systems. But up till now, most of the investigation on Bayesian network has been on its theory, algorithms and software implementations. This paper presents the Bayesian network from a totally new perspective-hardware circuit implementation. By using the new-born technology of probabilistic CMOS (PCMOS) (K.V. Palem, 2005), (S. Cheemalavagu et al.), (S. Cheemalavagu et al., 2004), (P. Korkmaz, 2006) and taking advantage of the statistical properties of simple logic gates, the Bayesian network can be constructed using hardware circuits. Such hardware implementation revealed the advantages in aspects of power consumption, delay time and quality of randomness.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4450131,no,no,1486911993.900664
Gesture tracking and recognition for lecture video editing,"This paper presents a gesture based driven approach for video editing. Given a lecture video, we adopt novel approaches to automatically detect and synchronize its content with electronic slides. The gestures in each synchronized topic (or shot) are then tracked and recognized continuously. By registering shots and slides and recovering their transformation, the regions where the gestures take place can be known. Based on the recognized gestures and their registered positions, the information in slides can be seamlessly extracted, not only to assist video editing, but also to enhance the quality of original lecture video.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334682,no,no,1486911993.900663
A software methodology for detecting hardware faults in VLIW data paths,"The proposed methodology aims to achieve processor data paths for VLIW architectures able to autonomously detect transient and permanent hardware faults while executing their applications. The approach, carried out on the compiled application software, provides the introduction of additional instructions for controlling the correctness of the computation with respect to failures in one of the data path functional units. The advantage of a software approach to hardware fault detection is interesting because it allows one to apply it only to the critical applications executed on the VLIW architecture, thus not causing a delay in the execution of noncritical tasks. Furthermore, by exploiting the intrinsic redundancy of this class of architectures no hardware modification is required on the data path so that no processor customization is necessary.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260596,no,no,1486911993.900662
Analyzing Performance of Web-Based Metrics for Evaluating Reliability and Maintainability of Hypermedia Applications,"This paper has been designed to identify the Web metrics for evaluating the reliability and maintainability of hypermedia applications. In the age of information and communication technology (ICT), Web and the Internet, have brought significant changes in information technology (IT) and their related scenarios. Therefore in this paper an attempt has been made to trace out the Web-based measurements towards the creation of efficient Web centric applications. The dramatic increase in Web site development and their relative usage has led to the need of Web-based metrics. These metrics will accurately assess the efforts in the Web-based applications. Here we promote the simple, but elegant approaches to estimate the efforts needed for designing Web-based applications with the help of user behavior model graph (UBMG), Web page replacement algorithms, and RS Web Application Effort Assessment (RSWAEA) method. Effort assessment of hyperdocuments is crucial for Web-based systems, where outages can result in loss of revenue and dissatisfied customers. Here we advocate a simple, but elegant approach for effort estimation for Web applications from an empirical point of view. The proposed methods and models have been designed after carrying out an empirical study with the students of an advanced university class and Web designers that used various client-server based Web technologies. Our first aim was to compare the relative importance of each Web-based metric and method. Second, we also implemented the quality of the designs obtained based by constructing the User Behavior Model Graphs (UBMGs) to capture the reliability of Web-based applications. Thirdly, we use Web page replacement algorithms for increasing the Web site usability index, maintainability, reliability, and ranking. The results obtained from the above Web-based metrics can help us to analytically identify the effort assessment and failure points in Web-based systems and makes the evaluation of reliability of thes- - e systems simple.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4696136,no,no,1486911993.8973
Quality-Based Fusion of Multiple Video Sensors for Video Surveillance,"In this correspondence, we address the problem of fusing data for object tracking for video surveillance. The fusion process is dynamically regulated to take into account the performance of the sensors in detecting and tracking the targets. This is performed through a function that adjusts the measurement error covariance associated with the position information of each target according to the quality of its segmentation. In this manner, localization errors due to incorrect segmentation of the blobs are reduced thus improving tracking accuracy. Experimental results on video sequences of outdoor environments show the effectiveness of the proposed approach.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4267881,no,no,1486911993.897299
Assessing Web Applications Consistently: A Context Information Approach,"In order to assess Web applications in a more consistent way we have to deal not only with non-functional requirement specification, measurement and evaluation (M&E) information but also with the context information about the evaluation project. When organizations record the collected data from M&E projects, the context information is very often neglected. This can jeopardize the validity of comparisons among similar evaluation projects. We highlight this concern by introducing a quality in use assessment scenario. Then, we propose a solution by representing the context information as a new add-in to the INCAMI M&E framework. Finally, we show how context information can improve Web application evaluations, particularly, data analysis and recommendation processes.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4577886,no,no,1486911993.897297
VAASAANUBAADA: automatic machine translation of bilingual Bengali-Assamese news texts,"This paper presents a project for translating bilingual Bengali-Assamese news texts using an example-based machine translation technique. The work involves machine translation of bilingual texts at sentence level. In addition, the work also includes preprocessing and post-processing tasks. The work is unique because of the language pair that is chosen for experimentation. We constructed and aligned the bilingual corpus manually by feeding real examples using pseudo code. The longer input sentence is fragmented at punctuations, which resulted in high quality translation. Backtracking is used when an exact match is not found at the sentence/fragment level, leading to further fragmentation of the sentence. Since bilingual Bengali-Assamese languages belong to the Magadha Prakrit group, the grammatical form of sentences is very similar and has no lexical word groups. The results when tested are fascinating with quality translation.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1182307,no,no,1486911993.897296
An immune model and its application to a mobile robot simulator,"Immune computation is burgeoning bioinformatics technique inspired from the natural immune system and can solve the information security problems such as antivirus and fault detection. And the immune model is a crucial problem of the artificial immune system. In this paper, an immune model was proposed for the application of a mobile robot simulator, which was infected by some worms, such the love worm and the happy-time worm. The immune model was comprised of three tiers, including the inherent immune tier, the adaptive immune tier and the parallel computing tier. This immune model was built on the theories of the natural immune system and had many excellent features, such as adaptability, immunity, memory, learning, and robustness. And the application example of the immune model in the mobile robot simulator showed, the artificial immune system can detect, recognize, learn and eliminate computer viruses, and can detect and repair faults such as software bugs, and so the immune computation is an excellent approach for antivirus security. Moreover, the application fields and prospect of the immune computation would be rich and successful in the near future",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1708591,no,no,1486911993.897295
Analyzing medical processes,"This paper shows how software engineering technologies used to define and analyze complex software systems can also be effective in detecting defects in human-intensive processes used to administer healthcare. The work described here builds upon earlier work demonstrating that healthcare processes can be defined precisely. This paper describes how finite-state verification can be used to help find defects in such processes as well as find errors in the process definitions and property specifications. The paper includes a detailed example, based upon a real-world process for transfusing blood, where the process defects that were found led to improvements in the process.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814174,no,no,1486911993.897294
ACCE: Automatic correction of control-flow errors,"Detection of control-flow errors at the software level has been studied extensively in the literature. However, there has not been any published work that attempts to correct these errors. Low-cost correction of CFEs is important for real-time systems where checkpointing is too expensive or impossible. This paper presents automatic correction of control-flow errors (ACCE), an efficient error correction algorithm involving addition of redundant code to the program. ACCE has been implemented by modifying GCC, a widely used C compiler, and performance measurements show that the overhead is very low. Fault injection experiments on SPEC and MiBench benchmark programs compiled with ACCE show that the correct output is produced with high probability and that CFEs are corrected with a latency of a few hundred instructions.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437639,no,no,1486911993.897293
A model for long-term environmental sound detection,"Knowledge on primary processing of sound by the human auditory system has tremendously increased. This paper exploits the opportunities this creates for assessing the impact of (unwanted) environmental noise on quality of life of people. In particular the effect of auditory attention in a multisource context is focused on. The typical application envisaged here is characterized by very long term exposure (days) and multiple listeners (thousands) that need to be assessed. Therefore, the proposed model introduces many simplifications. The results obtained show that the approach is nevertheless capable of generating insight in the emergence of annoyance and the appraisal of open area soundscapes.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4634075,no,no,1486911993.897292
Active Authorization Rules for Enforcing RBAC with Spatial Characteristics,"The integration of the spatial dimension into RBAC-based models has been the hot topic as a consequence of the growing relevance of geo-spatial information in advanced GIS and mobile applications. Dynamically monitoring the state changes of an underlying system, detecting and reacting to changes without delay are crucial for the success of any access control enforcement mechanism. Thus, current systems or models should provide a flexible mechanism for enforcing RBAC with spatial characteristics in a seamless way, and adapt to policy or role structure changes in enterprises, which are indispensable to make RBAC with spatial characteristics usable in diverse domains. In this paper we will show how On-If-Then-Else authorization rules (or enhanced ECA rules) are used for enforcing RBAC with spatial characteristics in a seamless way. Large enterprises have hundreds of roles, which requires thousands of rules for providing access control, and generating these rules manually is error-prone and a cognitive-burden for non-computer specialists. Thus, in this paper, we will discuss briefly how these authorization rules can be automatically generated from high level specifications of enterprise access control policies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731703,no,no,1486911993.89729
An Approach to Separating Security Concerns in E-Commerce Systems at the Architecture Level,"Security is a requisite and vital concern that should be addressed in e-commerce systems. Traditionally, to add security properties to the application, developers had to specify when, where and how to apply what security policies manually. Such a process is often complicate and error-prone. This paper describes an aspect oriented approach to separating security and application concerns at the architecture level. In the approach, security and application concerns are specified in security aspect models and a base model separately. By specifying the crosscutting relationship between them, the two kinds of models are combined together through weaving. The weaving is based on process algebras and is automatic. Separating security aspects at the early stage of software development can promote maintainability and traceability of the system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4606167,no,no,1486911993.897289
Helmet-mounted display image quality evaluation system,"Helmet-mounted displays (HMDs) provide essential pilotage and fire control imagery information for pilots. To maintain system integrity and readiness, there is a need to develop an image quality evaluation system for HMDs. In earlier work, a framework was proposed for an HMD system called the integrated helmet and display sighting system (IHADSS), used with the U.S. Army's Apache helicopter. This paper describes prototype development and interface design and summarizes bench test findings using three IHADSS helmet display units (HDUs). The prototype consists of hardware (cameras, sensors, image capture/data acquisition cards, battery pack, HDU holder, moveable rack and handle, and computer) and software algorithms for image capture and analysis. Two cameras with different-size apertures are mounted in parallel on a rack facing an HDU holder. A handle allows users to position the HDU in front of the two cameras. The HMD test pattern is then captured. Sensors detect the position of the holder and whether the HDU is angled correctly in relation to the camera. Algorithms detect HDU features captured by the two cameras, including focus, orientation, displacement, field-of-view, and number of grayshades. Bench testing of three field-quality HDUs indicates that the image analysis algorithms are robust and able to detect the desired image features. Suggested future directions include development of a learning algorithm to automatically develop or revise feature specifications as the number of inspection samples increases.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1246558,no,undetermined,0
Quality of service provision assessment for campus network,"The paper presents a methodology for assessing the quality of service (QoS) provision for a campus network. The author utilizes the Staffordshire University's network communications infrastructure (SUNCI) as a testing platform and discusses a new approach and QoS provision, by adding a component of measurement to the existing model presented by J.L. Walker (see J. Services Marketing, vol.9, no.1, p.5-14, 1995). The QoS provision is assessed in light of users' perception compared with the network traffic measurements and online monitoring reports. The users' perception of the QoS provision of a telecommunications network infrastructure is critical to the successful business management operation of any organization. The computing environment in modern campus networks is complex, employing multiple heterogeneous hardware and software technologies. In support of highly interactive user applications, QoS provision is essential to the users' ever increasing level of expectations. The paper offers a cost effective approach to assessing the QoS provision within a campus network.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1249093,no,undetermined,0
A system for controlling software inspections,"Software inspections are a powerful tool for detecting faults in software during the early phases of the life cycle. Deciding when to stop inspections is an important determinant of inspection effectiveness. Capture-recapture (CR) models can be used to estimate defect content, and hence help to make a reinspection decision. We present Monte Carlo simulations of six CR models. The objective is to find the best CR model. This builds on previous work by simulating the context of high-reliability systems. The results indicate that model MtCh, which underestimates median relative error, gives zero failures and has the best decision accuracy.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1226148,no,undetermined,0
Evaluating four white-box test coverage methodologies,"This paper presents an illustrative study aimed at evaluating the effectiveness of four white-box test coverage techniques for software programs. In the study, an experimental design was considered which was used to evaluate the chosen testing techniques. The evaluation criteria were determined both in terms of the ability to detect faults and the number of test cases required. Faults were seeded artificially into the program and several faulty-versions of programs (mutants) were generated taking help of mutation operators. Test case execution and coverage measurements were done with the help of two testing tools, Cantata and OCT. Separate regression models relating coverage and effectiveness (fault detection ability and number of test cases required) are developed. These models can be helpful for determining test effectiveness when the coverage levels are known in a problem domain.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1226246,no,undetermined,0
Use of Data Mining to Enhance Security for SOA,"Service-oriented architecture (SOA) is an architectural paradigm for developing distributed applications so that their design is structured on loosely coupled services such as Web services. One of the most significant difficulties with developing SOA concerns its security challenges, since the responsibilities of SOA security are based on both the servers and the clients. In recent years, a lot of solutions have been implemented, such as the Web services security standards, including WS-Security and WS-SecurityPolicy. However, those standards are completely insufficient for the promising new generations of Web applications, such as Web 2.0 and its upgraded edition, Web 3.0. In this work, we are proposing an intelligent security service for SOA using data mining to predict the attacks that could arise with SOAP (Simple Object Access Protocol) messages. Moreover, this service will validate the new security policies before deploying them on the service provider side by testing the probability of their vulnerability.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4682084,no,undetermined,0
Next generation application integration: challenges and new approaches,"Integrating multiple heterogeneous data sources into applications is a time-consuming, costly and error-prone engineering task. Relatively mature technologies exist that make integration tractable from an engineering perspective. These technologies, however, have many limitations, and hence present opportunities for breakthrough research. This paper briefly describes some of these limitations, and enumerates a subset of the general open research problems. It then describes the Data Concierge research project and prototype that is attempting to provide solutions to some of these problems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245398,no,undetermined,0
Applications of service curve theory,"In this paper, we study well-known utilization-based admission control schemes which are derived by ad-hoc mathematical manipulations in previous literatures. We show that the same results can easily be achieved by using service curve theory (SCT). We also investigate the problem of providing statistical quality of service (QoS) in wireless network. With the same idea as the service curve theory, we can find an approximation formula to estimate drop probabilities in the Cruz (1995) and Agarwal et. al. (1999) delay system. In general, this analysis methodology is effective and can be used in the delay analysis of other applications.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1215856,no,undetermined,0
Quantifying architectural attributes,"Summary form only given. Traditional software metrics are inapplicable to software architectures, because they require information that is not available at the architectural level, and reflect attributes that are not meaningful at this level. We briefly present architecture-relevant quality attributes, then we introduce architecture-enabled quantitative functions, and run an experiment which shows how and to what extent the latter are correlated to (hence can be used to predict) the former.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227498,no,undetermined,0
ADOM: An Adaptive Objective Migration Strategy for Grid Platform,"Object migration is the movement of objects from one machine to another during execution. It can be used to enhance the efficiency and the reliability of grid systems, such as to balance load distribution, to enable fault resilience, to improve system administration, and to minimize communication overhead. Most existing schemes apply fixed object migration strategies, which are unadaptable to changing requirements of applications. In this paper,we address the issue of object migration for large scale grid system with multiple object levels.First, we devise a probabilistic object tree model and formulate the object migration problem as an optimization problem. Then we proposed an adaptive object migration algorithm called ADOM to solve the problem. the ADOM algorithm applies the breadth first search scheme to traversal the object tree and migrates object adaptively according to their access probability. Finally we evaluate the performance of different object migration algorithms in our grid platform, which show that the ADOM algorithm outperforms other algorithms under large object tree size.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662898,no,undetermined,0
An overview of configurable computing machines for software radio handsets,"The advent of software radios has brought a paradigm shift to radio design. A multimode handset with dynamic reconfigurability has the promise of integrated services and global roaming capabilities. However, most of the work to date has been focused on software radio base stations, which do not have as tight constraints on area and power as handsets. Base station software radio technology progressed dramatically with advances in system design, adaptive modulation and coding techniques, reconfigurable hardware, A/D converters, RF design, and rapid prototyping systems, and has helped bring software radio handsets a step closer to reality. However, supporting multimode radios on a small handset still remains a design challenge. A configurable computing machine, which is an optimized FPGA with application-specific capabilities, show promise for software radio handsets in optimizing hardware implementations for heterogeneous systems. In this article contemporary CCM architectures that allow dynamic hardware reconfiguration with maximum flexibility are reviewed and assessed. This is followed by design recommendations for CCM architectures for use in software radio handsets.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1215650,no,undetermined,0
Numerical simulation of low pressure die-cast of magnesium alloy wheel,"The simulation of low pressure die casting process of a magnesium alloy wheel is practiced. Using professional casting software, the temperature field during filling and solidification process is simulated, and then the potential defects are predicted and previewed. It is found that there will be shrinkage at the center of hub through analyzing. And this shrinkage can not be eliminated by reducing the pouring velocity. But installing cooling pipe system in the top mold alone is a valid way to enhance the cooling capacity at the areas near the center. The solidified order is adjusted by the cooling pipe system and the shrinkage in the center of the hub is eliminated.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4675457,no,undetermined,0
Introducing SW-based fault handling mechanisms to cope with EMI in embedded electronics: are they a good remedy?,"We summarize a study on the effectiveness of two software-based fault handling mechanisms in terms of detecting conducted electromagnetic interference (EMI) in microprocessors. One of these techniques deals with processor control flow checking. The second one is used to detect errors in code variables. In order to check the effectiveness of such techniques in RF ambient, an EIC 61.000-4-29 normative-compliant conducted RF-generator was implemented to inject spurious electromagnetic noise into the supply lines of a commercial off-the-shelf (COTS) microcontroller-based system. Experimental results suggest that the considered techniques present a good effectiveness to detect this type of faults, despite the multiple-fault injection nature of EMI in the processor control and data flows, which in most cases result in a complete system functional loss (the system must be reset).",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1214389,no,undetermined,0
Improving Chinese/English OCR performance by using MCE-based character-pair modeling and negative training,"In the past several years, we've been developing a high performance OCR engine for machine printed Chinese/ English documents. We have reported previously (1) how to use character modeling techniques based on MCE (minimum classification error) training to achieve the high recognition accuracy, and (2) how to use confidence-guided progressive search and fast match techniques to achieve the high recognition efficiency. In this paper, we present two more techniques that help reduce search errors and improve the robustness of our character recognizer. They are (1) to use MCE-trained character-pair models to avoid error-prone character-level segmentation for some trouble cases, and (2) to perform a MCE-based negative training to improve the rejection capability of the recognition models on the hypothesized garbage images during recognition process. The efficacy of the proposed techniques is confirmed by experiments in a benchmark test.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227690,no,undetermined,0
Yield analysis of compiler-based arrays of embedded SRAMs,This paper presents a detailed analysis of the yield of embedded static random access memories (eSRAM) which are generated using a compiler. Defect and fault analysis inclusive of industrial data are presented for these chips by taking into account the design constructs (referred to as kernels) and the physical properties of the layout. The new tool CAYA (Compiler-based Array Yield Analysis) is based on a characterization of the design process which accounts for fault types and the relation between functional and structural faults; a novel empirical model is proposed to facilitate the yield calculation. Industrial data is provided for the analysis of various configurations with different structures and redundancy. The effectiveness and accuracy as provided by CAYA are assessed with respect to industrial designs.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250089,no,undetermined,0
Dependability analysis of CAN networks: an emulation-based approach,"Today many safety-critical applications are based on distributed systems where several computing nodes exchange information via suitable network interconnections. An example of this class of applications is the automotive field, where developers are exploiting the CAN protocol for implementing the communication backbone. The capability of accurately evaluating the dependability properties of such a kind of systems is today a major concern. In this paper we present a new environment that can be fruitfully exploited to assess the effects of faults in CAN-based networks. The entire network is emulated via an ad-hoc hardware/software system that allows easily evaluating the effects of faults in all the network components, namely the network nodes, the protocol controllers and the transmission channel. In this paper, we report a detailed description of the environment we set-up and we present some preliminary results we gathered to assess the soundness of the proposed approach.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250153,no,undetermined,0
A segmentation method for bibliographic references by contextual tagging of fields,"In this paper, a method based on part-of-speech tagging (PoS) is used for bibliographic reference structure. This method operates on a roughly structured ASCII file, produced by OCR. Because of the heterogeneity of the reference structure, the method acts in a bottom-up way, without an a priori model, gathering structural elements from basic tags to sub-fields and fields. Significant tags are first grouped in homogeneous classes according to their grammar categories and then reduced in canonical forms corresponding to record fields: """"authors"""", """"title"""", """"conference name"""", """"date"""", etc. Non labelled tokens are integrated in one or another field by either applying PoS correction rules or using a structure model generated from well-detected records. The designed prototype operates with a great satisfaction on different record layouts and character recognition qualities. Without manual intervention, 96.6% words are correctly attributed, and about 75.9% references are completely segmented from 2500 references.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227694,no,undetermined,0
A Heuristic on Job Scheduling in Grid Computing Environment,"This paper introduces a model and a job scheduling algorithm in grid computing environments. In grid computing several applications require numerous resources for execution which are not often available for them, thus presence of a scheduling system to allocate resources to input jobs is vital. The resource selection criteria in the proposed algorithm are based on input jobs, communication links and resource computational capability. Then, the proposed algorithm will be assessed in simulated grid environment with statistical patterns of job insertion into system which each of them follow the normal, Poisson and exponential distribution. The results show that the new proposed algorithm has a better efficiency in comparison with the results obtained from other known algorithms.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662855,no,undetermined,0
Genetic programming-based decision trees for software quality classification,"The knowledge of the likely problematic areas of a software system is very useful for improving its overall quality. Based on such information, a more focused software testing and inspection plan can be devised. Decision trees are attractive for a software quality classification problem which predicts the quality of program modules in terms of risk-based classes. They provide a comprehensible classification model which can be directly interpreted by observing the tree-structure. A simultaneous optimization of the classification accuracy and the size of the decision tree is a difficult problem, and very few studies have addressed the issue. This paper presents an automated and simplified genetic programming (gp) based decision tree modeling technique for the software quality classification problem. Genetic programming is ideally suited for problems that require optimization of multiple criteria. The proposed technique is based on multi-objective optimization using strongly typed GP. In the context of an industrial high-assurance software system, two fitness functions are used for the optimization problem: one for minimizing the average weighted cost of misclassification, and one for controlling the size of the decision tree. The classification performances of the GP-based decision trees are compared with those based on standard GP, i.e., S-expression tree. It is shown that the GP-based decision tree technique yielded better classification models. As compared to other decision tree-based methods, such as C4.5, GP-based decision trees are more flexible and can allow optimization of performance objectives other than accuracy. Moreover, it provides a practical solution for building models in the presence of conflicting objectives, which is commonly observed in software development practice.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250214,no,undetermined,0
A scheme for dynamic detection of concurrent execution of object-oriented software,"Program testing is the most widely adopted approach for assuring the quality and reliability of software systems. Despite the popularity of the objected-oriented programs, its testing is much more challenging than that of the conventional programs. We proposed previously a methodology known as TACCLE for testing object-oriented software. It has not, however, addressed the aspects of concurrency and non-determinism. In this paper, we propose a scheme for dynamically detecting and testing concurrency in object-oriented software by executing selected concurrent pairs of operations. The scheme is based on OBJSA nets and addresses concurrency and nondeterminism problems. An experimental case study is reported to show the effectiveness of the scheme in detecting deadlocks, race conditions and other coherence problems. The scheme supplements our previous static approach to detecting deadlock in Java multithreaded programs.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245747,no,undetermined,0
An embryonic approach to reliable digital instrumentation based on evolvable hardware,"Embryonics encompasses the capability of self-repair and self-replication in systems. This paper presents a technique based on reconfigurable hardware coupled with a novel backpropagation algorithm for reconfiguration, together referred to as evolvable hardware (EHW), for ensuring reliability in digital instrumentation. The backpropagation evolution is much faster than genetic learning techniques. It uses the dynamic restructuring capabilities of EHW to detect faults in digital systems and reconfigures the hardware to repair or adapt to the error in real-time. An example application is presented of a robust BCD to a seven-segment decoder driving a digital display. The results obtained are quite interesting and promise quick and low cost embryonic schemes for reliability in digital instrumentation.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1246539,no,undetermined,0
Improvement of sensor accuracy in the case of a variable surface reflectance gradient for active laser range finders,"In active laser range finders, the computation of the (x, y, z) coordinates of each point of a scene can be performed using the detected centroid p~ of the image spot on the sensor. When the reflectance of the scene under analysis is uniform, the intensity profile of the image spot is a Gaussian and its centroid is correctly detected assuming an accurate peak position detector. However, when a change of reflectance occurs on the scene, the intensity profile of the image spot is no longer Gaussian. This change introduces a deviation p on the detected centroid p~, which will lead to erroneous (x, y, z) coordinates. This paper presents two heuristic models to improve the sensor accuracy in the case of a variable surface reflectance gradient. Simulation results are presented to show the quality of the correction and the resulting accuracy.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1246552,no,undetermined,0
Detection of errors in case hardening processes brought on by cooling lubricant residues,"Life cycle of case hardened steel work pieces depends on the quality of hardening. A large influencing factor on the quality of hardening is the cleanliness of the work pieces. In manufacturing a large amount of auxiliary materials such as cooling lubricants and drawing compounds are used to ensure correct execution of cutting and forming processes. Especially the residues of cooling lubricants are carried into following processes on the surfaces of the machined parts. Stable and controlled conditions cannot be guaranteed for these subsequent processes as the residues' influence on the process performance is known insufficiently, leading to a high uncertainty and consequently high expense factor. Therefore, information is needed about the type and amount of contamination. In practice the influence of these cooling lubricants on case hardening steels is a well-known phenomenon but correlation of the residue volume and resulting hardness are not known. A short overview of the techniques to detect cooling lubricant residues will be given in this paper and a method to detect the influence of the residues on the hardening process of case hardening steels will be shown. An example will be given for case hardening steel 16MnCr5 (1.7131). The medium of contamination is ARAL SAROL 470 EP.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1217209,no,undetermined,0
Nonscan design for testability for synchronous sequential circuits based on conflict resolution,"A testability measure called conflict, based on conflict analysis in the process of sequential circuit test generation is introduced to guide nonscan design for testability. The testability measure indicates the number of potential conflicts to occur or the number of clock cycles required to detect a fault. A new testability structure is proposed to insert control points by switching the extra inputs to primary inputs, using whichever extra inputs of all control points can be controlled by independent signals. The proposed design for testability approach is economical in delay, area, and pin overheads. The nonscan design for testability method based on the conflict measure can reduce many potential backtracks and make many hard-to-detect faults easy-to-detect; therefore, it can enhance actual testability of the circuit greatly. Extensive experimental results are presented to demonstrate the effectiveness of the method.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1223640,no,undetermined,0
An experimental comparison of usage-based and checklist-based reading,"Software quality can be defined as the customers' perception of how a system works. Inspection is a method to monitor and control the quality throughout the development cycle. Reading techniques applied to inspections help reviewers to stay focused on the important parts of an artifact when inspecting. However, many reading techniques focus on finding as many faults as possible, regardless of their importance. Usage-based reading helps reviewers to focus on the most important parts of a software artifact from a user's point of view. We present an experiment, which compares usage-based and checklist-based reading. The results show that reviewers applying usage-based reading are more efficient and effective in detecting the most critical faults from a user's point of view than reviewers using checklist-based reading. Usage-based reading may be preferable for software organizations that utilize or start utilizing use cases in their software development.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1223644,no,undetermined,0
Sequential Monte Carlo video text segmentation,This paper presents a probabilistic algorithm for segmenting and recognizing text embedded in video sequences. The algorithm approximates the posterior distribution of segmentation thresholds of video text by a set of weighted samples. After initialization the set of samples is recursively refined by random sampling under a temporal Bayesian framework. The proposed methodology allows us to estimate the optimal text segmentation parameters directly in function of the string recognition results instead of segmentation quality. Results on a database of 6944 images demonstrate the validity of the algorithm.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1247171,no,undetermined,0
Co-histogram and its application in remote sensing image compression evaluation,"Peak signal-to-noise ratio (PSNR) has found its application as an evaluation metric for image coding, but in many instances it provides an inaccurate representation of the image quality. The new tool proposed in this paper is called co-histogram, which is a statistic graph generated by counting the corresponding pixel pairs of two images. For image coding evaluation, the two images are the original image and a compressed and recovered image. The graph is a two-dimensional joint probability distribution of the two images. A co-histogram shows how the pixels are distributed among combinations of two image pixel values. By means of co-histogram, we can have a visual interpretation of PSNR, and the symmetry of a co-histogram is also significant for objective evaluation of remote sensing image compression. Our experiments with two SAR images and a TM image using DCT-based JPEG and wavelet-based SPIHT coding methods perform the importance of the co-histogram symmetry.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1247210,no,undetermined,0
Quality certification based on hierarchical classification of software packages,"With the advance of software and computer technology, COTS (Commercial-Off-The-Shelf) software is radically diversified and its application areas are also being extended. Because software quality evaluation is dependent upon types of software and environments where software is operated, certification organizations need different certification programs that are foundations or basic frames for certifying software. Although we can certify new software products by making new certification programs or referring to previous ones, both of them are somewhat ineffective methods. Therefore, we need to systematically generate certification programs to assess new software, and consider types of software and software environments at the same time. In this paper, we propose a meta model in order to systematically derive certification programs from previous ones. With this model, we can construct certification programs incrementally based on hierarchical classification of software packages. Furthermore, by generating certification programs with quality data on some certified software products, we validate the meta model.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222594,no,undetermined,0
Research on Enterprise Culture Maturity Evaluation Basing on KPA,"Based on the selection of the key process area ( KPA) in the development of enterprise culture, a model is created to assess the enterprise culture maturity in this thesis. This maturity model serves as a core guideline for the enterprise culture evaluation system, which is used to assess the dynamic process of the development of enterprise culture,and it also reveals the key areas and the key operable problems in the evaluation of enterprise culture.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4679861,no,undetermined,0
State-based power analysis for systems-on-chip,"Early power analysis for systems-on-chip (SoC) is crucial for determining the appropriate packaging and cost. This early analysis commonly relies on evaluating power formulas for all cores for multiple configurations of voltage, frequency, technology and application parameters, which is a tedious and error-prone process. This work presents a methodology and algorithms for automating the power analysis of SoCs. Given the power state machines for individual cores, this work defines the product power state machine for the whole SoC and uses formal symbolic simulation algorithms for traversing and computing the minimum and maximum power dissipated by sets of power states in the SoC.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1219096,no,undetermined,0
The Application of Improved Genetic Algorithm in Optimization of Function,"This paper points out defects of the traditional genetic algorithm (TGA), and has made improvement in it. An optimization strategy of combination is described. The improved genetic algorithm (IGA) is used to search the better answer in the whole feasible domain, and TGA is used to find the best answer in the local domain. The example shows the rationality and efficiency of this algorithm. This algorithm improves population diversity in the process of evolution, adapting bigger probability of crossover and mutation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4679184,no,undetermined,0
A scalable software-based self-test methodology for programmable processors,"Software-based self-test (SBST) is an emerging approach to address the challenges of high-quality, at-speed test for complex programmable processors and systems-on chips (SoCs) that contain them. While early work on SBST has proposed several promising ideas, many challenges remain in applying SBST to realistic embedded processors. We propose a systematic scalable methodology for SBST that automates several key steps. The proposed methodology consists of (i) identifying test program templates that are well suited for test delivery to each module within the processor, (ii) extracting input/output mapping functions that capture the controllability/observability constraints imposed by a test program template for a specific module-under-test, (iii) generating module-level tests by representing the input/output mapping functions as virtual constraint circuits, and (iv) automatic synthesis of a software self-test program from the module-level tests. We propose novel RTL simulation-based techniques for template ranking and selection, and techniques based on the theory of statistical regression for extraction of input/output mapping functions. An important advantage of the proposed techniques is their scalability, which is necessitated by the significant and growing complexity of embedded processors. To demonstrate the utility of the proposed methodology, we have applied it to a commercial state-of-the-art embedded processor (Xtensa form Tensilica Inc.). We believe this is the first practical demonstration of software-based self-test on a processor of such complexity. Experimental results demonstrate that software self-test programs generated using the proposed methodology are able to detect most (95.2%) of the functionally testable faults, and achieve significant simultaneous improvements in fault coverage and test length compared with conventional functional test.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1219068,no,undetermined,0
Random access using isolated regions,"Random access is a desirable feature in many video communication systems. Intra pictures is conventionally used as random access points, but correct picture content is recovered gradually within a range of pictures starting from a non-intra random access point. This paper proposes the use of the isolated regions technique for gradual decoder refresh and presents how the proposed method can be used in the upcoming ITU-T recommendation H.264, also known as MPEG-4 part 10 or advanced video coding. The presented simulations reveal that the proposed method outperforms intra-picture-based random access points in error-prone network conditions. It is also shown that the proposed method is more flexible and suits packet-based transmission better compared to progressively located intra-coded slices.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1247376,no,undetermined,0
Automatic communication refinement for system level design,"This paper presents a methodology and algorithms for automatic communication refinement. The communication refinement task in system-level synthesis transforms abstract data-transfer between components to its actual bus level implementation. The input model of the communication refinement is a set of concurrently executing components, communicating with each other through abstract communication channels. The refined model reflects the actual communication architecture. Choosing good communication architecture in system level design requires sufficient exploration through evaluation of various architectures. However, this would not be possible with manually refining the system model for each communication architecture. For one, manual refinement is tedious and error-prone. Secondly, it wastes substantial amount of precious designer time. We solve this problem with automatic model refinement. We also present a set of experimental results to demonstrate how the proposed approach works on a typical system level design.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1219013,no,undetermined,0
Configuration and management of a real-time smart transducer network,"Smart transducer technology supports the composability, configurability, and maintainability of sensor/actuator networks. The configuration and management of such networks, when carried out manually, is an expensive and error-prone task. Therefore, many existing fieldbus systems provide means of """"plug-and-play"""" that assist the user in these tasks. In this paper we describe configuration and management aspects in the area of dependable real-time fieldbus systems. We propose a configuration and management framework for a low-cost real-time fieldbus network. The framework uses formal XML descriptions to describe node and network properties in order to enable a data representation that comes with low overhead on nodes and enables the easy integration with software tools. The framework builds on the infrastructure and interfaces defined in the OMG smart transducers interface standard. As a case study, we have implemented a TTP/A configuration tool operating on a basic framework using the described concepts and mechanisms.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1247735,no,undetermined,0
The distributed diagnosis system of vehicles based on TH-OSEK embedded platform,"We proposed a fully-software distributed failure diagnosis system for vehicles based on the TH-OSEK real-time embedded OS platform we previously developed. The diagnosis system puts all the ECUs into a virtual logical ring and uses the MR(Maintain Ring) algorithm and OL(Off Line) algorithm to detect a faulted ECU and isolate it without destroying the structure of the logical ring. When a faulted ECU is recovered, with the proposed algorithms the system can also add it to the logical ring by updating the predecessor and successor of every node in the ring in time. The experiment result on the TH-OpenECU platform is also presented which shows that the system works well and usefulness for diagnosing the faults of vehicles.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677687,no,undetermined,0
Computer-aided coordination of power system protection,"In electrical power networks, faults can occur for various reasons. The task of the protection devices in electrical power systems (EPS) is to detect these faults and to eliminate the faulty parts of an EPS. In order to minimize the fault's consequences, different protection devices have to be coordinated. In this article, a computer-aided approach is described that should help project-planning engineers with this task and reduce the very time-consuming procedure of protection-strategy development and documentation preparation to a minimum. The main goal of the proposed concept is to make the tool as user-friendly as possible. The user communicates with a program system and device databases via a graphical user interface (GUI), which enables the visualization of a network. The clear graphical representation of the problem reduces the possibility of human errors. In this paper, the concept of the tool for overcurrent, overload and distance-protection coordination, its main features, and a typical example are presented.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1248197,no,undetermined,0
Reliable heterogeneous applications,"This paper explores the notion of computational resiliency to provide reliability in heterogeneous distributed applications. This notion provides both software fault-tolerance and the ability to tolerate information-warfare attacks. This technology seeks to strengthen a military mission, rather than to protect its network infrastructure using static defense measures such as network security, intrusion sensors, and firewalls. Even if a failure or attack is successful and never detected, it should be possible to continue information operations and achieve mission objectives. Computational resiliency involves the dynamic use of replicated software structures, guided by mission policy, to achieve reliable operation. However, it goes further to regenerate, automatically, replication in response to a failure or attack, allowing the level of system reliability to be restored and maintained. This paper examines a prototype concurrent programming technology to support computational resiliency in a heterogeneous distributed computing environment. The performance of the technology is explored through two example applications.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1248650,no,undetermined,0
Evaluating SEE: a benchmarking system for document page segmentation,"The decomposition of a document into segments such as text regions and graphics is a significant part of the document analysis process. The basic requirement for rating and improvement of page segmentation algorithms is systematic evaluation. The approaches known from the literature have the disadvantage that manually generated reference data (zoning ground truth) are needed for the evaluation task. The effort and cost of the creation of these data are very high. This paper describes the evaluation system SEE and presents an assessment of its quality. The system requires the OCR generated text and the original text of the document in correct reading order (text ground truth) as input. No manually generated zoning ground truth is needed. The implicit structure information that is contained in the text ground truth is used for the evaluation of the automatic zoning. Therefore, an assignment of the corresponding text regions in the text ground truth and those in the OCR generated text (matches) is sought. A fault tolerant string matching algorithm underlies a method, able to tolerate OCR errors in the text. The segmentation errors are determined as a result of the evaluation of the matching. Subsequently, the edit operations which are necessary for the correction of the recognized segmentation errors are computed to estimate the correction costs. Furthermore, SEE provides a version of the OCR generated text, which is corrected from the detected page segmentation errors.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227739,no,undetermined,0
Data Flow Testing of SQL-Based Active Database Applications,"The relevance of reactive capabilities as a unifying paradigm for handling a number of database features and applications is well-established. Active database systems have been used to implement the persistent data requirements of applications on several knowledge domains. They extend passive ones by automatically performing predefined actions in response to events that they monitor. These reactive abilities are generally expressed with active rules defined within the database itself. We investigate the use of data flow-based testing to identify the presence of faults in active rules written in SQL. The goal is to improve reliability and overall quality in this realm. Our contribution is the definition of a family of adequacy criteria, which require the coverage of inter-rule persistent data flow associations, and its effectiveness in various data flow analysis precisions. Both theoretical and empirical investigations show that the criteria have strong fault detecting ability at a polynomial complexity cost.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668113,no,undetermined,0
Information quality assessment of a yellow-pages location-based service,"The main problems in assessing the information quality of a particular kind of location-based service are described. Verifying the conformance of query results to the real world is identified as the most expensive problem. An approach from database quality assessment is adapted, by combining it with conventional software reliability testing, so that the verification can be replaced with two significantly easier test obligations. The resulting procedure should achieve a good compromise between the confidence level of the results and the testing effort. The approach seems also applicable to a broader range of systems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245360,no,undetermined,0
Usefulness and effectiveness of HW and SW protection mechanisms in a processor-based system,Fault-injection based dependability analysis has proved to be an efficient mean to predict the behavior of a circuit in presence of faults. Emulation-based approaches enable fast and flexible analyses of significant designs such as processors running significant application software. This paper presents the results obtained with an encryption application and questions the usefulness and the effectiveness of detection mechanisms in both hardware and software.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4674804,no,undetermined,0
HistoSketch: A Semi-Automatic Annotation Tool for Archival Documents,"This article describes a sketch-based framework for semi-automatic annotation of historical document collections. It is motivated by the fact that fully automatic methods, while helpful for extracting metadata from large collections, have two main drawbacks in a real-world application: (i) they are error-prone and (ii) they only capture a subset of all the knowledge in the document base, both meaning that manual intervention is always required. Therefore, we have developed a practical framework for allowing experts to extract knowledge from document collections in a sketch-based scenario. The main possibilities of the proposed framework are: (a) browsing the collection efficiently, (b) providing gestures for metadata input, (c) supporting handwritten notes and (d) providing gestures for launching automatic extraction processes such as OCR or word spotting.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670001,no,undetermined,0
Statistical analysis on a case study of load effect on PSD technique for induction motor broken rotor bar fault detection,Broken rotor bars in an induction motor create asymmetries and result in abnormal amplitude of the sidebands around the fundamental supply frequency and its harmonics. Monitoring the power spectral density (PSD) amplitudes of the motor currents at these frequencies can be used to detect the existence of broken rotor bar faults. This paper presents a study on an actual three-phase induction motor using the PSD analysis as a broken rotor bar fault detection technique. The distributions of PSD amplitudes of experimental healthy and faulty motor data sets at these specific frequencies are analyzed statistically under different load conditions. Results indicate that statistically significant conclusions on broken rotor bar detection can vary significantly under different load conditions and under different inspected frequencies. Detection performance in terms of the variation of PSD amplitudes is also investigated as a case study.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1234558,no,undetermined,0
Policy-guided software evolution,"Ensuring that software systems evolve in a desired manner has thus far been an elusive goal. In a continuing effort towards this objective, in this paper we propose a new approach that monitors an evolving software system, or its evolution process, against evolutionary policies so that any feedback obtained can be used to improve the system or its process. Two key concepts that make this possible are: (1) a mechanism to detect policy violations; and (2) a contextual framework to support activities of evolving a software system beyond the next release. Together, they could provide a wide and deep scope for managing software evolution. The benefit of our approach is that it would help in: sustaining the quality of a software system as it evolves; reducing evolutionary costs; and improving evolutionary processes.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235408,no,undetermined,0
Dynamic Web Service Selection for Reliable Web Service Composition,"This paper studies the dynamic web service selection problem in a failure-prone environment, which aims to determine a subset of Web services to be invoked at run-time so as to successfully orchestrate a composite web service. We observe that both the composite and constituent web services often constrain the sequences of invoking their operations and therefore propose to use finite state machine to model the permitted invocation sequences of Web service operations. We assign each state of execution an aggregated reliability to measure the probability that the given state will lead to successful execution in the context where each web service may fail with some probability. We show that the computation of aggregated reliabilities is equivalent to eigenvector computation and adopt the power method to efficiently derive aggregated reliabilities. In orchestrating a composite Web service, we propose two strategies to select Web services that are likely to successfully complete the execution of a given sequence of operations. A prototype that implements the proposed approach using BPEL for specifying the invocation order of a web service is developed and served as a testbed for comparing our proposed strategies and other baseline Web service selection strategies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4663050,no,undetermined,0
Automating construction project quality assurance with RFID- and mobile technologies,"The most critical part of a construction project is normally the construction elements that form the structure of the building. Concrete construction elements are often used as support structures and on the facade of the building, but they are vulnerable to manufacturing defects as well as being damaged on-site. Such defects can create a lot of additional costs to all parties involved in the project. Discovering and communicating about errors can be problematic as normally quality assurance is done manually and data is stored in paper format. Our research concentrated on automating the quality assurance of concrete elements. The task was approached by embedding RFID-tags in the elements enabling them to be identified wirelessly and associated with information in a data system. In field conditions users identify the construction elements with a mobile phone and interact electronically with the quality assurance system. As the information is created in digital form, the system can analyze it to detect errors and react automatically, notifying people responsible of faults. This paper presents the implementation of the system and discusses challenges and benefits.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669484,no,undetermined,0
Augmented reality for programming industrial robots,"Existing practice for programming robots involves teaching it a sequence of waypoints in addition to process-related events, which defines the complete robot path. The programming process is time consuming, error prone and, in most cases, requires several iterations before the program quality is acceptable. By introducing augmented reality technologies in this programming process, the operator gets instant real-time, visual feedback of a simulated process in relation to the real object, resulting in reduced programming time and increased quality of the resulting robot program. This paper presents a demonstrator of a standalone augmented reality pilot system allowing an operator to program robot waypoints and process specific events related to paint applications. During the programming sequence, the system presents visual feedback of the paint result for the operator, allowing him to inspect the process result before the robot has performed the actual task.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240739,no,undetermined,0
Usage of MCA8 software for improving reliability of electrical networks,"The MCA8 software application is described and applied to model task in this paper. This software application was developed for the purpose of the support of multi-criteria decision-making in the field of electrical power engineering. The MCA8 runs on Windows with .NET framework 2.0 and it is user-friendly. The MCA8 offers six methods of multi-criteria analysis (MCA) for solving multi-criteria decision-making tasks. These methods are WSA, IPA, TOPSIS, AGREPREF, CDA and PROMETHEE. We can use them for example for selecting the most suitable old devices in electrical networks, which we need to replace by new devices (Reclosers in this paper). The application of these remote-controlled devices causes acceleration in handling and thus shortening of duration of a fault in the network. This results in rising of probability of faultless service and thus the reliability of electrical energy supply.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4664333,no,undetermined,0
QuaTrace: a tool environment for (semi-) automatic impact analysis based on traces,"Cost estimation of changes to software systems is often inaccurate and implementation of changes is time consuming, cost intensive, and error prone. One reason for these problems is that relationships between documentation entities (e.g., between different requirements) are not documented at all or only incompletely. In this paper, we describe a constructive approach to support later changes to software systems. Our approach consists of a traceability technique and a supporting tool environment. The tracing approach describes which traces should be established in which way. The proposed tool environment supports the application of the guidelines in a concrete development context. The tool environment integrates two existing tools: a requirements management tool (i.e., RequisitePro) and a CASE tool (i.e., Rhapsody). Our approach allows traces to be established, analyzed, and maintained effectively and efficiently.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235427,no,undetermined,0
Low-cost on-line fault detection using control flow assertions,"A control flow fault occurs when a processor fetches and executes an incorrect next instruction. Executable assertions, i.e., special instructions that check some invariant properties of a program, provide a powerful and low-cost method for on-line detection of hardware-induced control flow faults. We propose a technique called ACFC (Assertions for Control Flow Checking) that assigns an execution parity to a basic block, and uses the parity bit to detect faults. Using a graph model of a program, we classify control flow faults into skip, re-execute and multi-path faults. We derive some necessary conditions for these faults to manifest themselves as execution parity errors. To force a control flow fault to excite a parity error, the target program is instrumented with additional instructions. Special assertions are inserted to detect such parity errors. We have a developed a preprocessor that takes a C program as input and inserts ACFC assertions automatically. We have implemented a software-based fault injection tool SFIG which takes advantage of the GNU debugger. Fault injection experiments show that ACFC incurs less performance overhead (around 47%) and memory overhead (around 30%) than previous techniques, with no significant loss in fault coverage.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1214380,no,undetermined,0
A framework for understanding conceptual changes in evolving source code,"As systems evolve, they become harder to understand because the implementation of concepts (e.g. business rules) becomes less coherent. To preserve source code comprehensibility, we need to be able to predict how this property will change. This would allow the construction of a tool to suggest what information should be added or clarified (e.g. in comments) to maintain the code's comprehensibility. We propose a framework to characterize types of concept change during evolution. It is derived from an empirical investigation of concept changes in evolving commercial COBOL II files. The framework describes transformations in the geometry and interpretation of regions of source code. We conclude by relating our observations to the types of maintenance performed and suggest how this work could be developed to provide methods for preserving code quality based on comprehensibility.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235453,no,undetermined,0
Automatic device configuration and data validation through mobile communication,"The introduction of personal computing and wireless communication technology provides an option for on site device software updating and data retrieving. This is especially true for any devices sitting in a remote site where computing network is not accessible. In many advanced computing systems, frequent software updating and configuration profiles refreshing are required. This is clumsy and error prone procedures when users are not familiar with the operating systems. Suppose all the necessary files and programs are predefined in a mobile computing device such as notebooks, PDAs, or even mobile phones. All necessary files and software can be transferred to the corresponding computing devices and PCs at remote sites through wireless communication links such as Bluetooth, infrared, general packet radio service (GPRS). This idea helps solve the initial installation cost of a communication network to a remote site.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235773,no,undetermined,0
A guaranteed quality of service wireless access scheme for CDMA networks,"Current wireless multimedia applications may require different quality-of-service (QoS) measures such as throughput, packet loss rate, delay, and delay jitter. In this paper, we propose an access scheme for CDMA networks that can provide absolute QoS guarantees for different service classes. The access scheme uses several M/D/1 queues, each representing a different service class, and allocates a transmission rate to each queue so as to satisfy the different QoS requirements. Operation in error-prone channels is enabled by a mechanism that compensates sessions, which experience poor channels. Analysis and simulation results are used to illustrate the viability of the access scheme.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1235836,no,undetermined,0
What test oracle should I use for effective GUI testing?,"Test designers widely believe that the overall effectiveness and cost of software testing depends largely on the type and number of test cases executed on the software. In this paper we show that the test oracle used during testing also contributes significantly to test effectiveness and cost. A test oracle is a mechanism that determines whether software executed correctly for a test case. We define a test oracle to contain two essential parts: oracle information that represents expected output; and an oracle procedure that compares the oracle information with the actual output. By varying the level of detail of oracle information and changing the oracle procedure, a test designer can create different types of test oracles. We design 11 types of test oracles and empirically compare them on four software systems. We seed faults in software to create 100 faulty versions, execute 600 test cases on each version, for all 11 types of oracles. In all, we report results of 660,000 test runs on software. We show (1) the time and space requirements of the oracles, (2) that faults are detected early in the testing process when using detailed oracle information and complex oracle procedures, although at a higher cost per test case, and (3) that employing expensive oracles results in detecting a large number of faults using relatively smaller number of test cases.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240304,no,undetermined,0
Tool-assisted unit test selection based on operational violations,"Unit testing, a common step in software development, presents a challenge. When produced manually, unit test suites are often insufficient to identify defects. The main alternative is to use one of a variety of automatic unit test generation tools: these are able to produce and execute a large number of test inputs that extensively exercise the unit under test. However, without a priori specifications, developers need to manually verify the outputs of these test executions, which is generally impractical. To reduce this cost, unit test selection techniques may be used to help select a subset of automatically generated test inputs. Then developers can verify their outputs, equip them with test oracles, and put them into the existing test suite. In this paper, we present the operational violation approach for unit test selection, a black-box approach without requiring a priori specifications. The approach dynamically generates operational abstractions from executions of the existing unit test suite. Any automatically generated tests violating the operational abstractions are identified as candidates for selection. In addition, these operational abstractions can guide test generation tools to produce better tests. To experiment dynamic approach, we integrated the use of Daikon (a dynamic invariant detection tool) and Jtest (a commercial Java unit testing tool). An experiment is conducted to assess this approach.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240293,no,undetermined,0
A Block-Structured Mining Approach to Support Process Discovery,"Deploying process-driven information systems is a time-consuming and error-prone. Constructing process models from scratch is a complicated time consuming task that often requires high expertise. And there are discrepancies between the actual workflow processes and the processes as perceived by the management. Therefore, techniques for discovering process models have been developed. Process mining just attempts to improve this by automatically generating a process model from sets of systems' executions (audit logs). In this paper, a block-structured process mining approach from audit logs to support process discovery is designed. Compare with other algorithms, the result of this approach is more visible and understanding of process model. This approach is used to a widely commercial tool for the visualization and analysis of process model.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666400,no,undetermined,0
Light-weight theorem proving for debugging and verifying units of code,"Software bugs are very difficult to detect even in small units of code. Several techniques to debug or prove correct such units are based on the generation of a set of formulae whose unsatisfiability reveals the presence of an error. These techniques assume the availability of a theorem prover capable of automatically discharging the resulting proof obligations. Building such a tool is a difficult, long, and error-prone activity. In this paper, we describe techniques to build provers which are highly automatic and flexible by combining state-of-the-art superposition theorem provers and BDDs. We report experimental results on formulae extracted from the debugging of C functions manipulating pointers showing that an implementation of our techniques can discharge proof obligations which cannot be handled by Simplify (the theorem prover used in the ESC/Java tool) and perform much better on others.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1236224,no,undetermined,0
The ModelCamera: a hand-held device for interactive modeling,"An important goal of automated modeling is to provide computer graphics applications with high quality models of complex real-world scenes. Prior systems have one or more of the following disadvantages: slow modeling pipeline, applicability restricted to small scenes, no direct color acquisition, and high cost. We describe a hand-held scene modeling device that operates at five frames per second and that costs $2,000. The device consists of a digital video camera with 16 laser pointers attached to it. As the operator scans the scene, the pointers cast blobs that are detected and triangulated to provide sparse, evenly spaced depth samples. The frames are registered and merged into an evolving model, which is rendered continually to provide immediate operator feedback.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240261,no,undetermined,0
Real time system-in-the-loop simulation of tactical networks,"Military mission success probability is closely related to careful planning of communication infrastructures for Command and Control Information Systems (C2IS). Over recent years, the designers of tactical networks have realized more and more need for using simulation tools in the process of designing networks with optimal performances, in regard to terrain conditions. One of the most demanding simulation problems is the modeling of protocols and devices; especially on the application layer, because the credibility of simulation results mainly depends on the quality of modeling. A new branch of communications network simulations has appeared for resolving these kinds of problems the - simulations with real communication devices in the simulation loop. Such simulations initiate real-time into the simulation process. The results of our research are aimed at simulation methodology. Using this system, military command personnel, can perform realistic training on the real C2IS equipment connected to the simulation tool, by modeled wireless links over a virtual terrain. In our research work, we used the OPNET Modeler simulation tool, with additional modules.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669461,no,undetermined,0
Using redundancies to find errors,"Programmers generally attempt to perform useful work. If they performed an action, it was because they believed it served some purpose. Redundant operations violate this belief. However, in the past, redundant operations have been typically regarded as minor cosmetic problems rather than serious errors. This paper demonstrates that, in fact, many redundancies are as serious as traditional hard errors (such as race conditions or pointer dereferences). We experimentally test this idea by writing and applying five redundancy checkers to a number of large open source projects, finding many errors. We then show that, even when redundancies are harmless, they strongly correlate with the presence of traditional hard errors. Finally, we show how flagging redundant operations gives a way to detect mistakes and omissions in specifications. For example, a locking specification that binds shared variables to their protecting locks can use redundancies to detect missing bindings by flagging critical sections that include no shared state.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237172,no,undetermined,0
Architectural-level risk analysis using UML,"Risk assessment is an essential part in managing software development. Performing risk assessment during the early development phases enhances resource allocation decisions. In order to improve the software development process and the quality of software products, we need to be able to build risk analysis models based on data that can be collected early in the development process. These models will help identify the high-risk components and connectors of the product architecture, so that remedial actions may be taken in order to control and optimize the development process and improve the quality of the product. In this paper, we present a risk assessment methodology which can be used in the early phases of the software life cycle. We use the Unified Modeling Language (UML) and commercial modeling environment Rational Rose Real Time (RoseRT) to obtain UML model statistics. First, for each component and connector in software architecture, a dynamic heuristic risk factor is obtained and severity is assessed based on hazard analysis. Then, a Markov model is constructed to obtain scenarios risk factors. The risk factors of use cases and the overall system risk factor are estimated using the scenarios risk factors. Within our methodology, we also identify critical components and connectors that would require careful analysis, design, implementation, and more testing effort. The risk assessment methodology is applied on a pacemaker case study.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237174,no,undetermined,0
Development of Functional Delay Tests,"With ever shrinking geometries, growing density and increasing clock rate of chips, delay testing is gaining more and more industry attention to maintain test quality for speed-related failures. The aim of this paper is to explore how functional delay tests constructed at algorithmic level detect transition faults at gate-level. Main attention was paid to investigation of the possibilities to improve the transition fault coverage using n-detection functional delay fault tests. The proposed functional delay test construction approaches allowed achieving 99% transition fault coverage which is acceptable even for manufacturing test.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669293,no,undetermined,0
An experimental evaluation of inspection and testing for detection of design faults,"The two most common strategies for verification and validation, inspection and testing, are in a controlled experiment evaluated in terms of their fault detection capabilities. These two techniques are in the previous work compared applied to code. In order to compare the efficiency and effectiveness of these techniques on a higher abstraction level than code, this experiment investigates inspection of design documents and testing of the corresponding program, to detect faults originating from the design document. Usage-based reading (UBR) and usage-based testing (UBT) were chosen for inspections and testing, respectively. These techniques provide similar aid to the reviewers as to the testers. The purpose of both fault detection techniques is to focus the inspection and testing from a user's viewpoint. The experiment was conducted with 51 Master's students in a two-factor blocked design; each student applied each technique once, each application on different versions of the same program. The two versions contained different sets of faults, including 13 and 14 faults, respectively. The general results from this study show that when the two groups of subjects are combined, the efficiency and effectiveness are significantly higher for usage-based reading and that testing tends to require more learning. Rework is not taken into account, thus the experiment indicates strong support for design inspection over testing.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1237976,no,undetermined,0
Randomized asynchronous consensus with imperfect communications,"We introduce a novel hybrid failure model, which facilitates an accurate and detailed analysis of round-based synchronous, partially synchronous and asynchronous distributed algorithms under both process and link failures. Granting every process in the system up to f<sub>l</sub> send and receive link failures (with f<sub>l</sub><sup>a</sup> arbitrary faulty ones among those) in every round, without being considered faulty, we show that the well-known randomized Byzantine agreement algorithm of (Srikanth & Toueg 1987) needs just n &ge; 4f<sub>l</sub> + 2ff<sub>l</sub><sup>a</sup>+ 3f<sub>a</sub> + 1 processes for coping with f<sub>a</sub> Byzantine faulty processes. The probability of disagreement after R iterations is only 2<sup>-R</sup>, which is the same as in the FLP model and thus much smaller than the lower bound 0(1/R) known for synchronous systems with lossy links. Moreover, we show that 2-stubborn links are sufficient for this algorithm. Hence, contrasting widespread belief, a perfect communications subsystem is not required for efficiently solving randomized Byzantine agreement.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238089,no,undetermined,0
Fault tolerance technology for autonomous decentralized database systems,"The Autonomous Decentralized Database System (ADDS) has been proposed in the background of e-business in respect to the dynamic and heterogeneous requirements of the users. With the rapid development of information technology, different companies in the field of e-business are supposed to cooperate in order to cope with the continuous changing demands of services in a dynamic market. In a diversified environment of service provision and service access, the ADDS provides flexibility to integrate heterogeneous and autonomous systems while assuring timeliness and high availability. A loosely-consistency management technology confers autonomy to each site for updating while maintaining the consistency of the whole system. Moreover, a background coordination technology, by utilizing a mobile agent, has been devised to permit the sites to coordinate and cooperate with each other while conferring the online property. The use of mobile agent, however, is critical and requires reliability with regard to mobile agent failures that may lead to bad response times and hence the availability of the system may lost. A fault tolerance technology is proposed in order that the system autonomously detect and recover the fault of the mobile agent due to a failure in a transmission link, site or bug in the software. The effectiveness of the proposition is shown by simulation.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238084,no,undetermined,0
Automatic evaluation of flickering sensitivity of fluorescent lamps caused by interharmonic voltages,"Recent studies have shown that fluorescent lamps are also prone to light flickering due to the increasing level of inter-harmonics in the power systems. This possibility and the levels of problematic inter-harmonics were confirmed through laboratory tests. These tests were manually conducted, and therefore were laborious and time consuming. It would be convenient to automate the testing by using the advanced features of data acquisition system to save time and manpower. This paper describes the development of an automated flicker measurement and testing system. It consists of a lighting booth, a programmable AC source, photo-sensing circuit, a data acquisition device and automated flicker measurement and testing software. It is developed in accordance to the IEC 61000-4-15 standard. Tests were later conducted on various types of compact fluorescent lamps, confirming their sensitivity to interharominc voltages.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668775,no,undetermined,0
The Copper Surface Defects Inspection System Based on Computer Vision,"The surface defects in copper strips severely affect the quality of copper. So detecting the surface defects in copper strip has great significance to improve the quality. This paper presents a copper strip surface inspection based on computer vision, which uses modularized frame of hardware and the software of image processing. The paper adopts a self-adaptive weight averaging filtering method to preprocess image, and uses the moment invariants to pick the characters of typical defects which eigenvector is identified with the RBF neural networks. Experiments show that the real-time method can effectively detect the copper strip surface defects in the production line.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4667196,no,undetermined,0
A reconfigurable Byzantine quorum approach for the Agile Store,"Quorum-based protocols can be used to manage data when it is replicated at multiple server nodes to improve availability and performance. If some server nodes can be compromised by a malicious adversary, Byzantine quorums must be used to ensure correct access to replicated data. This paper introduces reconfigurable Byzantine quorums, which allow various quorum protocol parameters to be adapted based on the behavior of compromised nodes and the performance needs of the system. We present a protocol that generalizes dynamic Byzantine quorums by allowing the system size to change as faulty servers are removed from the system, in addition to adapting the fault threshold. A new architecture and algorithm that provide the capability to detect and remove faulty servers are also described. Finally, simulation results are presented that demonstrate the benefits offered by our approach.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238071,no,undetermined,0
A formal experiment comparing extreme programming with traditional software construction,This paper describes an experiment carried out during the Spring/2002 academic semester with computer science students at the University of Sheffield. The aim of the experiment was to assess extreme programming and compare it with a traditional approach. With this purpose the students constructed software for real clients. We observed 20 teams working for 4 clients. Ten teams worked with extreme programming and ten with the traditional approach. In terms of quality and size teams working with extreme programming produced similar final products to traditional teams. The major implication for the current practice of traditional software engineering is that in spite of the absence of design and the presence of testing before coding the product obtained still has similar quality and size. The implication for extreme programming is the possibility of growth and maturation given the fact that it provided results that were as good as those from the traditional approach.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232877,no,undetermined,0
Accurate dependability analysis of CAN-based networked systems,"Computer-based systems where several nodes exchange information via suitable network interconnections are today exploited in many safety-critical applications, like those belonging to the automotive field. Accurate dependability analysis of such a kind of systems is thus a major concern for designers. In this paper, we present an environment we developed in order to assess the effects of faults in CAN-based networks. We developed an IP core implementing the CAN protocol controller, and we exploited it to set-up a network composed of several nodes. Thanks to the approach we adopted, we were able to assess via simulation-based fault injection the effects of faults both in the bus used to carry information and inside each CAN controller as well. In this paper, we report a detailed description of the environment we set-up and we present some preliminary results we gathered to assess the soundness of the proposed approach.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232850,no,undetermined,0
A consumer report on BDD packages,"BDD packages have matured to a state where they are often considered a commodity. Does this mean that all (publicly and commercially) available packages are equally good? Does this preclude any new developments? In this paper, we present a consumer report on 13 BDD packages and thereby try to answer these questions. We argue that there is a substantial spectrum in quality as measured by various metrics and we found that even the better packages do not always deploy the latest technology. We show how various design decisions underlying the studied packages exhibit themselves at the programming interface level, and we claim that this allows us to predict performance to a certain extent.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232832,no,undetermined,0
Effect of tilt angle variations in a halo implant on V<sub>th</sub> values for 0.14-m CMOS devices,"Sensitivity of critical transistor parameters to halo implant tilt angle for 0.14-m CMOS devices was investigated. V<sub>th</sub> sensitivity was found to be 3% per tilt degree. A tilt angle mismatch between two serial ion implanters used in manufacturing was detected by tracking V<sub>th</sub> performance for 0.14-m production lots. Even though individual implanters may be within tool specifications for tilt angle control (0.5 for our specific tool type), the relative mismatch could be as large as 1, and therefore, result in a V<sub>th</sub> mismatch of over 3% from nominal. The V<sub>th</sub> mismatch results are in qualitative agreement with simulation results using SUPREM and MEDICI software.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1243978,no,undetermined,0
RTGEN-an algorithm for automatic generation of reservation tables from architectural descriptions,"Reservation Tables (RTs) have long been used to detect conflicts between operations that simultaneously access the same architectural resource. Traditionally, these RTs have been specified explicitly by the designer. However, the increasing complexity of modern processors makes the manual specification of RTs cumbersome and error prone. Furthermore, manual specification of such conflict information is infeasible for supporting rapid architectural exploration. In this paper, we present an algorithm to automatically generate RTs from a high-level processor description with the goal of avoiding manual specification of RTs, resulting in more concise architectural specifications and also supporting faster turnaround time in design space exploration. We demonstrate the utility of our approach on a set of experiments using the TI C6201 very long instruction word digital signal processor and DLX processor architectures, and a suite of multimedia and scientific applications.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1229878,no,undetermined,0
Reasoning of fuzzy Causality Diagram with interval probability,"Causality diagram is a probabilistic reasoning method. Fuzzy set theory was introduced to develop causality diagram methodology after discussing the development and the restriction of conventional causality diagram. The application of causality diagram is extended to fuzzy field by introducing fuzzy set theory. Fuzzy causality diagram can overcome the shortcomings that it is difficult to gain the accurate probability of the event in conventional causality diagram. Interval numbers can express all kinds of fuzzy number. So it is necessary to dicuss the reasoning of fuzzy causality diagram with interval probability. Based on the interval number, operator, fuzzy conditional probability and the normalization method were discussed in this paper. Then two reasoning algorithm of single-value fuzzy causality diagram is proposed, some remarks about these algorithms are given. The result of numerical simulating of a subsystem in nuclear plant is coincident with the fact, and it shows the normalizing method is effective. The research shows that Interval Fuzzy causality diagram is so effective in fault analysis, and it is more flexible and adaptive than conventional method.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670925,no,undetermined,0
Business rule evolution and measures of business rule evolution,"There is an urgent industrial need to enforce the changes of business rules (BRs) to software systems quickly, reliably and economically. Unfortunately, evolving BRs in most existing software systems is both time-consuming and error-prone. In order to manage, control and improve BR evolution, it is necessary that the software evolution community comes to an understanding of the ways in which BRs are implemented and how BR evolution can be facilitated or hampered by the design of software systems. We suggest that new software metrics are needed to allow us to measure the characteristics of BR evolution and to help us to explore possible improvements in a systematic way. A suitable set of BR-related metrics help us to discover the root causes of the difficulties inherent in BR evolution, evaluate the success of proposed approaches to BR evolution and improve the BR evolution process as a whole.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231218,no,undetermined,0
An investigation into formatting and layout errors produced by blind word-processor users and an evaluation of prototype error prevention and correction techniques,"This paper presents the results of an investigation into tools to support blind authors in the creation and checking of word processed documents. Eighty-nine documents produced by 14 blind authors are analyzed to determine and classify common types of layout and formatting errors. Based on the survey result, two prototype tools were developed to assist blind authors in the creation of documents: a letter creation wizard, which is used before the document is produced; and a format/layout checker that detects errors and presents them to the author after the document has been created. The results of a limited evaluation of the tools by 11 blind computer users are presented. A survey of word processor usage by these users is also presented and indicates that: authors have concerns about the appearance of the documents that they produce; many blind authors fail to use word processor tools such as spell checkers, grammar checkers and templates; and a significant number of blind people rely on sighted help for document creation or checking. The paper concludes that document formatting and layout is a problem for blind authors and that tools should be able to assist.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231235,no,undetermined,0
E-MAGINE: the development of an evaluation method to assess groupware applications,"This paper describes the development of the evaluation method E-MAGINE. The aim of this evaluation method is to support groups to efficiently assess the groupware applications, which fit them best. The method focuses on knowledge sharing groups and follows a modular structure. E-MAGINE is based on the Contingency Perspective in order to structurally characterize groups in their context. Another main building block of the method forms the new ISO-norm for ICT tools, """"Quality in Use."""" The overall method comprises two main phases. The initial phase leads to a first level profile and provides an indication of possible mismatches between group and application. The formulation of this initial profile has the benefit of providing a clear guide for further decisions on what instruments should be applied in the final phase of the evaluation process. It is argued that E-MAGINE fulfills the demand for a more practical and efficient groupware evaluation approach.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231400,no,undetermined,0
Research on rotary dump health monitoring expert system based on causality diagram theory,"Causality diagram theory is a kind of uncertainty reasoning theory based on the belief network. It expresses the knowledge and causality relationship by diagrammatic form and direct causality intensity. Furthermore, it resolves the shortages of the belief network, and realizes a hybrid model which can process discrete and continuous variations. The theory of causality diagram model and the steps of causality diagram reasoning methodology are studied in this paper, and a model of rotary dump health monitoring expert system is proposed. In addition, this paper establishes the causality diagram of rotary dump and converts it to the causality tree. According to the causality tree of rotary dump, the causality diagram reasoning methodology composed of four steps is described. Finally, an application of rotary dump health monitoring expert system is shown, and the system performance analysis is discussed.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670773,no,undetermined,0
Automatic detection and diagnosis of faults in generated code for procedure calls,"In this paper, we present a compiler testing technique that closes the gap between existing compiler implementations and correct compilers. Using formal specifications of procedure-calling conventions, we have built a target-sensitive test suite generator that builds test cases for a specific aspect of compiler code generators: the procedure-calling sequence generator. By exercising compilers with these specification-derived target-specific test suites, our automated testing tool has exposed bugs in every compiler tested on the MIPS and one compiler on the SPARC. These compilers include some that have been in heavy use for many years. Once a fault has been detected, the system can often suggest the nature of the problem. The testing system is an invaluable tool for detecting, isolating, and correcting faults in today's compilers.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245304,no,undetermined,0
Object-oriented mutation to assess the quality of tests,The quality of a test suite can be measured using mutation analysis. Groups of OO mutation operators are proposed for testing object-oriented features. The OO operators applied to UML specification and C++ code are illustrated by an example. Experimental results demonstrate effectiveness of different mutation operators and the reduction of functional test suite.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231626,no,undetermined,0
Detection and recovery techniques for database corruption,"Increasingly, for extensibility and performance, special purpose application code is being integrated with database system code. Such application code has direct access to database system buffers, and as a result, the danger of data being corrupted due to inadvertent application writes is increased. Previously proposed hardware techniques to protect from corruption require system calls, and their performance depends on details of the hardware architecture. We investigate an alternative approach which uses codewords associated with regions of data to detect corruption and to prevent corrupted data from being used by subsequent transactions. We develop several such techniques which vary in the level of protection, space overhead, performance, and impact on concurrency. These techniques are implemented in the Dali main-memory storage manager, and the performance impact of each on normal processing is evaluated. Novel techniques are developed to recover when a transaction has read corrupted data caused by a bad write and gone on to write other data in the database. These techniques use limited and relatively low-cost logging of transaction reads to trace the corruption and may also prove useful when resolving problems caused by incorrect data entry and other logical errors.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232268,no,undetermined,0
Uncovering hidden contracts: the .NET example,"Software contracts take the form of routine preconditions, postconditions, and class invariants written into the program itself. The design by contract methodology uses such contracts for building each software element, an approach that is particularly appropriate for developing safety-critical software and reusable libraries. This methodology is a key design element of some existing libraries, especially the Eiffel Software development environment, which incorporates contract mechanisms in the programming language itself. Because the authors see the contract metaphor as inherent to quality software development, they undertook the work reported in the article as a sanity check to determine whether they see contracts everywhere simply because their development environment makes using them natural or whether contracts are intrinsically present, even when other designers don't express or even perceive them. They studied classes from the .NET collections library for implicit contracts and assessed improvements that might result from making them explicit.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1244535,no,undetermined,0
Strategies for software reuse: a principal component analysis of reuse practices,"This research investigates the premise that the likelihood of success of software reuse efforts may vary with the reuse strategy employed and, hence, potential reuse adopters must be able to understand reuse strategy alternatives and their implications. We use survey data collected from 71 software development groups to empirically develop a set of six dimensions that describe the practices employed in reuse programs. The study investigates the patterns in which these practices co-occur in the real world, demonstrating that the dimensions cluster into five distinct reuse strategies, each with a different potential for reuse success. The findings provide a means to classify reuse settings and assess their potential for success.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232287,no,undetermined,0
Online control design for QoS management,"In this paper we present an approach for QoS management that can be applied for a general class of real-time distributed computation systems. In this paper, the QoS adaptation problem is formulated based on a utility function that measures the relative performance of the system. A limited-horizon online supervisory controller is used for this purpose. The online controller explores a limited region of the state-space of the system at each time step and decides the best action accordingly. The feasibility and accuracy of the online algorithm can be assessed at design time.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1244265,no,undetermined,0
Automatic backdoor analysis with a network intrusion detection system and an integrated service checker,"We examine how a network intrusion detection system can be used as a trigger for service checking and reporting. This approach reduces the amount of false alerts (false positives) and raises the quality of the alert report. A sample data over the Christmas period of year 2002 is analyzed as an example and detection of unauthorized SSH servers used as the main application. Unauthorized interactive backdoors to a network belong to the most dangerous class of intrusions (D. Zamboni et al., 1998). These backdoors are usually installed by root-kits, to hide the system compromise activity. They are a gateway to launch exploits, gain super-user access to hosts in the internal network and use the attacked network as a stepping stone to attack other networks. In this research, we have developed software and done statistical analysis to assess and prevent such situations.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232410,no,undetermined,0
Automatically Determining Compatibility of Evolving Services,"A major advantage of Service-Oriented Architectures (SOA) is composition and coordination of loosely coupled services. Because the development lifecycles of services and clients are decoupled, multiple service versions have to be maintained to continue supporting older clients. Typically versions are managed within the SOA by updating service descriptions using conventions on version numbers and namespaces. In all cases, the compatibility among services description must be evaluated, which can be hard, error-prone and costly if performed manually, particularly for complex descriptions. In this paper, we describe a method to automatically determine when two service descriptions are backward compatible. We then describe a case study to illustrate how we leveraged version compatibility information in a SOA environment and present initial performance overheads of doing so. By automatically exploring compatibility information, a) service developers can assess the impact of proposed changes; b) proper versioning requirements can be put in client implementations guaranteeing that incompatibilities will not occur during run-time; and c) messages exchanged in the SOA can be validated to ensure that only expected messages or compatible ones are exchanged.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670172,no,undetermined,0
When can we test less?,"When it is impractical to rigorously assess all parts of complex systems, test engineers use defect detectors to focus their limited resources. We define some properties of an ideal defect detector and assess different methods of generating one. In the case study presented here, traditional methods of generating such detectors (e.g. reusing detectors from the literature, linear regression, model trees) were found to be inferior to those found via a PACE analysis.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232459,no,undetermined,0
A Brief History of Software Technology,"To mark IEEE Software's 25th anniversary, Software Technology column editor Christ of Ebert presents a review and road map of major software technologies, starting with the magazine's inauguration, 1984. Learning from the many hypes and often long introduction cycles, he provides some timeless principles of technology evaluation and introduction. Good car drivers assess situations past, present, and future with a mix of skills and qualities. They make unconscious decisions and meld impressions, experiences, and skills into appropriate real-time actions. The same holds for assessing software technology. When reflecting on which technologies have had the most impact in the past 25 years, we can assess it quantitatively, by looking at research papers or """"hype-cycle"""" duration, for example. Alternatively, we might judge it like the expert driver, intuitively evaluating what was achieved compared to what was promised from a user perspective. Of course, many major technology breakthroughs happened before 1984: Milestones such as the IBM OS/360 and the microprocessor, and even many still-relevant software engineering practices, had been developed much earlier.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670707,no,undetermined,0
Accounting for false indication in a Bayesian diagnostics framework,"Accounting for the effects of test uncertainty is a significant problem in test and diagnosis. Specifically, assessment of the level of uncertainty and subsequent utilization of that assessment to improve diagnostics must be addressed. One approach, based on measurement science, is to treat the probability of a false indication (false alarm or missed detection) as the measure of uncertainty. Given the ability to determine such probabilities, a Bayesian approach to diagnosis suggests itself. In the paper, we present a mathematical derivation for false indication and apply it to the specification of Bayesian diagnosis. We draw from measurement science, reliability theory, and the theory of Bayesian networks to provide an end-to-end probabilistic treatment of the fault diagnosis problem.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1243587,no,undetermined,0
An enhanced passive testing tool for network protocols,We study passive testing on protocols to detect faults in network devices. An enhanced passive testing tool is developed using integer linear programming in determining the ranges of the variables. On-line pruning reveals the current configuration of the system and the transition covered. Network system monitoring is conducted in a formal and fine-granularity way.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1243103,no,undetermined,0
A Method for Layout Evaluation of Online Handwritten Chinese Character Quality Based on Template,"Evaluation of Chinese handwriting character quality is an important function of computer assisted Chinese learning technology it can point out the errors in a handwritten character and make objective assessment on the writing quality of the character. However, only a few studies were reported on this new research topic in the literature. In this paper, the main target of handwriting evaluation has been presented. The common layout errors in handwriting samples are summarized and then a new layout evaluation method is proposed. The method is consisted of three parts of stroke layout evaluation, component layout evaluation and entire character shape evaluation, through the application of nine assessment rules. The experiment results show that the method is capable for detecting layout errors of handwriting samples and making objective assessment on whether a character is written good or not.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4663028,no,undetermined,0
Fuzzy critical analysis for an electric generator protection system,"The paper explains fuzzy critical analysis for the electric generator (EG) protection system. Power system electric generator (EG) is protected to various types of faults and abnormal workings. The protection system (PS) is composed of waiting subsystems, which must properly respond to each kind of dangerous events. An original fuzzy logic- system enables us to analyze the qualitative evaluation of the event-tree, modeling PS behavior. Fuzzy - set logic is used to account for imprecision and uncertainty in data while employing event-tree analysis. The fuzzy event-tree logic allows the use of verbal statement for the probabilities and consequences, such as very high, moderate and low probability.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670403,no,undetermined,0
Analyzing the cost and benefit of pair programming,"We use a combination of metrics to understand, model, and evaluate the impact of pair programming on software development. Pair programming is a core technique in the hot process paradigm of extreme programming. At the expense of increased personnel cost, pair programming aims at increasing both the team productivity and the code quality as compared to conventional development. In order to evaluate pair programming, we use metrics from three different categories: process metrics such as the pair speed advantage of pair programming; product metrics such as the module breakdown structure of the software; and project context metrics such as the market pressure. The pair speed advantage is a metric tailored to pair programming and measures how much faster a pair of programmers completes programming tasks as compared to a single developer. We integrate the various metrics using an economic model for the business value of a development project. The model is based on the standard concept of net present value. If the market pressure is strong, the faster time to market of pair programming can balance the increased personnel cost. For a realistic sample project, we analyze the complex interplay between the various metrics integrated in our model. We study for which combinations of the market pressure and pair speed advantage the value of the pair programming project exceeds the value of the corresponding conventional project. When time to market is the decisive factor and programmer pairs are much faster than single developers, pair programming can increase the value of a project, but there also are realistic scenarios where the opposite is true. Such results clearly show that we must consider metrics from different categories in combination to assess the cost-benefit relation of pair programming.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232465,no,undetermined,0
WebVizOr: A Visualization Tool for Applying Automated Oracles and Analyzing Test Results of Web Applications,"Web applications are used extensively for a variety of critical purposes and, therefore, must be reliable. Since Web applications often contain large amounts of code and frequently undergo maintenance, testers need automated tools to execute large numbers of test cases to determine if an application is behaving correctly. Evaluating the voluminous output-typically Web pages full of content-is tedious and error-prone. To ease the difficulty, testers can apply automated oracles, which have tradeoffs in false positives and false negatives. In this paper, we present the design, implementation, and evaluation of WebVizOr, a tool that aids testers byapplying a set of oracles to the output from test cases and highlighting the symptoms of possible faults. Using WebVizOr, a tester can compare the test results from several executions of a test case and can more easily determine if a test case exposes a fault.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670307,no,undetermined,0
Using service utilization metrics to assess the structure of product line architectures,"Metrics have long been used to measure and evaluate software products and processes. Many metrics have been developed that have lead to different degrees of success. Software architecture is a discipline in which few metrics have been applied, a surprising fact given the critical role of software architecture in software development. Software product line architectures represent one area of software architecture in which we believe metrics can be of especially great use. The critical importance of the structure defined by a product line architecture requires that its properties be meaningfully assessed and that informed architectural decisions be made to guide its evolution. To begin addressing this issue, we have developed a class of closely related metrics that specifically target product line architectures. The metrics are based on the concept of service utilization and explicitly take into account the context in which individual architectural elements are placed. We define the metrics, illustrate their use, and evaluate their strengths and weaknesses through their application on three example product line architectures.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1232476,no,undetermined,0
Static test compaction for multiple full-scan circuits,"Current design methodologies and methodologies for reducing test data volume and test application time for full-scan circuits allow testing of multiple circuits (or subcircuits of the same circuit) simultaneously using the same test data. We describe a static compaction procedure that accepts test sets generated independently for multiple full-scan circuits, and produces a compact test set that detects all the faults detected by the individual test sets. The resulting test set can be used for testing the circuits simultaneously using the same test data. This procedure provides an alternative to test generation procedures that perform test generation for complex circuits made up of multiple circuits. Such procedures also reduce the amount of test data and test application time required for testing all the circuits by testing them simultaneously using the same test data. However, they require consideration of a more complex circuit.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240926,no,undetermined,0
Improving Fault Injection of Soft Errors Using Program Dependencies,"Research has shown that modern micro-architectures are vulnerable to soft errors, i.e., temporary errors caused by voltage spikes produced by cosmic radiation. Soft-error impact is usually evaluated using fault injection, a black-box testing approach similar to mutation testing. In this paper, we complement an existing evaluation of a prototype brake-by-wire controller, developed by Volvo Technology, with static-analysis techniques to improve test effectiveness. The fault-injection tests are both time- and data-intensive, which renders their qualitative and quantitative assessment difficult. We devise a prototype visualization tool, which groups experiments by injection point and provides an overview of both instruction and fault coverage, and the ability to detect patterns and anomalies. We use the program-dependence graph to identify experiments with a priori known outcome, and implement a static analysis to reduce the test volume. The existing pre-injection heuristic is extended with liveness analysis to enable an unbiased fault-to-failure probability.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670305,no,undetermined,0
Building Profit-Aware Service-Oriented Business Applications,"Service composition is becoming a prevalent way to building service-oriented business applications (SOBAs). In an open service environment, the profit of composition (PoC) is a primary concern of building such applications. How to improve the PoC is a significant issue in developing SOBAs but was largely overlooked by current research. Particularly, the modeling and prediction of PoC should play a key role to drive the composition process. In this paper, we focus on how to model and predict PoC in SOBAs. We regard the PoC of a composite service as a function of the quality of service (QoS) attributes, defined in the service level agreement (SLA) between the service and its external partners. Based on the PoC prediction approach, we further propose a profit-driven composition methodology to assist enterprises to make more profit in their SOBAs.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670212,no,undetermined,0
Tolerance of control-flow testing criteria,"Effectiveness of testing criteria is the ability to detect failure in a software program. We consider not only effectiveness of some testing criterion in itself but a variance of effectiveness of different test sets satisfied the same testing criterion. We name this property """"tolerance"""" of a testing criterion and show that, for practical using a criterion, a high tolerance is as well important as high effectiveness. The results of empirical evaluation of tolerance for different criteria, types of faults and decisions are presented. As well as quite simple and well-known control-flow criteria, we study more complicated criteria: full predicate coverage, modified condition/decision coverage and reinforced condition/decision coverage criteria.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245339,no,undetermined,0
Software Tool for Real Time Power Quality Disturbance Analysis and Classification,"Real time detection and classification of power quality disturbances is important for quick diagnosis and mitigation of such disturbances. This paper presents the development of a software tool based on MatLab for power quality disturbance analysis and classification. Prior to the development of the software tool, the disturbance signals are captured and processed in real-time using the TMS320C6711DSP starter kit. Digital signal processor is used to provide fast data capture, fast data processing and signal processing flexibility with increased system performance and reduced system cost. The developed software tool can be used for real-time and off-line disturbance analysis by displaying the detected disturbance, % harmonics of a signal, total harmonic distortion and results of the S-transform, fast Fourier transform and continuous wavelet transform analyses. In addition, graphical representation of the input signal, power quality indices including sag and swell magnitudes are also displayed on the graphical user interface. PQ disturbance classification results show that accurate disturbance classification can be obtained with a total % of correct classification of 99.3%. Such a software tool can serve as a simple and reliable means for PQ disturbance detection and classification.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4451390,no,undetermined,0
A class of random multiple bits in a byte error correcting and single byte error detecting (S<sub>t</sub>b/EC-S<sub>b</sub>ED) codes,"Correcting multiple random bit errors that corrupt a single DRAM chip becomes very important in certain applications, such as semiconductor memories used in computer and communication systems, mobile systems, aircraft, and satellites. This is because, in these applications, the presence of strong electromagnetic waves in the environment or the bombardment of an energetic particle on a DRAM chip is highly likely to upset more than just one bit stored in that chip. On the other hand, entire chip failures are often presumed to be less likely events and, in most applications, detection of errors caused by single chip failures are preferred to correction due to check bit length considerations. Under this situation, codes capable of correcting random multiple bit errors that are confined to a single chip output and simultaneously detecting errors caused by single chip failures are attractive for application in high speed memory systems. This paper proposes a class of codes called Single t/b-error Correcting-Single b-bit byte Error Detecting (S<sub>t</sub>b/EC-S<sub>b</sub>ED) codes which have the capability of correcting random t-bit errors occurring within a single b-bit byte and simultaneously indicating single b-bit byte errors. For the practical case where the chip data output is 8 bits, i.e., b = 8, the S<sub>3</sub>8/EC-S<sub>8</sub>ED code proposed in this paper, for example, requires only 12 check bits at information length 64 bits. Furthermore, this S<sub>3</sub>8/EC-S<sub>8</sub>ED code is capable of correcting errors caused by single subarray data faults, i.e., single 4-bit byte errors, as well. This paper also shows that perfect S<sub>(b-t)</sub>b/EC-S<sub>b</sub>ED codes, i.e., perfect S<sub>t</sub>b/EC-S<sub>b</sub>ED codes for the case where t = b - 1, do exist and provides a theorem to construct these codes.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1214333,no,undetermined,0
API Fuzz Testing for Security of Libraries in Windows Systems: From Faults To Vulnerabilites,"Application programming interface (API) fuzz testing is used to insert unexpected data into the parameters of functions and to monitor for resulting program errors or exceptions in order to test the security of APIs. However, vulnerabilities through which a user cannot insert data into API parameters are not security threats, because attackers cannot exploit such vulnerabilities. In this paper, we propose a methodology that can automatically find paths between inputs of programs and faulty APIs. Where such paths exist, faults in APIs represent security threats. We call our methodology Automated Windows API Fuzz Testing II (AWAFTII). This method extends our previous research for performing API fuzz testing into the AWAFTII process. The AWAFTII process consists of finding faults using API fuzz testing, analyzing those faults, and searching for input data related to parameters of APIs with faults. We implemented a practical tool for AWAFTII and applied it to programs in the system folder of Windows XP SP2. Experimental results show that AWAFTII can detect paths between input of programs and APIs with faults.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4682305,no,undetermined,0
Making the (business) case for software reliability,"A business case can be developed to justify the use of higher reliability software to senior management based on the potential profits and improved market position associated with improved software development processes that use software process improvement (SPI) techniques, Results from the literature that demonstrate a positive return on investment (ROI) resulting from SPI program initiatives suggest that the highest potential ROI comes from -SPJ initiatives aimed at the earliest stages of software development. Established analytical reliability techniques are well-suited to supporting development of inherently reliable software early in the life cycle. A spreadsheet-based model developed by the Data and Analysis Center for Software (DACS) to assess the ROI, as well as secondary benefits and risks resulting from various types and levels of SPI, demonstrates the relationship of improved software reliability to the financial bottom line. Future capabijities of the DACS model will leverage new data to expand and refresh the existing model",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981656,no,undetermined,0
Trace Normalization,"Identifying truly distinct traces is crucial for the performance of many dynamic analysis activities. For example, given a set of traces associated with a program failure, identifying a subset of unique traces can reduce the debugging effort by producing a smaller set of candidate fault locations. The process of identifying unique traces, however, is subject to the presence of irrelevant variations in the sequence of trace events, which can make a trace appear unique when it is not. In this paper we present an approach to reduce inconsequential and potentially detrimental trace variations. The approach decomposes traces into segments on which irrelevant variations caused by event ordering or repetition can be identified, and then used to normalize the traces in the pool. The approach is investigated on two well-known client dynamic analyses by replicating the conditions under which they were originally assessed, revealing that the clients can deliver more precise results with the normalized traces.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700311,no,undetermined,0
Modeling the economics of testing: a DFT perspective,"Decision-makers typically make test tradeoffs using models that mainly represent direct costs such as test generation time and tester use. Analyzing a test strategy's impact on other significant factors such as test quality and yield learning requires an understanding of the dynamic nature of the interdomain dependencies of test, manufacturing, and design. Our research centers on modeling the tradeoffs between these domains. To answer the DFT question, we developed the Carnegie Mellon University Test Cost Model, a DFT cost-benefit model, derived inputs to the model for various IC cases with different assumptions about volume, yield, chip size, test attributes, and so forth; and studied DFT's impact on these cases. We used the model to determine the domains for which DFT is beneficial and for which DFT should not be used. The model is a composite of simple cause-and-effect relationships derived from published research. It incorporates many factors affecting test cost, but we don't consider it a complete model. Our purpose is to illustrate the necessity of using such models in assessing the effectiveness of various test strategies",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=980051,no,undetermined,0
Test generation and testability alternatives exploration of critical algorithms for embedded applications,"Presents an analysis of the behavioral descriptions of embedded systems to generate behavioral test patterns that are used to perform the exploration of design alternatives based on testability. In this way, during the hardware/software partitioning of the embedded system, testability aspects can be considered. This paper presents an innovative error model for algorithmic (behavioral) descriptions, which allows for the generation of behavioral test patterns. They are converted into gate-level test sequences by using more-or-less accurate procedures based on scheduling information or both scheduling and allocation information. The paper shows, experimentally, that such converted gate-level test sequences provide a very high stuck-at fault coverage when applied to different gate-level implementations of the given behavioral specification. For this reason, our behavioral test patterns can be used to explore testability alternatives, by simply performing fault simulation at the gate level with the same set of patterns, without regenerating them for each circuit. Furthermore, whenever gate-level ATPGs are applied on the synthesized gate-level circuits, they obtain lower fault coverage with respect to our behavioral test patterns, in particular when considering circuits with hard-to-detect faults",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=980008,no,undetermined,0
ED<sup>4</sup>I: error detection by diverse data and duplicated instructions,"Errors in computing systems can cause abnormal behavior and degrade data integrity and system availability. Errors should be avoided especially in embedded systems for critical applications. However, as the trend in VLSI technologies has been toward smaller feature sizes, lower supply voltages and higher frequencies, there is a growing concern about temporary errors as well as permanent errors in embedded systems; thus, it is very essential to detect those errors. Software-implemented hardware fault tolerance (SIHFT) is a low-cost alternative to hardware fault-tolerance techniques for embedded processors: It does not require any hardware modification of commercial off-the-shelf (COTS) processors. ED<sup>4</sup>I (error detection by data diversity and duplicated instructions) is a SIHFT technique that detects both permanent and temporary errors by executing two """"different"""" programs (with the same functionality) and comparing their outputs. ED<sup>4</sup>I maps each number, x, in the original program into a new number x', and then transforms the program so that it operates on the new numbers so that the results can be mapped backwards for comparison with the results of the original program. The mapping in the transformation of ED<sup>4</sup>I is x' = kx for integer numbers, where k<sub>f </sub> determines the fault detection probability and data integrity of the system. For floating-point numbers, we find a value of k<sub>f</sub> for the fraction and k<sub>e</sub> for the exponent separately, and use k = k<sub>f</sub>2<sup>k</sup> for the value of k. We have demonstrated how to choose an optimal value of k for the transformation. This paper shows that, for integer programs, the transformation with k = -2 was the most desirable choice in six out of seven benchmark programs we simulated. It maximizes the fault detection probability under the condition that the data integrity is highest",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=980007,no,undetermined,0
DB2 and Web services,"The World Wide Web offers a tremendous amount of information. Accessing and integrating the available information is a challenge. Screen scraping and reverse template engineering are manual and error-prone integration techniques from the past. The advent of Simple Object Access Protocol (SOAP) from the World Wide Web Consortium (W3C) allowed Web sites to become programmable Web services. W3C SOAP is a lightweight protocol, based on Extensible Markup Language (XML), that provides a service-oriented architecture for applications on the Web. Clients compose requests and send SOAP envelopes to providers, who reply through SOAP responses. In this paper, we describe DB2 and Web services, with techniques for integrating information from multiple Web service providers and exposing the collective information through Web services.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5386892,no,undetermined,0
Modeling nonlinear loads for aerospace power systems,"More and more electronic equipment is being added to airplane electrical systems. The addition of these nonlinear loads could result in a number of power quality problems when the system is fully integrated. These disturbances include excessive harmonic distortion, voltage sags, transient voltages outside specified limits, and power transfer problems. We will describe modeling and simulation of nonlinear loads for aerospace power systems using Saber simulation software. These modeling tools help in developing and validating requirements, predicting power quality disturbances before the power system is fully integrated, troubleshooting once the system is fully integrated, and verification of the system. We will describe modeling a suite of nonlinear single phase and three-phase loads including various types of rectifiers including 6-pulse, 12-pulse, and 18-pulse circuits. We will also present system simulation studies using these nonlinear models and various types of linear models. System studies include the use of corrective measures such as the use active power factor correction (PFC) circuits that are used to reduce harmonic distortion.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1391970,no,undetermined,0
The importance of life cycle modeling to defect detection and prevention,"In many low mature organizations dynamic testing is often the only defect detection method applied. Thus, defects are detected rather late in the development process. High rework and testing effort, typically under time pressure, lead to unpredictable delivery dates and uncertain product quality. This paper presents several methods for early defect detection and prevention that have been in existence for quite some time, although not all of them are common practice. However, to use these methods operationally and scale them to a particular project or environment, they have to be positioned appropriately in the life cycle, especially in complex projects. Modeling the development life cycle, that is the construction of a project-specific life cycle, is an indispensable first step to recognize possible defect injection points throughout the development project and to optimize the application of the available methods for defect detection and prevention. This paper discusses the importance of life cycle modeling for defect detection and prevention and presents a set of concrete, proven methods that can be used to optimize defect detection and prevention. In particular, software inspections, static code analysis, defect measurement and defect causal analysis are discussed. These methods allow early, low cost detection of defects, preventing them from propagating to later development stages and preventing the occurrence of similar defects in future projects.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1267624,no,undetermined,0
Automated Identification of Failure Causes in System Logs,"Log files are commonly inspected by system administrators and developers to detect suspicious behaviors and diagnose failure causes. Since size of log files grows fast, thus making manual analysis impractical, different automatic techniques have been proposed to analyze log files. Unfortunately, accuracy and effectiveness of these techniques are often limited by the unstructured nature of logged messages and the variety of data that can be logged.This paper presents a technique to automatically analyze log files and retrieve important information to identify failure causes. The technique automatically identifies dependencies between events and values in logs corresponding to legal executions, generates models of legal behaviors and compares log files collected during failing executions with the generated models to detect anomalous event sequences that are presented to users. Experimental results show the effectiveness of the technique in supporting developers and testers to identify failure causes.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700316,no,undetermined,0
Development and evaluation of physical properties for digital intra-oral radiographic system,"As a part of the development of a dental digital radiographic (DDR) system using a CMOS sensor, we developed hardware and software based on a graphical user interface (GUI) to acquire and display intra-oral images. The aims of this study were to develop the DDR system and evaluate its physical properties. Electric signals generated from the CMOS sensor were transformed to digital images through a control computer equipped with a USB board. The distance between the X-ray tube and the CMOS sensor was varied between 10-40 cm for optimal image quality. To evaluate the image quality according to dose variance, phantom images (60 kVp, 7 mA) were obtained at 0.03, 0.05, 0.08, 0.10, and 0.12 s of exposure time and signal-to-noise ratio (SNR) was calculated form the phantom image data. The modulation transfer function (MTF) was obtained as the Fourier transform of the line spread function (LSF), a derivative of the edge spread function (ESF) of sharp edge images acquired at exposure conditions of 60 kVp and 0.56 mA. The most compatible contrast and distinct focal point length was recorded at 20 cm. The resolution of the DDR system was approximately 6.2 line pair per mm. The developed DDR system could be used for clinical diagnosis with improvement of acquisition time and resolution. Measurement of other physical factors such as detected quantum efficiency (DQE) would be necessary to evaluate the physical properties of the DDR system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1239571,no,undetermined,0
Test generation for hardware-software covalidation using non-linear programming,"Hardware-software covalidation involves the cosimulation of a system description with a functional test sequence. Functional test generation is heavily dependent on manual interaction, making it a time-consuming and expensive process. We present an automatic test generation technique to detect design errors in hardware-software systems. The design errors targeted are those caused by incorrect synchronization between concurrent tasks/processes whose detection is dependent on event timing. We formulate the test generation problem as a nonlinear program on integer variables and we use a public domain finite domain solver to solve the problem. We present the formulation and show the results of test generation for a number of potential design errors.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1224449,no,undetermined,0
Subjective evaluation of synthetic intonation,"The paper describes a method for evaluating the quality of synthetic intonation using subjective techniques. This perceptual method of assessing intonation, not only evaluates the quality of synthetic intonation, but also allows us to compare different models of intonation to know which one is the most natural from a perceptual point of view. This procedure has been used to assess the quality of an implementation of Fujisaki's intonation model (Fujisaki, H. and Hirose, K., 1984) for the Basque language (Navas, E. et al., 2000). The evaluation involved 30 participants and results show that the intonation model developed has introduced a considerable improvement and that the overall quality achieved is good.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1224364,no,undetermined,0
Compensation of waveform distortions and voltage fluctuations of DC arc furnaces: the decoupled compensator,"DC arc furnaces are more and more applied in large industrial power systems. They represent a source of perturbations for the feeding system, in dependence on the available short-circuit power at the point of common coupling and on the arc furnace rating. This paper deals with the problem of the compensation of perturbations due to DC arc furnaces behavior; in particular, the application of a new device, the decoupled compensator, is investigated. The study is performed with reference to a typical DC arc furnace and by using computer simulations developed by means of power system blockset, that is a tool of Matlab. Waveform distortions and voltage fluctuations indices are assessed in presence and in absence of the compensation device.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1221440,no,undetermined,0
The harmonic impact of self-generating in power factor correction equipment of industrial loads: real cases studies,"This paper shows the impact of the self-generating installation in industrial loads, the problems occurred in field and the proposed solutions based from the harmonic point of view. To illustrate these points, the paper describes two facilities that installed self-generating and all the measurements and studies performed to analyze the electrical problems detected. Some studies results are shown and the implemented solutions are also described.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1221416,no,undetermined,0
Assessing harmonic penetration in terms of phase and sequence component indices,"The purpose of this paper is to simulate a balanced and unbalanced network with the DIgSILENT PowerFactory software package and to perform a harmonic penetration study on these networks in terms of phase and sequence component indices and verify the results obtained by hand calculations. The hand calculations were done using the IEEE Working Group Definitions for calculating powers in a system with nonsinusoidal waveforms. Modelling and the correct referencing of harmonic source phase angles are important when calculating power indices. A third case study is conducted to demonstrate two methods of phase angle referencing and modelling of harmonic sources, one used by DIgSILENT and the other used by the ERACS software program. The spectrum of a drive measured in the field is used. The results are compared and they show that the same values for the power indices are obtained provided that the harmonic phase angles are adjusted correctly.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1221413,no,undetermined,0
A SOM-based method for feature selection,"This paper presents a method, called feature competitive algorithm (FCA), for feature selection, which is based on an unsupervised neural network, the self-organising map (SOM). The FCA is capable of selecting the most important features describing target concepts from a given whole set of features via the unsupervised learning. The FCA is simple to implement and fast in feature selection as the learning can be done automatically and no need for training data. A quantitative measure, called average distance distortion ratio, is figured out to assess the quality of the selected feature set. An asymptotic optimal feature set can then be determined on the basis of the assessment. This addresses an open research issue in feature selection. This method has been applied to a real case, a software document collection consisting of a set of UNIX command manual pages. The results obtained from a retrieval experiment based on this collection demonstrated some very promising potential.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1202830,no,undetermined,0
Finding Faults: Manual Testing vs. Random+ Testing vs. User Reports,"The usual way to compare testing strategies, whether theoretically or empirically, is to compare the number of faults they detect. To ascertain definitely that a testing strategy is better than another, this is a rather coarse criterion: shouldn't the nature of faults matter as well as their number? The empirical study reported here confirms this conjecture. An analysis of faults detected in Eiffel libraries through three different techniques-random tests, manual tests, and user incident reports-shows that each is good at uncovering significantly different kinds of faults. None of the techniques subsumes any of the others, but each brings distinct contributions.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700320,no,undetermined,0
Validation of guidance control software requirements specification for reliability and fault-tolerance,"A case study was performed to validate the integrity of a software requirements specification (SRS) for guidance control software (GCS) in terms of reliability and fault-tolerance. A partial verification of the GCS specification resulted. Two modeling formalisms were used to evaluate the SRS and to determine strategies for avoiding design defects and system failures. Z was applied first to detect and remove ambiguity from a part of the natural language based (NL-based) GCS SRS. Next, statecharts and activity-charts were constructed to visualize the Z description and make it executable. Using this formalism, the system behavior was assessed under normal and abnormal conditions. Faults were seeded into the model (i.e., an executable specification) to probe how the system would perform. The result of our analysis revealed that it is beneficial to construct a complete and consistent specification using this method (Z-to-statecharts). We discuss the significance of this approach, compare our work with similar studies, and propose approaches for improving fault tolerance. Our findings indicate that one can better understand the implications of the system requirements using Z-statecharts approach to facilitate their specification and analysis. Consequently, this approach can help to avoid the problems that result when incorrectly specified artifacts (i.e., in this case requirements) force corrective rework",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981660,no,undetermined,0
A Theoretical Model of the Effects of Losses and Delays on the Performance of SIP,The session initiation protocol (SIP) is widely used for VoIP communication. Losses caused by network or server overload would cause retransmissions and delays in the session establishment and would hence reduce the perceived service quality of the users. In order to be able to take counter measures network and service planers require detailed models that would allow them to predict such effects in advance. This paper presents a theoretical model of SIP that can be used for determining various parameters such as the delay and the number of messages required for establishing a SIP session when taking losses and delays into account. The model is restricted to the case when SIP is transported over UDP. The theoretical results are then verified using measurements.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4698067,no,undetermined,0
Multidimensional modeling of image quality,"In this paper, multidimensional models of image quality are discussed. In such models, alternative images, for instance, obtained through different processing or coding of the same scene, are represented as points in a multidimensional space. The positioning is such that the correlation between geometrical properties of the points and the subjective impressions mediated by the corresponding images is optimized. More specifically, perceived dissimilarities between images are monotonically related to interpoint distances, while the strengths of image quality attributes (such as perceived noise and blur or image quality) are, for instance, monotonically related to point coordinates along specified directions. The goal of multidimensional models is to capture subjective impressions into a single picture that is easy to interpret. We apply multidimensional models to two existing data sets to demonstrate that they indeed account very well for experimental data on image quality. The program XGms is introduced as a new interactive tool for constructing multidimensional models from experimental data. Although XGms is introduced here within the context of image-quality modeling, it is also potentially useful in other applications that rely on multidimensional models",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=982411,no,undetermined,0
Detecting changes in XML documents,"We present a diff algorithm for XML data. This work is motivated by the support for change control in the context of the Xyleme project that is investigating dynamic warehouses capable of storing massive volumes of XML data. Because of the context, our algorithm has to be very efficient in terms of speed and memory space even at the cost of some loss of quality. Also, it considers, besides insertions, deletions and updates (standard in diffs), a move operation on subtrees that is essential in the context of XML. Intuitively, our diff algorithm uses signatures to match (large) subtrees that were left unchanged between the old and new versions. Such exact matchings are then possibly propagated to ancestors and descendants to obtain more matchings. It also uses XML specific information such as ID attributes. We provide a performance analysis of the algorithm. We show that it runs in average in linear time vs. quadratic time for previous algorithms. We present experiments on synthetic data that confirm the analysis. Since this problem is NP-hard, the linear time is obtained by trading some quality. We present experiments (again on synthetic data) that show that the output of our algorithm is reasonably close to the optimal in terms of quality. Finally we present experiments on a small sample of XML pages found on the Web",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994696,no,undetermined,0
WARE: a tool for the reverse engineering of Web applications,"The development of Web sites and applications is increasing dramatically to satisfy the market requests. The software industry is facing the new demand under the pressure of a very short time-to-market and an extremely high competition. As a result, Web sites and applications are usually developed without a disciplined process: Web applications are directly coded and no, or poor, documentation is produced to support the subsequent maintenance and evolution activities, thus compromising the quality of the applications. This paper presents a tool for reverse engineering Web applications. UML diagrams are used to model a set of views that depict several aspects of a Web application at different abstraction levels. The recovered diagrams ease the comprehension of the application and support its maintenance and evolution. A case study, carried out with the aim of assessing the effectiveness of the proposed tool, allowed relevant information about some real Web applications to be successfully recovered and modeled by UML diagrams",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995811,no,undetermined,0
An Efficient Forward and Backward Fault-Tolerant Mobile Agent System,"Mobile agent is a special program and it can switch and execute the task of user commands among the networks and hosts. During the task executing, the mobile agent can convey the data, state and program code to another host in order to autonomously execute and continue the task in another host. While the mobile agent is executing the task at any of the software and hardware fault incurred or network problems, there will be two conditions as following lists: 1. Users continuously wait the reply from the agent, but users will never have the reply because of the some faults incurred to the agent in the networks or hosts. 2. Users assign a new agent as the former agent has been lost to restart the former task, but the former agent only congested the delay problem of the network or host. This causes that these two agents have the same task to be executed. Therefore, the fault detecting and recovering of the mobile agent are important issues to be discussed. However, this paper propose a front behind failure detection and recovery method that the task agent has to report the task process at the present stage to the former and latter agent and agents will exchange their messages for the present stage. This is more accurate than the method in for the task at present because it can reduce the loading of the task fault report from the network congested.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4696307,no,undetermined,0
A versatile family of consensus protocols based on Chandra-Toueg's unreliable failure detectors,"This paper is on consensus protocols for asynchronous distributed systems prone to process crashes, but equipped with Chandra-Toueg's (1996) unreliable failure detectors. It presents a unifying approach based on two orthogonal versatility dimensions. The first concerns the class of the underlying failure detector. An instantiation can consider any failure detector of the class S (provided that at least one process does not crash), or oS (provided that a majority of processes do not crash). The second versatility dimension concerns the message exchange pattern used during each round of the protocol. This pattern (and, consequently, the round message cost) can be defined for each round separately, varying from O(n) (centralized pattern) to O(n<sup>2</sup>) (fully distributed pattern), n being the number of processes. The resulting versatile protocol has nice features and actually gives rise to a large and well-identified family of failure detector-based consensus protocols. Interestingly, this family includes at once new protocols and some well-known protocols (e.g., Chandra-Toueg's oS-based protocol). The approach is also interesting from a methodological point of view. It provides a precise characterization of the two sets of processes that, during a round, have to receive messages for a decision to be taken (liveness) and for a single value to be decided (safety), respectively. Interestingly, the versatility of the protocol is not restricted to failure detectors: a simple timer-based instance provides a consensus protocol suited to partially synchronous systems",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995450,no,undetermined,0
Adapting extreme programming for a core software engineering course,"Over a decade ago, the manufacturing industry determined it needed to be more agile to thrive and prosper in a changing, nonlinear, uncertain and unpredictable business environment The software engineering community has come to the same realization. A group of software methodologists has created a set of software development processes, termed agile methodologies that have been specifically designed to respond to the demands of the turbulent software industry. Each of the processes in the set of agile processes comprises a set of practices. As educators, we must assess the emerging agile practices, integrate them into our courses (carefully), and share our experiences and results from doing so. The paper discusses the use of extreme programming, a popular agile methodology, in a senior software engineering course at North Carolina State University. It then provides recommendations for integrating agile principles into a core software engineering course",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995210,no,undetermined,0
Diagnosing quality of service faults in distributed applications,"QoS management refers to the allocation and scheduling of computing resources. Static QoS management techniques provide a guarantee that resources will be available when needed. These techniques allocate resources based on worst-case needs. This is especially important for applications with hard QoS requirements. However, this approach can waste resources. In contrast, a dynamic approach allocates and deallocates resources during the lifetime of an application. In the dynamic approach the application is started with an initial resource allocation. If the application does not meet its QoS requirements, a resource manager attempts to allocate more resources to the application until the application's QoS requirement is met. While this approach offers the opportunity to better manage resources and meet application QoS requirements, it also introduces a new set of problems. In particular, a key problem is detecting why a QoS requirement is not being satisfied and determining the cause and, consequently, which resource needs to be adjusted. This paper investigates a policy-based approach for addressing these problems. An architecture is presented and a prototype described. This is followed by a case study in which the prototype is used to diagnose QoS problems for a web application based on Apache",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=995173,no,undetermined,0
Automatic model refinement for fast architecture exploration [SoC design],We present a methodology and algorithms for automatic refinement from a given design specification to an architecture model based on decisions in architecture exploration. An architecture model is derived from the specification through a series of well defined steps in our design methodology. Traditional architecture exploration relies on manual refinement which is painfully time consuming and error prone. The automation of the refinement process provides a useful tool to the system designer to quickly evaluate several architectures in the design space and make the optimal choice. Experiments with the tool on a system design example show the robustness and usefulness of the refinement algorithm,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994944,no,undetermined,0
Error detection by duplicated instructions in super-scalar processors,"This paper proposes a pure software technique """"error detection by duplicated instructions"""" (EDDI), for detecting errors during usual system operation. Compared to other error-detection techniques that use hardware redundancy, EDDI does not require any hardware modifications to add error detection capability to the original system. EDDI duplicates instructions during compilation and uses different registers and variables for the new instructions. Especially for the fault in the code segment of memory, formulas are derived to estimate the error-detection coverage of EDDI using probabilistic methods. These formulas use statistics of the program, which are collected during compilation. EDDI was applied to eight benchmark programs and the error-detection coverage was estimated. Then, the estimates were verified by simulation, in which a fault injector forced a bit-flip in the code segment of executable machine codes. The simulation results validated the estimated fault coverage and show that approximately 1.5% of injected faults produced incorrect results in eight benchmark programs with EDDI, while on average, 20% of injected faults produced undetected incorrect results in the programs without EDDI. Based on the theoretical estimates and actual fault-injection experiments, EDDI can provide over 98% fault-coverage without any extra hardware for error detection. This pure software technique is especially useful when designers cannot change the hardware, but they need dependability in the computer system. To reduce the performance overhead, EDDI schedules the instructions that are added for detecting errors such that """"instruction-level parallelism"""" (ILP) is maximized. Performance overhead can be reduced by increasing ILP within a single super-scalar processor. The execution time overhead in a 4-way super-scalar processor is less than the execution time overhead in the processors that can issue two instructions in one cycle",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994913,no,undetermined,0
Testable design and testing of high-speed superconductor microelectronics,"True software-defined radio cellular base stations require extremely fast data converters, which can not currently be implemented in semiconductor technology. Superconductor niobium-based delta ADCs have shown to be able to perform this task. The problem of testing these devices is a severe task, as very little is known about possible defects in this technology. This paper shows an approach for gaining information on these defects and illustrates how BIST can be a solution of detecting defects in ADCs under extreme conditions",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994580,no,undetermined,0
Enhancement of degraded document images using hybrid thresholding,"The paper presents a hybrid thresholding approach for binarization and enhancement of degraded documents. Historical documents contain information of great cultural and scientific value. But such documents are frequently degraded over time. Digitized degraded documents require specialized processing to remove different kinds of noise and to improve readability. The approach for enhancing degraded documents uses a combination of two thresholding . First, iterative global thresholding is applied to the smoothed degraded image until the stopping criteria is reached. Then a threshold selection method from gray level histogram is used to binarize the image. The next step is detecting areas where noise still remains and applying iterative thresholding locally. A method to improve the quality of textual information in the document is also done as a post processing stage, thus making the approach efficient and more suited for OCR applications.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4697271,no,undetermined,0
Monitoring software requirements using instrumented code,"Ideally, software is derived from requirements whose properties have been established as good. However, it is difficult to define and analyze requirements. Moreover derivation of software from requirements is error prone. Finally, the installation and use of compiled software can introduce errors. Thus, it can be difficult to provide assurances about the state of a software's execution. We present a framework to monitor the requirements of software as it executes. The framework is general, and allows for automated support. The current implementation uses a combination of assertion and model checking to inform the monitor. We focus on two issues: (1) the expression of """"suspect requirements"""", and (2) the transparency of the software and its environment to the monitor. We illustrate these issues with the widely known problems of the Dining Philosophers and the CCITT X.509 authentication. Each are represented as Java programs which are then instrumented and monitored.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994468,no,undetermined,0
Prototype implementations of an architectural model for service-based flexible software,"The need to change software easily to meet evolving business requirements is urgent, and a radical shift is required in the development of software, with a more demand-centric view leading to software which will be delivered as a service, within the framework of an open marketplace. We describe a service architecture and its rationale, in which components may be bound instantly, just at the time they are needed and then the binding may be disengaged. This allows highly flexible software services to be evolved in """"Internet time"""". The paper focuses on early results: some of the aims have been demonstrated and amplified through two experimental implementations, enabling us to assess the strengths and weakness of the approach. It is concluded that some of the key underpinning concepts discovery and late binding - are viable and demonstrate the basic feasibility of the architecture.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=993996,no,undetermined,0
Current status of information technologies used in support of task-oriented collaboration,"Many organizations use information technology (IT) as a way to enable global networking, group negotiations, and expertise sharing amongst end-users in distributed work environments. IT can potentially play a significant role in effective and efficient negotiation and collaboration if it can enhance the quality of communication and coordination between group members asynchronously or synchronously. The paper empirically assesses the pattern of deployment of IT in task-oriented collaboration in US organizations. Data collected from one hundred and nineteen organizations is analyzed to gain insights into adoption and use patterns, and the benefits of seven popular IT approaches that have the capability to support collaboration and negotiation between workgroup members. Our analyses show that e-mail and audio teleconferencing are the most widely adopted and used technologies, while Web-based tools and electronic meeting systems (EMS) have the lowest level of adoption and use. Implications of these findings are discussed, along with some directions for practice and research.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=993891,no,undetermined,0
An approach for intelligent detection and fault diagnosis of vacuum circuit breakers,"In this paper, an approach for intelligent detection and fault diagnosis of vacuum circuit breakers is introduced, by which, the condition of a vacuum circuit breaker can be monitored on-line, and the detectable faults can be identified, located, displayed and saved for the use of analyzing their change tendencies. The main detecting principles and diagnostics are described. Both the hardware structure and software design are also presented.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=993739,no,undetermined,0
Image processing techniques for wafer defect cluster identification,"Electrical testing determines whether each die on a wafer functions as originally designed. But these tests don't detect all the defective dies in clustered defects on the wafer, such as scratches, stains, or localized failed patterns. Although manual checking prevents many defective dies from continuing on to assembly, it does not detect localized failure patterns-caused by the fabrication process-because they are invisible to the naked eye. To solve these problems, we propose an automatic, wafer-scale, defect cluster identifier. This software tool uses a median filter and a clustering approach to detect the defect clusters and to mark all defective dies. Our experimental results verify that the proposed algorithm effectively detects defect clusters, although it introduces an additional 1% yield loss of electrically good dies. More importantly, it makes automated wafer testing feasible for application in the wafer-probing stage.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990441,no,undetermined,0
Planning and executing time-bound projects,Looks at how the SPID (statistically planned incremental deliveries) approach combines critical chain planning with incremental development and rate monitoring to help software developers meet project deadlines. SPID focuses on how best to organize a project to guarantee delivery of at least a working product with an agreed subset of the total functionality by the required date,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989933,no,undetermined,0
A stochastic approach for fine grain QoS control,"We present a method for fine grain QoS control of multimedia applications. This method takes as input an application software composed of actions parameterized by quality levels. Our method allows the construction of a Quality Manager which computes adequate action quality levels, so as to meet QoS requirements (action deadlines are met and quality levels are maximal) for a given platform.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4697008,no,undetermined,0
A process for software architecture evaluation using metrics,"Software systems often undergo changes. Changes are necessary not only to fix defects but also to accommodate new features demanded by users. Most of the time, changes are made under schedule and budget constraints and developers lack time to study the software architecture and select the best way to implement the changes. As a result, the code degenerates, making it differ from the planned design. The time spent on the planned design to create architecture to satisfy certain properties is lost, and the systems may not satisfy those properties any more. We describe an approach to systematically detect and correct deviations from the planned design as soon as possible based on architectural guidelines. We also describe a case study, in which the process was applied.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199475,no,undetermined,0
Packaging and disseminating lessons learned from COTS-based software development,"The appropriate management of experience and knowledge has become a crucially important capability for organizations of all types and software organizations are no exception. We describe an initiative aimed at helping the software engineering community share experience, in the form of lessons learned. The Center for Empirically Based Software Engineering (CeBASE) COTS lessons learned repository (CLLR) is described, including its motivation, its current status and capabilities, and the plans for its evolution. The contribution of this work lies not only in the approach itself and its validation, but also in the creation of a community of interest, which is fundamental in order to ensure the success of such an initiative. The knowledge and experience that are captured, carefully processed, and made available to the software engineering community also form part of this contribution. The community is supported by eWorkshops that bring COTS experts together, letting them discuss, share, and synthesize COTS knowledge. This knowledge is analyzed, refined and shared through the repository, which is designed to be self-monitoring in several ways. It provides several mechanisms for users to provide feedback, both in the form of new lessons learned and additional insight into existing lessons in the repository. This feedback is used to shape the repository contents and capabilities over time. Also, the repository itself tracks its own usage patterns in order to better assess and meet the needs of its users. Although the focus of the CLLR has been on COTS based software development, the technologies and approaches we have employed are applicable to any sub-area of software engineering or any other community of interest.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199459,no,undetermined,0
An on-line test platform for component-based systems,"One of the most provocative research areas in software engineering field is the testing of modern component based distributed applications in order to assure required quality parameters. Dynamic interactions and structural embedding, run-time loadable configurations, and services that can be deployed in arbitrary executions environments results in an increased complexity. Moreover, that the variety of possible states and behaviors becomes unpredictable. Thus, since testing during the development phase is always applied in simulated environments, it is almost impossible to detect faults, which appear under real condition, during production phase of a system. We therefore aim at concepts and methodologies that achieve on-line testing of distributed component based systems in their production phase. In comparison with off-line testing (i.e. testing that takes place during system development), on-line testing addresses particular aspects of the behavior of distributed systems, such as: functionality under limited time and resources available, complex transactions that are performed between components provided by different vendors, deployment, and composition of different services.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199455,no,undetermined,0
Formal Support for Quantitative Analysis of Residual Risks in Safety-Critical Systems,"With the increasing complexity in software and electronics in safety-critical systems new challenges to lower the costs and decrease time-to-market, while preserving high assurance have emerged. During the safety assessment process, the goal is to minimize the risk and particular, the impact of probable faults on system level safety. Every potential fault must be identified and analysed in order to determine which faults that are most important to focus on. In this paper, we extend our earlier work on formal qualitative analysis with a quantitative analysis of fault tolerance. Our analysis is based on design models of the system under construction. It further builds on formal models of faults that have been extended for estimated occurence probability allowing to analyse the system-level failure probability. This is done with the help of the probabilistic model checker PRISM. The extension provides an improvement in the costly process of certification in which all forseen faults have to be evaluated with respect to their impact on safety and reliability. We demonstrate our approach using an application from the avionic industry: an Altitude Meter System.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4708874,no,undetermined,0
Maximum distance testing,"Random testing has been used for years in both software and hardware testing. It is well known that in random testing each test requires to be selected randomly regardless of the tests previously generated. However, random testing could be inefficient for its random selection of test patterns. This paper, based on random testing, introduces the concept of Maximum Distance Testing (MDT) for VLSI circuits in which the total distance among all test patterns is chosen maximal so that the set of faults detected by one test pattern is as different as possible from that of faults detected by the tests previously applied. The procedure for constructing a Maximum Distance Testing Sequence (MDTS) is described in detail. Experimental results on Benchmark as well as other circuits are also given to evaluate the performances of our new approach.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181678,no,undetermined,0
Exact computation of maximally dominating faults and its application to n-detection tests,"n-detection test sets for stuck-at faults have been shown to be useful in detecting unmodeled defects. It was also shown that a set of faults, called maximally dominating faults, can play an important role in controlling the increase in the size of an n-detection test set as n is increased. In an earlier work, a superset of the maximally dominating fault set was used. In this work, we propose a method to determine exact sets of maximally dominating faults. We also define a new type of n-detection test sets based on the exact set of maximally dominating faults. We present experimental results to demonstrate the usefulness of this exact set in producing high-quality n-detection test sets.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181677,no,undetermined,0
Using static analysis to improve communications infrastructure,"Static analysis is a promising technique for improving the safety and reliability of software used in avionics infrastructure. Source code analyzers are effective at locating a significant class of defects that are not detected by compilers during standard builds and often go undetected during runtime testing as well. Related to bug finders are a number of other static code improvement tasks, including automated unit test generation, programmer and software metrics tracking, and coding standards enforcement. However, adoption of these tools for everyday avionics software developer has been low. This paper will discuss the major barriers to adoption of these important tools and provide advice regarding how they can be effectively promulgated across the enterprise. Case studies of popular open source applications will be provided for illustration.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4702765,no,undetermined,0
Probabilistic analysis of CAN with faults,"As CANs (controller area networks) are being increasingly used in safety-critical applications, there is a need for accurate predictions of failure probability. In this paper we provide a general probabilistic schedulability analysis technique which is applied specifically to CANs to determine the effect of random network faults on the response times of messages. The resultant probability distribution of response times can be used to provide probabilistic guarantees of real-time behaviour in the presence of faults. The analysis is designed to have as little pessimism as possible but never be optimistic. Through simulations, this is shown to be the case. It is easy to apply and can provide useful evidence for justification of an event-triggered bus in a critical system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181581,no,undetermined,0
Managing software evolution with a formalized abstraction hierarchy,"Complex computer systems are seldom made from scratch but they contain significant amounts of legacy code, which then is under continuous pressure for evolution. Therefore, a need for a rigorous method for managing evolution in this setting is evident. We propose a management method for reactive and distributed systems. The method is based on creating a formal abstraction hierarchy to model the system with abstractions that exceed those that are used as implementation facilities. This hierarchy is then used to assess the cost of a modification by associating the modification to appropriate abstractions in the hierarchy and by determining the abstractions that need to be revisited to retain the hierarchy consistent.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181515,no,undetermined,0
Fault detection effectiveness of spathic test data,"This paper presents an approach for generating test data for unit-level, and possibly integration-level, testing based on sampling over intervals of the input probability distribution, i.e., one that has been divided or layered according to criteria. Our approach is termed """"spathic"""" as it selects random values felt to be most likely or least likely to occur from a segmented input probability distribution. Also, it allows the layers to be further segmented if additional test data is required later in the test cycle. The spathic approach finds a middle ground between the more difficult to achieve adequacy criteria and random test data generation, and requires less effort on the part of the tester. It can be viewed as guided random testing, with the tester specifying some information about expected input. The spathic test data generation approach can be used to augment """"intelligent"""" manual unit-level testing. An initial case study suggests that spathic test sets defect more faults than random test data sets, and achieve higher levels of statement and branch coverage.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181511,no,undetermined,0
A framework for performability modeling of messaging services in distributed systems,"Messaging services are a useful component in distributed systems that require scalable dissemination of messages (events) from suppliers to consumers. These services decouple suppliers and consumers, and take care of client registration and message propagation, thus relieving the burden on the supplier Recently performance models for the configurable delivery and discard policies found in messaging services have been developed, that can be used to predict response time distributions and discard probabilities under failure-free conditions. However, these messaging service models do not include the effect of failures. In a distributed system, supplier, consumer and messaging services can fail independently leading to different consequences. In this paper we consider the expected loss rate associated with messaging services as a performability measure and derive approximate closed-form expressions for three different quality of service settings. These measures provide a quantitative framework that allows different messaging service configurations to be compared and design trade-off decisions to be made.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181495,no,undetermined,0
Verification of Web service flows with model-checking techniques,"Web service is an emerging software technology to use remote services in the Internet. As it becomes pervasive, some """"language"""" to describe Web service flows is needed to combine existing services flexibly. The flow essentially describes distributed collaborations and is not easy to write and verify, while the fault that the flow description may contain can only be detected at runtime. The faulty flow description is not desirable because a tremendous amount of publicly shared network resources are consumed. The verification of the Web service flow prior to its execution in the Internet is mandatory. This paper proposes to use the software model-checking technology for the verification of the Web service flow descriptions. For a concrete discussion, the paper adapts WSFL (Web Services Flow Language) as the language to describe the Web service flows, and uses the SPIN model-checker for the verification engine. The experiment shows that the software model-checking technology is usable as a basis for the verification of WSFL descriptions.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180904,no,undetermined,0
Reliable file transfer in Grid environments,Grid-based computing environments are becoming increasingly popular for scientific computing. One of the key issues for scientific computing is the efficient transfer of large amounts of data across the Grid. In this paper we present a reliable file transfer (RFT) service that significantly improves the efficiency of large-scale file transfer. RFT can detect a variety of failures and restart the file transfer from the point of failure. It also has capabilities for improving transfer performance through TCP tuning.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181855,no,undetermined,0
An Interaction-Based Test Sequence Generation Approach for Testing Web Applications,"Web applications often use dynamic pages that interact with each other by accessing shared objects, e.g., session objects. Interactions between dynamic pages need to be carefully tested, as they may give rise to subtle faults that cannot be detected by testing individual pages in isolation. Since it is impractical to test all possible interactions, a trade-off must be made between test coverage (in terms of number of interactions covered in the tests) and test effort. In this paper, we present a test sequence generation approach to cover all pairwise interactions, i.e., interactions between any two pages. Intuitively, if a page P could reach another page Ppsila, there must exist a test sequence in which both P and Ppsila are visited in the given order. We report a test sequence generation algorithm and two case studies in which test sequences are generated to achieve pairwise interaction coverage for two Web applications. The empirical results indicate that our approach achieves good code coverage and is effective for detecting interaction faults in the subject applications.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4708879,no,undetermined,0
Detection and Diagnosis of Recurrent Faults in Software Systems by Invariant Analysis,"A correctly functioning enterprise-software system exhibits long-term, stable correlations between many of its monitoring metrics. Some of these correlations no longer hold when there is an error in the system, potentially enabling error detection and fault diagnosis. However, existing approaches are inefficient, requiring a large number of metrics to be monitored and ignoring the relative discriminative properties of different metric correlations. In enterprise-software systems, similar faults tend to reoccur. It is therefore possible to significantly improve existing correlation-analysis approaches by learning the effects of common recurrent faults on correlations. We present methods to determine the most significant correlations to track for efficient error detection, and the correlations that contribute the most to diagnosis accuracy. We apply machine learning to identify the relevant correlations, removing the need for manually configured correlation thresholds, as used in the prior approaches. We validate our work on a multi-tier enterprise-software system. We are able to detect and correctly diagnose 8 of 10 injected faults to within three possible causes, and to within two in 7 out of 8 cases. This compares favourably with the existing approaches whose diagnosis accuracy is 3 out of 10 to within 3 possible causes. We achieve a precision of at least 95%.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4708890,no,undetermined,0
Preventing network instability caused by propagation of control plane poison messages,"We present a framework of fault management for a particular type of failure propagation that we refer to as """"poison message failure propagation"""": Some or all of the network elements have a software or protocol 'bug' which is activated on receipt of a certain network control/management message (the poison message). This activated 'bug' will cause the node to fail with some probability. If the network control or management is such that this message is persistently passed among the network nodes, and if the node failure probability is sufficiently high, large-scale instability can result. In order to mitigate this problem. we propose a combination of passive diagnosis and active diagnosis. Passive diagnosis includes protocol analysis of messages received and sent by failed nodes, correlation of messages among multiple failed nodes and analysis of the pattern of failure propagation. This is combined with active diagnosis in which filters are dynamically configured to block suspect protocols or message types. OPNET simulations show the effectiveness of passive diagnosis. Message filtering is formulated as a sequential decision problem, and a heuristic policy is proposed for this problem.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180420,no,undetermined,0
A Self-Managing Brokerage Model for Quality Assurance in Service-Oriented Systems,"Service-oriented system quality is not just a function of the quality of a provided service, but the interdependencies between services, the resource constraints of the runtime environment and network outages. This makes it difficult to anticipate how the consequences of these factors might influence system behaviour, which in turn makes it difficult to specify the right system environment in advance. Current quality management schemes for service-oriented systems are inadequate for assuring system quality as they rely largely on static service properties to predict system quality. Secondly, they offer the consumer only limited control over the quality of service. This paper describes a self-managing, consumer-centred approach based on a brokerage architecture that allows different monitoring, negotiation, forecasting and provider reputation schemes to be integrated into a runtime quality assurance framework for service-oriented systems. We illustrate our solution with a small service-oriented application.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4708900,no,undetermined,0
Service time optimal self-stabilizing token circulation protocol on anonymous undirectional rings,"We present a self-stabilizing token circulation protocol on unidirectional anonymous rings. This protocol requires no processor identifiers or distinguished processor (i.e. all processors perform the same algorithm). The protocol is randomized and self-stabilizing, meaning that starting from an arbitrary configuration (in response to an arbitrary perturbation modifying the memory state), it reaches (with probability 1) a legitimate configuration (i.e. a configuration with only one token in the network). All previous randomized self-stabilizing token circulation protocols designed to work under unfair distributed schedulers have the same drawback: once stabilized, service time is slow (in the best case, it is bounded by 2N where N is the ring size). Once stabilized, our protocol provides an optimal service: after N computation steps, each processor has obtained the token once. The protocol can be used to implement fair distributed mutual exclusion in any ring topology network.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180176,no,undetermined,0
A fault-tolerant approach to secure information retrieval,"Several private information retrieval (PIR) schemes were proposed to protect users' privacy when sensitive information stored in database servers is retrieved. However, existing PIR schemes assume that any attack to the servers does not change the information stored and any computational results. We present a novel fault-tolerant PIR scheme (called FT-PIR) that protects users' privacy and at the same time ensures service availability in the presence of malicious server faults. Our scheme neither relies on any unproven cryptographic assumptions nor the availability of tamper-proof hardware. A probabilistic verification function is introduced into the scheme to detect corrupted results. Unlike previous PIR research that attempted mainly to demonstrate the theoretical feasibility of PIR, we have actually implemented both a PIR scheme and our FT-PIR scheme in a distributed database environment. The experimental and analytical results show that only modest performance overhead is introduced by FT-PIR while comparing with PIR in the fault-free cases. The FT-PIR scheme tolerates a variety of server faults effectively. In certain fail-stop fault scenarios, FT-PIR performs even better than PIR. It was observed that 35.82% less processing time was actually needed for FT-PIR to tolerate one server fault.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180169,no,undetermined,0
The quality of service evaluating tool for WAAS,"In this paper, we introduce a software kit for evaluating the Quality of Service (QoS) provided by the Wide Area Augmentation System (WAAS). This tool, named Service Volume Model (SVM), is a professional software providing types of real-time QoS criteria evaluations for WAAS, such as service accuracy, integrity, continuity, availability and safety. Those QoS evaluations can provide the user accurate safety-critical positioning reference information.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1179078,no,undetermined,0
Challenges in Scaling Software-Based Self-Testing to Multithreaded Chip Multiprocessors,"Functional software-based self-testing (SBST) has been recently studied by leading academic research groups and applied by major microprocessor manufacturers as a complement to other classic structural testing techniques for microprocessors and processor-based SoCs. Is the SBST paradigm scalable to testing multithreaded chip multiprocessors (CMPs) and effectively detect faults not only in the functional components but also in the thread-specific and core interoperability logic? We study the challenges in scaling existing software-based self-test capital (uniprocessor self-test programs and self-test generation techniques) to real, multithreaded CMPs, like Sun's OpenSPARC T1 and T2. Since this type of CMPs is built around well studied microprocessor cores of mature architecture (like SPARC v9 in the OpenSPARC case), tailoring, enhancing and scheduling of existing uniprocessor self-test programs can be an effective methodology for software-based self-test of CMPs.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700679,no,undetermined,0
A Practical Monitoring Framework for ESB-Based Services,"Services in service-oriented computing (SOC) are often black-box since they are typically developed by 3rd party developers and deployed only with their interface specifications. Therefore, internal details of services and implementation details of service components are not readily available. Also, services in SOC are highly evolvable since new services can be registered into repositories, and existing services maybe modified for their logic and interfaces, or they may suddenly disappear. As the first and most essential step for service management, service monitoring is to acquire useful data and information about the services and to assess various quality of service (QoS). Being able to monitor services is a strong prerequisite to effective service management. In this paper, we present a service monitoring framework for ESB-based services, as a sub-system of our Open Service Management Framework (OSMaF). We firstly define the criteria for designing QoS monitoring framework, and present the architecture and key components of the framework. Then, we illustrate the key techniques used to efficiently monitor services and compute QoS metrics. Finally, an implementation of the framework is presented to show its applicability.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700499,no,undetermined,0
Reliability Assessment of Mass-Market Software: Insights from Windows Vista,"Assessing the reliability of mass-market software (MMS), such as the Windows<sup>reg</sup> operating system, presents many challenges. In this paper, we share insights gained from the Windows Vista<sup>reg</sup> and Windows Vista<sup>reg</sup> SP1 operating systems. First, we find that the automated reliability monitoring approach, which periodically reports reliability status, provides higher quality data and requires less effort compared to other approaches available today. We describe one instance in detail: the Windows reliability analysis component, and illustrate its advantages using data from Windows Vista. Second, we show the need to account for usage scenarios during reliability assessments. For pre-release versions of Windows Vista and Vista SP1, usage scenarios differ by 2-4X for Microsoft internal and external samples; corresponding reliability assessments differ by 2-3X. Our results help motivate and guide further research in reliability assessment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700332,no,undetermined,0
On Reliability Analysis of Open Source Software - FEDORA,"Reliability analyses of software systems often focus only on the number of faults reported against the software. Using a broader set of metrics, such as problem resolution times and field software usage levels, can provide a more comprehensive view of the product. Some of these metrics are more readily available for open source products. We analyzed a suite of FEDORA releases and obtained some interesting findings. For example, we show that traditional reliability models may be used to predict problem rates across releases. We also show that security related reports tend to have a different profile than non-security related problem reporting and repair.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700358,no,undetermined,0
Software reliability corroboration,"We suggest that subjective reliability estimation from the development lifecycle, based on observed behavior or the reflection of one's belief in the system quality, be included in certification. In statistical terms, we hypothesize that a system failure occurs with the estimated probability. Presumed reliability needs to be corroborated by statistical testing during the reliability certification phase. As evidence relevant to the hypothesis increases, we change the degree of belief in the hypothesis. Depending on the corroboration evidence, the system is either certified or rejected. The advantage of the proposed theory is an economically acceptable number of required system certification tests, even for high assurance systems so far considered impossible to certify.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199453,no,undetermined,0
A rigorous approach to reviewing formal specifications,"A new approach to rigorously reviewing formal specifications to ensure their internal consistency and validity is forwarded. This approach includes four steps: (1) deriving properties as review targets based on the syntax and semantics of the specification, (2) building a review task tree to present all the necessary review tasks for each property, (3) carrying out reviews based on the review task tree, and (4) analyzing the review results to determine whether faults are detected or not. We apply this technique to the SOFL specification language, which is an integrated formalism of VDM, Petri nets, and data flow diagrams to discuss how each step is performed.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199452,no,undetermined,0
Architecting for Reliability - Recovery Mechanisms,Telecommunications systems achieve high levels of reliability by implementing detection and recovery mechanisms with high coverage. With the trend towards the use of more COTS components in these systems the choices available for the systems detection and recovery mechanisms are more limited. An escalating recovery model with varying coverage factors and recovery durations is developed to provide insight into high availability design alternatives for commercial products. This work extends our previous examination of escalating detection by considering recovery.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700339,no,undetermined,0
Static Detection of Redundant Test Cases: An Initial Study,"As software systems evolve, the size of their test suites grow due to added functionality and customer-detected defects. Many of these tests may contain redundant elements with previous tests. Existing techniques to minimize test suite size generally require dynamic execution data, but this is sometimes unavailable. We present a static technique that identifies test cases with redundant instruction sequences, allowing them to be merged or eliminated. Initial results at ABB show that 7%-23% of one test suite may be redundant.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700347,no,undetermined,0
Optimal age-dependent checkpoint strategy with retry of rollback recovery,"We consider a file system with age-dependent checkpointing, where the rollback recovery operation after a system failure may be imperfect with positive unsuccessful probability. After a few mathematical preliminaries, we formulate the steady-state availability and obtain the age-dependent checkpoint strategy analytically, maximizing the steady-state availability. In a numerical example, we examine the dependence of the unsuccessful probability of rollback recovery on the optimal checkpoint interval, and refer to the effect of imperfect rollback recovery quantitatively.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194658,no,undetermined,0
Developing highly dependable application in a distributed system environment,"The aims of the research are to investigate techniques that support the development of highly dependable applications in a distributed system environment. Techniques we are investigating include task allocation and fault-tolerant protocols supporting redundant task allocation, load balance, fault-tolerant computing and communication, error detecting and reconfiguration, test case generation and fault injection. The highly dependable environment co-exists with the original communication and operating system. It is transparent to applications that do not need the highly dependable environment. Applications that wish to use the highly dependable environment need only to specify the level of criticality of their tasks in order for the system to assign the level of redundancy and to activate the relevant fault tolerant protocols. The application we intend to implement in the environment is the firewall application. The firewall is run in redundant mode. Each incoming or outgoing packet is checked by two or more copies of the firewall application. Only when the majority of the firewall copies decide to accept the packet, the packet can go through the firewall. Otherwise, the packet will be rejected: Different decisions from the different firewall copies signify a possible hardware fault or a software error in the underlying system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194645,no,undetermined,0
Forward resource reservation for QoS provisioning in OBS systems,"This paper addresses the issue of providing QoS services for optical burst switching (OBS) systems. We propose a linear predictive filter (LPF)-based forward resource reservation method to reduce the burst delay at edge routers. An aggressive reservation method is proposed to increase the successful forward reservation probability and to improve the delay reduction performance. We also discuss a QoS strategy that achieves burst delay differentiation for different classes of traffic by extending the FRR scheme. We analyze the latency reduction improvement gained by our FRR scheme, and evaluate the bandwidth cost of the FRR-based QoS strategy. Our scheme yields significant delay reduction for time-critical traffic, while maintaining the bandwidth overhead within limits.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1189135,no,undetermined,0
Connecting physical layer and networks layer QoS in DS-CDMA networks-multiple traffic case,"This paper proposes and evaluates two tier call admission control in DS-CDMA networks which reserves bandwidth for handoff events. Under this scheme outage conditions due to handoffs occur only with small and controlled probability. A two-level admission policy is defined: in tier 1 policy, the network capacity is calculated on the basis of the bound on outage probability. However, this policy does not suffice to prevent outage events upon handoffs for various traffic types, and henceforth, we propose an extension which reserves extra bandwidth for handoff calls, thus ensuring that handoff calls will not violate the outage probability bound. The modified second-tier bandwidth reservation policy is adaptive with respect to the traffic intensity, and we show that it can provide satisfactory call (flow) quality during its lifetime.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1188191,no,undetermined,0
Data coverage testing,"Generating test data sets which are sufficiently large to effectively cover all the tests required before a software component can be certified as reliable is a time consuming and error-prone task if carried out manually. A key parameter when testing collections is the size of the collection to be tested: an automatic test generator builds a set of collections containing n elements where n ranges from 0 to n<sub>crit</sub>. Data coverage analysis allows us to determine rigorously a collection size such that testing with collections of size > n<sub>crit</sub> does not provide any further useful information, i.e. will not uncover any new faults. We conducted a series of experiments on modules from the C++ Standard Template Library which were seeded with errors. Using a test model appropriate to each module, we generated data sets of sizes up to and exceeding the predicted value of n<sub>crit</sub> and verified that after all collections of size n<sub>crit</sub> have been tested, no further errors are discovered. Data coverage was also compared with statement coverage testing and random test data set generation. The three testing techniques were compared for effectiveness at revealing errors compared to the number of test data sets used. Statement coverage testing was confirmed as the cheapest, in the sense that it produces its maximal effect for the smallest number of tests applied, but the least effective technique in terms of numbers of errors uncovered. Data coverage was significantly better than random test generation: it uncovered more faults with fewer tests at every point.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183018,no,undetermined,0
An algorithm for dividing ambiguity sets for analog fault dictionary,"A new algorithm for dividing ambiguity sets based on the lowest error probability for analog fault dictionary is proposed. The problem of tolerance affecting diagnostic accuracy in analog circuits is discussed. A statistical approach is used to derive the probability distribution of the tolerances of the output signal characteristics both in the absence and in the presence of faults in the circuit. For example, in this paper, Monte Carlo technique has been applied for the analysis of tolerance. The lowest error probabilities are computed according to Bayesian strategy. Using the PSpice software package, a detailed simulation program was developed to implement the proposed technique. The simulation software was packaged and then integrated with a symbolic analysis program that divides the ambiguity sets and structure the software package for the analysis before testing in the fault dictionary. Furthermore, the proposed approach can be easily extended to select the testing nodes leading to the selection of optimized nodes for the analog fault diagnosis.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1187163,no,undetermined,0
Assessment design for mathematics web project-based learning,"Four mathematics project-based learning activities were carried out on a web environment in this study. Several mindtools such as dynamic geometry scratch pad, concept mapping and Powerpoint were also adopted as learning aids. Forty second to fifth grade elementary school students from Tainan and Kaohsiung cities were included. They registered as the collaborative learners for one year. The quality of final reports, collaborating skills and the ability of using mindtools for each group were assessed while they were doing mathematics project learning through Internet collaboration. Based upon the process-oriented assessment design, students' learning progress was discussed in detail. Overall, the inter-rater reliability of the three assignment designs is between 0.84 and 0.92. These flexible structured and process oriented assessment designs would be of great potential to monitor the web collaborative learning.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1186235,no,undetermined,0
AADSS: Agent-Based Adaptive Dynamic Semantic Web Service Selection,"At present, Web Services invocation follows the ldquobind-once-invoke-many-timesrdquo pattern, while the Web Services run in a failure prone environment. Therefore, Web Services are not actually used on a large scale in real business world which calls for high robustness and trustworthiness. To solve this problem we propose an Agent-based Adaptive Dynamic Semantic Web Service Selection framework, called AADSS, in which Web Service consumers can dynamically select the ldquorightrdquo service, and adaptively change the bound services according to the real-time conditions such as QoS properties values, service reputation ranking scores and so on. We also describe a four-phase semantic web service selection strategy for this dynamic selection.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700386,no,undetermined,0
Reliability evaluation of multi-state systems subject to imperfect coverage using OBDD,"This paper presents an efficient approach based on OBDD for the reliability analysis of a multi-state system subject to imperfect fault-coverage with combinatorial performance requirements. Since there exist dependencies between combinatorial performance requirements, we apply the multi-state dependency operation (MDO) of OBDD to deal with these dependencies in a multi-state system. In addition, this OBDD-based approach is combined with the conditional probability methods to find solutions for the multi-state imperfect coverage models. Using conditional probabilities, we can also apply this method for modular structures. The main advantage of this algorithm is that it will take computational time that is equivalent to the same problem without assuming imperfect coverage (i.e. with perfect coverage). This algorithm is very important for complex systems such as fault-tolerant computer systems, since it can obtain the complete results quickly and accurately even when there exist a number of dependencies such as shared loads (reconfiguration), degradation and common-cause failures.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1185638,no,undetermined,0
Hardware/software co-reliability of configurable digital systems,"This paper investigates the co-effect of hardware and software on the reliability as measured by quality level (or defect level) of configurable multichip module (CMCM) systems. Hardware architecture of CMCM can be configured to accommodate target application design. An application, as provided in a form of software, is partitioned and mapped on the provided configurable hardware. Granularity of an application can be used as a criteria of partitioning and mapping, and can determine the utilization pattern of hardware resources. The utilization pattern of CMCM determines the configuration strategy of available hardware resources based on the application's granularity. Different utilization patterns of an application design on CMCM may result in various impacts on escape tolerance (i.e. the probability to avoid inclusion of hardware resources in the configuration that escaped from testing). A quality level model of CMCM is proposed to capture and trace the co-effect of hardware and software, referred to as co-reliability, with respect to escape-tolerance. Various configuration strategies are proposed and evaluated against various criterion granularity and utilization distributions based on the proposed models and evaluation techniques. Extensive analytical and parametric simulation results are shown.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1185620,no,undetermined,0
2D automated visual inspection system for the remote quality control of SMD assembly,"In this paper, we present a general description of a new automated visual inspection system designed with a mechatronic approach, to address the problem of quality control in a SMT assembly process line. The system provides hardware and software facilities to be used as a test bench for new algorithms related to inspection and decision making, and it allows for remote access. We include a description of our first application to detect presence/absence or misplace of surface mounted devices using 2D analysis and 3D reconstruction. Also, we describe the remote access package developed with JiniTM technologies to integrate our system to the JiniTM Virtual Manufacturing Lab. This AVI system was created as a part of the Mexico-USA project in manufacturing research, MANET.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1185317,no,undetermined,0
Industrial utilization of linguistic equations for defect detection on printed circuit boards,"This paper describes how linguistic equations, an intelligent method derived from fuzzy algorithms, have been used in a decision-helping tool for electronic manufacturing. In our case the company involved in the project, PKC Group, is mainly producing control cards for the automotive industry. In their business, nearly 70 percent of the cost of a product is material cost. Detecting defects and repairing the printed circuit boards is therefore a necessity. With an ever increasing complexity of the products, defects are very likely to occur, no matter how much attention is put into their prevention. That's the reason why the system described in this paper comes into use only during the final testing of the product and is purely oriented towards the detection and localization of defects. Final control is based on functional testing. Using linguistic equations and expert knowledge, the system is able to analyze that data and successfully detect and trace a defect into a small area of the printed circuit board. If sufficient amount of data is provided, self-tuning and self-learning methods can be used. Diagnosis effectiveness can therefore be improved from detection of a functional area towards component level analysis.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1185259,no,undetermined,0
Reliability test of MESFETs in presence of hot electrons,"Temperature profiles of hot electrons were modeled in MESFETs undergoing stress tests, where the gate voltage was close to pinch-off and the drain voltage was slightly lower than breakdown. These profiles were compared with the results of degradation during the stress. We present the results of two-terminal hot electron stress on MESFETs, and discuss the probability of various defect formations resulting from this stress.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996641,no,undetermined,0
Testing of analogue circuits via (standard) digital gates,"The possibility of using window comparators for on-chip (and potentially on-line) response evaluation of analogue circuits is investigated. No additional analogue test inputs are required and the additional circuitry can be realised either by means of standard digital gates taken from an available library or by full custom designed gates to obtain an observation window tailored to the application. With this approach, the test overhead can be kept extremely low. Due to the low gate capacitance also the load on the observed nodes is very low. Simulation results for some examples show that 100% of all assumed layout-realistic faults could be detected.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996709,no,undetermined,0
VC rating and quality metrics: why bother? [SoC],"System-on-a-chip (SoC) is the paramount challenge of the electronic industry for the next millennium. The semiconductor industry has delivered what we were expecting and what was predicted: silicon availability for over 10 million gates. The VSIA (Virtual Socket Initiative Alliance) has defined industry standards and data formats for SoC. The reuse methodology manual, first 'how-to-do' book to create reusable IPs (intellectual properties) for SoC designs has been published. EDA tool providers understand the issues and are proposing new tools and solutions on a quarterly basis. The last stage needs to be run: consolidate the experience and know-how of VSIA and IP OpenMORE rating system into an industry adopted VC (virtual component) quality metrics, and then pursue to tackle the next challenges: formal system specifications and VC transfer infrastructure. The objective of this paper is to set the stage for the final step towards a VC quality metrics effort that the industry needs to adopt, and define the next achievable goals.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996745,no,undetermined,0
"The impact of pair programming on student performance, perception and persistence","This study examined the effectiveness of pair programming in four lecture sections of a large introductory programming course. We were particularly interested in assessing how the use of pair programming affects student performance and decisions to pursue computer science related majors. We found that students who used pair programming produced better programs, were more confident in their solutions, and enjoyed completing the assignments more than students who programmed alone. Moreover, pairing students were significantly more likely than non-pairing students to complete the course, and consequently to pass it. Among those who completed the course, pairers performed as well on the final exam as non-pairers, were significantly more likely to be registered as computer science related majors one year later, and to have taken subsequent programming courses. Our findings suggest that not only does pairing not compromise students' learning, but that it may enhance the quality of their programs and encourage them to pursue computer science degrees.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201243,no,undetermined,0
Rule-Based Maintenance of Post-Requirements Traceability Relations,"An accurate set of traceability relations between software development artifacts is desirable to support evolutionary development. However, even where an initial set of traceability relations has been established, their maintenance during subsequent development activities is time consuming and error prone, which results in traceability decay. This paper focuses solely on the problem of maintaining a set of traceability relations in the face of evolutionary change, irrespective of whether generated manually or via automated techniques, and it limits its scope to UML-driven development activities post-requirements specification. The paper proposes an approach for the automated update of existing traceability relations after changes have been made to UML analysis and design models. The update is based upon predefined rules that recognize elementary change events as constituent steps of broader development activities. A prototype traceMaintainer has been developed to demonstrate the approach. Currently, traceMaintainer can be used with two commercial software development tools to maintain their traceability relations. The prototype has been used in two experiments. The results are discussed and our ongoing work is summarized.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4685649,no,undetermined,0
Software fault tolerance of distributed programs using computation slicing,"Writing correct distributed programs is hard. In spite of extensive testing and debugging, software faults persist even in commercial grade software. Many distributed systems, especially those employed in safety-critical environments, should be able to operate properly even in the presence of software faults. Monitoring the execution of a distributed system, and, on detecting a fault, initiating the appropriate corrective action is an important way to tolerate such faults. This gives rise to the predicate detection problem which involves finding a consistent cut of a distributed computation, if it exists, that satisfies the given global predicate. Detecting a predicate in a computation is, however, an NP-complete problem. To ameliorate the associated combinatorial explosion problem, we introduce the notion of computation slice in our earlier papers [5, 10]. Intuitively, slice is a concise representation of those consistent cuts that satisfy a certain condition. To detect a predicate, rather than searching the state-space of the computation, it is much more efficient to search the state-space of the slice. In this paper we provide efficient algorithms to compute the slice for several classes of predicates. Our experimental results demonstrate that slicing can lead to an exponential improvement over existing techniques in terms of lime and space.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1203457,no,undetermined,0
Using Goal-Oriented Requirements Engineering for Improving the Quality of ISO/IEC 15504 based Compliance Assessment Frameworks,Within the context of business processes design and deployment we introduce and illustrate the use of goal models for capturing compliance requirements applicable over business processes configurations. More specifically we explain how a goal-oriented approach can be used together with the ISO/IEC 15504 standard in order to provide a formal framework according to which the compliance of business processes against regulations and their associated requirements can be assessed and measured. The overall approach is discussed and illustrated through the handling of a real business case related to the Basel II Accords on operational risk management in the financial sector.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4685650,no,undetermined,0
Electromagnetic environment analysis of a software park near transmission lines,"The electromagnetic environments (EMEs) of planned Zhongguancun Software Park near transmission lines, including electrical field, magnetic field and grounding potential rise under three cases of lightning, normal operation and short circuit faults, are assessed by numerical analysis. The power frequency electromagnetic environments of the software park are below the maximum ecological allowed exposure values for the general public, nevertheless the power frequency magnetic field may interfere with the sensitive computer display unit. The influence of short-circuit fault in two different cases of remote short-circuit and neighboring short-circuit on the software park was discussed. The main problem we must pay attention to is the ground potential rise in the software park due to neighboring short-circuit fault, it would threaten the safe operation of electronic devices in the software park. On the other hand, the lightning strike is a serious threat to the software park, protective countermeasures should be adopted to improve the electromagnetic environments of the software park.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201491,no,undetermined,0
Using Scenarios to Discover Requirements for Engine Control Systems,"Rolls-Royce control systems are complex, safety critical and developed in ever-compressed timescales. Scenario techniques are utilised during systems design, safety analysis and systems verification. Scenarios can be used to improve requirements quality and to ensure greater confidence in requirements coverage for both normal and exception behaviour. A study was undertaken to investigate whether the ART-SCENE process and tool could enable engineers to identify exception behaviours earlier in the system design process, thus reducing cost and improving quality. ART-SCENE provides automatic generation of scenarios and alternative course events through the Scenario Presenter. These recognition cues are used to prompt engineers to identify deviations that may otherwise be missed. This paper describes a comparative evaluation between ART-SCENE and a standard hazard identification technique to assess the effectiveness of this approach.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4685675,no,undetermined,0
Designing software architectures for usability,"Usability is increasingly recognized as a quality attribute that one has to design for. The conventional alternative is to measure usability on a finished system and improve it. The disadvantage of this approach is, obviously, that the cost associated with implementing usability improvements in a fully implemented system are typically very high and prohibit improvements with architectural impact. In this tutorial, we present the insights gained, techniques developed and lessons learned in the EU-IST project STATUS (SofTware Architectures That supports USability). These include a forward-engineering perspective on usability, a technique for specifying usability requirements, a method for assessing software architectures for usability and, finally, for improving software architectures for usability. The topics are extensively illustrated by examples and experiences from many industrial cases.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201273,no,undetermined,0
Using Formal Verification to Reduce Test Space of Fault-Tolerant Programs,"Testing object-oriented programs is still a hard task, despite many studies on criteria to better cover the test space. Test criteria establish requirements one want to achieve in testing programs to help in finding software defects. On the other hand, program verification guarantees that a program preserves its specification but its application is not very straightforward in many cases. Both program testing and verification are expensive tasks and could be used to complement each other.This paper presents a new approach to automate and integrate testing and program verification for fault-tolerant systems. In this approach we show how to assess information from programs verification in order to reduce the test space regarding exceptions definition/use testing criteria. As properties on exception-handling mechanisms are checked using a model checker(Java PathFinder), programs are traced. Information from these traces can be used to realize how much testing criteria have been covered, reducing the further program test space.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4685805,no,undetermined,0
Assessing test-driven development at IBM,"In a software development group of IBM Retail Store Solutions, we built a non-trivial software system based on a stable standard specification using a disciplined, rigorous unit testing and build approach based on the test-driven development (TDD) practice. Using this practice, we reduced our defect rate by about 50 percent compared to a similar system that was built using an ad-hoc unit testing approach. The project completed on time with minimal development productivity impact. Additionally, the suite of automated unit test cases created via TDD is a reusable and extendable asset that will continue to improve quality over the lifetime of the software system. The test suite will be the basis for quality checks and will serve as a quality contract between all members of the team.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201238,no,undetermined,0
Supporting Requirements Change Management in Goal Oriented Analysis,"Requirements changes frequently occur at any time of a software development process and their management is a crucial issue to develop software of high quality. Meanwhile, recently goal-oriented analysis techniques are being put into practice to elicit requirements. In this situation, the change management of goal graphs and its support is necessary. This paper presents two topics related to change management of goal graphs; 1) version control of goal graphs and 2) impact analysis on a goal graph when its modifications occur. In our version control system, we extract the differences between successive versions of a goal graph by means of monitoring modification operations performed through a goal graph editor, and store them in a repository. Our impact analysis detects conflicts that arise when a new goal is added, and investigates the achievability of the other goals when the existing goal is deleted.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4685647,no,undetermined,0
Automated support for classifying software failure reports,This paper proposes automated support for classifying reported software failures in order to facilitate prioritizing them and diagnosing their causes. A classification strategy is presented that involves the use of supervised and unsupervised pattern classification and multivariate visualization. These techniques are applied to profiles of failed executions in order to group together failures with the same or similar causes. The resulting classification is then used to assess the frequency and severity of failures caused by particular defects and to help diagnose those defects. The results of applying the proposed classification strategy to failures of three large subject programs are reported These results indicate that the strategy can be effective.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201224,no,undetermined,0
Understanding and predicting effort in software projects,"We set out to answer a question we were asked by software project management: how much effort remains to be spent on a specific software project and how will that effort be distributed over time? To answer this question we propose a model based on the concept that each modification to software may cause repairs at some later time and investigate its theoretical properties and application to several projects in Avaya to predict and plan development resource allocation. Our model presents a novel unified framework to investigate and predict effort, schedule, and defects of a software project. The results of applying the model confirm a fundamental relationship between the new feature and defect repair changes and demonstrate its predictive properties.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201207,no,undetermined,0
Improving test suites via operational abstraction,"This paper presents the operational difference technique for generating, augmenting, and minimizing test suites. The technique is analogous to structural code coverage techniques, but it operates in the semantic domain of program properties rather than the syntactic domain of program text. The operational difference technique automatically selects test cases; it assumes only the existence of a source of test cases. The technique dynamically generates operational abstractions (which describe observed behavior and are syntactically identical to formal specifications)from test suite executions. Test suites can be generated by adding cases until the operational abstraction stops changing. The resulting test suites are as small, and detect as many faults, as suites with 100% branch coverage, and are better at detecting certain common faults. This paper also presents the area and stacking techniques for comparing test suite generation strategies; these techniques avoid bias due to test suite size.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201188,no,undetermined,0
Improving web application testing with user session data,"Web applications have become critical components of the global information infrastructure, and it is important that they be validated to ensure their reliability. Therefore, many techniques and tools for validating web applications have been created. Only a few of these techniques, however, have addressed problems of testing the functionality of web applications, and those that do have not fully considered the unique attributes of web applications. In this paper we explore the notion that user session data gathered as users operate web applications can be successfully employed in the testing of those applications, particularly as those applications evolve and experience different usage profiles. We report results of an experiment comparing new and existing test generation techniques for web applications, assessing both the adequacy of the generated tests and their ability to detect faults on a point-of-sale web application. Our results show that user session data can produce test suites as effective overall as those produced by existing white-box techniques, but at less expense. Moreover the classes of faults detected differ somewhat across approaches, suggesting that the techniques may be complimentary.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201187,no,undetermined,0
Architecting for evolvability by means of traceability and features,"The frequent changes during the development and usage of large software systems often lead to a loss of architectural quality which hampers the implementation of further changes and thus the systemspsila evolution. To maintain the evolvability of such software systems, their architecture has to fulfil particular quality criteria. Available metrics and rigour approaches do not provide sufficient means to evaluate architectures regarding these criteria, and reviews require a high effort. This paper presents an approach for an evaluation of architectural models during design decisions, for early feedback and as part of architectural assessments. As the quality criteria for evolvability, model relations in terms of traceability links between feature model, design and implementation are evaluated. Indicators are introduced to assess these model relations, similar to metrics, but accompanied by problem resolution actions. The indicators are defined formally to enable a tool-based evaluation. The approach has been developed within a large software project for an IT infrastructure.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4686323,no,undetermined,0
Design method for parameterized IP generator using structural and creational design patterns,"Parameterized intellectual property (IP) core generators, which produce synthesizable hardware design based on predefined microarchitectural-level parameters, can improve the efficiency of IP reuse. However, the process of designing such IP core generators is usually time-consuming and error-prone because it has to couple SW with HW designs in one product. In this paper, we propose a pattern-oriented design method for parameterized IP core generators. The paper shows that IP core generators quality can be improved through combining structural design pattern with creational design pattern. We also demonstrate the method through a soft-decision Viterbi decoder generator application.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4688408,no,undetermined,0
Understanding change-proneness in OO software through visualization,"During software evolution, adaptive, and corrective maintenance are common reasons for changes. Often such changes cluster around key components. It is therefore important to analyze the frequency of changes to individual classes, but, more importantly, to also identify and show related changes in multiple classes. Frequent changes in clusters of classes may be due to their importance, due to the underlying architecture or due to chronic problems. Knowing where those change-prone clusters are can help focus attention, identify targets for re-engineering and thus provide product-based information to steer maintenance processes. This paper describes a method to identify and visualize classes and class interactions that are the most change-prone. The method was applied to a commercial embedded, real-time software system. It is object-oriented software that was developed using design patterns.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199188,no,undetermined,0
Restoration schemes with differentiated reliability,"Reliability of data exchange is becoming increasingly important. In addition, applications may require multiple degrees of reliability. The concept of differentiated reliability (DiR) was recently introduced in [A. Fumagalli and M. Tacca, January 2001] to provide multiple degrees of reliability in protection schemes that provision spare resources. With this paper, the authors extend the DiR concept to restoration schemes in which network resources for a disrupted connection along secondary paths are sought upon failure occurrence, i.e., they are not provisioned before the fault. The DiR concept is applied in two dimensions: restoration blocking probability i.e., the probability that the disrupted connection is not recovered due to lack of network resources - and restoration time - i.e., the time necessary to complete the connection recovery procedure. Differentiation in the two dimensions is accomplished by proposing three preemption policies that allow high priority connections to preempt resources allocated to low priority connections. The three policies trade complexity, i.e., number of preempted connections, for better reliability differentiation. Obtained results indicate that by using the proposed preemption policies, it is possible to guarantee a significant differentiation of both restoration blocking probability and restoration time. By carefully choosing the preemption policy, the desired reliability degree can be obtained, while minimizing the number of preempted connections.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1203942,no,undetermined,0
Voltage flicker calculations for single-phase AC railroad electrification systems,"Rapid load variations can cause abrupt changes in the utility voltage, so-called voltage flicker. The voltage flicker may result in light flickering and, in extreme cases, damage to electronic equipment. Electrified railroads are just one example where such rapid load variation occurs as trains accelerate, decelerate, and encounter and leave grades. For balanced loads, the voltage flicker is easily determined using per-phase analysis. AC electrification system substations operating at a commercial frequency, however, are supplied from only two phases of utility three-phase transmission system. In order to calculate the voltage flicker for such an unbalanced system, symmetrical component method needs to be used. In this paper, a procedure is developed for evaluating the effects of short-time traction load variation onto utility system. Applying the symmetrical component method, voltage flicker equations are developed for loads connected to A-B, B-C, and C-A phases of a three-phase utility system. Using a specially-developed software simulating the train and electrification system performance, loads at the traction power substation transformers are calculated in one-second intervals. Subsequently, voltages at the utility busbars are calculated for each interval, and the voltage variation from interval to interval is expressed in percent. The calculated voltage flicker is then compared to the utility accepted limits. Based on this comparison, the capability of the utility power system to support the traction loads can be assessed and the suitability of the proposed line taps for the traction power substations evaluated.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1204662,no,undetermined,0
An improvement project for distribution transformer load management in Taiwan,"This paper introduces an application program that is based on an automated mapping/facilities management/geographic information system (AM/FM/GIS) to provide information expectation, load forecasting, and power flow calculation capability in distribution systems. First, the database and related data structure used in the Taipower distribution automation pilot system is studied and thoroughly analyzed. Then, our program, developed by the AM/FM FRAMME and Visual Basic software, is integrated into the above pilot system. Moreover, this paper overcomes the weak points of the pilot system, such as difficult use, incomplete function, nonuniform sampling for billing and dispatch of bills and inability to simultaneously transfer customer data. This program can enforce the system and can predict future load growth on distribution feeders, considering the effects of temperature variation, and power needed for air-conditioners. In addition, on the basis of load density and diversity factors of typical customers, the saturation load of a new housing zone can be estimated. As for the power flow analysis, it can provide three-phase quantities of voltage drop at each node, the branch current, and the system loss. The program developed in this study can effectively aid public electric utilities in distribution system planning and operation.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1198327,no,undetermined,0
Accurate modeling and simulation of SAW RF filters,"The popularity of wireless services and the increasing demand for higher quality, new services, and the need for higher data rates will boost the cellular terminal market. Today, third generation (3G) systems exist in many metropolitan areas. In addition, wireless LAN systems, such as Bluetooth or IEEE 802.11-based systems, are emerging. The key components in the microwave section of the mobile terminals of these systems incorporate - apart from active radio frequency integrated circuits (RFICs) and RF modules - a multitude of passive components. The most unique passive components used in the microwave section are surface acoustic wave (SAW) filters. Due to the progress of integration in the active part of the systems the component count in modern terminals is decreasing. On the other hand, the average number of SAW RF filters per cellular phone is increasing due to multi-band terminals. As a consequence, the passive components outnumber the RFICs by far in today's systems. The market is demanding smaller and smaller terminals and, thus, the size of all components has to be reduced. Further reduction of component count and required PCB area is obtained by integration of passive components and SAW devices using low-temperature co-fired ceramic (LTCC). The trend or reducing the size dramatically while keeping or even improving the performance of the RF filters requires accurate software tools for the simulation of all relevant effects and interactions. In the past it was sufficient to predict the acoustic behavior on the SAW chip, but higher operating frequencies, up to 2.5 GHz, and stringent specifications up to 6 GHz demand to account for electromagnetic (EM) effects, too. The combination of accurate acoustic simulation tools together with 2.5/3D EM simulation software packages allows to predict and optimize the performance of SAW filters and SAW-based front-end modules.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1210554,no,undetermined,0
Transparent distributed threads for Java,"Remote method invocation in Java RMI allows the flow of control to pass across local Java threads and thereby span multiple virtual machines. However, the resulting distributed threads do not strictly follow the paradigm of their local Java counterparts for at least three reasons. Firstly, the absence of a global thread identity causes problems when reentering monitors. Secondly, blocks synchronized on remote objects do not work properly. Thirdly, the thread interruption mechanism for threads executing a remote call is broken. These problems make multi-threaded distributed programming complicated and error prone. We present a two-level solution: On the library level, we extend KaRMI (Philippsen et al. (2000)), a fast replacement for RMI, with global thread identities for eliminating problems with monitor reentry. Problem with synchronization on remote objects are solved with a facility for remote monitor acquisition. Our interrupt forwarding mechanism enables the application to get full control over its distributed threads. On the language level, we integrate these extensions with JavaParty's transparent remote objects (Philippsen et al. (1997)) to get transparent distributed threads. We finally evaluate our approach with benchmarks that show costs and benefits of our overall design.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1213261,no,undetermined,0
The efficient bus arbitration scheme in SoC environment,"This paper presents the dynamic bus arbiter architecture for a system on chip design. The conventional bus-distribution algorithms, such as the static fixed priority and the round robin, show several defects that are bus starvation, and low system performance because of bus distribution latency in a bus cycle time. The proposed dynamic bus architecture is based on a probability bus distribution algorithm and uses an adaptive ticket value method to solve the impartiality and starvation problems. The simulation results show that the proposed algorithm reduces the buffer size of a master by 11% and decreases the bus latency of a master by 50%.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1213054,no,undetermined,0
A mobile location-based vehicle fleet management service application,"The convergence of multiple technologies, including the Internet, wireless communications, geographic information system, location technologies, and mobile devices, has given rise to new types of information utilities that may be referred as mobile location-based services (LBS). LBS can be described as applications that exploit knowledge about where an information device (user) is located. For example, location information can be used to provide automobile drivers with optimal routes to a geographic destination. Specifically, we examine a context where the people need to move physically from one location to another via taxis. In this scenarios the user is in control of the location information associated with the mobile device. However, problems arise when a fleet management application use that dynamic information to provide the best taxi assignment. This paper presents a new approach to the taxi assignment problem in a mobile environment based on optimization and simulation. Our specific interest is to predict the impact of the interaction between the assignment algorithm and fleet management in the mobile environment on the desired quality of service for the mobile users. The paper also discusses the current approach and limitations of the new approach and some simulation results, such as average transport time and unserved mobile users average. Finally, we present some conclusions.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1212877,no,undetermined,0
Concurrent fault detection in a hardware implementation of the RC5 encryption algorithm,"Recent research has shown that fault diagnosis and possibly fault tolerance are important features when implementing cryptographic algorithms by means of hardware devices. In fact, some security attack procedures are based on the injection of faults. At the same time, hardware implementations of cryptographic algorithms, i.e. crypto-processors, are becoming widespread. There is however, only very limited research on implementing fault diagnosis and tolerance in crypto-algorithms. Fault diagnosis is studied for the RC5 crypto-algorithm, a recently proposed block-cipher algorithm that is suited for both software and hardware implementations. RC5 is based on a mix of arithmetic and logic operations, and is therefore a challenge for fault diagnosis. We study fault propagation in RC5, and propose and evaluate the cost/performance tradeoffs of several error detecting codes for RC5. Costs are estimated in terms of hardware overhead, and performances in terms of fault coverage. Our most important conclusion is that, despite its nonuniform nature, RC5 can be efficiently protected by using low-cost error detecting codes.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1212865,no,undetermined,0
Modeling and measurements of novel high k monolithic transformers,"This paper presents modeling and measurements of a novel monolithic transformer with high coupling k and quality factor Q characteristics. The present transformer utilizes a Z-shaped multilayer metalization to increase k without sacrificing Q. The new transformer has been fabricated using Motorola 0.18 micron copper process. A simple 2-port lumped circuit model is used to model the new design. Experimental data shows a good agreement with predicted data obtained from an HFSS software simulator. An increase of about 10% in mutual coupling and 15% in Q has been achieved. For a modest increase in k of about 5%, Q can be increased by up to 20%.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1212595,no,undetermined,0
Event-Based Data Dissemination on Inter-Administrative Domains: Is it Viable?,"Middleware for timely and reliable data dissemination is a fundamental building block of the event driven architecture (EDA), an ideal platform for developing air traffic control, defense systems, etc. Many of these middlewares are compliant to the data distribution service (DDS) specification and they have been traditionally designed to be deployed on managed environments where they show predictable behaviors. However, the enterprise setting can be unmanaged and characterized by geographic inter-domain scale and heterogeneous resources. In this paper we present a study aimed at assessing the strengths and weaknesses of a commercial DDS implementation deployed on an unmanaged setting. Our experiments campaign outlines that, if the application manages a small number of homogeneous resources, this middleware perform timely and reliably. In a more general setting with fragmentation and heterogeneous resources, reliability and timeliness rapidly degenerate pointing out a need of research in self-configuring scalable event dissemination with QoS guarantee on unmanaged settings.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4683113,no,undetermined,0
A birth-process approach to Moranda's geometric software-reliability model,"To alleviate some of the objections to the basic Jelinski Moranda (JM) model for software failures, Moranda proposed a geometric de-eutrophication model. This model assumes that the times between failures are statistically-independent exponential random variables with given failure rates. In this model the failure rates decrease geometrically with the detection of a fault. Using an intuitive approach, Musa, Iannino, Okumoto , see also Farr , derived expressions for the mean and the intensity functions of the process N (t) which counts the number of faults detected in the time interval [O, t] for the Moranda geometric de-eutrophication model. N (t) is studied as a pure birth stochastic process; its probability generating function is derived, as well as its mean, intensity and reliability functions. The expressions for the mean and intensity functions derived by MIO are only approximations and can be quite different from the true functions for certain choices of the failure rates. The exact expressions for the mean function and the intensity function of N (t) are used to find the optimum release time of software based on a cost structure for Moranda's geometric de-eutrophication model.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1211107,no,undetermined,0
Model checking for probability and time: from theory to practice,"Probability features increasingly often in software and hardware systems: it is used in distributed coordination and routing problems, to model fault-tolerances and performance, and to provide adaptive resource management strategies. Probabilistic model checking is an automatic procedure for establishing if a desired property holds in a probabilistic specifications such as """"leader election is eventually resolved with probability 1"""", """"the chance of shutdown occurring is at most 0.01%"""", and """"the probability that a message will be delivered within 30ms is at least 0.75"""". A probabilistic model checker calculates the probability of a given temporal logic property being satisfied, as opposed to validity. In contrast to conventional model checkers, which rely on reachability analysis of the underlying transition system graph, probabilistic model checking additionally involves numerical solutions of linear equations and linear programming problems. This paper reports our experience with implementing PRISM (www.cs.bham.ac.uk/dxp/prism), a probabilistic symbolic model checker, demonstrates its usefulness in analyzing real-world probabilistic protocols, and outlines future challenges for this research direction.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1210075,no,undetermined,0
Fault detection for OSPF based E-NNI routing with probabilistic testing algorithm,"In this paper, a probabilistic testing algorithm is proposed to increase the fault coverage for OSPF based E-NNI routing protocol testing. It automatically constructs random network topologies and checks database information consistency with real optical network topology and resource for each generated topology. Theoretical analysis indicates that our algorithm can efficiently increase the fault coverage. This algorithm has been implemented in a software test tool called E-NNI Routing Testing System (ERTS). Experiment results based on ERTS are also reported.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4685145,no,undetermined,0
Quality-based auto-tuning of cell uplink load level targets in WCDMA,"The objective of this paper was to validate the feasibility of auto-tuning WCDMA cell uplink load level targets based on quality of service. The uplink cell load level was measured with received wideband total power. The quality indicators used were called blocking probability, packet queuing probability and degraded block error ratio probability. The objective was to improve performance and operability of the network with control software aiming for a specific quality of service. The load level targets in each cell were regularly adjusted with a control method in order to improve performance. The approach was validated using a dynamic WCDMA system simulator. The conducted simulations support the assumption that the uplink performance can be managed and improved by the proposed cell-based automated optimization.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1208913,no,undetermined,0
Asymptotic insensitivity of least-recently-used caching to statistical dependency,"We investigate a widely popular least-recently-used (LRU) cache replacement algorithm with semiMarkov modulated requests. SemiMarkov processes provide the flexibility for modeling strong statistical correlation, including the broadly reported long-range dependence in the World Wide Web page request patterns. When the frequency of requesting a page n is equal to the generalized Zipf's law c/n<sup></sup>,  > 1, our main result shows that the cache fault probability is asymptotically, for large cache sizes, the same as in the corresponding LRU system with i.i.d. requests. This appears to be the first explicit average case analysis of LRU caching with statistically dependent request sequences. The surprising insensitivity of LRU caching performance demonstrates its robustness to changes in document popularity. Furthermore, we show that the derived asymptotic result and simulation experiments are in excellent agreement, even for relatively small cache sizes. The potential of using our results in predicting the behavior of Web caches is tested using actual, strongly correlated, proxy server access traces.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1208695,no,undetermined,0
"Modelling a secure, mobile, and transactional system with CO-OPN","Modelling complex concurrent systems is often difficult and error-prone, in particular when new concepts coming from advanced practical applications are considered. These new application domains include dynamicity, mobility, security, and localization dependent computing. In order to fully model and prototype such systems we propose to use several concepts existing in our specification language CO-OPN, like context, dynamicity, mobility, subtyping, and inheritance. CO-OPN (concurrent object oriented Petri net) is a formal specification language for modelling distributed systems; it is based on coordinated algebraic Petri nets. We focus on the use of several basic mechanisms of CO-OPN for modelling mobile systems and the generation of corresponding Java code. A significant example of distributors accessible through mobile devices (for example, PDA with Bluetooth) is fully modelled and implemented with our technique.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207702,no,undetermined,0
A 2-level call admission control scheme using priority queue for decreasing new call blocking & handoff call dropping,"In order to provide a fast moving mobile host (MH) supporting multimedia applications with a consistent quality of service (QoS), an efficient call admission mechanism is in need. This paper proposes the 2-level call admission (2LCAC) scheme based on a call admission scheme using the priority to guarantee the consistent QoS for mobile multimedia applications. The 2LCAC consists of the basic call admission and advanced call admission; the former determines call admission based on bandwidth available in each cell and the latter determines call admission by utilizing delay tolerance time (DTT) and priority queue (PQueue) algorithms. In order to evaluate the performance of our scheme, we measure the metrics such as the blocking probability of new calls, dropping probability of handoff calls and bandwidth utilization. The result shows that the performance of our scheme is superior to that of existing schemes such as complete sharing policy (CSP), guard channel policy (GCP) and adaptive guard channel policy (AGCP).",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207582,no,undetermined,0
Practical code inspection techniques for object-oriented systems: an experimental comparison,"Although inspection is established as an effective mechanism for detecting defects in procedural systems, object-oriented systems have different structural and execution models. This article describes the development and empirical investigation of three different techniques for reading OO code during inspection.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207450,no,undetermined,0
Human-Intention Driven Self Adaptive Software Evolvability in Distributed Service Environments,"Evolvability is essential to adapting to the dynamic and changing requirements in response to the feedback from context awareness systems. However, most of current context models have limited capability in exploring human intentions that often drive system evolution. To support service requirements analysis of real-world applications in distributed service environments, this paper focuses on human-intention driven software evolvability. In our approach, requirements analysis via an evolution cycle provides the means of speculating requirement changes, predicting possible new generations of system behaviors, and assessing the corresponding quality impacts. Furthermore, we also discuss evolvability metrics by observing intentions from user contexts.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4683114,no,undetermined,0
Tight bounded localization of facial features with color and rotational independence,"Human face detection plays an important role in applications such as video surveillance, human computer interface, face recognition and face image database management. In this paper, we propose a novel facial feature detection algorithm for various face image types, conditions, invariant rotation, and any appearances. There are three main steps. First, Radon transform is used for face angle detection on rotated image. Subsequently, the feature regions are detected using Neural Visual Model (NVM). Finally, using image dilation and Radon transform, the facial features are extracted from the detected regions. Input parameters are obtained from the face characteristics and the positions of facial features not including any intensity informations. Our algorithm is successfully tested with various types of faces which are color images, gray images, binary images, wearing the sunglasses, wearing the scarf, lighting effect, low-quality images, color and sketch images from animated cartoon, rotated face images, and rendered face images.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1206436,no,undetermined,0
A study of the effect of imperfect debugging on software development cost,"It is widely recognized that the debugging processes are usually imperfect. Software faults are not completely removed because of the difficulty in locating them or because new faults might be introduced. Hence, it is of great importance to investigate the effect of the imperfect debugging on software development cost, which, in turn, might affect the optimal software release time or operational budget. In this paper, a commonly used cost model is extended to the case of imperfect debugging. Based on this, the effect of imperfect debugging is studied. As the probability of perfect debugging, termed testing level here, is expensive to be increased, but manageable to a certain extent with additional resources, a model incorporating this situation is presented. Moreover, the problem of determining the optimal testing level is considered. This is useful when the decisions regarding the test team composition, testing strategy, etc., are to be made for more effective testing.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199075,no,undetermined,0
Testable design and testing of micro-electro-fluidic arrays,"The testable design and testing of a fully software-controllable lab-on-a-chip, including a fluidic array of FlowFETs, control and interface electronics is presented. Test hardware is included for detecting faults in the DMOS electro-fluidic interface and the digital parts. Multidomain fault modeling and simulation shows the effects of faults in the (combined) fluidic and electrical parts. The fault simulations also reveal important parameters of multi-domain test-stimuli, e.g. fluid velocity, for detecting both electrical and fluidic defects.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1197681,no,undetermined,0
Improving the efficiency and quality of simulation-based behavioral model verification using dynamic Bayesian criteria,"In order to improve the effectiveness of simulation-based behavioral verification, it is important to determine when to stop the current test strategy and to switch to an expectantly more rewarding test strategy. The location of a stopping point is dependent on the statistical model one chooses to describe the coverage behavior during verification. In this paper, we present dynamic Bayesian (DB) and confidence-based dynamic Bayesian (CDB) stopping rules for behavioral VHDL model verification. The statistical assumptions of the proposed stopping rules are based on experimental evaluation of probability distribution functions and correlation functions. Fourteen behavioral VHDL models were experimented with to determine the high efficiency of the proposed stopping rules over the existing ones. Results show that the DB and the CDB stopping rules outperform all the existing stopping rules with an average improvement of at least 69% in coverage per testing patterns used.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996761,no,undetermined,0
Model-based programming of intelligent embedded systems and robotic space explorers,"Programming complex embedded systems involves reasoning through intricate system interactions along lengthy paths between sensors, actuators, and control processors. This is a challenging, time-consuming, and error-prone process requiring significant interaction between engineers and software programmers. Furthermore, the resulting code generally lacks modularity and robustness in the presence of failure. Model-based programming addresses these limitations, allowing engineers to program reactive systems by specifying high-level control strategies and by assembling commonsense models of the system hardware and software. In executing a control strategy, model-based executives reason about the models """"on the fly,"""" to track system state, diagnose faults, and perform reconfigurations. This paper develops the reactive model-based programming language (RMPL) and its executive, called Titan. RMPL provides the features of synchronous, reactive languages, with the added ability of reading and writing to state variables that are hidden within the physical plant being controlled. Titan executes an RMPL program using extensive component-based declarative models of the plant to track states, analyze anomalous situations, and generate novel control sequences. Within its reactive control loop, Titan employs propositional inference to deduce the system's current and desired states, and it employs model-based reactive planning to move the plant from the current to the desired state.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173215,no,undetermined,0
Integrate hardware/software device testing for use in a safety-critical application,"In train and transit applications, the occurrence of a single hazard (fault) may be quite catastrophic resulting in significant societal costs, ranging from loss of life to major asset damages. The axiomatic safety-critical assessment process (ASCAP) has been demonstrated as a competent method for assessing the risk associated with train and transit systems. ASCAP concurrently simulates the movement of n-trains within a given system from the perspective of the individual trains. During simulation, each train interacts with a series of appliances that are located along the track, within the trains and at a central office. Within ASCAP, each appliance is represented by a probabilistic multistate model, whose state selection is decided using a Monte Carlo process. In lieu of exercising this multistate model for a given appliance, the ASCAP methodology supports the inclusion of actual appliances within the simulation platform. Hence, an appliance can be fault tested in a simulation environment that emulates the actual operational environment to which it will be exposed. The ASCAP software can interface with a given appliance through an input/output (I/O) node contained within its executing platform. This node provides the ASCAP software with the capability of communicating with an external device, such as a track or an onboard appliance. When a train intersects with a particular appliance, the actual appliance can be queried by the ASCAP simulator to ascertain its status. This state information can then be used by ASCAP in lieu of its multi-state model representation of the appliance. This simulation process provides a mechanism to determine the appliance's ability to perform its intended safety-critical function in the presence of hardware/software design faults within its intended operational environment. By being able to quantify these effects prior to deploying a new appliance, credible and convincing evidences can be prepared the to ensure that overall system safety will not be adversely impacted.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181914,no,undetermined,0
Robust reliability design of diagnostic systems,"Diagnostic systems are software-based built-in-test systems which detect, isolate and indicate the failures of the prime systems. The use of diagnostic systems reduces the losses due to the failures of the prime systems and facilitates subsequent repairs. Thus diagnostic systems have found extensive applications in industry. The algorithms performing operations for diagnosis are important parts of the diagnostic systems. If the algorithms are not adequately designed, the systems will be sensitive to noise sources, and commit type I error (a) and type II error (). This paper is to improve the robustness and reliability of the diagnostic systems through robust design of the algorithms by using reliability as an experimental response. To conduct the design, we define the reliability and robustness of the systems, and propose their metrics. The influences of  and  errors on reliability are evaluated and discussed. The effects of noise factors on robustness are assessed. The classical P-diagram is modified; a generic P-diagram containing both prime and diagnostic systems is created. Based on the proposed dynamic reliability metric, we describe the steps for robust reliability design and develop a method for experimental data analysis. The robustness and reliability of the diagnostic systems are maximized by choosing optimal levels of algorithm parameters. An automobile example is presented to illustrate how the proposed design method is used. The example shows that the method is efficient in defining, measuring and building robustness and reliability.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181899,no,undetermined,0
Optimal cost-effective design of parallel systems subject to imperfect fault-coverage,"Computer-based systems intended for critical applications are usually designed with sufficient redundancy to be tolerant of errors that may occur. However, under imperfect fault-coverage conditions (such as the system cannot adequately detect, locate, and recover from faults and errors in the system), system failures can result even when adequate redundancy is in place. Because parallel architecture is a well-known and powerful architecture for improving the reliability of fault-tolerant systems, this paper presents the cost-effective design policies of parallel systems subject to imperfect fault-coverage. The policies are designed by considering (1) cost of components, (2) failure cost of the system, (3) common-cause failures, and (4) performance levels of the system. Three kinds of cost functions are formulated considering that the total average cost of the system is based on: (1) system unreliability, (2) failure-time of the system, and (3) total processor-hours. It is shown that the MTTF (mean time to failure) of the system decreases by increasing the spares beyond a certain limit. Therefore, this paper also presents optimal design policies to maximize the MTTF of these systems. The results of this paper can also be applied to gracefully degradable systems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181898,no,undetermined,0
Implementation and control of grid connected AC-DC-AC power converter for variable speed wind energy conversion system,"30 kW electrical power conversion system is developed for a variable speed wind turbine system. In the wind energy conversion system (WECS) a synchronous generator converts the mechanical energy into electrical energy. As the voltage and frequency of generator output vary along the wind speed change, a DC-DC boosting chopper is utilized to maintain constant DC link voltage. The input DC current is regulated to follow the optimized current reference for maximum power point operation of turbine system. Line side PWM inverter supply currents into the utility line by regulating the DC link voltage. The active power is controlled by q-axis current whereas the reactive power can be controlled by d-axis current. The phase angle of utility voltage is detected using software PLL (phased locked loop) in d-q synchronous reference frame. Proposed scheme gives a low cost and high quality power conversion solution for variable speed WECS.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1179207,no,undetermined,0
Identifying extensions required by RUP (rational unified process) to comply with CMM (capability maturity model) levels 2 and 3,"This paper describes an assessment of the rational unified process (RUP) based on the capability maturity model (CMM). For each key practice (KP) identified in each key process area (KPA) of CMM levels 2 and 3, the Rational Unified Process was assessed to determine whether it satisfied the KP or not. For each KPA, the percentage of the key practices supported was calculated, and the results were tabulated. The report includes considerations about the coverage of each key process area, describing the highlights of the RUP regarding its support for CMM levels 2 and 3, and suggests where an organization using it will need to complement it to conform to CMM. The assessment resulted in the elaboration of proposals to enhance the RUP in order to satisfy the key process areas of CMM. Some of these are briefly described in this article.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1178058,no,undetermined,0
Formal behavioural synthesis of Handel-C parallel hardware implementations from functional specifications,"Enormous improvements in efficiency can be achieved through exploiting parallelism and realizing implementation in hardware. On the other hand, conventional methods for achieving these improvements are traditionally costly, complex and error prone. Two significant advances in the past decade have radically changed these perceptions. Firstly, the FPGA, which gives us the ability to reconfigure hardware through software, dramatically reducing the costs of developing hardware implementations. Secondly, the language Handel-C with primitive explicit parallelism which can compile programs down to an FPGA. In this paper, we build on these recent technological advances and present a systematic approach of behavioural synthesis. Starting with an intuitive high level functional specification of a problem, given without annotation of parallelism, the approach aims at deriving an efficient parallel implementation in Handel-C, which is subsequently compiled into a circuit implemented on reconfigurable hardware. Algebraic laws are systematically used for exposing implicit parallelism and transforming the specification into a collection of interacting components. Formal methods based on data refinement and a small library of higher order functions are then used to derive behavioural description in Handel-C of each component. A small case study illustrates the use of this approach.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1174808,no,undetermined,0
Assessing the quality of a cross-national e-government Web site: a study of the forum on strategic management knowledge exchange,"As organizations have begun increasingly to communicate and interact with consumers via the Web, so the appropriate design of offerings has become a central issue. Attracting and retaining consumers requires acute understanding of the requirements of users and appropriate tailoring of solutions. Recently, the development of Web offerings has moved beyond the commercial domain to government, both national and international. In this paper, we examine the results of a quality survey of a cross-national e-government Web site provided by the OECD. The site is examined before and after a major redesign process. The instrument, WebQual, draws on previous work in three areas: Web site usability, information quality, and service interaction quality to provide a rounded framework for assessing e-commerce and e-government offerings. The metrics and findings demonstrate not only the strengths and weaknesses of the sites before and after design, but the very different impressions of users in different member countries. These findings have implications for cross-national e-government Web site offerings.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1174306,no,undetermined,0
Design of intelligent testing device for airplane navigation radar,"Modern airborne radar systems are very complex electronic equipment systems, high reliability is demanded, and fine functions of automatic detect is needed for guarantee. In this dissertation, we have studied the detect method of a new kind of airborne radar systems. With Embed machinery control as its core, adoption unit wooden blocks type construction, examining the faults of radar one by one by exciting the fault models input and checking out the response, to get the faults localized. Being programmed by Visual Basic6.0, the software can be enlarged and advanced and it provides the users with an intelligent and automatic testing environment and amicable interface. Under the guidance of testing interface, testers can complete the fault localization of radar circuit automatically. By testing, this intelligent and synthesis detect system holds well-found function advanced techniques and predominant capabilities. It can proceed the all-directions performance test to airplane navigation radar. So it has important significance for ensuring flight safety and increasing combat effectiveness.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694806,no,undetermined,0
Sahinoglu-Libby (SL) probability density function-component reliability applications in integrated networks,"The forced outage ratio of a hardware (or a software) component is defined as the failure rate divided by the sum of the failure and the repair rates. The probability density function (PDF) of the FOR is a three-parameter beta distribution (G3B), renamed to be the Sahinoglu-Libby (SL) probability distribution that was pioneered in 1981. The failure and repair rates are assumed to be the generalized gamma variables where the corresponding shape and scale parameters, respectively, are unequal. A three-parameter beta or G3B PDF, equivalent to an FOR PDF, renamed to be the SL, is shown to default to an ordinary two-parameter beta PDF when the shape parameters are identical. Furthermore, the authors will present a wide perspective of the usability and limitations of the said PDF in theoretical and practical terms, also referring to work done by some other authors in the area. In the new era of quality and reliability, the usage of the SL will assist studies in correctly formulating the PDF of the unavailability or availability random variables to estimate the network reliability and quality indices for engineering and utility considerations. Bayesian methodology is employed to compute small-sample estimators by using informative and noninformative priors for the component failure and repair rates in terms of loss functions, as opposed to the uncontested and erroneous usage of the mle, regardless of the inadequacy of the historical data. Case studies illustrate a phenomenon of overestimation of the availability index in safety and time critical components as well as in systems, when mle is conventionally employed. This work assists the network planners, and analysts, like those of Internet Service Providers, providing a targeted reliability measure of their integrated computer network in a quality-conscious environment under the pressure of an ever-expanding demand and a risk, that needs to be mitigated.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181939,no,undetermined,0
Automatic QoS control,"User sessions, usually consisting of sequences of consecutive requests from customers, comprise most of an e-commerce site's workload. These requests execute e-business functions such as browse, search, register, login, add to shopping cart, and pay. Once we properly understand and characterize a workload, we must assess its effect on the site's quality of service (QoS), which is defined in terms of response time, throughput, the probability that requests will be rejected, and availability. We can assess an e-commerce site's QoS in many different ways. One approach is by measuring the site's performance, which we can determine from a production site using a real workload or from a test site using a synthetic workload (as in load testing). Another approach consists of using performance models. I look at the approach my colleagues at George Mason and I took that uses performance models in the design and implementation of automatic QoS controller for e-commerce sites.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167347,no,undetermined,0
On the relation between design contracts and errors: a software development strategy,"When designing a software module or system, a systems engineer must consider and differentiate between how the system responds to external and internal errors. External errors cannot be eliminated and must be tolerated by the system, while the number of internal errors should be minimized and the resulting faults should be detected and removed. This paper presents a development strategy based on design contracts and a case study of an industrial project in which the strategy was successfully applied. The goal of the strategy is to minimize the number of internal errors during the development of a software system while accommodating external errors. A distinction is made between weak and strong contracts. These two types of contracts are applicable to external and internal errors, respectively. According to the strategy, strong contracts should be applied initially to promote the correctness of the system. Before releasing, the contracts governing external interfaces should be weakened and error management of external errors enabled. This transformation of a strong contract to a weak one is harmless to client modules",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=999829,no,undetermined,0
Flexible development of dependability services: an experience derived from energy automation systems,"This paper describes an approach for the flexible development of dependable automation services starting from their requirements. The approach is presented through the use of a case study in the field of energy automation systems. The approach is based on the use of a custom compositional recovery language that allows one to achieve, in software, a flexible and dependable solution for the specified requirements. The qualitative and quantitative properties of different configurations of the solution are then assessed by modelling, using stochastic Petri nets",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=999826,no,undetermined,0
Bottom up approach to enhance top level SoC verification,SoCs today rely heavily on behavioral models of analog circuits for Top Level Verification. The minimum modeling requirement is to model the functional behavior of the circuit. A lot of ongoing work is also focused on modeling analog circuits to predict the system performance of the SoC. This paper presents a methodology to enhance the quality of SoC verification by using a bottom up approach to verify the equivalence of building blocks and then work at higher levels to increase coverage. It is shown that this methodology can be used to verify functional and performance equivalence of behavioral models.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4695928,no,undetermined,0
Recording and analyzing eye movements during ocular fixation in schizophrenic subjects,"Previous studies have been shown that schizophrenic patient compared to healthy subject present abnormality in eye fixation tasks. But in these studies the evaluations of the eye movement are not objective. They are based on visual inspection of the records. The quality of fixation is assessed in term of absence of saccades. By using a predefined scale the records are rating from the best to worst. In this paper, we propose a new method to quantify eye fixation. in this method, our analyze examine the metric proprieties of each component of eye fixation movement (saccades, square wave jerks, and drift). A computer system is developed to record, stimulate and analyze the eye movement. A variety of software tools are developed to assist a clinician in the analysis of the data",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=999543,no,undetermined,0
A performance model of a PC based IP software router,"We can define a software router as a general-purpose computer that executes a computer program capable of forwarding IP datagrams among network interface cards attached to its I/O bus. This paper presents a parametrical model of a PC based IP software router. Validation results clearly show that the model accurately estimates the performance of the modeled system at different levels of detail. On the other hand, the paper presents experimental results that provide insights about the detailed functioning of such a system and demonstrate the model is valid not only for the characterized systems but for a reasonably range of CPU, memory and I/O bus operation speeds.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=997046,no,undetermined,0
Integrated inductors modeling and tools for automatic selection and layout generation,In this work we propose new equivalent circuit models for integrated inductors based on the conventional lumped element model. Automatic tools to assist the designers in selecting and automatically laying-out integrated inductors are also reported. Model development is based on measurements taken from more than 100 integrated spiral inductors designed and fabricated in a standard silicon process. We demonstrate the capacity of the proposed models to accurately predict the integrated inductor behavior in a wider frequency range than the conventional model. Our equations are coded in a set of tools that requests the desired inductance value at a determined frequency and gives back the geometry of the better inductors available in a particular technology.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996779,no,undetermined,0
Reliability Centered Maintenance Maturity Level Roadmap,"Numerous maintenance organizations are implementing various forms of reliability centered maintenance (RCM). Whether it is a classic or a streamlined RCM program, the challenge is to do it fast, but with predictable performance, quality, cost, and schedule. Hence, organizations need guidance to ensure their RCM programs are consistently implemented across the company and to improve their ability to manage key RCM process areas such as analysis, training, and metrics. The RCM Maturity Level Roadmap provides the structure for an organization to assess its RCM maturity and key process area capability. In addition, the Roadmap helps establish priorities for improvement and guide the implementation of these improvements.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181930,no,undetermined,0
Design and implementation of real-time digital video streaming system over IPv6 network using feedback control,"In this paper, we discuss a design of a real-time DV (Digital Video) streaming system, which dynamically adjusts packet transmission rate from the source host according to feedback information from the network. In our DV streaming system, the destination host continuously notifies the source host of network status (e.g., the end-to-end packet transmission delay and the packet loss probability in the network). The source host dynamically adjusts its packet transmission rate by lowering the quality of the video stream using a feedback-based control mechanism. Our DV streaming system achieves an efficient utilization of network resources, and prevents packet losses in the network. Thus, our DV streaming system realizes high-quality and real-time video streaming services on the Internet. By modifying the existing DVTS (Digital Video Transmission System), we implement a prototype of our real-time DV streaming system. Through several experimental results, we demonstrate the effectiveness of our DV streaming system.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183039,no,undetermined,0
On maximizing the fault coverage for a given test length limit in a synchronous sequential circuit,"When storage requirements or limits on test application time do not allow a complete (compact) test set to be used for a circuit, a partial test set that detects as many faults as possible is required. Motivated by this application, we address the following problem. Given a test sequence T of length L for a synchronous sequential circuit and a length M<L, find a test sequence T<sub>S</sub> of length at most M such that the fault coverage of T<sub>S</sub> is maximal. A similar problem was considered before for combinational and scan circuits, and solved by test ordering. Test ordering is not possible with the single test sequence considered here. We solve this problem by using a vector omission process that allows the length of the sequence T to be reduced while allowing controlled reductions in the number of detected faults. In this way, it is possible to obtain a sequence T<sub>S</sub> that has the desired length and a maximal fault coverage.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1197648,no,undetermined,0
A simple system for detection of EEG artifacts in polysomnographic recordings,"We present an efficient parametric system for automatic detection of electroencephalogram (EEG) artifacts in polysomnographic recordings. For each of the selected types of artifacts, a relevant parameter was calculated for a given epoch. If any of these parameters exceeded a threshold, the epoch was marked as an artifact. Performance of the system, evaluated on 18 overnight polysomnographic recordings, revealed concordance with decisions of human experts close to the interexpert agreement and the repeatability of expert's decisions, assessed via a double-blind test. Complete software (Matlab source code) for the presented system is freely available from the Internet at http://brain.fuw.edu.pl/artifacts.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1193788,no,undetermined,0
A General QoS Error Detection and Diagnosis Framework for Accountable SOA,"Accountability is a composite measure for different but related quality aspects. To be able to ensure accountability in practice, it is required to define specific quality attributes of accountability, and metrics for each quality attribute. In this paper, we propose a quality detection and diagnosis framework for the service accountability. We first identify types of quality attributes which are essential to manage QoS in accountability framework. We then present a detection and diagnosis model for problematic situations in services system. In this model, we design situation link representing dependencies among quality attributes, and provide information to detect and diagnose problems and their root causes. Based on the model, we propose an integrated model-based and case-based diagnosis method using the situation link.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4690621,no,undetermined,0
Assessing XP at a European Internet company,"Fst, a European Internet services company, has been experimenting with introducing XP in its development work. The article describes the company's experiences with XP, explores its implementation practice by practice, and discusses XPs pros and cons in three key areas; customer relationships, project management, and ISO 9001 quality assurance.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1196318,no,undetermined,0
An automated method for test model generation from switch level circuits,"Custom VLSI design at the switch level is commonly applied when a chip is required to meet stringent operating requirements in terms of speed, power, or area. ATPG requires gate level models, which are verified for correctness against switch level models. Typically, test models are created manually from the switch level models - a tedious, error-prone process requiring experienced DFT engineers. This paper presents an automated flow for creating gate level test models from circuits at the switch level. The proposed flow utilizes Motorola's Switch Level Verification (SLV) tool, which employs detailed switch level analysis to model the behavior of MOS transistors and represent them at a higher level of abstraction. We present experimental results, which demonstrate that the automated flow is capable of producing gate models that meet the ATPG requirements and are comparable to manually created ones.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1195123,no,undetermined,0
System health and intrusion monitoring (SHIM): project summary,"Computer systems and networks today are vulnerable to attacks. In addition to preventive strategies, intrusion detection has been used to further improve the security of computers and networks. Nevertheless, current intrusion detection and response system can detect only known attacks and provide primitive responses. The System Health and Intrusion Monitoring (SHIM) project aims at developing techniques to monitor and assess the health of a large distributed system. SHIM can accurately detect novel attacks and provide strategic information for further correlation, assessments, and response management.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194966,no,undetermined,0
Systematic Structural Testing of Firewall Policies,"Firewalls are the mainstay of enterprise security and the most widely adopted technology for protecting private networks. As the quality of protection provided by a firewall directly depends on the quality of its policy (i.e., configuration), ensuring the correctness of security policies is important and yet difficult.To help ensure the correctness of a firewall policy, we propose a systematic structural testing approach for firewall policies. We define structural coverage (based on coverage criteria of rules, predicates, and clauses) on the policy under test. Considering achieving higher structural coverage effectively, we develop three automated packet generation techniques: the random packet generation, the one based on local constraint solving (considering individual rules locally in a policy), and the most sophisticated one based on global constraint solving (considering multiple rules globally in a policy).We have conducted an experiment on a set of real policies and a set of faulty policies to detect faults with generated packet sets. Generally, our experimental results show that a packet set with higher structural coverage has higher fault detection capability (i.e., detecting more injected faults). Our experimental results show that a reduced packet set (maintaining the same level of structural coverage with the corresponding original packet set) maintains similar fault detection capability with the original set.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4690805,no,undetermined,0
Advanced analysis of dynamic neural control advisories for process optimization and parts maintenance,"This paper details an advanced set of analyses designed to drive specific process variable setpoint adjustments or maintenance actions required for cost effective process control using the Dynamic Neural ControllerTM (DNC) wafer-to-wafer advisories for semiconductor manufacturing advanced process control. The new analytic displays and metrics are illustrated using data obtained on a LAM 4520XL at STMicroelectronics as part of a SEMATECH SPIT beta test evaluation. The DNC represents a comprehensive modeling environment that uses as its input extensive process chamber information and history of the time since maintenance actions occurred. The DNC uses a neural network to predict multiple quality output metrics and a closed-loop risk-based optimization to maximize process quality performance while minimizing overall cost of tool operation and machine downtime. The software responds in an advisory mode on a wafer-to-wafer basis as to the optimal actions to be taken. In this paper, we present three specific instances of patterns arising during wafer processing over time that signal the process or equipment engineer to the need for corrective action: either a process setpoint adjustment or specific maintenance actions. Based on the controller's recommended corrective action set with the overall risk reduction predicted by such actions, a metric of corrective action """"urgency"""" can be created. The tracking of this metric over time yields different pattern types that signify a quantified need for a specific type of corrective action. Three basic urgency patterns are found: 1. a pattern in a given maintenance action over time showing increasing urgency or """"risk reduction"""" capability for the action; 2. a pattern in a process variable specific to a given recipe indicating a chronic request over time to only adjust the variable setpoint either above or below the current target; 3. a pattern in a process variable existing over all recipes processed through the chamber indicating chronic request to adjust the variable setpoint in either or both directions over time. This pattern is a pointer to the need for a maintenance action that is either corroborated by the urgency graph for that maintenance action, or if no such action has been previously take- n, a guide to the source of the equipment malfunction.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1194514,no,undetermined,0
Fault-Tolerant Coverage Planning in Wireless Networks,"Typically wireless networks coverage is planned with static redundancy to compensate temporal variations in the environment. As a result, the service still is delivered but the network coverage could have entered a critical state, meaning that further changes in the environment may lead to service failure. Service failures have to be explicitly notified by the applications. Therefore, in this paper we propose a methodology for fault-tolerant coverage planning. The idea is detecting the critical state and removing it by on-line system reconfiguration, and restoration of the original static redundancy. Even in case of a failure the system automatically generates a new configuration to restore the service, leading to shorter repair times. We describe how this approach can be applied to wireless mesh networks, often used in industrial applications like manufacturing, automation and logistics. The evaluation results show that the underlying model used for error detection and system recovery is accurate enough to correctly identify the system state.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4690812,no,undetermined,0
Location-detection strategies in pervasive computing environments,"Pervasive computing environments accommodate interconnected and communicating mobile devices. Mobility is a vital aspect of everyday life and technology must offer support for moving users, objects, and devices. Their growing number has strong implications on the bandwidth of wireless and wired networks. Network bandwidth becomes a scare resource and its efficient use is crucial for the quality of service in pervasive computing. In this article we study process models for detecting location changes of moving objects and their effect on the network bandwidth. We simulate a scenario of 10/sup 4/ moving objects for a period of 10/sup 7/ time cycles while monitoring the quality of service with respect to network bandwidth for different location detection strategies. The simulation shows that the class of strategies implementing a synchronous model offers better quality of service than the timed model. We conclude the article with a set of guidelines for the application of the strategies we have investigated.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1192750,no,undetermined,0
Task graph extraction for embedded system synthesis,"Consumer demand and improvements in hardware have caused distributed real-time embedded systems to rapidly increase in complexity. As a result, designers faced with time-to-market constraints are forced to rely on intelligent design tools to enable them to keep up with demand. These tools are continually being used earlier in the design process when the design is at higher levels of abstraction. At the highest level of abstraction are hardware/software co-synthesis tools which take a system specification as input. Although many embedded systems are described in C, the system specifications for many of these tools are often in the form of one or more task graphs. These tools are very effective at solving the co-synthesis problem using task graphs but require that designers manually transform the specification from C code to task graphs, a tedious and error-prone job. The task graph extraction tool described in this paper reduces the potential for error and the time required to design an embedded system by automating the task graph extraction process. Such a tool can drastically improve designer productivity. As far as we know, this is the first tool of its kind. It has been made available on the web.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183180,no,undetermined,0
A tamper-resistant framework for unambiguous detection of attacks in user space using process monitors,"Replication and redundancy techniques rely on the assumption that a majority of components are always safe and voting is used to resolve any ambiguities. This assumption may be unreasonable in the context of attacks and intrusions. An intruder could compromise any number of the available copies of a service resulting in a false sense of security. The kernel based approaches have proven to be quite effective but they cause performance impacts if any code changes are in the critical path. We provide an alternate user space mechanism consisting of process monitors by which such user space daemons can be unambiguously monitored without causing serious performance impacts. A framework that claims to provide such a feature must itself be tamper-resistant to attacks. We theoretically analyze and compare some relevant schemes and show their fallibility. We propose our own framework that is based on some simple principles of graph theory and well-founded concepts in topological fault tolerance, and show that it can not only unambiguously detect any such attacks on the services but is also very hard to subvert. We also present some preliminary results as a proof of concept.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1192456,no,undetermined,0
A metric-based approach to enhance design quality through meta-pattern transformations,"During the evolution of object-oriented legacy systems, improving the design quality is. most often a highly demanded objective. For such systems which have a large number of classes and are subject to frequent modifications, detection and correction of design defects is a complex task. The use of automatic detection and correction tools can be helpful for this task. Various research approaches have proposed transformations that improve the quality of an object-oriented systems while preserving its behavior This paper proposes a framework where a catalogue of object-oriented metrics can be used-as indicators for automatically detecting situations where a particular transformation can be applied to improve the quality of an object-oriented legacy system. The correction process is based on analyzing the impact of various meta-pattern transformations on these object-oriented metrics.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1192426,no,undetermined,0
Estimating bounds on the reliability of diverse systems,"We address the difficult problem of estimating the reliability of multiple-version software. The central issue is the degree of statistical dependence between failures of diverse versions. Previously published models of failure dependence described what behavior could be expected """"on average"""" from a pair of """"independently generated"""" versions. We focus instead on predictions using specific information about a given pair of versions. The concept of """"variation of difficulty"""" between situations to which software may be subject is central to the previous models cited, and it turns out to be central for our question as well. We provide new understanding of various alternative imprecise estimates of system reliability and some results of practical use, especially with diverse systems assembled from pre-existing (e.g., """"off-the-shelf"""") subsystems. System designers, users, and regulators need useful bounds on the probability of system failure. We discuss how to use reliability data about the individual diverse versions to obtain upper bounds and other useful information for decision making. These bounds are greatly affected by how the versions' probabilities of failure vary between subdomains of the demand space or between operating regimes-it is even possible in some cases to demonstrate, before operation, upper bounds that are very close to the true probability of failure of the system-and by the level of detail with which these variations are documented in the data.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1191798,no,undetermined,0
A frame-level measurement apparatus for performance testing of ATM equipment,"Performance testing of asynchronous transfer mode (ATM) equipment is dealt with here. The attention is principally paid to frame-level metrics, recently proposed by the ATM Forum because of their suitability to reflect user-perceived performance better than traditional cell-level metrics. Following the suggestions of the ATM Forum, more and more network engineers and production managers are interested today in these metrics, thus increasing the need of instruments and measurement solutions appropriate to their estimation. Trying to satisfy this exigency, a new VME extension for instrumentation (VXI) based measurement apparatus is proposed in the paper. The apparatus features a suitable software, developed by the authors, which allows the evaluation of the aforementioned metrics by simply making use of common ATM analyzers; only two VXI line interfaces, capable of managing the physical and ATM layers, are, in fact, adopted. Some details concerning ATM technology and its hierarchical structure, as well as the main differences between frames, specific to the ATM adaptation layer, and cells, characterizing the underlying ATM layer, are first given. Both the hardware and software solutions of the measurement apparatus are then described in detail, paying particular attention to the measurement procedures implemented. In the end, the performance of a new ATM device is assessed through the proposed apparatus.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1191406,no,undetermined,0
"Assessing attitude towards, knowledge of, and ability to apply, software development process","Software development is one of the most economically critical engineering activities. It is unsettling, therefore, that regularly published analyses reveal that the percentage of projects that fail, by coming in far over budget or far past schedule, or by being cancelled with significant financial loss, is considerably greater in software development than in any other branch of engineering. The reason is that successful software development requires expertise in both state of the art (software technology) and state of the practice (software development process). It is widely recognized that failure to follow best practice, rather than technological incompetence, is the cause of most failures. It is critically important, therefore, that (i) computer science departments be able assess the quality of the software development process component of their curricula and that industry be able to assess the efficacy of SPI (software process improvement) efforts. While assessment instruments/tools exist for knowledge of software technology, none exist for attitude toward, knowledge of, or ability to use, software development process. We have developed instruments for measuring attitude and knowledge, and are working on an instrument to measure ability to use. The current version of ATSE, the instrument for measuring attitude toward software engineering, is the result of repeated administrations to both students and software development professionals, post-administration focus groups, rewrites, and statistical reliability analyses. In this paper we discuss the development of ATSE, results, both expected an unexpected, of recent administrations of ATSE to students and professionals, the various uses to which ATSE is currently being put and to which it could be put, and ATSE's continuing development and improvement.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1191386,no,undetermined,0
Parametric fault tree for the dependability analysis of redundant systems and its high-level Petri net semantics,"In order to cope efficiently with the dependability analysis of redundant systems with replicated units, a new, more compact fault-tree formalism, called Parametric Fault Tree (PFT), is defined. In a PFT formalism, replicated units are folded and indexed so that only one representative of the similar replicas is included in the model. From the PFT, a list of parametric cut sets can be derived, where only the relevant patterns leading to the system failure are evidenced regardless of the actual identity of the component in the cut set. The paper provides an algorithm to convert a PFT into a class of High-Level Petri Nets, called SWN. The purpose of this conversion is twofold: to exploit the modeling power and flexibility of the SWN formalism, allowing the analyst to include statistical dependencies that could not have been accommodated into the corresponding PFT and to exploit the capability of the SWN formalism to generate a lumped Markov chain, thus alleviating the state explosion problem. The search for the minimal cut sets (qualitative analysis) can be often performed by a structural T-invariant analysis on the generated SWN. The advantages that can be obtained from the translation of a PFT into a SWN are investigated considering a fault-tolerant multiprocessor system example.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183940,no,undetermined,0
Vision based pointing device with slight body movement,"It is widely acknowledged that computer is powerful tool to improve the quality of life of the people with disability. One problem is how the user manipulates the computer using suitable input device designed for him or her. This paper proposes tow pointing devices based on vision for the people with disability as input devices. These devices detect the position of the marks attached on the users head. According to the head movements, computer cursor on the computer display is moved in two-dimensionally. A distinct advantage of these devices is that various strategies could be installed based on the situation of the users. The pointing device was applied to a drawing tool of the painting software for an patients successfully.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694425,no,undetermined,0
Definition of a systematic method for the generation of software test programs allowing the functional verification of system on chip (SoC),We present a novel approach for hardware functional verification of system on chip (SoC). Our approach is based on the use of on chip programmable processors like CPUs or DSPs to generate test programs for hardware parts of the design. Traditionally test programs are written at a low level using specific functions for hardware accesses. This method is time consuming and error prone as tests are hand written. We introduce a method allowing the use of high level software test programs. The link between hardware and software is achieved by using a custom operating system. We focus on the benefits that are obtained by handling high level test programs.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250257,no,undetermined,0
Measurement-based admission control in UMTS multiple cell case,"In this paper, we develop an efficient call admission control (CAC) algorithm for UMTS systems. We first introduce the expressions that we developed for signal-to-interference (SIR) for both uplink and downlink, to obtain a novel CAC algorithm that takes into account, in addition to SIR constraints, the effects of mobility, coverage as well as the wired capacity in the LMTS terrestrial radio access network (UTRAN). for the uplink, and the maximal transmission power of the base station, for the downlink. As of its implementation, we investigate the measurement-based approach as a means to predict future, both handoff and new, call arrivals and thus manage different priority levels. Compared to classical CAC algorithms, our CAC mechanism achieves better performance in terms of outage probability and QoS management.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260419,no,undetermined,0
An IDE framework for grid application development,"Grid computing enables the aggregation of a large number of computational resources for solving complex scientific and engineering problems. However, writing, deploying, and testing grid applications over highly heterogeneous and distributed infrastructure are complex and error prone. A number of grid integrated development environments (IDEs) have been proposed and implemented to simplify grid application development. This paper presents an extension to our previous work on a grid IDE in the form of a software framework with a well-defined API and an event mechanism. It provides novel tools to automate routine grid programming tasks and allow programmable actions to be invoked based on certain events. Its system model regards resources as first-class objects in the IDE and allows tight integration between the execution platforms and the code development process. We discuss how the framework improves the process of grid application development.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662798,no,undetermined,0
Intelligent component selection,"Component-based software engineering (CBSE) provides solutions to the development of complex and evolving systems. As these systems are created and maintained, the task of selecting components is repeated. The context-driven component evaluation (CdCE) project is developing strategies and techniques for automating a repeatable process for assessing software components. This paper describes our work using artificial intelligence (AI) techniques to classify components based on an ideal component specification. Using AI we are able to represent dependencies between attributes, overcoming some of the limitations of existing aggregation-based approaches to component selection",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342839,no,undetermined,0
Development of an intelligent system for architecture design and analysis [software architecture],"Software architecture plays a pivotal role in allowing an organization to meet its business goals, in terms of the early insights it provides into the system, the communication it enables among stakeholders, and the value it provides as a re-usable asset. Unfortunately, designing and analyzing architecture for a certain system is recognized as a hard task for most software engineers, because the process of collecting, maintaining, and validating architectural information is complex, knowledge-intensive, iterative, and error prone. The needs of software architectural design and analysis have led to a desire to create tools to support the process. This paper introduces an intelligent system, which serves the following purposes: to obtain meaningful nonfunctional requirements from users; to aid in exploring architectural alternatives; and to facilitate architectural analysis.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1345092,no,undetermined,0
Case studies on the application of the CORE model for requirements engineering process assessment [software engineering],"Existing requirements engineering (RE) process assessment models lack the components necessary to provide enough information about the quality of an RE process. The concept of """"concern of requirement engineering"""" (CORE) and the assessment models proposed in our previous research provide a method and new perspectives to assess the quality of an RE process. The case studies presented in this paper provide a comprehensive view of the application of the CORE model. The advantages of using the model for RE process assessment are twofold. First, it is more flexible because the major COREs assess the main contents of the activities of an RE process. Second, the categories classified in the model allow RE process assessment based on several categories. This allows for process improvement in an incremental manner. The CORE model is part of our RE process development framework and is used to assess the quality of the RE process under development.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1345021,no,undetermined,0
Tempest: Towards early identification of failure-prone binaries,"Early estimates of failure-proneness can be used to help inform decisions on testing, refactoring, design rework etc. Often such early estimates are based on code metrics like churn and complexity. But such estimates of software quality rarely make their way into a mainstream tool and find industrial deployment. In this paper we discuss about the Tempest tool that uses statistical failure-proneness models based on code complexity and churn metrics across the Microsoft Windows code base to identify failure-prone binaries early in the development process. We also present the tool architecture and its usage as of date at Microsoft.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630079,no,undetermined,0
A novel ant clustering algorithm based on cellular automata,"Based on the principle of cellular automata in artificial life, an artificial ant sleeping model (ASM) and an ant algorithm for cluster analysis (A4C) are presented. Inspired by the behaviors of gregarious ant colonies, we use the ant agent to represent a data object. In ASM, each ant has two states: a sleeping state and an active state. The ant's state is controlled by a function of the ant's fitness to the environment it locates and a probability for the ants becoming active. The state of an ant is determined only by its local information. By moving dynamically, the ants form different subgroups adaptively, and hence the data objects they represent are clustered. Experimental results show that the A4C algorithm on ASM is significantly better than other clustering methods in terms of both speed and quality. It is adaptive, robust and efficient, achieving high autonomy, simplicity and efficiency.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342937,no,undetermined,0
Services-oriented dynamic reconfiguration framework for dependable distributed computing,"Recently service-oriented architecture (SOA) has received significant attention and one reason is that it is potentially survivable as services are located, bound, and executed at runtime over the Internet. However, this is not enough for dependable computing because the system must also be able to reconfigure once a system failure or overload is detected, and this reconfiguration must be done in real-time at runtime with minimum disruption to the current operation. This work presents reconfiguration requirements for building dependable SOA, and proposes a dynamic reconfiguration framework based on distributed monitoring, synchronization, and runtime verification with distributed agents.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342894,no,undetermined,0
Proof-guided testing: an experimental study,"Proof-guided testing is intended to enhance the test design with information extracted from the argument for correctness. The target application field is the verification of fault-tolerance algorithms where a paper proof is published Ideally, testing should be focused on the weak parts of the demonstration. The identification of weak parts proceeds by restructuring the informal discourse as a proof tree and analyzing it step by step. The approach is experimentally assessed using the example of a flawed group membership protocol (GMP). Results are quite promising: (1) compared to crude random testing, the proof-guided method allowed us to significantly improve the fault revealing power of test data; (2) the overall method also provided useful feedback on the proof and its potential flaw(s).",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342890,no,undetermined,0
On the testing of particular input conditions,"Generating test cases from a specification can be done at an early stage. However, so many important aspects relevant to testing can be identified from the specification that exhaustively testing their combinations can be very costly. A common approach to reduce testing costs is to identify some particular input conditions and test each of them only once. We argue that such an approach should be used judiciously, or else inadequate tests may result. This paper explores several alternatives to assess the validity of the tester's hypothesis that a particular condition can be tested adequately with only one test case. These alternatives help to test the particular conditions more reliably and, hence, reduce the risk of not revealing the existence of faults",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342850,no,undetermined,0
Software reliability growth models incorporating fault dependency with various debugging time lags,"Software reliability is defined as the probability of failure-free software operation for a specified period of time in a specified environment. Over the past 30 years, many software reliability growth models (SRGMs) have been proposed and most SRGMs assume that detected faults are immediately corrected. Actually, this assumption may not be realistic in practice. In this paper we first give a review of fault detection and correction processes in software reliability modeling. Furthermore, we show how several existing SRGMs based on NHPP models can be derived by applying the time-dependent delay function. On the other hand, it is generally observed that mutually independent software faults are on different program paths. Sometimes mutually dependent faults can be removed if and only if the leading faults were removed. Therefore, here we incorporate the ideas of fault dependency and time-dependent delay function into software reliability growth modeling. Some new SRGMs are proposed and several numerical examples are included to illustrate the results. Experimental results show that the proposed framework to incorporate both fault dependency and time-dependent delay function for SRGMs has a fairly accurate prediction capability",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342826,no,undetermined,0
Empirical evaluation of the fault-detection effectiveness of smoke regression test cases for GUI-based software,"Daily builds and smoke regression tests have become popular quality assurance mechanisms to detect defects early during software development and maintenance. In previous work, we addressed a major weakness of current smoke regression testing techniques, i.e., their lack of ability to automatically (re)test graphical user interface (GUI) event interactions - we presented a GUI smoke regression testing process called daily automated regression tester (DART). We have deployed DART and have found several interesting characteristics of GUI smoke tests that we empirically demonstrate in this paper. We also combine smoke tests with different types of test oracles and present guidelines for practitioners to help them generate and execute the most effective combinations of test-case length and test oracle complexity. Our experimental subjects consist of four GUI-based applications. We generate 5000-8000 smoke tests (enough to be run in one night) for each application. Our results show that: (1) short GUI smoke tests with certain test oracles are effective at detecting a large number of faults; (2) there are classes of faults that our smoke test cannot detect; (3) short smoke tests execute a large percentage of code; and (4) the entire smoke testing process is feasible to do in terms of execution time and storage space.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357785,no,undetermined,0
Application of maximum entropy principle to software failure prediction,"Predicting failures from software input is still a tough issue. Two models, namely the surface model and structure model, are presented in this paper to predict failure by applying the maximum entropy principle. The surface model forecasts a failure from the statistical co-occurrence between input and failure, while the structure model does from the statistical cause-effect between fault and failure. To evaluate the models, precision is applied and 17 testing experiments are conducted on 5 programs. Based on the experiments, the surface model and structure model get an average precision of 0.876 and 0.858, respectively",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342825,no,undetermined,0
Consistency check in modelling multi-agent systems,"In model-driven software development, inconsistency of a model must be detected and eliminated to ensure the quality of the model. This paper investigates the consistency check in the modelling of multi-agent systems (AMS). Consistency constraints are formally defined for the CAMLE language, which was proposed in our previous work for modelling MAS. Uses of the consistency constraints in the implementation of a modelling environment for automatic consistency check and model transformation are discussed",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342814,no,undetermined,0
T-UPPAAL: online model-based testing of real-time systems,"The goal of testing is to gain confidence in a physical computer based system by means of executing it. More than one third of typical project resources are spent on testing embedded and real-time systems, but still it remains ad-hoc, based on heuristics, and error-prone. Therefore systematic, theoretically well-founded and effective automated real-time testing techniques are of great practical value. Testing conceptually consists of three activities: test case generation, test case execution and verdict assignment. We present T-UPPAAL-a new tool for model based testing of embedded real-time systems that automatically generates and executes tests """"online"""" from a state machine model of the implementation under test (IUT) and its assumed environment which combined specify the required and allowed observable (realtime) behavior of the IUT. T-UPPAAL implements a sound and complete randomized testing algorithm, and uses a formally defined notion of correctness (relativized timed input/output conformance) to assign verdicts. Using online testing, events are generated and simultaneously executed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342774,no,undetermined,0
Verifiable concurrent programming using concurrency controllers,"We present a framework for verifiable concurrent programming in Java based on a design pattern for concurrency controllers. Using this pattern, a programmer can write concurrency controller classes defining a synchronization policy by specifying a set of guarded commands and without using any of the error-prone synchronization primitives of Java. We present a modular verification approach that exploits the modularity of the proposed pattern, i.e., decoupling of the controller behavior from the threads that use the controller. To verify the controller behavior (behavior verification) we use symbolic and infinite state model checking techniques, which enable verification of controllers with parameterized constants, unbounded variables and arbitrary number of user threads. To verify that the threads use a controller in the specified manner (interface verification) we use explicit state model checking techniques, which allow verification of arbitrary thread implementations without any restrictions. We show that the correctness of the user threads can be verified using the concurrency controller interfaces as stubs, which improves the efficiency of the interface verification significantly. We also show that the concurrency controllers can be automatically optimized using the specific notification pattern. We demonstrate the effectiveness of our approach on a Concurrent Editor implementation which consists of 2800 lines of Java code with remote procedure calls and complex synchronization constraints.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342742,no,undetermined,0
Rostra: a framework for detecting redundant object-oriented unit tests,"Object-oriented unit tests consist of sequences of method invocations. Behavior of an invocation depends on the state of the receiver object and method arguments at the beginning of the invocation. Existing tools for automatic generation of object-oriented test suites, such as Jtest and J Crasher for Java, typically ignore this state and thus generate redundant tests that exercise the same method behavior, which increases the testing time without increasing the ability to detect faults. This work proposes Rostra, a framework for detecting redundant unit tests, and presents five fully automatic techniques within this framework. We use Rostra to assess and minimize test suites generated by test-generation tools. We also present how Rostra can be added to these tools to avoid generation of redundant tests. We have implemented the five Rostra techniques and evaluated them on 11 subjects taken from a variety of sources. The experimental results show that Jtest and JCrasher generate a high percentage of redundant tests and that Rostra can remove these redundant tests without decreasing the quality of test suites.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342737,no,undetermined,0
AGIS: Towards automatic generation of infection signatures,"An important yet largely uncharted problem in malware defense is how to automate generation of infection signatures for detecting compromised systems, i.e., signatures that characterize the behavior of malware residing on a system. To this end, we develop AGIS, a host-based technique that detects infections by malware and automatically generates an infection signature of the malware. AGIS monitors the runtime behavior of suspicious code according to a set of security policies to detect an infection, and then identifies its characteristic behavior in terms of system or API calls. AGIS then statically analyzes the corresponding executables to extract the instructions important to the infectionpsilas mission. These instructions can be used to build a template for a static-analysis-based scanner, or a regular-expression signature for legacy scanners. AGIS also detects encrypted malware and generates a signature from its plaintext decryption loop. We implemented AGIS on Windows XP and evaluated it against real-life malware, including keyloggers, mass-mailing worms, and a well-known mutation engine. The experimental results demonstrate the effectiveness of our technique in detecting new infections and generating high-quality signatures.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630092,no,undetermined,0
WS-FIT: a tool for dependability analysis of Web services,This work provides an overview of fault injection techniques and their applicability to testing SOAP RPC based Web service systems. We also give a detailed example of the WS-FIT package and use it to detect a problem in a Web service based system.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342690,no,undetermined,0
A reliability study on switching fabric based system architecture,"Processor and networking technologies have outrun system technologies, such that the system itself is the bottleneck. Many vendors and organisations are trying to address this problem by replacing the popular bus based architecture with a switching fabric. The paper studies the new switching technology and compares it with the old bus based architecture. Both architectures are to be used in a highly-available parallel processing environment. It is assumed that both systems have incorporated the same fault tolerant strategies. A hierarchical approach is used to break down the systems into subsets, such that both software and hardware components are considered. Each component is represented by a hazard function or a failure intensity. The fault tolerant strategies are used to estimate the repair time probability distribution function. The availability of the system is calculated. It is found that the switching fabric has a slightly higher availability.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1345286,no,undetermined,0
Specifications overview for counter mode of operation. Security aspects in case of faults,"In 2001, after a selection process, NIST added the counter mode of operation to be used with the advanced encryption standard (AES). In the NIST recommendation a standard incrementing function is defined for generation of the counter blocks which are encrypted for each plaintext block, IPsec Internet draft (R. Housley et al., May 2003) and ATM security specifications contain implementation specifications for counter mode standard incrementing function. In this paper we present those specifications. We analyze the probability to reveal useful information in case of faults in standard incrementing function described in NIST recommendation. The confidentiality of the mode can be compromised with the fault model presented in this paper. We recommend another solution to be used in generation of the standard incrementing function in the context of the counter mode.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347044,no,undetermined,0
Design and evaluation of a fault-tolerant mobile-agent system,"The mobile agents create a new paradigm for data exchange and resource sharing in rapidly growing and continually changing computer networks. In a distributed system, failures can occur in any software or hardware component. A mobile agent can get lost when its hosting server crashes during execution, or it can get dropped in a congested network. Therefore, survivability and fault tolerance are vital issues for deploying mobile-agent systems. This fault tolerance approach deploys three kinds of cooperating agents to detect server and agent failures and recover services in mobile-agent systems. An actual agent is a common mobile agent that performs specific computations for its owner. Witness agents monitor the actual agent and detect whether it's lost. A probe recovers the failed actual agent and the witness agents. A peer-to-peer message-passing mechanism stands between each actual agent and its witness agents to perform failure detection and recovery through time-bounded information exchange; a log records the actual agent's actions. When failures occur, the system performs rollback recovery to abort uncommitted actions. Moreover, our method uses checkpointed data to recover the lost actual agent.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347066,no,undetermined,0
Decidability results for parametric probabilistic transition systems with an application to security,"We develop a model of parametric probabilistic transition systems. In this model probabilities associated with transitions may be parameters, and we show how to find instances of parameters that satisfy a given property and instances that either maximize or minimize the probability of reaching a given state. We show, as an application, the model of a probabilistic non repudiation protocol. The theory we develop, allows us to find instances that maximize the probability that the protocol ends in a fair state (no participant has an advantage over the others).",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347512,no,undetermined,0
Power quality factor and line-disturbances measurements in three-phase systems,"A power quality meter (PQM) is presented for measuring, as a first objective, a single indicator, designated power quality factor (PQF), in the range between zero to one, which integrally reflect the power transfer quality of a general three phase network feeding unbalanced nonlinear loads. PQF definition is based on the analysis of functions in the frequency domain, separating the fundamental terms from the harmonic terms of the Fourier series. Then, quality aspects considered in the PQF definition can be calculated: a) the voltage and current harmonic levels b) the degree of unbalance and c) the phase displacement factor in the different phases at the fundamental frequency. As a second objective, the PQM has been designed for detecting, classifying and organizes power line disturbances. For monitoring power line disturbances, the PQM is configured as virtual instrument, which automatically classifies and organizes them in a database while they are being recorded. The type of disturbances includes: impulse, oscillation, sag, swell, interruption, undervoltage, overvoltage, harmonics and frequency variation. For amplitude disturbances (impulse, sag, swell, interruption, undervoltage and overvoltage), the PQM permits the measurement of parameters such as amplitude, start time and final time. Measurement of harmonic distortion allows recording and visual presentation of the spectrum of amplitudes and phases corresponding to the first 40 harmonics. Software tools use the database structure to present summaries of power disturbances and locate an event by severity or time of occurrence. Simulated measurements are included to demonstrate the versatility of the instrument.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1355328,no,undetermined,0
Discovery of policy anomalies in distributed firewalls,"Firewalls are core elements in network security. However, managing firewall rules, particularly in multi-firewall enterprise networks, has become a complex and error-prone task. Firewall filtering rules have to be written, ordered and distributed carefully in order to avoid firewall policy anomalies that might cause network vulnerability. Therefore, inserting or modifying filtering rules in any firewall requires thorough intra- and inter-firewall analysis to determine the proper rule placement and ordering in the firewalls. We identify all anomalies that could exist in a single- or multi-firewall environment. We also present a set of techniques and algorithms to automatically discover policy anomalies in centralized and distributed legacy firewalls. These techniques are implemented in a software tool called the """"Firewall Policy Advisor"""" that simplifies the management of filtering rules and maintains the security of next-generation firewalls.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1354680,no,undetermined,0
Measuring application error rates for network processors,"Faults in computer systems can occur due to a variety of reasons. In many systems, an error has a binary effect, i.e. the output is either correct or it is incorrect. However, networking applications exhibit different properties. For example, although a portion of the code behaves incorrectly due to a fault, the application can still work correctly. Integrity of a network system is often unchanged during faults. Therefore, measuring the effects of faults on the network processor applications require new measurement metrics to be developed. In this paper, we highlight essential application properties and data structures that can be used to measure the error behavior of network processors. Using these metrics, we study the error behavior of seven representative networking applications under different cache access fault probabilities.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1354210,no,undetermined,0
The future of software infrastructure protection,"In this paper the author describes how a Gatekeeper prototype had detected 83 percent of all unknown real viruses thrown at it. Even more intriguing was that the 17 percent of viruses missed were all due to the prototype code's immaturity, rather than any failing of the method used to detect them. Stated another way: An enterprise-ready version of the prototype would have captured every virus the Internet could have thrown at it during its testing period. Of course, many signature-based virus detection tools can detect 100 percent of known viruses. But very few of them can recognize new viruses.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1353224,no,undetermined,0
Study on Recognition Characteristics of Acoustic Emission Based on Fractal Dimension,"It is difficult to recognise acoustic emission (AE) signal because of serious pollution by noise. Fractal dimension is a new method to describe the characteristics of AE signal. According to the complex computation and low precision of box counting dimension, correlation dimension and Katz dimension, an algorithm of logarithmic fractal dimension based on waveform length was proposed in this paper the deduction process was introduced. The experiment datas was rub impact AE signals sampled from rotating test stand. Gaussian white noise and non-stationary noise were added to simulate the field AE signal which polluted seriously by noise. Then, three algorithms based on box counting dimension, Katz dimension and logarithmic fractal dimension were compared in AE signal recognition. The results show that the algorithm of logarithmic dimension distinguishes rub impact AE signals from strong noise is more effectively, and has lower computation and higher precision than others. It provides a new approach to identify characteristics of AE signal and detect rub impact fault of rotating machinery.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4627207,no,undetermined,0
Effective Web Service Composition in Diverse and Large-Scale Service Networks,"The main research focus of Web services is to achieve the interoperability between distributed and heterogeneous applications. Therefore, flexible composition of Web services to fulfill the given challenging requirements is one of the most important objectives in this research field. However, until now, service composition has been largely an error-prone and tedious process. Furthermore, as the number of available web services increases, finding the right Web services to satisfy the given goal becomes intractable. In this paper, toward these issues, we propose an AI planning-based framework that enables the automatic composition of Web services, and explore the following issues. First, we formulate the Web-service composition problem in terms of AI planning and network optimization problems to investigate its complexity in detail. Second, we analyze publicly available Web service sets using network analysis techniques. Third, we develop a novel Web-service benchmark tool called WSBen. Fourth, we develop a novel AI planning-based heuristic Web-service composition algorithm named WSPR. Finally, we conduct extensive experiments to verify WSPR against state-of-the-art AI planners. It is our hope that both WSPR and WSBen will provide useful insights for researchers to develop Web-service discovery and composition algorithms, and software.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4629388,no,undetermined,0
Checkpointing for peta-scale systems: a look into the future of practical rollback-recovery,"Over the past two decades, rollback-recovery via checkpoint-restart has been used with reasonable success for long-running applications, such as scientific workloads that take from few hours to few months to complete. Currently, several commercial systems and publicly available libraries exist to support various flavors of checkpointing. Programmers typically use these systems if they are satisfactory or otherwise embed checkpointing support themselves within the application. In this paper, we project the performance and functionality of checkpointing algorithms and systems as we know them today into the future. We start by surveying the current technology roadmap and particularly how Peta-Flop capable systems may be plausibly constructed in the next few years. We consider how rollback-recovery as practiced today will fare when systems may have to be constructed out of thousands of nodes. Our projections predict that, unlike current practice, the effect of rollback-recovery may play a more prominent role in how systems may be configured to reach the desired performance level. System planners may have to devote additional resources to enable rollback-recovery and the current practice of using """"cheap commodity"""" systems to form large-scale clusters may face serious obstacles. We suggest new avenues for research to react to these trends.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1350776,no,undetermined,0
An energy-aware framework for coordinated dynamic software management in mobile computers,"Energy efficiency is a very important and challenging issue for resource-constrained mobile computers. We propose a dynamic software management (DSM) framework to improve battery utilization, and avoid competition for limited energy resources from multiple applications. We have designed and implemented a DSM module in user space, independent of the operating system (OS), which explores quality-of-service (QoS) adaptation to reduce system energy and employs a priority-based preemption policy for multiple applications. It also employs energy macromodels for mobile applications to aid in this endeavor. By monitoring the energy supply and predicting energy demand at each QoS level, the DSM module is able to select the best possible trade-off between energy conservation and application QoS. To the best of our knowledge, this is the first energy-aware coordinated framework utilizing adaptation of mobile applications. It honors the priority desired by the user and is portable to POSIX-compliant OSs. Our experimental results for some mobile applications (video player, speech recognizer, voice-over-IP) show that this approach can meet user-specified task-oriented goals and improve battery utilization significantly. They also show that prediction of application energy demand based on energy macro-models is a key component of this framework.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1348285,no,undetermined,0
At-speed functional verification of programmable devices,In this paper we present a novel approach for functional verification of programmable devices. The proposed methodology is suited to refine the results obtained by a functional automatic test pattern generator (ATPG). The hard-to-detect faults are examined by exploiting the controllability ability of a high-level ATPG in conjunction with the observability potentiality of software instructions targeted to the programmable device. Generated test programs can be used for both functional verification and at-speed testing.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347863,no,undetermined,0
Automated duplicate detection for bug tracking systems,"Bug tracking systems are important tools that guide the maintenance activities of software developers. The utility of these systems is hampered by an excessive number of duplicate bug reports-in some projects as many as a quarter of all reports are duplicates. Developers must manually identify duplicate bug reports, but this identification process is time-consuming and exacerbates the already high cost of software maintenance. We propose a system that automatically classifies duplicate bug reports as they arrive to save developer time. This system uses surface features, textual semantics, and graph clustering to predict duplicate status. Using a dataset of 29,000 bug reports from the Mozilla project, we perform experiments that include a simulation of a real-time bug reporting environment. Our system is able to reduce development cost by filtering out 8% of duplicate bug reports while allowing at least one report for each real defect to reach developers.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630070,no,undetermined,0
Compression of VLSI test data by arithmetic coding,This work presents arithmetic coding and its application to data compression for VLSI testing. The use of arithmetic codes for compression results in a codeword whose length is close to the optimal value as predicted by entropy in information theory. Previous techniques (such as those based on Huffman or Golomb coding) result in optimal codes for test data sets in which the probability model of the symbols satisfies specific requirements. We show that Huffman and Golomb codes result in large differences between entropy bound and sustained compression. We present compression results of arithmetic coding for circuits through a practical integer implementation of arithmetic coding/decoding and analyze its deviation from the entropy bound as well. A software implementation approach is proposed and studied in detail using industrial embedded DSP cores.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347835,no,undetermined,0
Using likely program invariants to detect hardware errors,"In the near future, hardware is expected to become increasingly vulnerable to faults due to continuously decreasing feature size. Software-level symptoms have previously been used to detect permanent hardware faults. However, they can not detect a small fraction of faults, which may lead to silent data corruptions(SDCs). In this paper, we present a system that uses invariants to improve the coverage and latency of existing detection techniques for permanent faults. The basic idea is to use training inputs to create likely invariants based on value ranges of selected program variables and then use them to identify faults at runtime. Likely invariants, however, can have false positives which makes them challenging to use for permanent faults. We use our on-line diagnosis framework for detecting false positives at runtime and limit the number of false positives to keep the associated overhead minimal. Experimental results using microarchitecture level fault injections in full-system simulation show 28.6% reduction in the number of undetected faults and 74.2% reduction in the number of SDCs over existing techniques, with reasonable overhead for checking code.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630072,no,undetermined,0
HyperTree for self-stabilizing peer-to-peer systems,"Peer-to-peer systems are prone to faults, thus it is vitally important to design peer-to-peer systems to automatically regain consistency, namely to be self-stabilizing. Toward this goal, we present a deterministic structure that defines for every n the entire (IP) pointers structure among the n machines. Namely, the next hop for the insert, delete and search procedures of the peer-to-peer system. Thus, the consistency of the system is easily defined, monitored, verified and repaired. We present the HyperTree (distributed) structure which support the peer-to-peer procedures while ensuring that the out-degree and in-degree (the number of outgoing/incoming pointers) are b log<sub>b</sub> N where N in the maximal number of machines and b is an integer parameter greater than 1. In addition the HyperTree ensures that the maximal number of hops involved in each procedure is bounded by log<sub>b</sub> N. A self-stabilizing peer-to-peer system based on the HyperTree is presented.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347757,no,undetermined,0
Fault tolerance in a layered architecture: a general specification pattern in B,"Dependable control systems are usually complex and prone to errors of various natures. Such systems are often built in a modular and layered fashion. To guarantee system dependability, we need to develop software that is not only fault-free but also is able to cope with faults of other system components. In this paper we propose a general formal specification pattern that can be recursively applied to specify fault tolerance mechanisms at each architectural layer. Iterative application of this pattern via stepwise refinement in the B method results in development of a layered fault tolerant system correct by construction. We demonstrate the proposed approach by an excerpt from a realistic case study - development of liquid handling workstation Fillwell.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347539,no,undetermined,0
Exception safety for C#,"Programming-language mechanisms for throwing and handling exceptions can simplify some computer programs. However the use of exceptions can also be error prone, leading to new programming errors and code that is hard to understand. This paper describes ways to tame the exception usage in C#. In particular the paper describes the treatment of exceptions in Spec#, an experimental superset of C# that includes code contracts.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347523,no,undetermined,0
Data flow analysis and testing of Java Server Pages,"Web applications often rely on server-side scripts to handle HTTP requests, to generate dynamic contents, and to interact with other components. The server-side scripts usually mix with HTML statements and are difficult to understand and test. In particular, these scripts do not have any compiling check and could be error-prone. Thus, it becomes critical to test the server-side scripts for ensuring the quality and reliability of Web applications. We adapt traditional dataflow testing techniques into the context of Java Server Pages (JSP), a very popular server-side script for developing Web applications with Java technology. We point out that the JSP implicit objects and action tags can introduce several unique dataflow test artifacts which need to be addressed. A test model is presented to capture the dataflow information of JSP pages with considerations of various implicit objects and action tags. Based on the test model, we describe an approach to compute the intraprocedural and interprocedural data flow test paths for uncovering the data anomalies of JSP pages.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342689,no,undetermined,0
"An integrated approach to resource pool management: Policies, efficiency and quality metrics","The consolidation of multiple servers and their workloads aims to minimize the number of servers needed thereby enabling the efficient use of server and power resources. At the same time, applications participating in consolidation scenarios often have specific quality of service requirements that need to be supported. To evaluate which workloads can be consolidated to which servers we employ a trace-based approach that determines a near optimal workload placement that provides specific qualities of service. However, the chosen workload placement is based on past demands that may not perfectly predict future demands. To further improve efficiency and application quality of service we apply the trace-based technique repeatedly, as a workload placement controller. We integrate the workload placement controller with a reactive controller that observes current behavior to i) migrate workloads off of overloaded servers and ii) free and shut down lightly-loaded servers. To evaluate the effectiveness of the approach, we developed a new host load emulation environment that simulates different management policies in a time effective manner. A case study involving three months of data for 138 SAP applications compares our integrated controller approach with the use of each controller separately. The study considers trade-offs between i) required capacity and power usage, ii) resource access quality of service for CPU and memory resources, and iii) the number of migrations. We consider two typical enterprise environments: blade and server based resource pool infrastructures. The results show that the integrated controller approach outperforms the use of either controller separately for the enterprise application workloads in our study. We show the influence of the blade and server pool infrastructures on the effectiveness of the management policies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630101,no,undetermined,0
Reliability-aware co-synthesis for embedded systems,"As technology scales, transient faults due to single event upsets have emerged as a key challenge for reliable embedded system design. This work proposes a design methodology that incorporates reliability into hardware-software co-design paradigm for embedded systems. We introduce an allocation and scheduling algorithm that efficiently handles conditional execution in multi-rate embedded systems, and selectively duplicates critical tasks to detect soft errors, such that the reliability of the system is increased. The increased reliability is achieved by utilizing the otherwise idle computation resources and incurs no resource or performance penalty. The proposed algorithm is fast and efficient, and is suitable for use in the inner loop of our hardware/software co-synthesis framework, where the scheduling routine has to be invoked many times.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342457,no,undetermined,0
Concurrent error detection in wavelet lifting transforms,"Wavelet transforms, central to multiresolution signal analysis and important in the JPEG2000 image compression standard, are quite susceptible to computer-induced errors because of their pipelined structure and multirate processing requirements. Such errors emanate from computer hardware, software bugs, or radiation effects from the surrounding environment. Implementations use lifting schemes, which employ update and prediction estimation stages, and can spread a single numerical error caused by failures to many output transform coefficients without any features to warn data users. We propose an efficient method to detect the arithmetic errors using weighted sums of the wavelet coefficients at the output compared with an equivalent parity value derived from the input data. Two parity values may straddle a complete multistage transform or several values may be used, each pair covering a single stage. There is greater error-detecting capability at only a slight increase in complexity when parity pairs are interspersed between stages. With the parity weighting design scheme, a single error introduced at a lifting section can be detected. The parity computation operation is properly viewed as an inner product between weighting values and the data, motivating the use of dual space functionals related to the error gain matrices. The parity weighting values are generated by a combination of dual space functionals. An iterative procedure for evaluating the design of the parity weights has been incorporated in Matlab code and simulation results are presented.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1327579,no,undetermined,0
A system for fault detection and reconfiguration of hardware based active networks,"An experimental Active Network based on a PC running the Linux OS and operating as a router has been implemented. The PC carries a PCI-based FPGA board, which is the execution environment of the Active Applications. The users are able to send Active Packets and program dynamically the network by remote configuration of the target FPGA board. The FPGA can be reconfigured multiple times on-the-fly with several Active Applications (IP-cores). A fault detection module is permanently configured in one of the FPGAs of the PCI board. Its function is to monitor the Active Applications at run time and check the PCI bus transactions for violations of predefined rules. The fault detector module works as a """"firewall"""" preventing the communication between the configured application and the host computer, if a violation is detected.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319689,no,undetermined,0
On service replication strategy for service overlay networks,"The service overlay network (SON) is an effective means to deliver end-to-end QoS guaranteed applications on the current Internet. Duan et al. (2002) address the bandwidth provisioning problem on a SON, specifically, in determining the appropriate amount of bandwidth capacity to purchase from various autonomous systems so as to satisfy the QoS requirements of the SON's end users and at the same time maximize the total revenue of operating the overlay network. In this paper, we extend the concept of the service overlay network. Since traffic demands are time varying and there may be some unexpected events which can cause a traffic surge, these will significantly increase the probability of QoS violation and will reduce the profit margin of a SON. To overcome these problems, we propose to replicate services on the service gateways so as to dynamically adapt to these traffic surges. We show that the service replication problem, in general, is intractable. We propose an efficient service replication algorithm which replicates services for a subset of traffic flows. Under our replication strategy, one does not need to increase the bandwidth capacity of underlying links and at the same time, be able to increase the average profit for the overlay network. Experiments are carried out to illustrate that replication algorithm provides higher flexibility during traffic fluctuations and can quickly find a near-optimal solution.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317752,no,undetermined,0
A transparent and centralized performance management service for CORBA based applications,"The quest for service quality in enterprise applications is driving companies to profile their online performance. Application management tools come in handy to deliver the required diagnosis. However, distributed applications are hard to manage due to their complexity and geographical dispersion. To cope with this problem, this paper presents a Java based management solution for CORBA distributed applications. The solution combines XML, SNMP and portable interceptors to provide a nonintrusive performance management service. Components can be attached to client and server sides to monitor messages and gather data into a centralized database. A detailed analysis can then be performed to expose behavioral problems in specific parts of the application. Performance reports and charts are supplied through a Web console. A prototypical implementation was tested against two available ORB to assess functionality and interposed overhead.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317684,no,undetermined,0
Fault management for networks with link state routing protocols,"For network fault management, we present a new technique that is based on on-line monitoring of networks with link state routing protocols, such as OSPF (open shortest path first) and integrated IS-IS. Our approach employs an agent that monitors the on-line information of the network link state database, analyzes the events generated by network faults for event correlation, and detects and localizes the faults. We apply our method to a real network topology with various types of network faults. Experimental results show that our approach can detect and localize the faults in a timely manner, yet without disrupting normal network operations.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317646,no,undetermined,0
Efficient clustered BVH update algorithm for highly-dynamic models,"We present a new algorithm that efficiently updates bounding volume hierarchy (BVH) for ray tracing. Our algorithm is applicable in handling various types of highly-dynamic models. The algorithm produces the SAH-based BVH of good quality for rendering. The algorithm unites the advantages of some previously developed methods and offers techniques and extensions to reduce the number of per-frame BVH update operations. It works with a binary BVH where every leaf is associated with a triangle. The algorithm always tries to perform less costly operations on BVH-clusters and avoids unnecessary work if it is possible. Firstly, it detects BVH-clusters of triangles that move coherently to each other, and reinserts only cluster-roots in the proper positions of the BVH. Thus it allows efficient handling of the structural motion. Secondly, the algorithm detects the exploded BVH-clusters for performing rebuild-operations on them. Careful and efficient localizing of rebuild-space into non-overlapping clusters greatly reduces the number of rebuild-operations. It can allow independent rebuilding of all detected clusters, even if one cluster is represented by an ancestor of another. Our algorithm accelerates the total BVH update time by 2-4times on the average in comparison to the full SAH-based binned-rebuild with a set of all triangles at input.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4634632,no,undetermined,0
Team-based fault content estimation in the software inspection process,"The main objective of software inspection is to detect faults within a software artifact. This helps to reduce the number of faults and to increase the quality of a software product. However, although inspections have been performed with great success, and although the quality of the product is increased, it is difficult to estimate the quality. During the inspection process, attempts with objective estimations as well as with subjective estimations have been made. These methods estimate the fault content after an inspection and give a hint of the quality of the product. This paper describes an experiment conducted throughout the inspection process, where the purpose is to compare the estimation methods at different points. The experiment evaluates team estimates from subjective and objective fault content estimation methods integrated with the software inspection process. The experiment was conducted at two different universities with 82 reviewers. The result shows that objective estimates outperform subjective when point and confidence intervals are used. This contradicts the previous studies in the area.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317448,no,undetermined,0
Visual timed event scenarios,"Formal description of real-time requirements is a difficult and error prone task. Conceptual and tool support for this activity plays a central role in the agenda of technology transference from the formal verification engineering community to the real-time systems development practice. In this article we present VTS, a visual language to define complex event-based requirements such as freshness, bounded response, event correlation, etc. The underlying formalism is based on partial orders and supports real-time constraints. The problem of checking whether a timed automaton model of a system satisfies these sort of scenarios is shown to be decidable. Moreover, we have also developed a tool that translates visually specified scenarios into observer timed automata. The resulting automata can be composed with a model under analysis in order to check satisfaction of the stated scenarios. We show the benefits of applying these ideas to some case studies.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317439,no,undetermined,0
One more step in the direction of modularized integration concerns,"Component integration creates value by automating the costly and error-prone task of imposing desired behavioral relationships on components manually. Requirements for component integration, however, complicate software design and evolution in several ways: first, they lead to coupling among components; second, the code that implements various integration concerns in a system is often scattered over and tangled with the code implementing the component behaviors. Straightforward software design techniques map integration requirements to scattered and tangled code, compromising modularity in ways that dramatically increase development and maintenance costs.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317414,no,undetermined,0
Automated reference-counted object recycling for real-time Java,"We introduce an aspect-oriented reformulation of reference-counting that is particularly well-suited to Java applications and does not share the error-prone characteristic of manual, user-driven reference counting. We present our method in the context of the real-time specification for Java and demonstrate that it can recycle dead objects in bounded time. We apply partial evaluation to specialize the aspect-generated code, which substantially reduces the reference-counting overhead.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317289,no,undetermined,0
"Perfect Generation, Monotonicity and Finite Queueing Networks","Perfect generation, also called perfect or exact simulation, provides a new technique to sample steady-state and avoids the burn-in time period. When the simulation algorithm stops, the returned state value is in steady-state. Initiated by Propp and Wilson in the context of statistical physics, this technique is based on a coupling from the past scheme that, provided some conditions on the system, ensures convergence in a finite time to steady-state. This approach has been successfully applied in various domains including stochastic geometry, interacting particle systems, statistical physics, networking.The aim of this tutorial is to introduce the concept of perfect generation and discuss about the algorithmic design of perfect samplers. To improve the efficiency of such samplers, structural properties of models such as monotonicity are enforced in the algorithm to improve drastically the complexity. Such samplers could then been used in brute force to estimate low probability events in finite queueing networks.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4634986,no,undetermined,0
Taming lambda's for applications: the OptIPuter system software,"Summary form only given. Dense wavelength-division multiplexing (DWDM), dark fiber, and low-cost optical switches provide the technological capability for private, high bandwidth communication. However, achieving any substantial application benefit from use of these resources is dauntingly complex and error prone. These emerging environments are often called lambda grids. We are developing a simple abstraction called a distributed virtual computer (DVC), which provides convenient application use of dynamic optical resources. DVC descriptions naturally express communication and computation resource requirements, enabling coordinated resource binding. In addition, their shared namespace provides a natural vehicle for incorporating a range of novel capabilities, including novel transport protocols which expose and exploit the capabilities DWDM environment, including efficient multi-point to point (GTP), optical multicast, real-time communication, and fast point to point transports. DVC's also provide a convenient model for integrating a wide array of network-attached instruments and storage. We describe initial experience with DVC's and how they provide an integrating architecture for lambda grids. The OptlPuter project is a large multi-institutional project led by Larry Smarr at the University of California, San Diego (UCSD) and Tom DeFanti at the University of Illinois at Chicago (UIC). Other software efforts include optical signaling software, visualization, distributed configuration management, and two driving applications involving petabytes of data (in conjunction with the Biomedical Informatics Research Network and the Scripps Institute of Oceanography). The project also includes construction of a high-speed OptlPuter testbed spanning UCSD and UIC.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1316073,no,undetermined,0
Correctness Verification and Quantitative Evaluation of Timed Systems Based on Stochastic State Classes,"This tutorial addresses the integration of correctness verification and quantitative evaluation of timed systems, based on the stochastic extension of the theory of DBM state classes. In the first part, we recall symbolic state space analysis of non-deterministic models based on DBM state classes, describing the algorithms for state space enumeration and for timing analysis of individual traces.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4634988,no,undetermined,0
Novelty detection with instance-based learning for optical character quality control,"Novelty detection involves modeling the normal behavior of a system and detecting any divergence from normality which may indicate onset of damage or faults. Using instance-based learning, a novelty detection approach for optical characters quality control in machine vision inspection application is given in the paper. A normal characters information pattern adapted to special application can be established by training and products information can be effectively inspected with no delay for the print error can be automatically distinguished from print quality in the process, which has been verified by the experiment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4636545,no,undetermined,0
FIESTA-EXTRA: cell-oriented software for the defect/fault analysis in VLSI circuits,"The main concepts which laid the foundation for the special software development are considered. This software tool is named FIESTA-EXTRA (Faults Identification and EStimation of TestAbility by EXTRAction of faults probabilities, kinds of faults and usefulness of test patterns for faults detection) and is developed for defect/fault analysis in the complex gates from industrial cell library. Specific features of the main three extractors of the developed software are considered. The results of the FIESTA-ExTRA approbation are described.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1314953,no,undetermined,0
Code generation for WSLAs using AXpect,"WSLAs can be viewed as describing the service aspect of Web services. By their nature, Web services are distributed. Therefore, integrating support code into a Web service application is potentially costly and error prone. Viewed from this AOP perspective, then, we present a method for integrating WSLAs into code generation using the AXpect weaver, the AOP technology for Infopipes. This helps to localize the code physically and therefore increase the eventual maintainability and enhance the reuse of the WSLA code. We then illustrate the weavers capability by using a WSLA document to codify constraints and metrics for a streaming image application that requires CPU resource monitoring.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1314732,no,undetermined,0
Selecting software reliability models with a neural network meta classifier,"Software reliability is one of the most important quality characteristics for almost all systems. The use of a software reliability model to estimate and predict the system reliability level is fundamental to ensure software quality. However, the selection of an appropriate model for a specific case can be very difficult for project managers. This is because, there are several models that can be used and none has proved to perform well considering different projects and databases. Each model is valid only if its assumptions are satisfied. To aim at the task of choosing the best software reliability model for a dataset, this paper presents a meta-learning approach and describes experimental results from the use of a neural network meta classifier for selection among different kind of reliability models. The obtained results validate the idea and are very promising.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4634336,no,undetermined,0
On-demand location-aided QoS routing in ad hoc networks,"With the development and application of position devices, location-based routing has received growing attention. However, little study has been done on QoS routing with the aid of location information. The existing location-based routing approaches, such as flooding-based routing schemes and localized routing schemes, have their limitations. Motivated by ticket-based routing, we propose an on-demand location-aided, ticket-based QoS routing protocol (LTBR). Two special cases of LTBR, LTBR-1 and LTBR-2, are discussed in detail. LTBR-1 uses a single ticket to find a route satisfying a given QoS constraint. LTBR-2 uses multiple tickets to search valid routes in a limited area. All tickets are guided via both location and QoS information. LTBR has lower overhead compared with the original ticket-based routing, because it does not rely on an underlying routing table. On the other hand, LTBR can find routes with better QoS qualities than traditional location-based protocols. Our simulation results show that LTBR-1 can find high quality routes in relatively dense networks with high probability and very low overhead. In sparse networks, LTBR-2 can be used to enhance the probability of finding high quality routes with acceptable overhead.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1327960,no,undetermined,0
Experimental study on QoS provisioning to heterogeneous VoIP sources in Diffserv environment,"The work presents a research activity focused on the experimental study of three different issues related to the provision of VoIP services with QoS guarantee in DiffServ networks. Firstly the study deals with the analysis of two dissimilar strategies for the setting of the parameters of a traffic control module in a DiffServ network node. Using these results, secondly the effectiveness of static and dynamic SLAs strategies for QoS provisioning in DiffServ environment is experimentally evaluated. This analysis is carried out considering aggregation of voice sources adopting two distinct codecs, i.e. G723.1 and G729. These codecs produce traffic with different statistical features (variable and constant bit rate respectively). Hence, this approach allows assessing the impact on the single sources' performance of the multiplexing of heterogeneous VoIP sources in a single class.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1341824,no,undetermined,0
Direct digital synthesis: a tool for periodic wave generation (part 2),"Direct digital synthesis (DDS) is a useful tool for generating periodic waveforms. In this two-part article, the basic idea of this synthesis technique is presented and then focused on the quality of the sinewave a DDS can create, introducing the SFDR quality parameter. Next effective methods to increase the SFDR are presented through sinewave approximations, hardware schemes such as dithering and noise shaping, and an extensive list of reference. When the desired output is a digital signal, the signal's characteristics can be accurately predicted using the formulas given in this article. When the desired output is an analog signal, the reader should keep in mind that the performance of the DDS is eventually limited by the performance of the digital-to-analog converter and the follow-on analog filter. Hoping that this article would incite engineers to use DDS either in integrated circuits DDS or software-implemented DDS. From the author's experience, this technique has proven valuable when frequency resolution is the challenge, particularly when using low-cost microcontrollers.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1328096,no,undetermined,0
A recurrence-relation-based reward model for performability evaluation of embedded systems,"Embedded systems for closed-loop applications often behave as discrete-time semi-Markov processes (DTSMPs). Performability measures most meaningful to iterative embedded systems, such as accumulated reward, are thus difficult to solve analytically in general. In this paper, we propose a recurrence-relation-based (RRB) reward model to evaluate such measures. A critical element in RRB reward models is the notion of state-entry probability. This notion enables us to utilize the embedded Markov chain in a DTSMP in a novel way. More specifically, we formulate state-entry probabilities, state-occupancy probabilities, and expressions concerning accumulated reward solely in terms of state-entry probability and its companion term, namely the expected accumulated reward at the point of state entry. As a result, recurrence relations abstract away all the intermediate points that lack the memoryless property, enabling a solvable model to be directly built upon the embedded Markov chain. To show the usefulness of RRB reward models, we evaluate an embedded system for which we leverage the proposed notion and methods to solve a variety of probabilistic measures analytically.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630124,no,undetermined,0
Nanolab: a tool for evaluating reliability of defect-tolerant nano architectures,"As silicon manufacturing technology reaches the nanoscale, architectural designs need to accommodate the uncertainty inherent at such scales. These uncertainties are germane in the miniscule dimension of the device, quantum physical effects, reduced noise margins, system energy levels reaching computing thermal limits, manufacturing defects, aging and many other factors. Defect tolerant architectures and their reliability measures gain importance for logic and micro-architecture designs based on nano-scale substrates. Recently, a Markov random field (MRF) has been proposed as a model of computation for nanoscale logic gates. In this paper, we take this approach further by automating this computational scheme and a belief propagation algorithm. We have developed MATLAB based libraries and toolset for fundamental logic gates that can compute output probability distributions and entropies for specified input distributions. Our tool eases evaluation of reliability measures of combinational logic blocks. The effectiveness of this automation is illustrated in this paper by automatically deriving various reliability results for defect-tolerant architectures, such as triple modular redundancy (TMR), cascaded triple modular redundancy (CTMR) and multi-stage iterations of these. These results are used to analyze trade-offs between reliability and redundancy for these architectural configurations.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1339504,no,undetermined,0
Automatic generation of bus functional models fromtransaction level models,"This paper presents methodology and algorithms for generating bus functional models from transaction level models in system level design. Tkansaction level models are often used by designers for prototyping the bus functional architecture of the system. Being at a higher level of abstraction gives transaction level models the unique advantage of high simulation speed. This means that the designer can explore several bus functional architectures before choosing the optimal one. However, the process of converting a transaction level model to a bus functional model is not trivial. A manual conversion would not only be time consuming but also error prone. A bus functional model should also accurately represent the corresponding transaction level model. We present algorithms for automating this refinement process. Experimantal results presented using a tool based on these algorithms show their usefulness and feasibility.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1337694,no,undetermined,0
Beliefs learning in fuzzy constraint-directed agent negotiation,"This paper presents a belief learning model for fuzzy constraint-directed agent negotiation. The main features of the proposed model include: 1) fuzzy probability constraints for increasing the efficiency on the convergence of behavior patterns, and eliminating the noisy hypotheses or beliefs, 2) fuzzy instance matching method for reusing the prior opponent knowledge to speed up the problem-solving, and inferring the proximate regularities to acquire a desirable result on forecasting opponent behavior, and 3) adaptive interaction for making a dynamic concession to fulfill a desirable objective. Experimental results suggest that the proposed framework can improve both negotiation qualities.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630652,no,undetermined,0
Supporting quality of service in a non-dedicated opportunistic environment,"In This work we investigate the utilization of non-dedicated, opportunistic resources in a desktop environment to provide statistical assurances to a class of QoS sensitive, soft real-time applications. Supporting QoS in such an environment presents unique challenges: (1) soft real-time tasks must have continuous access to resources in order to deliver meaningful services. Therefore the tasks will fail if not enough idle resources are available in the system. (2) Although soft real-time tasks can be migrated from one machine to another, their QoS may be affected if there are frequent migrations. In this paper, we define two new QoS metrics (task failure rate and probability of bad migrations) to characterize these QoS failures/degradations. We also design admission control and resource recruitment algorithms to provide statistical guarantees on these metrics. Our model based simulation results show that the admission control algorithms are effective at providing the desired level of assurances, and are robust to different resource usage patterns. Our resource recruitment algorithm may need long time of observations to provide the desired guarantee. But even with moderate observations, we can reduce the probability of a bad migration from 12% to less than 4%, which is good enough for most real applications.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1336551,no,undetermined,0
Browsing and searching behavior in the Renardus Web service: a study based on log analysis,"Renardus is a distributed Web-based service, which provides integrated searching and browsing access to quality-controlled Web resources. With the overall purpose of improving Renardus, the research aims to study: the detailed usage patterns (quantitative/qualitative, paths through the system); the balance between browsing and searching or mixed activities; typical sequences of usage steps and transition probabilities in a session; typical entry points, referring sites, points of failure and exit points; and, the usage degree of the browsing support features.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1336157,no,undetermined,0
Speeding up requirements management in a product software company: linking customer wishes to product requirements through linguistic engineering,"Developing large complex software products aimed for a broad market involves a great flow of wishes and requirements. The former are elicited from customers while the latter are brought forth by the developing organization. These are preferably kept separated to preserve the different perspectives. The interrelationships should however be identified and maintained to enable well-founded decisions. Unfortunately, the current manual linkage is cumbersome, time-consuming, and error-prone. This work presents a pragmatic linguistic engineering approach to how statistical natural language processing may be used to support the manual linkage between customer wishes and product requirements by suggesting potential links. An evaluation with real requirements from industry is presented. It shows that in a realistic setting, automatic support could make linkage faster for at least 50% of the links. An estimation based on our evaluation also shows that considerable time savings are possible. The results, together with the identified enhancement, are promising for improving software quality and saving time in industrial requirements engineering.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1335685,no,undetermined,0
Helping analysts trace requirements: an objective look,"This work addresses the issues related to improving the overall quality of the requirements tracing process for independent verification and validation analysts. The contribution of the paper is three-fold: we define requirements for a tracing tool based on analyst responsibilities in the tracing process; we introduce several measures for validating that the requirements have been satisfied; and we present a prototype tool that we built, RETRO (REquirements TRacing On-target), to address these requirements. We also present the results of a study used to assess RETRO's support of requirements and requirement elements that can be measured objectively.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1335682,no,undetermined,0
A case study of reading techniques in a software company,"Software inspection is an efficient method to detect faults early in the software lifecycle. This has been shown in several empirical studies together with experiments on reading techniques. However, experiments in industrial settings are often considered expensive for a software organization. Hence, many evaluations are performed in the academic environment with artificial documents. In this paper, we describe an empirical study in a software organization where a requirements document under development is used to compare two reading techniques. There are several benefits as well as drawbacks of using this kind of approach, which are extensively discussed in the paper. The reading techniques compared is the standard technique used in the organization (checklist-based) with the test perspective of perspective-based reading. The main result is that the test perspective of perspective-based reading seems more effective and efficient than the company standard method. The impact of this study is that the software organization will apply the new reading technique in future requirements inspections.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334910,no,undetermined,0
On the persistence of computer dreams - an application framework for robust adaptive deployment,"The anticipated rewards of adaptive approaches will only be fully realised when autonomic algorithms can take configuration and deployment decisions that match and exceed those of human engineers. Such decisions are typically characterised as being based on a foundation of experience and knowledge. In humans, these underpinnings are themselves founded on the ashes of failure, the exuberance of courage and (sometimes) the outrageousness of fortune. We describe an application framework that will allow the incorporation of similarly risky, error prone and downright dangerous software artifacts into live systems - without undermining the certainty of correctness at application level. We achieve this by introducing the notion of application dreaming.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333559,no,undetermined,0
Impact of statechart implementation techniques on the effectiveness of fault detection mechanisms,"This work presents the analysis of an experiment series aiming at the discovery of the impact of two inherently different statechart implementation methods on the behavior of the resulting executables in the presence of faults. The discussion identifies the key features of implementation techniques influencing the effectiveness of standard fault detection mechanisms (memory protection, assertions etc.) and an advanced statechart-level watchdog scheme used for detecting the deviations from the abstract implementation-independent behavioral specification.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333365,no,undetermined,0
Applying generic timing tests for distributed multimedia software systems,"With recent advances in network technologies and computing power, multimedia systems have become a popular means for information delivery. However, testing of these systems is difficult. Due to incomplete control of their runtime and communication environment, precise temporal properties of multimedia systems are nonreproducible. Traditional software testing, which mainly deals with functional correctness, cannot be directly applied to testing temporal properties. Furthermore, time points are hard to be measured exactly, and in this sense are nondeterministic and nonreproducible. To address this problem, we propose a framework for testing the generic temporal properties of media objects in distributed multimedia software systems (DMSS). The timing properties are based on Allen's basic binary temporal relations between two objects, which can be extended to cover multiple objects. We have developed techniques for test case generation, and test result analysis based on a distributed tester architecture. Test templates are used in test case generation to reduce the possibility of human error, and the entire testing procedure can be automated. A prototype system has been built to test a DEC HPAS multimedia presentation system, which is a multimedia system supporting W3C's SMIL standard. Detailed discussions on practical issues illustrated with a number of actual tests are given. Experimental results have shown that our framework is effective in detecting errors in temporal properties. Furthermore, ways to reduce the test effort have been discussed, and guidelines for coming up with criteria for verdict computation based on the real-time requirements of the applications have been suggested.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1331675,no,undetermined,0
A Grammatical Swarm for protein classification,"We present a grammatical swarm (GS) for the optimization of an aggregation operator. This combines the results of several classifiers into a unique score, producing an optimal ranking of the individuals. We apply our method to the identification of new members of a protein family. Support vector machine and naive Bayes classifiers exploit complementary features to compute probability estimates. A great advantage of the GS is that it produces an understandable algorithm revealing the interest of the classifiers. Due to the large volume of candidate sequences, ranking quality is of crucial importance. Consequently, our fitness criterion is based on the area under the ROC curve rather than on classification error rate. We discuss the performances obtained for a particular family, the cytokines and show that this technique is an efficient means of ranking the protein sequences.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631142,no,undetermined,0
Module documentation based testing using Grey-Box approach,"Testing plays an important role to assure the quality of software. Testing is a process of detecting errors that can be highly effective if performed rigorously. The use of formal specifications provides significant opportunity to develop effective testing techniques. Grey-box testing approach usually based on knowledge obtains from specification and source code while seldom the design specification is concerned. In this paper, we propose an approach for testing a module with internal memory from its formal specification based on grey-box approach. We use formal specifications that are documented using Parnass Module Documentation (MD) method. The MD provides us with the information of external and internal view of a module that can be useful in greybox testing approach.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631651,no,undetermined,0
Resistance factors in the implementation of software process improvement project,"Over decades, software model for improving the quality of software through management of the software process has became significant in the software industry. Many companies are now being assessed according to standards such as the CMM, SIX-SIGMA or ISO 9000, which have brought substantial profit to the companies that utilize them to improve the quality of software product. Several companies in Malaysia have been carried out software process improvement projects. However, a software process improvement initiative is still sometimes delayed, costs are over budgeted and some of them surrender before the project ends. Therefore, this paper attempt to analyze and identify the resistance factors which influence the implementation of the software process improvement project initiated by the company. This paper will serve as reference to the professionals in the area. In the other hand, it may also helping the other companies to manage future projects through the use of preventive actions that will eliminate or at least lessening the resistance factors consequences during the implementation of the software process improvement projects. This paper present a survey with 8 Malaysias companies around Kuala Lumpur and Selangor which have an experience in initiating and conducting software process improvement project. A total of 117 respondents from various background have participated this survey.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631933,no,undetermined,0
Weighted least square estimation algorithm with software phase-locked loop for voltage sag compensation by SMES,"A superconducting magnetic energy storage (SMES) system is developed to protect a critical load from momentary voltage sags. This system is composed of a 0.3 MJ SMES coil, a 150 KVA IGBT-based current source inverter and a phase-shift inductor. In order to compensate the load voltage effectively whenever a source voltage sag happens, it is crucial for the signal processing algorithm to extract the fundamental components from the sample signals quickly, precisely and stably. In this paper, an estimation algorithm based on the weighted least square principle is developed to detect the positive- and negative-sequence fundamental components from the measured AC voltages and currents. A software phase-locked loop (SPLL) is applied to track the positive-sequence component of the source voltage. Simulations and experiments are carried out to demonstrate the algorithms. The results are presented.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1355430,no,undetermined,0
Energy-Aware Multi-Path Streaming of MPEG-4 FGS Video over Wireless,"The wireless mobile nodes are self-powered and energy-sensitive. It is critical to prevent rapid energy dissipation while streaming high quality video over wireless. We investigate the energy consumption mode of mobile nodes and propose an energy-aware scheme for efficient streaming of MPEG-4 FGS video over multiple paths in wireless. We calculate the decoding aptitude of each FGS-coded frame before its decoding deadline, based on the available energy of a mobile node, in order to fully utilize its capacity to decode frames and avoid energy waste. We give the multi-path selection model that tries to minimize the packets drop probability on each path, while taking congestion, contention, channel error, interference and mobility into considerations. By incorporating the decoding aptitude with the path selection model, packets in each frame can be transmitted according to the available energy and throughput between mobile nodes, thus no energy is wasted. If the decoding aptitude is higher than the bandwidth on a single path, more packets can be transmitted over another path, thus the quality of received video can be progressively improved. This scheme is validated on Xscale-based mobile nodes.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4627199,no,undetermined,0
Briefing a new approach to improve the EMI immunity of DSP systems,"Hereafter, we present an approach dealing to improve the reliability of digital signal processing (DSP) systems operating in real noisy (electromagnetic interference - EMI) environments. The approach is based on the coupling of two techniques: the """"DSP-oriented signal integrity improvement"""" technique deals to increase the signal-to-noise ratio (SNR) and is essentially a modification of the classic Recovery Blocks Scheme. The second technique, named """"SW-based fault handling"""" aims to detect in real-time data- and control-flow faults throughout modifications of the processor C-code. When compared to conventional approaches using Fast Fourier Transform (FIT) and Hamming Code, the primary benefit of such an approach is to improve system reliability by means of a considerably low complexity, reasonably low performance degradation and, when implemented in hardware, with reduced area overhead. Aiming to illustrate the proposed approach, we present a case study for a speech recognition system, which was partially implemented in a PC microcomputer and in a COTS microcontroller. This system was tested under a home-tailored EMI environment according to the International Standard Normative IEC 61.0004-29. The obtained results indicate that the proposed approach can effectively improve the reliability of DSP systems operating in real noise (EMI) environments.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250858,no,undetermined,0
Quantifying the quality of object-oriented design: the factor-strategy model,"The quality of a design has a decisive impact on the quality of a software product; but due to the diversity and complexity of design properties (e.g., coupling, encapsulation), their assessment and correlation with external quality attributes (e.g., maintenance, portability) is hard. In contrast to traditional quality models that express the """"goodness"""" of design in terms of a set of metrics, the novel Factor-Strategy model proposed by This work, relates explicitly the quality of a design to its conformance with a set of essential principles, rules and heuristics. This model is based on a novel mechanism, called detection strategy, that raises the abstraction level in dealing with metrics, by allowing to formulate good-design rules and heuristics in a quantifiable manner, and to detect automatically deviations from these rules. This quality model provides a twofold advantage: (i) an easier construction and understanding of the model as quality is put in connection with design principles rather than """"raw numbers""""; and (ii) a direct identification of the real causes of quality flaws. We have validated the approach through a comparative analysis involving two versions of a industrial software system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374319,no,undetermined,0
An integrated framework of the modeling of failure-detection and fault-correction processes in software reliability analysis,"The failure-detection and fault-correction are critical processes in attaining good performance of software quality. In this paper, we propose several improvements on the conventional software reliability growth models (SRGMs) to describe actual software development process by eliminating some unrealistic assumptions. Most of these models have focused on the failure detection process and not given equal priority to modeling the fault correction process. But, most latent software errors may remain uncorrected for a long time even after they are detected, which increases their impact. The remaining software faults are often one of the most unreliable reasons for software quality. Therefore, we develop a general framework of the modeling of the failure detection and fault correction processes. Furthermore, we also analyze the effect of applying the delay-time non-homogeneous Poisson process (NHPP) models. Finally, numerical examples are shown to illustrate the results of the integration of the detection and correction process.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4618163,no,undetermined,0
Considering fault dependency and debugging time lag in reliability growth modeling during software testing,"Since the early 1970s tremendous growth has been seen in the research of software reliability growth modeling. In general, software reliability growth models (SRGMs) are applicable to the late stages of testing in software development and they can provide useful information about how to improve the reliability of software products. For most existing SRGMs, most researchers assume that faults are immediately detected and corrected. However, in practice, this assumption may not be realistic and satisfied. In this paper we first give a review of fault detection and correction processes in SRGMs. We show how several existing SRGMs based on NHPP models can be comprehensively derived by applying the time-dependent delay function. Furthermore, we show how to incorporate both failure dependency and time-dependent delay function into software reliability growth modeling. We present stochastic reliability models for software failure phenomenon based on NHPPs. Some numerical examples based on real software failure data sets are presented. The results show that the proposed framework to incorporate both failure dependency and time-dependent delay function into software reliability modeling has a useful interpretation in testing and correcting the software.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376588,no,undetermined,0
Failure analysis of open faults by using detecting/un-detecting information on tests,"Recently, manufacturing defects including opens in the interconnect layers have been increasing. Therefore, a failure analysis for open faults has become important in manufacturing. Moreover, the failure analysis for open faults under BIST environment is demanded. Since the quality of the failure analysis is engaged by the resolution of locating the fault, we propose the method for locating single open fault at a stem, based on only detecting/un-detecting information on tests. Our method deduces candidate faulty stems based on the number of detections for single stuck-at fault at each fan-out branches, by performing single stuck-at fault simulation with both detecting and un-detecting tests. To improve the ability of locating the fault, the method reduces the candidate faulty stems based on the number of detections for multiple stuck-at faults at fanout branches of the candidate faulty stem, by performing multiple stuck-at fault simulation with detecting tests.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376562,no,undetermined,0
Automatic model generation of IEC 61499 function block using net condition/event systems,"The IEC 61499 standard establishes a framework specifically designed for the implementation of decentralized reconfigurable industrial automation systems. However, the process of distributed systempsilas validation and verification is difficult and error-prone. This paper discusses the needs of model generators which are capable of automatically translating IEC 61499 function blocks into formal models following specific execution semantics. In particular, this paper introduces the prototype Net Condition/Event Systems model generator and aims to summarize the generic techniques of model translation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4618273,no,undetermined,0
An initial experiment in reverse engineering aspects,"We evaluate the benefits of applying aspect-oriented software development techniques in the context of a large-scale industrial embedded software system implementing a number of crosscutting concerns. Additionally, we assess the feasibility of automatically extracting these crosscutting concerns from the source code. In order to achieve this, we present an approach for reverse engineering aspects from an ordinary application automatically. This approach incorporates both a concern verification and an aspect construction phase. Our results show that such automated support is feasible, and can lead to significant improvements in source code quality.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374335,no,undetermined,0
A static reference flow analysis to understand design pattern behavior,"Design patterns are actively used by developers expecting that they provide the design with good quality such as flexibility and reusability. However, according to industrial reports on the use of design patterns, the expectation is not always realized . Especially, points out two causes of inappropriately applied patterns from a case study on a large commercial project: developers inexperienced in design patterns and no connection with project requirement. Wrong decisions on the use of design patterns make the program difficult to understand, and refactoring the program to improve the underlying structure, especially without documentation, can be very tricky. To eliminate wrongly applied patterns or document important decisions automatically, design pattern recovery is important for not only the development phase but also the maintenance phase. Many design pattern recovery approaches focus on structural characteristics and do not touch set-up behavior that configures links between participants and precedes pattern behavior. To detect design patterns implemented in program code more precisely and to show their behavior, we analyze program at expression level. Our approach is based on statically approximating run time behavior among pattern participants. For this, a static program analysis technique is used. Many static analysis techniques for object-oriented languages exist mainly for optimizing compiler in program analysis area.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374332,no,undetermined,0
"An initial approach to assessing program comprehensibility using spatial complexity, number of concepts and typographical style","Software evolution can result in making a program harder to maintain, as it becomes more difficult to comprehend. This difficulty is related to the way the source code is formatted, the complexity of the code, and the amount of information contained within it. This work presents an initial approach that uses measures of typographical style, spatial complexity and concept assignment to measure these factors, and to model the comprehensibility of an evolving program. The ultimate aim of which is to identify when a program becomes more difficult to comprehend, triggering a corrective action to be taken to prevent this. We present initial findings from applying this approach. These findings show that this approach, through measuring these three factors, can model the change in comprehensibility of an evolving program. Our findings support the well-known claim that programs become more complex as they evolve, explaining this increase in complexity in terms of layout changes, conceptual coherence, spatial relationships between source code elements, and the relationship between these factors. This in turn can then be used to understand how maintenance affects program comprehensibility and to ultimately reduce its burden on software maintenance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374324,no,undetermined,0
MultiAgent architecture for function blocks: Intelligent configuration strategies allocation,"This paper presents multiagent architecture which detects faults in process automation and allocates intelligent algorithms in field device function blocks to solve these faults. This architecture is a FIPA-standard based agent platform and was developed using JADE and foundation fieldbus technology. The main objective is to enable problem detection activities independent of userpsilas intervention. The use of artificial neural network (ANN) based algorithms, enables the agents to find out about problem patterns and to make decisions about which algorithm can be used in which situations. With this we intend to reduce the supervisor intervention to select and implement an appropriate structure of function blocks algorithms. Furthermore these algorithms, when implemented in device function blocks, provide a solution at fieldbus level, reducing data traffic between gateway and device, and speeding up the process of dealing with the problem. An example is demonstrated with a laboratory test process where fault scenarios have been imitated.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4618319,no,undetermined,0
Middleware for Dependable Computing,"As applications become more distributed and complex, the probability of faults undoubtedly increases. Distributed systems often face some challenges, such as node failure, object crash, network partition, value fault in applications, and so on. To support designers building dependable applications, research in the field of middleware systems has proliferated. In this paper, we examine some key issues of dependable middleware systems, introduce several basic concepts related to dependable middleware, present a detailed of review of the major dependable middleware systems in this field. Finally, we point out future directions of research and conclude the paper.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4627175,no,undetermined,0
Workflow mining: Extending the algorithm to mine duplicate tasks,"Designing a workflow model is a complicated, time-consuming and error-prone process. A possible solution is workflow mining which extracts workflow models from workflow logs. Considerable researches have been done to develop heuristics to mine event-data logs in order to make a workflow model. However, if there are cyclic tasks in workflow traces, the current research in workflow mining still has problems in mining duplicate tasks. Based on the alpha-algorithm, an improved workflow mining algorithm called alpha<sup>#</sup>-algorithm is presented. The complete experiments have been done to evaluate the proposed algorithm.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620432,no,undetermined,0
Rotor cage fault diagnosis in induction motors based on spectral analysis of current Hilbert modulus,"Hilbert transformation is an ideal phase shifting tool in data signal processing. Being Hilbert transformed, the conjugated one of a signal is obtained. The Hilbert modulus is defined as the square of a signal and its conjugation. This work presents a method by which rotor faults of squirrel cage induction motors, such as broken rotor bars and eccentricity, can be diagnosed. The method is based on the spectral analysis of the stator current Hilbert Modulus of the induction motors. Theoretical analysis and experimental results demonstrate that has the same rotor fault detecting ability as the extended Park' vector approach. The vital advantage of the former is the smaller hardware and software spending compared with the existing ones.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1373123,no,undetermined,0
Strategic power infrastructure defense (SPID),"Summary form only given. An advanced system called, """"strategic power infrastructure defense (SPID) system,"""" was developed by the Advanced Power Technologies (APT) Consortium consisting of the University of Washington, Arizona State University, Iowa State University and Virginia Tech. By incorporating multi-agent system technologies, the SPID system is able to assess power system vulnerability, monitor hidden failures of protective devices, and provide adaptive control actions to prevent catastrophic failures and cascading sequences of events. The SPID program was sponsored by EPRI and the U.S. Department of Defense. In this session, the panelist will summarize the SPID methodology and the multi-agent system technologies that are critical for the implementation of the SPID system. The software agents in the SPID system are organized in a multi-layer structure to facilitate collaboration among the agents. The agents communicate through a protocol called FIPA. SPID has the ability to adapt to changes in the power infrastructure environment through the embedded machine learning capability. Simulation examples of the multi-agent system is provided.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1372735,no,undetermined,0
Possible implications of design decisions based on predictions,"Software systems and applications are increasingly constructed as assemblies of preexisting components. This makes software development cheaper and faster, and results in more favorable preconditions for achieving higher quality. This approach, however, introduces several problems, most of them originating from the fact that preexisting software components behave as black boxes. One problem is that it is difficult to analyze the properties of systems in which they are incorporated. To simplify the evaluation of system properties, different techniques have been developed to predict the behavior of systems on the basis of the properties of the constituent components. Because many cannot be formally specified, these techniques make use of statistical terms such as probability or mean value to express system properties. This paper discusses ethical aspects of the interpretation of such predictions. This problem is characteristic of many domains (data mining, safety-critical systems, etc.) but it is inherent in component-based software development",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1372495,no,undetermined,0
Systematic operational profile development for software components,"An operational profile is a quantification of the expected use of a system. Determining an operational profile for software is a crucial and difficult part of software reliability assessment in general and it can be even more difficult for software components. This paper presents a systematic method for deriving an operational profile for software components. The method uses both actual usage data and intended usage assumptions to derive a usage structure, usage distribution and characteristics of parameters (including relationships between parameters). A usage structure represents the flow and interaction of operation calls. Statecharts are used to model the usage structures. A usage distribution represents probabilities of the operations. The method is illustrated on two Java classes but can be applied to any software component that is accessed through an application program interface (API).",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371957,no,undetermined,0
Graphical Representation as a Factor of 3D Software User Satisfaction: A Metric Based Approach,"During the last few years, an increase in the development and research activity on 3D applications, mainly motivated by the rigorous growth of the game industry, is observed. This paper deals with assessing user satisfaction, i.e. a critical aspect of 3D software quality, by measuring technical characteristics of virtual worlds. Such metrics can be easily calculated in games and virtual environments of different themes and genres. In addition to that, the metric suite would provide an objective mean of comparing 3D software. In this paper, metrics concerning the graphical representation of a virtual world are introduced and validated through a pilot experiment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4621546,no,undetermined,0
Performing high efficiency source code static analysis with intelligent extensions,"This paper presents an industry practice for highly efficient source code analysis to promote software quality. As a continuous work of previously reported source code analysis system, we researched and developed a few engineering-oriented intelligent extensions to implement more cost-effective extended code static analysis and engineering processes. These include an integrated empirical scan and filtering tool for highly accurate noise reduction, and a new code checking test tool to detect function call mismatch problems, which may lead to many severe software defects. We also extended the system with an automated defect filing and verification procedure. The results show that, for a huge code base of millions of lines, our intelligent extensions not only contribute to the completeness and effectiveness of static analysis, but also establish significant engineering productivity.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371937,no,undetermined,0
Content-aware streaming of lecture videos over wireless networks,"Video streaming over wireless networks becomes increasingly important in a variety of applications. To accommodate the dynamic change of wireless networks, quality of service (QoS) scalable video streams need to be provided. This paper presents a system of content-aware wireless streaming of lecture (instructional) videos for e-learning applications. A method for real-time analysis of instructional videos is first provided to detect video content regions and classify video frames, then a 'leaking video buffer"""" model is applied to dynamically compress video streams. In our content-aware video streaming, instructional video content is detected and different QoS are selected for different types of video content. Our adaptive feedback control scheme is able to transmit properly compressed video streams to video clients not only based on wireless network bandwidth, but also based on video content and the feedback of video clients. Finally, we demonstrate the scalability and content awareness of our system and show experimental results of two lecture videos.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376695,no,undetermined,0
Safety supervision layer,"This work covers a generic approach to fault detection for operating systems in fail-safe environments. A safety supervision layer between the application layer and the operating system interface is discussed. It is an attempt to detect operating system and hardware faults in an end-to-end way. Standard POSIX system calls are wrapped by procedures that provide fault detection features. Furthermore, potentials of an additional watchdog module on top of the operating system interface are analyzed. Applications that use the Safety Supervision Layer are notified of detected faults and deal with them by providing specific handlers to bring the fail-safe system to its safe state. The goal of the presented layer is to encapsulate the operating system and hardware layers a safety-critical application resides on, in order to detect faults produced by those and bring the system to a safe state. Advantages of such an attempt are portability, lower time-to-market, higher cost efficiency in building fail-safe systems and - most important - reduced error detection latency compared to usual periodic supervision approaches.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4618104,no,undetermined,0
FarMAS: a MAS for extended quality workflow,"To date, the supply chain management systems offer no solutions for quality control of electrical domestic appliances through the traceability of components (hw or sw) information. Failures in assembled products may be detected at many points of the product life, therefore an early diagnosis could depend on the retrieval of all significant information recorded along the extended supply chain. The basic idea proposed in this work is to define a society of autonomous agents created to support the traceability of components information in a federated enterprises environment. We discuss, as a case study, a simple supply chain for the production of electrical appliances such as washing machines, refrigerators or dishwashers whose components' traceability is defined in terms of a kind of workflow extended for quality control (EQuW: extended quality workflow).",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376874,no,undetermined,0
A taxonomy and catalog of runtime software-fault monitoring tools,"A goal of runtime software-fault monitoring is to observe software behavior to determine whether it complies with its intended behavior. Monitoring allows one to analyze and recover from detected faults, providing additional defense against catastrophic failure. Although runtime monitoring has been in use for over 30 years, there is renewed interest in its application to fault detection and recovery, largely because of the increasing complexity and ubiquitous nature of software systems. We present taxonomy that developers and researchers can use to analyze and differentiate recent developments in runtime software fault-monitoring approaches. The taxonomy categorizes the various runtime monitoring research by classifying the elements that are considered essential for building a monitoring system, i.e., the specification language used to define properties; the monitoring mechanism that oversees the program's execution; and the event handler that captures and communicates monitoring results. After describing the taxonomy, the paper presents the classification of the software-fault monitoring systems described in the literature.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1377185,no,undetermined,0
Meta-heuristic Enabled MAS Optimization in Supply Chain Procurement,"This paper introduces a meta-heuristic enabled multi-agent optimization architecture for dynamic transportation planning in the supply chain procurement (SCP) plans. When multi-agent systems (MAS) are used for real-time dynamic optimization, agents seek the solution using distributed heuristics. However, distributed heuristics based on local information are prone to converge at local optimality. To escape from local optimality toward higher quality solution, we introduce meta-heuristics over agent interactions to advise agents' searching process. In this paper, we mainly propose variable neighborhood search meta-heuristic (VNS-MH) over distributed market based heuristic (DMBH), a distributed heuristic based on market interactions for transportation planning. The numerical results show that VNS-MH performs better on achieving optimality than DMBH.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617413,no,undetermined,0
An exploration of software faults and failure behaviour in a large population of programs,"A large part of software engineering research suffers from a major problem-there are insufficient data to test software hypotheses, or to estimate parameters in models. To obtain statistically significant results, a large set of programs is needed, each set comprising many programs built to the same specification. We have gained access to such a large body of programs (written in C, C++, Java or Pascal) and in this paper we present the results of an exploratory analysis of around 29,000 C programs written to a common specification. The objectives of this study were to characterise the types of fault that are present in these programs; to characterise how programs are debugged during development; and to assess the effectiveness of diverse programming. The findings are discussed, together with the potential limitations on the realism of the findings.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383110,no,undetermined,0
Are found defects an indicator of software correctness? An investigation in a controlled case study,"In quality assurance programs, we want indicators of software quality, especially software correctness. The number of found defects during inspection and testing are often used as the basis for indicators of software correctness. However, there is a paradox in this approach, since the remaining defects is what impacts negatively on software correctness, not the found ones. In order to investigate the validity of using found defects or other product or process metrics as indicators of software correctness, a controlled case study is launched. 57 sets of 10 different programs from the PSP course are assessed using acceptance test suites for each program. In the analysis, the number of defects found during the acceptance test are compared to the number of defects found during development, code size, share of development time spent on testing etc. It is concluded from a correlation analysis that 1) fewer defects remain in larger programs 2) more defects remain when larger share of development effort is spent on testing, and 3) no correlation exist between found defects and correctness. We interpret these observations as 1) the smaller programs do not fulfill the expected requirements 2) that large share effort spent of testing indicates a """"hacker"""" approach to software development, and 3) more research is needed to elaborate this issue.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383109,no,undetermined,0
Reliability estimation for statistical usage testing using Markov chains,"Software validation is an important activity in order to test whether or not the correct software has been developed. Several testing techniques have been developed, and one of these is statistical usage testing (SUT). The main purpose of SUT is to test a software product from a user's point of view. Hence, usage models are designed and then test cases are developed from the models. Another advantage of SUT is that the reliability of the software can be estimated. In this paper, Markov chains are used to represent the usage models. Several approaches using Markov chains have been applied. This paper extends these approaches and presents a new approach to estimate the reliability from Markov chains. The reliability estimation is implemented in a new tool for statistical usage testing called MaTeLo. The tool is developed in a joint European project involving six industrial partners and two university partners. The purpose of the tool is to provide an estimate of the reliability and to automatically produce test cases based on usage models described as to Markov models.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383106,no,undetermined,0
A generic method for statistical testing,"This paper addresses the problem of selecting finite test sets and automating this selection. Among these methods, some are deterministic and some are statistical. The kind of statistical testing we consider has been inspired by the work of Thevenod-Fosse and Waeselynck. There, the choice of the distribution on the input domain is guided by the structure of the program or the form of its specification. In the present paper, we describe a new generic method for performing statistical testing according to any given graphical description of the behavior of the system under test. This method can be fully automated. Its main originality is that it exploits recent results and tools in combinatorics, precisely in the area of random generation of combinatorial structures. Uniform random generation routines are used for drawing paths from the set of execution paths or traces of the system under test. Then a constraint resolution step is performed, aiming to design a set of test data that activate the generated paths. This approach applies to a number of classical coverage criteria. Moreover, we show how linear programming techniques may help to improve the quality of test, i.e. the probabilities for the elements to be covered by the test process. The paper presents the method in its generality. Then, in the last section, experimental results on applying it to structural statistical software testing are reported.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383103,no,undetermined,0
Deriving test sets from partial proofs,"Proof-guided testing is intended to enhance the test design with information extracted from the argument for correctness. The target application field is the verification of fault-tolerance algorithms where a complete formal proof is not available. Ideally, testing should be focused on the pending parts of the proof. The approach is experimentally assessed using the example of a group membership protocol (GMP), a complete proof of which has been developed by others in the PVS environment. In order to obtain a partial proof example, we proceed to flaw insertion into the PVS specification. Test selection criteria are then derived from the analysis of the reconstructed (now partial) proof. Their efficiency for revealing the flaw is experimentally assessed, yielding encouraging results.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383102,no,undetermined,0
Development of on-line vibration condition monitoring system of hydro generators,"Mechanical vibration information is critical to diagnosing the health of a generator. Existing vibration monitoring systems have poor resistibility against the strong electromagnetic interference in field and difficult to extend. In order to solve these problems, an on-line monitoring system based on LonWorks control network has been developed. In this paper, the structure of hardware and software, the functions and characteristics of system have been described in detail. The analysis result of a vibration signal has proved that this monitoring system can detect fault of generators efficiently. The system has been employed to monitor the vibration of hydro generators operation in a water power plant in Hubei province, China.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1382338,no,undetermined,0
Applying SPC to autonomic computing,"Statistical process control (SPC) is proposed as the method to frame autonomic computing system. SPC follows a data-driven approach to characterize, evaluate, predict, and improve the system services. Perspectives that are central to process measurement including central tendency, variation, stability, capability are outlined. The principles of SPC hold that by establishing and sustaining stable levels of variability, processes will yield predictable results. SPC is explored to meet and support individual autonomic computing elements' requirement. One timetabling example illustrates how SPC discover and incorporate domain-specific knowledge, thus stabilize and optimize the application service quality. The example represents reasonable application of process control that has been demonstrated to be successful in engineering point of view.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1382283,no,undetermined,0
A hybrid genetic algorithm for tasks scheduling in heterogeneous computing systems,"Efficient application scheduling is critical for achieving high performance in heterogeneous computing systems (HCS). Because an application can be partitioned into a group of tasks and represented as a directed acyclic graph (DAG), the problem can be stated as finding a schedule for a DAG to be executed in a HCS so that the schedule length can be minimized. The tasks scheduling problem is NP-hard in general, except in a few simplified situations. In order to obtain optimal or suboptimal solutions, a large number of scheduling heuristics have been presented in the literature. Genetic algorithm (GA), as a power tool to achieve global optimal, has been successfully used in this field. This work presents a new hybrid genetic algorithm to solve the scheduling problem in HCS. It uses a direct method to encode a solution into chromosome. Topological sort of DAG is used to repair the offspring in order to avoid yielding illegal or infeasible solutions, and it also guarantees that all feasible solutions can be reached with some probability. In order to remedy the GA's weakness in fine-tuning, this paper uses a greedy strategy to improve the fitness of the individuals in crossover operator, based on Lamarckian theory in the evolution. The simulation results comparing with a typical genetic algorithm and a typical list heuristic, both from the literature, show that this algorithm produces better results in terms of both quality of solution and convergence speed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1382217,no,undetermined,0
Computational promise of simultaneous recurrent network with a stochastic search mechanism,"This work explores the computational promise of enhancing Simultaneous recurrent neural networks with a stochastic search mechanism as static optimizers. Successful application of simultaneous recurrent neural networks to static optimization problems, where the training had been achieved through one of a number of deterministic gradient descent algorithms including recurrent backpropagation, backpropagation and resilient propagation, was recently reported in the literature. Accordingly at the present time, it became highly desirable to assess if enhancing the neural optimization algorithm with a stochastic search mechanism would be of substantial utility and value, which is the focus of the study reported in this paper. Two techniques are employed to assess the added value of a potential enhancement through a stochastic search mechanism: one method entails comparison of SRN performance with a stochastic search algorithm, the genetic algorithm, and the second method leverages estimation for the quality of optimal solutions through Held-Karp bounds. The traveling salesman problem is employed as the benchmark for the simulation study reported herein. Simulation results suggest that there is likely to be significant improvement possible in the quality of solutions for the traveling salesman problem, and potentially other static optimization problems, if the Simultaneous recurrent neural network is augmented with a stochastic search mechanism.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1380969,no,undetermined,0
Case study of condition based health maintenance of large power transformer at Rihand substation using on-line FDD-EPT,"The constant monitoring of large, medium and small power transformers for purposes of assessing their health, operating condition while maximizing personnel resources and preserving capital is often a topic of spirited discussions at both national and international conferences. Further, the considerations of transformers being out of service for extended periods of time due to conditions that, with the proper diagnostic monitoring equipment could have preemptively detected, diagnosed and prevented catastrophic equipment failure is of prime importance in today's economic conditions. These operating conditions are becoming more serious in several locations around the world. A recent case study of PD monitoring done at the Riband substation in India is discussed here in the sequel. Additional transformers tested in India for PGCIL (Ballabgarh) and NTPC (Noida, Delhi) in 1999 will be presented using the FDD-EPT system. Demonstrations of the FDD-EPT (fault diagnostic device for electrical power transformers) system on transformers for BHEL, MP and Tata Power in Mumbai also provided encouraging results. This will further illustrate the efficacy of this system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1380659,no,undetermined,0
Wireless download agent for high-capacity wireless LAN based ubiquitous services,"We propose a wireless download agent which effectively controls the point at which a download starts for providing a comfortable mobile computing environment. A user can get the desired data with the wireless download agent while walking through a service area without stopping. We conducted simulations to evaluate its performance in terms of throughput, download period, and probability of successful download. Our results show that the proposed scheme very well suits the wireless download agent in high-speed wireless access systems with many users. Furthermore, we describe the use of the proposed scheme considering the randomness of the walking directions of users.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1378951,no,undetermined,0
Reliability of intelligent power routers,"In this work we seek to determine the reliability of intelligent power routers (IPR). The IPR is our building block to provide scalable coordination in a distributed model for the next generation power network. Our goal in the IPR project is to show that by distributing network intelligence and control functions using the IPR, we will be capable of achieving improved survivability, security, reliability, and re-configurability. In order to calculate the change in reliability of a system operated with and without IPR, the IPR failure mechanisms and failure probabilities must be determined. Since there is no actual data on IPR reliability, none has been built; its failure mechanisms and failure probability will be established by analogy to data routers. In this paper we consider a basic IPR structure consisting of: power hardware (breakers or other power switching elements), computer hardware (communication between IPR and CPU functions), and software. We establish the failure modes for each element of the selected IPR structure to estimate the IPR reliability. This estimate of failure probability will be used in our future work to measure the change in reliability of a power system operated with and without IPR.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1378796,no,undetermined,0
A Crossover Game Routing Algorithm for Wireless Multimedia Sensor Networks,"The multi-constrained QoS-based routing problem of wireless multimedia sensor networks is an NP hard problem. Genetic algorithms (GAs) have been used to handle these NP hard problems in wireless networks. Because the crossover probability is a key factor of GAs' action and performance, and affects the convergence of GAs, and the selection of crossover probability is very difficult, so we propose a novel method - crossover game instead of probability crossover. The crossover game in routing problems is based of each node has restricted energy, and each node trend to get maximal whole network benefit but pay out minimum cost. The players of crossover game are individual routes. The individual would perform crossover operator if it is Nash equilibrium in crossover game. The Simulation results demonstrate that this method is effective and efficient.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617437,no,undetermined,0
Static analyzer of vicious executables (SAVE),"Software security assurance and malware (Trojans, worms, and viruses, etc.) detection are important topics of information security. Software obfuscation, a general technique that is useful for protecting software from reverse engineering, can also be used by hackers to circumvent the malware detection tools. Current static malware detection techniques have serious limitations, and sandbox testing also fails to provide a complete solution due to time constraints. In this paper, we present a robust signature-based malware detection technique, with emphasis on detecting obfuscated (or polymorphic) malware and mutated (or metamorphic) malware. The hypothesis is that all versions of the same malware share a common core signature that is a combination of several features of the code. After a particular malware has been first identified, it can be analyzed to extract the signature, which provides a basis for detecting variants and mutants of the same malware in the future. Encouraging experimental results on a large set of recent malware are presented.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1377239,no,undetermined,0
Adaptive random testing by localization,"Based on the intuition that widely spread test cases should have greater chance of hitting the nonpoint failure-causing regions, several adaptive random testing (ART) methods have recently been proposed to improve traditional random testing (RT). However, most of the ART methods require additional distance computations to ensure an even spread of test cases. In this paper, we introduce the concept of localization that can be integrated with some ART methods to reduce the distance computation overheads. By localization, test cases would be selected from part of the input domain instead of the whole input domain, and distance computation would be done for some instead of all previous test cases. Our empirical results show that the fault detecting capability of our method is comparable to those of other ART methods.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371931,no,undetermined,0
A BBN Based Approach for Improving a Telecommunication Software Estimation Process,"This paper describes analytically a methodology for improving the estimation process of a small-medium telecommunication (TLC) company. All the steps required for the generation of estimates such as data collection, data transformation, estimation model extraction and finally exploitation of the knowledge explored are described and demonstrated as a case study involving a Greek TLC company. Based on this knowledge certain interventions are suggested in the current process of the company under study in order to include formal estimation procedures in each development phase.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4621548,no,undetermined,0
Rating Agencies Interoperation for Peer-to-Peer Online Transactions,"In current peer-to-peer systems users interact with unknown services and users for the purpose of online transactions such as file sharing and trading of commodities. Peer-to-Peer reputation systems allow users to assess the trustworthiness of unknown entities based on subjective feedback from the other peers. However, this cannot constitute sufficient proof for many transactions like service composition, negotiations and coalition formation in which users require more solid proof of the quality of unknown services. Ratings certified by trusted third parties in the form of a security token are objective and reliable and, hence, allow building trust between peers. Because of the decentralized and distributed nature of peer-to-peer networks, a central authority (or hierarchy of them) issuing such certificates would not scale up. We propose a framework for peer-to-peer agencies interoperation based on rating certificates and meta certificates describing bilateral agencies relations.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4622579,no,undetermined,0
A Fault Tolerance Scheme for Hierarchical Dynamic Schedulers in Grids,"In dynamic grid environment failures (e.g. link down, resource failures) are frequent. We present a fault tolerance scheme for hierarchical dynamic scheduler (HDS) for grid workflow applications. In HDS all resources are arranged in a hierarchy tree and each resource acts as a scheduler. The fault tolerance scheme is fully distributed and is responsible for maintaining the hierarchy tree in the presence of failures. Our fault tolerance scheme handles root failures specially, which avoids root becoming single point of failure. The resources detecting failures are responsible for taking appropriate actions. Our fault tolerance scheme uses randomization to get rid of multiple simultaneous failures. Our simulation results show that the recovery process is fast and the failures affect minimally to the scheduling process.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4626780,no,undetermined,0
Automatic generation of Markov chain usage models from real-time software UML models,"The paper concerns automatic generation of usage models from real-time software UML models. Firstly, we define the reasonably constrained real-time software UML artifacts, which include use case diagrams, timed sequence diagrams and the execution probability of each sequence diagram in its associated use case. Secondly, the paper presents a method that derives the software usage model from the constrained UML artifacts. The method elicits the messages associated with the objects under testing and their occurrence probabilities to generate the usage model of each use case. Timing constraints in sequence diagrams are considered during usage model generation. Then the usage models of use cases are integrated into the software usage model by utilizing the execution sequence relations between use cases. The usage models can be used to generate real-time software statistical test cases and facilitate real-time software statistical testing.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357941,no,undetermined,0
Comparing several coverage criteria for detecting faults in logical decisions,"Many testing coverage criteria, including decision coverage and condition coverage, are well-known to be inadequate for software characterised by complex logical decisions, such as those in safety-critical software. In the past decade, more sophisticated testing criteria have been advocated. In particular, compliance of MC/DC has been mandated in the aviation industry for the approval of airborne software. On the other hand, the MUMCUT criterion has been proved to guarantee the detection of certain faults in logical decisions in irredundant disjunctive normal form. We analyse and empirically evaluate the ability of test sets satisfying these testing criteria in detecting faults in logical decisions. Our results show that MC/DC test sets are effective, but they may still miss some faults that can almost always be detected by test sets satisfying the MUMCUT criterion.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357940,no,undetermined,0
Software failure rate and reliability incorporating repair policies,"Reliability of a software application, its failure rate and the residual number of faults in an application are the three most important metrics that provide a quantitative assessment of the failure characteristics of an application. Typically, one of many stochastic models known as software reliability growth models (SRGMs) is used to describe the failure behavior of an application in its testing phase, and obtain an estimate of the above metrics. In order to ensure analytical tractability, SRGMs are based on an assumption of instantaneous repair and thus the estimates of the metrics obtained using SRGMs tend to be optimistic. In practice, fault repair activity consumes a nonnegligible amount of time and resources. Also, repair may be conducted according to many policies which are reflective of the schedule and budget constraints of a project. A few research efforts that have sought to incorporate repair into SRGMs are restrictive, since they consider only one of the several SRGMs, model the repair process using a constant rate, and provide an estimate of only the residual number of faults. These techniques do not address the issue of estimating application failure rate and reliability in the presence of repair. In this paper we present a generic framework which relies on the rate-based simulation technique in order to provide the capability to incorporate various repair policies into the finite failure nonhomogeneous Poisson process (NHPP) class of software reliability growth models. We also present a technique to compute the failure rate and the reliability of an application in the presence of repair. The potential of the framework to obtain quantitative estimates of the above three metrics taking into consideration different repair policies is illustrated using several scenarios.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357924,no,undetermined,0
Assessing usability through perceptions of information scent,Information scent is an establish concept for assessing how users interact with information retrieval systems. This paper proposes two ways of measuring user perceptions of information scent in order to assess the product quality of Web or Internet information retrieval systems. An empirical study is presented which validates these measures through an evaluation based on a live e-commerce application. This study shows a strong correlation between the measures of perceived scent and system usability. Finally the wider applicability of these methods is discussed.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357919,no,undetermined,0
Assessing quantitatively a programming course,"The focus on assessment and measurement represents the main distinction between programming course and software engineering courses in computer curricula. We introduced testing as an essential asset of a programming course. It allows precise measurement of the achievements of the students and allows an objective assessment of the teaching itself. We measured the size and evolution of the programs developed by the students and correlated these metrics with the grades. We plan to collect progressively a large baseline. We compared the productivity and defect density of the program developed by the students during the exam to industrial data and similar academic experiences. We found that the productivity of our students is very high even compared to industrial settings. Our defect density (before rework) is higher than the industrial, which includes rework.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357918,no,undetermined,0
Assessment of software measurement: an information quality study,"This paper reports on the first phase of an empirical research project concerning methods to assess the quality of the information in software measurement products. Two measurement assessment instruments are developed and deployed in order to generate two sets of analyses and conclusions. These sets will be subjected to an evaluation of their information quality in phase two of the project. One assessment instrument was based on AIMQ, a generic model of information quality. The other instrument was developed by targeting specific practices relating to software project management and identifying requirements for information support. Both assessment instruments delivered data that could be used to identify opportunities to improve measurement The generic instrument is cheap to acquire and deploy, while the targeted instrument requires more effort to build. Conclusions about the relative merits of the methods, in terms of their suitability for improvement purposes, await the results from the second phase of the project.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357917,no,undetermined,0
Assessing the impact of active guidance for defect detection: a replicated experiment,"Scenario-based reading (SBR) techniques have been proposed as an alternative to checklists to support the inspectors throughout the reading process in the form of operational scenarios. Many studies have been performed to compare these techniques regarding their impact on the inspector performance. However, most of the existing studies have compared generic checklists to a set of specific reading scenarios, thus confounding the effects of two SBR key factors: separation of concerns and active guidance. In a previous work we have preliminarily conducted a repeated case study at the University of Kaiserslautern to evaluate the impact of active guidance on inspection performance. Specifically, we compared reading scenarios and focused checklists, which were both characterized as being perspective-based. The only difference between the reading techniques was the active guidance provided by the reading scenarios. We now have replicated the initial study with a controlled experiment using as subjects 43 graduate students in computer science at University of Bari. We did not find evidence that active guidance in reading techniques affects the effectiveness or the efficiency of defect detection. However, inspectors showed a better acceptance of focused checklists than reading scenarios.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357909,no,undetermined,0
A controlled experiment for evaluating a metric-based reading technique for requirements inspection,"Natural language requirements documents are often verified by means of some reading technique. Some recommendations for defining a good reading technique point out that a concrete technique must not only be suitable for specific classes of defects, but also for a concrete notation in which requirements are written. Following this suggestion, we have proposed a metric-based reading (MBR) technique used for requirements inspections, whose main goal is to identify specific types of defects in use cases. The systematic approach of MBR is basically based on a set of rules as """"if the metric value is too low (or high) the presence of defects of type de fType<sub>1</sub>,...de fType<sub>n</sub> must be checked"""". We hypothesised that if the reviewers know these rules, the inspection process is more effective and efficient, which means that the defects detection rate is higher and the number of defects identified per unit of time increases. But this hypotheses lacks validity if it is not empirically validated. For that reason the main goal is to describe a controlled experiment we carried out to ascertain if the usage of MBR really helps in the detection of defects in comparison with a simple checklist technique. The experiment result revealed that MBR reviewers were more effective at detecting defects than checklist reviewers, but they were not more efficient, because MBR reviewers took longer than checklist reviewers on average.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357908,no,undetermined,0
A Simulation Framework for Dependable Distributed Systems,"The use of discrete-event simulators in the design and development of distributed systems is appealing due to their efficiency and scalability. Their core abstractions of process and event map neatly to the components and interactions of modern-day distributed systems and allow designing realistic simulation scenarios. MONARC, a multi-threaded, process oriented simulation framework designed for modeling large scale distributed systems, allows the realistic simulation of a wide-range of distributed system technologies, with respect to their specific components and characteristics. In this paper we present an innovative solution to the problem of evaluating the dependability characteristic of distributed systems. Our solution is based on several proposed extensions to the simulation model of the MONARC simulation framework. These extensions refer to fault tolerance and system orchestration mechanisms being added in order to assess the reliability and availability of distributed systems. The extended simulation model includes the necessary components to describe various actual failure situations and provides the mechanisms to evaluate different strategies for replication and redundancy procedures, as well as security enforcement mechanisms.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4626799,no,undetermined,0
DAST: A QoS-Aware Routing Protocol for Wireless Sensor Networks,"In wireless sensor networks (WSNs), a challenging problem is how to advance network QoS. Energy-efficiency, network communication traffic and failure-tolerance, these important factors of QoS are closely related with the applied performance of WSNs. Hence a QoS-aware routing protocol called directed alternative spanning tree (DAST) is proposed to balance the above three factors of QoS. A directed tree-based model is constructed to bring data transmission more motivated and efficient. Based on Markov, a communication state predicted mechanism is proposed to choose reasonable parent, and packet transmission to double-parent is submitted with alternative algorithm. For enhancing network failure-tolerance, routing reconstruction is studied on. With the simulations, the proposed protocol is evaluated in comparison with the existing protocols from energy efficiency to the failure-tolerance. The performance of DAST is verified to be efficient and available, and it is competent for satisfying QoS of WSNs.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4627168,no,undetermined,0
Probabilistic evaluation of object-oriented systems,"The goal of this study is the development of a probabilistic model for the evaluation of flexibility of an object-oriented design. In particular, the model estimates the probability that a certain class of the system gets affected when new functionality is added or when existing functionality is modified. It is obvious that when a system exhibits a large sensitivity to changes, the corresponding design quality is questionable. Useful conclusions can be drawn from this model regarding the comparative evaluation of two or more object-oriented systems or even the assessment of several generations of the same system, in order to determine whether or not good design principles have been applied. The proposed model has been implemented in a Java program that can automatically analyze the class diagram of a given system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357888,no,undetermined,0
A neuro-fuzzy tool for software estimation,"Accurate software estimation such as cost estimation, quality estimation and risk analysis is a major issue in software project management. We present a soft computing framework to tackle this challenging problem. We first use a preprocessing neuro-fuzzy inference system to handle the dependencies among contributing factors and decouple the effects of the contributing factors into individuals. Then we use a neuro-fuzzy bank to calibrate the parameters of contributing factors. In order to extend our framework into fields that lack of an appropriate algorithmic model of their own, we propose a default algorithmic model that can be replaced when a better model is available. Validation using industry project data shows that the framework produces good results when used to predict software cost.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357862,no,undetermined,0
"Analysis, testing and re-structuring of Web applications","The current situation in the development of Web applications is reminiscent of the early days of software systems, when quality was totally dependent on individual skills and lucky choices. In fact, Web applications are typically developed without following a formalized process model: requirements are not captured and design is not considered; developers quickly move to the implementation phase and deliver the application without testing it. Not differently from more traditional software system, however, the quality of Web applications is a complex, multidimensional attribute that involves several aspects, including correctness, reliability, maintainability, usability, accessibility, performance and conformance to standards. In this context, aim of this PhD thesis was to investigate, define and apply a variety of conceptual tools, analysis, testing and restructuring techniques able to support the quality of Web applications. The goal of analysis and testing is to assess the quality of Web applications during their development and evolution; restructuring aims at improving the quality by suitably changing their structure.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357838,no,undetermined,0
Design and Analysis of Embedded GPS/DR Vehicle Integrated Navigation System,"Global Position system (GPS) is a positioning system with superior long-term error performance, while Dead Reckoning (DR) system has good positioning precision in short-term, through advantage complementation, a GPS/DR integration provides position data with high reliability for vehicle navigation system. This paper focuses on the design of the embedded GPS/DR vehicle integrated navigation system using the nonlinear Kalman filtering approach. The signal's observation gross errors are detected and removed at different resolution levels based on statistic 3sigma- theory, and navigation data are solved with Extended Kalman filter in real-time, the fault tolerance and precision of the vehicle integrated navigation system are improved greatly.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4627169,no,undetermined,0
Extracting facts from open source software,"Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But because open source software is often developed without proper management, the quality and reliability of the code may be uncertain. The quality of the code needs to be measured and this can be done only with the help of proper tools. We describe a framework called Columbus with which we calculate the object oriented metrics validated by Basili et al. for illustrating how fault-proneness detection from the open source Web and e-mail suite called Mozilla can be done. We also compare the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development. The Columbus framework has been further developed recently with a compiler wrapping technology that now gives us the possibility of automatically analyzing and extracting information from software systems without modifying any of the source code or makefiles. We also introduce our fact extraction process here to show what logic drives the various tools of the Columbus framework and what steps need to be taken to obtain the desired facts.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357790,no,undetermined,0
On the statistical properties of the F-measure,"The F-measure - the number of distinct test cases to detect the first program failure - is an effectiveness measure for debug testing strategies. We show that for random testing with replacement, the F-measure is distributed according to the geometric distribution. A simulation study examines the distribution of two adaptive random testing methods, to study how closely their sampling distributions approximate the geometric distribution, revealing that in the worst case scenario, the sampling distribution for adaptive random testing is very similar to random testing. Our results have provided an answer to a conjecture that adaptive random testing is always a more effective alternative to random testing, with reference to the F-measure. We consider the implications of our findings for previous studies conducted in the area, and make recommendations to future studies.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357955,no,undetermined,0
An integrated design of multipath routing with failure survivability in MPLS networks,"Multipath routing employs multiple parallel paths between the source and destination for a connection request to improve resource utilization of a network. In this paper, we provide an integrated design of multipath routing in MPLS networks. In addition, we take into account the quality of service (QoS) in carrying delay-sensitive traffic and failure survivability in the design. Path protection or restoration policies enables the network to accommodate link failures and avoid traffic loss. We evaluate the performance of the proposed schemes in terms of call blocking probability, network resource utilization and load balancing factor. The results demonstrate that the proposed integrated design framework can provide effective network failure survivability, and also achieve better load balancing and/or higher network resource utilization",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359429,no,undetermined,0
An improved repository system for effective and efficient reuse of formal verification efforts,"This paper presents several enhancements to ARIFS, a reuse environment that sets the foundations for reusing formal verification efforts in an iterative and incremental software process for the design of distributed reactive systems. A criterion based on generic components is added, together with a self-learning mechanism, to reduce the search space and maximize the probability of retrieving useful information. Besides, a formalization is given on how to apply verification tasks on a reduced number of states when the retrieved information is not enough for the user's intents. These enhancements are shown to improve both the effectiveness and the efficiency of ARIFS.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371903,no,undetermined,0
Assessing and improving state-based class testing: a series of experiments,"This work describes an empirical investigation of the cost effectiveness of well-known state-based testing techniques for classes or clusters of classes that exhibit a state-dependent behavior. This is practically relevant as many object-oriented methodologies recommend modeling such components with statecharts which can then be used as a basis for testing. Our results, based on a series of three experiments, show that in most cases state-based techniques are not likely to be sufficient by themselves to catch most of the faults present in the code. Though useful, they need to be complemented with black-box, functional testing. We focus here on a particular technique, Category Partition, as this is the most commonly used and referenced black-box, functional testing technique. Two different oracle strategies have been applied for checking the success of test cases. One is a very precise oracle checking the concrete state of objects whereas the other one is based on the notion of state invariant (abstract states). Results show that there is a significant difference between them, both in terms of fault detection and cost. This is therefore an important choice to make that should be driven by the characteristics of the component to be tested, such as its criticality, complexity, and test budget.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359770,no,undetermined,0
Crystal Ball and Design for Six Sigma,"8 In today's competitive market, businesses are adopting new practices like Design For Six Sigma (DFSS), a customer driven, structured methodology for faster-to-market, higher quality, and less costly new products and services. Monte Carlo simulation and stochastic optimization can help DFSS practitioners understand the variation inherent in a new technology, process, or product, and can be used to create and optimize potential designs. The benefits of understanding and controlling the sources of variability include reduced development costs, minimal defects, and sales driven through improved customer satisfaction. This tutorial uses Crystal Ball Professional Edition, a suite of easy-to-use Microsoft Excel-based software, to demonstrate how stochastic simulation and optimization can be used in all five phases of DFSS to develop the design for a new compressor.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371517,no,undetermined,0
"Quality assessment, verification, and validation of modeling and simulation applications","Many different types of modeling and simulation (M&S) applications are used in dozens of disciplines under diverse objectives including acquisition, analysis, education, entertainment, research, and training. M&S application verification and validation (V&V) are conducted to assess mainly the accuracy, which is one of many indicators affecting the M&S application quality. Much higher confidence can be achieved in accuracy if a quality-centered approach is used. This paper presents a quality model for assessing the quality of large-scale complex M&S applications as integrated with V&V. The guidelines provided herein should be useful for assessing the overall quality of an M&S application.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371309,no,undetermined,0
Assessing biogenic emission impact on the ground-level ozone concentration by remote sensing and numerical model,"Emission inventory data is one of the major inputs for all air quality simulation models. Emission inventory data for the prediction of ground-level ozone concentration, grouped as point, area, mobile and biogenic sources, are a composite of all reported and estimated pollutant emission information from many organizations. Before applying air quality simulation model, the emission inventory data generally require additional processing for meeting spatial, temporal, and speciation requirements using advanced information technologies. In this study, SMOKE was setup to update the essential emission processing. The emission processing work was performed to prepare emission input for U.S. EPA's Models-3/CMAQ. The fundamental anthropogenic emission inventory commonly used in Taiwan is the TEDS 4.2 software package. However, without the proper inclusion of accurate estimation of biogenic emission, the estimation of ground-level ozone concentration may not be meaningful. With the aid of SPOT satellite image, biogenic gas emission modeling analysis can be achieved to fit in BEIS-2 in SMOKE. Improved utilization of land use identification data, based on SPOT outputs and emission factors, may be influential in support of the modeling work. During this practice, land use was identified via an integrated assessment based on both geographical information system and remote sensing technologies; and emission factors were adapted from a series of existing database in the literature. The research findings clearly indicate that the majority of biogenic VOCs emissions occurred in the mountains and farmland actually exhibit fewer impacts on ground-level ozone concentration in populated areas than the anthropogenic emissions in South Taiwan. This implies that fast economic growth ends up with sustainability issue due to overwhelming anthropogenic emissions",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1370183,no,undetermined,0
Dynamic Multipath Allocation in Ad Hoc Networks,"Ad hoc networks are characterized by fast dynamic changes in the topology of the network. A known technique to improve QoS is to use multipath routing where packets (voice/video/...) from a source to a destination travel in two or more maximal disjoint paths. We observe that the need to find a set of maximal disjoint paths can be relaxed by finding a set of paths S wherein only bottlenecked links are bypassed. In the proposed model we assume that there is only one edge along a path in S is a bottleneck and show that by selecting random paths in S the probability that bottlenecked edges get bypassed is high. We implemented this idea in the MRA system which is a highly accurate visual ad hoc simulator currently supporting two routing protocols AODV and MRA. We have extended the MRA protocol to use multipath routing by maintaining a set of random routing trees from which random paths can be easily selected. Random paths are allocated/released by threshold rules monitoring the session quality. The experiments show that: (1) session QoS is significantly improve, (2) the fact that many sessions use multiple paths in parallel does not depredate overall performances, (3) the overhead in maintaining multipath in the MRA algorithm is negligible.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4622762,no,undetermined,0
A screened Coulomb scattering module for displacement damage computations in Geant4,"A new software module adding screened Coulomb scattering to the Monte Carlo radiation simulation code Geant4 has been applied to compute the nonionizing component of energy deposited in semiconductor materials by energetic protons and other forms of radiation. This method makes it possible to create three-dimensional maps of nonionizing energy deposition from all radiation sources in structures with complex compositions and geometries. Essential aspects of previous NIEL computations are confirmed, and issues are addressed both about the generality of NIEL and the ability of beam experiments to simulate the space environment with high fidelity, particularly for light ion irradiation at very high energy. A comparison of the displacement energy deposited by electromagnetic and hadronic interactions of a proton beam with published data on GaAs LED degradation supports the conclusion of previous authors that swift light ions and slower heavy ions produce electrically active defects with differing efficiencies. These results emphasize that, for devices with extremely small dimensions, it is increasingly difficult to predict the response of components in space without the assistance of computational modeling.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1369542,no,undetermined,0
Development of Fault Detection System in Air Handling Unit,"Monitoring systems used at present to operate air handling unit(AHU) optimally do not have a function that enables to detect faults properly when there are faults of such as operating plants or performance falling, so they are unable to manage faults rapidly and operate optimally. In this paper, we have developed a classified rule-based fault detection system which can be inclusively used in AHU system of a building by installation of sensor which is composed of AHU system and required low costs compare to the model based fault detection system which can be used only in a special building or system. In order to experiment this algorithm, it was applied to AHU system which is installed inside environment chamber(EC), verified its own practical effect, and confirmed its own applicability to the related field in the future.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4622840,no,undetermined,0
The Study of Response Model & Mechanism Against Windows Kernel Compromises,"Malicious codes have been widely documented and detected in information security breach occurrences of Microsoft Windows platform. Legacy information security systems are particularly vulnerable to breaches, due to Window kernel-based malicious codes,that penetrate existing protection and remain undetected. To date there has not been enough quality study into and information sharing about Windows kernel and inner code mechanisms, and this is the corereason for the success of these codes into entering systems and remaining undetected. This paper focus on classification and formalization of type, target and mechanism of various Windows kernel-based attacks, and will present suggestions for effective response methodologies in the categories of ; """"Kernel memory protection"""", """"process & driver protection"""" and """"File system & registry protection"""". An effective Windows kernel protection system will be presented through the collection and analysis of Windows kernel and inside mechanisms, and through suggestions for the implementation methodologies of unreleased and new Windows kernel protection skill. Results presented in this paper will explain that the suggested system be highly effective and has more accurate for intrusion detection ratios, then the current legacy security systems (i.e., virus vaccines, and Windows IPS, etc) intrusion detection ratios. So, It is expected that the suggested system provides a good solution to prevent IT infrastructure from complicated and intelligent Windows kernel attacks.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4622892,no,undetermined,0
A Scalable Method for Improving the Performance of Classifiers in Multiclass Applications by Pairwise Classifiers and GA,"In this paper, a new combinational method for improving the recognition rate of multiclass classifiers is proposed. The main idea behind this method is using pairwise classifiers to enhance the ensemble. Because of more accuracy of them, they can decrease the error rate in error-prone feature space. Firstly, a multiclass classifier has been trained. Then, regarding to confusion matrix and evaluation data, the pair-classes that have the most error have been derived. After that, pairwise classifiers have been trained and added to ensemble of classifiers. Finally, weighted majority vote for combining the primary results is applied. In this paper, multi layer perceptron is used as base classifier. Also, GA determines the optimized weights in final classifier. This method is evaluated on a Farsi digit handwritten dataset. Using proposed method, the recognition rate of simple multiclass classifier has been improved from 97.83 to 98.89 which shows an adequate improvement.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624131,no,undetermined,0
A route for qualifying/certifying an affordable structural prognostic health management (SPHM) system,"There is a growing interest in developing affordable SPHM systems that use artificial intelligence (AI) techniques and existing flight parameters to track how each individual aircraft is used and to quantify the damaging effects of usage. Over the past four years, Smiths and BAE systems have launched collaboration work to evolve a practical SPRM system. The collaborative work has built on BAE systems vast experience of operational load monitoring (OLM) that has spanned more than 30 years. The collaborative work has also built on the unique experience of Smiths over the past 20 years that has produced automatic data correction algorithms, mathematical networks (MNs), dynamic models and flight and usage management software (FUMSTM). Smiths and BAE systems have been also carrying out extensive investigations that lead to establishing and agreeing a route for qualifying/certifying AI based systems, an essential work element to avoid a delay in introducing SPHM into service use. The investigations have covered assessing the adequacy and quality of the truth data required to train/configure an AI method. They have also addressed the regulatory authority (RA) concern that any volume of data gathered to train an AI method would not capture the truth and some operations/configurations may produce novel data outside the training data. Guidelines for management approaches based on individual aircraft tracking (IAT) have been also investigated. The paper presents the results of the Smiths and BAE systems collaborative work and presents preliminary guidelines for qualifying/certifying AI methods.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1368197,no,undetermined,0
Real-time diagnosis and prognosis with sensors of uncertain quality,"This work presents a real-time approach to the detection, isolation, and prediction of component failures in large-scale systems through the combination of two modules. The modules themselves are then used in conjunction with an inference engine, TEAMS-RT, which is part of Qualtech Systems integrated diagnostic toolset, to provide the end user with accurate diagnostic and prognostic information about the state of the system. The first module is a filter used to """"clean"""" observed test results from multiple sensors from system noise. The sensors have false alarm and missed detection probabilities that are not known a-priori, and must be estimated - ideally along with the accuracies of these estimates - online, within the inference engine. Further, recognizing a practical concern in most real systems, a sparsely instantiated observation vector must not be problematic. Multiple hypothesis tracking (MHT) is at the heart of the filtering algorithm and beta prior distributions are applied to the sensor errors. The second module is a prognostic engine that uses an interacting multiple model (IMM) approach to track the """"trajectory"""" of degrading sensors. Kalman filters estimate the movement in each dimension of the sensors. The current state and trajectory of each sensor is then used to predict the time to failure value, i.e., when the component corresponding to the sensor is no longer usable. The modules are integrated together and as part of the TEAMS-RT suite; logic is presented for the cases that they disagree.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1368178,no,undetermined,0
Configurable fault-tolerant processor (CFTP) for spacecraft onboard processing,"The harsh radiation environment of space, the propensity for SEUs to perturb the operations of silicon-based electronics, the rapid development of microprocessor capabilities and hence software applications, and the high cost (dollars and time) to develop and prove a system, require flexible, reliable, low cost, rapidly developed system solutions. A reconfigurable triple-modular-redundant (TMR) system-on-a-chip (SOC) utilizing field-programmable gate arrays (FPGAs) provides a practical solution for space-based systems. The configurable fault-tolerant processor (CFTP) is such a system, designed specifically for the purpose of testing and evaluating, on orbit, both the reliability of instantiated TMR soft-core microprocessors, the ability to reconfigure the system to support any onboard processor function, and the means for detecting and correcting SEU-induced configuration faults. The CFTP utilizes commercial off-the-shelf (COTS) technology to investigate a low-cost, flexible alternative to processor hardware architecture, with a total-ionizing-dose (TID) tolerant FPGA as the basis for a SOC. The flexibility of a configurable processor, based on FPGA technology, enables on-orbit upgrades, reconfigurations, and modifications to the soft-core architecture in order to support dynamic mission requirements. Single event upsets (SEU) to the data stored in the FPGA-based soft-core processors are detected and corrected by the TMR architecture. SEUs affecting the FPGA configuration itself are corrected by background """"scrubbing"""" of the configuration. The CFTP payload consists of a printed circuit board (PCB) of 5.3 inches7.3 inches utilizing a slightly modified PC/104 bus interface. The initial FPGA configuration is an instantiation of a TMR processor, with included error detection and correction (EDAC) and memory controller circuitry. The PCB is designed with requisite supporting circuitry including a configuration controller FPGA, SDRAM, and flash memory in order to allow the greatest variety of possible configurations. The CFTP is currently manifested as a space test program (STP) experimental payload on the Naval Postgraduate School's NPSAT1 and the United States Naval Academy's MidSTAR-1 satellites, which was launched into low earth orbit in March 2003- .",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1368020,no,undetermined,0
SeSFJava harness: service and assertion checking for protocol implementations,"Many formal specification languages and associated tools have been developed for network protocols. Ultimately, formal language specifications have to be compiled into a conventional programming language and this involves manual intervention (even with automated tools). This manual work is often error prone because the programmer is not familiar with the formal language. So our goal is to verify and test the ultimate implementation of a network protocol, rather than an abstract representation of it. We present a framework, called services and systems framework (SeSF), in which implementations and services are defined by programs in conventional languages, and mechanically tested against each other. SeSF is a markup language that can be integrated with any conventional language. We integrate SeSF into Java, resulting in what we call SeSFJava. We present a service-and-assertion checking harness for SeSFJava, called SeSFJava harness, in which distributed SeSFJava programs can be executed, and the execution checked against services and any other correctness assertions. The harness can test the final implementation of a concurrent system. We present an application to a data transfer service and sliding window protocol implementation. SeSFJava and the harness has been used in networking courses to specify and test transmission control protocol-like transport protocols and service.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1362714,no,undetermined,0
A practical consistent-quality two-pass VBR video coding algorithm for digital storage application,"This paper presents a practical two-pass VBR coding algorithm for digital storage applications, which provides consistent visual quality perceptually and satisfies a fixed total bit-budget constraint. In the first-pass coding, we detect scene changes precisely and obtain scene complexity as well as other statistical parameters of the video sequence. The total target bits are allocated to frames optimally according to scene spatial and temporal complexity with decoder buffer overflow and underflow consideration. In the second-pass coding, we employ an improved iterative Qp selection algorithm to search for the optimal picture-level reference Qp that results in minimum difference between the number of coded bits and that of target bits. Adaptive quantization and the non-integer picture-level reference Qp selection guarantee uniform quantization artifacts perceptually and the precision of iterative search. Experimental results show that the proposed algorithm can offer consistent visual quality with smaller PSNR variation and higher average PSNR improvement as compared to TM5 algorithm and a typical two-pass coding algorithm. The application of the proposed algorithm includes DVD, digital library, VOD, and other digital media applications.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1362511,no,undetermined,0
On-chip testing of embedded silicon transducers,"System-on-chip (SoC) technologies are evolving towards the integration of highly heterogeneous devices, including hardware of a different nature, such as digital, analog and mixed-signal, together with software components. Embedding transducers, as predicted by technology roadmaps, is yet another step in this continuous search for higher levels of integration and miniaturisation. Embedded transducers fabricated with silicon/CMOS compatible technologies may have more limitations than transducers fabricated with fully dedicated technologies. However, they offer industry the possibility of providing low cost applications for very large market niches, while still keeping acceptable transducer sensitivity. This is the case, for example, for accelerometers, micromirrors display devices or CMOS imagers. Embedded transducers are analog components. But given the fact that they work with signals other than electrical, the test of these embedded parts poses new challenges. Test technology for SoC devices is rapidly maturing but many difficulties still remain, in particular for addressing the test of analog and mixed-signal parts. In this paper, we present our work in the field of MEMS (microelectromechanical systems) on-chip testing with a brief overview of the state-of-the-art.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1362334,no,undetermined,0
Adaptive selection combining for soft handover in OVSF W-CDMA systems,"In W-CDMA, soft handover is supported at cell boundaries to maintain communication quality. The maximal ratio combining (MRC) and generalized selection combining (GSC) , are two possible approaches. However, soft handover is resource-intensive. In this letter, we propose an adaptive selection combining (ASC) scheme that can switch flexibly between MRC and GSC so as to take care of both channel loading and communication quality. The signal-to-interference-and-noise ratio (SINR) is kept as high as that of MRC while the blocking probability can remain at about the same level as that of GSC.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359883,no,undetermined,0
Closing gaps by clustering unseen directions,"Although in recent years the 3D-scanning field has reached a good level of maturity, it is still far from being perceived by common users as a 3D-photography approach, as simple as standard photography is. The main reason for that is that obtaining good 3D models without human intervention is still very hard. In particular, two problems remain open: automatic registration of single shots and planning of the acquisition session. In this paper we address the second issue and propose a solution to improve the coverage of automatically acquired objects. Rather than searching for the next-best-view in order to minimise the number of acquisitions, we propose a simple and easy-to-implement algorithm limiting our scope to closing gaps (i.e. filling unsampled regions) in roughly acquired models. The idea is very simple: detect holes in the current model and cluster their estimated normals in order to determine new views. Some results are shown to support our approach.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1314518,no,undetermined,0
Blocking probability analysis in future wireless networks,"This paper proposes to model each cell of future wireless networks as a G/G/c/c queueing system. As such a model has not been explicitly addressed in the literature, we apply maximum entropy principles to evaluate both traffic distribution and blocking probability within each cell. Analysis of numerical results enables to specify the conditions under which the system offers good quality of service in terms of blocking probability. More specifically, such an analysis reveals that coefficient of variation of call arrivals has more impact over the blocking probability than coefficient of variation of channel holding time.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1312933,no,undetermined,0
A New Mitigation Approach for Soft Errors in Embedded Processors,"Embedded processors, like for example processor macros inside modern FPGAs, are becoming widely used in many applications. As soon as these devices are deployed in radioactive environments, designers need hardening solutions to mitigate radiation-induced errors. When low-cost applications have to be developed, the traditional hardware redundancy-based approaches exploiting m-way replication and voting are no longer viable as too expensive, and new mitigation techniques have to be developed. In this paper we present a new approach, based on processor duplication, checkpoint and rollback, to detect and correct soft errors affecting the memory elements of embedded processors. Preliminary fault injection results performed on a PowerPC-based system confirmed the efficiency of the approach.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4636948,no,undetermined,0
Towards statistical inferences of successful prostate surgery,"Prostate cancer continues to be the leading cancer in the United States male population. The options for local therapy have proliferated and include various forms of radiation delivery, cryo-destruction, and novel forms of energy delivery as in high-intensity focused ultrasound. Surgical removal, however, remains the standard procedure for cure. Currently there are little objective parameters that are used to compare the efficiency of each form of surgical removal. As surgeons apply these different surgical approaches, a quality assessment would be most useful, not only with regard to overall comparison of one approach vs. another but also surgeon evaluation of personal surgical performance as they relate to a standard. To this end, we discuss the development of a process employing image reconstruction and analysis techniques to assess the volume and extent of extracapsular soft tissue removed with the prostate. Parameters such as the percent of capsule covered by soft tissue and where present the average depth of soft tissue coverage are assessed. A final goal is to develop software for the purpose of a quality assurance assessment for pathologists and surgeons to evaluate the adequacy/appropriateness of each surgical procedure; laparoscopic versus open perineal or retropubic prostatectomy.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1279809,no,undetermined,0
An Ultrasonic System for Detecting Channel Defects in Flexible Packages,"Ultrasonic system was developed for detecting channel defects embedded in bonded 2-sheet flexible packages film. This hardware system consisted of spherically focused 22.66-MHz ultrasonic transducer, four-axis precision positioning system, NI PXI-bus embedded controller and ultrasonic pulser-receiver. The software system was designed based on the modularization, realized the echo signal on-line processing using ultrasonic backscattered echo envelope integral (BEEI) imaging method. Some experimental results were presented, and the BEEI-mode imaging of channel defect was shown. The system can be easily used to detect the channel defects in flexible packages.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659633,no,undetermined,0
Electronic test solutions for FlowFET fluidic arrays,"The testable design and test of a software-controllable lab-on-a-chip, including a fluidic array of FlowFETs, control and interface electronics is presented. Test hardware is included for detecting faults in the DMOS electro-fluidic interface and the digital parts. Multi-domain fault modelling and simulation shows the effects of faults in the (combined) fluidic and electrical parts. Fault simulations also reveal important parameters of multi-domain test-stimuli for detecting both electrical and fluidic defects.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1287003,no,undetermined,0
A QoS-based routing algorithm in multimedia satellite networks,"Real-time multimedia applications impose strict delay bounds and are sensitive to delay variations. Satellite link handover increases delay jitter and signaling overhead as well as the termination probability of ongoing connections. To satisfy the QoS requirements of multimedia applications, satellite routing protocols should consider link handovers and minimize their effect on the active connections. A new routing algorithm is proposed to reduce both the inter-satellite handover and ISL handover. Once a connection request arrives, the remaining coverage time of satellites is used in the deterministic UDL routing. In the probabilistic ISL routing, the propagation delay and existence probability of ISL links are considered to reduce the delay and ISL handover probability. The rerouting algorithm is called when link handover occurs. Experiments show that this routing algorithm results in small delay jitter, low rerouting frequency, and low rerouting processing overhead.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1286057,no,undetermined,0
Performance of common and dedicated traffic channels for point-to-multipoint transmissions in W-CDMA,"We present a performance evaluation of two strategies for transmitting packet data to a group of multicast users over the W-CDMA air interface. In the first scheme, a single common or broadcast channel is used to transmit multicast data over the entire cell, whereas in the second scheme multiple dedicated channels are employed for transmitting to the group. We evaluate the performance of both schemes in terms of the number of users that can be supported by either scheme at a given quality of service defined in terms of a target outage probability.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285951,no,undetermined,0
The WindSat calibration/validation plan and early results,"Summary form only given. The WindSat radiometer was launched as part of the Coriolis mission in January 2003. WindSat was designed to provide fully polarimetric passive microwave measurements globally, and in particular over the oceans for ocean surface wind vector retrieval. Due to prohibitive risk and cost associated with an end-to-end pre-launch absolute radiometer calibration (i.e. from the energy incident on the main reflector through the receiver digitized output) it was important to develop an on-orbit calibration plan that verifies instrument conformance to specification and, if necessary, derive suitable calibration coefficients or sensor algorithms that bring the instrument into specification. This is especially true for the WindSat Cal/Val, in view of the fact that it is the first fully polarimetric spaceborne radiometer. This paper will provide an overview of the WindSat Cal/Val Program. The Cal/Val plan, patterned after the very successful SSM/I Cal/Val, is designed to ensure the highest quality data products and maximize the return on the available Cal/Val resources. The Cal/Val is progressive and will take place in well-defined stages: Early Orbit Evaluation, Initial Assessment, Detailed Calibration, and EDR Validation. The approach allows us to focus efforts more quickly on issues as they are identified and ensures a logical progression of activities. At each level of the Cal/Val the examination of the instrument and algorithm errors becomes more detailed. Along with the WindSat Cal/Val structure overview, we will present the independent data sources to be used and the analysis techniques to be employed. During the Early Orbit phase, special instrument operating modes have been developed to monitor the sensor health from the time of instrument turn-on to a short time after it reaches stable operating conditions. This mode is uniquely important for the WindSat since it affords the only opportunity to examine all of the data in the full 360 degree scan- > - > and to directly assess potential field-of-view intrusion effects from the spacecraft or other sensors on-board the satellite and evaluate the spin control system by observing the statistical distribution of data as the horns scan through the calibration loads. The next phase of the WindSat Cal/Val consists of an initial assessment of all sensor and environmental data products generated by the Ground Processing Software. The primary focus of the assessment is to conduct an end-to-end review of the output files (TDR, SDR and EDR) to verify the following: proper functioning of the on-line GPS modules, instrument calibration (including Antenna Pattern Correction and Stokes Coupling) does not contain large errors, EDR algorithms provide reasonable products, and there are no major geo-iocation errors. This paper will provide a summary of the results of the Early Orbit and Initial Assessment phases of the WindSat Cal/Val Program.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1282429,no,undetermined,0
Solenoidal and planar microcoils for NMR spectroscopy,"The extraction of nuclear magnetic resonance (NMR) spectra of samples having smaller and smaller volumes is a real challenge. Reductions of volume are dictated by the difficulties of production of sufficiently large samples or by necessities of miniaturization of the analyzing system. In both cases a careful design of the radiofrequency (RF) coil, ensuring an optimum reception of the NMR signal, is mandatory. We evaluated the usefulness of an electromagnetic simulation software for the design and optimization of these radio-frequency coils, which are more and more used in biology and health research projects. The contribution of different effects (dc, skin and proximity) at the total resistance of the coils were assessed as well as the expected SNR per sample volume unit and the quality factor. Designed for a biological application, these coils have to be as less invasive as possible and they must allow small quantities analysis of metabolites inside capillary tubs or surrounding the microcoil. In order to evaluate detection efficiency and spectral resolution, preliminary experiments have been performed at 85.13 MHz under a static magnetic field of 2 T.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1280783,no,undetermined,0
Multisensor based power diagnosis system for an intelligent robot,"This paper presents a power supply diagnosis system using redundant managed sensors method detection for intelligent security robot. The power system of the security robot we have developed in our lab. Consists of three parts, namely, computer power, drive motor power and circuit system power. We intend to detect the current value and diagnose the fault sensors of the power system. In this paper, we focus on the PC power supply, and we use eight current sensors to detect the current variety of the PC power and diagnose which sensor to be fault. First, we use computer simulation and implement it in the industry standard PC using A/D converter card. Next, we use the diagnosis algorithm in order to design the agent-based power supply system using microprocessor. Finally, we implement the proposed method in the PC power system of the intelligent security robot.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1280638,no,undetermined,0
Early estimation of the size of VHDL projects,"The analysis of the amount of human resources required to complete a project is felt as a critical issue in any company of the electronics industry. In particular, early estimation of the effort involved in a development process is a key requirement for any cost-driven system-level design decision. In this paper, we present a methodology to predict the final size of a VHDL project on the basis of a high-level description, obtaining a significant indication about the development effort. The methodology is the composition of a number of specialized models, tailored to estimate the size of specific component types. Models were trained and tested on two disjoint and large sets of real VHDL projects. Quality-of-result indicators show that the methodology is both accurate and robust.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275285,no,undetermined,0
On benchmarking the dependability of automotive engine control applications,"The pervasive use of ECUs (electronic control units) in automotive systems motivates the interest of the community in methodologies for quantifying their dependability in a reproducible and cost-effective way. Although the core of modern vehicle engines is managed by the control software embedded in engine ECUs, no practical approach has been proposed so far to characterise the impact of faults on the behaviour of this software. This paper proposes a dependability benchmark for engine control applications. The essential features of such type of applications are first captured in a general model, which is then exploited in order to specify a standard procedure to assess dependability measures. These measures are defined taking into account the expectations of industrials purchasing engine ECUs with integration purposes. The benchmark also considers the current set of technological limitations that the manufacturing of modern engine ECUs imposes to the experimental process. The approach is exemplified on two engine control applications.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311956,no,undetermined,0
Synthesizing operating system based device drivers in embedded systems,"This paper presents a correct-by-construction synthesis method for generating operating system based device drivers from a formally specified device behavior model. Existing driver development is largely manual using an ad-hoc design methodology. Consequently, this task is error prone and becomes a bottleneck in embedded system design methodology. Our solution to this problem starts by accurately specifying device access behavior with a formal model, viz. extended event driven finite state machines. We state easy check soundness conditions on the model that subsequently guarantee properties such as bounded execution time and deadlock-free behavior. We design a deadlock-free resource accessing scheme for our device access model. Finally, we synthesize an operating system (OS) based event processing mechanism, which is the core of the device driver, using a disciplined methodology that assures the correctness of the resulting driver. We validate our synthesis method using two case studies: an infrared port and the USB device controller for an SA1100 based handheld. Besides assuring a correct-by-construction driver, the size of the specification is 70% smaller than a manually written driver, which is a strong indicator of improved design productivity.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275253,no,undetermined,0
RTOS scheduling in transaction level models,"Raising the level of abstraction in system design promises to enable faster exploration of the design space at early stages. While scheduling decision for embedded software has great impact on system performance, it's much desired that the designer can select the right scheduling algorithm at high abstraction levels so as to save him from the error-prone and time consuming task of tuning code delays or task priority assignments at the final stage of system design. In this paper we tackle this problem by introducing a RTOS model and an approach to refine any unscheduled transaction level model (TLM) to a TLM with RTOS scheduling support. The refinement process provides a useful tool to the system designer to quickly evaluate different dynamic scheduling algorithms and make the optimal choice at an early stage of system design.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275252,no,undetermined,0
Power Quality Auto-monitoring for Distributed Generation Based on Virtual Instrument,"Distributed generation (DG) brings new green energy to the power system, meanwhile it also brings more power quality disturbances. According to the power quality disturbances of grid-connected distributed generation system, a new idea of the power quality auto-monitoring for distributed generation based on virtual instrument is proposed in this paper. Its realization of hardware system and software system are analyzed, its main functions and characteristics are expatiated on. The theory algorithm of auto-monitoring is researched and simulation analysis is carried out in detail. The results of simulation and experiment show that the auto-monitoring system can detect the power quality disturbances of distributed generation roundly, real-timely and accurately.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659663,no,undetermined,0
Research and Design of Multifunctional Intelligent Melted Iron Analyzer,"A multifunctional intelligent melted iron analyzer is researched and designed. This device combined the function of thermal analysis for quality analysis of melted iron and ultrasonic measuring for nodularity. By thermal analysis, equation of linear regression wasused and the percentage composition of carbon, silicon and phosphorus etc. were obtained. In addition, thermal analysis was used to predict gray iron inoculation result, structure and performance. Therefore, ultrasonic measuring was adopted to be applied in this study to survey the nodularity of spheroidal graphite iron. In order to protect the analyzer far away from boiling melted iron, wireless temperature collection was adopted. The system is composed of PC104, single chip SPCE061 and other peripheral circuits. Designed temperature collection and ultrasonic measuring module, the software of data analysis and management is programmed by VB6.0 based on PC104 and windows operation system. After running for more than one year, the analyzer goes very well in the foundry.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659881,no,undetermined,0
Beat: Boolean expression fault-based test case generator,"We present a system which generates test cases from Boolean expressions. The system is based on the integration of several fault-based test case selection strategies developed by us. Our system generates test cases that are guaranteed to detect all single operator fault and all single operand faults when the Boolean expression is in irredundant disjunctive normal form. Apart from being an automated test case generation tool developed for software testing practitioners, this system can also be used as a training or self-learning tool for students as well as software testing practitioners.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1270695,no,undetermined,0
Flour quality control using image processing,This document presents the description of an application for the automatic visual inspection of flour quality. The flour quality depends on the number of impurities detected in the flour after a predefined settling time. The software was developed with IMAQ Vision for LabVTEW software-developing tool and it uses a commercial camera as image acquisition device. The paper along its sections describes the main system blocks. An illustrative example is used for better description of the several steps during the digital processing of the acquired images.,2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1267318,no,undetermined,0
Low-cost power quality monitor based on a PC,"This paper presents the development of a low-cost digital system useful for power quality monitoring and power management. Voltage and current measurements are made through Hall-effect sensors connected to a standard data acquisition board, and the applications were programmed in LabVIEWTM , running on Windows in a regular PC. The system acquires data continuously, and stores in files the events that result from anomalies detected in the monitored power system. Several parameters related to power quality and power management can be analyzed through 6 different applications, named: """"scope and THD"""", """"strip chart"""", """"wave shape"""", """"sags and swells"""", """"classical values"""" and """"p-q theory"""". The acquired information can be visualized in tables and/or in charts. It is also possible to generate reports in HTML format. These reports can be sent directly to a printer, embedded in other software applications, or accessed through the Internet, using a Web browser. The potential of the developed system is shown, namely the advantages of virtual instrumentation, regarding to flexibility, cost and performance, in the scope of power quality monitoring and power management.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1267267,no,undetermined,0
Analysis of techniques for building intrusion tolerant server systems,"The theme of intrusion detection systems (IDS) is detection because prevention mechanisms alone are not guaranteed to keep intruders out. The research focus of IDS is therefore on how to detect as many attacks as possible, as soon as we can, and at the same time to reduce the false alarm rate. However, a growing recognition is that a variety of mission critical applications need to continue to operate or provide a minimal level of services even when they are under attack or have been partially compromised; hence the need for intrusion tolerance. The goal of this paper is to identify common techniques for building highly available and intrusion tolerant server systems and characterize with examples how various techniques are applied in different application domains. Further, we want to point out the potential pitfalls as well as challenging open research issues which need to be addressed before intrusion tolerant systems (ITS) become prevalent and truly useful beyond a specific range of applications.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1290202,no,undetermined,0
Particle swarm optimization for worst case tolerance design,"Worst case tolerance analysis is a major subtask in modern industrial electronics. Recently, the demands on industrial products like production costs or probability of failure have become more and more important in order to be competitive in business. The main key to improve the quality of electronic products is the challenge to reduce the effects of parameter variations, which can be done by robust parameter design. This paper addresses the applicability of particle swarm optimization combined with pattern search for worst case circuit design. The main advantages of this approach are the efficiency and robustness of the particle swarm optimization strategy. The method is also well suited for higher order problems, i.e. for problems with a high number of design parameters, because of the linear complexity of the pattern search algorithm.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1290245,no,undetermined,0
Goal trees and fault trees for root cause analysis,"Typical enterprise applications are built upon different platforms, operate in a heterogeneous, distributed environment, and utilize different technologies, such as middleware, databases and Web services. Diagnosing the root causes of problems in such systems is difficult in part due to the number of possible configuration and tuning parameters. Today a variety of tools are used to aid operators of enterprise applications identify root causes. For example, a user input validation tool detects and prevents Website intrusions or a log analysis tool identifies malfunctioning components. Searching for the root causes of such failures in a myriad of functional and non-functional requirements poses significant challenges-not only for users, but also for experienced operators when monitoring, auditing, and diagnosing systems. We propose the notion of a guide map-a set of goal trees and fault trees-to aid users in the process of choosing (supported by high level goal trees) and applying (supported by low level fault trees) suitable diagnostic tools. In this paper we discuss two case studies to illustrate how the guide map aids users to apply two home grown diagnostic tools.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658098,no,undetermined,0
Supporting software evolution analysis with historical dependencies and defect information,"More than 90% of the cost of software is due to maintenance and evolution. Understanding the evolution of large software systems is a complex problem, which requires the use of various techniques and the support of tools. Several software evolution approaches put the emphasis on structural entities such as packages, classes and structural relationships. However, software evolution is not only about the history of software artifacts, but it also includes other types of data such as problem reports, mailing list archives etc. We propose an approach which focuses on historical dependencies and defects. We claim that they play an important role in software evolution and they are complementary to techniques based on structural information. We use historical dependencies and defect information to learn about a software system and detect potential problems in the source code. Moreover, based on design flaws detected in the source code, we predict the location of future bugs to focus maintenance activities on the buggy parts of the system. We validated our defect prediction by comparing it with the actual defects reported in the bug tracking system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658092,no,undetermined,0
Analysis of channel allocation schemes for cellular mobile communication networks,"The coverage area and the number of users of mobile communication networks are in a continuous and fast growing, while the allocated frequency spectrum remains unchanged. New efficient management schemes have to be deployed in order to maintain a good grade of service. This paper deals with a software package meant to evaluate by simulation the blocking probability in a cellular system induced by a channel allocation scheme and, thus, to select its parameters in accordance with the network architecture and the traffic distribution on its coverage area.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5731340,no,undetermined,0
Identifying rate mismatch through architecture transformation,"Rate mismatches are often missed when simulation is used as a validation tool for embedded systems because it is hard to determine if the mismatch was introduced by the coupling of the software models, the communication protocol, a flawed system design or a combination of the three factors. By eliminating the ambiguity introduced by the software model couplings and the communication protocol, it is possible to trace rate mismatches to flawed system design. The High Level Architecture details the couplings between the various components present in the system, while the Time Triggered Protocol provides deterministic, reliable and fault-tolerant communication between the components. This paper presents an mapping of the High Level Architecture specification of the system to the Time Triggered Protocol. Any rate mismatches detected, can then be attributed to flawed system design, making simulation a powerful validation tool for the design and implementation of hard real-time embedded systems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5731121,no,undetermined,0
On the design of Modular Software Defined Radio Systems,"Software Radio and its enabling technologies have been discussed extensively in the past. The focus of most work is either on a specific hardware part of the transceiver's signal processing chain, or on algorithm design for the digital baseband. However, a methodology for managing open, Software Defined Radio systems is still in demand. In this paper, the implementation of a software defined physical layer is perceived as a real-time embedded system design problem on multiprocessor hardware, using pieces of software which are unknown a priori. We propose a new way of modeling this design situation. Granularity G is introduced to describe the degree of modularity in software defined communication functions. The speedup s is used to assess the quality of modular implementations. Computer simulations lead us to first observations under the premises of the new model. A mathematical analysis complements these experiments and reveals how to adjust the simulation parameters for interpretable results. The speedup behavior of a simple graph structure is predicted from this analysis. A more complicated structure is then reassessed by means of simulation, finally leading to enhanced guidelines for the design of Modular Software Defined Radio systems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5699840,no,undetermined,0
Component based development for transient stability power system simulation software,"A component-based development (CBD) approach has become increasingly important in the software industry. Any software that apply the CBD will not only save time and cost through reusability of component, but also have the capability to handle the complex problems. Since CBD design is based on object-oriented programming (OOP), the components with a good quality and reusability can be created, classified and managed for future reuse. The methodology of OOP is based on the real object. The mechanism of OOP such as encapsulation, inheritance, and polymorphism are the advantages that could be used to define real objects associated with the program. This paper focused on the implementation of the CBD to power system transient stability simulation (TSS). There are many methods to solve transient stability problem, but in this paper two methods are applied to solve TSS problems, namely trapezoidal method and modified Euler method. The performance of two approaches, CBD and non CBD applications of power system transient stability simulation is assessed through tests carried out using IEEE data test systems.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1437409,no,undetermined,0
3D reconstruction and model acquisition of objects in real world scenes using stereo imagery,"Recently, the use of three dimensional computer models has greatly increased, in part due to the availability of fast, inexpensive hardware and technologies like VRML-ready Internet browsers. These models often represent objects in real world scenes and are typically built by hand using CAD software, an error-prone and time consuming process. The paper outlines a simple and efficient method based on passive stereo imaging techniques by which these models may be acquired, processed and utilized with little effort. The described method extracts the 3D geometrical data from the stereo images, for the purpose of creating realistic 3D models. The approach uses calibrated cameras",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416611,no,undetermined,0
Application of system models in regression test suite prioritization,"During regression testing, a modified system needs to be retested using the existing test suite. Since test suites may be very large, developers are interested in detecting faults in the system as early as possible. Test prioritization orders test cases for execution to increase potentially the chances of early fault detection during retesting. Most of the existing test prioritization methods are based on the code of the system, but model-based test prioritization has been recently proposed. System modeling is a widely used technique to model state-based systems. The existing model based test prioritization methods can only be used when models are modified during system maintenance. In this paper, we present model-based prioritization for a class of modifications for which models are not modified (only the source code is modified). After identification of elements of the model related to source-code modifications, information collected during execution of a model is used to prioritize tests for execution. In this paper, we discuss several model-based test prioritization heuristics. The major motivation to develop these heuristics was simplicity and effectiveness in early fault detection. We have conducted an experimental study in which we compared model-based test prioritization heuristics. The results of the study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658073,no,undetermined,0
Automated Control Systems for the Safety Integrity Levels 3 and 4,"Programs employed for purposes of safety related control must be formally safety licensed, which constitutes a very difficult and hitherto not satisfactorily solved problem. Striving for utmost simplicity and easy comprehensibility of verification methods, the programming methods cause/effect tables and function block diagrams based on verified libraries are assigned to the upper two Safety Integrity Levels SIL 4 and SIL 3, resp., as they are the only ones so far allowing to verify highly safety critical automation software in trustworthy, easy and economic ways. For each of the two SILs a dedicated, a low complexity execution platform is presented supporting the corresponding programming method architecturally. Their hardware is fault detecting or supervised by a fail safe logic, resp., to initiate emergency shut-downs in case of malfunctions. By design, there is no semantic gap between the programming and machine execution levels, enabling the safety licensing of application software by extremely simple, but rigorous methods, viz., diverse back translation and inspection. Operating in strictly periodic fashion, the controllers exhibit fully predictable real time behaviour.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410943,no,undetermined,0
Using random test selection to gain confidence in modified software,"This paper presents a method that addresses two practical issues concerning the use of random test selection for regression testing: the number of random samples needed from the test suite to provide reliable results, and the confidence levels of the predictions made by the random samples. The method applies the Chernoff bound, which has been applied in various randomized algorithms, to compute the error bound for random test selection. The paper presents three example applications, based on the method, for regression testing. The main benefits of the method are that it requires no distribution information about the test suite from which the samples are taken, and the computation of the confidence level is independent of the size of the test suite. The paper also presents the results of an empirical evaluation of the technique on a set of C programs, which have been used in many testing experiments, along with three of the GCC compilers. The results demonstrate the effectiveness of the method and show its potential for regression testing on real-world, large-scale applications.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658075,no,undetermined,0
Experiences in the inspection process characterization techniques,"Implementation of a disciplined engineering approach to software development requires the existence of an adequate supporting measurement & analysis system. Due to demands for increased efficiency and effectiveness of software processes, measurement models need to be created to characterize and describe the various processes usefully. The data derived from these models should then be analyzed quantitatively to assess the effects of new techniques and methodologies. In recent times, statistical and process thinking principles have led software organizations to appreciate the value of applying statistical process control techniques. As part of the journey towards SW-CMM&reg; Level 5 at the Motorola Malaysia Software Center, which the center achieved in October 2001, considerable effort was spent on exploring SPC techniques to establish process control while focusing on the quantitative process management KPA of the SW-CMM. This paper discusses the evolutionary learning experiences, results and lessons learnt by the center in establishing appropriate analysis techniques using statistical and other derivative techniques. The paper discusses the history of analysis techniques that were explored with specific focus on characterizing the inspection process. Future plans to enhance existing techniques and to broaden the scope to cover analysis of other software processes are also discussed.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319126,no,undetermined,0
Deriving software statistical testing model from UML model,"Software statistical testing is concerned with testing the entire software systems based on their usage models. In the context of UML-based development, it is desired that usage models can be derived from UML analysis artifacts. This paper presents a method that derives the software usage models from reasonably constrained UML artifacts. The method utilizes use case diagrams, sequence diagrams and the execution probability of each sequence diagram in its associated use case. By projecting the messages in sequence diagrams onto the objects under test, the method elicits messages and their occurrence probabilities for generating the usage model of each use case for the objects under test. Then the usage models of use cases are integrated into the system usage model. The integration procedure utilizes the execution sequential relations between use cases.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319120,no,undetermined,0
Combining behavior and data modeling in automated test case generation,"Software testing plays a critical role in the process of creating and delivering high-quality software products. Manual software testing can be an expensive, tedious and error-prone process, therefore testing is often automated in an attempt to reduce its cost and improve its defect detection capability. Model-based testing, a technique used in automated test case generation, is an important topic because it addresses the need for test suites that are of high-quality and yet, maintainable. Current model-based techniques often use a single model to represent system behavior. Using a single model may restrict the number and type of test cases that may be generated. In this paper, system-level test case generation is accomplished using two models to represent system behavior. The results of case studies used to evaluate this technique indicate that for the systems studied a larger percentage of the required test cases can be generated using the combined modeling approach.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319108,no,undetermined,0
Software cost estimation through conceptual requirement,"Software cost estimation is vital for the effective control and management of the whole software development process. Currently, the constructive cost model (COCOMO II) is the most popular tool for estimating software cost. It uses lines of code and function points to assess software size. However, these are actually implementation details and difficult to estimate during the early stage of software development. The entity relationship (ER) model is well used in conceptual modeling (requirements analysis) for data-intensive systems. In this article, we explore the use of ER model for the estimation of software cost. A new term, path complexity, is proposed. Based on path complexity and other factors, we built a multiple regression model for software cost estimation. The approach has been validated statistically through system data from the real industry projects.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319096,no,undetermined,0
Assessing the value of coding standards: An empirical study,"In spite of the widespread use of coding standards and tools enforcing their rules, there is little empirical evidence supporting the intuition that they prevent the introduction of faults in software. Not only can compliance with a set of rules having little impact on the number of faults be considered wasted effort, but it can actually result in an increase in faults, as any modification has a non-zero probability of introducing a fault or triggering a previously concealed one. Therefore, it is important to build a body of empirical knowledge, helping us understand which rules are worthwhile enforcing, and which ones should be ignored in the context of fault reduction. In this paper, we describe two approaches to quantify the relation between rule violations and actual faults, and present empirical data on this relation for the MISRA C 2004 standard on an industrial case study.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658076,no,undetermined,0
Constructive architecture compliance checking  an experiment on support by live feedback,"This paper describes our lessons learned and experiences gained from turning an analytical reverse engineering technology - architecture compliance checking - into a constructive quality engineering technique. Constructive compliance checking constantly monitors the modifications made by developers. When a structural violation is detected, the particular developer receives live feedback allowing prompt removal of the violations and hence, training the developers on the architecture. An experiment with six component development teams gives evidence that this training pro-actively prevents architecture decay. The three teams supported by the live compliance checking inserted about 60% less structural violations into the architecture than did the three other development teams. Based on the results, we claim that constructive compliance checking is a promising application of reverse engineering technology to the software implementation phase.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658077,no,undetermined,0
A convergence model for asynchronous parallel genetic algorithms,"We describe and verify a convergence model that allows the islands in a parallel genetic algorithm to run at different speeds, and to simulate the effects of communication or machine failure. The model extends on present theory of parallel genetic algorithms and furthermore it provides insight into the design of asynchronous parallel genetic algorithms that work efficiently on volatile and heterogeneous networks, such as cycle-stealing applications working over the Internet. The model is adequate for comparing migration parameter settings in terms of convergence and fault tolerance, and a series of experiments show how the convergence is affected by varying the failure rate and the migration topology, migration rate, and migration interval. Experiments conducted show that while very sparse topologies are inefficient and failure-prone, even small increases in topology order result in more robust models with convergence rates that approach the ones found in fully-connected topologies.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1299419,no,undetermined,0
Risk responsibility for supply in Latin America - the Argentinean case,"In deregulation of electricity sectors in Latin America two approaches have been used to allocate the responsibility on the electricity supply: (1) The government keeps the final responsibility on the supply. Suppliers (distribution companies or traders) do not have control on the rationing when it becomes necessary to curtail load. In such case they cannot manage the risks associated to the supply. This is the case in the markets of Brazil and Colombia. (2) The responsibility is fully transferred to suppliers. The regulatory entity supervises the quality of the supply and different types of penalties are applied when load is not supplied. This approach is currently used in Argentina, Chile and Peru. In Argentina the bilateral contracts, that are normally financial, become physical when a rationing event happens. This approach permits suppliers to have a great control on risks. Both approaches have defenders and detractors. In some cases, the conclusions on a same event have completely opposite interpretations and diagnoses. For instance, the crisis of supply in Brazil during 2002 was interpreted as a fault of the market by the defenders of the final responsibility of the state, or attributed to excess of regulation and of interference of the government by the advocates of decentralized schemes. This presentation will analyze the performance of both approaches in Latin America, assessing the diverse types of arguments used to criticize or to defend to each one of these approaches, and finally to present some conclusions on the current situation and future of the responsibility on supply and risks associated.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1267237,no,undetermined,0
An improvement project for distribution transformer load management in Taiwan,"Summary form only given. This paper introduces an application program that is based on an automated mapping/facilities management/geographic information system (AM/FM/GIS) to provide information expectation, load forecasting and power flow calculation capability in distribution systems. First, the database and related data structure used in the Taipower distribution automation pilot system is studied and thoroughly analyzed. Then, our program, developed by the AM/FM FRAMME and Visual Basic software, is integrated into the above pilot system. Moreover, this paper overcomes the weak points of the pilot system, such as difficult use, incomplete function, nonuniform sampling for billing and dispatch of bills and inability to simultaneously transfer customer data. This program can enforce the system and can predict future load growth on distribution feeders, considering the effects of temperature variation, and power needed for air-conditioners. In addition, on the basis of load density and diversity factors of typical customers, the saturation load of a new housing zone can be estimated. As for the power flow analysis, it can provide three-phase quantities of voltage drop at each node, the branch current, and the system loss. The program developed in this study can effectively aid public electric utilities in distribution system planning and operation.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1267167,no,undetermined,0
Growing a software quality culture in an educational environment,"The technical skills students must acquire in a typical computer science program are often mandated through standards or curricular requirements. How are nontechnical skills assessed? computer science educators must teach and encourage the development of other critical skills needed in the workplace such as personal accountability, a strong work ethic and an ability to deliver on-time and correct work. This paper describes the results of a student survey designed to provoke some thoughts about the evolving work ethic and work culture of today's students. Along with the survey results, the importance in asking the questions and a brief analysis of how the behavior or activity fits into the quality cycle are presented. Finally, a section on continuous improvement strategies is proposed.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1266021,no,undetermined,0
Fault detection and visualization through micron-resolution X-ray imaging,"This paper describes a novel, non-intrusive method for the detection of faults within printed circuit boards (PCBs) and their components using digital imaging and image analysis techniques. High-resolution X-ray imaging systems provide a means to detect and analyze failures and degradations down to micron-levels both within the PCB itself and the components that populate the board. Further, software tools can aid in the analysis of circuit features to determine whether a failure has occurred, and to obtain positive visual confirmation that a failure has occurred. Many PCB and component failures previously undetectable through todaypsilas test methodologies are now detectable using this approach.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662635,no,undetermined,0
Creating value through test,"Test is often seen as a necessary evil; it is a fact of life that ICs have manufacturing defects and those need to be filtered out by testing before the ICs are shipped to the customer. In this paper, we show that techniques and tools used in the testing field can also be (re-)used to create value to (1) designers, (2) manufacturers, and (3) customers alike. First, we show how the test infrastructure can be used to detect, diagnose, and correct design errors in prototype silicon. Secondly, we discuss how test results are used to improve the manufacturing process and hence production yield. Finally, we present test technologies that enable systems of high reliability for safety-critical applications.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253643,no,undetermined,0
On the Infiniband subnet discovery process,"InfiniBand is becoming an industry standard both for communication between processing nodes and I/O devices, and for interprocessor communication. Instead of using a shared bus, InfiniBand employs an arbitrary (possibly irregular) switched point-to-point network. InfiniBand specification defines a basic management infrastructure that is responsible for subnet configuration, activation, and fault tolerance. After the detection of a topology change, management entities collect the current subnet topology. The topology discovery algorithm is one of the management issues that are outside the scope of the current specification. Preliminary implementations obtain the entire topological information each time a change is detected. In this work, we present and analyze an optimized implementation, based on exploring only the region that has been affected by the change.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253361,no,undetermined,0
A data clustering algorithm for mining patterns from event logs,"Today, event logs contain vast amounts of data that can easily overwhelm a human. Therefore, mining patterns from event logs is an important system management task. The paper presents a novel clustering algorithm for log file data sets which helps one to detect frequent patterns from log files, to build log file profiles, and to identify anomalous log file lines.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251233,no,undetermined,0
New quality estimations in random testing,"By reformulating the issue of random testing into an equivalent problem we are able to introduce a new kind of quality estimations based on Monte Carlo integration and the central limit theorem. This method also provides a limited but working """"success theory"""" in the case of no detected failures. In an empirical evaluation using hundreds of billions of simulated tests we furthermore find a very good match between the quality estimations presented in this article and the true failure frequencies. Both simple modulus defects as well as seeded defects in two extensively employed numerical routines were subject to investigation in the empirical work.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251067,no,undetermined,0
Augmenting simulated annealing to build interaction test suites,"Component based software development is prone to unexpected interaction faults. The goal is to test as many-potential interactions as is feasible within time and budget constraints. Two combinatorial objects, the orthogonal array and the covering array, can be used to generate test suites that provide a guarantee for coverage of all t-sets of component interactions in the case when the testing of all interactions is not possible. Methods for construction of these types of test suites have focused on two main areas. The first is finding new algebraic constructions that produce smaller test suites. The second is refining computational search algorithms to find smaller test suites more quickly. In this paper we explore one method for constructing covering arrays of strength three that combines algebraic constructions with computational search. This method leverages the computational efficiency and optimality of size obtained through algebraic constructions while benefiting from the generality of a heuristic search. We present a few examples of specific constructions and provide some new bounds for some strength three covering arrays.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251061,no,undetermined,0
Automating control and evaluation of FPGA testing using SJ BIST,"SJ BIST<sup>reg</sup> is a method to detect intermittent faults in ball grid array (BGA) packages of field programmable gate arrays (FPGAs). Failure of monitored I/O pins on operational, fully-programmed FPGAs is detected and reported by SJ BIST to provide positive indication of damage to one or more I/O solder-joint networks of an FPGA on an electronic digital board. The board can then be replaced before accumulated fatigue damage results in intermittent or long-lasting operational faults. This paper presents the test procedures to provide a lap-top-based test bed for controlling SJ BIST in the FPGAs on those evaluation boards. The procedures include using a Spartan 3trade development kit, a verilog-based test program, and a MATLAB<sup>reg</sup> program for collecting, saving and displaying test data, all of which reside on a lap-top PC with a serial data port. The FPGA on a HALT evaluation board is programmed with SJ BIST (patent pending).",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662648,no,undetermined,0
A Bayesian belief network for assessing the likelihood of fault content,"To predict software quality, we must consider various factors because software development consists of various activities, which the software reliability growth model (SRGM) does not consider. In this paper, we propose a model to predict the final quality of a software product by using the Bayesian belief network (BBN) model. By using the BBN, we can construct a prediction model that focuses on the structure of the software development process explicitly representing complex relationships between metrics, and handling uncertain metrics, such as residual faults in the software products. In order to evaluate the constructed model, we perform an empirical experiment based on the metrics data collected from development projects in a certain company. As a result of the empirical evaluation, we confirm that the proposed model can predict the amount of residual faults that the SRGM cannot handle.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251044,no,undetermined,0
Automating the analysis of voting systems,"Voting is a well-known technique to combine the decisions of peer experts. It is used in fault tolerant applications to mask errors from one or more experts using n-modular redundancy (NMR) and n-version programming. Voting strategies include: majority, weighted voting, plurality; instance runoff voting, threshold voting, and the more general weighted k-out-of-n systems. Before selecting a voting schema for a particular application, we have to understand the various tradeoffs and parameters and how they impact the correctness, reliability, and confidence in the final decision made by a voting system. In this paper, we propose an enumerated simulation approach to automate the behavior analysis of voting schemas with application to majority and plurality voting. We conduct synthetic studies using a simulator that we develop to analyze results from each expert, apply a voting mechanism, and analyze the voting results. The simulator builds a decision tree and uses a depth-first traversal algorithm to obtain the system reliability among other factors. We define and study the following behaviors: 1) the probability of reaching a consensus, """"Pc""""; 2) reliability of the voting system, """"R""""; 3) certainly index, """"T""""; and 4) the confidence index, """"C"""". The parameters controlling the analysis are the number of participating experts, the number of possible output symbols that can be produced by an expert, the probability distribution of each expert's output, and the voting schema. The paper presents an enumerated simulation approach for analyzing voting systems which can be used when the use of theoretical models are challenged by dependencies between experts or uncommon probability distributions of the expert's output.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251043,no,undetermined,0
Shared semantic domains for computational reliability engineering,"Modeling languages and the software tools which support them are essential to engineering. However, as these languages become more sophisticated, it becomes difficult to assure both the validity of their semantic specifications and the dependability of their program implementations. To ameliorate this problem we propose to develop shared semantic domains and corresponding implementations for families of related modeling languages. The idea is to amortize investments at the intermediate level across multiple language definitions and implementations. To assess the practicality of this approach for modeling languages, we applied it to two languages for reliability modeling and analysis. In earlier work, we developed the intermediate semantic domain of failure automata (FA), which we used to formalize the semantics of dynamic fault trees (DFTs). in this paper, we show that a variant of the original FA can serve as a common semantic domain for both DFTs and reliability block diagrams (RBDs). Our experiences suggest that the use of a common semantic domain and a shared analyzer for expressions at this level can ease the task of formalizing and implementing modeling languages, reducing development costs and improving their dependability.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251040,no,undetermined,0
Regression analysis of automated measurment systems,"Automated measurement systems are dependent upon successfully application of multiple integrated systems to perform measurement analysis on various units-under-tests (UUT)s. Proper testing, fault isolation and detection of a UUT are contingent upon accurate measurements of the automated measurement system. This paper extends previous presentation from 2007 AUTOTESTCON on the applicability of measurement system analysis for automated measurement systems. The motivation for this research was to reduce risk of transportability issues from legacy measurement systems to emerging systems. Improving regression testing utilizing parametric metadata for large scale automated measurement systems over existing regression testing techniques which provides engineers, developers and management increased confidence that mission performance is not compromised. The utilization of existing software statistical tools such as Minitab<sup>R</sup> provides the necessary statistical techniques to evaluate measurement capability of automated measurement systems. By applying measurement system analysis to assess the measurement variability between the US Navypsilas two prime automated test systems the Consolidated Automated Support System (CASS) and the Reconfigurable-Transportable Consolidated Automated Support System (RTCASS). Measurement system analysis shall include capability analysis between one selected CASS and RTCASS instrument to validate measurement process capability; general linear model to assess variability between stations, multivariate analysis to analyze measurement variability of UUTs between measurement systems, and gage repeatability and reproducibility analysis to isolate sources of variability at the UUT testing level.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662676,no,undetermined,0
A new software testing approach based on domain analysis of specifications and programs,"Partition testing is a well-known software testing technique. This paper shows that partition testing strategies are relatively ineffective in detecting faults related to small shifts in input domain boundary. We present an innovative software testing approach based on input domain analysis of specifications and programs, and propose the principle and procedure of boundary test case selection in functional domain and operational domain. The differences of the two domains are examined by analyzing the set of their boundary test cases. To automatically determine the operational domain of a program, the ADSOD system is prototyped. The system supports not only the determination of input domain of integer and real data types, but also non-numeric data types such as characters and enumerated types. It consists of several modules in finding illegal values of input variables with respect to specific expressions. We apply the new testing approach to some example studies. A preliminary evaluation on fault detection effectiveness and code coverage illustrates that the approach is highly effective in detecting faults due to small shifts in the input domain boundary, and is more economical in test case generation than the partition testing strategies.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251031,no,undetermined,0
Scheduling on the Grid via multi-state resource availability prediction,"To make the most effective application placement decisions on volatile large-scale heterogeneous Grids, schedulers must consider factors such as resource speed, load, and reliability. Including reliability requires availability predictors, which consider different periods of resource history, and use various strategies to make predictions about resource behavior. Prediction accuracy significantly affects the quality of the schedule, as does the method by which schedulers combine various factors, including the weight given to predicted availability, speed, load, and more. This paper explores the question of how to consider predicted availability to improve scheduling, concentrating on multi-state availability predictors. We propose and study several classes of schedulers, and a method for combining factors. We characterize the inherent tradeoff between application makespan and the number of evictions due to failure, and demonstrate how our schedulers can navigate this tradeoff under various scenarios. We vary application load and length, and the percentage of jobs that are checkpointable. Our results show that the only other multi-state prediction based scheduler causes up to 51% more evicted jobs while simultaneously increasing average job makespan by 18% when compared with our scheduler.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662791,no,undetermined,0
Optimal resource allocation for the quality control process,"Software development project employs some quality control (QC) process to detect and remove defects. The final quality of the delivered software depends on the effort spent on all the QC stages. Given a quality goal, different combinations of efforts for the different QC stages may lead to the same goal. In this paper, we address the problem of allocating resources to the different QC stages, such that the optimal quality is obtained. We propose a model for the cost of QC process and then view the resource allocation among different QC stages as an optimization problem. We solve this optimization problem using non-linear optimization technique of sequential quadratic programming. We also give examples to show how a sub-optimal resource allocation may either increase the resource requirement significantly or lower the quality of the final software.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251028,no,undetermined,0
Detection or isolation of defects? An experimental comparison of unit testing and code inspection,"Code inspections and white-box testing have both been used for unit testing. One is a static analysis technique, the other, a dynamic one, since it is based on executing test cases. Naturally, the question arises whether one is superior to the other, or, whether either technique is better suited to detect or isolate certain types of defects. We investigated this question with an experiment with a focus on detection of the defects (failures) and isolation of the underlying sources of the defects (faults). The results indicate that there exist significant differences for some of the effects of using code inspection versus testing. White-box testing is more effective, i.e. detects significantly more defects while inspection isolates the underlying source of a larger share of the defects detected. Testers spend significantly more time, hence the difference in efficiency is smaller, and is not statistically significant. The two techniques are also shown to detect and identify different defects, hence motivating the use of a combination of methods.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251026,no,undetermined,0
Study on the cost/benefit/optimization of software safety test,"Although the safety-critical system has high demand on safety, the cost of software test therefore must be taken account of. In the test of railway computer interlocking software carried out, the safety test for a station software last several months, therefore, in order to reduce the test time, it is practical to choose functions from the function set to test through optimization. Software safety test is realized by running testing cases at the cost of labor and time. It is expected to detect dangerous function defects and reduce system loss to gain benefit. Optimization strategy is a best choice to consider testing cases.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250881,no,undetermined,0
"Detecting soft errors by a purely software approach: method, tools and experimental results","In this paper is described a software technique allowing the detection of soft errors occurring in processor-based digital architectures. The detection mechanism is based on a set of rules allowing the transformation of the target application into a new one, having the same functionalities but being able to identify bit-flips arising in memory areas as well as those perturbing the processor's internal registers. Experimental results issued from fault injection sessions and preliminary radiation test campaigns, performed on a complex DSP processor, provide objective figures about the efficiency of the proposed error detection technique.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253806,no,undetermined,0
Testing criteria for data flow software,"We propose the use of accessibility measures in some testing strategies to specify testing objectives based on a functional model. The functional model, which is founded on the information transfer within software, was used with success to analyze testability for data-flow software. The testing strategies based on this model allow specification of testing objectives in relation to faults diagnostic, i.e. they allow not only faults to be detected but also to be located in the software. The approach is applied on a dataflow design provided by THALES Avionics to specify testing objectives.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254387,no,undetermined,0
Link quality assessment in mobile satellite communication systems,"This paper presents a simulation tool aimed to assess the relation between the link quality and the overall capacity on a MSS (mobile satellite system). This tool is particularly appropriated to compare different radio interfaces and to estimate the applicability of the UTRA radio interface in future satellite systems. Moreover, it permits the analysis of innovative radio resource management techniques. All the models included in the simulator, namely spotbeam projection on the Earth, multibeam antennas and radio propagation models are also presented.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1264303,no,undetermined,0
Assessment and optimization of TEA-PRESS sequences in 1H MRS and MRSI of the breast,"Magnetic Resonance Spectroscopy (MRS) and Magnetic Resonance Spectroscopic Imaging (MRSI) are useful tools when used in combination with standard imaging methods that may offer a significant advantage in certain clinical applications such as cancer localization and staging. Incorporation of these tools in clinical practice is, however, limited due to the considerable amount of user intervention that spectrum processing and quantification requires and due to the importance of acquisition parameter optimization in spectrum quality. In this work various acquisition parameters and their effects in spectrum quality are investigated. In order to assess the quality of various spectroscopic techniques, a series of experiments were conducted using a standard solution. The application of water and fat suppression techniques and their compatibility with other parameters were also investigated. A number of artifacts were provoked to study the effects in spectrum quality. The stability of the equipment, the appearance of errors and artifacts and the reproducibility of the results were also examined to obtain useful conclusions for the interaction of acquisition parameters. All the data were processed with specialized computer software (jMRUI 2.2, FUNCTOOL) to analyze various aspects of the measurements and quantify various parameters such as signal to noise ratio (SNR), full width at half maximum (FWHM), peak height and j-modulation. The experience acquired from the conducted experiments was successfully applied in acquisition parameter optimization and improvement of clinical applications for the biochemical analysis of breast lesions by significantly improving the spectrum quality, SNR and spatial resolution.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659988,no,undetermined,0
Advanced concepts in time-frequency signal processing made simple,"Time-frequency representations (TFRs) such as the spectrogram are important two-dimensional tools for processing time-varying signals. In this paper, we present the Java software module we developed for the spectrogram implementation together with the associated programming environment. Our aim is to introduce to students the advanced concepts of TFRs at an early stage in their education without requiring a rigorous theoretical background. We developed two sets of exercises using the spectrogram based on signal analysis and speech processing together with on-line evaluation forms to assess student learning experiences. In the paper, we also provide the positive statistical and qualitative feedback we obtained when the Java software and corresponding exercises were used in a signal processing course.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1263306,no,undetermined,0
Investigation of interfaces with analytical tools,"This paper focuses on advancements in three areas of analyzing interfaces, namely, acoustic microscopy for detecting damage to closely spaced interfaces, thermal imaging to detect damage and degradation of thermal interface materials and laser spallation, a relatively new concept to understand the strength of interfaces. Acoustic microscopy has been used widely in the semiconductor assembly and package area to detect delamination, cracks and voids in the package, but the resolution in the axial direction has always been a limitation of the technique. Recent advancements in acoustic waveform analysis has now allowed for detection and resolution of closely spaced interfaces such as layers within the die. Thermal imaging using infrared (IR) thermography has long been used for detection of hot spots in the die or package. With recent advancements in very high-speed IR cameras, improved pixel resolution, and sophisticated software programming, the kinetics of heat flow can now be imaged and analyzed to reveal damage or degradation of interfaces that are critical to heat transfer. The technique has been demonstrated to be useful to understand defects and degradation of thermal interface materials used to conduct heat away from the device. Laser spallation is a method that uses a short duration laser pulse to cause fracture at the weakest interface and has the ability to measure the adhesion strength of the interface. The advantage of this technique is that it can be used for fully processed die or wafers and even on packaged devices. The technique has been used to understand interfaces in devices with copper metallization and low-k dielectrics.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261732,no,undetermined,0
Molecular imaging of the myoskeletal system through Diffusion Weighted and Diffusion Tensor Imaging with parallel imaging techniques,"Diffusion weighted imaging (DWI) and diffusion tensor imaging (DTI) are useful tools when used in combination with standard imaging methods that may offer a significant advantage in certain clinical applications such as orthopedics and myoskeletal tissue imaging. Incorporation of these tools in clinical practice is limited due to the considerable amount of user intervention that apparent diffusion coefficient (ADC) and anisotropy data require in terms of processing and quantification require and due to the importance of acquisition parameter optimization in image quality. In this work various acquisition parameters and their effects in DWI and DTI are investigated. To assess the quality of these techniques, a series of experiments were conducted using a phantom. The application of lipid suppression techniques and their compatibility with other parameters were also investigated. Artifacts were provoked to study the effects in imaging quality. All the data were processed with specialized software to analyze various aspects of the measurements and quantify various parameters such as signal to noise ratio (SNR), contrast to noise ratio (CNR), and the accuracy of ADC and fractional anisotropy values. The experience acquired from the experiments was applied in acquisition parameter optimization and improvement of clinical applications for the rapid screening and differential diagnosis of myoskeletal pathologies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4659972,no,undetermined,0
Addressing workload variability in architectural simulations,"The inherent variability of multithreaded commercial workloads can lead to incorrect results in architectural simulation studies. Although most architectural simulation studies ignore space variability's effects, our results demonstrate that space variability has serious implications for architectural simulation studies using multithreaded workloads. The standard solution - running long enough - does not easily apply to simulation because of its enormous slowdown. To address this problem, we propose a simulation methodology combining multiple simulations with standard statistical techniques, such as confidence intervals and hypothesis testing. This methodology greatly decreases the probability of drawing incorrect conclusions, and permits reasonable simulation times given sufficient simulation hosts.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1261392,no,undetermined,0
A heuristic for refresh policy selection in heterogeneous environments,"We address data warehouse maintenance, i.e. how changes to autonomous sources should be detected and propagated to a warehouse. We have extended our work on source characteristics and timings relevant to single source views by exploring data integration from (multiple) heterogeneous sources. We identify relevant maintenance policies and develop a set of heuristics to guide policy choice. On the basis of empirical (testbed) experiments, we claim that resulting selections are good.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260831,no,undetermined,0
Application of automated mapping system to distribution transformer load management,"This paper develops an application program that is based on the automated mapping and facility management system (AM/FM) to provide load forecasting and power flow calculation capability in distribution systems. First, the database and related data structure used in the Taipower distribution automation pilot system is studied and thoroughly analyzed. Then, our program, developed by the AM/FM ODL software, is integrated into the above pilot system. This program can predict future load growth on distribution feeders, considering the effects of temperature variation, and power needed for air conditioners. In addition, on the basis of load density and diversity factors of typical customers, the saturation load of a new housing zone can be estimated. As for the power flow analysis, it can provide three-phase quantities of voltage drop at each node, the branch current, and the system loss. The program developed in this study can effectively aid public electric utilities in distribution system planning and operation.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1177645,no,undetermined,0
Preserving non-programmers' motivation with error-prevention and debugging support tools,"A significant challenge in teaching programming to disadvantaged populations is preserving learners' motivation and confidence. Because programming requires such a diverse set of skills and knowledge, the first steps in learning to program can be highly error-prone, and can quickly exhaust whatever attention learners are willing to give to a programming task. Our approach to preserving learners' motivation is to design highly integrated support tools to prevent the errors they would otherwise make. In this paper, the results of a recent study on programming errors are summarized, and many novel error-preventing tools are proposed.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260245,no,undetermined,0
Visual composition of Web services,"Composing Web services into a coherent application can be a tedious and error prone task when using traditional textual scripting languages. As an alternative, complex interactions patterns and data exchanges between different Web services can be effectively modeled using a visual language. In this paper we discuss the requirements of such an application scenario and we present the design of the BioOpera Flow Language. This visual composition language has been fully implemented in a development environment for Web service composition with usability features emphasizing rapid development and visual scalability.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260208,no,undetermined,0
Unbounded system model allows robust communication infrastructure for power quality measurement and control,"A robust information infrastructure is required to collect power quality measurements and to execute corrective actions. It is based on a software architecture, designed at middleware level, that makes use of Internet protocols for communication over different media. While the middleware detects the anomalies in the communication and computation system and reacts appropriately, the application functionality is maintained through system reconfiguration or graceful degradation. Such anomalies may come from dynamic changes in the topology of the underlying communication system, or the enabling/disabling of processing nodes on the network. The added value of this approach comes from the flexibility to deal with a dynamic environment based on an unbounded system model. The paper illustrates this approach in a power quality measurement and control system with compensation based on active filters.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1259348,no,undetermined,0
An adaptive bandwidth reservation scheme in multimedia wireless networks,"Next generation wireless networks target to provide quality of service (QoS) for multimedia applications. In this paper, the system supports two QoS criteria, i.e., the system should keep the handoff dropping probability always less than a predefined QoS bound, while maintaining the relative priorities of different traffic classes in terms of blocking probability. To achieve this goal, a dynamic multiple-threshold bandwidth reservation scheme is proposed, which is capable of granting differential priorities to different traffic class and to new ad handoff traffic for each class by dynamically adjusting bandwidth reservation thresholds. Moreover, in times of network congestion, a preventive measure by use of throttling new connection acceptance is taken. Another contribution of this paper is to generalize the concept of relative priority, hence giving the network operator more flexibility to adjust admission control policy by incorporating some dynamic factors such as offered load. The elaborate simulation is conducted to verify the performance of the scheme.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1258751,no,undetermined,0
Detection of invalid routing announcements in RIP protocol,"Traditional routing protocol designs have focused solely on the functionality of the protocols and implicitly assume that all routing update messages received by a router carry valid information. However operational experience suggests that hardware faults, software implementation bugs, operator misconfigurations, let alone malicious attacks can all lead to invalid routing protocol announcements. Although several recent efforts have developed cryptography-based authentication for routing protocols, such enhancements alone are rendered ineffective in the face of faults caused by misconfigurations or hardware/software errors. In this paper we develop a simple routing update validation algorithm for the RIP protocol, RIP with triangle theorem checking and probing (RIP-TP). In RIP-TP routers utilize a triangle theorem to identify suspicious new routing announcements, and then use probing messages to verify the correctness of the announcements. We have evaluated the effectiveness of RIP-TP through simulation using various faulty node behaviors, link failure dynamics and network sizes. The results show that, with an overhead as low as about one probing message per received update message in the worst case, RIP-TP can effectively detect 95% or more invalid routing announcements.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1258478,no,undetermined,0
"Using Internet-based, distributed collaborative writing tools to improve coordination and group awareness in writing teams","The paper argues for using specialized collaborative writing (CW) tools to improve the results of distributed, Internet-based writing teams. The key features of collaborative tools that support enhanced coordination and group awareness are compared to existing writing tools. The first Internet-based CW tool, Collaboratus, is introduced, and its group features are compared with those of Microsoft Word. Next, theoretical propositions, hypotheses, and constructs are formulated to predict outcomes of distributed groups that use CW tools. A four-week-long synchronous-distributed experiment then compares the outcomes of Collaboratus and Word groups. Innovative measures show that Collaboratus groups generally experience better outcomes than Word groups, in terms of productivity, document quality, relationships, and communication, but not in terms of satisfaction. The results buttress the conclusion that Internet-based CW teams can benefit from specialized collaborative technologies that provide enhanced coordination, group awareness, and CW activity support.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1255526,no,undetermined,0
A model for battery lifetime analysis for organizing applications on a pocket computer,"A battery-powered portable electronic system shuts down once the battery is discharged; therefore, it is important to take the battery behavior into account. A system designer needs an adequate high-level battery model to make battery-aware decisions targeting the maximization of the system's online lifetime. We propose such a model that allows a designer to analytically predict the battery time-to-failure for a given load. Our model also allows for a tradeoff between the accuracy and the amount of computation performed. The quality of the proposed model is evaluated using typical pocket computer applications and a detailed low-level simulation of a lithium-ion electrochemical cell. In addition, we verify the proposed model against actual measurements taken on a real lithium-ion battery.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1255477,no,undetermined,0
Flexible interface matching for Web-service discovery,"The Web-services stack of standards is designed to support the reuse and interoperation of software components on the Web. A critical step, to that end, is service discovery, i.e., the identification of existing Web services that can potentially be used in the context of a new Web application. UDDI, the standard API for publishing Web-services specifications, provides a simple browsing-by-business-category mechanism for developers to review and select published services. In our work, we have developed a flexible service discovery method, for identifying potentially useful services and assessing their relevance to the task at hand. Given a textual description of the desired service, a traditional information-retrieval method is used to identify the most similar service description files, and to order them according to their similarity. Next, given this set of likely candidates and a (potentially partial) specification of the desired service behavior, a structure-matching step further refines and assesses the quality of the candidate service set. In this paper, we describe and experimentally evaluate our Web-service discovery process.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254478,no,undetermined,0
Automated support for data exchange via XML,"XML has recently emerged as a standard for exchanging data between different software applications. We present an approach for automatic code generation to interpret information in an XML document. The approach is based on a user-defined mapping of the XML document's structure onto the application's API. This mapping is declarative in nature, and thus easy to specify, and is used by code generator that applies advanced code generation and manipulation techniques to generate the appropriate code. The approach relieves developers from the time-consuming and error-prone task of writing the interpreter themselves, and complements existing XML technologies such as XSLT.",2003,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1254424,no,undetermined,0
Refinement and test case generation in Unifying Theory of Programming,"This talk presents a theory of testing that integrates into Hoare and Hepsilas Unifying Theory of Programming (UTP). We give test cases a denotational semantics by viewing them as specification predicates. This reformulation of test cases allows for relating test cases via refinement to specifications and programs. Having such a refinement order that integrates test cases, we develop a testing theory for fault-based testing. Fault-based testing uses test data designed to demonstrate the absence of a set of pre-specified faults. A well-known fault-based technique is mutation testing. In mutation testing, first, faults are injected into a program by altering (mutating) its source code. Then, test cases that can detect these errors are designed. The assumption is that other faults will be caught, too. We apply the mutation technique to both specifications and programs. Using our theory of testing, two new test case generation laws for detecting injected (anticipated) faults are presented: one is based on the semantic level of design specifications, the other on the algebraic properties of a programming language.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658048,no,undetermined,0
Citation recognition for scientific publications in digital libraries,"A method based on part-of-speech tagging (PoS) is used for bibliographic reference structure. This method operates on a roughly structured ASCII file, produced by OCR. Because of the heterogeneity of the reference structure, the method acts in a bottom-up way, without an a priori model, gathering structural elements from basic tags to subfields and fields. Significant tags are first grouped in homogeneous classes according to their categories and then reduced in canonical forms corresponding to record fields: """"authors"""", """"title"""", """"conference name"""", """"date"""", etc. Nonlabeled tokens are integrated in one or another field by either applying PoS correction rules or using a interor intra-field model generated from well-detected records. The designed prototype operates with a great satisfaction on different record layouts and character recognition qualities. Without manual intervention, 96.6% words are correctly attributed, and about 75,9% references are completely segmented from 2,575 references.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1263253,no,undetermined,0
"A multi-interface, multi-profiling system for chronic disease management learning","A key aspect of successful chronic disease management is active partnership between consumer and provider - this is particularly important in diabetes management, where many key activities are in the hands of the patient. We have developed a multi-interface system that promotes high quality diabetes management through profiling and adaptive support of both consumer and provider in the context of a university podiatry clinic. Handheld devices are used for decision support, data capture and notification of patient concerns in consultation. Consultation data integrates with Web based learning environments for podiatry students and consumers. The architecture implements our approach to patient provider partnership and exemplifies integration of system goals across platforms, users and devices. Upcoming field trials assess whether we have achieved an acceptable system that improves quality of management activities.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265392,no,undetermined,0
A real-time monitoring and diagnosis system for manufacturing automation,"Condition monitoring and fault diagnosis in modern engineering practices is of great practical significance for improving the quality and productivity, preventing the machinery from damages. In general, this practice consists of two parts: extracting appropriate features from sensor signals and recognizing possible faulty patterns from the features. In order to cope with the complex manufacturing operations and develop a feasible system for real-time application, we proposed three approaches. By defining the marginal energy, a new feature representation emerged, while by real-time learning algorithms with support vector techniques and hidden Markov model representations, a modular software architecture and a new similarity measure were developed for comparison, monitoring, and diagnosis. A novel intelligent computer-based system has been developed and evaluated in over 30 factories and numerous metal stamping processes as an example of manufacturing operations. The real-time operation of this system demonstrated that the proposed system is able to detect abnormal conditions efficiently and effectively resulting in a low-cost, effective approach to real-time monitoring in manufacturing. The related technologies have been transferred to industry, presenting a tremendous impact in current automation practice in Asia and the world.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1308024,no,undetermined,0
Semidefinite programming for ad hoc wireless sensor network localization,We describe an SDP relaxation based method for the position estimation problem in wireless sensor networks. The optimization problem is set up so as to minimize the error in sensor positions to fit distance measures. Observable gauges are developed to check the quality of the point estimation of sensors or to detect erroneous sensors. The performance of this technique is highly satisfactory compared to other techniques. Very few anchor nodes are required to accurately estimate the position of all the unknown nodes in a network. Also the estimation errors are minimal even when the anchor nodes are not suitably placed within the network or the distance measurements are noisy.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307322,no,undetermined,0
Thinking about thinking aloud: a comparison of two verbal protocols for usability testing,"We report on an exploratory experimental comparison of two different thinking aloud approaches in a usability test that focused on navigation problems in a highly nonstandard Web site. One approach is a rigid application of Ericsson and Simon's (for original paper see Protocol Analysis: Verbal Reports as Data, MIT Press (1993)) procedure. The other is derived from Boren and Ramey's (for original paper see ibid., vol. 43, no. 3, p. 261-278 (2000)) proposal based on speech communication. The latter approach differs from the former in that the experimenter has more room for acknowledging (mm-hmm) contributions from subjects and has the possibility of asking for clarifications and offering encouragement. Comparing the verbal reports obtained with these two methods, we find that the process of thinking aloud while carrying out tasks is not affected by the type of approach that was used. The task performance does differ. More tasks were completed in the B and R condition, and subjects were less lost. Nevertheless, subjects' evaluations of the Web site quality did not differ, nor did the number of different navigation problems that were detected.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303808,no,undetermined,0
A heuristic for multi-constrained multicast routing,"In contrast to the situation that the constrained minimum Steiner tree (CMST) problem has attracted much attention in the quality of service (QoS) routing area, little work has been done on multicast routing subject to multiple additive constraints, even though the corresponding applications are obvious. We propose a heuristic, HMCMC, to solve this problem. The basic idea of HMCMC is to construct the multicast tree step by step, which is done essentially based on the latest research results on multi-constrained unicast routing. Computer simulations demonstrate that, if there is one, the proposed heuristic can find a feasible multicast tree with a fairly high probability.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303497,no,undetermined,0
An investigation into the application of different performance prediction techniques to e-Commerce applications,Summary form only given. Predictive performance models of e-Commerce applications allows grid workload managers to provide e-Commerce clients with qualities of service (QoS) whilst making efficient use of resources. We demonstrate the use of two 'coarse-grained' modelling approaches (based on layered queuing modelling and historical performance data analysis) for predicting the performance of dynamic e-Commerce systems on heterogeneous servers. Results for a popular e-Commerce benchmark show how request response times and server throughputs can be predicted on servers with heterogeneous CPUs at different background loads. The two approaches are compared and their usefulness to grid workload management is considered.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303306,no,undetermined,0
Implementing a reconfigurable atomic memory service for dynamic networks,"Summary form only given. Transforming abstract algorithm specifications into executable code is an error-prone process in the absence of sophisticated compilers that can automatically translate such specifications into the target distributed system. We present a framework that was developed for translating algorithms specified as Input/Output Automata (IOA) to distributed programs. The framework consists of a methodology that guides the software development process and a core set of functions needed in target implementations that reduce unnecessary software development. The systems developed using this methodology preserve the modularity of the original specifications, making it easier to track refinements and effect optimizations. As a proof of concept, this work also presents a distributed implementation of a reconfigurable atomic memory service for dynamic networks (RAMBO). This service emulates atomic read/write shared objects in the dynamic setting where processors can arbitrarily crash, or join and leave the computation. The algorithm tolerates processor crashes and message loss and guarantees atomicity for arbitrary patterns of asynchrony and failure. The algorithm implementing the service is given in terms of IOA. An important consideration in formulating RAMBO was that it could be employed as a building block in real systems. Following a formal presentation of RAMBO algorithm, this work describes an optimized implementation that was developed using the methodology presented here. The system is implemented in Java and runs on a network of workstations. Empirical data illustrates the behavior of the system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303237,no,undetermined,0
On static WCET analysis vs. run-time monitoring of execution time,"Summary form only given. Dynamic, distributed, real-time control systems control a widely varying environment, are made up of application programs that are dispersed among loosely-coupled computers, and must control the environment in a timely manner. The environment determines the number of threats; thus, it is difficult to determine the range of the workload at design time using static worst-case execution time analysis. While a system is lightly loaded, it is wasteful to reserve resources for the heaviest load. Likewise, it is also possible that the load will increase higher than the assumed worst case. A system that has a preset number of resources reserved to it is no longer guaranteed to meet its deadlines under such conditions. In order to ensure that such applications meet their real-time requirements, a mechanism is required to monitor and maintain the real-time quality of service (QoS): a QoS manager, which monitors the processing timing (latency) and resource usage of a distributed real-time system, forecasts, detects and diagnoses violations of the timing constraints, and requests more or fewer resources to maintain the desired timing characteristics. To enable better control over the system, the goals are as follows: 1) Gather detailed information about antiair warfare and air-traffic control application domains and employ it in the creation of a distributed real-time sensing and visualization testbed for air-traffic control. 2) Identify mathematical relationships among independent and dependent variables, such as performance and fault tolerance vs. resource usage, and security vs. performance. 3) Uncover new techniques for ensuring performance, fault tolerance, and security by optimizing the variables under the constraints of resource availability and user requirements.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303089,no,undetermined,0
Finding satisfying global states: all for one and one for all,"Summary form only given. Given a distributed computation and a global predicate, predicate detection involves determining whether there exists at least one consistent cut (or global state) of the computation that satisfies the predicate. On the other hand, computation slicing is concerned with computing the smallest sub-computation - with the least number of consistent cuts - that contains all consistent cuts of the computation satisfying the predicate. We investigate the relationship between predicate detection and computation slicing and show that the two problems are equivalent. Specifically, given an algorithm to detect a predicate b in a computation C, we derive an algorithm to compute the slice of C with respect to b. The time-complexity of the (derived) slicing algorithm is O(n|E|) times the time-complexity of the detection algorithm, where n is the number of processes and E is the set of events. We discuss how the """"equivalence """" result can be utilized to derive a faster algorithm for solving the general predicate detection problem. Slicing algorithms described in our earlier papers are all off-line in nature. We also give an online algorithm for computing the slice for a predicate that can be detected efficiently. The amortized time-complexity of the algorithm is O(n(c + n)) times the time-complexity of the detection algorithm, where c is the average concurrency in the computation.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1302994,no,undetermined,0
On identifying stable ways to configure systems,We consider the often error-prone process of initially building and/or reconfiguring a computer system. We formulate an optimization framework for capturing certain aspects of this system (re)configuration process. We describe offline and online algorithms that could aid operators in making decisions for how best to take actions on their computers so as to maintain the health of their systems.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1301358,no,undetermined,0
Autonomic pervasive computing based on planning,"Pervasive computing envisions a world with users interacting naturally with device-rich environments to perform various kinds of tasks. These environments must, thus, be self-managing and autonomic systems, receiving only high-level guidance from users. However, these environments are also highly dynamic $the context and resources available in these environments can change rapidly. They are also prone to failures - one or more entities can fail due to variety of reasons. The dynamic and fault-prone nature of these environments poses major challenges to their autonomic operation. In this paper we present a paradigm for the operation of pervasive computing environments that is based on goal specification and STRIPS-based planning. Users as well as application developers can describe tasks to be performed in terms of abstract goals and a planning framework decides how these goals are to be achieved. This paradigm helps improve the fault-tolerance, adaptability, ease of programming and usability of these environments. We have developed and used a prototype planning system within our pervasive computing system, Gaia.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1301350,no,undetermined,0
Assessing the robustness of self-managing computer systems under highly variable workloads,"Computer systems are becoming extremely complex due to the large number and heterogeneity of their hardware and software components, the multilayered architecture used in their design, and the unpredictable nature of their workloads. Thus, performance management becomes difficult and expensive when carried out by human beings. An approach, called self-managing computer systems, is to build into the systems the mechanisms required to self-adjust configuration parameters so that the quality of service requirements of the system are constantly met. In this paper, we evaluate the robustness of such methods when the workload exhibits high variability in terms of the interarrival time and service times of requests. Another contribution of this paper is the assessment of the use of workload forecasting techniques in the design of QoS controllers.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1301348,no,undetermined,0
Hardware - software structure for on-line power quality assessment: part I,"The main objective of the proposed work is to introduce a new concept of advanced power quality assessment. The introduced system is implemented using applications of a set of powerful software algorithms and a digital signal processor based hardware data acquisition system. The suggested scheme is mainly to construct a system for real time detection and identification of different types of power quality disturbances that produce a sudden change in the power quality levels. Moreover, a new mitigation technique through generating feedback correction signals for disturbance compensation is addressed. The performance of the suggested system is tested and verified through real test examples. The obtained results reveal that, the introduced system detects fast and accurately most of the power quality disturbance events and introduces new indicative factors estimating the performance of any supply system subjected to a set number of disturbance events.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1300913,no,undetermined,0
A model of scalable distributed network performance management,"Quality of service in IP networks necessitates the use of performance management. As Internet continues to grow exponentially, a management system should be scalable in terms of network size, speed and number of customers subscribed to value-added services. This article proposes a flexible, scalable, self-adapting model for managing large-scale distributed network. In this model, Web services framework is used to build the software architecture and XML is used to build the data exchange interface. Policy-based hierarchical event-processing mechanism presented by this paper can efficiently balance the loads and improve the flexibility of the system. The prediction algorithm adopted by this model can predict the network performance more effectively and accurately.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1300545,no,undetermined,0
Generic and reflective graph transformations for the checking and enforcement of modeling guidelines,"In the automotive industry, the model driven development of software, today considered as the standard paradigm, is generally based on the use of the tool MATLAB Simulink/Stateflow. To increase the quality, the reliability, and the efficiency of the models and the generated code, checking and elimination of detected guideline violations defined in huge catalogues has become an essential task in the development process. It represents such a tremendous amount of boring work that it must necessarily be automated. In the past we have shown that graph transformation tools like Fujaba/MOFLON allow for the specification of single modeling guidelines on a very high level of abstraction and that guideline checking tools can be generated from these specifications easily. Unfortunately, graph transformation languages do not offer appropriate concepts for reuse of specification fragments - a MUST, when we deal with hundreds of guidelines. As a consequence we present an extension of MOFLON that supports the definition of generic rewrite rules and combines them with the reflective programming mechanisms of Java and the model repository interface standard JMI.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4639088,no,undetermined,0
A framework for classifying and comparing software architecture evaluation methods,"Software architecture evaluation has been proposed as a means to achieve quality attributes such as maintainability and reliability in a system. The objective of the evaluation is to assess whether or not the architecture lead to the desired quality attributes. Recently, there have been a number of evaluation methods proposed. There is, however, little consensus on the technical and nontechnical issues that a method should comprehensively address and which of the existing methods is most suitable for a particular issue. We present a set of commonly known but informally described features of an evaluation method and organizes them within a framework that should offer guidance on the choice of the most appropriate method for an evaluation exercise. We use this framework to characterise eight SA evaluation methods.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1290484,no,undetermined,0
Evaluating Models for Model-Based Debugging,"Developing model-based automatic debugging strategies has been an active research area for several years, with the aim of locating defects in a program by utilising fully automated generation of a model of the program from its source code. We provide an overview of current techniques in model-based debugging and assess strengths and weaknesses of the individual approaches. An empirical comparison is presented that investigates the relative accuracy of different models on a set of test programs and fault assumptions, showing that our abstract interpretation based model provides high accuracy at significantly less computational effort than slightly more accurate techniques. We compare a range of model-based debugging techniques with other state-of-the-art automated debugging approaches and outline possible future developments in automatic debugging using model-based reasoning as the central unifying component in a comprehensive framework.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4639316,no,undetermined,0
Exploring the evolution of software quality with animated visualization,"Assessing software quality and understanding how events in its evolution have lead to anomalies are two important steps toward reducing costs in software maintenance. Unfortunately, evaluation of large quantities of code over several versions is a task too time-consuming, if not overwhelming, to be applicable in general. To address this problem, we designed a visualization framework as a semi-automatic approach to quickly investigate programs composed of thousands of classes, over dozens of versions. Programs and their associated quality characteristics for each version are graphically represented and displayed independently. Real-time navigation and animation between these representations recreate visual coherences often associated with coherences intrinsic to subsequent software versions. Exploiting such coherences can reduce cognitive gaps between the different views of software, and allows human experts to use their visual capacity and intuition to efficiently investigate and understand various quality aspects of software evolution. To illustrate the interest of our framework, we report our results on two case studies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4639052,no,undetermined,0
Exact analysis of a class of GI/G/1-type performability models,"We present an exact decomposition algorithm for the analysis of Markov chains with a GI/G/1-type repetitive structure. Such processes exhibit both M/G/1-type & GI/M/1-type patterns, and cannot be solved using existing techniques. Markov chains with a GI/G/1 pattern result when modeling open systems which accept jobs from multiple exogenous sources, and are subject to failures & repairs; a single failure can empty the system of jobs, while a single batch arrival can add many jobs to the system. Our method provides exact computation of the stationary probabilities, which can then be used to obtain performance measures such as the average queue length or any of its higher moments, as well as the probability of the system being in various failure states, thus performability measures. We formulate the conditions under which our approach is applicable, and illustrate it via the performability analysis of a parallel computer system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1308668,no,undetermined,0
Optimizing testing efficiency with error-prone path identification and genetic algorithms,"We present a method for optimizing software testing efficiency by identifying the most error prone path clusters in a program. We do this by developing variable length genetic algorithms that optimize and select the software path clusters which are weighted with sources of error indexes. Although various methods have been applied to detecting and reducing errors in a whole system, there is little research into partitioning a system into smaller error prone domains for testing. Exhaustive software testing is rarely possible because it becomes intractable for even medium sized software. Typically only parts of a program can be tested, but these parts are not necessarily the most error prone. Therefore, we are developing a more selective approach to testing by focusing on those parts that are most likely to contain faults, so that the most error prone paths can be tested first. By identifying the most error prone paths, the testing efficiency can be increased.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1290463,no,undetermined,0
Analyzing information flow control policies in requirements engineering,"Currently security features are implemented and validated during the last phases of the software development life cycle. This practice results in less secure software systems and higher cost of fixing defects software vulnerability. To achieve more secure systems, security features must be considered during the early phases of the software development process. This work presents a high-level methodology that analyzes the information flow requirements and ensures the proper enforcement of information flow control policies. The methodology uses requirements specified in the Unified Modeling Language (UML) as its input and stratified logic programming language as the analysis language. The methodology improves security by detecting unsafe information flows before proceeding to latter stages of the life cycle.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1309167,no,undetermined,0
Diverse firewall design,"Firewalls are safety-critical systems that secure most private networks. An error in a firewall either leaks secret information from its network or disrupts legitimate communication between its network and the rest of the Internet. How to design a correct firewall is therefore an important issue. In this paper, we propose the method of diverse firewall design, which is inspired by the well-known method of design diversity for building fault-tolerant software. Our method consists of two phases: a design phase and a comparison phase. In the design phase, the same requirement specification of a firewall is given to multiple teams who proceed independently to design different versions of the firewall. In the comparison phase, the resulting multiple versions are compared with each other to find out all the discrepancies between them, then each discrepancy is further investigated and a correction is applied if necessary. The technical challenge in the method of diverse firewall design is how to discover all the discrepancies between two given firewalls. We present a series of three efficient algorithms for solving this problem: (I) a construction algorithm for constructing an equivalent ordered firewall decision diagram from a sequence of rules, (2) a shaping algorithm for transforming two ordered firewall decision diagrams to become semi-isomorphic without changing their semantics, and (3) a comparison algorithm for detecting all the discrepancies between two semi-isomorphic firewall decision diagrams.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311930,no,undetermined,0
Why PCs are fragile and what we can do about it: a study of Windows registry problems,"Software configuration problems are a major source of failures in computer systems. In this paper, we present a new framework for categorizing configuration problems. We apply this categorization to Windows registry-related problems obtained from various internal as well as external sources. Although infrequent, registry-related problems are difficult to diagnose and repair. Consequently they frustrate the users. We classify problems based on their manifestation and the scope of impact to gain useful insights into how problems affect users and why PCs are fragile. We then describe techniques to identify and eliminate such registry failures. We propose health predicate monitoring for detecting known problems, fault injection for improving application, robustness, and access protection mechanisms for preventing fragility problems.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311926,no,undetermined,0
Fault tolerant multipath routing with overlap-aware path selection and dynamic packet distribution on overlay network for real-time streaming applications,In this paper we propose overlap-aware path selection and dynamic packet distribution due to failure detection in multipath routing overlay network. Real-time communications that utilize UDP do not ensure reliability for realizing fast transmission. Therefore congestion or failure in a network deteriorates the quality of service significantly. The proposed method seeks an alternate path that hardly overlaps an IP path so as to improve its reliability. The proposed method also detects congestion or failure by differential of packet loss rate and apportion packets to the IP path and the alternate path dynamically. Evaluation on PlanetLab shows the proposed method avoids congestion Consequently the influence of congestion and failure lessens and the proposed multipath routing improves the reliability which can be used for real-time communications.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4638724,no,undetermined,0
Does your result checker really check?,"A result checker is a program that checks the output of the computation of the observed program for correctness. Introduced originally by Blum, the result-checking paradigm has provided a powerful platform assuring the reliability of software. However, constructing result checkers for most problems requires not only significant domain knowledge but also ingenuity and can be error prone. In this paper we present our experience in validating result checkers using formal methods. We have conducted several case studies in validating result checkers from the commercial LEDA system for combinatorial and geometric computing. In one of our case studies, we detected a logical error in a result checker for a program computing max flow of a graph.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311909,no,undetermined,0
A bi-criteria scheduling heuristic for distributed embedded systems under reliability and real-time constraints,"Multi-criteria scheduling problems, involving optimization of more than one criterion, are subject to a growing interest. In this paper, we present a new bi-criteria scheduling heuristic for scheduling data-flow graphs of operations onto parallel heterogeneous architectures according to two criteria: first the minimization of the schedule length, and second the maximization of the system reliability. Reliability is defined as the probability that none of the system components will fail while processing. The proposed algorithm is a list scheduling heuristics, based on a bi-criteria compromise function that introduces priority between the operations to be scheduled, and that chooses on what subset of processors they should be scheduled. It uses the active replication of operations to improve the reliability. If the system reliability or the schedule length requirements are not met, then a parameter of the compromise function can be changed and the algorithm re-executed. This process is iterated until both requirements are met.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311904,no,undetermined,0
Dependable initialization of large-scale distributed software,"Most documented efforts in fault-tolerant computing address the problem of recovering from failures that occur during normal system operation. To bring a system to a point where it can begin performing its duties first requires that the system successfully complete initialization. Large-scale distributed systems may take hours to initialize. For such systems, a key challenge is tolerating failures that occur during initialization, while still completing initialization in a timely manner. In this paper, we present a dependable initialization model that captures the architecture of the system to be initialized, as well as interdependencies among system components. We show that overall system initialization may sometimes complete more quickly if recovery actions are deferred as opposed to commencing recovery actions as soon as a failure is detected. This observation leads us to introduce a recovery decision function that dynamically assesses when to take recovery actions. We then describe a dependable initialization algorithm that combines the dependable initialization model and the recovery decision function for achieving fast initialization. Experimental results show that our algorithm incurs lower initialization overhead than that of a conventional initialization algorithm. This work is the first effort we are aware of that formally studies the challenges of initializing a distributed system in the presence of failures.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311903,no,undetermined,0
Quantifying the reliability of proven SPIDER group membership service guarantees,"For safety-critical systems, it is essential to quantify the reliability of the assumptions that underlie proven guarantees. We investigate the reliability of the assumptions of the SPIDER group membership service with respect to transient and permanent faults. Modeling 12,600 possible system configurations, the probability that SPIDER's maximum fault assumption does not hold for an hour mission varies from less likely than l0<sup>-11</sup> to more likely than 10<sup>-3</sup>. In most cases examined, a transient fault tolerance strategy was superior to the permanent fault tolerance strategy previously in use for the range of transient fault arrival rates expected in aerospace systems. Reliability of the maximum fault assumption (upon which the proofs are based) differs greatly when subjected to asymmetric, symmetric, and benign faults. This case study demonstrates the benefits of quantifying the reliability of assumptions for proven properties.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311897,no,undetermined,0
An intelligent admission control scheme for next generation wireless systems using distributed genetic algorithms,"A different variety of services requiring different levels of quality of service (QoS) need be addressed for mobile users of the next generation wireless system (NGWS). An efficient handoff technique with intelligent admission control can accomplish this aim. In this paper, a new, intelligent handoff scheme using distributed genetic algorithms (DGA) is proposed for NGWS. This scheme uses DGA to achieve high network utilization, minimum cost and handoff latency. A performance analysis is provided to assess the efficiency of the proposed DGA scheme. Simulation results show a significant improvement in handoff latencies and costs over traditional genetic algorithms and other admission control schemes.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311817,no,undetermined,0
Requirements driven software evolution,"Software evolution is an integral part of the software life cycle. Furthermore in the recent years the issue of keeping legacy systems operational in new platforms has become critical and one of the top priorities in IT departments worldwide. The research community and the industry have responded to these challenges by investigating and proposing techniques for analyzing, transforming, integrating, and porting software systems to new platforms, languages, and operating environments. However, measuring and ensuring that compliance of the migrant system with specific target requirements have not been formally and thoroughly addressed. We believe that issues such as the identification, measurement, and evaluation of specific re-engineering and transformation strategies and their impact on the quality of the migrant system pose major challenges in the software re-engineering community. Other related problems include the verification, validation, and testing of migrant systems, and the design of techniques for keeping various models (architecture, design, source code) during evolution, synchronized. In this working session, we plan to assess the state of the art in these areas, discuss on-going work, and identify further research issues.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311070,no,undetermined,0
An investigation of the approach to specification-based program review through case studies,"Software review is an effective means to enhance the quality of software systems. However, traditional review methods emphasize the importance of the way to organize reviews and rely on the quality of the reviewers' experience and personal skills. In this paper we propose a new approach to rigorously reviewing programs based on their formal specifications. The fundamental idea of the approach is to use a formal specification as a standard to check whether all the required functions and properties in the specification are correctly implemented by its program. To help investigate the effectiveness and the weakness of the approach, we conduct two case studies of reviewing two program systems that implement the same formal specification of """"A Research Management Policy"""" using different strategies, and present the evaluation of the case studies. The results show that the review approach is effective in detecting faults when the reviewer is different from the programmer, but less effective when the reviewer is the same as the programmer.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310924,no,undetermined,0
ASAAM: aspectual software architecture analysis method,"Software architecture analysis methods aim to predict the quality of a system before it has been developed. In general, the quality of the architecture is validated by analyzing the impact of predefined scenarios on architectural components. Hereby, it is implicitly assumed that an appropriate refactoring of the architecture design can help in coping with critical scenarios and mending the architecture. This paper shows that there are also concerns at the architecture design level which inherently crosscut multiple architectural components, which cannot be localized in one architectural component and which, as such, can not be easily managed by using conventional abstraction mechanisms. We propose the aspectual software architecture analysis method (ASAAM) to explicitly identify and specify these architectural aspects and make them transparent early in the software development life cycle. ASAAM introduces a set of heuristic rules that help to derive architectural aspects and the corresponding tangled architectural components from scenarios. The approach is illustrated for architectural aspect identification in the architecture design of a window management system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310685,no,undetermined,0
Impact of process variation phenomena on performance and quality assessment,"Summary form only given. Logic product density and performance trends have continued to follow the course predicted by Moore's Law. To support the trends in the future and build logic products approaching one billion or more transistors before the end of the decade, several challenges must be met. These challenges include: 1) maintaining transistor/interconnect feature scaling, 2) the increasing power density dilemma, 3) increasing relative difficulty of 2-D feature resolution and general critical dimension control, 4) identifying cost effective solutions to increasing process and design database complexity, and 5), improving general performance and quality predictability in the face of the growing control, complexity and predictability issues. The trend in transistor scaling can be maintained while addressing the power density issue with new transistor structures, design approaches, and product architectures (e.g. high-k, metal gate, etc.). Items 3 to 5 are the focus of this work and are also strongly inter-related. The general 2-D patterning and resolution control problems will require several solution approaches both through design and technology e.g. reduce design degrees of freedom, use of simpler arrayed structures, improved uniformity, improved tools, etc. The data base complexity/cost problem will require solutions likely to involve use of improved data structure, improved use of hierarchy, and improved software and hardware solutions. Performance assessment, predictability and quality assessment will benefit from solutions to the control and complexity issues noted above. In addition, new design techniques/tools as well as improved process characterization models and methods can address the general performance/quality assessment challenge.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1309897,no,undetermined,0
"Requirements triage: what can we learn from a """"medical"""" approach?","New-product development is commonly risky, judging by the number of high-profile failures that continue to occur-especially in software engineering. We can trace many of these failures back to requirements-related issues. Triage is a technique that the medical profession uses to prioritize treatment to patients on the basis of their symptoms' severity. Trauma triage provides some tantalizing insights into how we might measure risk of failure early, quickly, and accurately. For projects at significant risk, we could activate a """"requirements trauma system"""" to include specialists, processes, and tools designed to correct the issues and improve the probability that the project ends successfully. We explain these techniques and suggest how we can adapt them to help identify and quantify requirements-related risks.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1309651,no,undetermined,0
Model-driven reverse engineering,"Reverse engineering is the process of comprehending software and producing a model of it at a high abstraction level, suitable for documentation, maintenance, or reengineering. But from a manager's viewpoint, there are two painful problems: 1) It's difficult or impossible to predict how much time reverse engineering will require. 2) There are no standards to evaluate the quality of the reverse engineering that the maintenance staff performs. Model-driven reverse engineering can overcome these difficulties. A model is a high-level representation of some aspect of a software system. MDRE uses the features of modeling technology but applies them differently to address the maintenance manager's problems. Our approach to MDRE uses formal specification and automatic code generation to reverse the reverse-engineering process. Models written in a formal specification language called SLANG describe both the application domain and the program being reverse engineered, and interpretations annotate the connections between the two. The ability to generate a similar version of a program gives managers a fixed target for reverse engineering. This, in turn, enables better effort prediction and quality evaluation, reducing development risk.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1309646,no,undetermined,0
The use of unified APC/FD in the control of a metal etch area,"An adaptive neural network-based advanced process control software, the Dynamic Neural ControllerTM (DNC), was employed at National Semiconductor's 200 mm fabrication facility, South Portland, Maine, to enhance the performance of metal etch tools. The installation was performed on 5 identical LAM 9600 TCP Metal etchers running production material. The DNC produced a single predictive model on critical outputs and metrology for each tool based on process variables, maintenance, input metrology and output metrology. Although process metrology is usually measured on only one wafer per lot, the process can be closely monitored on a wafer-by-wafer basis with the DNC models. The DNC was able to provide recommendations for maintenance (replacing components in advance of predicted failure) and process variable adjustments (e.g. gas flow) to maximize tool up time and to reduce scrap. This enabled the equipment engineers to both debug problems more quickly on the tool and to make adjustments to tool parameters before out-of-spec wafers were produced. After a comparison of the performance of all 5 tools for a 2-month period prior to DNC installation vs. a 2-month post-DNC period, we concluded that the software was able to predict when maintenance actions were required, when process changes were required, and when maintenance actions were being taken but were not required. We observed a significant improvement in process C<sub>pk</sub>s for the metal etchers in this study.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1309573,no,undetermined,0
Enforcing system-wide properties,"Policy enforcement is a mechanism for ensuring that system components follow certain programming practices, comply with specified rules, and meet certain assumptions. Unfortunately, the most common mechanisms used today for policy enforcement are documentation, training, and code reviews. The fundamental problem is that these mechanisms are expensive, time-consuming, and still error-prone. To cope with this problem, we present IRC (Implementation Restriction Checker), an extensible framework for automatically enforcing system-wide policies or contracts. The framework is built on top of a platform for aspect-oriented programming at the level of Java byte-code instructions and is available as an eclipse plug-in as well as a standalone application. It includes a set of directly usable checkers and can be easily extended to implement new ones.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1290468,no,undetermined,0
Tolerating late memory traps in dynamically scheduled processors,"In the past few years, exception support for memory functions such as virtual memory, informing memory operations, software assist for shared memory protocols, or interactions with processors in memory has been advocated in various research papers. These memory traps may occur on a miss in the cache hierarchy or on a local or remote memory access. However, contemporary, dynamically scheduled processors only support memory exceptions detected in the TLB associated with the first-level cache. They do not support memory exceptions taken deep in the memory hierarchy. In this case, memory traps may be late, in the sense that the exception condition may still be undecided when a long-latency memory instruction reaches the retirement stage. In this paper we evaluate through simulation the overhead of memory traps in dynamically scheduled processors, focusing on the added overhead incurred when a memory trap is late. We also propose some simple mechanisms to reduce this added overhead while preserving the memory consistency model. With more aggressive memory access mechanisms in the processor we observe that the overhead of all memory traps - either early or late - is increased while the lateness of a trap becomes largely tolerated so that the performance gap between early and late memory traps is greatly reduced. Additionally, because of caching effects in the memory hierarchy, the frequency of memory traps usually decreases as they are taken deeper in the memory hierarchy and their overall impact on execution times becomes negligible. We conclude that support for memory traps taken throughout the memory hierarchy could be added to dynamically scheduled processors at low hardware cost and little performance degradation.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1288548,no,undetermined,0
A cost-benefit stopping criterion for statistical testing,"Determining when to stop a statistical test is an important management decision. Several stopping criteria have been proposed, including criteria based on statistical similarity, the probability that the system has a desired reliability, and the expected cost of remaining faults. This paper proposes a new stopping criterion based on a cost-benefit analysis using the expected reliability of the system (as opposed to an estimate of the remaining faults). The expected reliability is used, along with other factors such as units deployed and expected use, to anticipate the number of failures in the field and the resulting anticipated cost of failures. Reductions in this number generated by increasing the reliability are balanced against the cost of further testing to determine when testing should be stopped.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265715,no,undetermined,0
The process of and the lessons learned from performance tuning of a product family software architecture for mobile phones,"Performance is an important nonfunctional quality attribute of a software system but not always is considered when a software is designed. Furthermore, software evolves and changes can negatively affect the performance. New requirements could introduce performance problems and the need for a different architecture design. Even if the architecture has been designed to be easy to extend and flexible enough to be modified to perform its function, a software component designed to be too general and flexible can slower the execution of the application. Performance tuning is a way to assess the characteristics of an existing software and highlight design flaws or inefficiencies. Periodical performance tuning inspections and architecture assessments can help to discover potential bottlenecks before it is too late especially when changes and requirements are added to the architecture design. In this paper a performance tuning experience of one Nokia product family architecture will be described. Assessing a product family architecture means also taking into account the performance of the entire line of products and optimizations must include or at least not penalize its members.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281429,no,undetermined,0
SRAT-distribution voltage sags and reliability assessment tool,"Interruptions to supply and sags of distribution system voltage are the main aspects causing customer complaints. There is a need for analysis of supply reliability and voltage sag to relate system performance with network structure and equipment design parameters. This analysis can also give prediction of voltage dips, as well as relating traditional reliability and momentary outage measures to the properties of protection systems and to network impedances. Existing reliability analysis software often requires substantial training, lacks automated facilities, and suffers from data availability. Thus it requires time-consuming manual intervention for the study of large networks. A user-friendly sag and reliability assessment tool (SRAT) has been developed based on existing impedance data, protection characteristics, and a model of failure probability. The new features included in SRAT are a) efficient reliability and sag assessments for a radial network with limited loops, b) reliability evaluation associated with realistic protection and restoration schemes, c) inclusion of momentary outages in the same model as permanent outage evaluation, d) evaluation of the sag transfer through meshed subtransmission network, and e) simplified probability distribution model determined from available faults records. Examples of the application of the tools to an Australian distribution network are used to illustrate the application of this model.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1278434,no,undetermined,0
Periodic partial validation: cost-effective source code validation process in cross-platform software development environment,"Enterprise software development typically involves cooperation among multiple entities. In a cross-platform software development environment, developers can categorize the source code of products into platform specific and platform generic components, so that common features can be deployed seamlessly across platforms. As the complexity of component and source code inter-dependency increases, build breakages occur more frequently, and the lack of an efficient detection mechanism often results in slow response with higher costs. We present a successful cost-effective method to automatically detect and identify such breakages. We deployed a centralized code validation and policing tool, and the results prove its effectiveness as an important quality assurance component in the software development process.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276597,no,undetermined,0
Dependability analysis of a class of probabilistic Petri nets,"Verification of various properties associated with concurrent/distributed systems is critical in the process of designing and analyzing dependable systems. While techniques for the automatic verification of finite-state systems are relatively well studied, one of the main challenges in the domain of verification is concerned with the development of new techniques capable of coping with problems beyond the finite state framework. We investigate a number of problems closely related to dependability analysis in the context of probabilistic infinite-state systems modelled by probabilistic conflict-free Petri nets. Using a valuation method, we are able to demonstrate effective procedures for solving the termination with probability 1, the self-stabilization with probability 1, and the controllability with probability 1 problems in a unified framework.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276593,no,undetermined,0
Protecting wavelet lifting transforms,"Wavelet transforms are the central to many applications in image processing and data compression. They have banks of multirate filters that are difficult to protect from computer-induced numerical errors. An efficient algorithm-based fault tolerance approach is proposed for detecting arithmetic errors in the output data. Concurrent weighted parity values are designed to detect the effects of a single numerical error within the transform structure. The parity calculations use weighted sums of data, where the input parity weighting is related to the weighting used on the output data. Each parity computation is properly viewed as an inner product between weighting values and the data motivating the use of dual space functionals related to the error gain matrices that describe error propagations to the output. The parity weighting values are defined by a combination of dual space functionals. An iterative procedure for evaluating the design of the parity weights has been incorporated in Matlab code.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276573,no,undetermined,0
Safety testing of safety critical software based on critical mission duration,"To assess the safety of software based safety critical systems, we firstly analyzed the differences between reliability and safety, then, introduced a safety model based on three-state Markov model and some safety-related metrics. For safety critical software it is common to demand that all known faults are removed. Thus an operational test for safety critical software takes the form of a specified number of test cases (or a specified critical mission duration) that must be executed unsafe-failure-free. When the previous test has been early terminated as a result of an unsafe failure, it has been proposed that the further test need to be more stringent (i.e. the number of tests that must be executed unsafe-failure-free should increase). In order to solve the problem, a safety testing method based on critical mission duration and Bayesian testing stopping rules is proposed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276557,no,undetermined,0
Towards dependable Web services,"Web services are the key technology for implementing distributed enterprise level applications such as B2B and grid computing. An important goal is to provide dependable quality guarantees for client-server interactions. Therefore, service level management (SLM) is gaining more and more significance for clients and providers of Web services. The first step to control service level agreements is a proper instrumentation of the application code in order to monitor the service performance. However, manual instrumentation of Web services is very costly and error-prone and thus not very efficient. Our goal was to develop a systematic and automated, tool-supported approach for Web services instrumentation. We present a dual approach for efficiently instrumenting Web services. It consists of instrumenting the frontend Web services platform as well as the backend services. Although the instrumentation of the Web services platform necessarily is platform-specific, we have found a general, reusable approach. On the backend-side aspect-oriented programming techniques are successfully applied to instrument backend services. We present experimental studies of performance instrumentation using the application response measurement (ARM) API and evaluate the efficiency of the monitoring enhancements. Our results point the way to systematically gain better insights into the behaviour of Web services and thus how to build more dependable Web-based applications.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1276547,no,undetermined,0
A Strategy for Automatic Conformance Testing in Embedded Systems,"Software testing is an expensive and time-consuming activity; it is also error-prone due to human factors. But, it still is the most common effort used in the software industry to achieve an acceptable level of quality for its products. An alternative is to use formal verification approaches, although they are not widespread in industry yet. This paper proposes an automatic verification approach to aid system testing based on refinement checking, where the underlying formalisms are hidden from the developers. Our approach consists in using a controlled natural language (a subset of English) to describe requirements (where it is automatically translated into the formal specification language CSP) and extracting a model directly from a mobile phone using a developed tool support; these artifacts are normalized in the same abstraction level and compared using the refinement checker FDR. This approach is being used at Motorola; the source of our case study.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4653860,no,undetermined,0
A Study of Analogy Based Sampling for interval based cost estimation for software project management,"Software cost estimation is one of the most challenging activities in software project management. Since the software cost estimation affects almost all activities of software project development such as: biding, planning, and budgeting, the accurate estimation is very crucial to the success of software project management. However, due to the inherent uncertainties in the estimation process and other factors, the accurate estimates are often obtained with great difficulties. Therefore, it is safer to generate interval based estimates with a certain probability over them. In the literature, many approaches have been proposed for interval estimation. In this study, we propose a navel method namely Analogy Based Sampling (ABS) and compare ABS against the well established Bootstrapped Analogy Based Estimation (BABE) which is the only existing variant of analogy based method with the capability to generate interval predictions. The results and comparisons show that ABS could improve the performance of BABE with much higher efficiencies and more accurate interval predictions.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4654377,no,undetermined,0
Stereo analysis by hybrid recursive matching for real-time immersive video conferencing,"Real-time stereo analysis is an important research area in computer vision. In this context, we propose a stereo algorithm for an immersive video-conferencing system by which conferees at different geographical places can meet under similar conditions as in the real world. For this purpose, virtual views of the remote conferees are generated and adapted to the current viewpoint of the local participant. Dense vector fields of high accuracy are required in order to guarantee an adequate quality of the virtual views. Due to the usage of a wide baseline system with strongly convergent camera configurations, the dynamic disparity range is about 150 pixels. Considering computational costs, a full search or even a local search restricted to a small window of a few pixels, as it is implemented in many real-time algorithms, is not suitable for our application because processing on full-resolution video according to CCIR 601 TV standard with 25 frames per second is addressed-the most desirable as a pure software solution running on available processors without any support from dedicated hardware. Therefore, we propose in this paper a new fast algorithm for stereo analysis, which circumvents the window search by using a hybrid recursive matching strategy based on the effective selection of a small number of candidates. However, stereo analysis requires more than a straightforward application of stereo matching. The crucial problem is to produce accurate stereo correspondences in all parts of the image. Especially, errors in occluded regions and homogenous or less structured regions lead to disturbing artifacts in the synthesized virtual views. To cope with this problem, mismatches have to be detected and substituted by a sophisticated interpolation and extrapolation scheme.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1273542,no,undetermined,0
Efficient monitoring to detect wireless channel failures for MPI programs,"In the last few years the use of wireless technology has increased by leaps and bounds and as a result powerful portable computers with wireless cards are viable nodes in parallel distributed computing. In this scenario it is natural to consider the possibility of frequent failures in the wireless channel. In MPI programs, such wireless network behavior is reflected as communication failure. Although the MPI standard does not handle failures, there are some projects that address this issue. To the best of our knowledge there is no previous work that presents a practical solution for fault-handling in MPI programs that run on wireless environments. We present a mechanism at the application level, that combined with wireless network monitoring software detects these failures and warns MPI applications to enable them to take appropriate action.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1271469,no,undetermined,0
Generating Version Convertors for Domain-Specific Languages,"Domain-specific languages (DSLs) improve programmer productivity by providing high-level abstractions for the development of applications in a particular domain. However,the smaller distance to the application domain entails more frequent changes to the language. As a result, existing DSL models need to be converted to the new version. Manual conversion is tedious and error prone.This paper presents an approach to support DSL evolution by generation of convertors between DSLs. By analyzing the differences between DSL meta-models, a mapping is reverse engineered which can be used to generate reengineering tools to automatically convert models between different versions of a DSL. The approach has been implemented for the Microsoft DSL Tools infrastructure in two tools called DSLCompare and ConverterGenerator. The approach has been evaluated by means of three case studies taken from the software development practice at the company Avanade.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4656410,no,undetermined,0
I2V Communication Driving Assistance System: On-Board Traffic Light Assistant,"Cooperative systems based on V2X wireless communications offer promising opportunities for automotive safety and traffic efficiency improvement. Under preventive safety, cooperative assistance systems increase in-vehicle integrated safety systems functionality, enlarging driver's time-space perception as well as the quality and reliability of the environment data, and therefore enhancing his response to incoming events. In this paper, an on-board driving assistance system that brings traffic light information inside the vehicle is presented. Making use of positioning and cooperative I2V communications technologies, this system predicts the forthcoming traffic light state and assists the driver by means of an intuitive graphical interface (HMI).",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4657269,no,undetermined,0
A generic RTOS model for real-time systems simulation with systemC,"The main difficulties in designing real-time systems are related to time constraints: if an action is performed too late, it is considered as a fault (with different levels of criticism). Designers need to use a solution that fully supports timing constraints and enables them to simulate early on the design process a real-time system. One of the main difficulties in designing HW/SW systems resides in studying the effect of serializing tasks on processors running a real-time operating system (RTOS). In this paper, we present a generic model of RTOS based on systemC. It allows assessing real-time performances and the influence of scheduling according to RTOS properties such as scheduling policy, context-switch time and scheduling latency.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1269211,no,undetermined,0
Nine-coded compression technique with application to reduced pin-count testing and flexible on-chip decompression,"This paper presents a new test data compression technique based on a compression code that uses exactly nine codewords. In spite of its simplicity, it provides significant reduction in test data volume and test application time. In addition, the decompression logic is very small and independent of the precomputed test data set. Our technique leaves many don't-care bits unchanged in the compressed test set, and these bits can be filled randomly to detect non-modeled faults. The proposed technique can be efficiently adopted for single- or multiple-scan chain designs to reduce test application time and pin requirement. Experimental results for ISCAS'89 benchmarks illustrate the flexibility and efficiency of the proposed technique.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1269072,no,undetermined,0
Quality improvement for adaptive deblocking filter in H.264/AVC System,"Blocking artifacts influences image quality most important factor in the low bit-rate. New generational video coding standard for H.264, the adaptive deblocking filter plays a very important role in order to detect and analyze real and artificially edges on coded block. This paper presents a new approach for the adaptive deblocking filter of the H.264/AVC in order to improve quality. Comparing with the standard algorithm, the experimental results demonstrate the improvement in both the objective and the subjective qualities, which can achieve the improvement about 0.25~0.35 dB PSNR in average compared with the original H.264/AVC reference software JM11.0.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4657879,no,undetermined,0
Unifying Models of Test Cases and Requirements,"In industry, due to market pressures, it is common that the system requirements are out of date or incomplete for certain parts of the system. Nevertheless, we can always find up to date test cases which implicitly complements the related requirements. Therefore, instead of simply using test cases to detect software failures, in this paper we present an approach to update requirements using test cases. To accomplish this, we first assume that both requirements and test cases are formally documented; we reuse previous works that provide such models automatically as CSP formal specifications. Thus, we formally define a merge operation using the operational semantics of CSP. Finally, we use part of a real case study to experience the proposed approach.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4653857,no,undetermined,0
Towards the definition of a maintainability model for Web applications,"The growing diffusion of Web-based services in many and different business domains has triggered the need for new Web applications (WAs). The pressing market demand imposes very short time for the development of new WAs, and frequent modifications for existing ones. Well-defined software processes and methodologies are rarely adopted both in the development and maintenance phases. As a consequence, WAs' quality usually degrades in terms of architecture, documentation, and maintainability. Major concerns regard the difficulties in estimating costs of maintenance interventions. Thus, a strong need for methods and models to assess the maintainability of existing WAs is growing more and more. In this paper we introduce a first proposal for a WA maintainability model; the model considers those peculiarities that makes a WA different from a traditional software system and a set of metrics allowing an estimate of the maintainability is identified. Results from some initial case studies to verify the effectiveness of the proposed model are presented in the paper.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281430,no,undetermined,0
Processing of abdominal ultrasound images using seed based region growing method,"There are many diseases relating to abdomen. Patients suffering by abdominal diseases will be experiencing chronic or acute abdominal pain or suspects of an abdominal mass. Abdomen has two major parts: liver and gallbladder. Gallbladder and liver diseases are very common not only in Malaysia but also all over the globe. Hundreds of patients die from such diseases every year. Doctors face difficulty in diagnosing the types of diseases and sometimes unnecessary measures like surgery has to be performed. An abdominal ultrasound image is a useful way of examining internal organs, including the liver, gallbladder, spleen and kidneys. Ultrasound is safe, radiation free, faster and cheaper. Ultrasound images themselves will not give a clear view of an affected region. In general raw ultrasound images contains lot of imbedded noises. So digital processing can improve the quality of raw ultrasound images. In this work a software tool called ultrasound processing tool (UPT) has been developed by employing the histogram equalization and region growing approach to give a clearer view of the affected regions in the abdomen. The system was tested on more than 20 cases. Here, the results of two cases are presented, one on gallbladder mass and another on liver cancer. The radiologists have reported that original ultrasound images were not at all clear enough to detect the shape and area of the affected regions and the ultrasound processing tool has provided them clear and better view of the internal details of the diseases.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1287624,no,undetermined,0
Enhancing real-time CORBA via real-time Java features,"End-to-end middleware predictability is essential to support quality of service (QoS) capabilities needed by distributed real-time and embedded (DRE) applications. Real-time CORBA is a middleware standard that allows DRE applications to allocate, schedule, and control the QoS of CPU, memory, and networking resources. Existing real-time CORBA solutions are implemented in C++, which is generally more complicated and error-prone to program than Java. The real-time specification for Java (RTSJ) provides extensions that enable Java to be used for developing DRE systems. Real-time CORBA does not currently leverage key RTSJ features, such as scoped memory and real-time threads. Thus, integration of real-time CORBA and RTSJ is essential to ensure the predictability required for Java-based DRE applications. We provide the following contributions to the study of middleware for DRE applications. First we analyze the architecture of ZEN, our implementation of real-time CORBA, identifying sources for the application of RTSJ features. Second, we describe how RTSJ features, such as scoped memory and real-time threads, can be associated with key ORB components to enhance the predictability of DRE applications using realtime CORBA and the RTSJ. Third, we perform preliminary qualitative and quantitative analysis of predictability enhancements arising from our application of RTSJ features. Our results show that use of RTSJ features can considerably improve the predictability of DRE applications written using Real-time CORBA and real-time Java.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281569,no,undetermined,0
ISP-operated protection of home networks with FIDRAN,"In order to fight against the increasing number of network security incidents due to mal-protected home networks permanently connected to the Internet via DSL, TV cable or similar technologies, we propose that Internet service providers (ISP) operate and manage intrusion prevention systems (IPS) which are to a large extend executed on the consumer's gateway to the Internet (e.g., DSL router). The paper analyses the requirements of ISP-operated intrusion prevention systems and presents our approach for an IPS that runs on top of an active networking environment and is automatically configured by a vulnerability scanner. We call the system FIDRAN (Flexible Intrusion Detection and Response framework for Active Networks). The system autonomously analyses the home network and correspondingly configures the IPS. Furthermore, our system detects and adjusts itself to changes in the home network (new service, new host, etc.). First performance comparisons show that our approach - while offering more flexibility and being able to support continuous updating by active networking principles - competes well with the performance of conventional intrusion prevention systems like Snort-Inline.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1286830,no,undetermined,0
The challenge of space nuclear propulsion and power systems reliability,"In October of 2002, The Power and Propulsion Office and The Risk Management Office of NASA Glenn Research Center in Cleveland, Ohio began developing the reliability, availability, and maintainability (RAM) engineering approach for the Space Nuclear Propulsion and Power Systems Project. The objective of the Space Nuclear Power and Propulsion Project is to provide safe and reliable propulsion and power systems for planetary missions. The safety of the crew, ground personnel, and the public has to be the highest priority of the RAM engineering approach for nuclear powered space systems. The project will require a top level reliability goal for substantial mission success in the range from 0.95 to 0.98. In addition, the probability of safe operation without loss of crew, vehicle, or danger to the public, cannot be less than 0.9999. The achievement of these operational goals will require the combined application of many RAM engineering techniques. These include: advanced reliability, availability, and maintainability analysis, probabilistic risk assessment that includes hardware, software, and human induced faults, accelerated life testing, parts stress analysis, and selective end to end sub-system testing. Design strategy must involve the selection of parts and materials specifically to withstand the stresses of prolonged operation in the space and planetary environments with a wide design margin. Interplanetary distances and resulting signal time delay drive the need for autonomous control of major system functions including redundancy management.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285487,no,undetermined,0
Cleman: Comprehensive Clone Group Evolution Management,"Recent research results have shown more benefits of the management of code clones, rather than detecting and removing them. However, existing management approaches for code clone group evolution are still ad hoc, unsatisfactory, and limited. In this paper, we introduce a novel method for comprehensive code clone group management in evolving software. The core of our method is Cleman, an algorithmic framework that allows for a systematic construction of efficient and accurate clone group management tools. Clone group management is rigorously formulated by a formal model, which provides the foundation for Cleman framework. We use Cleman framework to build a clone group management tool that is able to detect high-quality clone groups and efficiently manage them when the software evolves. We also conduct an empirical evaluation on real-world systems to show the flexibility of Cleman framework and the efficiency, completeness, and incremental updatability of our tool.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4639364,no,undetermined,0
A Comprehensive Ontology-Based Approach for SLA Obligations Monitoring,"Specifying clear quality of service (QoS) agreements between service providers and consumers is particularly important for the successful deployment of service-oriented architectures. The related challenges include correctly elaborating and monitoring QoS contracts (SLA: service level agreement) to detect and handle their violations. In this paper, first, we study and analyze existing SLA-related models. Then, we elaborate a complete, generic and semantically richer ontology-based model of SLA. We used the Semantic Web Rule Language (SWRL) to express SLA obligations in our model. This language facilitates the SLA monitoring process and the eventual action triggering in case of violations. We used this model to automatically generate semantic-enabled QoS obligations monitors. We have also developed a prototype to validate our model and our monitoring approach. Finally, we believe that this work is a step ahead to the total automation of the SLA management process.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641021,no,undetermined,0
Reliability and robustness assessment of diagnostic systems from warranty data,"Diagnostic systems are software-intensive built-in-test systems, which detect, isolate and indicate the failures of prime systems. The use of diagnostic systems reduces the losses due to the failures of prime systems and facilitates the subsequent correct repairs. Therefore, they have found extensive applications in industry. Without loss of generality, this paper utilizes the on-board diagnostic systems of automobiles as an illustrative example. A failed diagnostic system generates  or .  error incurs unnecessary warranty costs to manufacturers, while  error causes potential losses to customers. Therefore, the reliability and robustness of diagnostic systems are important to both manufacturers and customers. This paper presents a method for assessing the reliability and robustness of the diagnostic systems by using warranty data. We present the definitions of robustness and reliability of the diagnostic systems, and the formulae for estimating ,  and reliability. To utilize warranty data for assessment, we describe the two-dimensional (time-in-service and mileage) warranty censoring mechanism, model the reliability function of the prime systems, and devise warranty data mining strategies. The impact of  error on warranty cost is evaluated. Fault tree analyses for  and  errors are performed to identify the ways for reliability and robustness improvement. The method is applied to assess the reliability and robustness of an automobile on-board diagnostic system.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285438,no,undetermined,0
Open design of networked power quality monitoring systems,Permanent continuous power quality monitoring is beginning to be recognized as an important aid for managing power quality. Preventive maintenance can only be initiated if such monitoring is available to detect the minor disturbances that may precede major disruptions. This paper establishes the need to encourage interoperability between power quality instruments from different vendors. It discusses the frequent problem of incompatibility between equipment that results from the inherent inflexibilities in existing designs. A new approach has been proposed to enhance interoperability through the use of open systems in their design. It is demonstrated that it is possible to achieve such open design using existing software and networking technologies. The benefits and disadvantages to both the end-users and the equipment manufacturers are also being discussed.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1284897,no,undetermined,0
A framework of software rejuvenation for survivability,"We propose a novel approach of the security issue to survivability. The main objectives are to detect the attacks in real time, to characterize the attacks, and to survive in face of the attacks. To counteract the attacks' attempts, we perform the software rejuvenation methods (SWRMS) such as killing the intruders' processes in their tracks, halting abuse before it happens, shutting down unauthorized connection, and responding and restarting in real time. These slogans will really frustrate and deter the attacks, as the attackers can't make their progress. This is a way of survivability to maximize the deterrence against the attacks in the target environment. We address a framework to model and analyze the critical intrusion tolerance problems ahead of intrusion detection and we present a set of innovative models to solve the security aging problems.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1283854,no,undetermined,0
Adding assurance to automatically generated code,"Code to estimate position and attitude of a spacecraft or aircraft belongs to the most safety-critical parts of flight software. The complex underlying mathematics and abundance of design details make it error-prone and reliable implementations costly. AutoFilter is a program synthesis tool for the automatic generation of state estimation code from compact specifications. It can automatically produce additional safety certificates which formally guarantee that each generated program individually satisfies a set of important safety policies. These safety policies (e.g., array-bounds, variable initialization) form a core of properties which are essential for high-assurance software. Here we describe the AutoFilter system and its certificate generator and compare our approach to the static analysis tool PolySpace.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281768,no,undetermined,0
Automated detection of injected faults in a differential equation solver,"Analysis of logical relationships between inputs and outputs of a computational system can significantly reduce the test execution effort via minimizing the number of required test cases. Unfortunately, the available specification documents are often insufficient to build a complete and reliable model of the tested system. In this paper, we demonstrate the use of a data mining method, called Info-Fuzzy Network (IFN), which can automatically induce logical dependencies from execution data of a stable software version, construct a set of non-redundant test cases, and identify faulty outcomes in new, potentially faulty releases of the same system. The proposed approach is applied to the Unstructured Mesh Finite Element Solver (UMFES) which is a general finite element program for solving 2D elliptic partial differential equations. Experimental results demonstrate the capability of the IFN-based testing methodology to detect several kinds of faults injected in the code of this sophisticated application.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281751,no,undetermined,0
Efficient modeling of a combined overhead-cable line for grounding-system analysis,"Simple compact models for combined overhead-cable lines supplying a substation are presented as an extension of a previous paper for grounding system analysis. The overhead line section can be equipped with uniform or combined ground wires, whereas the cable line section can consist of coated metal sheathed cables, with/without intermediate grounding, or uncoated metal sheathed cables in continuous contact with the earth. Besides the calculation of the earth current at the faulted substation, the proposed modeling method allows the evaluation of the leakage current at the transition station, where cables are connected to the overhead line, as well as at critical overhead line towers. In this manner, the effects of the so called ldquofault application transferrdquo phenomenon can be conveniently estimated at the design stage in order to assess the most appropriate safety conditions. Some numerical examples are given by applying a computer program based on the proposed methodology.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641721,no,undetermined,0
Differential protection of three-phase transformers using Wavelet Transforms,"This paper proposes a novel formulation for differential protection of three-phase transformers using Wavelet Transforms (WTs). The new proposed methodology implements the WTs to extract predominant transient signals originated by transformer internal faults and captured from the current transformers. The Wavelet Transform is an efficient signal processing tool used to study non stationary signals with fast transition (high frequency components), mapping the signal in time-frequency representation. The three phase differential currents are the input signals used on-line to detect internal faults. The performance of this algorithm is demonstrated through simulation of different internal faults and switching conditions on a power transformer using ATP/EMTP software. The analyzed data is obtained from simulation of different normal and faulty operating conditions such as internal faults - phase/phase, phase/ground-, magnetizing inrush and external faults. The case study shows that the new algorithm is highly accurate and effective.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641818,no,undetermined,0
Modeling and control of grid-connected photovoltaic energy conversion system used as a dispersed generator,"This paper proposes a detailed mathematical model and a multi-level control scheme of a three-phase grid-connected photovoltaic (PV) system used as a dispersed generator, including the PV array and the electronic power conditioning (PCS) system, based on the Matlab/Simulink software. The model of the PV array proposed uses theoretical and empirical equations together with data provided by the manufacturer, solar radiation and cell temperature among others variables, in order to accurately predict the current-voltage curve. The PCS utilizes a two-stage energy conversion system topology that meets all the requirement of high quality electric power, flexibility and reliability imposed for applications of modern distributed energy resources (DER). The control approach incorporates a maximum power point tracker (MPPT) for dynamic active power generation jointly with reactive power compensation of the distribution power system. Validation of simulation results has been carried out by using a 250 Wp PV experimental set-up.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641871,no,undetermined,0
An approach for designing and assessing detectors for dependable component-based systems,"In this paper, we present an approach that helps in the design and assessment of detectors. A detector is a program component that asserts the validity of a predicate in a given program state. We first develop a theory of error detection, and identify two main properties of detectors, namely completeness and accuracy. Given the complexity of designing efficient detectors, we introduce two metrics, namely completeness (C) and inaccuracy (I), that capture the operational effectiveness of detector operations, and each metric captures one efficiency aspect of the detector. Subsequently, we present an approach for experimentally evaluating these metrics, and is based on fault-injection. The metrics developed in our approach also allow a system designer to perform a cost-benefit analysis for resource allocation when designing efficient detectors for fault-tolerant systems. The applicability of our approach is suited for the design of reliable component-based systems.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281731,no,undetermined,0
Knowledge-centric and language independent framework for safety analysis tools,"This paper presents a knowledge-centric and language independent framework and its application to develop safety analysis tools for avionics systems. A knowledge-centric approach is important to address domain-specific needs, with respect to the types of problems the tools detect and the strategies used to analyze and adapt the code. The knowledge is captured by formally specified patterns used to detect a variety of problems, ranging from simple syntactic issues to difficult semantic problems requiring global analysis. Patterns can also be used to describe transformations of the software, used to rectify problems detected through software inspection, and to support interactive inspection and adaptation when full automation is impractical. This paper describes the Knowledge Centric Software (KCS) framework. It focuses on two key aspects: an eXtensible Common Intermediate Language (XCIL) for language independent analysis, and an eXtensible Pattern Specification Language (XPSL) for representing domain-specific knowledge.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1281729,no,undetermined,0
Coal Management Module (CMM) for power plant,"Importance of coal management in power plant is very much significant and also one of the most critical areas in view of plant operation as well as cost involvement, so it forms an important part of the management process in a power plant. It deals with the management of commercial, operational and administrative functions pertaining to estimating coal requirements, selection of coal suppliers, coal quality check, transportation and coal handling, payment for coal received, consumption and calculation of coal efficiency. The results are then used for cost benefit analysis to suggest further plant improvement. At various levels, management information reports need to be extracted to communicate the required information across various levels of management. The core processes of coal management involve a huge amount of paper work and manual labour, which makes it tedious, time-consuming and prone to human errors. Moreover, the time taken at each stage as well as the transparency of the relevant information has a direct bearing on the economics and efficient operation of the power plant. Both system performance and information transparency can be enhanced by the introduction of Information Technology in managing this area. This paper reports on the design & development of Coal Management Module (CMM) Software, which aims at systematic functioning of the Core Business Processes of Coal Management of a typical coal-fired power plant.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4651474,no,undetermined,0
A Compression Framework for Personal Image Used in Mobile RFID System,"Radio frequency identification (RFID), a novel automatic identification technology, has been widely used in modern society. To improve the security of RFID card, a novel idea of inserting personal image to the card and restoring it rapidly on the mobile device is proposed in this paper. A compression framework based on facial feature is proposed to solve the key problem caused by memory limitation of RFID card and resource limitations of the mobile system. In this framework, the facial region is detected and extracted at first, and then compressed with high compression ratio using fast lifting wavelet transform. After that, the compressed data is encoded and saved in the card. Experimental results indicate that higher compression ratio, better image quality and rapid decompression can be achieved by using this framework.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4709071,no,undetermined,0
Error resilient macroblock rate control for H.264/AVC video coding,"In this paper, an error resilient rate control scheme for the H.264/AVC standard is proposed. This scheme differs from traditional rate control schemes in that macroblock mode decisions are not made only to minimize their rate-distortion cost, but also take into account that the bitstream will have to be transmitted through an error-prone network. Since channel errors will probably occur, error propagation due to predictive coding should be mitigated by adequate Intra coding refreshes. The proposed scheme works by comparing the rate-distortion cost of coding a macroblock in Intra and Inter modes: if the cost of Intra coding is only slightly larger than the cost of Inter coding, the coding mode is changed to Intra, thus reducing error propagation. Additionally, cyclic Intra refresh is also applied to guarantee that all macroblocks are eventually refreshed. The proposed scheme outperforms the H.264/AVC reference software, for typical test sequences, for error-free transmission and several packet loss rates.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4712209,no,undetermined,0
Reaction to errors in robot systems,"The paper analyzes the problem of error (failure) detection and handling in robot programming. First an overview of the subject is provided and later error detection and handling in MRROC++ are described. To facilitate system reaction to the detected failures, the errors are classified and certain suggestions are made as to how to handle those classes of errors.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1177108,no,undetermined,0
Intelligent Fault Diagnosis System in Large Industrial Networks,"Traditional fault diagnosis system in large industrial networks is not intelligent enough and cannot predict faults. It is too expensive for industrial corporations. This paper brings forward an intelligent fault diagnosis system-IFDS, which uses new types of intelligent database technology, and has the ability of effectively solving the fault diagnosis and prediction issue of current industrial Ethernet network. In addition, this paper discusses some methods which can be implemented in IBM DB2 database.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4770033,no,undetermined,0
Separating recovery strategies from application functionality: experiences with a framework approach,"Industry-oriented fault tolerance solutions for embedded distributed systems should be based on adaptable, reusable elements. Software-implemented fault tolerance can provide such flexibility via the presented framework approach. It consists of (1) a library of fault tolerance functions, (2) a backbone coordinating these functions, and (3) a language expressing configuration and recovery. This language is a sort of ancillary application layer, separating recovery aspects from functional ones. Such a framework approach allows for a flexible combination of the available hardware redundancy with software-implemented fault tolerance. This increases the availability and reliability of the application at a justifiable cost thanks to the re-usability of the library elements in different targets systems. It also increases the maintainability due to the separation of the functional behavior from the recovery strategies that are executed when an error is detected as the modifications to functional and nonfunctional behavior are, to some extent, independent and hence less complex. Practical experience is reported from the integration of this framework approach in an automation system for electricity distribution. This case study illustrates the power of software-based fault tolerance solutions and of the configuration-and-recovery language ARIEL to allow flexibility and adaptability to changes in the environment",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902475,no,undetermined,0
System reliability analysis: the advantages of using analytical methods to analyze non-repairable systems,"Most of the system analysis software available on the market today employs the use of simulation methods for estimating the reliability of nonrepairable systems. Even though simulation methods are easy to apply and offer great versatility in modeling and analyzing complex systems, there are some limitations to their effectiveness. For example, if the number of simulations performed is not large enough, these methods can be error prone. In addition, performing a large number of simulations can be extremely time-consuming and simulation offers a small range of calculation results when compared to analytical methods. Analytical methods have been avoided due to their complexity in favor of the simplicity of using simulation. A software tool has been developed that calculates the exact analytical solution for the reliability of a system. Given the reliability equation for the system, further analyses on the system can be performed, such as computing exact values of the reliability, failure rate, at specific points in time, as well as computing the system MTTF (mean time to failure), and reliability importance measures for the components of the system. In addition, optimization and reliability allocation techniques can be utilized to aid engineers in their design improvement efforts. Finally, the time-consuming calculations and the non-repeatability issue of the simulation methodology are eliminated",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902446,no,undetermined,0
Does code decay? Assessing the evidence from change management data,"A central feature of the evolution of large software systems is that change-which is necessary to add new functionality, accommodate new hardware, and repair faults-becomes increasingly difficult over time. We approach this phenomenon, which we term code decay, scientifically and statistically. We define code decay and propose a number of measurements (code decay indices) on software and on the organizations that produce it, that serve as symptoms, risk factors, and predictors of decay. Using an unusually rich data set (the fifteen-plus year change history of the millions of lines of software for a telephone switching system), we find mixed, but on the whole persuasive, statistical evidence of code decay, which is corroborated by developers of the code. Suggestive indications that perfective maintenance can retard code decay are also discussed",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895984,no,undetermined,0
A Short-Circuit Current Study for the Power Supply System of Taiwan Railway,"The western Taiwan railway transportation system consists mainly on a mountain route and ocean route. Taiwan Railway Administration (TRA) has conducted a series of experiments on the ocean route in recent years to identify the possible causes of unknown events that cause the trolley contact wires to melt down frequently. The conducted tests include the short-circuit fault test within the power supply zone of the Ho Long Substation (Zhu Nan to Tong Xiao) that had the highest probability for the melt down events. Those test results, based on the actual measured maximum short-circuit current, provide a valuable reference for TRA when comparing against the said events. The Le Blanc transformer is the main transformer of the Taiwan railway electrification system. The Le Blanc transformer mainly transforms the Taiwan Power Company (TPC) generated three-phase alternating power supply system (69kV, 60Hz) into two single-phase alternating power distribution systems (M phase and T phase) (26kV, 60Hz) needed for the trolley traction. As a unique winding connection transformer, the conventional software for fault analysis will not be able to simulate its internal current and phase difference between each phase current. Therefore, besides extracts of the short-circuit test results, this work presents an EMTP model based on the Taiwan Railway Substation equivalent circuit model with a Le Blanc transformer. The proposed circuit model can simulate the same short-circuit test to verify the actual fault current and accuracy of the equivalent circuit model.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4311565,no,undetermined,0
Finite Element Analysis of Internal Winding Faults in Distribution Transformers,"With the appearance of deregulation, distribution transformer predictive maintenance is becoming more important for utilities to prevent forced outages with the consequential costs. To detect and diagnose a transformer internal fault requires a transformer model to simulate these faults. This paper presents finite element analysis of internal winding faults in a distribution transformer. The transformer with a turn-to-earth fault or a turn-to-turn fault is modeled using coupled electromagnetic and structural finite elements. The terminal behaviors of the transformer are studied by an indirect coupling of the finite element method and circuit simulation. The procedure was realized using a commercially available software. The normal case and various faulty cases were simulated and the terminal behaviors of the transformer were studied and compared with field experimental results. The comparison results validate the finite element model to simulate internal faults in a distribution transformer.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4311335,no,undetermined,0
StegoHunter: Passive audio steganalysis using Audio Quality Metrics and its realization through genetic search and X-means approach,"Steganography is used to hide the occurrence of communication. This creates a potential problem when this technology is misused for planning criminal activities. Differentiating anomalous audio document (stego audio) from pure audio document (cover audio) is difficult and tedious. This paper investigates the use of a Genetic-X-means classifier, which distinguishes a pure audio document from the adulterated one. The basic idea is that, the various audio quality metrics (AQMs) calculated on cover audio signals and on stego-audio signals vis-a-vis their denoised versions, are statistically different. Our model employs these AQMs to steganalyse the audio data. Genetic paradigm is exploited to select the AQMs that are sensitive to various embedding techniques. The classifier between cover and stego-files is built using X-means clustering on the selected feature set. The presented method can not only detect the presence of hidden message but also identify the hiding domains. The experimental results show that the combination strategy (Genetic-X-means) can improve the classification precision even with lesser payload compared to the traditional ANN (Back Propagation Network).",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766490,no,undetermined,0
Modeling SPECT acquisition and processing of changing radiopharmaceutical distributions,"The accuracy of SPECT images is compromised and artifacts may be produced when the radiopharmaceutical distribution changes during image acquisition. Optimization of SPECT acquisition protocols for changing tracer distributions can be difficult not only in patient studies (undesirability of performing repeat studies on the same patient) but also in phantom studies (difficulty of emulating the changing distributions). This study proposes a simulation that allows computer modeling of both tracer kinetics and different acquisition schemes. <sup>99m</sup>Tc Teboroxime (Bracco Diagnostics) is used as a model. SPECT acquisition of a software phantom (NCAT, UNC Chapel Hill) is simulated with photon attenuation, collimator resolution, Compton scatter, Poisson noise, and changing tracer distribution. Short-axis uniformity is used to assess the severity of artifacts in the myocardium. The simulation produces similar artifacts to those found in patient studies with <sup>99m</sup>Tc Teboroxime. This simulation methodology can provide a valuable tool for testing novel acquisition and processing techniques and to facilitate the optimization of SPECT images of changing tracer distributions. Summed fanning (back and forth) acquisitions have been tested and artifact reduced short-axis images obtained. Image restoration techniques are proposed to further improve the image quality. Furthermore, the simulated studies can be compared to the simulations with assigned low liver uptake and no tracer clearance from the myocardium to detect and resolve artifacts through variations in the acquisition and processing schemes.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008591,no,undetermined,0
Optimal decomposition for wavelet image compression,"The paper discusses important features of wavelet transform in compression of still images including the extent to which the quality of image is degraded by process of wavelet compression and decompression. A set of wavelet functions (wavelets) for implementation in a still image compression system is examined. The effects of different wavelet functions, image contents and compression ratios are assessed. The benefit of this transform relating to today's methods is stressed. Our results provide a good reference for application developers to choose a good wavelet compression system for their application",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914914,no,undetermined,0
"An integrated resource negotiation, pricing, and QoS adaptation framework for multimedia applications","We study a dynamic, usage- and congestion-dependent pricing system in conjunction with price-sensitive user adaptation of network usage. We first present a resource negotiation and pricing (RNAP) protocol and architecture to enable users to select and dynamically renegotiate network services. We develop mechanisms within the RNAP architecture for the network to dynamically formulate prices and communicate pricing and charging information to the users. We then outline a general pricing strategy in this context. We discuss candidate algorithms by which applications (singly, or as part of a multi-application system) can adapt their rate and QoS requests, based on the user-perceived value of a given combination of transmission parameters. Finally, we present experimental results to show that usage- and congestion-dependent pricing can effectively reduce the blocking probability, and allow bandwidth to be shared fairly among applications, depending on the elasticity of their respective bandwidth requirements.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=898734,no,undetermined,0
Virtual sensor for fault detection and isolation in flight control systems - fuzzy modeling approach,"A virtual sensor for normal acceleration has been developed and implemented in the flight control system of a small commercial aircraft. The inputs of the virtual sensor are the consolidated outputs of dissimilar sensor signals. The virtual sensor is a fuzzy model of the Takagi-Sugeno type and it has been identified from simulated data, using a detailed, realistic Matlab/Simulink<sup>TM</sup> model used by the aircraft manufacturer. This virtual sensor can be applied to identify a failed sensor in the case that only two real sensors are available and even to detect a failure of the last available sensor",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914204,no,undetermined,0
LACE frameworks and technique-identifying the legacy status of a business information system from the perspectives of its causes and effects,"This paper first presents a definition of the concept `legacy status' with a three-dimensional model. It then discusses LACE frameworks and techniques, which can be used to assess legacy status from the cause and effects perspectives. A method of applying the LACE frameworks is shown and a technique with a mathematical model and metric so that the legacy status of a system can be calculated. This paper describes a novel and practical way to identify legacy status of a system, and has pointed out a new direction for research in this area",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=913235,no,undetermined,0
Structural defects: general approach and application to textile inspection,"This paper addresses detection of imperfections in repetitive regular structures (textures). Humans can easily find such defects without prior knowledge of the `good' pattern. In this study, it is assumed that structural defects are detected as irregularities, that is, locations of lower regularity. We define pattern regularity features and find defects by robust detection of outliers in the feature space. Two tests are presented to assess the approach. In the first test, diverse texture patterns are processed individually and outliers are searched in each pattern. In the second test, classified defects in a group of textiles are considered. Defect-free patterns are used to learn distance thresholds that separate defects",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=905390,no,undetermined,0
Achievable QoS for multiple delay classes in cellular TDMA environments,"In a real-time wireless TDMA environment, every packet generated by applications has a deadline associated with it. If the system cannot allocate enough resources to serve the packet before the deadline, the packet would be dropped. Different applications have different delay requirements that should be guaranteed by the system so as to maintain some given packet dropping probabilities. In this paper, a single-cell system traffic of multiple delay classes is mathematically analyzed, and it is proved to be independent of the scheduling algorithm used, for all work-conserving earliest-due-date (WC-EDD) scheduling algorithms. The dropping requirements of all individual applications are guaranteed using deadline-sensitive ordered-head-of-line (DSO-HoL) priority schemes. Verification of the model is shown through extensive simulations",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=904788,no,undetermined,0
Mobile agents for personalized information retrieval: when are they a good idea?,Mobile agent technology has been proposed as an alternative to traditional client-server computing for personalized information retrieval by mobile and wireless users from fixed wired servers. We develop a very simplified analytical model that examines the claimed performance benefits of mobile agents over client-server computing for a mobile information retrieval scenario. Our evaluation of this simple model shows that mobile agents are not necessarily better than client-server calls in terms of average response times; they are only beneficial if the space overhead of the mobile agent code is not too large or if the wireless link connecting the mobile user to the fixed servers of the virtual enterprise is error-prone. We quantify the tradeoffs involved for a variety of scenarios and point out issues for further research,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=904635,no,undetermined,0
The application of mold flow simulation in electronic package,"The application of CAE in mold flow of IC packaging has been developed for years. However, to predict EMC flow behavior accurately in IC packages during transfer molding is still a huge challenge due to its intrinsic limitations. In this paper, modeling technologies to analyze mold flow during semiconductor encapsulation have been developed. The leadframe separates the whole molding cavity into top and bottom cavities. Cavity thickness is the most important factor to the mold flow behavior. Unbalanced flow, due to large thickness difference between top and bottom cavities, causes air trapping and die pad tilt. Some packages which have larger thickness difference, such as 1 to 3 thickness-ratio TSOP, LOC-TSOP, DHS, EDHS and DPH Q-series packages, have a seriously unbalanced melt-front during molding. By observing the flow phenomenon from short-shot samples, it is found that the cavity thickness, bonding wire density, the size of leadframe openings, and surface roughness all affect EMC flow behavior. By considering these factors into the construction of a simulation model, numerical results show excellent agreement with actual experimental results for a DPH-LQFP package. The melt-fronts of numerical and experimental results are compared and shown. Further investigation to improve the package moldability was also studied. By using CAE software, molding defects can be easily detected and moldability problems can be improved efficiently to reduce manufacturing cost and design cycle time",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=904175,no,undetermined,0
Wafer probe process verification tools,"We present some tools we have developed that ensure the good quality of the wafer probing, or wafer test, process. Most of the problems at Wafer Probe appear in the same way and by detecting their pattern, even not knowing the exact source of the problem, we can prevent the product and its yield from being affected. The most common patterns of failures are: A certain category failing consecutively, a certain test failing above statistical limits expected, based on the historical results of that product, same wafers yielding different in two different testers, and results in a lot going worse wafer by wafer. For addressing these issues, we have generated a set of programs that are run at the end of every wafer tested, in real time, and that generate alarms and tell actions to the operator when the above problems are detected",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902589,no,undetermined,0
"Code coverage, what does it mean in terms of quality?","Unit code test coverage has long been known to be an important metric for testing software, and many development groups require 85% coverage to achieve quality targets. Assume we have a test, T<sub>1</sub> which has 100% code coverage and it detects a set of defects, D<sub>1</sub>. The question, which is answered here, is """"What percentage of the defects in D<sub>1</sub> will be detected if a random subset of the tests in T<sub>1</sub> are applied to the code, which has code coverage of X% of the code?"""" The purpose of this paper is to show the relation between code quality and code coverage. The relationship is derived via a model of code defect levels. A sampling technique is employed and modeled with the hypergeometric distribution while assuming uniform probability and a random distribution of defects in the code, which invokes the binomial distribution. The result of this analysis is a simple relation between defect level and quality of the code delivered after the unit code is tested. This model results in the rethinking of the use of unit code test metrics and the use of support tools",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902502,no,undetermined,0
Probabilistic communication optimizations and parallelization for distributed-memory systems,"In high-performance systems execution time is of crucial importance justifying advanced optimization techniques. Traditionally, optimization is based on static program analysis. The quality of program optimizations, however, can be substantially improved by utilizing runtime information. Probabilistic data-flow frameworks compute the probability with what data-flow facts may hold at some program point based on representative profile runs. Advanced optimizations can use this information in order to produce highly efficient code. In this paper we introduce a novel optimization technique in the context of High Performance Fortran (HPF) that is based on probabilistic data-flow information. We consider statically undefined attributes which play an important role for parallelization and compute for those attributes the probabilities to hold some specific value during runtime. For the most probable attribute values highly-optimized, specialized code is generated. In this way significantly better performance results can be achieved. The implementation of our optimization is done in the context of VFC, a source-to-source parallelizing compiler for HPF/F90",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=905042,no,undetermined,0
Design and implementation of secure Web-based LDAP management system,"As the Internet grows quickly, more and more services are available. How to provide high quality, convenient, and personalized services to users are the important issues for Internet service providers to keep customers connected to their Web sites. The directory is an important part of Internet technology used to support such needs. It exists in a multitude of applications ranging from operating systems, asset management systems, security systems, etc. Furthermore, The Gartner Group, a market research firm, predicts that 40% to 90% of new software and hardware will be directory related products, at end of the period 2001 to 2003. In the directory industry, we can divide products into 3 fields: directory server, management system, and directory application. The management system is one of the important parts of directory services. The directory management system is focused on non-Web-based systems. While directory services are applied on Internet services, it is necessary to provide a Web-based management interface. This interface will provide the advantages of ubiquity, cross platform, thin client, and reduced TCO (total cost of ownership). We proposed and implemented a Web-based lightweight directory access protocol (LDAP) management architecture to provide such benefits and to manage multiple LDAP servers. We used the standard protocol and popular software of Internet technology usually used to build the system, so the system is easy to be ported and minimized the changes of the original system. In addition, we also considered the security factors while designing and constructing the system",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=905437,no,undetermined,0
TRAM: a tool for requirements and architecture management,"Management of system requirements and system architectures is part of any software engineering project. But it is usually very tedious and error prone. In particular, managing the traceability between system requirements and system architectures is critical but difficult. The article introduces a tool, TRAM, for managing system requirements, system architectures and more importantly the traceability between them. Its primary design objective is being practical and ready for practitioners to use without much overhead. The issues discussed in the paper include an information model that underlies the capture of requirements, architectures and their traceability, a set of document templates implementing the information model, and the support tool",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=906624,no,undetermined,0
Scalable and Accurate Application Signature Discovery,"Newly emerged applications are producing a large amount of traffic and connection in the Internets. And they are becoming increasingly difficult to detect. Signature based method are currently the approaches for discovering and detecting the patterns of application. However, these methods may confront their difficulty in validating the efficiency and quality of signatures for unknown applications. Therefore, how to generate the more accurate and representative patterns and validate the quality of signatures is a critical issue.In this paper, a new method has been proposed with a new structure to generate high quality signatures. Different from traditional methods, this one employs a signature learning mechanism that is designed to refine the signatures by merging the similar patterns to improve the signature quality. The experiment indicates that this method is efficient to generate accurate and robust signatures. And the quality of signatures is improved by signature learning.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756606,no,undetermined,0
An Efficient Local Bandwidth Management System for Supporting Video Streaming,"In order to guarantee continuous delivery of video streaming over best-effort (BE) forwarding network, some quality-of-service (QoS) strategies such as RSVP and DiffServ must be used to improve the transmission performance. However, these methods are too difficult to be employed in practical applications since their technical complexity. In this paper, we design and implement an efficient local bandwidth management system to tackle this problem in IPv6 environment. The system monitors local access network and provides assured forwarding (AF) service through controlling the video streaming requests based on available network bandwidth. To assess the benefit of this system, we perform tests to compare its performance with that of conventional BE service. Our test results indicate convincingly that AF offers substantially better performance than BE.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756767,no,undetermined,0
A controlled experiment to assess the effectiveness of inspection meetings,"Software inspection is one of the best practices for detecting and removing defects early in the software development process. In a software inspection, review is first performed individually and then by meeting as a team. In the last years, some empirical studies have shown that inspection meetings do not improve the effectiveness of the inspection process with respect to the number of true discovered defects. While group synergy allows inspectors to find some new defects, these meeting gains are offset by meeting losses, that is defects found by individuals but not reported as a team. We present a controlled experiment with more than one hundred undergraduate students who inspected software requirements documents as part of a university course. We compare the performance of nominal and real teams, and also investigate the reasons for meeting losses. Results show that nominal teams outperformed real teams, there were more meeting losses than meeting gains, and that most of the losses were defects found by only one individual in the inspection team",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915514,no,undetermined,0
Current trends in the design of automotive electronic systems,"Today's situation in this field is characterized by three distinct development phases: First, the analysis and design of functionality. This type of work is typically performed in the laboratory, i.e. on the desk. Second, the implementation of a prototype system, realized by (semi)automatic code generation and followed by a test with a Lab-car or in a real vehicle. The third and final step comprises the calibration and fine-tuning of algorithms and their parameters, commonly done in a real car. However, there are some flaws associated with this approach. There is no support for multiple interconnected electronic control units. Automatic generation of code of production quality is still a challenging task. And there is a large gap between the properties of a virtual car and the behavior of the real vehicle. The latter is one reason why nowadays the adjustment of calibration parameters still needs to be done manually. In the future, the picture outlined above will change remarkably. Function development tools will be able to generate efficient and reliable software code automatically. Vehicle models will mimic the characteristics of the real object to an extent we cannot imagine today. And automated test without manual interference will unprecedented degree of optimization and quality throughout a complex network of electronic control units. Almost the entire development process will be shifted to the desk with no need for costly, risky, and error-prone experiments with prototype engines or vehicles",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914998,no,undetermined,0
Virtualization in Grid,"In grid environment where resources are generally owned by different people, communities or organizations with varied administration policies and capabilities, managing the grid resources is not a simple task. Resource brokers simplify this process by providing an abstraction layer for users to access heterogeneous resources transparently. However, discovery of grid resources that suits to the user's job requirements is always a difficult job as the probability of grid resources satisfying the user's requirements is very less. Conventionally, in order to run a job on the grid a user has to identify a set of platforms capable of running that job by the virtue of having the required installation of operating system, libraries, tools, and the configuration of environment variables, etc. In practice, the availability of such software environments will either be limited to a very narrow set, or the job has to be made compatible with an environment supported by a large resource provider, such as TeraGrid. Further, if we could identify such an environment, it is hard to guarantee that the resource will be available when needed, for as long as needed, and that user will get his or her fair share of that resource. This difficulty can be overcome by incorporating the concept of virtualization in grid environment that enables the creation of dynamic user defined execution environment in a grid resource. Virtualization is a methodology of dividing the resources of a computer into multiple execution environments, by applying one or more concepts or technologies such as hardware and software partitioning, time-sharing, partial or complete machine simulation, emulation, quality of service, and many others. Recognizing the importance of virtualization in grid environment, our Centre for Advanced Computing Research and Education (CARE) attempts to develop a virtualization framework that facilitates job submission to the virtualized infrastructure by creating and managing virtual w- - orkspaces, and also it monitors the job execution on to the workspaces. This tutorial presents the proposed CRB (CARE resource broker) capable of supporting virtualization of resources, co-allocation and trust based resources for job execution in the grid environment. CRB supports on-demand job scheduling and SLA-based resource allocation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4760417,no,undetermined,0
An Efficient Event Based Approach for Verification of UML Statechart Model for Reactive Systems,"This paper describes an efficient method to detect safety specification violations in dynamic behavior model of concurrent/reactive systems. The dynamic behavior of each concurrent object in a reactive system is assumed to be represented using UML (Unified Modeling Language) statechart diagram. The verification process involves building a global state space graph from these independent statechart diagrams and traversal of large number of states in global state space graph for detecting a safety violation. In our approach, a safety property to be verified is read first and a set of events, which could violate this property, is computed from the model description. We call them as """"relevant events"""". The global state space graph is constructed considering only state transitions caused by the occurrence of these relevant events. This method reduces the number of states to be traversed for finding a property violation. Hence, this technique scales well for complex reactive systems. As a case study, the proposed technique is applied to verification of Generalized Railroad Crossing (GRC) system and safety property """"When train is at railroad crossing, the gate always remain closed"""" is checked. We could detect a flaw in the infant UML model and eventually, correct model is built with the help of counter example generated. The result of the study shows that, this technique reduces search space by 59% for the GRC example.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4760473,no,undetermined,0
A new clustering approach based on graph partitioning for navigation patterns mining,We present a study of the Web based user navigation patterns mining and propose a novel approach for clustering of user navigation patterns. The approach is based on the graph partitioning for modeling user navigation patterns. For the clustering of user navigation patterns we create an undirected graph based on connectivity between each pair of Web pages and we propose novel formula for assigning weights to edges in such a graph. The experimental results represent that the approach can improve the quality of clustering for user navigation pattern in Web usage mining systems. These results can be use for predicting userpsilas next request in the huge Web sites.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761808,no,undetermined,0
A support tool for annotated program manipulation,"The paper describes the AFORT system intended to be an integrated environment for support of analysis, transformation and instrumentation of FORTRAN 77 programs. It takes into account information that is known about the program being processed and conveyed in formalized comments (annotations). The AFORT system is based upon two approaches suggested by the author (V.N. Kasyanov, 1991; 1997): so-called annotated program concretization whereby a given general-purpose program can be correctly transformed into a number of special-purpose programs of higher quality, and so-called implausibility properties (anomalies) which permit us to detect dynamic errors statically and informal errors formally",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914972,no,undetermined,0
Coupling and cohesion as modularization drivers: are we being over-persuaded?,"For around three decades software engineering gurus have sold us the ideal of minimal coupling and maximal cohesion at all levels of abstraction as a way to reduce the effort to understand and maintain software systems. The object oriented paradigm brought a new design philosophy and encapsulation mechanisms that apparently would help us to achieve that desideratum. However, after a decade where this paradigm has emerged as the dominant one, we are faced with practitioners' reality: coupling and cohesion do not seem to be the dominant driving forces when it comes to modularization. This conclusion was based on a relatively large sample of heterogeneous systems. We describe an environment that allows us not only to assess this reality but also to derive better modularization solutions in what concerns coupling and cohesion. These solutions are generated by means of cluster analysis techniques and partially preserve the original modularization criteria. We believe this approach can be of great help in reengineering actions of object oriented legacy systems",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914968,no,undetermined,0
Evaluating the effect of inheritance on the modifiability of object-oriented business domain models,"The paper describes an experiment to assess the impact of inheritance on the modifiability of object oriented business domain models. This experiment is part of a research project on the quality determinants of early systems development artefacts, with a special focus on the maintainability of business domain models. Currently there is little empirical information about the relationship between the size, structural and behavioural properties of business domain models and their maintainability. The situation is different in object oriented software engineering where a number of experimental investigations into the maintainability of object oriented software have been conducted. The results of our experiment indicate that extensive use of inheritance leads to models that are more difficult to modify. These findings are in line with the conclusions drawn from three similar controlled experiments on inheritance and modifiability of object oriented software",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914964,no,undetermined,0
Systems failures: an approach to building a coping strategy,"When systems fail, they can cause havoc everywhere. They affect the organisations involved in creating, maintaining and using them and they can have a profound effect on the people involved, directly or indirectly. The causes of systems and project failures, vary considerably. Each case has to be taken in isolation and examined, to see where it has gone wrong in the past, or is starting to go wrong at present. In a true-life scenario, it is essential to be able to predict likely problems that may arise or accurately recognise failure symptoms when they occur. To achieve this it is important to be able to identify what is really going on and when these facts have been established, to be able to select a suitable means of handling the situation. Two European Esprit research projects examined software and multimedia quality practices and provided a framework for addressing these issues. These frameworks did not just promote best practices within software engineering, but sought to address some of the wider issues of systems and their role within the business. Middlesex University has taken this theme forward with a specific remit to address the subject of systems failures",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=913842,no,undetermined,0
Professional Engineers Ontario's approach to licensing software engineering practitioners,"Professional Engineers Ontario (PEO) has developed a methodology to assess software practitioners' qualifications for licensing purposes. It entails a comprehensive assessment of the applicants' academic preparation and work experience vis-a&grave;-vis PEO's software engineering body of knowledge and criteria for acceptable experience. Using this approach, PEO has licensed close to 200 software engineering practitioners to date",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=913823,no,undetermined,0
Experimental application of extended Kalman filtering for sensor validation,"A sensor failure detection and identification scheme for a closed loop nonlinear system is described. Detection and identification tasks are performed by estimating parameters directly related to potential failures. An extended Kalman filter is used to estimate the fault-related parameters, while a decision algorithm based on threshold logic processes the parameter estimates to detect possible failures. For a realistic evaluation of its performance, the detection scheme has been implemented on an inverted pendulum controlled by real-time control software. The failure detection and identification scheme is tested by applying different types of failures on the sensors of the inverted pendulum. Experimental results are presented to validate the effectiveness of the approach",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=911389,no,undetermined,0
Impact of metrics based refactoring on the software quality: a case study,"As the software system changes, the design of the software deteriorates hence reducing the quality of the system. This paper presents a case study in which an inventory application is considered and efforts are made to improve the quality of the system by refactoring. The code is an open source application namely ldquoinventor deluxe v 1.03rdquo, which was first, assessed using the tool Metrics 1.3.6 (an Eclipse plug-in). The code was then refactored and three more versions were built. At the end of creation of every version the code was assessed to find the improvement in the quality. The results obtained after measuring various metrics helped in tracing the spots in the code, which requires further improvement and hence can be refactored. Most of the refactoring was done manually with little tool support. Finally, a trend was found which shows, that average complexity and size of the code reduces with refactoring - based development, which helps to make the software more maintainable. Thus, although refactoring is time consuming and a labor-intensive work, it has a positive impact on the software quality.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766459,no,undetermined,0
Systematically deriving partial oracles for testing concurrent programs,"The problem of verifying the correctness of test executions is well-known: while manual verification is time-consuming and error-prone, developing an oracle to automatically verify test executions can be as costly as implementing the original program. This is especially true for concurrent programs, due to their non-determinism and complexity. In this paper, we present a method that uses partial specifications to systematically derive oracles for concurrent programs. We illustrate the method by deriving an Ada task that monitors the execution of a concurrent Ada program and describe a prototype tool that partially automates the derivation process. We present the results of a study that shows the derived oracles are surprisingly effective at error detection. The study also shows that manual verification is an inaccurate means of failure detection, that large test case sets must be used to ensure adequate testing coverage, and that test cases must be run many times to cover for variations in run-time behaviour",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=906627,no,undetermined,0
Verification and validation of object-oriented artifacts throughout the simulation model development life cycle,"The purpose of this paper is to present a series of questions (or indicators) for assessing the verity and validity of the artifacts produced during the entire object-oriented simulation model development life cycle. Using modern object-oriented development processes, artifacts developed in one phase flow seamlessly from those of the previous phase. This provides forward and backward traceability between artifacts. This inherent backward traceability has been exploited by tracing defects in artifacts back to their defective ancestral artifacts. Questions are then phrased such that when answered in the negative indicate the presence of defects. Use of the Evaluation Environment software tool facilitates the integration of the answers to the assessment questions and enables an overall evaluation. The collection of questions can be useful for the verification and validation of artifacts in any object-oriented simulation model development",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899886,no,undetermined,0
Australian Snowy Mountains Hydro Scheme earthing system safety assessment,"The task of determining the condition of the earthing in the Upper Tumut generation system, was undertaken as part of the Snowy Mountains Hydro Electric Authority's (SMHEA) safety risk assessment and asset condition monitoring programme. The testing programme to ascertain performance under earth fault and lightning conditions had to overcome considerable physical difficulties as well as the restrictions of 'close' proximity injection loops. The application of software, test instrumentation and testing procedures developed within Australia in collaboration between Energy Australia, Newcastle University, and SMHEA, to obtain real solutions are described in this paper. Also discussed are condition assessment processes that complement the current injection testing programme. This paper also provides a summary of the minimum requirements of an earthing system injection test to satisfactorily assess the condition of complex electrical power system installations",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=898167,no,undetermined,0
Aspect-oriented programming takes aim at software complexity,"As global digitalization and the size of applications expand at an exponential rate, software engineering's complexities are also growing. One feature of this complexity is the repetition of functionality throughout an application. An example of the problems this complexity causes occurs when programmers must change an oft-repeated feature for an updated or new version of an application. It is often difficult for programmers to find every instance of such a feature in millions of lines of code. Failing to do so, however, can introduce bugs. To address this issue, software researchers are developing methodologies based on a new programming element: the aspect. An aspect is a piece of code that describes a recurring property of a program. Applications can, of course, have multiple aspects. Aspects provide cross-cutting modularity. In other words, programmers can use aspects to create software modules for issues that cut across various parts of an application. Aspects have the potential to make programmers' work easier, less time-consuming and less error-prone. Proponents say aspects could also lead to less expensive applications, shorter upgrade cycles and software that is flexible and more customizable. A number of companies and universities are working on aspects or aspect-like concepts",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=917532,no,undetermined,0
Time Sensitive Ranking with Application to Publication Search,"Link-based ranking has contributed significantly to the success of Web search. PageRank and HITS are the best known link-based ranking algorithms. These algorithms do not consider an important dimension, the temporal dimension. They favor older pages because these pages have many in-links accumulated over time. Bringing new and quality pages to the users is important because most users want the latest information. Existing remedies to PageRank are mostly heuristic approaches. This paper investigates the temporal aspect of ranking with application to publication search, and proposes a principled method based on the stationary probability distribution of the Markov chain. The proposed techniques are evaluated empirically using a large collection of high energy particle physics publication. The results show that the proposed methods are highly effective.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781197,no,undetermined,0
Computer-aided fault to defect mapping (CAFDM) for defect diagnosis,"Defect diagnosis in random logic is currently done using the stuck-at fault model, while most defects seen in manufacturing result in bridging faults. In this work we use physical design and test failure information combined with bridging and stuck-at fault models to localize defects in random logic. We term this approach computer-aided fault to defect mapping (CAFDM). We build on top of the existing mature stuck-at diagnosis infrastructure. The performance of the CAFDM software was tested by injecting bridging faults into samples of a Streaming audio controller chip and comparing the predicted defect locations and layers with the actual values. The correct defect location and layer was predicted in all 9 samples for which scan-based diagnosis could be performed. The experiment was repeated on production samples that failed scan test, with promising results",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894269,no,undetermined,0
Reducing test application time for full scan circuits by the addition of transfer sequences,"A test set for scan designs may consist of tests where primary input vectors are embedded between a scan-in and a scan-out operation. A static compaction procedure proposed earlier reduces the test application time of such a test set by removing the scan operations at the end of one test and at the beginning of another test, and concatenating the primary input vectors of the two tests. In this work, we investigate a method to increase the number of tests that can be combined in this way, thus further reducing the number of scan operations and the test application time. This is done by inserting one or more primary input vectors between the two tests being combined. The inserted vectors help detect faults that were originally detected due to the scan operations, allowing us to combine tests that cannot be combined otherwise. We present experimental results to demonstrate that improved levels of compaction can be achieved by this method",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=893643,no,undetermined,0
Single-control testability of RTL data paths for BIST,"This paper presents a new BIST method for RTL data paths based on single-control testability a new concept of testability. The BIST method adopts hierarchical test. Test pattern generators are placed only on primary inputs and test patterns are propagated to and fed into each module. Test responses are similarly propagated to response analyzers placed only on primary outputs. For the propagation of test patterns and test responses, paths existing in the data path are utilized. The DFT method for the single-control testability is also proposed. The advantages of the proposed method are high fault coverage (for single stuck-at faults), low hardware overhead and capability of at-speed testing. Moreover test patterns generated by test pattern generators can be fed into each module at consecutive system clocks, and thus, the BIST can also detect some faults of other fault models (e.g., transition faults and delay faults) that require consecutive application of test patterns at the speed of the system clock",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=893627,no,undetermined,0
Towards automatic verification of autonomous systems,"While autonomous systems offer great promise in terms of capability and flexibility, their reliability is particularly hard to assess. This paper describes research to apply formal verification methods to languages used to develop autonomy software. In particular, we describe tools that automatically convert autonomy software into formal models that are then verified using model checking. This approach has been applied to MPL code for the Livingstone fault diagnosis system and to TDL task descriptions for mobile robot systems. Our long-term objective is to create tools that enable engineers and roboticists to use formal verification as part of the normal software development cycle",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=893218,no,undetermined,0
A model-based fault-tolerant CSCW architecture. Application to biomedical signals visualization and processing,"The paper describes a methodological approach that uses Petri nets (PNs) and Time Petri nets (TPNs) for modeling, analysis and behavior control of fault-tolerant computer supported synchronous cooperative work (CSSCW) architectures inside which a high level of interactivity between users is required. Modeling allows architectures to be formally studied under different functioning conditions (normal communications and deficient communications). Results show that the model is able to predict interlocking and state inconsistencies in the presence of errors. TPNs are used to extend PN models in order to detect communication errors and avoid subsequent dysfunctions. The approach is illustrated through the improvement of a recently presented collaborative application dedicated to biomedical signal visualization and analysis",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=892420,no,undetermined,0
Detecting a network failure,"Measuring the properties of a large, unstructured network can be difficult: one may not have full knowledge of the network topology, and detailed global measurements may be infeasible. A valuable approach to such problems is to take measurements from selected locations within the network and then aggregate them to infer large-scale properties. One sees this notion applied in settings that range from Internet topology discovery tools to remote software agents that estimate the download times of popular Web pages. Some of the most basic questions about this type of approach, however, are largely unresolved at an analytical level. How reliable are the results? How much does the choice of measurement locations affect the aggregate information one infers about the network? We describe algorithms that yield provable guarantees for a particular problem of this type: detecting a network failure. Suppose we want to detect events of the following form: an adversary destroys up to k nodes or edges, after which two subsets of the nodes, each at least an &epsi; fraction of the network, are disconnected from one another. We call such an event an (&epsi;,k) partition. One method for detecting such events would be to place agents at a set D of nodes, and record a fault whenever two of them become separated from each other. To be a good detection set, D should become disconnected whenever there is an (&epsi;,k)-partition; in this way, it witnesses all such events. We show that every graph has a detection set of size polynomial in k and &epsi;<sup>-1</sup>, and independent of the size of the graph itself. Moreover, random sampling provides an effective way to construct such a set. Our analysis establishes a connection between graph separators and the notion of VC-dimension, using techniques based on matchings and disjoint paths",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=892110,no,undetermined,0
SOCRATES on IP router fault detection,"SOCRATES is a software system for testing correctness of implementations of IP routing protocols such as RIP, OSPF and BGP. It uses a probabilistic algorithm to automatically construct random network topologies. For each generated network topology, it checks the correctness of routing table calculation and the IP packet forwarding behavior. For OSPF, it also checks the consistency between network topologies and the link-state databases of router under test. For BGP, it further checks the BGP update redistribution. Being different than commercial testing tools, which select their test cases in an ad-hoc manner, SOCRATES chooses test cases with a guaranteed fault coverage",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=891904,no,undetermined,0
Self-calibration of metrics of Java methods,"Self-calibration is a new technique for the study of internal product metrics, sometimes called observations and calibrating these against their frequency, or probability of occurring in common programming practice (CPP). Data gathering and analysis of the distribution of observations is an important prerequisite for predicting external qualities, and in particular software complexity. The main virtue of our technique is that it eliminates the use of absolute values in decision-making, and allows gauging local values in comparison with a scale computed from a standard and global database. Method profiles are introduced as a visual means to compare individual projects or categories of methods against the CPP. Although the techniques are general and could in principle be applied to traditional programming languages, the focus of the paper is on object oriented languages using Java. The techniques are employed in a suite of 17 metrics in a body of circa thirty thousand Java methods",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=891361,no,undetermined,0
A simulation method for estimating supply voltage dips in electrical power networks,"This paper describes the probabilistic approach for estimating voltage dips in electrical power networks. A method has been worked out which can estimate the typical magnitude and frequency of voltage dips, as well as momentary interruptions that may be expected at a given site. Evaluating such characteristics of voltage dips allows assessing compatibility between loads and the supply network. This methodology has been used to draw up a simulation tool by means of the LABVIEW program. The paper presents the results of simulation performed for a given electrical transmission and distribution network and discusses them",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897770,no,undetermined,0
Towards Process Rebuilding for Composite Web Services in Pervasive Computing,"The emerging paradigm of pervasive computing and web services needs a flexible service discovery and composition infrastructure. A composite Web service is essential a process in a loosely-coupled service-oriented architecture. It is usually a black box for service requestors and only its interfaces can be seen externally. In some scenarios, to conduct performance debugging and analysis, a workflow representation of the underlying process is required. This paper describes a method to discover such underlying processes from execution logs. Based on a probabilistic assumption model, the algorithm can discover sequential, parallel, exclusive choice and iterative structures. Some examples are given to illustrate the algorithm.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4783614,no,undetermined,0
Software architecture analysis based on statechart semantics,"High assurance architecture-based and component-based software development relies fundamentally on the quality of the components of which a system is composed and their configuration. Analysis over those components and their integration as a system plays a key role in the software development process. This paper describes an approach to develop and assess architecture and component-based systems based on specifying software architecture augmented by statecharts representing component behavioral specifications. The approach is applied for the C2 style and associated ADL and is supported within a quality-focussed environment, called Argus-I, which assists specification-based analysis and testing at both the component and architecture levels",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=891134,no,undetermined,0
Dynamic distributed software architecture design with PARSE-DAT,"The paper presents a novel software architecture design and verification methodology. Architects employ a pragmatic graphical design method called Dynamic PARSE to design the software architecture. At the same time, they capture the concurrent and dynamic features of the system. Such dynamic features include the creation and deletion of processes and re-configurable communication links. Lastly, the correctness of the design can be verified, and possible design faults may be detected by using an automatic design analysis and verification tool called PARSE-DAT",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=890435,no,undetermined,0
Experience with designing a requirements and architecture management tool,"Effective tool support is much needed for the tedious and error prone task of managing system requirements and system architectures. With the primary objective of providing practical support for software engineers, we have developed a tool for managing system requirements, system architectures and their traceability which is being used in real-world industrial projects. The tool is based on a well considered information model of system requirements and architecture, and embodies a set of document templates providing guidance for software engineers. The author reports on experience in designing and improving the tool. In particular, we highlight a number of case studies that played a significant role formulating the information model and document templates, and provide an assessment of the tool relative to existing practice",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=890433,no,undetermined,0
VerifyESD: a tool for efficient circuit level ESD simulations of mixed-signal ICs,"For many classes of technologies and circuits, it is beneficial to perform circuit simulations for ESD design, verification, and performance prediction. This is particularly true for mixed-signal ICs, where complex interaction between I/Os and multiple power supplies make manual analysis difficult and error prone. Unfortunately, high node and component counts typically prohibit simulations of an entire circuit. Thus, a manual intervention by the designer is usually required to minimize the circuit size. This paper introduces a new tool which automatically reduces the number of voltage nodes per ESD simulation by including only those devices that are necessary. In addition, a simple method for modeling ESD device failure while maintaining compatibility with existing CAD tools and libraries is discussed.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=890117,no,undetermined,0
A genetic algorithm-based system for generating test programs for microprocessor IP cores,"The current digital systems design trend is quickly moving toward a design-and-reuse paradigm. In particular, intellectual property cores are becoming widely used. Since the cores are usually provided as encrypted gate-level netlist, they raise several testability problems. The authors propose an automatic approach targeting processor cores that, by resorting to genetic algorithms, computes a test program able to attain high fault coverage figures. Preliminary results are reported to assess the effectiveness of our approach with respect to a random approach",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889869,no,undetermined,0
Post-mold cure process simulation of IC packaging,"Epoxy molding compound (EMC) is a common material used in IC packaging. One of its defects is warpage. Warpage could be a serious issue for some IC encapsulation processes. To alleviate the warpage problem during encapsulation, post mold cure process (PMC) is the most common strategy used. However, there are still no adequate tools or models to simulate the post mold cure process. Since EMC behaves like a viscoelastic material during post mold cure process, a viscoelastic model must be considered. The object of this paper was to construct a correct viscoelastic model, and then to input this model into a software package. This study adopted a dualistic shift factor Maxwell Model to simulate the post mold cure process. With this model, the amount of warpage after PMC process could be predicted. With dynamic mechanical analyzer (DMA) testing, the Generalized Maxwell model and Williams-Landel-Ferry (WLF) equation of fully cured EMC under different temperatures could be derived. Then, the partially cured EMC was tested by DMA. The viscoelastic properties of partially cured EMC were considered to have the similar behavior as temperature. Thus, a new model considering partially cured EMC as a cure induced shift factor similar to temperature shift factor could be derived. This model then became a model with dualistic shift factor Maxwell model. With some modification of structure analysis tool such as ANSYS, this dualistic shift factor Maxwell model could be applied and predict the post mold cure behavior of EMC. The results of calculation showed reasonable agreement between experiments and simulation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4784241,no,undetermined,0
A Stochastic Performance Model Supporting Time and Non-time QoS Matrices for Web Service Composition,"In recently years, Web service composition becomes a new approach to overcome many difficult problems confronted by B2B e-commerce, inter-organization workflow management, enterprise application integration etc. Due to the uncertainty of the Internet and various Web services, the performance of the composed Web service can not be ensured. How to model and predict the performance of the composed Web service is a difficult problem in the Web service composition. A novel simulation model that can model and simulate time and non-time performance characters, called STPM<sup>+</sup>, is presented in this paper. Based on Petri net, the STPM<sup>+</sup> model can simulate and predict multiple performance characters, such as the cost, the reliability and the reputation of the composed Web service etc. To examine the validation of the STMP<sup>+</sup> model, a visual performance evaluation tool, called VisualWSCPE, has been implemented. Besides, some simulation experiments have been fulfilled based on VisualWSCPE. The experiment results demonstrate the feasibility and efficiency of the STPM<sup>+</sup> model.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4780808,no,undetermined,0
Online Optimization in Application Admission Control for Service Oriented Systems,"In a service oriented environment an application is created through the composition of different service components. In this paper, we investigate the problem of application admission control in a service oriented environment. We propose an admission control system that makes admission decisions using an online optimization approach. The core part of the proposed system is an online optimization algorithm that solves a binary integer programming problem which we formulate in this paper. An online optimizer maximizes the system revenue given the system's available resources as well as the system's previous commitments. Another part of the proposed system carries out a feasibility evaluation that is intended to guarantee an agreed level of probability of success for each admitted application instance. We use simulations and performance comparisons to show that the proposed application admission control system can improve the system revenue while guaranteeing the required level of quality of service.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4780721,no,undetermined,0
Process certification: a double-edged sword,"On 27 June 2000, health authorities in Osaka city received a call from the hospital. They learned that people were suffering from diarrhoea, stomach pains, and vomiting after drinking low-fat milk products produced by Snow Brand Milk Product, one of Japan's largest dairy companies. On 1 July, officials at a medical laboratory in Wakayama prefecture announced that, when they tested the milk the victims had drunk, they detected a gene linked to the toxin present in yellow staphylococcus, an exit toxin found in leftover milk. Milk is far easier to test than software, because we can physically and chemically measure it. However, processing milk is similar to the software development process in terms of tangibility. Consequently, process is an essential part of both food production and software development, which is why both industries require process standards. Thus, the software community can learn from the fiasco at Snow Brand. In particular, there are four areas on which we should focus: process logs are easily faked; process is not a final objective; safety is the highest priority; and formal certification and authorization are a double-edged sword",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895176,no,undetermined,0
Formal specification techniques as a catalyst in validation,"The American Heritage Dictionary defines a catalyst as a substance, usually present in small amounts relative to the reactants, that modifies and especially increases the rate of a chemical reaction without being consumed in the process. This article reports on the experience gained in an industrial project that formal specification techniques form such a catalyst in the validation of complex systems. These formal development methods improve the validation process significantly by generating precise questions about the system's intended functionality very early and by uncovering ambiguities and faults in textual requirement documents. This project has been a cooperation between the IST and the company Frequentis. The Vienna Development Method (VDM) has been used for validating the functional requirements and the existing acceptance tests of a network node for voice communication in air traffic control. In addition to several detected requirement faults, the formal specification highlighted how additional test-cases could be derived systematically",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895462,no,undetermined,0
Towards the application of a model based design methodology for reliable control systems on HEP experiments,"The software development process of user interfaces for complex control system can constantly change in requirements. In those systems changes are costly (time consuming) and error prone, since we must guarantee that the resulting system implementation will still be robust and reliable. A way to tackle this problem is to bring a software model based approach for specification and providing at the same time rapid prototyping capabilities (to speed up design) and Simulation/Verification capabilities (to assure quality). We propose a full model-based methodology to guide designers through specification changes.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4774651,no,undetermined,0
Managing an employee ownership model in an IPO world,"Athens Group Inc. (AG), founded in June 1998, is an Austin-based, 100% employee-owned consulting firm specializing in technology strategy and custom software development. At a time when start-ups are plentiful-but rarely profitable-and talented people are scarce, AG has almost 50 outstanding software professionals, each with an average of 15 years of experience; has completed projects of global impact for Fortune 100 clients; and expects to earn $6 million in revenue in 2000. Athens Group builds software processes into the infrastructure of each client project. The backbone of each project is a repeatable software development process that is consistent with the Software Engineering Institutue's guidelines and appropriate to the needs of the client. These same principles have also been used on internal projects since the company's first few months, when most start-ups are prone to using a get it done and we'll figure out what we did later approach. Employee ownership as a founding principle has connected profit to process",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897347,no,undetermined,0
Implementation and evaluation for dependable bus control using CPLD,"Bus systems are used in computers as essential architecture, and dependability of bus systems should be accomplished reasonably for various applications. In this paper, we will present dependable bus operations with actual implementation and evaluation by CPLD. Most of the bus systems control transition of some classified phases with synchronous clock or guard time to avoid incorrect phase transition. However, these phase control methods may degrade system performance or cause incorrect operations. We design an asynchronous sequential circuit for bus phase control without clock or guard time. This circuit prevents incorrect phase transition at the time when large input delay or erroneous input occurs. We estimate probability of incorrect phase transition with single stuck-at fault on input signals. From the result of estimation, we also design checking system verifying outputs of initiator and target devices. Incorrect phase transition with single stuck-at fault occurred between both sequential circuits is inhibited completely by implementation of the system",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897279,no,undetermined,0
Application of data mining in Web pre-fetching,"To speed up fetching Web pages, we give an intelligent technology of Web pre-fetching. We use a simplified WWW data model to represent the data in the cache of a Web browser to mine the association rules. We store these rules in a knowledge base so as to predict the user's actions. Intelligent agents are responsible for mining the users' interest and pre-fetching Web pages, based on the interest association repository. In this way user browsing time has been reduced transparently",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897238,no,undetermined,0
Global random early estimation for nipping cells in ATM networks,"Asynchronous transfer mode was designed for multimedia communication networks. In multimedia networks, there are various kinds of data with different quality of service (QoS) requiring transmission. Consequently, the control of QoS in ATM becomes very important. Therefore, this paper proposes a novel buffer management algorithm named GREEN (Global Random Early Estimation for Nipping cells). In buffer management, to drop a cell randomly and early is the mainstream. Specifically, the designed random probability function considers not only network statuses but also QoS requirements. Also, GREEN speeds up the decision making by early estimation. The properties of global random and early estimation of the GREEN algorithm are obtained by globally considering the delay requirement and early estimating the queue length for decision making",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=897204,no,undetermined,0
Detecting of water shortage information in crops with acoustic emission technology and automatic irrigation system,"The automatic and real time irrigation system based on detecting of water shortage information in crops with acoustic emission (AE) technology was studied and developed, an experimental study in greenhouse with the crop of tomato as the target was carried out. PCI-2 AE board-card, R15 sensor, electronic balance, temperature sensor, humidity sensor, CO2 sensor, illumination sensor and PCI-8333 DAQ were adopted to compose the hardware detecting system, the virtual instrument technology was used to construct the software system, the three factors of soil, crops and atmosphere in SPAC system were effectively integrated, the real time acquisition and detecting system of information between crop acoustic emission and each environmental factor was established. It shows that: to some extent, the frequency counts of AE of the crops in water stress increase gradually with the increase of the water stress extent in crops, and are related with the transpiration rate of crops; in order to avoid the influence from water stress on crops, it has the potential to realize the automatic irrigation and regulation in crops based on the information acquired from crops with AE sensor, to make the transpiration amount and irrigation amount of crops achieve an balanced regulation, aiming to make the crops grow in an optimum soil water environment, to increase the utilization of water, and improve the quality of crop fruit.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4775854,no,undetermined,0
Analysis of the impact of reading technique and inspector capability on individual inspection performance,"Inspection of software documents is an effective quality assurance measure to detect defects in the early stages of software development. It can provide timely feedback on product quality to both developers and managers. This paper reports on a controlled experiment that investigated the influence of reading techniques and inspector capability on individual effectiveness to find given sets of defects in a requirements specification document. Experimental results support the hypothesis that reading techniques can direct inspectors' attention towards inspection targets, i.e. on specific document parts or severity levels, which enables inspection planners to divide the inspection work among several inspectors. Further, they suggest a tradeoff between specific and general detection effectiveness regarding document coverage and inspection effort. Inspector capability plays a significant role in inspection performance, while the size of the effect varies with the reading technique employed and the inspected document part",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896692,no,undetermined,0
Bloodshot eyes: workload issues in computer science project courses,"Workload issues in computer science project courses are addressed. We briefly discuss why high workloads occur in project courses and the reasons they are a problem. We then describe some course changes we made to reduce the workload in a software engineering project course, without compromising course quality. The techniques include: adopting an iterative and incremental process, reducing the requirements for writing documents, and gathering accurate data on time spent on various activities. We conclude by assessing the techniques, providing good evidence for a dramatic change in the workload, and an increase in student satisfaction levels. We provide some evidence, and an argument, that learning has not been affected by the changes",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896682,no,undetermined,0
Eliminating annotations by automatic flow analysis of real-time programs,"There is an increasing demand for methods that calculate the worst case execution time (WCET) of real time programs. The calculations are typically based on path information for the program, such as the maximum number of iterations in loops and identification of infeasible paths. Most often, this information is given as manual annotations by the programmer. Our method calculates path information automatically for real time programs, thereby relieving the programmer from tedious and error-prone work. The method, based on abstract interpretation, generates a safe approximation of the path information. A trade-off between quality and calculation cost is made, since finding the exact information is a complex, often intractable problem for nontrivial programs. We describe the method by a simple, worked example. We show that our prototype tool is capable of analyzing a number of program examples from the WCET literature, without using any extra information or consideration of special cases needed in other approaches",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896435,no,undetermined,0
A framework to model dependable real-time systems based on real-time object model,"Proposes a framework to model fault-tolerant real-time systems consisting of RobustRTOs (Robust Real-Time Objects) and RMOs (Region Monitor real-time Objects). A RobustRTO is an object which is capable of tolerating faults in itself. Many existing fault-tolerant mechanisms, such as RB (recovery blocks) and NVP (N-version programming), are modeled as RobustRTOs. An RMO is an object which is capable of monitoring a set of objects, named regions. The RMO detects any abnormal behavior of the objects within a region, diagnoses the symptoms and performs appropriate recovery and/or reconfiguration. Although the concepts of RobustRTOs and RMOs are introduced based on a real-time object model, we believe they are applicable to the modeling and design of any dependable embedded real-time system",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896368,no,undetermined,0
Practical applications of statistical process control [in software development projects],Applying quantitative methods such as statistical process control (SPC) to software development projects can provide a positive cost-benefit return. The authors used SPC on inspection and test data to assess product quality during testing and to predict post-shipment product quality for a major software release,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896249,no,undetermined,0
Bayesian framework for reliability assurance of a deployed safety critical system,"The existence of software faults in safety-critical systems is not tolerable. Goals of software reliability assessment are estimating the failure probability of the program, , and gaining statistical confidence that  is realistic. While in most cases reliability assessment is performed prior to the deployment of the system, there are circumstances when reliability assessment is needed in the process of (re)evaluation of the fielded (deployed) system. Post deployment reliability assessment provides reassurance that the expected dependability characteristics of the system have been achieved. It may be used as a basis of the recommendation for maintenance and further improvement, or the recommendation to discontinue the use of the system. The paper presents practical problems and challenges encountered in an effort to assess and quantify software reliability of NASA's Day-of-Launch I-Load Update (DOLILU II) system DOLILU II system has been in operational use for several years. A Bayesian framework is chosen for reliability assessment, because it allows incorporation of (in this specific case failure free) program executions observed in the operational environment. Furthermore, we outline the development of a probabilistic framework that allows accounting of rigorous verification and validation activities performed prior to a system's deployment into the reliability assessment",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895477,no,undetermined,0
A meta-measurement approach for software test processes,"Existing test process assessment and improvement models intend to raise maturity of an organization with reference to testing activities. Such process assessments are based on ldquowhatrdquo testing activities are being carried out, and thus implicitly evaluate process quality. Other test process measurement techniques attempt to directly assess some partial quality attribute such as efficiency or effectiveness some test metrics. There is a need for a formalized method of test process quality evaluation that addresses both implicitly and partially of these current evaluations. This paper describes a conceptual framework to specify and explicitly evaluate test process quality aspects. The framework enables provision of evaluation results in the form of objective assessments, and problem-area identification to improve the software testing processes.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777759,no,undetermined,0
Global Sensitivity Analysis (GSA) Measures the Quality of Parameter Estimation. Case of Soil Parameter Estimation with a Crop Model,"The behavior of crops can be accurately predicted when all the parameters of the crop model are well known, and assimilating data observed on crop status in the model is one way of estimating parameters. Nevertheless, the quality of the estimation depends on the sensitivity of model output variables to the parameters. In this paper, we quantify the link between the global sensitivity analysis (GSA) of the soil parameters of the mechanistic crop model STICS, and the ability to retrieve the true values of these parameters. The Global sensitivity indices were computed by a variance based method (Extended FAST) and the quality of parameter estimation (RRMSE) was computed with an importance sampling method based on Bayes theory (GLUE). Criteria based on GSA were built to link GSA indices with the quality of parameters estimation. The result shows that the higher the criteria, the better the quality of parameters estimation and GSA appeared to be useful to interpret and predict the performance of the estimation parameters process.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4779531,no,undetermined,0
A high-assurance measurement repository system,"High-quality measurement data are very useful for assessing the efficacy of high-assurance system engineering techniques and tools. Given the rapidly evolving suite of modern tools and techniques, it is helpful to have a large repository of up-to-date measurement data that can be used to quantitatively assess the impact of state-of-the-art techniques on the quality of the resulting systems. For many types of defects, including Y2K failures, infinite loops, memory overflow, access violations, arithmetic overflow, divide-by-zero, off-by-one errors, timing errors, deadlocks, etc., it may be possible to combine data from a large number of projects and use these to make statistical inferences. This paper presents a highly secure and reliable measurement repository system for measurement data acquisition, storage and analysis. The system is being used by the QuEST Forum, which is a new industry forum consisting of over 100 leading telecommunications companies. The paper describes the decisions that were made in the design of the measurement repository system, as well as implementation strategies that were used in achieving a high-level of confidence in the security and reliability of the system",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895471,no,undetermined,0
Towards a Requirements-Aware Common Web Engineering Metamodel,"In recent years, Web engineering development projects have grown increasingly complex and critical for the smooth running of the organizations. However, recent studies reveal that, due to an incorrect requirements management, a high percentage of these projects miss the quality parameters required by stakeholders. Despite this, current Web Engineering methodologies continue focusing on web design features, thus limiting the Requirements Engineering tasks to the elicitation of high-level functional requirements. This fact has caused a requirements-support gap in the recently proposed Common Web Engineering Metamodel. This paper tries to cover this gap and proposes a requirements extension for this Common Web Engineering metamodel that supports measurable requirements. The reinforcement of the role that requirements play in current Web engineering methodologies, and their explicit connection with quantitative measures that contribute to their fulfilment, is a necessary step in order to reduce some of the quality failures detected in Web engineering development projects, thus increasing the satisfaction of their users.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756164,no,undetermined,0
The application of neural networks to fuel processors for fuel-cell vehicles,"Passenger vehicles fueled by hydrocarbons or alcohols and powered by proton exchange membrane (PEM) fuel cells address world air quality and fuel supply concerns while avoiding hydrogen infrastructure and on-board storage problems. Reduction of the carbon monoxide concentration in the on-board fuel processor's hydrogen-rich gas by the preferential oxidizer (PrOx) under dynamic conditions is crucial to avoid poisoning of the PEM fuel cell's anode catalyst and thus malfunction of the fuel-cell vehicle. A dynamic control scheme is proposed for a single-stage tubular cooled PrOx that performs better than, but retains the reliability and ease of use of, conventional industrial controllers. The proposed hybrid control system contains a cerebellar model articulation controller artificial neural network in parallel with a conventional proportional-integral-derivative (PID) controller. A computer simulation of the preferential oxidation reactor was used to assess the abilities of the proposed controller and compare its performance to the performance of conventional controllers. Realistic input patterns were generated for the PrOx by using models of vehicle power demand and upstream fuel-processor components to convert the speed sequences in the Federal Urban Driving Schedule to PrOx inlet temperatures, concentrations, and flow rates. The proposed hybrid controller generalizes well to novel driving sequences after being trained on other driving sequences with similar or slower transients. Although it is similar to the PID in terms of software requirements and design effort, the hybrid controller performs significantly better than the PID in terms of hydrogen conversion setpoint regulation and PrOx outlet carbon monoxide reduction",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=917898,no,undetermined,0
Safe virtual execution using software dynamic translation,"Safe virtual execution (SVE) allows a host computer system to reduce the risks associated with running untrusted programs. SVE prevents untrusted programs from directly accessing system resources, thereby giving the host the ability to control how individual resources may be used. SVE is used in a variety, of safety-conscious software systems, including the Java Virtual Machine (JVM), software fault isolation (SFI), system call interposition layers, and execution monitors. While SVE is the conceptual foundation for these systems, each uses a different implementation technology. The lack of a unifying framework for building SVE systems results in a variety of problems: many useful SVE systems are not portable and therefore are usable only on a limited number of platforms; code reuse among different SVE systems is often difficult or impossible; and building SVE systems from scratch can be both time consuming and error prone. To address these concerns, we have developed a portable, extensible framework for constructing SVE systems. Our framework, called Strata, is based on software dynamic translation (SDT), a technique for modifying binary programs as they execute. Strata is designed to be ported easily to new platforms and to date has been targeted to SPARC/Solaris, x86/Linux, and MIPS/IRIX. This portability ensures that SVE applications implemented in Strata are available to a wide variety of host systems. Strata also affords the opportunity for code reuse among different SVE applications by establishing a common implementation framework. Strata implements a basic safe virtual execution engine using SDT The base functionality supplied by this engine is easily extended to implement specific SVE systems. In this paper we describe the organization of Strata and demonstrate its extension by building two SVE systems: system call interposition and stack-smashing prevention. To illustrate the use of the system call interposition extensions, the paper presents implementations of several useful security policies.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1176292,no,undetermined,0
Detection and identification of odorants using an electronic nose,"Gas sensing systems for detection and identification of odorant molecules are of crucial importance in an increasing number of applications. Such applications include environmental monitoring, food quality assessment, airport security, and detection of hazardous gases. We describe a gas sensing system for detecting and identifying volatile organic compounds (VOC), and discuss the unique problems associated with the separability of signal patterns obtained by using such a system. We then present solutions for enhancing the separability of VOC patterns to enable classification. A new incremental learning algorithm that allows new odorants to be learned is also introduced",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=940323,no,undetermined,0
Detecting Defects in Golden Surfaces of Flexible Printed Circuits Using Optimal Gabor Filters,"This paper studies the application of advanced computer image processing techniques for solving the problem of automated defect detection for golden surfaces of flexible printed circuits (FPC). A special defect detection scheme based on semi-supervised mechanism is proposed, which consists of an optimal Gabor filter and a smoothing filter. The aim is to automatically discriminate between """"known"""" non-defective background textures and """"unknown"""" defective textures of golden surfaces of FPC. In developing the scheme, the parameters of the optimal Gabor filter are searched with the help of the genetic algorithm based on constrained minimization of a Fisher cost function. The performance of the proposed defect detection scheme is evaluated off-line by using a set of golden images acquired from CCD. The results exhibit accurate defect detection with low false alarms, thus showing the effectiveness and robustness of the proposed scheme.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4739587,no,undetermined,0
Power quality assessment from a wave-power station,This paper describes the development and testing of a software based flickermeter used in order to assess the supply quality from the LIMPET wave-power station on Islay. It describes the phenomenon of voltage flicker and the effect that a wave-power station has on this quantity. The paper also explains techniques developed in order to improve flickermeter performance when used with pre-recorded data. It also shows that the standard flickermeter sample frequency may be reduced for wave-station applications. Finally the paper presents flicker results from preliminary data collected from the LIMPET station and shows that the device is operating well within acceptable limits,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942975,no,undetermined,0
Model-aided diagnosis: an inexpensive combination of model-based and case-based condition assessment,"Online condition monitoring and diagnosis are being utilized more and more for increasing the reliability and availability of technical systems and to reduce their maintenance costs. Today's model-based diagnosis (MBD) tools are able to detect and identify incipient and sudden faults very reliably. For application to cost-sensitive equipment, such as high-voltage circuit breakers (HVCBs), however, the presently available MBD systems are not feasible for economic reasons. In this paper, a novel combination of the model-based with the case-based approach to condition diagnosis is presented, which can be implemented on a low-cost computer and which offers satisfactory performance. The technique is divided into two parts: (1) preparation and (2) diagnosis. The diagnosis part can be executed on an inexpensive low-performance computer. Successful tests on real HVCBs confirm the usefulness of this new approach to condition diagnosis",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941838,no,undetermined,0
Improving Keyphrase Extraction Using Wikipedia Semantics,"Keyphrase extraction plays a key role in various fields such as information retrieval, text classification etc. However, most traditional keyphrase extraction methods relies on word frequency and position instead of document inherent semantic information, often results in inaccurate output. In this paper, we propose a novel automatic keyphrase extraction algorithm using semantic features mined from online Wikipedia. This algorithm first identifies candidate keyphrases based on lexical methods, and then a semantic graph which connects candidate keyphrases with document topics is constructed. Afterwards, a link analysis algorithm is applied to assign semantic feature weight to the candidate keyphrases. Finally, several statistical and semantic features are assembled by a regression model to predict the quality of candidates. Encouraging results are achieved in our experiments which show the effectiveness of our method.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4739723,no,undetermined,0
A new image coding quality assessment,"Based on characteristics of HVS (human visual system), a new objective image quality assessment is proposed. In this method, the information of luminance, frequency and edge is used to predict the quality of compressed images. Multi-linear regression analysis is used to integrate these information. Experimental results show that this new image quality assessment closely approximates human subjective tests such as MOS (mean opinion score) with high Pearson and Spearman correlation coefficients of 0.970 and 0.976, which are of significant improvement over some typical objective image quality estimations such as PSNR (peak signal-to-noise ratio).",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4746074,no,undetermined,0
A novel basic unit level rate control algorithm and architecture for H.264/AVC video encoders,"Rate control (RC) techniques play an important role for interactive video coding applications, especially in video streaming applications with bandwidth constraints. Among the RC algorithms in H.264 reference software JM, the basic unit (BU)-level RC algorithm achieves better video quality than frame-level one. However, the inherent sequential processing in H.264 BU-level RC algorithm makes it difficult to be realized in a pipelined H.264 hardware encoder without increasing the processing latency. In this paper we propose a new H.264 BU-level rate control algorithm and the associated architecture by exploiting a new predictor model to predict the MAD value and target bits for hardware realization. The proposed algorithm breaks down the sequential processing dependence in the original H.264 RC algorithm and reduces up to 80.6% of internal buffer size for H.264 D1 video encoding, while maintaining good video quality.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4746266,no,undetermined,0
Run-time fault detection in monitor based concurrent programming,"The monitor concept provides a structured and flexible high-level programming construct to control concurrent accesses to shared resources. It has been widely used in concurrent programming environments for implicitly ensuring mutual exclusion and explicitly achieving process synchronization. This paper proposes an extension to the monitor construct for detecting run-time errors in monitor operations. Monitors are studied and classified according to their functional characteristics. A taxonomy of concurrency control faults over a monitor is then defined. The concepts of a monitor event sequence and a monitor state sequence provide a uniform approach to history information recording and fault detection. Rules for detecting various types of faults are defined. Based on these rules, fault detection algorithms are developed. A prototypical implementation of the proposed monitor construct with run-time fault detection mechanisms has been developed in Java. We briefly report our experience with and evaluation of our robust monitor prototype.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941420,no,undetermined,0
Automatic support for verification of secure transactions in distributed environment using symbolic model checking,"Electronic commerce needs the aid of software tools to check the validity of business processes in order to fully automate the exchange of information through the network. Symbolic model checking has been used to formally verify specifications of secure transactions in a business-to-business system. The fundamental principles behind symbolic model checking are presented, along with techniques used to model mutual exclusion of processes and atomic transactions. The computational resources required to check the example process are presented, and faults detected in this process through symbolic verification are documented.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=938054,no,undetermined,0
Test generation for time critical systems: Tool and case study,"Generating timed test sequences by hand is error-prone and time consuming, and it is easy to overlook important scenarios. The paper presents a tool based on formal methods that automatically computes a test suite for conformance testing of time critical systems. The generated tests are selected on the basis of a coverage criterion of the specification. The tool guarantees production of sound test cases only, and is able to produce a complete covering test suite. We demonstrate the tool by generating test cases for the Philips Audio Protocol",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=934021,no,undetermined,0
The concept of quality information system (QIS),"The product quality characteristics should be the prime drivers when assessing and improving the quality of the software development process as we are concerned with the product quality. The quality of the software product is determined by the quality of the software process. This seems intuitive but there is no empirical evidence to prove its validity yet. QIS establishes a system that enables to analyse the relation between base practices and processes of the SPICE model and the eleven product quality factors and criteria of McCall's (1977) model for software product evaluation. The main goal of QIS is to evaluate and verify benefits gained by improving the process maturity level. In front line of both, the process model and product quality model is the software product improvement, resulting in a high quality software product delivered on time and at less cost.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=938001,no,undetermined,0
Reliability properties assessment at system level: a co-design framework,"The reliability co-design project aims at integrating in a standard hw/sw co-design flow the elements for achieving a final system able to detect the occurrence of a fault during its operational life. The paper presents the focus of the project, the definition and identification of design methodologies for implementing the nominal, checking and checker functionalities either in hardware or in software. An outline of the system specification and system partitioning aspects is also provided",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=937837,no,undetermined,0
A framework for segmentation of talk and game shows,"In this paper, we present a method to remove commercials from talk and game show videos and to segment these videos into host and guest shots. In our approach, we mainly rely on information contained in shot transitions, rather than analyzing the scene content of individual frames. We utilize the inherent differences in scene structure of commercials and talk shows to differentiate between them. Similarly, we make use of the well-defined structure of talk shows, which can be exploited to classify shots as host or guest shots. The entire show is first segmented into camera shots based on color histogram. Then, we construct a data-structure (shot connectivity graph) which links similar shots over time. Analysis of the shot connectivity graph helps us to automatically separate commercials from program segments. This is done by first detecting stories, and then assigning a weight to each story based on its likelihood of being a commercial. Further analysis on stories is done to distinguish shots of the hosts from shots of the guests. We have tested our approach on several full-length shows (including commercials) and have achieved video segmentation with high accuracy. The whole scheme is fast and works even on low quality video (160120 pixel images at 5 Hz)",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=937671,no,undetermined,0
A simple method for extracting models from protocol code,"The use of model checking for validation requires that models of the underlying system be created. Creating such models is both difficult and error prone and as a result, verification is rarely used despite its advantages. In this paper we present a method for automatically extracting models from low level software implementations. Our method is based on the use of an extensible compiler system, xg++, to perform the extraction. The extracted model is combined with a model of the hardware, a description of correctness, and an initial state. The whole model is then checked with the Mur model checker. As a case study, we apply our method to the cache coherence protocols of the Stanford FLASH multiprocessor. Our system has a number of advantages. First, it reduces the cost of creating models, which allows model checking to be used more frequently. Second, it increases the effectiveness of model checking since the automatically extracted models are more accurate and faithful to the underlying implementation. We found a total of 8 errors using our system. Two errors were global resource errors, which would be difficult to find through any other means. We feel the approach is applicable to other low level systems",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=937448,no,undetermined,0
A predictive measurement-based fuzzy logic connection admission control,"This paper presents a novel measurement-based connection admission control (CAC) which uses fuzzy set and fuzzy logic theory. Unlike conventional CAC, the proposed CAC does not use complicated analytical models or a priori traffic descriptors. Instead, traffic parameters are predicted by an on-line fuzzy logic predictor (Qiu et al. 1999). QoS requirements are targeted indirectly by an adaptive weight factor. This weight factor is generated by a fuzzy logic inference system which is based on arrival traffic, queue occupancy and link load. Admission decisions are then based on real-time measurement of aggregate traffic statistics with the fuzzy logic adaptive weight factor as well as the predicted traffic parameters. Both homogeneous and heterogeneous traffic were used in the simulation. Fuzzy logic prediction improves the efficiency of both conventional and measurement-based CAC. In addition, the measurement-based approach incorporating fuzzy logic inference and using fuzzy logic prediction is shown to achieve higher network utilization while maintaining QoS",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=937372,no,undetermined,0
Application QoS management for distributed computing systems,"As a large number of distributed multimedia systems are deployed on computer networks, quality of service (QoS) for users becomes more important. This paper defines it as application QoS, and proposes the application QoS management system (QMS). It controls the application QoS according to the system environment by using simple measurement-based control methods. QMS consists of three types of modules. These are a notificator for module detecting QoS deterioration, a manager module for deciding the control method according to the application management policies, and a controller module for executing the control. The QMS manages the application QoS by communicating between these modules distributed on the network. Moreover, this paper especially focuses on the function setting QoS management policies to the QMS and proposes the setting method. By a simulation experiment, we confirmed that the system made it possible to negotiate the QoS among many applications and it was able to manage the whole applications according to the policies",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=936872,no,undetermined,0
Application of vibration sensing in monitoring and control of machine health,"In this paper, an application for monitoring and control of machine health using vibration sensing is developed. This vibration analyzer is able to continuously monitor and compare the actual vibration pattern against a vibration signature, based on a fuzzy fusion technique. More importantly, this intelligent knowledge-based real-time analyzer is able to detect excessive vibration conditions much sooner than a resulting fault could be detected by an operator. Subsequently, appropriate actions can be taken, say to provide a warning or automatic corrective action. This approach may be implemented independently of the control system and as such can be applied to existing equipment without modification of the normal mode of operation. Simulation and experimental results are provided to illustrate the advantages of the approach taken in this application",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=936484,no,undetermined,0
An agent-based framework with QoS-aware content negotiation for gateway-based nomadic applications,"The advances in both wireless data communications and portable computing technologies have recently generated a new trend: nomadic computing. The concept of nomadic computing has emerged as the expectation of nomadic end-users to retain one's personal computing environments and access capabilities wherever one happens to be. The changing environment in wireless due to mobility and interference gives rise to varying bandwidth, latency, error rate, loss probability, interoperability, and quality of display. Confronted with these circumstances, the ability to support adaptability and QoS-awareness in a transparent and integrated fashion is essential. This paper describes an agent-based framework with QoS-aware content negotiation for gateway-based nomadic applications",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=944164,no,undetermined,0
Time-to-failure estimation for batteries in portable electronic systems,Nonlinearity of the energy source behavior in portable systems needs to be modeled in order for the system software to make energy-conscious decisions. We describe an analytical battery model for predicting the battery time-to-failure under variable discharge conditions. Our model can be used to estimate the impact of various system load profiles on the energy source lifetime. The quality of our model is evaluated based on the simulation of a lithium-ion battery,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=945380,no,undetermined,0
Benchmarking of advanced technologies for process control: an industrial perspective,"Global competition is forcing industrial plants to continuously improve product quality and reduce costs in order to be more profitable. This scenario doesn't allow producing in less than excellent performance. Combining higher, more consistent product quality with a larger production volume and an increased flexibility however places special stresses on plant assets and equipment jeopardizing safety and environmental compliance. Consequently, process industries are called to operate on a very narrow, constrained path that needs to be continuously monitored, assessed and adjusted. The talk aims at reviewing the main points of the problem and at discussing how benchmarking practices should include and take advantage of advanced automation technologies. It will briefly consider basics for project justification and what an automation vendor may (really should) do in order to help process industries customer to select the best solutions for their plants. A particular emphasis will be placed on sometimes neglected aspects, such as hardware-software integration issues; life-cycle cost-benefit analysis; and operator acceptance and living with the APC tools and strategies. Finally some basic suggestions taken out of field experience will be given",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=945656,no,undetermined,0
The Use of E-SQ to establish the internet bank service quality table,"In order to assist Internet bank to be able to reach the enterpriseAs goal, which is satisfying the customerAs demand and this goal is different from the PZB service quality model; thus, this study uses ZPM e-service quality model as the foundation to assess Web sites. The study object would be the companies that provide Internet bank services at present. Then, the factors that influence customersA quality satisfaction towards services would be generalized, and the questionnaire survey would be carried out the users, administrators, and employees of Internet bank. A service quality table that assesses Internet bank would be established through the evidence-based study result, it also verifies that information gap, design gap and fulfillment gap are significant. The result also finds out eight dimensions, including AefficiencyA, AreliabilityA, AprivacyA, AcompensationA, AresponsivenessA, AcontactA, Asense of beautyA and AindividualizationA, are the key factors that influence the service quality of Internet bank.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4738110,no,undetermined,0
On systematic design of protectors for employing OTS items,"Off-the-shelf (OTS) components are increasingly used in application areas with stringent dependability requirements. Component wrapping is a well known structuring technique used in many areas. We propose a general approach to developing protective wrappers that assist in integrating OTS items with a focus on the overall system dependability. The wrappers are viewed as redundant software used to detect errors or suspicious activity and to execute appropriate recovery when possible; wrapper development is considered as a part of system integration activities. Wrappers are to be rigorously specified and executed at run time as a means of protecting OTS items against faults in the rest of the system, and the system against the OTS item's faults. Possible symptoms of erroneous behaviour to be detected by a protective wrapper and possible actions to be undertaken in response are listed and discussed. The information required for wrapper development is provided by traceability analysis. Possible approaches to implementing protectors in the standard current component technologies are briefly outlined",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952434,no,undetermined,0
Management of the cross media impacts of municipal landfill sites: the Delphi technique,"Most of the existing solid waste disposal sites in Malaysia are practising either open dumping or controlled tipping because the technology of proper sanitary landfill practice is not totally implemented. The environmental conditions from these sites are thus expected to be bad especially in terms of the contamination of soil, air, surface and underground water, and also impacts on flora and fauna including human. The contamination associated with solid waste disposal sites involved three major environmental compartments or media, i.e. the atmosphere, water and soil. This 'Cross media' or 'Multimedia' impacts phenomenon has been recognised in various countries as being of potential importance and complicated. This study discusses on the development of simple evaluation systems by using the Delphi Approach, which emphasises on the development of weightage for different parameters selected in the evaluation procedures. Environmental conditions of all closed and active disposal sites in the study area from 9 different points of view (water quality, social, gas emissions, landuse, hydrology, geology, ecotoxicology, plant ecology and chemical constituents) were assessed, which has taken into consideration 59 selected parameters. The Landfill Pollution Index (LPI) was introduced and made into a software, which is more user friendly and requires minimum inputs from the user. The LPI incorporated with 4 other subindices, i.e. the Environmental Degradation Index (EDI) for water quality, gas emission, chemicals in surface water and chemicals in groundwater. The results of assessments indicated that most of the solid waste disposal sites in the study area showed relatively bad environmental conditions especially the operating or active site, i.e. Taman Beringin landfill site. Taman Beringin was the most polluted landfill with the LPI of 719.56, followed by Jinjang Utara (383.51), Paka 1 (197.66), Brickfields (128.90), Paka 2 (113.72), Sri Petaling (30.81) and Sungei Besi (17.87)",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952414,no,undetermined,0
Simulation of modular building construction,"Modular construction has the advantage of producing structures quickly and efficiently, while not requiring the resources to build a structure to be co-located with the construction site. Large modules can be produced in quality controlled environments, and then shipped to the construction site and assembled with minimal labor requirements. An additional advantage is that once the modules are on-site, construction can proceed extremely quickly. This is ideal for situations where compressed schedules are required in order to meet clientAs time constraints. This paper examines using software simulation, specifically Simphony.NET, in the design and analysis of the construction process. This is done both before and after project execution to predict productivity and duration and also to allow for exploration of alternate construction scenarios.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736356,no,undetermined,0
In search of efficient reliable processor design,"In this paper, we investigate an efficient reliable processor which can detect and recover from transient faults. There are two driving forces to study fault-tolerant techniques for microprocessors. One is deep submicron fabrication technology. Future semiconductor technologies could become more susceptible to alpha particles and other cosmic radiation. The other is increasing popularity of mobile platforms. Recently cell phones are used for applications which are critical to our financial security, such as flight ticket reservation, mobile banking, and mobile trading. In such applications, it is expected that computer systems will always work correctly. From these observations, we have proposed a mechanism which is based on instruction reissue technique for incorrect data speculation recovery and utilizes time redundancy. In order to mitigate overhead caused by including fault-tolerant facility, we evaluate some alternative designs and find that speculatively updating branch predictors and removing redundant memory accesses are very effective.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952100,no,undetermined,0
Management indicators model to evaluate performance of IT organizations,"There is no arguing nowadays about the importance of IT for the growth and competitive edge of organizations. But if technology is to be a true asset for a company, it must be aligned with the business strategic goals by means of a formalized system of strategic planning, maturity of development process, technology management and corporative quality vision. The accrued benefits can be manifold: the development of training and learning environments for an effective improvement of procedures and product quality, efficient use of assets and resources, opportunity for innovation and technologic advancement, an approach to problem solving in areas critical to the organization among others. Many companies make use of these practices, but find it hard to evaluate how effective they are and what is the final quality of the achieved results at diverse customer levels both in project vision and the continuity of service. One cause of these drawbacks is failure to apply measurement models which provide objective pointers to assess how effective the IT strategies used have actually been considering the strategic business goals. To incorporate models of measures is no easy task because it entails working on several aspects: technical, processes, products and the peculiar culture of each organization. This paper presents a model of indicators to evaluate IT performance using three well known methods: balanced scorecard, GQM and PSM",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952021,no,undetermined,0
Establishing enterprise communities,"One of the most important challenges facing the builders of enterprise software is the reliable implementation of the policies that are supposed to govern the various communities operating within an enterprise. Such policies are widely considered fundamental to enterprise modeling, and their specification were the subject of several recent investigations. But specification of the policy that is to govern a given community is only the first step towards its implementation; the second, and more critical step is to ensure that all members of the community actually conform to the specified policy. The conventional approach to the implementation of a policy is to build it into all members of the community subject to it. But if the community in question is large and heterogeneous, and if its members are dispersed throughout a distributed enterprise, then such """"manual"""" implementation of its policy would be too laborious and error-prone to be practical. Moreover, a policy implemented in this manual manner would be very unstable with respect to the evolution of the system, because it can be violated by a change in the code of any member of community subject to it. It is our thesis that the only reliable way for ensuring that an heterogeneous distributed community of software modules and people conforms to a given policy is for this policy to be strictly enforced. A mechanism for establishing enterprise communities by formally specifying their policies, and by having these policies enforced is the subject of the paper",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=950422,no,undetermined,0
A multi-sensor based temperature measuring system with self-diagnosis,"A new multi-sensor based temperature measuring system with self-diagnosis is developed to replace a conventional system that uses only a single sensor. Controlled by a 16-bit microprocessor, each sensor output from the sensor array is compared with a randomly selected quantised reference voltage at a voltage comparator and the result is a binary """"one"""" or """"zero"""". The number of """"ones"""" and """"zeroes"""" is counted and the temperature can be estimated using statistical estimation and successive approximation. A software diagnostic algorithm was developed to detect and isolate the faulty sensors that may be present in the sensor array and to recalibrate the system. Experimental results show that temperature measurements obtained are accurate with acceptable variances. With the self-diagnostic algorithm, the accuracy of the system in the presence faulty sensors is significantly improved and a more robust measuring system is produced",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949727,no,undetermined,0
Reliability modeling incorporating error processes for Internet-distributed software,"The paper proposes several improvements to conventional software reliability growth models (SRGMs) to describe actual software development processes by eliminating an unrealistic assumption that detected errors are immediately corrected. A key part of the proposed models is the """"delay-effect factor"""", which measures the expected time lag in correcting the detected faults during software development. To establish the proposed model, we first determine the delay-effect factor to be included In the actual correction process. For the conventional SRGMs, the delay-effect factor is basically non-decreasing. This means that the delayed effect becomes more significant as time moves forward. Since this phenomenon may not be reasonable for some applications, we adopt a bell-shaped curve to reflect the human learning process in our proposed model. Experiments on a real data set for Internet-distributed software has been performed, and the results show that the proposed new model gives better performance in estimating the number of initial faults than previous approaches",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949540,no,undetermined,0
On-the-fly software replacement in faulty remote robots,"Remote robots are too expensive to be abandoned once a fault is detected. Moreover, faults may be overcome by wireless transmission of software replacements. But improper timing and replacements could cause crash and damages rather than system recovery. We designed a dynamic software controller that tracks robot applications, builds and continuously populates relevant data structures, for posterior accurate on-the-fly component replacement. The non-invasive controller implementation and validating tests are described.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736610,no,undetermined,0
An application of diagnostic inference modeling to vehicle health management,"We discuss the approach we have taken to applying diagnostic modeling and associated reasoning techniques to the problem of diagnosing and prognosing faults. as part of vehicle health, management systems. We present a brief background of diagnostic fault modeling based on lessons learned from ongoing research as part of the NASA/FAA Aviation Safety Program. We discuss the application of these techniques and possible implementation scenarios to commercial aircraft health management. We identify information sources available on a typical commercial transport and discuss methods for evaluating them, either singly or in combination, to establish knowledge of the current or predicted health state of the aircraft",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949454,no,undetermined,0
Scientific Computing Autonomic Reliability Framework,"Large scientific computing clusters require a distributed dependability subsystem that can provide fault isolation and recovery and is capable of learning and predicting failures, to improve the reliability of scientific workflows. In this paper, we outline the key ideas in the design of a Scientific Computing Autonomic Reliability Framework (SCARF) for large computing clusters used in the Lattice Quantum Chromo Dynamics project at Fermi Lab.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736792,no,undetermined,0
Design of dual-duplex system and evaluation of RAM,"We develop the dual-duplex system that detects a fault using a hardware comparator which switches to a hot standby redundancy. This system is designed on the basis of MC68000 and can be used in VMEbus. To improve the reliability and safety, the dual-duplex system is designed in double modular redundancy. The failure rate of the electrical element is calculated in MILSPEC-217F by RELEX6.0 tool, and the system RAMS (reliability, availability, maintainability, safety) and MTTF (mean time to failure) are designed by Markov modeling and evaluated by Matlab. Since the dual-duplex system has high reliability, availability, and safety, it can be applied to embedded control systems like airplanes and high-speed railway systems",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948747,no,undetermined,0
An agent-based approach to computer assisted code inspections,"Formal code inspections have been established as an effective way to decrease the cost of software development. However, implementing formal code inspections successfully is a challenging endeavour. We propose that the use of software tools to support the inspection process can help reduce the cost of code inspections, while increasing the number of defects detected. Intelligent agents provide a suitable model for designing a set of intelligent and flexible tools",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948508,no,undetermined,0
On the relationships of faults for Boolean specification based testing,"Various methods of generating test cases based on Boolean specifications have previously been proposed. These methods are fault-based in the sense that test cases are aimed at detecting particular types of faults. Empirical results suggest that these methods are good at detecting particular types of faults. However, there is no information on the ability of these test cases in detecting other types of faults. The paper summarizes the relationships of faults in a Boolean expression in the form of a hierarchy. A test case that detects the faults at the lower level of the hierarchy will always detect the faults at the upper level of the hierarchy. The hierarchy helps us to better understand the relationships of faults in a Boolean expression, and hence to select fault-detecting test cases in a more systematic and efficient manner",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948494,no,undetermined,0
Fault detection and accommodation in dynamic systems using adaptive neuro-fuzzy systems,"Fault detection and accommodation plays a very important role in critical applications. A new software redundancy approach based on all adaptive neuro-fuzzy inference system (ANFIS) is introduced. An ANFIS model is used to detect the fault while another model is used to accommodate it. An accurate plant model is assumed with arbitrary additive faults. The two models are trained online using a gradient-based approach. The accommodation mechanism is based on matching the output of the plant with the output of a reference model. Furthermore, the accommodation mechanism does not assume a special type of system or nonlinearity. Simulation studies prove the effectiveness of the new system even when a severe failure occurs. Robustness to noise and inaccuracies in the plant model are also demonstrated",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=948364,no,undetermined,0
Multiple fault diagnostics for communicating nondeterministic finite state machines,"During the last decade, different methods were developed to produce optimized test sequences for detecting faults in, communication protocol implementations. However, the application of these methods gives only limited information about the location of detected faults. We propose a complementary step, which localizes the faults, once detected. It consists of a generalized diagnostic algorithm for the case where more than one fault may be present in the transitions of a system represented by communicating nondeterministic finite state machines, if existing faults are detected, this algorithm permits the generation of a minimal set of diagnoses, each of which is formed by a set of transitions suspected of being faulty. A simple example is used to demonstrate the functioning of the proposed diagnostic algorithm. The complexity of each step in the algorithm are calculated",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=935446,no,undetermined,0
A knowledge base for program debugging,"We present a Conceptual Model for Software Fault Localization (CMSFL), and an Automated Assistant (AASFL) called BUG-DOCTOR to aid programmers with the problem of software fault localization. A multi-dimensional approach is suggested with both shallow and deep reasoning phases to enhance the probability of localizing many types of faults. BUG-DOCTOR uses these two approaches and switches between them to localize the faults. The AASFL is being developed based on this theoretical model. It is programming language independent, capable of handling different programming styles and implementations",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=934005,no,undetermined,0
Self-aware services: using Bayesian networks for detecting anomalies in Internet-based services,"We propose a general architecture and implementation for the autonomous assessment of the health of arbitrary service elements, as a necessary prerequisite to self-control. We describe a health engine, the central component of our proposed `self-awareness and control' architecture. The health engine combines domain independent statistical analysis and probabilistic reasoning technology (Bayesian networks) with domain dependent measurement collection and evaluation methods. The resultant probabilistic assessment enables open, non-hierarchical communications about service element health. We demonstrate the validity of our approach using HP's corporate email service and detecting email anomalies: mail loops and a virus attack",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918070,no,undetermined,0
ATPG for combinational circuits on configurable hardware,"In this paper, a new approach for generating test vectors that detects faults in combinational circuits is introduced. The approach is based on automatically designing a circuit which implements the D-algorithm, an automatic test pattern generation (ATPG) algorithm, specialized for the combinational circuit. Our approach exploits fine-grain parallelism by performing the following in three clock cycles: direct backward/forward implications, conflict checking, selecting next gate to propagate fault or to justify a line, decisions on gate inputs, and loading the state of the circuit after backup. In this paper, we show the feasibility of this approach in terms of hardware cost and speed and how it compares with software-based techniques.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=920827,no,undetermined,0
Empirical comparison of software-based error detection and correction techniques for embedded systems,"Function Tokens and NOP Fills are two methods proposed by various authors to deal with instruction pointer corruption in microcontrollers, especially in the presence of high electromagnetic interference levels. An empirical analysis to assess and compare these two techniques is presented in this paper. Two main conclusions are drawn: [1] NOP Fills are a powerful technique for improving the reliability of embedded applications in the presence of EMI, and [2] the use of function tokens can lead to a reduction in overall system reliability",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=924681,no,undetermined,0
SmartFISMATM,"Foreign and domestic hackers have been increasingly attacking the U.S. Government computing environments with impunity, bypassing impressively expensive defenses and threatening our capability to defend and support our nation and allies. Adversaries are now appearing as legitimate users to Department of Defense (DoD) applications and networks, while threatening the integrity and confidentiality of DoD information. Attackers are frequently exploiting hardware and software vulnerabilities before DoD can test and disseminate effective patches. The complexity of information technology (IT) management operations and security is a constant challenge for enterprises (both large and small). Balancing the workforcepsilas need for availability and ease of use while complying with the frequent security advisories, bulletins, changes, and reporting requirements can be daunting. The continuous enhancements and upgrades combined with the requirement to react to security threats to both operating systems and applications are overwhelming the routine operational capability for the system and security administrators. Many organizations continue to treat asset management; configuration management; data protection; access control; intrusion prevention; risk analysis; compliance; vulnerability management; certification and accreditation (C&A); incident detection and response; and reporting as isolated processes that rarely, if ever, interact. The stove-piping of these critical network and system operations results in inconsistent views of IT assets and their security postures, inefficient use of resources, and the inability to accurately assess the overall security status of the organization at any given time. Additionally, the C&A and Information Assurance Vulnerability Management (IAVM) processes, along with the annual Federal Information Security Management Act (FISMA) reporting, has become a resource intense, complex, and sometimes unpredictable process. These processes a- - nd procedures are particularly challenging for IT managers in establishing and maintaining a secure computing environment the naval workforce expects without sacrificing quality of service. Smartronix Inc., in conjunction with the Office of Naval Research (ONR), and security product partners Telos Corporation, IBM Internet Security Systems, Inc. and McAfee have developed a solution to address these issues.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4753139,no,undetermined,0
A probabilistic priority scheduling discipline for high speed networks,"In high speed networks, the strict priority (SP) scheduling discipline is perhaps the most common and simplest method to schedule packets from different classes of applications, each with diverse performance requirements. With this discipline, however, packets at higher priority levels can starve packets at lower priority levels. To resolve this starvation problem, we propose to assign a parameter to each priority queue in the SP discipline. The assigned parameter determines the probability with which its corresponding queue is served when the queue is polled by the server. We thus form a new packet scheduling discipline, referred to as the probabilistic priority (PP) discipline. By properly setting the assigned parameters, service differentiation as well as fairness among traffic classes can be achieved in PP. In addition, the PP discipline can be easily reduced to the ordinary SP discipline or to the reverse SP discipline",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=923593,no,undetermined,0
Recognizing geometric patterns for beautification of reconstructed solid models,"Boundary representation models reconstructed from 3D range data suffer from various inaccuracies caused by noise in the data and the model building software. The quality of such models can be improved in a beautification step, which finds regular geometric patterns approximately present in the model and imposes a maximal consistent subset of constraints deduced from these patterns on the model. This paper presents analysis methods seeking geometric patterns defined by similarities. Their specific types are derived from a part survey estimating the frequencies of the patterns in simple mechanical components. The methods seek clusters of similar objects which describe properties of faces, loops, edges and vertices, try to find special values representing the clusters, and seek approximate symmetries of the model. Experiments show that the patterns detected appear to be suitable for the subsequent beautification steps",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=923370,no,undetermined,0
Designing a service of failure detection in asynchronous distributed systems,"Even though introduced for solving the consensus problem in asynchronous distributed systems, the notion of unreliable failure detector can be used as a powerful tool for any distributed protocol in order to get better performance by allowing the usage of aggressive time-outs to detect failures of entities executing the protocol. We present the design of a Failure Detection Service (FDS) based on the notion of unreliable failure detectors introduced by T. Chandra and S. Toueg (1996). FDS is able to detect crashed objects and entities that permanently omit to send messages without imposing changes to the source code of the underlying protocols that use this service. Also, FDS provides an object oriented interface to its subscribers and, more important, it does not add network overhead if no entity subscribes to the service. The paper can be also seen as a first step towards a distributed implementation of a heartbeat-based failure management system as defined in fault-tolerant CORBA specification",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922825,no,undetermined,0
Protected variation: the importance of being closed,"The Pattern Almanac 2000 (Addison Wesley, 2000) lists around 500 software-related patterns, and given this reading list, the curious developer has no time to program! Of course, there are underlying, simplifying themes and principles to this pattern plethora that developers have long considered and discussed. One example is L. Constantine's (1974) coupling and cohesion guidelines. Yet, these principles must continually resurface to help each new generation of developers and architects cut through the apparent disparity in myriad design ideas and help them see the underlying and unifying forces. One such principle, which B. Meyer (1988) describes is the Open-Closed Principle (OCP): modules should be both open (for extension and adaptation) and closed (to avoid modification that affect clients). OCP is essentially equivalent to the Protected Variation (PV) pattern: identify points of predicted variation and create a stable interface around them. OCP and PV formalize and generalize a common and fundamental design principle described in many guises. OCP and PV are two expressions of the same principle: protection against change to the existing code and design at variation and evolution points, with minor differences in emphasis",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922731,no,undetermined,0
An internally replicated quasi-experimental comparison of checklist and perspective based reading of code documents,"The basic premise of software inspections is that they detect and remove defects before they propagate to subsequent development phases where their detection and correction cost escalates. To exploit their full potential, software inspections must call for a close and strict examination of the inspected artifact. For this, reading techniques for defect detection may be helpful since these techniques tell inspection participants what to look for and, more importantly, how to scrutinize a software artifact in a systematic manner. Recent research efforts investigated the benefits of scenario-based reading techniques. A major finding has been that these techniques help inspection teams find more defects than existing state-of-the-practice approaches, such as, ad-hoc or checklist-based reading (CBR). We experimentally compare one scenario-based reading technique, namely, perspective-based reading (PBR), for defect detection in code documents with the more traditional CBR approach. The comparison was performed in a series of three studies, as a quasi experiment and two internal replications, with a total of 60 professional software developers at Bosch Telecom GmbH. Meta-analytic techniques were applied to analyze the data",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922713,no,undetermined,0
Theory of software reliability based on components,"We present a foundational theory of software system reliability based on components. The theory describes how component developers can design and test their components to produce measurements that are later used by system designers to calculate composite system reliability, without implementation and test of the system being designed. The theory describes how to make component measurements that are independent of operational profiles, and how to incorporate the overall system-level operational profile into the system reliability calculations. In principle, the theory resolves the central problem of assessing a component, which is: a component developer cannot know how the component will be used and so cannot certify it for an arbitrary use; but if the component buyer must certify each component before using it, component based development loses much of its appeal. This dilemma is resolved if the component developer does the certification and provides the results in such a way that the component buyer can factor in the usage information later without repeating the certification. Our theory addresses the basic technical problems inherent in certifying components to be released for later use in an arbitrary system. Most component research has been directed at functional specification of software components; our theory addresses the other equally important side of the coin: component quality.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919109,no,undetermined,0
A cross-layer quality driven approach in Web service selection,"In order to make Web services operate in a performance optimal status, it is necessary to make an effective decision on selecting the most suitable service provider among a set Web services that provide identical functions. We argue that the network performance between the service container and service consumer can pose a significant influence to the performance of Web service that the consumer actually receive, while current researches have limited emphasis on this issue. In this paper, we propose a cross-layer approach for Web service selection which takes the network performance issue into consideration during the service selection process. A discrete representation of cross-layer performance correlation is proposed. Based on which, a qualitative reasoning method is introduced to predict the performance at the service user side. The integration of the quality driven Web service selection method to service oriented architecture is also considered. Simulation is designed and experiment results suggest that the new approach significantly improves the accuracy of Web service selection and delivers a performance elevation for Web services.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4746795,no,undetermined,0
Incorporating varying test costs and fault severities into test case prioritization,"Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. In previous work (S. Elbaum et al., 2000; G. Rothermel et al., 1999), we provided a metric, APFD, for measuring rate of fault detection, and techniques for prioritizing test cases to improve APFD, and reported the results of experiments using those techniques. This metric and these techniques, however, applied only in cases in which test costs and fault severity are uniform. We present a new metric for assessing the rate of fault detection of prioritized test cases that incorporates varying test case and fault costs. We present the results of a case study illustrating the application of the metric. This study raises several practical questions that might arise in applying test case prioritization; we discuss how practitioners could go about answering these questions.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919106,no,undetermined,0
Improving software reliability and security with automated analysis,"Static-analysis tools that identify defects and security vulnerabilities in source and executables have advanced significantly over the last few years. A brief description of how these tools work is given. Their strengths and weaknesses in terms of the kinds of flaws they can and cannot detect are discussed. Methods for quantifying the accuracy of the analysis are described, including sources of ambiguity for such metrics. Recommendations for deployment of tools in a production setting are given.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4753207,no,undetermined,0
Investigating the cost-effectiveness of reinspections in software development,"Software inspection is one of the most effective methods to detect defects. Reinspection repeats the inspection process for software products that are suspected to contain a significant number of undetected defects after an initial inspection. As a reinspection is often believed to be less efficient than an inspection an important question is whether a reinspection justifies its cost. In this paper we propose a cost-benefit model for inspection and reinspection. We discuss the impact of cost and benefit parameters on the net gain of a reinspection with empirical data from an experiment in which 31 student teams inspected and reinspected a requirements document. Main findings of the experiment are: a) For reinspection benefits and net gain were significantly lower than for the initial inspection. Yet, the reinspection yielded a positive net gain for most teams with conservative cost-benefit assumptions. B) Both the estimated benefits and number of major defects are key factors for reinspection net gain, which emphasizes the need for appropriate estimation techniques.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919090,no,undetermined,0
Design and implementation of a composable reflective middleware framework,"With the evolution of the global information infrastructure, service providers will need to provide effective and adaptive resource management mechanisms that can serve more concurrent clients and deal with applications that exhibit quality-of-service (QoS) requirements. Flexible, scalable and customizable middleware can be used as an enabling technology for next-generation systems that adhere to the QoS requirements of applications that execute in highly dynamic distributed environments. To enable application-aware resource management, we are developing a customizable and composable middleware framework called CompOSE|Q (Composable Open Software Environment with QoS), based on a reflective meta-model. In this paper, we describe the architecture and runtime environment for CompOSE|Q and briefly assess the performance overhead of the additional flexibility. We also illustrate how flexible communication mechanisms can be supported efficiently in the CompOSE|Q framework",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918995,no,undetermined,0
Optimistic active replication,"Replication is a powerful technique for increasing availability of a distributed service. Algorithms for replicating distributed services do however face a dilemma: they should be: efficient (low latency); while ensuring consistency of the replicas, which are two contradictory goals. The paper concentrates on active replication, where all the replicas handle the clients' requests. Active replication is usually implemented using the atomic broadcast primitive. To be efficient, some atomic broadcast algorithms deliberately sacrifice consistency, if inconsistency is likely to occur with a low probability. We present an algorithm that handles replication efficiently in most scenarios, while preventing inconsistencies. The originality of the algorithm is to take the client-server interaction into account, while traditional solutions consider atomic broadcast as a black box",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918963,no,undetermined,0
Client-transparent fault-tolerant Web service,"Most of the existing fault tolerance schemes for Web servers detect server failure and route future client requests to backup servers. These techniques typically do not provide transparent handling of requests whose processing was in progress when the failure occurred. Thus, the system may fail to provide the user with confirmation for a requested transaction or clear indication that the transaction was not performed. We describe a client-transparent fault tolerance scheme for Web servers that ensures correct handling of requests in progress at the time of server failure. The scheme is based on a standby backup server and simple proxies. The error handling mechanisms of TCP are used to multicast requests to the primary and backup as well as to reliably deliver replies from a server that may fail while sending the reply. Our scheme does not involve OS kernel changes or use of user-level TCP implementations and requires minimal changes to the Web server software",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918654,no,undetermined,0
Automated discovery of information services in heterogeneous distributed networks,"The Global Information Grid (GIG) will be comprised of collections of different service capability domains (SCDs). Each SCD offers a set of information services, such as voice over IP (VoIP), video delivery, and information translation, and is managed as a separate system. The GIG information services will include several types of communications services (e.g., VoIP and streaming video), translation services (e.g., document translation and data translation) and information services (e.g., content discovery and domain name service). These different services may be described using various methods, including specification documents and lookup tables, and will have associated service level agreements (SLAs). As SCDs become richer in their service offerings and more dynamic in their service availability, the discovery of end-to-end services meeting end-user needs becomes extremely challenging. Currently manual methods are employed to map end-user needs to the end-to-end service combinations that GIG deployments can support. Such manual methods are inefficient and error prone. Automation of end-to-end service discovery within the GIG is highly desirable, but is an exceedingly complex task. Current efforts to automate service discovery, for example, the service location protocol or the service oriented architecture, provide service discovery through registration and strict service type definitions. These require coordination across all SCDs for all possible information technology (IT) service offerings. Ideally, individual SCDs would describe their services through individual service description documents. Then mapping of end-user service requests to appropriate collections of SCD services could be performed automatically. This requires the development of semantic reasoning and ontology for service descriptions and service capability matching. This paper describes our approach for automated discovery of information services on GIG-like network deployments.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4753375,no,undetermined,0
Trading off execution time for reliability in scheduling precedence-constrained tasks in heterogeneous computing,"This paper investigates the problem of matching and scheduling of an application, which is composed of tasks with precedence constraints, to minimize both execution time and probability of failure of the application in a heterogeneous computing system. In general, however, it is impossible to satisfy both objectives at the same time because of conflicting requirements. The best one can do is to trade off execution time for reliability or vice versa, according to users' needs. Furthermore, there is a need for an algorithm which can assign tasks of an application to satisfy both of the objectives to some degree. Motivated from these facts, two different algorithms, which are capable of trading off execution time for reliability, are developed. To enable the proposed algorithms to account for the reliability of resources in the system, an expression which gives the reliability of the application under a given task assignment is derived. The simulation results are provided to validate the performance of the proposed algorithms",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=925005,no,undetermined,0
Performance analysis of image compression using wavelets,"The aim of this paper is to examine a set of wavelet functions (wavelets) for implementation in a still image compression system and to highlight the benefit of this transform relating to today's methods. The paper discusses important features of wavelet transform in compression of still images, including the extent to which the quality of image is degraded by the process of wavelet compression and decompression. Image quality is measured objectively, using peak signal-to-noise ratio or picture quality scale, and subjectively, using perceived image quality. The effects of different wavelet functions, image contents and compression ratios are assessed. A comparison with a discrete-cosine-transform-based compression system is given. Our results provide a good reference for application developers to choose a good wavelet compression system for their application",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=925596,no,undetermined,0
X-33 redundancy management system,"The X-33 is an unmanned advanced technology demonstrator with a mission to validate new technologies for the next generation of Reusable Launch Vehicles. Various system redundancies are designed in the X-33 to enhance the probability of successfully completing its mission in the event of faults and failures during flight. One such redundant system is the Vehicle and Mission Computer that controls the X-33 ea, and manages the avionics subsystems. Historically, redundancy management and applications such as flight control and vehicle management tended to be highly coupled. One of the technologies that the X-33 will demonstrate is the Redundancy Management System (RMS) that uncouples the applications from the redundancy management details, in the same way that real-time operating systems have uncoupled applications from task scheduling, communication and synchronization details",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=925693,no,undetermined,0
Assessing the quality of auction Web sites,"WebQual is an instrument for assessing the quality of Internet sites from the perspective of the customer. Earlier versions of WebQual focused on information and interaction quality. This paper reports on a new version of WebQual that incorporates three quality dimensions: information quality, interaction quality and Web site design quality. WebQual is applied in the domain of Internet auctions and the results are used to assess the reliability of the instrument for assessing the quality of Web sites. Three auction sites (Amazon, eBay and QXL) are evaluated through an intervention that involves buying and selling at auction. The results of the intervention are analyzed quantitatively to assess the validity of the WebQual instrument and supplemented by qualitative data that is used to consider the relative merits of the three sites evaluated.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=927087,no,undetermined,0
Optimal distributed generation allocation in MV distribution networks,"The necessity for flexible electric systems, changing regulatory and economic scenarios, energy savings and environmental impact are providing impetus to the development of distributed generation (DG), which is predicted to play an increasing role in the electric power system of the future. With so much new distributed generation being installed, it is critical that the power system impacts be assessed accurately so that DG can be applied in a manner that avoids causing degradation of power quality, reliability and control of the utility system. For these reasons, the paper proposes a new software procedure, based on a genetic algorithm, capable of establishing the optimal distributed generation allocation on an existing MV distribution network, considering all the technical constraints, like feeder capacity limits, feeder voltage profile and three-phase short circuit current in the network nodes",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=932323,no,undetermined,0
Intrusion tolerant software architectures,"The complexity of the software systems built today virtually guarantees the existence of security vulnerabilities. When the existence of specific vulnerabilities becomes known - typically as a result of detecting a successful attack - intrusion prevention techniques such as firewalls and anti-virus software seek to prevent future attackers from exploiting these vulnerabilities. However, vulnerabilities cannot be totally eliminated, their existence is not always known and preventing mechanisms cannot always be built. Intrusion tolerance is a new concept, a new design paradigm, and potentially a new capability for dealing with residual security vulnerabilities. In this article, we describe our initial exploration of the hypothesis that intrusion tolerance is best designed and enforced at the software architecture level",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=932175,no,undetermined,0
An IT Body of Knowledge: The Key to an Emerging Profession,"The information technology body of knowledge (BOK) is reviewed for its support to IT as an emerging profession. The author discusses the IT BOK for the important roles it can fulfill in support of education, certification, professional stature, professional development, and organizational improvement. Efforts to develop and maintain the IT BOK also have beneficial side effects, such as focusing attention on global perceptions and practices and keeping the characterization of IT current. The author offers recommendations for IT professionals to enhance the IT BOK by participating in its development, experimenting with ways to represent it more effectively, and assessing the potential benefits of creating a new BOK oriented to IT professional practice.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4747650,no,undetermined,0
Multiphysic modeling and design of carbon nanotubes based variable capacitors for microwave applications,"This paper describes the multiphysic methodology developed to design carbon nanotubes (CNT) based variable capacitor (varactor). Instead of using classical RF-MEMS design methodologies; we take into account the real shape of the CNT, its nanoscale dimensions and its real capacitance to ground. A capacitance-based numerical algorithm has then been developed in order to predict the pull-in voltage and the RF-capacitance of the CNT-based varactor. This software, which has been validated by measurements on various devices, has been used to design varactor device for which 20V of actuation voltage has been predicted. We finally extend the numerical modeling to describe the electromagnetical behavior of the devices. The RF performances has also been efficiently predicted and the varactor (in parallel configuration) exhibits predicted losses of 0.3 dB at 5 GHz and quality factor of 22at 5 GHz, which is relevant for high quality reconfigurable circuits requirements where as the expected sub-microsecond switching time range opens the door to real time tunability.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4751672,no,undetermined,0
An integrated diagnostics virtual test bench for life cycle support,"Qualtech Systems, Inc. (QSI) has developed an architecture that utilizes the existing TEAMS (Testability Engineering and Maintenance Systems) integrated tool set as the foundation to a computing environment for modeling and rigorous design analysis. This architecture is called a Virtual Test Bench (VTB) for Integrated Diagnostics. The VTB approach addresses design for testability, safety, and risk reduction because it provides an engineering environment to develop/provide: 1. Accurate, comprehensive, and graphical model based failure mode, effects and diagnostic analysis to understand failure modes, their propagation, effects, and ability of diagnostics to address these failure modes. 2. Optimization of diagnostic methods and test sequencing supporting the development of an effective mix of diagnostic methods. 3. Seamless integration from analysis, to run-time implementation, to maintenance process and life cycle support. undetected fault lists, ambiguity group lists, and optimized diagnostic trees. 4. A collaborative, widely distributed engineering environment to """"ring-out"""" the design before it is built and flown. The VTB architecture offers an innovative solution in a COTS package for system/component modeling, design for safety, failure mode/effect analysis, testability engineering, and rigorous integration/testing of the IVHM (Integrated Vehicle Health Management) function with the rest of the vehicle. The VTB approach described in this paper will use the TEAMS software tool to generate detailed, accurate """"failure"""" models of the design, assess the propagation of the failure mode effects, and determine the impact on safety, mission and support costs. It will generate FMECA, mission reliability assessments, incorporate the diagnostic and prognostic test designs, and perform testability analysis. Diagnostic functions of the VTB include fault detection and isolation metrics undetected fault lists, ambiguity group lists, and optimized diagnostic trees",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931400,no,undetermined,0
A general prognostic tracking algorithm for predictive maintenance,"Prognostic health management (PHIM) is a technology that uses objective measurements of condition and failure hazard to adaptively optimize a combination of availability, reliability, and total cost of ownership of a particular asset. Prognostic utility for the signature features are determined by transitional failure experiments. Such experiments provide evidence for the failure alert threshold and of the likely advance warning one can expect by tracking the feature(s) continuously. Kalman filters are used to track changes in features like vibration levels, mode frequencies, or other waveform signature features. This information is then functionally associated with load conditions using fuzzy logic and expert human knowledge of the physics and the underlying mechanical systems. Herein is the greatest challenge to engineering. However, it is straightforward to track the progress of relevant features over time using techniques such as Kalman filtering. Using the predicted states, one can then estimate the future failure hazard, probability of survival, and remaining useful life in an automated and objective methodology",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931317,no,undetermined,0
Advanced test cell diagnostics for gas turbine engines,"Improved test cell diagnostics capable of detecting and classifying engine mechanical and performance faults as well as instrumentation problems is critical to reducing engine operating and maintenance costs while optimizing test cell effectiveness. Proven anomaly detection and fault classification techniques utilizing engine Gas Path Analysis (GPA) and statistical/empirical models of structural and performance related engine areas can now be implemented for real-time and post-test diagnostic assessments. Integration and implementation of these proven technologies into existing USAF engine test cells presents a great opportunity to significantly improve existing engine test cell capabilities to better meet today's challenges. A suite of advanced diagnostic and troubleshooting tools have been developed and implemented for gas turbine engine test cells as part of the Automated Jet Engine Test Strategy (AJETS) program. AJETS is an innovative USAF program for improving existing engine test cells by providing more efficient and advanced monitoring, diagnostic and troubleshooting capabilities. This paper describes the basic design features of the AJETS system; including the associated data network, sensor validation and anomaly detection/diagnostic software that was implemented in both a real-time and post-test analysis mode. These advanced design features of AJETS are currently being evaluated and advanced utilizing data from TF39 test cell installations at Travis AFB and Dover AFB",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931313,no,undetermined,0
Advances in computational resiliency,"The notion of computational resiliency refers to the ability of a distributed application to tolerate intrusion when under information warfare (IW) attack. It is one of several new technologies under development by the U.S. Air Force that aim to harden the battlefield information structure from an IW perspective. These technologies seek to strengthen a military mission, rather than protect its network infrastructure using static defensive measures such as network security, intrusion sensors, and firewalls. Even if an IW attack is never detected, it should be possible to continue information operations and achieve mission objectives. Computational resiliency involves the dynamic use of replication, guided by mission policy, to achieve intrusion tolerance. However, it goes further to dynamically regenerate replication in response to an IW attack, allowing the level of system assurance to be maintained. Replicated structures are protected through several techniques such as camouflage, dispersion, and layered security policy. This paper describes a prototype concurrent programming technology that we have developed to support computational resiliency. Brief outlines describe how the library has been applied to prototypical applications",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931303,no,undetermined,0
Reversi: Post-silicon validation system for modern microprocessors,"Verification remains an integral and crucial phase of todaypsilas microprocessor design and manufacturing process. Unfortunately, with soaring design complexities and decreasing time-to-market windows, todaypsilas verification approaches are incapable of fully validating a microprocessor before its release to the public. Increasingly, post-silicon validation is deployed to detect complex functional bugs in addition to exposing electrical and manufacturing defects. This is due to the significantly higher execution performance offered by post-silicon methods, compared to pre-silicon approaches. Validation in the post-silicon domain is predominantly carried out by executing constrained-random test instruction sequences directly on a hardware prototype. However, to identify errors, the state obtained from executing tests directly in hardware must be compared to the one produced by an architectural simulation of the designpsilas golden model. Therefore, the speed of validation is severely limited by the necessity of a costly simulation step. In this work we address this bottleneck in the traditional flow and present a novel solution for post-silicon validation that exposes its native high performance. Our framework, called Reversi, generates random programs in such a way that their correct final state is known at generation time, eliminating the need for architectural simulations. Our experiments show that Reversi generates tests exposing more bugs faster, and can speed up post-silicon validation by 20x compared to traditional flows.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4751878,no,undetermined,0
Reconfigurable semi-virtual computer architecture for long available small space vehicles,"This paper presents a new hardware architecture for a hybrid space computer composed of both physical and virtual processors. The architecture emulates a multiple modular computer, including both physical and virtual spares, with a small amount of physical processors (flight computer) and virtual redundancies (payload processors). The flight computer contains a main processor, as well as a backup and a redundant processor. However, the instrumentation for the Satex mission also includes a redundant LAN with autonomous capabilities to detect its failures, to reconfigure by itself and to provide on-line maintenance by automated means. Communications between flight computer and payload microcomputers are accomplished over this LAN, allowing a versatile operating behavior in terms of data communication as well as in terms of distributed fault tolerance. Under this scenario a semi-virtual expanded flight architecture is periodically implemented in the microsatellite in order to emulate a bigger and safer computer with increased fault-tolerant features. Previous topology is conformed periodically aiming at failure detection, fault isolation, and hardware reconfiguration of processors to obtain high availability; moreover, the architecture can be applied in any small space vehicle. The paper also concerns with fault containment regions, Byzantine majority voting mechanisms, reconfiguration procedures, hardware protections, hardware and software diversity and flight computer interface with satellite instrumentation",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931179,no,undetermined,0
Global scheduling for flexible transactions in heterogeneous distributed database systems,"A heterogeneous distributed database environment integrates a set of autonomous database systems to provide global database functions. A flexible transaction approach has been proposed for the heterogeneous distributed database environments. In such an environment, flexible transactions can increase the failure resilience of global transactions by allowing alternate (but in some sense equivalent) executions to be attempted when a local database system fails or some subtransactions of the global transaction abort. We study the impact of compensation, retry, and switching to alternative executions on global concurrency control for the execution of flexible transactions. We propose a new concurrency control criterion for the execution of flexible and local transactions, termed F-serializability, in the error-prone heterogeneous distributed database environments. We then present a scheduling protocol that ensures F-serializability on global schedules. We also demonstrate that this scheduler avoids unnecessary aborts and compensation",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=929901,no,undetermined,0
"Low-cost, software-based self-test methodologies for performance faults in processor control subsystems","A software-based testing methodology for processor control subsystems, targeting hard-to-test performance faults in high-end embedded and general-purpose processors, is presented. An algorithm for directly controlling, using the instruction-set architecture only, the branch-prediction logic, a representative example of the class of processor control subsystems particularly prone to such performance faults, is outlined. Experimental results confirm the viability of the proposed methodology as a low-cost and effective answer to the problem of hard-to-test performance faults in processor architectures",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=929769,no,undetermined,0
A frame-level measurement apparatus for performance testing of ATM equipment,"Performance testing of ATM equipment is here dealt with. In particular, the attention is paid to frame-level metrics, recently proposed by the ATM forum because of their suitability to reflect user-perceived performance better than traditional cell-level metrics. Following the suggestions of the ATM forum, more and more network engineers and production managers are nowadays interested in these metrics, thus increasing the need of instruments and measurement solutions appropriate to their estimation. Trying to satisfy this exigency, a new VXI-based measurement apparatus is proposed in the paper. The apparatus features a suitable software, developed by the authors, which allows the evaluation of the aforementioned metrics by making simply use of common ATM analyzers; only two VXI line interfaces, capable of managing both the physical and ATM layer, are, in fact, adopted. At first, some details about the hierarchical structure of the ATM technology as used as the main differences between frames, peculiar to the ATM adaptation layer, and cells characterizing the lower ATM layer are given. Then, both the hardware and software solutions of the measurement apparatus are described in detail with a particular attention to the measurement procedures implemented. At the end the performance of a new ATM device, developed by Ericsson, is assessed in terms of frame-level metrics by means of the proposed apparatus",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=929479,no,undetermined,0
Design and development of a digital multifunction relay for generator protection,This paper presents the design and development of a rotor earth fault protection function as part of a multifunction generator protection relay. The relay design is based on a low frequency square wave injection method in detecting rotor earth faults. The accuracy of rotor earth fault resistance measurement is improved by applying piecewise quadratic approximation to the nonlinear gain characteristic of the measurement circuit. The paper also presents the hardware and software architecture of the relay,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=929370,no,undetermined,0
New approach to the modeling of Command and Control Information Systems,"Probability of military activities successes in great degree depends on the qualities of their tactical planning processes. A very important part of tactical planning is the planning of a command and control information systempsilas (C2IS) communication infrastructure, respectively tactical communication networks. There are several known software tools that assist the process of tactical planning. However, they are often useless because different armies use different information and communication technologies. This is the main reason we started to develop a simulation system that helps in the process of planning tactical communication networks. Our solution is based on the well-known OPNET modeler simulation environment, which is also used for some other solutions in this area. In addition to the simulation and modeling methodologies we have also developed helper software tools. TPGEN is a tool which enables the user-friendly entering and editing of tactical network models. It performs mapping from C2IS and tactical communication network descriptions to an OPNET simulation modelpsilas parameters. Because the simulation results obtained by an OPNET modeler are user-unfriendly and need expert knowledge when analyzing them, we have developed an expert system for the automatic analysis of simulation results. One of outputs from this expert system is the user-readable evaluation of tactical networkspsila performances with guidance on how to improve the network. Another output is formatted to use in our tactical player. This tactical player is an application which helps when visualizing simulation and expert system results. It is also designed to control an OPNET history player in combination with 3DNV visualization of a virtual terrain. This developed solution, in user-friendly way, helps in the process of designing and optimizing tactical networks.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4753134,no,undetermined,0
Code simulation concept for S/390 processors using an emulation system,"An innovative simulation concept has been developed for the IBM S/390 system of the year 2000 in the area of microcode verification. The goal is to achieve a long-term improvement in the quality of the delivered microcode, detecting and solving the vast majority of code problems in simulation before the system is first powered on. The number of such problems has a major impact on the time needed during system integration to bring the system up from power on to general availability. Within IBM, this is the first time that much a code simulation concept has been developed and implemented. Our element of that concept is the usage of a large emulation system for hardware/software co-verification",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889568,no,undetermined,0
Behavioral-level test vector generation for system-on-chip designs,"Co-design tools represent an effective solution for reducing costs and shortening time-to-market, when system-on-chip design is considered. In a top-down design flow, designers would greatly benefit from the availability of tools able to automatically generate test sequences, which can be reused during the following design steps, from the system-level specification to the gate-level description. This would significantly increase the chance of identifying testability problems early in the design flow, thus reducing the costs and increasing the final product quality. The paper proposes an approach for integrating the ability to generate test sequences into an existing co-design tool. Preliminary experimental results are reported, assessing the feasibility of the proposed approach",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889554,no,undetermined,0
JavaSymphony: a system for development of locality-oriented distributed and parallel Java applications,"Most Java-based systems that support portable parallel and distributed computing either require the programmer to deal with intricate low-level details of Java which can be a tedious, time-consuming and error-prone task, or prevent the programmer from controlling locality of data. In this paper we describe JavaSymphony, a programming paradigm for distributed and parallel computing that provides a software infrastructure for wide classes of heterogeneous systems ranging from small-scale cluster computing to large scale wide-area meta-computing. The software infrastructure is written entirely in Java and runs on any standard compliant Java virtual machine. In contrast to most existing systems, JavaSymphony provides the programmer with the flexibility to control data locality and load balancing by explicit mapping of objects to computing nodes. Virtual architectures are specified to impose a virtual hierarchy on a distributed system of physical computing nodes. Objects can be mapped and dynamically migrated to arbitrary components of virtual architectures. A high-level API to hardware/software system parameters is provided to control mapping, migration, and load balancing of objects. Objects can interact through synchronous asynchronous and one-sided method invocation. Selective remote class loading may reduce the overall memory requirement of an application. Moreover; objects can be made persistent by explicitly storing and loading objects to/from external storage. A prototype of the JavaSymphony software infrastructure has been implemented. Preliminary experiments on a heterogeneous cluster of workstations are described that demonstrate reasonable performance values",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889023,no,undetermined,0
A new rolling stock architecture using safety computers and networks,"Many embedded systems carrying out safety functions in railway applications are still operated with conventional relay-based systems, at the expense of kilometers of wiring in each train. To cope with this issue, Technicatome has developed a demonstrator for RATP (the Paris subway company) to assess pros and cons of alternative architectures based on interconnected digital systems. This demonstrator is currently in operation on an MF 88 train set. The paper summarizes the main features of this experiment",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857529,no,undetermined,0
A multiresolution model for image interpolation with adaptive filtering,"This paper proposes an image interpolation model based on a multiresolution scheme. The method uses two different known image resolutions to establish a mapping between them. The mapping is then used to extrapolate the unknown resolution to produce an expanded image with increased definition and with a mean square reconstruction error lower than for the conventional methods. In other words, details in a high resolution image are implicitly predicted based only on its low-frequency resolutions. A high subjective quality is obtained due to the similar nature of the mappings, which are constructed by means of an adaptive bidimensional FIR filter",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=859275,no,undetermined,0
An analysis of a supply chain management agent architecture,"The authors illustrate a methodology for early agent systems architecture analysis and evaluation with the focus on risk identification, evaluation, and mitigation. Architectural decisions on software qualities such as performance, modifiability, and security can be assessed. The illustration is drawn from the supply chain management application domain",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=858495,no,undetermined,0
Identifying factors influencing reliability of professional systems,"Modern product development strategies call for a more proactive approach to fight intense global competition in terms of technological innovation, shorter time to market, quality and reliability and accommodative price. From a reliability engineering perspective, development managers would like to estimate as early as possible how reliably the product is going to behave in the field, so they can then focus on system reliability improvement. To steer such a reliability driven development process, one of the important aspects in predicting the reliability behavior of a new product, is to know the factors that may influence its field performance. In this paper, two methods are proposed for identifying reliability factors and their significance in influencing the reliability of the product.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4925771,no,undetermined,0
Probabilistic analysis of safety-critical adaptive systems with temporal dependences,"Dynamic adaptation means that components are reconfigured at run time. Consequently, the degree to which a system fulfils its functional and safety requirements depends on the current system configuration at run time. The probability of a violation of functional requirements in combination with an importance factor for each requirement gives us a measure for reliability. In the same way, the degree of violation of safety requirements can be a measure for safety. These measures can easily be derived based on the probabilities of possible system configurations. For this purpose, we are introducing a new probabilistic analysis technique that determines configuration probabilities based on Fault trees, Binary Decision Diagrams (BDDs) and Markov chains. Through our recent work we have been able to determine configuration probabilities of systems but we neglected timing aspects . Timing delays have impact on the adaptation behavior and are necessary to handle cyclic dependences. The contribution of the present article is to extend analysis towards models with timing delays. This technique builds upon the Methodologies and Architectures for Runtime Adaptive Systems (MARS) , a modeling concept we use for specifying the adaptation behavior of a system at design time. The results of this paper determine configuration probabilities, that are necessary to quantify the fulfillment of functional and safety requirements by adaptive systems.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4925786,no,undetermined,0
Formal modeling and analysis of atomic commitment protocols,"The formal specification and mechanical verification of an atomic commitment protocol (ACP) for distributed real-time and fault-tolerant databases is presented. As an example, the non-blocking ACP of Babaoglu and Toueg (1993) is analyzed. An error in their termination protocol for recovered participants has been detected. We propose a new termination protocol which has been proved correct formally. To stay close to the original formulation of the protocol, timed state machines are used to specify the processes, whereas the communication mechanism between processes is defined using assertions. Formal verification has been performed incrementally: adding recovery from crashes only after having proved the basic protocol. The verification system PVS was used to deal with the complexity of this fault-tolerant protocol",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857694,no,undetermined,0
Designing high-performance and reliable superscalar architectures-the out of order reliable superscalar (O3RS) approach,"As VLSI geometry continues to shrink and the level of integration increases, it is expected that the probability of faults, particularly transient faults, will increase in future microprocessors. So far, fault tolerance has chiefly been considered for special purpose or safety critical systems, but future technology will likely require integrating fault tolerance techniques into commercial systems. Such systems require low cost solutions that are transparent to the system operation and do not degrade overall performance. This paper introduces a new superscalar architecture, termed as 03RS that aims to incorporate such simple fault tolerance mechanisms as part of the basic architecture",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857578,no,undetermined,0
Joint evaluation of performance and robustness of a COTS DBMS through fault-injection,"Presents and discusses observed failure modes of a commercial off-the-shelf (COTS) database management system (DBMS) under the presence of transient operational faults induced by SWIFI (software-implemented fault injection). The Transaction Processing Performance Council (TPC) standard TPC-C benchmark and its associated environment is used, together with fault-injection technology, building a framework that discloses both dependability and performance figures. Over 1600 faults were injected in the database server of a client/server computing environment built on the Oracle 8.1.5 database engine and Windows NT running on COTS machines with Intel Pentium processors. A macroscopic view on the impact of faults revealed that: (1) a large majority of the faults caused no observable abnormal impact in the database server (in 96% of hardware faults and 80% of software faults, the database server behaved normally); (2) software faults are more prone to letting the database server hang or to causing abnormal terminations; (3) up to 51% of software faults lead to observable failures in the client processes",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857547,no,undetermined,0
Executable assertions for detecting data errors in embedded control systems,"In order to be able to tolerate the effects of faults, we must first detect the symptoms of faults, i.e. the errors. This paper evaluates the error detection properties of an error detection scheme based on the concept of executable assertions aiming to detect data errors in internal signals. The mechanisms are evaluated using error injection experiments in an embedded control system. The results show that using the mechanisms allows one to obtain a fairly high detection probability for errors in the areas monitored by the mechanisms. The overall detection probability for errors injected to the monitored signals was 74%, and if only errors causing failure are taken into account we have a detection probability of over 99%. When subjecting the target system to random error injections in the memory areas of the application, i.e., not only the monitored signals, the detection probability for errors that cause failure was 81%",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857510,no,undetermined,0
Validation of an approach for improving existing measurement frameworks,"Software organizations are in need of methods to understand, structure, and improve the data their are collecting. We have developed an approach for use when a large number of diverse metrics are already being collected by a software organization (M.G. Mendonca et al., 1998; M.G. Mendonca, 1997). The approach combines two methods. One looks at an organization's measurement framework in a top-down goal-oriented fashion and the other looks at it in a bottom-up data-driven fashion. The top-down method is based on a measurement paradigm called Goal-Question-Metric (GQM). The bottom-up method is based on a data mining technique called Attribute Focusing (AF). A case study was executed to validate this approach and to assess its usefulness in an industrial environment. The top-down and bottom-up methods were applied in the customer satisfaction measurement framework at the IBM Toronto Laboratory. The top-down method was applied to improve the customer satisfaction (CUSTSAT) measurement from the point of view of three data user groups. It identified several new metrics for the interviewed groups, and also contributed to better understanding of the data user needs. The bottom-up method was used to gain new insights into the existing CUSTSAT data. Unexpected associations between key variables prompted new business insights, and revealed problems with the process used to collect and analyze the CUSTSAT data. The paper uses the case study and its results to qualitatively compare our approach against current ad hoc practices used to improve existing measurement frameworks.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852739,no,undetermined,0
Intermittent faults and effects on reliability of integrated circuits,"A significant amount of research has been aimed at analyzing the effects of high energy particles on semiconductor devices. However, less attention has been given to the intermittent faults. Field collected data and failure analysis results presented in this paper clearly show intermittent faults are a major source of errors in modern integrated circuits. The root cause for these faults ranges from manufacturing residuals to oxide breakdown. Burstiness and high error rates are specific manifestations of the intermittent faults. They may be activated and deactivated by voltage, frequency, and operating temperature variations. The aggressive scaling of semiconductor devices and the higher circuit complexity are expected to increase the likelihood of occurrence of the intermittent faults, despite the extensive use of fault avoidance techniques. Herein we discuss the effectiveness of several fault tolerant approaches, taking into consideration the specifics of the errors generated by intermittent faults. Several solutions, previously proposed for handling particle induced soft errors, are exclusively based on software and too slow for handling large bursts of errors. As a result, hardware implemented fault tolerant techniques, such as error detecting and correcting codes, self checking, and hardware implemented instruction retry, are necessary for mitigating the impact of the intermittent faults, both in the case of microprocessors, and other complex integrated circuits.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4925824,no,undetermined,0
A methodology for validating digital circuits with mutation testing,"This paper proposes a systematic methodology for improving functional validation vectors developed to check digital circuits. This method exploits the mutation testing concept originally proposed for software validation. Mutation injects specific functional transformations in circuit descriptions expressed in languages like VHDL or Verilog. These programs, called mutant, are syntactically correct but functionally incorrect. Knowing how these vectors detect functional faults improves the confidence in the design and provide information on the coverage of validation vectors. The paper identifies limits of previous work on mutation testing applied to hardware and proposes method that are better suited to the task",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=857100,no,undetermined,0
A low-cost power quality meter for utility and consumer assessments,"Power quality has become a major concern, in particular, to the domestic consumers because the reliability, efficiency and liability of their devices very much rely on the quality of the electric supply. However, up to now, there is no cost-effective instrument for power quality measurement. It is partially due to the lacking of a world-wide accepted indicator, power performance index (PPI) in the authors' case, and partially due to the conventionally high cost of power quality measurement equipment. In this paper, a good performance index, PPI, is proposed for the consideration by the power industry. Individual components arriving at the PPI are defined and the method of calculation is explained in detail. Such PPI will also be useful to assess the supply quality of independent power providers, especially for deregulation purposes. All algorithms have been implemented on a cost-effective power quality meter (PQM) while both hardware and software designs are described in this paper",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=855645,no,undetermined,0
A methodology for quantitative evaluation of software reliability using static analysis,"This paper proposes a methodology for quantitative evaluation of software reliability in updated COTS or Open Source components. The model combines static analysis of existing source code modules, limited testing with execution path capture, and a series of Bayesian Belief Networks. Static analysis is used to detect faults within the source code which may lead to failure. Code coverage is used to determine which paths within the source code are executed as well as their execution rate. A series of Bayesian Belief Networks is then used to combine these parameters and estimate the reliability for each method. A second series of Bayesian Belief Networks then combines the module reliabilities to estimate the net software reliability. A proof of concept for the model is provided, as the model is applied to five different open-source applications and the results are compared with reliability estimates using the STREW (Software Testing and Early Warning) metrics. The model is shown to be highly effective and the results are within the confidence interval for the STREW reliability calculations, and typically the results differed by less than 2%. This model offers many benefits to practicing software engineers. Through the usage of this model, it is possible to quickly assess the reliability of a given release of a software module supplied by an external vendor to determine whether it is more or less reliable than a previous release. The determination can be made independent of any knowledge of the developer's software development process and without any development metrics.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4925829,no,undetermined,0
A New Methodology for the Test of SoCs and for Analyzing Elusive Failures,The increasing complexity of SoCs in form of more complex architecture designs and smaller structures qualifies test procedures and failure analysis as one of the key skills in the semiconductor industry. In this contribution we show a new SoC test methodology which significantly increases the test coverage of a SoC. The integrated system integrity control functionality of the hid ICE approach observes a SoC core and is able to detect irregularities in comparison to a reference system. In case of a failure detection an exhaustive trace is available which helps to identify the root cause. Especially in multi-core systems the interference between different subsystems can be tested under real operating conditions. Also the effort to identify and analyze elusive failures will be reduced.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070929,no,undetermined,0
Worst-case execution times analysis of MPEG-2 decoding,"Presents the first worst-case execution times (WCET) analysis of MPEG decoding. Solutions for two scenarios-video-on-demand (VoD) and live-are presented, serving as examples for a variety of real-world applications. A significant reduction of over-estimations (down to 17%, including overheads) during WCET analysis of the live scenario can be achieved by using our new two-phase decoder with built-in WCET analysis, which can be universally applied. It is even possible to predict the exact execution times in the VoD scenario. This work is motivated by the fact that media streaming service providers are under great pressure to fulfil the quality of service promised to their customers, preferably in an efficient way",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=853994,no,undetermined,0
QoS mapping along the protocol stack: discussion and preliminary results,"Quality of service (QoS) mechanisms are needed in integrated networks so that the performance requirements of heterogeneous applications are met. We outline a framework for predicting end-to-end QoS at the application layer based on mapping of QoS guarantees across layers in the protocol stack and concatenation of guarantees across multiple dissimilar sub-networks. Mapping is needed to translate QoS guarantees provided in the lower layers into their effects on upper-layer performance indicators. We illustrate the process with some preliminary results for QoS mapping due to segmentation of packets; we conduct a worst-case analysis, generating a closed-form mapping of delay and losses between two layers. Future extensions of this work will take into account flow aggregation processes and the combination of quantitative and qualitative guarantees",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=853592,no,undetermined,0
Finding faces in wavelet domain for content-based coding of color images,"Human face images form the important database in police departments, banks, security kiosks, and they are also found in abundance in day-to-day life. In these databases the important content, of course, is the face region. We present a highly efficient system that detects the human faces in the wavelet transform for discriminative quantization to achieve high perceptual quality content-based image coding technique. The proposed method gives superior subjective performance over JPEG without sacrificing the performance in the rate-distortion spectrum",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=859309,no,undetermined,0
A randomness test based on T-codes,"In this paper, a new randomness test is proposed based on the T-complexity of a T-code, which is a variable-length self-synchronizing code introduced by Titchener in 1984. The proposed test can be used instead of the Lempel-Ziv compression test, which was removed from the NIST statistical test suite because the LZ-complexity has a defect such that its distribution of P-values is strictly discrete for random sequences of length 10<sup>6</sup>. We show that T-complexity has almost ideal continuous distribution of P-values for the same sequences. In order to calculate T-complexity, a new T-decomposition algorithm is also proposed to realize forward parsing for a given sequence although the original T-decomposition uses backward parsing. Furthermore, it is shown that the proposed randomness test can detect undesirable pseudorandom numbers that the NIST statistical test suite cannot detect.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4895570,no,undetermined,0
Localized watermarking: methodology and application to template mapping,"The semiconductor industry has adopted the intellectual property (IP) business model as a dominant system-on-chip development platform. Since copyright fraud has been recognized as the most devastating obstruction to this model, a number of techniques for IP protection have been introduced. Most of them rely on a selection of a global solution to an optimization problem according to a unique user-specific digital signature. Although such techniques may provide convincing proof of authorship with little hardware overhead, they fail to protect design partitions, do not provide an easy procedure for watermark detection, and are not capable of detecting the watermark when the design or its part is augmented in another larger design. Since these demands are of the highest interest for the IP business, we introduce localized watermarking as an IP protection technique which enables these features while satisfying the demand for low-cost and transparency. We have applied the new watermarking technology to template mapping, a behavioral synthesis task. This watermarking method has been tested on a set of real-life benchmarks where high likelihood of authorship has been achieved with negligible overhead in solution quality",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=860089,no,undetermined,0
DSP core verification using automatic test case generation,"The verification methodology for a TMS320C25 compatible embedded DSP core is described. The DSP core has been implemented in synthesizable VHDL and has been cosimulated with the original DSP to verify correct behavior. Automatic test case generation together with hand-crafted code has been used as a means of providing stimuli to achieve increased RTL-simulation coverage. The cosimulation environment for this verification and the process of automatic test case generation is described in detail. Experimental results in terms of simulation coverage are discussed. Finally, a classification of all identified design flaws in the implementation is given and error-prone parts of the HDL design are identified",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=860098,no,undetermined,0
A case study in root cause defect analysis,"There are three interdependent factors that drive our software development processes: interval, quality and cost. As market pressures continue to demand new features ever more rapidly, the challenge is to meet those demands while increasing, or at least not sacrificing, quality. One advantage of defect prevention as an upstream quality improvement practice is the beneficial effect it can have on interval: higher quality early in the process results in fewer defects to be found and repaired in the later parts of the process, thus causing an indirect interval reduction. We report a retrospective root cause defect analysis study of the defect Modification Requests (MRs) discovered while building, testing, and deploying a release of a transmission network element product. We subsequently introduced this analysis methodology into new development projects as an in-process measurement collection requirement for each major defect MR. We present the experimental design of our case study discussing the novel approach we have taken to defect and root cause classification and the mechanisms we have used for randomly selecting the MRs to analyze and collecting the analyses via a Web interface. We then present the results of our analyses of the MRs and describe the defects and root causes that we found, and delineate the countermeasures created to either prevent those defects and their root causes or detect them at the earliest possible point in the development process. We conclude with lessons learned from the case study and resulting ongoing improvement activities",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870433,no,undetermined,0
A replicated assessment and comparison of common software cost modeling techniques,"Delivering a software product on time, within budget, and to an agreed level of quality is a critical concern for many software organizations. Underestimating software costs can have detrimental effects on the quality of the delivered software and thus on a company's business reputation and competitiveness. On the other hand, overestimation of software cost can result in missed opportunities to funds in other projects. In response to industry demand, a myriad of estimation techniques has been proposed during the last three decades. In order to assess the suitability of a technique from a diverse selection, its performance and relative merits must be compared. The current study replicates a comprehensive comparison of common estimation techniques within different organizational contexts, using data from the European Space Agency. Our study is motivated by the challenge to assess the feasibility of using multi-organization data to build cost models and the benefits gained from company-specific data collection. Using the European Space Agency data set, we investigated a yet unexplored application domain, including military and space projects. The results showed that traditional techniques, namely, ordinary least-squares regression and analysis of variance outperformed analogy-based estimation and regression trees. Consistent with the results of the replicated study no significant difference was found in accuracy between estimates derived from company-specific data and estimates derived from multi-organizational data",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870428,no,undetermined,0
Object model resurrection-an object oriented maintenance activity,"This paper addresses the problem of reengineering object-oriented systems that have incurred increased maintenance cost due to long development time-span and project lifecycle. When an Incremental Approach is used to develop an object-oriented system, there is a risk that the class design and the overall object model will deteriorate in quality with each increment. A recent research work suggested a process activity (Class Deterioration Detection and Resurrection-CDDR process activity) and a technique for the detection and resurrection of deteriorated classes. That work focussed on one particular aspect of object-oriented software maintenance-Class Quality Deterioration due to lack of cohesion induced by high coupling. This paper addresses the problem of deteriorating object-oriented design due to code and class growth (increase in the number of classes) within a system. A Code/Class Growth Control process activity (CGC) is suggested to avoid and eliminate Repetitious Code and Classes within the evolving system. The CDDR and CGC process activities are used to build an evolving Maintenance process model for object-oriented systems. The presented maintenance process model is an effective way to periodically assess and resurrect the quality of an object-oriented design during incremental development",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870423,no,undetermined,0
An integrated cost model for software reuse,"Several cost models have been proposed in the past for estimating, predicting, and analyzing the costs of software reuse. The authors analyze existing models, explain their variance, and propose a tool-supported comprehensive model that encompasses most of the existing models",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870407,no,undetermined,0
Wireless communications based system to monitor performance of rail vehicles,"This paper describes a recently developed remote monitoring system, based on a combination of embedded computing, digital signal processing, wireless communications, GPS, and GIS technologies. The system includes onboard platforms installed on each monitored vehicle and a central station located in an office. Each onboard platform detects various events onboard a moving vehicle, tags them with time and location information, and delivers the data to an office through wireless communications channels. The central station logs the data into a database and displays the location and status of each vehicle, as well as detected events, on a map. Waveform traces from all sensor channels can be sent with each event and can be viewed by the central station operator. The system provides two-way wireless communication between the central station and mobile onboard platforms. Depending on coverage requirements and customer preferences, communication can be provided through satellite, circuit-switch cellular, digital wireless communication links or a combination of these methods. Settings and software changes may be made remotely from the central station, eliminating the need to capture the monitored vehicle. The onboard platform can be configured for installation on any rail vehicle, including locomotives, passenger cars and freight cars. Depending on the application, the onboard platform can monitor either its own sensors or existing onboard sensors. The system has been used for several railroad applications including ride quality measurement, high cant deficiency monitoring, truck hunting detection, and locomotive health monitoring. The paper describes the system, these applications, and discusses some of the results",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=869993,no,undetermined,0
How perspective-based reading can improve requirements inspections,"Because defects constitute an unavoidable aspect of software development, discovering and removing them early is crucial. Overlooked defects (like faults in the software system requirements, design, or code) propagate to subsequent development phases where detecting and correcting them becomes more difficult. At best, developers will eventually catch the defects, but at the expense of schedule delays and additional product-development costs. At worst, the defects will remain, and customers will receive a faulty product. The authors explain their perspective based reading (PBR) technique that provides a set of procedures to help developers solve software requirements inspection problems. PBR reviewers stand in for specific stakeholders in the document to verify the quality of requirements specifications. The authors show how PBR leads to improved defect detection rates for both individual reviewers and review teams working with unfamiliar application domains.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=869376,no,undetermined,0
A framework for interoperability analysis on the semantic web using architecture models,IT decision making requires analysis of possible future scenarios. The quality of the decisions can be enhanced by the use of architecture models that increase the understanding of the components of the system scenario. It is desirable that the created models support the needed analysis effectively since creation of architecture models often is a demanding and time consuming task. This paper suggests a framework for assessing interoperability on the systems communicating over the semantic web as well as a metamodel suitable for this assessment. Extended influence diagrams are used in the framework to capture the relations between various interoperability factors and enable aggregation of these into a holistic interoperability measure. The paper is concluded with an example using the framework and metamodel to create models and perform interoperability analysis.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815020,no,undetermined,0
Optimal and suboptimal reliable scheduling of precedence-constrained tasks in heterogeneous distributed computing,"Introduces algorithms which can produce both optimal and suboptimal task assignments to minimize the probability of failure of an application executing on a heterogeneous distributed computing system. A cost function which defines this probability under a given task assignment is derived. To find optimal and suboptimal task assignments efficiently, a reliable matching and scheduling problem is converted into a state-space search problem in which the cost function derived is used to guide the search. The A* algorithm for finding optimal task assignments and the A*<sub>m</sub> and hill-climbing algorithms for finding suboptimal task assignments are presented. Simulation results are provided to confirm the performance of the proposed algorithms",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=869148,no,undetermined,0
Automating delegation in class-based languages,"Some designers of class-based object oriented languages choose not to support multiple inheritance. As a result, programmers often resort to ad hoc workarounds. The most common of these workarounds is delegation. Even delegation is tedious and error prone, however. We believe that language designers who choose against multiple inheritance should consider automating delegation in order to alleviate these problems. In this paper we present Jamie, a language extension for Java that automates delegation. We also discuss the advantages and disadvantages of both delegation and automating it in a class-based programming language. Many of our observations are based on our experiences with implementing and using Jamie",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=868969,no,undetermined,0
Improvement model for collaborative networked organizations,Small and medium enterprises (SMEs) have huge improvement potential both in domain and in collaboration/interoperability capabilities. Before implementing respective improvement measures it's necessary to assess the performance in specific process areas which we divide in domain (e.g. tourism) and collaboration oriented ones. Both in enterprise collaboration (EC) and in enterprise interoperability (EI) the behavior of organizations regarding interoperability must be improved.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815027,no,undetermined,0
Bypass: a tool for building split execution systems,"Split execution is a common model for providing a friendly environment on a foreign machine. In this model, a remotely executing process sends some or all of its system calls back to a home environment for execution. Unfortunately, hand-coding split execution systems for experimentation and research is difficult and error-prone. We have built a tool, called Bypass, for quickly producing portable and correct split execution systems for unmodified legacy applications. We demonstrate Bypass by using it to transparently connect a POSIX application to a simple data staging system based on the Globus toolkit",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=868637,no,undetermined,0
Multiphysics modelling for electronics design,"The future of many companies will depend to a large extent on their ability to initiate techniques that bring schedules, performance, tests, support, production, life-cycle-costs, reliability prediction and quality control into the earliest stages of the product creation process. Important questions for an engineer who is responsible for the quality of electronic parts such as printed circuit boards (PCBs) during design, production, assembly and after-sales support are: What is the impact of temperature? What is the impact of this temperature on the stress produced in the components? What is the electromagnetic compatibility (EMC) associated with such a design? At present, thermal, stress and EMC calculations are undertaken using different software tools that each require model build and meshing. This leads to a large investment in time, and hence cost, to undertake each of these simulations. This paper discusses the progression towards a fully integrated software environment, based on a common data model and user interface, having the capability to predict temperature, stress and EMC fields in a coupled manner. Such a modelling environment used early within the design stage of an electronic product will provide engineers with fast solutions to questions regarding thermal, stress and EMC issues. The paper concentrates on recent developments in creating such an integrated modeling environment with preliminary results from the analyses conducted. Further research into the thermal and stress related aspects of the paper is being conducted under a nationally funded project, while their application in reliability prediction will be addressed in a new European project called PROFIT",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=866175,no,undetermined,0
Drivers for software development method usage,"In this research, the authors examine factors affecting the use of product development methods. Based on established behavioral theories, they develop and test a model that can explain, and hence predict, the extent of use of development methods. Although their model can be adapted to any development process, they apply it to software development. To test their model, they examine the combined effects of a number of important usage factors that contribute to the depth and breadth of use of two software development approaches: the waterfall model and prototyping. Two main constructs, process quality and facilitating conditions, are found to be the drivers of method usage. The dominating facilitating conditions and process quality indicators vary from one method to another product quality was not found to be a statistically significant factor in explaining usage. The authors' results are consistent with the view taken by the software process improvement movement, i.e., that a quality process will result in a quality product",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=865904,no,undetermined,0
Diagnostic tests for communicating nondeterministic finite state machines,"Systematic test sequence generation for conformance testing of communication protocol implementations, has been an active research area during the last decade. Methods were developed to produce optimized test sequences for detecting faults in such systems. However the application of these methods gives only limited information about the location of detected faults. In this paper we propose a complementary step, which localizes the fault, once detected. It consists of a generalized diagnostic algorithm for the case where distributed system specifications (implementations) are given in the form of communicating nondeterministic finite state machines. Such algorithm localizes the faulty transition once the fault has been detected. The algorithm guarantees the correct diagnosis of any single (output and/or transfer) fault. A simple example is used to demonstrate the functioning of the proposed algorithm. The complexity of each step in the algorithm and hence, the overall complexity are calculated",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=860675,no,undetermined,0
A constrained minimisation approach to optimise Gabor filters for detecting flaws in woven textiles,"Gabor filters have proved to be an effective segmentation and flaw detection tool. This study addresses the issue of an optimal 2-D Gabor filter design for automatically detecting defects in homogeneously textured woven fabrics. The parameters of these filters are derived through an optimisation process performing the minimisation of a Fisher cost function. By constraining some of the Gabor filter parameters to specific values the aim is to optimise the filter to detect a certain type of flaw as it appears in a particular textile background. To account for the potentially large variety of flaw types, the optimal parameters for multiple sets of constraints are computed. The detection outcomes from each set of optimal filters are combined to produce a final classification result. Successful detection results (with low false alarm rates) suggest that this optimal Gabor filter approach is a promising method for automated detection of flaws in homogenous textiles",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=860182,no,undetermined,0
A Deterministic Methodology for Identifying Functionally Untestable Path-Delay Faults in Microprocessor Cores,"Delay testing is crucial for most microprocessors. Software-based self-test (SBST) methodologies are appealing, but devising effective test programs addressing the true functionally testable paths and assessing their actual coverage are complex tasks. In this paper, we propose a deterministic methodology, based on the analysis of the processor instruction set architecture, for determining rules arbitrating the functional testability of path-delay faults in the data path and control unit of processor cores. Moreover, the performed analysis gives guidelines for generating test programs. A case study on a widely used 8-bit microprocessor is provided.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5070942,no,undetermined,0
Automated quality analysis of component software for embedded systems,"The Java programming language has gained increasing importance for the development of embedded systems. To be cost efficient, such systems have to cope with significant hardware restrictions which result in certain software programming restrictions. Recently, companies have started to apply Java component technology also in the area of embedded systems. Components are pieces of software with a defined interface which can be reused in different applications. Typically, components are not developed under programming restrictions for specific embedded systems, because those restrictions depend highly on the underlying hardware. Executing such software on a micro controller with very limited resources often results in unforeseen problems, e.g., in a memory overflow. Failure to detect such problems in an early stage might lead to significant costs, e.g., for replacing software on thousands of produced controllers. The authors present a semi-automatic approach to inspect Java source code in order to check for predefined hardware dependent restrictions. As an application domain we have chosen Java Smart Cards, which are very popular today, introduce their specific restrictions, and present how to inspect Java code to ensure that all restrictions are considered",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852476,no,undetermined,0
Virtual instrumentation and its application in diagnosis of faults in power transformers,"This paper presents a new approach to detect, localize and investigate the feasibility of identifying winding insulation failures. The diagnosis is based on the time-frequency analysis of signals recorded during lightning impulse tests. The virtual instrument is implemented with an acquisition board inserted into a PC and with software developed with lab view tools which sample the voltage and current signal and furnish the extent of insulation failure. The acquired signal is decomposed using multiresolution signal decomposition techniques to detect and localize the time instant of occurrence of fault",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888807,no,undetermined,0
Survivability through customization and adaptability: the Cactus approach,"Survivability, the ability of a system to tolerate intentional attacks or accidental failures or errors, is becoming increasingly important with the extended use of computer systems in society. While techniques such as cryptographic methods, intrusion detection, and traditional fault tolerance are currently being used to improve the survivability of such systems, new approaches are needed to help reach the levels that will be required in the near future. This paper proposes the use of fine-grain customization and dynamic adaptation as key enabling technologies in a new approach designed to achieve this goal. Customization not only supports software diversity, but also allows customized tradeoffs to be made between different QoS attributes including performance, security, reliability and survivability. Dynamic adaptation allows survivable services to change their behavior at runtime as a reaction to anticipated or detected intrusions or failures. The Cactus system provides support for both fine-grain customization and dynamic adaptation, thereby offering a potential solution for building survivable software in networked systems",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=825033,no,undetermined,0
Reaching efficient fault-tolerance for cooperative applications,"Cooperative applications are widely used, e.g. as parallel calculations or distributed information processing systems. Whereby such applications meet the users demand and offer a performance improvement, the susceptibility to faults of any used computer node is raised. Often a single fault may cause a complete application failure. On the other hand, the redundancy in distributed systems can be utilized for fast fault detection and recovery. So, we followed an approach that is based an duplication of each application process to detect crashes and faulty functions of single computer nodes. We concentrate on two aspects of efficient fault-tolerance-fast fault detection and recovery without delaying the application progress significantly. The contribution of this work is first a new fault detecting protocol for duplicated processes. Secondly, we enhance a roll forward recovery scheme so that it is applicable to a set of cooperative processes in conformity to the protocol",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839463,no,undetermined,0
Sensitivity analysis of modular dynamic fault trees,"Dynamic fault tree analysis, as currently supported by the Galileo software package, provides an effective means for assessing the reliability of embedded computer-based systems. Dynamic fault trees extend traditional fault trees by defining special gates to capture sequential and functional dependency characteristics. A modular approach to the solution of dynamic fault trees effectively applies Binary Decision Diagram (BOD) and Markov model solution techniques to different parts of the dynamic fault tree model. Reliability analysis of a computer-based system tells only part of the story, however. Follow-up questions such as Where are the weak links in the system?, How do the results change if my input parameters change? and What is the most cost effective way to improve reliability? require a sensitivity analysis of the reliability analysis. Sensitivity analysis (often called Importance Analysis) is not a new concept, but the calculation of sensitivity measures within the modular solution methodology for dynamic and static fault trees raises some interesting issues. In this paper we address several of these issues, and present a modular technique for evaluating sensitivity, a single traversal solution to sensitivity analysis for BOD, a simplified methodology for estimating sensitivity for Markov models, and a discussion of the use of sensitivity measures in system design. The sensitivity measures for both the Binary Decision Diagram and Markov approach presented in this paper is implemented in Galileo, a software package for reliability analysis of complex computer-based systems",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839462,no,undetermined,0
A network measurement architecture for adaptive applications,"The quality of network connectivity between a pair of Internet hosts can vary greatly. Adaptive applications can cope with these differences in connectivity by choosing alternate representations of objects or streams or by downloading the objects from alternate locations. In order to effectively adapt, applications must discover the condition of the network before communicating with distant hosts. Unfortunately, the ability to predict or report the quality of connectivity is missing in today's suite of Internet services. To address this limitation, we have developed SPAND (shared passive network performance discovery), a system that facilitates the development of adaptive network applications. In each domain, applications make passive application specific measurements of the network and store them in a local centralized repository of network performance information. Other applications may retrieve this information from the repository and use the shared experiences of all hosts in a domain to predict future performance. In this way, applications can make informed decisions about adaptation choices as they communicate with distant hosts. In this paper, we describe and evaluate the SPAND architecture and implementation. We show how the architecture makes it easy to integrate new applications into our system and how the architecture has been used with specifics types of data transport. Finally, we describe LookingGlass, a WWW mirror site selection tool that uses SPAND. LookingGlass meets the conflicting goals of collecting passive network performance measurements and maintaining good client response times. In addition, LookingGlass's server selection algorithms based on application level measurements perform much better than techniques that rely on geographic location or route metrics",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=832198,no,undetermined,0
"When management agents become autonomous, how to ensure their reliability?","Increasingly nowadays, networks are managed in a hierarchical, yet evolving to a distributed manner. The managed network is divided into sub-networks or domains that are managed more or less independently by autonomous agents. Once the failure of an agent is detected, it becomes even possible to have a further improvement by re-affecting the management tasks of the unreliable agent among the other agents in a way to ensure that the whole network continues to be reliably managed. This provides the property of graceful degradation to the distributed management system. The work presented in this paper provides a first step towards this interesting improvement. To ensure that the whole network is still managed even if a number of agents become unreliable, it is necessary to install a mechanism that continuously checks the reliability of the agents. When unreliable agents are detected, the management tasks that they have been performing are re-distributed amongst the other still-reliable agents. At some time in the future, the agent with the abnormal behavior might recover, for example following a human intervention, and the tasks that have been re-distributed on the other agents should be assigned back to the recovered agents",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=830451,no,undetermined,0
Database Mediation Using Multi-agent Systems,"This paper first proposes a multi-agent architecture to mediate access to data sources. The mediator follows the classical approach to process user queries. However, in the background, it post-processes query results to gradually construct matchings between the export schemas and the mediated schema. The central theme of the paper is an extensional schema matching strategy based on similarity functions. The paper concludes with experimental results that assess the quality of the matching strategy.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328417,no,undetermined,0
Technology transfer issues for formal methods of software specification,"Accurate and complete requirements specifications are crucial for the design and implementation of high-quality software. Unfortunately, the articulation and verification of software system requirements remains one of the most difficult and error-prone tasks in the software development lifecycle. The use of formal methods, based on mathematical logic and discrete mathematics, holds promise for improving the reliability of requirements articulation and modeling. However, formal modeling and reasoning about requirements has not typically been a part of the software analyst's education and training, and because the learning curve for the use of these methods is nontrivial, adoption of formal methods has proceeded slowly. As a consequence, technology transfer is a significant issue in the use of formal methods. In this paper, several efforts undertaken at NASA aimed at increasing the accessibility of formal methods are described. These include the production of the following: two NASA guidebooks on the concepts and applications of formal methods, a body of case studies in the application of formal methods to the specification of requirements for actual NASA projects, and course materials for a professional development course introducing formal methods and their application to the analysis and design of software-intensive systems. In addition, efforts undertaken at two universities to integrate instruction on formal methods based on these NASA materials into the computer science and software engineering curricula are described.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=827014,no,undetermined,0
Optimization of economizer tubing system renewal decisions,"The economizer is a critical component in coal fired power stations. An optimal renewal strategy is needed for minimizing the lifetime cost of this component. Here we present an effective optimization approach which considers economizer tubing failure probabilities, repair and renewal costs, potential production losses, and fluctuations in electricity market prices.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439663,no,undetermined,0
Test suite consistency verification,"Test cases are themselves prone to errors, thus techniques and tools to validate tests are needed. In this paper, we suggest a method to check mutual consistency of tests in a test suite.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580145,no,undetermined,0
Evaluation of Test Criteria for Space Application Software Modeling in Statecharts,"Several papers have addressed the problem of knowing which software test criteria are better than others with respect to parameters such as cost, efficiency and strength. This paper presents an empirical evaluation in terms of cost and efficiency for one test method for finite state machines, switch cover, and two test criteria of the statechart coverage criteria family, all-transitions and all-simple-paths, for a reactive system of a space application. Mutation analysis was used to evaluate efficiency in terms of killed mutants. The results show that the two criteria and the method presented the same efficiency but all-simple-paths presented a better cost because its test suite is smaller than the one generated by switch cover. Besides, test suite due to the all-simple-paths criterion killed the mutants faster than the other test suites meaning that it might be able to detect faults in the software more quickly than the other criteria.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172617,no,undetermined,0
The SecureGroup group communication system,"The SecureGroup group communication system multicasts messages to a group of processors over a local-area network, and delivers messages reliably and in total order. It also maintains the membership of the group, detecting and removing faulty processors and admitting new and repaired processors. The SecureGroup system provides resistance against Byzantine faults such as might be caused by a captured or subverted processor or by a Trojan horse. The reliable message delivery protocol employs hardware broadcasts and novel acknowledgment mechanisms that reduce the number of acknowledgments and messages required to ensure reliable delivery. The total ordering protocol continues to order messages despite the presence of Byzantine and crash faults, provided that a resilience requirement is satisfied. The group membership protocol operates above the total ordering protocol and, thus, simplifies its design and protects it against malicious attacks",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=821525,no,undetermined,0
A physics/engineering of failure based analysis and tool for quantifying residual risks in hardware,"NASA Code Q is supporting efforts to improve the verification and validation and the risk management processes for spaceflight projects. A physics-of-failure based Defect Detection and Prevention (DDP) methodology previously developed has been integrated into a software tool and is currently being implemented on various NASA projects and as part of NASA's new model-based spacecraft development environment. The DDP methodology begins with prioritizing the risks (or failure modes, FMs) relevant to a mission which need to be addressed. These risks can be reduced through the implementation of a set of detection and prevention activities referred to herein as PACTs (preventative measures, analyses, process controls and tests). Each of these PACTs has some effectiveness against one or more FMs but also has an associated resource cost. The FMs can be weighted according to their likelihood of occurrence and their mission impact should they occur. The net effectiveness of various combinations of PACTs can then be evaluated against these weighted FMs to obtain the residual risk for each of these FMs and the associated resource costs to achieve these risk levels. The process thus identifies the project-relevant tall pole FMs and design drivers and allows real time tailoring with the evolution of the design and technology content. The DDP methodology allows risk management in its truest sense: it identifies and assesses risk, provides options and tools for risk decision making and mitigation and allows for real-time tracking of current risk status",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816338,no,undetermined,0
Enhancing the predictive performance of the Goel-Okumoto software reliability growth model,"In this paper, enhancement of the performance of the Goel-Okumoto Reliability Growth model is investigated using various smoothing techniques. The method of parameter estimation for the model is the maximum likelihood method. The evaluation of the performance of the model is judged by the relative error of the predicted number of failures over future time intervals relative to the number of failures eventually observed during the interval. The use of data analysis procedures utilizing the Laplace trend test are investigated. These methods test for reliability growth throughout the data and establish """"windows"""" that censor early failure data and provide better model fits. The research showed conclusively that the data analysis procedures resulted in improvement in the models' predictive performance for 41 different sets of software failure data collected from software development labs in the United States and Europe",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816292,no,undetermined,0
Analysis of safety systems with on-demand and dynamic failure modes,"An approach for the reliability analysis of systems with on demand and dynamic failure modes is presented. Safety systems such as sprinkler systems or other protection systems are characterized by such failure behavior. They have support subsystems to start up the system on demand, and once they start running, they are prone to dynamic failure. Failure on demand requires an availability analysis of components (typically electromechanical components) which are required to start or support the safety system. Once the safety system is started, it is often reasonable to assume that these support components do not fail while running. Further, these support components may be tested and maintained periodically while not in active use. Dynamic failure refers to the failure while running (once started) of the active components of the safety system. These active components may be fault tolerant and utilize spares or other forms of redundancy, but are not maintainable while in use. In this paper, the authors describe a simple yet powerful approach to combining the availability analysis of the static components with a reliability analysis of the dynamic components. This approach is explained using a hypothetical example sprinkler system, and applied to a water deluge system taken from the offshore industry. The approach is implemented in the fault tree analysis software package, Galileo",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816277,no,undetermined,0
Use of fault tree analysis for evaluation of system-reliability improvements in design phase,"Traditional failure mode and effects analysis is applied as a bottom-up analytical technique to identify component failure modes and their causes and effects on the system performance, estimate their likelihood, severity and criticality or priority for mitigation. Failure modes and their causes, other than those associated with hardware, primarily electronic, remained poorly addressed or not addressed at all. Likelihood of occurrence was determined on the basis of component failure rates or by applying engineering judgement in their estimation. Resultant prioritization is consequently difficult so that only the apparent safety-related or highly critical issues were addressed. When thoroughly done, traditional FMEA or FMECA were too involved to be used as a effective tool for reliability improvement of the product design. Fault tree analysis applied to the product as a top down in view of its functionality, failure definition, architecture and stress and operational profiles provides a methodical way of following products functional flow down to the low level assemblies, components, failure modes and respective causes and their combination. Flexibility of modeling of various functional conditions and interaction such as enabling events, events with specific priority of occurrence, etc., using FTA, provides for accurate representation of their functionality interdependence. In addition to being capable of accounting for mixed reliability attributes (failure rates mixed with failure probabilities), fault trees are easy to construct and change for quick tradeoffs as roll up of unreliability values is automatic for instant evaluation of the final quantitative reliability results. Failure mode analysis using fault tree technique that is described in this paper allows for real, in-depth engineering evaluation of each individual cause of a failure mode regarding software and hardware components, their functions, stresses, operability and interactions",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816275,no,undetermined,0
Resource-constrained compaction of sequential circuit test sets,"We investigate a new, resource-constrained method for static compaction of large, sequential circuit test sets. Our approach is based on two key observations: (1) since all physical defects cannot be covered using a single defect model, test sets include tests generated using multiple defect models like stuck-at, delay, or bridging fault models. Therefore, it is unlikely that a marginal drop (0.5% or less) in fault coverage during compaction of tests generated for a single defect model will adversely affect the test quality of the overall test set. (2) Fault coverage is an aggregate measure that can be preserved as long as the original and compacted test sets detect the same number of faults. The specific faults detected by the two test sets can be significantly different. In particular, the compacted vector set may detect new faults that are not detected by the original vector set. The new compaction technique was implemented as part of the recently proposed two-phase static compaction technique. Experimental results on ISCAS benchmarks and several production circuits show that: (1) the actual loss in fault coverage, if any, was significantly less than the pre-specified tolerance limit of 1%; (2) fault coverage of the compacted test set can be higher than the original test set; and (3) significantly higher compaction is achieved using fewer CPU seconds, as compared to the baseline system that compacts test sets to preserve fault coverage",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=812640,no,undetermined,0
A model for the sizing of software updates,"The quality and reliability of software updates (SUs) are critical to a system vendor and its customers. As a result, it is important that SUs shipped to customers be successfully integrated into the field generic. A large amount of code must be shipped in an SU because customers want as many fixes and features as possible without compromising the reliability of their systems. However, as the size of an SU increases, so does its probability of field failure, thus making larger SUs riskier. The fundamental question is: How large should an SU be to keep the risk under control? This paper studies the tradeoff between the desire to ship large SUs and the failure risk carried with them. We formulate the problem as a nonlinear programming (NLP) problem, investigate it under various conditions, and derive sizing strategies for the SU. In particular, we derive a formula for the maximal SU size. We make a connection between software reliability and linear programming which, to the best of our knowledge, appears here for the first time. We also introduce some basic ideas related to the customer operational environment and explain the importance of the environment to software performance using an interesting analogy.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6772126,no,undetermined,0
Analysis of ROV video imagery for krill identification and counting under Antarctic sea ice,"An off-the-shelf SeaBotix ROV was deployed under Antarctic sea ice near Palmer Station Antarctica during the September-October 2007 Sea Ice Mass Balance in the Antarctic (SIMBA) project from the research vessel NB Palmer. Video imagery taken showed significant numbers of Antarctic krill (sp. <i>Euphasia superba</i> and/or <i>Euphasia crystallorophis</i>) under the sea ice at the two stations deployed. The goal of this image analysis is to estimate the krill population densities, as well, as to identify other life forms. The relative motion between the krill and vehicle complicate the video analysis process. The avoidance behavior of the krill adds to this challenge along with the changing lighting conditions under the ice. We discuss these challenges and the algorithms that are under development in this paper. The ROV videos were converted into a string of images, which were used to simulate a running speed of 3 frames/second (fps). A 5 second clip (15 frames) was selected as an initial test for the vision software. The LabVIEW<sup>reg</sup> Vision Builder AI software has been selected as an image processing platform, which allows us to rapidly prototype algorithms. We have applied noise reduction techniques to reduce some of the noise. Edge detection filters (such as, but not limited to the Roberts Filter) have been applied to further reduce the image's noise level and to increase the contrast between the krill and the water. Next, we applied thresholds to detect the object, which was subsequently used to identify and count and log the number of distinct objects in the image. Once all of the parameters were set, the 15 images were cycled through the configured inspection in chronological order to simulate an actual inspection of the ROV video. The results of our automated krill population estimate to that of humans counting the krill manually using the same video was conducted. We found that illumination and image quality allowed the most prominent individuals in - proximity of the camera to be counted. However, because of background noise and low scattering of some individuals the filtering removed suspected individuals that would suggest this krill density is significantly underestimated. The technique however appears practical. In considering the motion of the krill relative to the vehicle, tracking becomes paramount. Plans for implementing this technique in ongoing development are discussed.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290532,no,undetermined,0
SAABNet: Managing qualitative knowledge in software architecture assessment,"Quantitative techniques have traditionally been used to assess software architectures. We have found that early in development process there is often insufficient quantitative information to perform such assessments. So far the only way to make qualitative assessments about an architecture, is to use qualitative assessment techniques such as peer reviews. The problem with this type of assessment is that they depend on the techniques knowledge of the expert designers who use them. In this paper we introduce a technique, SAABNet (Software Architecture Assessment Belief Network), that provides support to make qualitative assessments of software architectures",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839860,no,undetermined,0
Evaluating system dependability in a co-design framework,"The widespread adoption of embedded microprocessor-based systems for safety critical applications mandates the use of co-design tools able to evaluate system dependability at every step of the design cycle. In this paper, we describe how fault injection techniques have been integrated in an existing co-design tool and which advantages come from the availability of such an enhanced tool. The effectiveness of the proposed tool is assessed on a simple case study",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840844,no,undetermined,0
Scalable QoS guaranteed communication services for real-time applications,"We propose an approach to flow-unaware admission control which is a combination with an aggregate packet forwarding scheme, improving scalability of networks while guaranteeing end-to-end deadlines for real-time applications. We achieve this by using an off-line delay computation and verification step, which allows to reduce the overhead at admission control while keeping admission probability and resource utilization high. Our evaluation data show that our system's admission probabilities are very close to those of significantly more expensive flow-aware approaches. At the same time, the admission control overhead during flow establishment is very low. Our results therefore support the claim from the DS architecture literature that scalability can be achieved through flow aggregation without sacrificing resource utilization and with significant reduction in run time overhead",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840928,no,undetermined,0
A MAC protocol with priority splitting algorithm for wireless ATM networks,"This paper deals proposes a medium access control (MAC) protocol for ensuring the quality of service (QoS) of integrated multimedia services on wireless links. The wireless ATM MAC protocol which incorporates dynamical polling, idle uplink (UL) data channel conversion, piggybacking and an interruptable priority splitting algorithm (named THBPSA) for resolving random access collisions is proposed and named TPICC. The effect of the priority splitting algorithm on the performance of the TPICC is simulated and compared with a counterpart of the TPICC which uses an unprioritised binary splitting algorithm (UBSA) in the RA slots. The effect of an invalid polling detecting (IPD) mechanism on the UL bandwidth efficiency is also simulated. The simulation results show that the THBPSA algorithm ensures a smaller medium access delay for realtime traffic classes than for non-realtime traffic classes. Comparing THBPSA with a priority scheduling scheme which is used in the base station (BS) and features packet time stamps, THBPSA provides realtime traffic classes with a much less UL packet delay than non-realtime traffic classes. The UL bandwidth used by the dynamic polling of realtime traffic classes is tolerable",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=851271,no,undetermined,0
Grading and predicting networked application behaviour,"The user-perceived quality of an application operating over a communication network has a considerable influence on the usefulness of that application. Intuitively, it may be assumed that this quality will be related to the current network performance, but in practice the relationship is often complex and difficult to determine. A scheme has been developed whereby the performance of network applications can be assessed and empirically graded for various controlled network loading conditions. Given the current network loading conditions on an operational network and information generated by the grading process, it is possible to predict the performance of the network application before the application is actually run. This has the potential for reducing the amount of traffic forced on a network as a consequence of aborted connections.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=850841,no,undetermined,0
The future growth trend of neutral currents in three-phase computer power systems caused by voltage sags,"This paper presents a summary of the power quality related concerns associated with the applications of the future growth trend of neutral currents in three-phase computer power systems. These concerns include power system harmonics and neutral current caused by voltage sags and short interruption. The neutral current characteristics under that condition are described. Methods for identifying these problems, analysis, determining their impact on utility and customer are also described. The EMTP simulation, field measurement and experimental results are used to verify the disturbance problems. The applications of regression model to predict neutral current growth trend during power line disturbances from measured data is proposed",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=850176,no,undetermined,0
A short circuit current study for the power supply system of Taiwan railway,"The western Taiwan railway consists mainly of a mountain route and ocean route. Taiwan Railway Administration (TRA) has conducted a series of experiments on the ocean route to identify the possible causes of unknown events which frequently cause trolley contact wires meltdown. The tests conducted include the short circuit fault test within the power supply zone of the Ho Long Substation (Zhu Nan to Tong Xiao) that had the highest probability for the meltdown events. Those test results based on the actual measured maximum short circuit current provide a valuable reference for TRA when comparing against the said events. Le Blanc transformer is the main transformer of the Taiwan railway electrification system. The Le Blanc transformer mainly transforms the Taiwan Power Company (TPC) generated three phase alternating power supply system (69 kV, 60 Hz) into two single phase alternating power distribution systems (M phase and T phase) (26 kV, 60 Hz) needed for the trolley traction. As a unique winding connection transformer, the conventional software for fault analysis will not be able to simulate its internal current and phase difference between each phase currents. Therefore, besides extracts of the short circuit test results, this work presents an EMTP model based on Taiwan Railway Substation equivalent circuit model with Le Blanc transformer. The proposed circuit model can simulate the same short circuit test to verify the actual fault current and accuracy of the equivalent circuit model. Moreover, the maximum short circuit current is further evaluated with reference to the proposed equivalent circuit",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=850083,no,undetermined,0
Applying a crystal ball to design pattern adoption,"Design patterns are gaining acceptance as a means to capture and disseminate best practice software design. Design patterns have already produced benefits for those organisations fostering their introduction. However, we argue that widespread adoption of design patterns as a general software improvement initiative is some way off and is subject to, as yet, unresolved factors. The paper examines the appeal of design patterns and uses diffusion of innovation theory to predict their adoption into mainstream practice. Recommendations for the implementation and use of design patterns in organisations are also made",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=848782,no,undetermined,0
Connector-Driven Gradual and Dynamic Software Assembly Evolution,"Complex and long-lived software need to be upgraded at runtime. Replacing a software component with a newer version is the basic evolution operation that has to be supported. It is error-prone as it is difficult to guarantee the preservation of functionalities and quality. Few existing work on ADLs fully support a component replacement process from its description to its test and validation. The main idea of this work is to have software architecture evolution dynamically driven by connectors (the software glue between components). It proposes a connector model which autonomically handle the reconfiguration of connections in architectures in order to support the versioning of components in a gradual, transparent and testable manner. Hence, the system has the choice to commit the evolution after a successful test phase of the software or rollback to the previous state.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172649,no,undetermined,0
Automatic Detection of Shared Objects in Multithreaded Java Programs,"This paper presents a simple and efficient automated tool called DoSSO that detects shared objects in multithreaded Java programs. Our main goal is to help programmers see all potentially shared objects that may cause some complications at runtime. This way programmers can implement a concurrent software without considering synchronization issues and then use appropriate locking mechanism based on the DoSSO results. To illustrate the effectiveness of our tool, we have performed an experiment on a multithreaded system with graphical user interfaces and remote method invocations and achieved promising results.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172680,no,undetermined,0
A skeleton-based approach for the design and implementation of distributed virtual environments,"It has long been argued that developing distributed software is a difficult and error-prone activity. Based on previous work on design patterns and skeletons, this paper proposes a template-based approach for the high-level design and implementation of distributed virtual environments (DVEs). It describes a methodology and its associated tool, which includes a user interface, a performance analyser and an automatic code generation facility. It also discusses some preliminary results on a surgical training system",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=847846,no,undetermined,0
Experimental testing of a neural network based digital differential relay for synchronous generators,This paper describes the laboratory implementation and real-time test results of a new neural network (NN) based digital differential relay used for detecting faults and classifying internal faults in the stator winding of synchronous generators. Details of the software written to enable the operation of the relay are described. Behavior of the NN based relay is observed and the experimental results are presented,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=847232,no,undetermined,0
An efficient parallel approach for identifying protein families in large-scale metagenomic data sets,"Metagenomics is the study of environmental microbial communities using state-of-the-art genomic tools. Recent advancements in high-throughput technologies have enabled the accumulation of large volumes of metagenomic data that was until a couple of years back was deemed impractical for generation. A primary bottleneck, however, is in the lack of scalable algorithms and open source software for large-scale data processing. In this paper, we present the design and implementation of a novel parallel approach to identify protein families from large-scale metagenomic data. Given a set of peptide sequences we reduce the problem to one of detecting arbitrarily-sized dense subgraphs from bipartite graphs. Our approach efficiently parallelizes this task on a distributed memory machine through a combination of divide-and-conquer and combinatorial pattern matching heuristic techniques. We present performance and quality results of extensively testing our implementation on 160 K randomly sampled sequences from the CAMERA environmental sequence database using 512 nodes of a BlueGene/L supercomputer.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5214891,no,undetermined,0
Modeling and analysis of software aging and rejuvenation,"Software systems are known to suffer from outages due to transient errors. Recently, the phenomenon of software aging, one in which the state of the software system degrades with time, has been reported. To counteract this phenomenon, a proactive approach of fault management, called software rejuvenation, has been proposed. This essentially involves gracefully terminating an application or a system and restarting it in a clean internal state. We discuss stochastic models to evaluate the effectiveness of proactive fault management in operational software systems and determine optimal times to perform rejuvenation, for different scenarios. The latter part of the paper deals with measurement-based methodologies to detect software aging and estimate its effect on various system resources. Models are constructed using workload and resource usage data collected from the UNIX operating system over a period of time. The measurement-based models are intended to help development of strategies for software rejuvenation triggered by actual measurements",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=844925,no,undetermined,0
A comparison of mobile agent and client-server paradigms for information retrieval tasks in virtual enterprises,"In next-generation enterprises it will become increasingly important to retrieve information efficiently and rapidly from widely dispersed sites in a virtual enterprise, and the number of users who wish to do using wireless and portable devices will increase significantly. We consider the use of mobile agent technology rather than traditional client-server computing for information retrieval by mobile and wireless users in a virtual enterprise. We argue that to be successful mobile agent platforms must coexist with, and be presented to the applications programmer side-by-side with, traditional client-server middleware like CORBA and DOOM, and we sketch a middleware architecture for doing so. We then develop an analytical model that examines the claimed performance benefits of mobile agents over client-server computing for a mobile information retrieval scenario. Our evaluation of the model shows that mobile agents are not always better than client-server calls in terms of average response times; they are only beneficial if the space overhead of the mobile agent code is not too large or if the wireless link connecting the mobile user to the fixed servers of the virtual enterprise is error-prone",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=843295,no,undetermined,0
Nimrod/K: Towards massively parallel dynamic Grid workflows,"A challenge for Grid computing is the difficulty in developing software that is parallel, distributed and highly dynamic. Whilst there have been many general purpose mechanisms developed over the years, Grid programming still remains a low level, error prone task. Scientific workflow engines can double as programming environments, and allow a user to compose dasiavirtualpsila Grid applications from pre-existing components. Whilst existing workflow engines can specify arbitrary parallel programs, (where components use message passing) they are typically not effective with large and variable parallelism. Here we discuss dynamic dataflow, originally developed for parallel tagged dataflow architectures (TDAs), and show that these can be used for implementing Grid workflows. TDAs spawn parallel threads dynamically without additional programming. We have added TDAs to Kepler, and show that the system can orchestrate workflows that have large amounts of variable parallelism. We demonstrate the system using case studies in chemistry and in cardiac modelling.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5215726,no,undetermined,0
An analytical model for loop tiling and its solution,"The authors address the problem of estimating the performance of loop tiling, an important program transformation for improved memory hierarchy utilization. We introduce an analytical model for estimating the memory cost of a loop nest as a rational polynomial in tile size variables. We also present a constant-time algorithm for finding an optimal solution to the model (i.e., for selecting optimal tile sizes) for the case of doubly nested loops. This solution can be applied to tiling of three loops by performing an iterative search on the value of the first tile size variable, and using the constant-time algorithm at each point in the search to obtain optimal tile size values for the remaining two loops. Our solution is efficient enough to be used in production-quality optimizing compilers, and has been implemented in the IBM XL Fortran product compilers. This solution can also be used by processor designers to efficiently predict the performance of a set of tiled loops for a range of memory hierarchy parameters",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=842294,no,undetermined,0
Computing global functions in asynchronous distributed systems prone to process crashes,"Global data is a vector with one entry per process. Each entry must be filled with an appropriate value provided by the corresponding process. Several distributed computing problems amount to compute a function on global data. This paper proposes a protocol to solve such problems in the context of asynchronous distributed systems where processes may fail by crashing. The main problem that has to be solved lies in computing the global data and in providing each non-crashed process with a copy of it, despite the possible crash of some processes. To be consistent, the global data must contain (at least) all the values provided by the processes that do not crash. This defines the global data computation (GDC) problem. To solve this problem, processes execute a sequence of asynchronous rounds during which they construct (in a decentralized way) the value of the global data, and eventually each process gets a copy of it. To cope with process crashes, the protocol uses a perfect failure detector. The proposed protocol has been designed to be time-efficient. It allows early decisions. Let t be the maximum number of processes that may crash (t<n where n is the total number of processes) and f be the actual number of process crashes (f&les;t). In the worst case, the protocol terminates in min(2f+2,t+1) rounds. Moreover the protocol does not require processes to exchange information on their perception of crashes. The message size depends only on the size of the global data",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840973,no,undetermined,0
Bandera: extracting finite-state models from Java source code,"Finite-state verification techniques, such as model checking, have shown promise as a cost-effective means for finding defects in hardware designs. To date, the application of these techniques to software has been hindered by several obstacles. Chief among these is the problem of constructing a finite-state model that approximates the executable behavior of the software system of interest. Current best-practice involves hand construction of models which is expensive (prohibitive for all but the smallest systems), prone to errors (which can result in misleading verification results), and difficult to optimize (which is necessary to combat the exponential complexity of verification algorithms). The authors describe an integrated collection of program analysis and transformation components, called Bandera, that enables the automatic extraction of safe, compact finite-state models from program source code. Bandera takes as input Java source code and generates a program model in the input language of one of several existing verification tools; Bandera also maps verifier outputs back to the original source code. We discuss the major components of Bandera and give an overview of how it can be used to model check correctness properties of Java programs",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870434,no,undetermined,0
Automated refactoring to introduce design patterns,"Software systems have to be flexible in order to cope with evolving requirements. However, since it is impossible to predict with certainty what future requirements will emerge, it is also impossible to know exactly what flexibility to build into a system. Design patterns are often used to provide this flexibility, so this question frequently reduces to whether or not to apply a given design pattern. We address this problem by developing a methodology for the construction of automated transformations that introduce design patterns. This enables a programmer to safely postpone the application of a design pattern until the flexibility it provides becomes necessary. Our approach deals with the issues of reuse of existing transformations, preservation of program behaviour and the application of the transformations to existing program code",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870480,no,undetermined,0
Analyzing software architectures with Argus-I,"This formal research demonstration presents an approach to develop and assess architecture and component-based systems based on specifying software architecture augmented by statecharts representing component behavioral specifications (Dias et al., 2000). The approach is applied for the C2 style (Medvidovic et al., 1999) and associated ADL and is supported within a quality-focused environment, called Argus-I, which assists specification-based analysis and testing at both the component and architecture levels",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870489,no,undetermined,0
IEEE 1232 and P1522 standards,"The 1232 family of standards were developed to provide standard exchange formats and software services for reasoning systems used in system test and diagnosis. The exchange formats and services am based on a model of information required to support test and diagnosis. The standards were developed by the Diagnostic and Maintenance Control (D&MC) subcommittee of IEEE SCC20. The current efforts by the D&MC are a combined standard made up of the 1232 family, and a standard on Testability and Diagnosability Metrics, P1522. The 1232 standards describe a neutral exchange format so one diagnostic reasoner can exchange model information with another diagnostic reasoner. In addition, software interfaces are defined whereby diagnostic tools can be developed to process the diagnostic information in a consistent and reliable way. The objective of the Testability and Diagnosability Metrics standard is to provide notionally correct and mathematically precise definitions of testability measures that may be used to either measure the testability characteristics of a system, or predict the testability of a system. The end purpose is to provide an unambiguous source for definitions of common and uncommon testability and diagnosability terms such that each individual encountering it can know precisely what that term means. This paper describes the 1232 and P1522 standards and details the recent changes in the Information models, restructured higher order services and simplified conformance requirements",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885619,no,undetermined,0
How to measure the impact of specific development practices on fielded defect density,"This author has mathematically correlated specific development practices to defect density and probability of on time delivery. She summarizes the results of this ongoing study that has evolved into a software prediction modeling and management technique. She has collected data from 45 organizations developing software primarily for equipment or electronic systems. Of these 45 organizations, complete and unbiased delivered defect data and actual schedule delivery data was available for 17 organizations. She presents the mathematical correlation between the practices employed by these organizations and defect density. This correlation can and is used to: predict defect density; and improve software development practices for the best return on investment",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885868,no,undetermined,0
Contributing to the bottom line: optimizing reliability cost schedule tradeoff and architecture scalability through test technology,"A challenging problem in software testing is finding the optimal point at which costs justify the stop-test decision. We first present an economic model that can be used to evaluate the consequences of various stop-test decisions. We then discuss two approaches for assessing performance, automated load test generation in the context of empirical testing and performance modeling, and illustrate how these techniques can affect the stop-test decision. We then illustrate the application of these two techniques to evaluating the performance of Web servers that performs significant server-side processing through object-oriented (OO) computing. Implications of our work for Web server performance evaluation in general are discussed",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885867,no,undetermined,0
Assessing the cost-effectiveness of inspections by combining project data and expert opinion,"There is a general agreement among software engineering practitioners that sofware inspections are an important technique to achieve high software quality at a reasonable cost. However, there are many ways to perform such inspections and many factors that affect their cost-effectiveness. It is therefore important to be able to estimate this cost-effectiveness in order to monitor it, improve it, and convince developers and management that the technology and related investments are worthwhile. This work proposes a rigorous but practical way to do so. In particular, a meaningful model to measure cost-effectiveness is selected and a method to determine the cost-effectiveness by combining project data and expert opinion is proposed. To demonstrate the feasibility of the proposed approach, the results of a large-scale industrial case study are presented",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885866,no,undetermined,0
Generating test cases for GUI responsibilities using complete interaction sequences,"Testing graphical user interfaces (GUI) is a difficult problem due to the fact that the GUI possesses a large number of states to be tested, the input space is extremely large due to different permutations of inputs and events which affect the GUI, and complex GUI dependencies which may exist. There has been little systematic study of this problem yielding a resulting strategy which is effective and scalable. The proposed method concentrates upon user sequences of GUI objects and selections which collaborate, called complete interaction sequences (CIS), that produce a desired response for the user. A systematic method to test these CIS utilizes a finite-state model to generate tests. The required tests can be substantially reduced by identifying components of the CIS that can be tested separately. Since consideration is given to defects totally within each CIS, and the components reduce required testing further, this approach is scalable. An empirical investigation of this method shows that substantial reduction in tests can still detect the defects in the GUI. Future research will prioritize testing related to the CIS testing for maximum benefit if testing time is limited",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885865,no,undetermined,0
Quantitative software reliability modeling from testing to operation,"We first describe how several existing software reliability growth models based on nonhomogeneous Poisson processes (NHPPs) can be derived based on a unified theory for NHPP models. Under this general framework, we can verify existing NHPP models and derive new NHPP models. The approach covers a number of known models under different conditions. Based on these approaches, we show a method of estimating and computing software reliability growth during the operational phase. We can use this method to describe the transitions from the testing phase to operational phase. That is, we propose a method of predicting the fault detection rate to reflect changes in the user's operational environments. The proposed method offers a quantitative analysis on software failure behavior in field operation and provides useful feedback information to the development process",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885862,no,undetermined,0
3-D laparoscopic imaging,"This paper describes a clinical study conducted on a porcine animal model to assess the 3-D imaging capability of a laparoscopic imaging probe. A commercially available 128 element 7 MHz laparoscopic imaging probe was modified by the addition of a motor for rotation and a position sensor to locate each image plane in the 3-D volume. Images were acquired on a number of organs, including the gallbladder, liver, kidney, and urinary bladder. Because the system uses a conventional one dimensional array for the acquisition, the image quality is high. Volumetric surface renderings of the organs with additional 2-D cross sections demonstrate the features of the software for a number of viewing modes.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803483,no,undetermined,0
Building trust into OO components using a genetic analogy,"Despite the growing interest for component based systems, few works tackle the question of the trust we can bring into a component. The paper presents a method and a tool for building trustable OO components. It is particularly adapted to a design-by-contract approach, where the specification is systematically derived into executable assertions (invariant properties, pre/postconditions of methods). A component is seen as an organic set composed of a specification, a given implementation and its embedded test cases. We propose an adaptation of mutation analysis to the OO paradigm that checks the consistency between specification/implementation and tests. Faulty programs, called mutants, are generated by systematic fault injection in the implementation. The quality of tests is related to the mutation score, i.e. the proportion of faulty programs it detects. The main contribution is to show how a similar idea can be used in the same context to address the problem of effective test optimization. To map the genetic analogy to the test optimization problem, we consider mutant programs to be detected as the initial preys population and test cases as the predators population. The test selection consists of mutating the predator test cases and crossing them over in order to improve their ability to kill the prey population. The feasibility of component validation using such a Darwinian model and its usefulness for test optimization are studied",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885856,no,undetermined,0
Design of an improved watchdog circuit for microcontroller-based systems,"This paper presents an improved design for a watchdog circuit. Previously, watchdog timers detected refresh inputs that were slower than usual. If a failure causes the microcontroller to produce faster than usual refresh inputs, the watchdog will not detect it. This new design detects failures that produce faster than usual as well as slower than usual refresh inputs. This will greatly improve the reliability of the system protected by this new design.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884831,no,undetermined,0
Agent based customer modelling: individuals who learn from their environment,"Understanding the rate of adoption of a telecommunications service in a population of customers is of prime importance to ensure that appropriate network capacity is provided to maintain quality of service. This problem goes beyond assessing the demand for a product based on usage and requires an understanding of how consumers learn about a service and evaluate its worth. Field studies have shown that word of mouth recommendations and knowledge of a service have a significant impact on adoption rates. Adopters of the Internet can be influenced through communications at work or children learning at school. The authors present an agent based model of a population of customers, with rules based on field data, which is being used to understand how services are adopted. Of particular interest is how customers interact to learn about the service through their communications with other customers. We show how the different structure, dynamics and distribution of the social networks affect the diffusion of a service through a customer population. Our model shows that real world adoption rates are a combination of these mechanisms which interact in a non-linear and complex manner. This complex systems approach provides a useful way to decompose these interactions",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870830,no,undetermined,0
Applying reflective middleware techniques to optimize a QoS-enabled CORBA component model implementation,"Although existing CORBA specifications, such as Real-time CORBA and CORBA Messaging, address many end-to-end quality of service (QoS) properties, they do not define strategies for configuring these properties into applications flexibly, transparently, and adaptively. Therefore, application developers must make these configuration decisions manually and explicitly which is tedious, error-prone, and often suboptimal. Although the recently adopted CORBA Component Model (CCM) does define a standard configuration framework for packaging and deploying software components, conventional CCM implementations focus on functionality rather than adaptive quality of service, which makes them unsuitable for next generation applications with demanding QoS requirements. The paper presents three contributions to the study of middleware for QoS-enabled component based applications. It outlines reflective middleware techniques designed to adaptively: (1) select optimal communication mechanisms; (2) manage QoS properties of CORBA components in their containers; and (3) (re)configure selected component executors dynamically. Based on our ongoing research on CORBA and the CCM, we believe the application of reflective techniques to component middleware will provide a dynamically adaptive and (re)configurable framework for COTS software that is well-suited for the QoS demands of next generation applications",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884772,no,undetermined,0
Cardiac monitoring using transducers attached directly to the heart,"Cardiac ultrasound systems deliver excellent information about the heart, but are constructed for intermittent imaging and interpretation by a skilled operator. This paper presents a dedicated ultrasound system to monitor cardiac function continuously during and after cardiac surgery. The system uses miniature 10 MHz transducers sutured directly to the heart surface. M-mode images give a visual interpretation of the contraction pattern, while tissue velocity curves give detailed quantitative information. The ultrasound measurements are supported by synchronous ECG and pressure recordings. The system has been tested on pigs, demonstrating M-mode and tissue velocity measurements of good quality. When occluding the LAD coronary artery, the system detected changes in contraction pattern that correspond with known markers of ischemia. The system uses dedicated analog electronics and a PC with digitizers and LabVIEW software, and may also be useful in other experimental ultrasound applications.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803609,no,undetermined,0
Dependability of complex software systems with component upgrading,"Some very large and complex systems, such as telecommunication systems, must (and typically do) exhibit exceptional dependability. These systems are seldom totally replaced with a new system because of the increased likelihood of a lapse in service. Rather, systems are upgraded incrementally while operational, albeit that this often involves large-scale software changes. It is especially important then to ensure that new or replacement components are ready for online installation before they are incorporated into an operational system. It is often costly and time-consuming to determine the readiness of new components for installation. Even then, the result may be unpredictable. Hence, we have developed effective and economical methods for software component verification that ensure and increase the overall system dependability. We tested our technologies using a telecommunication application, an Internet call-agent. Our experimental results show that our dynamic design analysis approach reduces computational costs and detects more errors than conventional approaches. The more frequently that changes are made, the greater the savings in the time required for model analysis and property prediction",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884758,no,undetermined,0
An Agent-Oriented System for Workflow Enactment Tracking,"The notion of workflows has evolved from a means to describe the flow of paperwork through an organization to a more abstract and general technique used to express best practices and lessons learned in many application domains. When putting workflow definitions into practice, it is important to stay informed which tasks are currently performed, as this allows detecting slipping schedules or unwanted deviations. In this paper, an agent-based approach for automatically tracking the set of active tasks is presented by observing the data produced during enactment. This enables short-term planning and quality control without obliging team members to explicitly document progress.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4806925,no,undetermined,0
Fault tolerant shared-object management system with dynamic replication control strategy,"This paper is based on a dynamic replication control strategy for minimizing communications costs. In dynamic environments where the access pattern to share resources cannot be predicted statically, it is required to monitor such parameter during the whole lifetime of the system so as to adapt it to new requirements. The shared-object management system is implemented in a centralized manner in which a master processor deals with the serialization of invocations. On one hand, we attempt to provide fault tolerance as a way to adjust the system parameters to work only with a set of correct processors so as to enhance system functionality. On the other hand, we attempt to furnish availability by masking the failure of the master processor. A new master processor is elected that resumes the master processor processing. Our shared-object management system modularity is realized through a meta level implementation",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884677,no,undetermined,0
Flexible Network Layer in dynamic networking architecture,"The authors propose an architecture of global communication networks with dynamic functions based on the concept of Flexible Network. The dynamic function enhances the capability of communication networks to deal with various changes detected in both human users and networked environment. In our architecture, a new Flexible Network Layer is introduced between the application layer and the IP network layer of the global communication networks. To elaborate the functions of the Flexible Network Layer, we demonstrate an agent based model of the Flexible Network Layer for a multimedia communication application and discuss the properties of the proposed architecture",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884670,no,undetermined,0
The 9 quadrant model for code reviews,"Discusses a decision-making model which can be used to determine the efficiency of a code review process. This model is based on statistical techniques such as control charts. The model has nine quadrants, each of which depicts a range of values of the cost and yield of a code review. The efficiency of the code review in detecting defects is determined by taking inputs from past data, in terms of the costs and yields of those code reviews. This estimate also provides an in-process decision-making tool. Other tools can be used effectively, in conjunction with this model, to plan for code reviews and to forecast the number of defects that could be expected in the reviews. This model can be successfully used to decide what the next step of the operational process should be. The decisions taken using this model help to reduce the number of defects present in the software delivered to the customer",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883792,no,undetermined,0
A software falsifier,"A falsifier is a tool for discovering errors by static source-code analysis. Its goal is to discover them while requiring minimal programmer effort. In contrast to lint-like tools or verifiers, which try to maximize the number of errors reported at the expense of allowing false errors, a falsifier's goal is to guarantee no false errors. To further minimize programmer effort, no specification or extra information about the program is required. That, however, does not preclude project-specific information from being built in. The class of errors that are detectable without any specification is important not only because of the low cost of detection, but also because it includes errors of portability, irreproducible behavior, etc., which are very expensive to detect by testing. This paper describes the design and implementation of such a falsifier, and reports on experience with its use for design automation software. The main contribution of this work lies in combining data-flow analysis with symbolic execution to take advantage of their relative advantages",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885870,no,undetermined,0
Comparison of the acoustic response of attached and unattached BiSphereTM microbubbles,"Two systems that independently allow the investigation of the response of individual unattached and attached microbubbles have previously been described. Both offered methods of studying the acoustic response of single microbubbles in well defined acoustic fields. The aim of the work described here was to investigate the responses of single attached microbubbles for a range of acoustic pressures and to compare these to the backscatter from unattached single microbubbles subjected to the same acoustic fields. Single attached BiSphere<sup>TM</sup> (Point Biomedical) microbubbles were attached to polyester with poly-L-lysine. Individual attached microbubbles were insonated at 1.6 MHz for acoustic pressures ranging from 300 to 1000 kPa using a Sonos5500 (Philips Medical Systems) research ultrasound scanner. Each microbubble was aligned to 6 cycle pulse, M-mode ultrasound beams, and unprocessed backscattered RF data captured using proprietary hardware and software. The backscatter from these microbubbles was compared to that of single unattached microbubbles subjected to the same acoustic parameters, microbubbles were insonated several times to determine possible differences in rate of decrease of backscatter between attached and unattached microbubbles. In total over 100 single attached microbubbles have been insonated. At 550kPa an acoustic signal was detected for 20 % of the attached microbubbles and at 1000 kPa for 63%. At acoustic pressures of 300kPa no signal was detected. Mean RMS fundamental pressure from attached and unattached microbubbles insonated at 800 kPa was 9.7 Pa and 8.7 Pa respectively. The ratio between the first two backscattered pulses decreased with increasing pressure. However, for unattached microbubbles the magnitude of the ratio was less than that of attached (at 550kPa mean ratio attached: 0.92 + 0.1, unattached: 0.28 + 0.2). There was no significant difference in the peak amplitude of the backscattered signal for unattached and attached micro- - bubbles. BiSphere<sup>TM</sup> microbubbles comprise an internal polymer shell with an albumin coating, resulting in a stiff shell. BiSphere<sup>TM</sup> microbubbles do not oscillate in the same manner as a softer shelled microbubble, but allow gas leakage which then performs free bubble oscillations. The results here agree with previous acoustic and optical microscopy measurements which show that a proportion of microbubbles will scatter and this number increases with acoustic pressure. The lack of difference in scatter between the unattached and attached microbubbles may be attributed to the free microbubble oscillation being in the vicinity of the stiff shell, which may provide the same motion damping to a wall. Second pulse exposure shows that the wall becomes important in the survival of the free gas. These high quality measurements can be further improved by incorporating microbubble sizing to increase the specificity of the comparisons between unattached and attached microbubbles.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803287,no,undetermined,0
Generating Requirements Analysis Models from Textual Requirements,"Use case modeling is a commonly used technique to describe functional requirements in requirements engineering. Typically, use cases are captured from textual requirements documents describing the functionalities the system should meet. Requirements elicitation, analysis and modeling is a time consuming and error-prone activity, which it is not usually supported by automated tools. This paper tackles this problem by taking free-form textual requirements and offering a semi-automatic process for generation of domain models, such as use cases. Our goal is twofold: (i) reduce the time spent to produce requirements artifacts; and (ii) enable future application of model-driven engineering techniques to maintain traceability information and consistency between textual and requirements visual models artifacts.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4797436,no,undetermined,0
Detecting spurious features using parity space,"Detection of spurious features is instrumental in many computer vision applications. The standard approach is feature based, where extracted features are matched between the image frames. This approach requires only vision, but is computer intensive and not yet suitable for real-time applications. We propose an alternative based on algorithms from the statistical fault detection literature. It is based on image data and an inertial measurement unit (IMU). The principle of analytical redundancy is applied to batches of measurements from a sliding time window. The resulting algorithm is fast and scalable, and requires only feature positions as inputs from the computer vision system. It is also pointed out that the algorithm can be extended to also detect non-stationary features (moving targets for instance). The algorithm is applied to real data from an unmanned aerial vehicle in a navigation application.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4795545,no,undetermined,0
Faulty version recovery in object-oriented N-version programming,"Many long-running applications would greatly benefit from being able to recover faulty versions in N-version programs since their exclusion from further use undermines the availability of the system. Developing a recovery feature, however, is a very complex and error-prone task, which the author believes has not received adequate attention. Although many researchers are aware of the importance of version recovery, there are very few schemes which include these features. Even when they do, they rely on ad hoc programming and are not suitable for object-oriented systems. The author believes that developing systematic approaches here is crucial, and formulates a general approach to version recovery in class diversity schemes, which is based on the concept of the abstract version state. The approach extends the recently-developed class diversity scheme and relies on important ideas motivated by community error recovery. The diversity scheme includes two-level error detection which allows error latency to be controlled. To use it, special application-specific methods for each version object have to be designed, which would map the internal state into the abstract state and at the same time, form a basis for one-level version recovery. The approach is discussed in detail, compared with the existing solutions, and additional benefits of using the abstract version state are shown. The intention is to outline a disciplined way for providing version recovery and thus make it more practical. Two promising approaches which can be used for developing new structuring techniques incorporating the abstract version state concept are discussed",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888327,no,undetermined,0
A strategy for Grid based t-way test data generation,"Although desirable as an important activity for ensuring quality assurances and enhancing reliability, complete and exhaustive software testing is next to impossible due to resources as well as timing constraints. While earlier work has indicated that pairwise testing (i.e. based on 2-way interaction of variables) can be effective to detect most faults in a typical software system, a counter argument suggests such conclusion cannot be generalized to all software system faults. In some system, faults may also be caused by more than two parameters. As the number of parameter interaction coverage (i.e. the strength) increases, the number of t-way test set also increases exponentially. As such, for large system with many parameters, considering higher order t-way test set can lead toward combinatorial explosion problem (i.e. too many data set to consider). We consider this problem for t-way generation of test set using the Grid strategy. Building and complementing from earlier work in In-Parameter-Order-General (or IPOG) and its modification (or MIPOG), we present the Grid MIPOG strategy (G_MIPOG). Experimental results demonstrate that G_MIPOG scales well against the sequential strategies IPOG and MIPOG with the increase of the computers as computational nodes.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4784416,no,undetermined,0
The Design and Fabrication of a Full Field Quantitative Mammographic Phantom,"Breast cancer is among the leading causes of death in women worldwide. Screen-film mammography (SFM) is still the standard method used to detect early breast cancer thus leading to early treatment. Digital mammography (DM) has recently been designated as the imaging technology with the greatest potential for improving the diagnosis of breast cancer. For successful mammography, high quality images must be achieved and maintained, and reproducible quantitative quality control (QC) testing is thus required. Assessing images of known reference phantoms is one accepted method of doing QC testing. Quantitative QC techniques are useful for the long-term follow-up of mammographic quality. Following a comprehensive critical evaluation of available mammography phantoms, it was concluded that a more suitable phantom for DM could be designed. A new relatively inexpensive Applied Physics Group (APG) phantom was designed to be fast and easy to use, to provide the user with quantitative and qualitative measures of high and low contrast resolution over the full field of view and to demonstrate any geometric distortions. It was designed to cover the entire image receptor so as to assess the heel effect, and to be suitable for both SFM and DM. The APG phantom was designed and fabricated with embedded test objects and software routines were developed to provide a complete toolkit for SFM and DM QC. The test objects were investigated before embedding them.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4786058,no,undetermined,0
Augmenting sequence constraints in Z and its application to testing,"The paper introduces sequence constraints into a formal specification language Z. Formal specification languages have been used to specify safety-critical applications, and many static and dynamic aspects of the system can be specified. However, the method calling constraints, a runtime behavior, are often missed. The paper introduces two kinds of sequence constraints: those constraints with respect to a schema and those with respect to multiple schemas. Once sequence constraints are specified, together with parameter specifications already in Z, one can generate test cases including test inputs and their expected outputs using various testing strategies such as partition testing, boundary testing, random testing, stress testing, and negative testing. An application has been specified in Z using sequence constraints, and test cases generated have been used to test the software. The results show that the test cases generated successfully detected all the faults seeded",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888030,no,undetermined,0
"Correctly assessing the -ilities"""" requires more than marketing hype","Understanding key system qualities can better equip you to correctly assess the technologies you manage. Dot-coms and enterprise systems often use terms like scalability, reliability and availability to describe how well they meet current and future service-level expectations. These ilities characterize an IT solution's architectural and engineering qualities. They collectively provide a vocabulary for discussing an IT solution's performance potential amid ever-changing IT requirements. The paper considers the role that ilities play in the solution architecture. They generally fall into four categories: strategic, systemic, service and user.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=888019,no,undetermined,0
How does resource utilization affect fault tolerance?,"Many fault-tolerant architectures are based on the single-fault assumption, hence accumulation of dormant faults represents a potential reliability hazard. Based on the example of the fail-silent Time-Triggered Architecture we study sources and effects of dormant faults. We identify software as being more prone to dormant faults than hardware. By means of modeling we reveal a high sensitivity of the MTTF to the existence of even a small amount of irregularly used resources. We propose on-line testing as a means of coping with dormant faults and sketch an appropriate test strategy",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=887163,no,undetermined,0
Single byte error control codes with double bit within a block error correcting capability for semiconductor memory systems,"Computer memory systems when exposed to strong electromagnetic waves or radiation are highly vulnerable to multiple random bit errors. Under this situation, we cannot apply existing SEC-DED or S<sub>b</sub>EC capable codes because they provide insufficient error control performance. This correspondence considers the situation where two random bits in a memory chip are corrupted by strong electromagnetic waves or radioactive particles and proposes two classes of codes that are capable of correcting random double bit errors occurring within a chip. The proposed codes, called Double bit within a block Error Correcting-Single byte Error Detecting ((DEC)<sub>B</sub>-S<sub>b</sub>ED) code and Double bit within a block Error Correcting-Single byte Error Correcting ((DEC)<sub>B</sub>-S<sub>b </sub>EC) code, are suitable for recent computer memory systems",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=887157,no,undetermined,0
Quality of service management for real-time embedded information systems,"Explores further how dynamic and distributed quality-of-service (QoS) management functions can be added to avionics applications built using commercial-off-the-shelf (COTS) standards and components, to provide more powerful adaptive software capabilities. This paper describes contributions in two principal areas. First, it outlines a QoS management architecture that meets the distributed resource management needs of real-time information systems, and describes our recent extensions to that architecture. Second, it presents empirical evidence of the utility and feasibility of dynamic and adaptive system behavior in realistic real-time embedded information systems. The discussion centers on the identification of key architectural features, and describes initial qualitative and quantitative results that are used to assess the benefits and costs of these segments of the overall architecture",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=886962,no,undetermined,0
The feasibility of applying object-oriented technologies to operational flight software,"As object-oriented technologies move from the laboratory to the mainstream, companies are beginning to realize cost benefits in terms of software reuse and reduced development time. These benefits have been elusive to developers of real-time flight software. Issues such as processing latencies, validated performance of mission critical functions, and integration of legacy code have inhibited the effective use of object-oriented technologies in this domain. Emerging design languages, development tools, and processes offer the potential to address the application of object technologies to real-time operational flight software development. This paper examines emerging object-based technologies and assess their applicability to operational flight software. It includes an analysis that compares and contrasts the current Comanche software development process with one based on object-oriented concepts",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=886959,no,undetermined,0
Redundancy management system for the X-33 vehicle and mission computer,"The X-33 is an unmanned advanced technology demonstrator with a mission to validate new technologies for the next generation of reusable launch vehicles. Various system redundancies are designed in the X-33 to enhance the probability of successfully completing its mission in the event of faults and failures during flight. One such redundant system is the vehicle and mission computer that controls the X-33 and manages the avionics subsystems. Historically, redundancy management and applications such as flight control and vehicle management tended to be highly coupled. One of the technologies that the X-33 will demonstrate is the redundancy management system (RMS) that uncouples the applications from the redundancy management details, much in the same way that real-time operating systems have uncoupled applications from task scheduling, communication and synchronization details. This paper describes Honeywell's RMS, its role and implementation in the X-33, some of the tradeoffs that were chosen, the fault tolerance concepts it embodies and its suitability as an off-the-shelf solution for a range of high reliability and high availability applications. This paper concludes with insights on current and future RMS developments",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=886886,no,undetermined,0
Software reliability prediction of digital fly control system,"With the rapid development of computer technology, software plays an important and decision-making role in the computer control system, especially in digital flight control systems. How to determine and improve the reliability of software is a critical problem demanding urgent solution. Traditionally, software reliability is determined according to a reliability model based on software testing data after the software has developed, which is not suitable for improving its reliability. This paper studies a method to predict the software reliability in the early development period so as to provide the basis for improving the reliability of software. Application to a digital control software system indicates that this method can predict the reliability of software in the early development period, especially in the requirement period and outline design period, effectively",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=886881,no,undetermined,0
In-service monitoring with Centralized Failure Detection System (CFDS) in FTTH access network,"This paper focuses on developing a simple, attracting and user-friendly graphical user interface (GUI) for centralized failure detection system (CFDS) by using MATLAB software. The developed program will be installed with optical line terminal (OLT) at central office (CO) to centrally monitoring each optical fiber line's status and detecting the failure location that occurs in the drop region of fiber to the home (FTTH) access network downwardly from CO towards customer premises. Conventionally, the faulty fiber and failure location can be detected by using optical time domain reflectometer (OTDR) upwardly from customer premises towards CO. However, OTDR can only display a single result of a line in a time and also time and cost misspend. CFDS is interfaced with the OTDR to accumulate every network testing result to be displayed on a single computer screen for further analysis. The program will identify and present the parameters of optical line such as the line's status either in working or nonworking condition, magnitude of decreasing, failure location and other details as shown in the OTDR's screen. The analysis result will be sent to field engineers or service providers for promptly action.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4786752,no,undetermined,0
Expanding design pattern to support parallel programming,"The design pattern concept is widely used in large object-oriented software development, but this should not be limited to the object-oriented field: it can be used in many other areas. Explicit parallel programming is well-known to be complex and error-prone, and design patterns can ease this work. This paper introduces a pattern-based approach for parallel programming, in which we classify design patterns into two levels to support (a) the parallel algorithm design phase and (b) the parallel coding phase, respectively. Through this approach, a programmer doesn't need much additional knowledge about parallel computing; what he need to do is to describe the problem he wants to solve and offer some parameters, sequential code or components. We demonstrate this approach with a case study in this paper",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885894,no,undetermined,0
Software reliability and maintenance concept used for automatic call distributor MEDIO ACD,"The authors present the software reliability and maintenance concept, which is used in the software development, testing, and maintenance process, for automatic call distributor MEDIO ACD. The concept has been successfully applied on systems, which are installed and fully operational in Moscow and Saint Petersburg, Russia. The authors concentrate on two main issues: (i) set of fault-tolerant mechanisms needed for the system exploitation (error logging, checkpoint-restart, overload protection and tandem configuration support); (ii) MEDIO ACD software maintenance concept, in which the quality of the new software update is predicted on the basis of the current update's metrics and quality, and the new update's metrics. This forecast aids software maintenance efficiency, and cost reduction",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885884,no,undetermined,0
Incorporating varying requirement priorities and costs in test case prioritization for new and regression testing,"Test case prioritization schedules the test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects the most severe faults at the earliest in its testing life cycle. Test case prioritization techniques have proved to be beneficial for improving regression testing activities. While code coverage based prioritization techniques are found to be taken by most scholars, test case prioritization based on requirements in a cost effective manner has not been taken for study so far. Hence, in this paper, we propose to put forth a model for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software that can also be cost effective and to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the six factors: customer priority, changes in requirement, implementation complexity, usability, application flow and fault impact. The proposed prioritization technique is experimented in three phases with student projects and two sets of industrial projects and the results show convincingly that the proposed prioritization technique improves the rate of severe fault detection.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4787662,no,undetermined,0
Testing for imperfect integration of legacy software components,"In the manufacturing domain, few new distributed systems are built ground-up; most contain wrapped legacy components. While the legacy components themselves are already well-tested, imperfect integration can introduce subtle faults that are outside the prime target area of generic integration and system tests. One might postulate that focused testing for integration faults could improve the yield of detected faults when used as part of a balanced integration and system test effort. We define such a testing strategy and describe a trial application to a prototype control system. The results suggest that focused testing does not add significant value over traditional black-box testing",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883789,no,undetermined,0
Quality Monitoring on Chinese Automatic Word-segmentation Software,"Many kinds of Chinese word-segmentation software have taken shapes in recent years. It is difficult to assess which is good and which is bad, for their various calculating methods and man decisions. The article tries to propose the quality monitoring content, methods and models of the Chinese automatic word-segmentation software according to its characteristics, and to monitor and control the software quality in the different stages of its development.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4810462,no,undetermined,0
Artificial immune system based on normal model and immune learning,"Inspired form natural immune system, a new artificial immune system was proposed to detect, recognize and eliminate the non-selfs such as computer worms and software faults. Because unknown non-selfs are very difficult to detect only by recognizing the features of the non-selfs, a normal model was built to provide an easy and effective tool for completely detecting the unknown non-selfs by detecting the known selfs. The probability for detecting unknown non-selfs with traditional approaches depends on the complexity for the features of unknown non-selfs, and the usage of the selfs for detecting the non-selfs in some systems has been neglected. After the normal model is built with the space-time properties of the selfs for the systems, the probability for detecting the unknown non-selfs can be improved with the normal model. To overcome the bottleneck for finding which to recognize and how to learn the unknown worms, an adaptive immune learning model was proposed against the unknown worms, by searching in the multi-dimension feature space of worms with random evolutionary computation. The goal of the adaptive immune learning was to find the most similar known worm to the unknown worm or establish a new class for the unknown worm. The normal model and the innate immune tier on the normal model provided a better source of unknown non-selfs so that the probability for recognizing the unknown worms was increased. To recognize and learn the unknown non-selfs with uncertainty in the artificial immune system, the evolutionary immune algorithm was used to search and reason with uncertainty. At last, a prototype for using the artificial immune system in anti-worm and fault diagnosis applications validated the models.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4811468,no,undetermined,0
An approach to detecting duplicate bug reports using natural language and execution information,"An open source project typically maintains an open bug repository so that bug reports from all over the world can be gathered. When a new bug report is submitted to the repository, a person, called a triager, examines whether it is a duplicate of an existing bug report. If it is, the triager marks it as duplicate and the bug report is removed from consideration for further work. In the literature, there are approaches exploiting only natural language information to detect duplicate bug reports. In this paper we present a new approach that further involves execution information. In our approach, when a new bug report arrives, its natural language information and execution information are compared with those of the existing bug reports. Then, a small number of existing bug reports are suggested to the triager as the most similar bug reports to the new bug report. Finally, the triager examines the suggested bug reports to determine whether the new bug report duplicates an existing bug report. We calibrated our approach on a subset of the Eclipse bug repository and evaluated our approach on a subset of the Firefox bug repository. The experimental results show that our approach can detect 67%-93% of duplicate bug reports in the Firefox bug repository, compared to 43%-72% using natural language information alone.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814157,no,undetermined,0
Analysis and simulation of smart antennas for GSM and DECT in indoor environments based on ray launching modeling techniques,"A software simulation of smart antennas mounted on base stations in a wireless communication system is described. We begin with the introduction of the smart antenna considered and its basic operation: the adaptive arrays based on a temporal reference. Next, the scheme of the simulated system is analysed, and in particular the characterization of the indoor mobile radio channel with ray launching techniques. Finally, we show some simulation results, making reference to the reduction of the uncorrelated multipath contributions, the quality improvement and the rejection of co-channel interference",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=878039,no,undetermined,0
A case study in on-line intelligent sensing,"A new method is described for online detection of parameter changes in a sensor. This is based on work by Yung and Clarke (1989) which employs a local ARIMA model of the sensor output to generate an innovation sequence. A statistical test, which quantifies the change to the variance of an innovation sequence, is developed and used to provide a decision process based on a likelihood ratio of probabilities. Real-time experimental results for detecting a change in a thermocouple time-constant are presented",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=876963,no,undetermined,0
Building Bayesian network-based information retrieval systems,Bayesian networks are suitable models to deal with the information retrieval problem because they are appropriate tools to manage the intrinsic uncertainty with which this area is pervaded. In this paper we introduce several modifications to the previous works on this field adding new features and showing how a good retrieval effectiveness can be achieved by improving the quality of the Bayesian networks used in the model and tuning some of their parameters,2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=875079,no,undetermined,0
A business process explorer,"A business process is composed of a set of interrelated tasks which are joined together by control flow elements. E-commerce systems implement business processes to automate the daily operations of an organization. Organizations must continuously modify their e-commerce systems to accommodate changes to business processes. However, modifying e-commerce systems is a time consuming and error prone task. To correctly perform this task, developers require an in-depth understanding of multi-tiered e-commerce systems and the business processes that they implement. In this paper, we present a business process explorer tool which automatically recovers business processes from three tier e-commerce systems. Developers can explore the recovered business processes and browse the corresponding source code. We integrate our tool with IBM WebSphere Business Modeler (WBM), a leading commercial tool for business process management and modeling. Business analysts could then visualize and analyze the recovered processes using WBM. The business process explorer eases the co-evolution of business processes and their e-commerce system implementation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814213,no,undetermined,0
Logic representation of programs to detect arithmetic anomalies,"Much interest, especially by banks and insurance companies is paid to detect arithmetic anomalies and inexactness of arithmetic expressions. Numerous examples in the past show that although mathematical methods for correct implementation of arithmetic expressions exist and are well understood, many programs contain arithmetic anomalies, impreciseness or faults. Software tests based on conventional coverage criteria (F. Belli, 1998) and functional tests are not well suited for detection of these faults. The detection of arithmetic anomalies by these methods strongly depends on the adequateness of test cases. The selection of effective test cases needs a lot of effort to detect context-sensitive arithmetic inexactness. The authors introduce a novel approach for detecting arithmetic anomalies. The method is based on the specification of fault classes combined with the transformation of the program under test into a predicate logic model. The number of potential context-sensitive faults is deployed as a criterion to precisely select modules in large software systems to increase the test effectiveness",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=874421,no,undetermined,0
Reduction of the number of terminal assignments for detecting feature interactions in telecommunication services,"Telecommunication systems are typical complex systems. Services that independently operate normally will behave differently when simultaneously initiated with another service. This behavior is called feature interaction and is recognized to affect the dependability. This article proposes a method of dramatically reducing the computation time required for detecting feature interactions in telecommunication services. One of the knotty problems in defecting feature interactions at the specification design stage is terminal assignment. For the same service specifications, occurrence of feature interactions depends on how to assign real terminals to terminal variables in the specifications. Consequently, all terminals connected to the network have to be considered in order to detect all interactions. As a result, the number of combinations of terminal assignments is enormous. This causes huge expansion of computation time needed for detection of feature interactions. By considering equivalent states, the proposed method can reduce the number of terminal assignments to one 400th compared with that of the conventional method",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873946,no,undetermined,0
Fault detection method for the subcircuits of a cascade linear circuit,"The fault detection method for the subcircuits of a cascade linear circuit is discussed. While there is any fault (either hard or soft and either single or multiple) at one subcircuit of a cascade linear circuit, it can be quickly detected by using the method proposed. While there are faults simultaneously existing at multiple subcircuits, they can generally be detected by the searching approach proposed here. The aforementioned method is the continuation and development of the unified fault detection dictionary method for linear circuits proposed previously by the authors (see ibid., vol. 46, Oct. 1999)",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873881,no,undetermined,0
System-level test bench generation in a co-design framework,"Co-design tools represent an effective solution for reducing costs and shortening time-to-market, when System-on-Chip design is considered. In a top-down design flow, designers would greatly benefit from the availability of tools able to automatically generate test benches, which can be used during every design step, from the system-level specification to the gate-level description. This would significantly increase the chance of identifying design bugs early in the design flow, thus reducing the costs and increasing the final product quality. The paper proposes an approach for integrating the ability to generate test benches into an existing co-design tool. Suitable metrics are proposed to guide the generation, and preliminary experimental results are reported, assessing the effectiveness of the proposed technique",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873775,no,undetermined,0
Automated security checking and patching using TestTalk,"In many computer system security incidents, attackers successfully intruded computer systems by exploiting known weaknesses. Those computer systems remained vulnerable even after the vulnerabilities were known because it requires constant attention to stay on top of security updates. It is often both time-consuming and error-prone to manually apply security patches to deployed systems. To solve this problem, we propose to develop a framework for automated security checking and patching. The framework, named Securibot, provides a self-operating mechanism for security checking and patching. Securibot performs security testing using security profiles and security updates. It can also detect compromised systems using attack signatures. Most important, the Securibot framework allows system vendors to publish recently discovered security weaknesses and new patches in a machine-readable form so that the Securibot system running on deployed systems can automatically check out security updates and apply the patches",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873673,no,undetermined,0
The use of abduction and recursion-editor techniques for the correction of faulty conjectures,"The synthesis of programs, as well as other synthetic tasks, often ends up with an unprovable, partially false conjecture. A successful subsequent synthesis attempt depends on determining why the conjecture is faulty and how it can be corrected. Hence, it is highly desirable to have an automated means for detecting and correcting fault conjectures. We introduce a method for patching faulty conjectures. The method is based on abduction and performs its task during an attempt to prove a given conjecture. On input X.G(X), the method builds a definition for a corrective predicate, P(X), such that X.P(X)G(X) is a theorem. The synthesis of a corrective predicate is guided by the constructive principle of formulae as types, relating inference to computation. We take the construction of a corrective predicate as a program transformation task. The method consists of a collection of construction commands. A construction command is a small program that makes use of one or more program editing commands, geared towards building recursive, equational procedures. A synthesised corrective predicate is guaranteed to be correct, turning a faulty conjecture into a theorem. If conditional, it will be well-defined. If recursive, it will also be terminating",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873654,no,undetermined,0
Mutation operators for specifications,"Testing has a vital support role in the software engineering process, but developing tests often takes significant resources. A formal specification is a repository of knowledge about a system, and a recent method uses such specifications to automatically generate complete test suites via mutation analysis. We define an extensive set of mutation operators for use with this method. We report the results of our theoretical and experimental investigation of the relationships between the classes of faults detected by the various operators. Finally, we recommend sets of mutation operators which yield good test coverage at a reduced cost compared to using all proposed operators",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873653,no,undetermined,0
Renaming detection,"Finding changed identifiers in programs is important for program comparison and merging. Comparing two versions of a program is complicated if renaming has occurred. Textual merging is highly unreliable if, in one version, identifiers were renamed, while in the other version, code using the old identifiers was added or modified. A tool that automatically detects renamed identifiers between pairs of program modules is presented. The detector is part of a suite of intelligent differencing and merging programs that exploit the static semantics of programming languages. No special editor is needed for tracking changes. The core of the renaming detector is language independent. The detector works with multiple file pairs, taking into account renamings that affect multiple files. Renaming detectors for Java and Scheme have been implemented. A case study is presented that demonstrates proof of concept. With renaming detection, a higher quality of program comparison and merging is achievable",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=873652,no,undetermined,0
Modeling Quality of Service Adaptability,"Quality of service adaptability refers to the ability of services (or components) to adapt the quality exhibited during run-time, or to the faculty of architectural models to show that several alternatives concerning quality could be implemented. Enclosing quality properties with architectural models has been typically used to improve system understanding. Nevertheless, these properties can also be used to compose subsystems whose quality can be adapted or/and to predict the behavior of the run-time adaptability. Existing software modeling languages lack enough mechanisms to cope with adaptability, e.g. to describe software elements that may offer/require several quality levels. This paper presents concepts that such a language needs to include to model quality-adaptable systems, and how we use those concepts to compose and analyze software architectures.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814999,no,undetermined,0
Automatic image event segmentation and quality screening for albuming applications,"In this paper, a system for automatic albuming of consumer photographs is described and its specific core components of event segmentation and screening of low quality images are discussed. A novel event segmentation algorithm was created to automatically cluster pictures into events and sub-events for albuming, based on date/time meta data information as well as color content of the pictures. A new quality-screening is developed based on object quality to detect problematic images due to underexposure, low contrast, and camera defocus or movement. Performance testing of these algorithms was conducted using a database of real consumer photos and showed that these functions provide a useful first-cut album layout for typical rolls of consumer pictures. A first version of the automatic albuming application software was rested through a consumer trial in the United States from August to December 1999",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=871558,no,undetermined,0
Mining user behavior for resource prediction in interactive electronic malls,"Applications in virtual multimedia catalogs are highly interactive. Thus, it is difficult to estimate resource demands required for presentation of catalog contents. We propose a method to predict presentation resource demands in interactive multimedia catalogs. The prediction is based on the results of mining the virtual mall action log file. The log file typically contains information about previous user interests and browsing behavior. These data are used for modeling users future behavior within a session. We define heuristics to generate a start-up user behavior model as a continuous time Markov chain and adapt this model during a running session to the current user",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=871496,no,undetermined,0
On test application time and defect detection capabilities of test sets for scan designs,"The test application time of test sets for scan designs can be reduced (without reducing the fault coverage) by removing some scan operations, and increasing the lengths of the primary input sequences applied between scan operations. In this paper, we study the effects of such a compaction procedure on the ability of a test set to detect defects. Defect detection is measured by the number of times the test set detects each stuck-at fault, which was shown to be related to the defect coverage of the test set. We also propose a compaction procedure that affects the numbers of detections of stuck-at faults in a controlled way",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=878314,no,undetermined,0
Detailed radiation fault modeling of the Remote Exploration and Experimentation (REE) first generation testbed architecture,"The goal of the NASA HPCC Remote Exploration and Experimentation (REE) Project is to transfer commercial supercomputing technology into space. The project will use state of the art, low-power, non-radiation-hardened, COTS hardware chips and COTS software to the maximum extent possible, and will rely on software-implemented fault tolerance to provide the required levels of availability and reliability. We outline the methodology used to develop a detailed radiation fault model for the REE Testbed architecture. The model addresses the effects of energetic protons and heavy ions which cause single event upset and single event multiple upset events in digital logic devices and which are expected to be the primary fault generation mechanism. Unlike previous modeling efforts, this model will address fault rates and types in computer subsystems at a sufficiently fine level of granularity (i.e., the register level) that specific software and operational errors can be derived. We present the current state of the model, model verification activities and results to date, and plans for the future. Finally, we explain the methodology by which this model will be used to derive application-level error effects sets. These error effects sets will be used in conjunction with our Testbed fault injection capabilities and our applications' mission scenarios to replicate the predicted fault environment on our suite of onboard applications",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=878499,no,undetermined,0
A framework for quantifying error proneness in software,"This paper proposes a framework for assessing quantitatively the error-proneness of computer program modules. The model uses an information theory approach to derive an error proneness index, that can be used in a practical way. Debugging and testing rake at least 40% of a software project's effort, but do not uncover all defects. While current research looks at identifying problem-modules in a program, no attempt is made for a quantitative error-proneness evaluation. By quantitatively assessing a module's susceptibility to error, we are able to identify error prone paths in a program and enhance testing efficiency. The goal is to identify error prone paths in a program using genetic algorithms. This increases software reliability, aids in testing design, and reduces software development cost",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883779,no,undetermined,0
A method for intellectualized detection and fault diagnosis of vacuum circuit breakers,"In this paper a method for intellectualized detection and fault diagnosis of vacuum circuit breakers is introduced. The system consists of sensors, single-chips, measuring circuits, processing circuits, controlling circuits, extended ports, communication interface, etc. It can monitor on-line the condition of a vacuum circuit breaker, analyze its change tendency, identify and locate and display the detectable faults. This paper describes the main detecting principles and diagnostic foundations. The hardware structure and software design are also given",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879101,no,undetermined,0
A formal mechanism for assessing polymorphism in object-oriented systems,"Although quality is not easy to evaluate since it is a complex concept compound by different aspects, several properties that make a good object-oriented design have been recognized and widely accepted by the software engineering community. We agree that both the traditional and the new object-oriented properties should be analyzed in assessing the quality of object-oriented design. However, we believe that it is necessary to pay special attention to the polymorphism concept and metric, since they should be considered one of the key concerns in determining the quality of an object-oriented system. In this paper, we have given a rigorous definition of polymorphism. On top of this formalization we propose a metric that provides an objective and precise mechanism to detect and quantify dynamic polymorphism. The metric takes information coming from the first stages of the development process giving developers the opportunity to early evaluate and improve the quality of the software product. Finally, a first approach to the theoretical validation of the metric is presented",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883778,no,undetermined,0
Techniques of maintaining evolving component-based software,"Component based software engineering has been increasingly adopted for software development. Such an approach using reusable components as the building blocks for constructing software, on one hand, embellishes the likelihood of improving software quality and productivity; on the other hand, it consequently involves frequent maintenance activities, such as upgrading third party components or adding new features. The cost of maintenance for conventional software can account for as much as two-thirds of the total cost, and it can likely be even more for maintaining component based software. Thus, an effective maintenance technique for component based software is strongly desired. The authors present a technique that can be applied on various maintenance activities over component based software systems. The technique proposed utilizes a static analysis to identify the interfaces, events and dependence relationships that would be affected by the modification in the maintenance activity. The results obtained from the static analysis along with the information of component interactions recorded during the execution of each test case are used to guide test selection in the maintenance phase. The empirical results show that with 19% effort our technique detected 71% of the faults in an industrial component based system, which demonstrates the great potential effectiveness of the technique",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883054,no,undetermined,0
C/C++ conditional compilation analysis using symbolic execution,"Conditional compilation is one of the most powerful parts of a C/C++ environment available for building software for different platforms with different feature sets. Although conditional compilation is powerful, it can be difficult to understand and is error-prone. In large software systems, file inclusion, conditional compilation and macro substitution are closely related and are often largely interleaved. Without adequate tools, understanding complex header files is a tedious task. This practice may even be complicated as the hierarchies of header files grow with projects. This paper presents our experiences of studying conditional compilation based on the symbolic execution of preprocessing directives. Our two concrete goals are: for any given preprocessor directive or C/C++ source code line, finding the simplest sufficient condition to reach/compile it, and finding the full condition to reach/compile that code line. Two different strategies were used to achieve these two goals. A series of experiments conducted on the Linux kernel are presented",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883045,no,undetermined,0
Can metrics help to bridge the gap between the improvement of OO design quality and its automation?,"During the evolution of object-oriented (OO) systems, the preservation of a correct design should be a permanent quest. However, for systems involving a large number of classes and that are subject to frequent modifications, the detection and correction of design flaws may be a complex and resource-consuming task. The use of automatic detection and correction tools can be helpful for this task. Various works have proposed transformations that improve the quality of an OO system while preserving its behavior. In this paper, we investigate whether some OO metrics can be used as indicators for automatically detecting situations where a particular transformation can be applied to improve the quality of a system. The detection process is based on analyzing the impact of various transformations on these OO metrics using quality estimation models",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=883034,no,undetermined,0
Data mining algorithms for web pre-fetching,"To speed up fetching web pages, this paper gives an intelligent technology of web pre-fetching. We use a simplified WWW data model to represent the data in the cache of web browser to mine the association rules. We store these rules in a knowledge base so as to predict the user's actions. Intelligent agents are responsible for mining the users' interest and pre-fetching web pages, based on the interest association repository. In this way user browsing time has been reduced transparently",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=882851,no,undetermined,0
Integrating reliability and timing analysis of CAN-based systems,"The paper outlines and illustrates a reliability analysis method developed with a focus on CAN based automotive systems. The method considers the effect of faults on schedulability analysis and its impact on the reliability estimation of the system, and attempts to integrate both to aid system developers. We illustrate the method by modeling a simple distributed antilock braking system, showing that even in cases where the worst-case analysis deem the system unschedulable, it may be proven to satisfy its timing requirements with a sufficiently high probability. From a reliability and cost perspective, the paper underlines the tradeoffs between timing guarantees, the level of hardware and software faults, and per-unit cost",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=882547,no,undetermined,0
Model checking of workflow schemas,"Practical experience indicates that the definition of real-world workflow applications is a complex and error-prone process. Existing workflow management systems provide the means, in the best case, for very primitive syntactic verification, which is not enough to guarantee the overall correctness and robustness of workflow applications. The paper presents an approach for formal verification of workflow schemas (definitions). Workflow behaviour is modelled by means of an automata-based method, which facilitates exhaustive compositional reachability analysis. The workflow behaviour can then be analysed and checked for safety and liveness properties. The model generation and the analysis procedure are governed by well-defined rules that can be fully automated. Therefore, the approach is accessible by designers who are not experts in formal methods",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=882357,no,undetermined,0
An Efficient Probability-Based t out of n Secret Image Sharing Scheme,"Noar and Shamir presented the concept of visual cryptography. Many researches following it go down the same track: to expand the secret pixels to blocks. As a result, the size of the secret image becomes larger, and the quality of the expanded secret image becomes worse. In order to prevent the pixels from expanding, Yang has proposed his probability-based visual secret sharing scheme, where the concept of probability is employed to pick out pixels from the black or white sets. In this paper, we shall propose a new scheme that is a modified version of Yangpsilas scheme. Our experimental results show that we can obtain better recovered image quality with high contrast.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4813562,no,undetermined,0
Fault Management for Self-Healing in Ubiquitous Sensor Network,"This work concerns the development of a fault model of sensor for detecting and isolating sensor, actuator, and various faults in USNs (Ubiquitous Sensor Network). USN are developed to create relationships between humans, objects and computers in various fields. A management research of sensor nodes is very important because the ubiquitous sensor network has the numerous sensor nodes. However, Self-healing technologies are insufficient to restore when an error event occurs in a sensor node in a USN environment. A layered healing architecture for each node layer (3-tier) is needed, because most sensor devices have different capacities in USN. In this paper, we design a fault model and architecture of the sensor and sensor node separately for self-healing in USN. In order to evaluate our approach, we implement prototype of the USN fault management system to evaluate our approach. We compare the resource use of self-healing components in the general distributed computing (wired network) and the USN.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4813627,no,undetermined,0
Computer analysis of LOS microwaves links clear air performance,"Microwave links have to be designed such that propagation effects do not reduce the quality of the transmitted signals. Measurements and the derived propagation parameters are analysed and discussed, for Cluj-Napoca county, in order to improve future planning of the radio links",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=880439,no,undetermined,0
ARTOO,"Intuition is often not a good guide to know which testing strategies will work best. There is no substitute for experimental analysis based on objective criteria: how many faults a strategy finds, and how fast. """"Random"""" testing is an example of an idea that intuitively seems simplistic or even dumb, but when assessed through such criteria can yield better results than seemingly smarter strategies. The efficiency of random testing is improved if the generated inputs are evenly spread across the input domain. This is the idea of adaptive random testing (ART). ART was initially proposed for numerical inputs, on which a notion of distance is immediately available. To extend the ideas to the testing of object-oriented software, we have developed a notion of distance between objects and a new testing strategy called ARTOO, which selects as inputs objects that have the highest average distance to those already used as test inputs. ARTOO has been implemented as part of a tool for automated testing of object-oriented software. We present the ARTOO concepts, their implementation, and a set of experimental results of its application. Analysis of the results shows in particular that, compared to a directed random strategy, ARTOO reduces the number of tests generated until the first fault is found, in some cases by as much as two orders of magnitude. ARTOO also uncovers faults that the random strategy does not find in the time allotted, and its performance is more predictable.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814118,no,undetermined,0
Early prediction of software component reliability,"The ability to predict the reliability of a software system early in its development, e.g., during architectural design, can help to improve the system's quality in a cost-effective manner. Existing architecture-level reliability prediction approaches focus on system-level reliability and assume that the reliabilities of individual components are known. In general, this assumption is unreasonable, making component reliability prediction an important missing ingredient in the current literature. Early prediction of component reliability is a challenging problem because of many uncertainties associated with components under development. In this paper we address these challenges in developing a software component reliability prediction framework. We do this by exploiting architectural models and associated analysis techniques, stochastic modeling approaches, and information sources available early in the development lifecycle. We extensively evaluate our framework to illustrate its utility as an early reliability prediction approach.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814122,no,undetermined,0
Automatic modularity conformance checking,"According to Parnas's information hiding principle and Baldwin and Clark's design rule theory, the key step to decomposing a system into modules is to determine the design rules (or in Parnas's terms, interfaces) that decouple otherwise coupled design decisions and to hide decisions that are likely to change in independent modules. Given a modular design, it is often difficult to determine whether and how its implementation realizes the designed modularity. Manually comparing code with abstract design is tedious and error-prone. We present an automated approach to check the conformance of implemented modularity to designed modularity, using design structure matrices as a uniform representation for both. Our experiments suggest that our approach has the potential to manifest the decoupling effects of design rules in code, and to detect modularity deviation caused by implementation faults. We also show that design and implementation models together provide a comprehensive view of modular structure that makes certain implicit dependencies within code explicit.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4814152,no,undetermined,0
Protection of gate movement in hydroelectric power stations,"Movement of the gates is an everyday task performed on a dam of hydroelectric a power station. This operation is often controlled remotely by measuring the positioning of the gates and a level of the current in the driving motors. This method of control cannot, however, detect anomalies, such as asymmetric movement of gates, faults in drive gearwheel etc. We therefore decided to devise a new improved system for the protection of gate movement which is described in our paper. It is based on measuring the forces applied to the transmission construction. The transducers with resistive strain gauges are mounted on the frame bearings and the strains are subsequently measured. The output signal from the transducer is proportional to a force applied to the frame. The transducers are installed at the points of the largest strain. For the protection against uneven movement of the left and right chains, the strain transmitters are inserted in the bearings of the main gearwheel, to measure the compression. The whole system is controlled by the microprocessor. The details on sensors, the electronic instrumentation needed, and the software of the controlling computer, are also described in the paper. This system has been tested, and regularly used, on the power stations of Drava river in Slovenia.",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879713,no,undetermined,0
"A reusable state-based guidance, navigation and control architecture for planetary missions","JPL has embarked on the Mission Data System (MDS) project to produce a reusable, integrated flight and ground software architecture. This architecture will then be adapted by future JPL planetary projects to form the basis of their flight and ground software. The architecture is based on identifying the states of the system under consideration. States include aspects of the system that must be controlled to accomplish mission objectives, as well as aspects that are uncontrollable but must be known. The architecture identifies methods to measure, estimate, model, and control some of these states. Some states are controlled by goals, and the natural hierarchy of the system is employed by recursively elaborating goals until primitive control actions are reached. Fault tolerance emerges naturally from this architecture. Failures are detected as discrepancies between state and model-based predictions of state. Fault responses are handled either by re-elaboration of goals, or by failures of goals invoking re-elaboration at higher levels. Failure modes an modelled as possible behaviors of the system, with corresponding state estimation processes. Architectural patterns are defined for concepts such as states, goals, and measurements. Aspects of state are captured in a state-analysis database. UML is used to capture mission requirements as Use Cases and Scenarios. Application of the state-based concepts to specific states is also captured in UML, achieving architectural consistency by adapting base classes for all architectural patterns",2000,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879294,no,undetermined,0
"Panel discussion: What makes good research in modeling and simulation: Assessing the quality, success, and utility of M&S research","This paper presents the Aposition papersA contributed by the participants of a panel at the 2008 Winter Simulation Conference. As the paper pre-dates the actual panel, the purpose of the paper is to provide some background information about the views of the individual panelists prior to the actual panel. Each panelist was asked to submit a position paper addressing the general question of AWhat makes good Modeling and Simulation research?A This paper presents a summary of these position papers along with an introduction and conclusion aimed at identifying the common themes to setup the conference panel.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736130,no,undetermined,0
Using reading techniques to focus inspection performance,"Software inspection is a quality assurance method to detect defects early during the software development process. For inspection planning there are defect detection techniques, so-called reading techniques, which let the inspection planner focus the effectiveness of individual inspectors on specific sets of defects. For realistic planning it is important to use empirically evaluated defect detection techniques. We report on the replication of a large-scale experiment in an academic environment. The experiment evaluated the effectiveness of defect detection for inspectors who use a checklist or focused scenarios on individual and team level. A main finding of the experiments is that the teams were effective to find defects: In both experiments the inspection teams found on average more than 70% of the defects in the product. The checklist consistently was overall somewhat more effective on individual level, while the scenarios traded overall defect detection effectiveness for much better effectiveness regarding their target focus, in our case specific parts of the documents. Another main result of the study is that scenario-based reading techniques can be used in inspection planning to focus individual performance without significant loss of effectiveness on team level",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=952461,no,undetermined,0
Mobile database procedures in MDBAS,"MDBAS is a prototype of a multidatabase management system based on mobile agents. The system integrates a set of autonomous databases distributed over a network, enables users to create a global database scheme, and manages transparent distributed execution of user requests and procedures including distributed transactions. The paper highlights the issues related to mobile database procedures, especially the MDBAS execution strategy. In order to adequately assess MDBAS's qualities and bottlenecks, we have carried out complex performance evaluation with real databases distributed in a real Internet. The evaluation included a comparison to a commercial database with distributed database capabilities. The most interesting results are presented and commented",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953118,no,undetermined,0
A test station health monitoring system [military aircraft],"This paper presents a process to monitor test station health using the Weibull method and statistical patterns. The methodology is currently being applied to the F-16 automated test equipment (ATE) at the Ogden, Utah Air Logistic Center (OO-ALC) maintenance depot. An automated stream of test data collected from ATEs is used to process test results and to identify improvements necessary to increase the failure forecast accuracy. The paper discusses solutions to identify causes of 're-test OK' (RTOK) due to discrepancies between software testing procedures in the line and shop repairable units. The process includes a decision support system that uses artificial intelligence methods, such as expert system and neural networks, and a knowledge database to improve the troubleshooting capability. The paper also discusses a prototype development that collects malfunction codes (MFL) originated by the aircraft bus monitoring system. The MFL information is correlated with test results to detect RTOK causes.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1036157,no,undetermined,0
First experiments relating behavior selection architectures to environmental complexity,"Assessing the performance of behavior selection architectures for autonomous robots is a complex task that depends on many factors. This paper reports a study comparing four motivated behavior-based architectures in different worlds with varying degrees and types of complexity, and analyzes performance results (in terms of viability, life span, and global life quality) relating architectural features to environmental complexity.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041732,no,undetermined,0
Managing software projects with business-based requirements,"For many organizations that are neither software product companies nor system integrators, the expense and cultural change required for full process rollout can be prohibitive. Proponents of agile processes/methods (such as extreme programming) suggest that these """"lightweight"""" approaches are extremely effective. I would agree that there are many powerful aspects within these approaches. I suggest, however, that by taking an objective-based business requirements approach to project management, software projects have a high probability of running on time, and remaining in scope and within budget. Addressing requirement challenges, independent of adopting a full process, can offer many of the benefits of full process adoption while avoiding most of the expense and human issues involved with full process rollout. A business-based requirements approach is an easy-to-adopt, risk-free entry point that offers tangible quality improvements. This approach suits any project scope. Whether building a complex system for enterprise resource planning or customer relationship management, or developing small, single-user software programs, defining business requirements improves any system delivery.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041174,no,undetermined,0
Quality-based tuning of cell downlink load target and link power maxima in WCDMA,"The objective of the paper is to validate the feasibility of auto-tuning WCDMA link power maxima and adjust cell downlink load level targets based on quality of service. The downlink cell load level is measured using total wideband transmission power. The quality indicators used are call-blocking probability, packet queuing probability and downlink link power outage. The objective is to improve performance and operability of the network with control software aiming for a specific quality of service. The downlink link maxima in each cell are regularly adjusted with a control method in order to improve performance under different load level targets. The approach is validated using a dynamic WCDMA system simulator. The conducted simulations support the assumption that the downlink performance can be managed and improved by the proposed cell-based automated optimization.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1040620,no,undetermined,0
A Universal Fault Diagnostic Expert System Based on Bayesian Network,"Fault diagnosis is an area of great concern of any industry to reduce maintenance cost and increase profitability in the mean time. But most of the researches tend to rely on sensor data and equipment structure, which are expensive because each category of equipment differs from the others. Thus developing a universal system remains a key challenge to be solved. A universal expert system is developed in this paper making full use of expertspsila knowledge to diagnose the possible root causes and the corresponding probabilities for maintenance decision making support. Bayesian network was chosen as the inference engine of the system through raw data analysis. Improved causal relationship questionnaire and probability scale method were applied to construct the Bayesian network. The system has been applied to the production line of a chipset factory and the results show that the system can support decision making for fault diagnosis promptly and correctly.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721738,no,undetermined,0
Data-based adviser to operators of complex processes,"A probabilistic advisory tool for operators of complex processes is being developed in the framework of the ProDaCTool international project. The project was motivated by the need to maintain the highest possible quality of the product - metal strip processed on a cold rolling mill - under various conditions. Even though all particular rolling mill controllers are tuned properly, there is a lot of possible settings of manually adjusted parameters which influence the quality of production. When, in addition, the rolling mill processes a variety of material types, it is difficult to find out the causes of potential slight deviations in quality.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1038708,no,undetermined,0
A fuzzy sets approach to new product portfolio management,"The evaluation of R&D projects in a high technology firm is very important. A lot of them quite often do not lead to new products as management did not take into consideration indexes such as probability of commercial success, technological success, strategic fit, etc which cannot be expressed in a quantitative form. An efficient and reliable approach for evaluating R&D projects capable of handling simultaneously the quantitative and qualitative criteria involved based on the theory of fuzzy logic is presented and a software model of the approach has been developed and tested in a real environment. It is a multiple criteria decision-making method where all projects are rated according to a number of quantitative and qualitative criteria capturing possibilities of technical and commercial success and the consistency of the projects with business strategy. We report on the criteria used for the evaluation of the projects and on the operation of the software model.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1038480,no,undetermined,0
Using SPIN model checking for flight software verification,"Flight software is the central nervous system of modern spacecraft. Verifying spacecraft flight software to assure that it operates correctly and safely is presently an intensive and costly process. A multitude of scenarios and tests must be devised, executed and reviewed to provide reasonable confidence that the software will perform as intended and not endanger the spacecraft. Undetected software defects on spacecraft and launch vehicles have caused embarrassing and costly failures in recent years. Model checking is a technique for software verification that can detect concurrency defects that are otherwise difficult to discover. Within appropriate constraints, a model checker can perform an exhaustive state-space search on a software design or implementation and alert the implementing organization to potential design deficiencies. Unfortunately, model checking of large software systems requires an often-too-substantial effort in developing and maintaining the software functional models. A recent development in this area, however, promises to enable software-implementing organizations to take advantage of the usefulness of model checking without hand-built functional models. This development is the appearance of """"model extractors"""". A model extractor permits the automated and repeated testing of code as built rather than of separate design models. This allows model checking to be used without the overhead and perils involved in maintaining separate models. We have attempted to apply model checking to legacy flight software from NASA's Deep Space One (DS1) mission. This software was implemented in C and contained some known defects at launch that are detectable with a model checker. We describe the model checking process, the tools used, and the methods and conditions necessary to successfully perform model checking on the DS1 flight software.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1036832,no,undetermined,0
Upgrading engine test cells for improved troubleshooting and diagnostics,"Upgrading military engine test cells with advanced diagnostic and troubleshooting capabilities will play a critical role in increasing aircraft availability and test cell effectiveness while simultaneously reducing engine operating and maintenance costs. Sophisticated performance and mechanical anomaly detection and fault classification algorithms utilizing thermodynamic, statistical, and empirical engine models are now being implemented as part of a United States Air Force Advanced Test Cell Upgrade Initiative. Under this program, a comprehensive set of realtime and post-test diagnostic software modules, including sensor validation algorithms, performance fault classification techniques and vibration feature analysis are being developed. An automated troubleshooting guide is also being implemented to streamline the troubleshooting process for both inexperienced and experienced technicians. This artificial intelligence based tool enhances the conventional troubleshooting tree architecture by incorporating probability of occurrence statistics to optimize the troubleshooting path. This paper describes the development and implementation of the F404 engine test cell upgrade at the Jacksonville Naval Air Station.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1036142,no,undetermined,0
An Immune Algorithm-Based Atmospheric Quality Assessment Model and Its Applications,"Atmospheric quality assessment is an important research subject, focusing on the evaluation of the quality of atmospheric environment, a model based on the immune algorithm (IM) is proposed in this paper. The model has the characters of pellucid principle and physical explication. Moreover, the simplification is the important advantage of the method. Experimental results show that the proposed model is effective and feasible for assessing atmospheric quality. It could provide a new reference basis and approach in the field of environment. Therefore it has great potential in the field of assessment the atmospheric quality.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721973,no,undetermined,0
A new audio skew detection and correction algorithm,"The lack of synchronisation between a sender clock and a receiver audio clock in an audio application results in an undesirable effect known as """"audio skew"""". This paper proposes and implements a new approach to detecting and correcting audio skew, focusing on the accuracy of measurements and on the algorithm's effect on the audio experience of the listener. The algorithms presented are shown to remove audio skew successfully, thus reducing delay and loss and hence improving audio quality.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035569,no,undetermined,0
Fault injection experiment results in space borne parallel application programs,"Development of the REE Commercial-Off-The-Shelf (COTS) based space-borne supercomputer requires a detailed knowledge of system behavior in the presence of Single Event Upset (SEU) induced faults. When combined with a hardware radiation fault model and mission environment data in a medium grained system model, experimentally obtained fault behavior data can be used to: predict system reliability, availability and performance; determine optimal fault detection methods and boundaries; and define high ROI fault tolerance strategies. The REE project has developed a fault injection suite of tools and a methodology for experimentally determining system behavior statistics in the presence of application level SEU induced transient faults. Initial characterization of science data application code for an autonomous Mars Rover geology application indicates that this code is relatively insensitive to SEUs and thus can be made highly immune to application level faults with relatively low overhead strategies.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035379,no,undetermined,0
Probability-Based Binary Particle Swarm Optimization Algorithm and Its Application to WFGD Control,"Sulfur dioxide is an air pollutant and an acid rain precursor. Coal-fired power generating plants are major sources of sulfur dioxide, so the limestone-gypsum wet flues gas desulphurization (WFGD) technology has been widely used in thermal power plant in China nowadays to reduce the emission of sulfur dioxide and protect the environment. The absorber slurry pH value control is very important for limestone-gypsum WFGD technique since it directly determine the desulphurization performance and the quality of product. However, it is hard to achieve the satisfactory adjustment performance for the traditional PID controller because of the complexity of the absorber slurry pH control. To tackle this problem, a novel probability-based binary particle swarm optimization (PBPSO) is proposed to tuning the PID parameters. The simulation results show PBPSO is valid and outperform the traditional binary PSO algorithm in terms of easy implementation and global optimal search ability. And it also presents that the proposed PBPSO algorithm can search the optimal PID parameters and achieves the expected control performance with PID controller by constructing the proper fitness function of PID tuning.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721782,no,undetermined,0
Neighborhood selection for I<sub>DDQ</sub> outlier screening at wafer sort,"To screen defective dies, I<sub>DDQ</sub> tests require a reliable estimate of each die's defect-free measurement. The nearest-neighbor residual (NNR) method provides a straightforward, data-driven estimate of test measurements for improved identification of die outliers",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1033795,no,undetermined,0
Strategy to improve the indoor coverage for mobile station,"This paper presents an evaluation of whether the indoor signal strength for commercial buildings fulfills the cell planning requirement. In order to provide high quality cellular service, it is necessary to place an array of distributed antennas connected using feeder cable within the building. With the feeder cable approach, splitters and computer software, we develop using Visual Basic 6.0. The effective radiated power (ERP) at the distributed antenna can be calculated and it can also be predicted how far the signal can go with the calculated ERP. This design process can be used to obtain an estimated indoor system requirement. All of the requirements can be achieved by applying the method in this paper.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1033145,no,undetermined,0
Image processing application in seismic reflection to evaluate geohazard region,"GeoJava is GUI (graphical user interface) software that is powerful, yet very simple to use, providing the means to filter color images in RGB (red-green-blue) color space and YUV color space, to produce high quality filtered images. The YUV color space model has been proved to filter images smoothly without losing any data, only enhancing image structure. Different images were used, and satisfactory results were obtained of multi-application objectives. Image processing enhances the images of the CDP (common depth midpoint) seismic reflection sections using different stacking filters to provide supplementary results that are useful for assessing geohazard zones. The results in images clarify the exact location and geometry of cavities and sinkholes, beside localizing the areas that are under stress, and also delineate weak zones.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1033137,no,undetermined,0
Conflict Resolution within Multi-agent System in Collaborative Design,"A critical challenge to create effective agent-based systems is allowing them to operate effectively when the design environment is complex, dynamic, and error-prone. Much research has been made on multi-agent system (MAS) in CSCD field, especially on conflict handling, which is not yet well-addressed due to its sophistication. Conflict resolution plays a central role in collaborative design. In this paper, a prototype system of conflict resolution is presented, adopting solution of deadlock problems in the operating system to conflict prevention and classifying the conflicts to match solution strategies according to their features. Video conference negotiation strategy based on MICAD(Multimedia Intelligent CAD) platform, which is researched and implemented by our team, is adopted to deal with new conflicts that can not get matching strategies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721801,no,undetermined,0
Static analysis of SEU effects on software applications,"Control flow errors have been widely addressed in literature as a possible threat to the dependability of computer systems, and many clever techniques have been proposed to detect and tolerate them. Nevertheless, it has never been discussed if the overheads introduced by many of these techniques are justified by a reasonable probability of incurring control flow errors. This paper presents a static executable code analysis methodology able to compute, depending on the target microprocessor platform, the upper-bound probability that a given application incurs in a control flow error.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041800,no,undetermined,0
Application of high-quality built-in test to industrial designs,"This paper presents an approach for high-quality built-in test using a neighborhood pattern generator (NPG). The proposed NPG is practically acceptable because (a) its structure is independent of circuit under test, (b) it requires low area overhead and no performance degradation, and (c) it can encode deterministic test cubes, not only for stuck-at faults but also transition faults, with high probability. Experimental results for large industrial circuits illustrate the efficiency of the proposed approach.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041856,no,undetermined,0
Measuring Web application quality with WebQEM,"This article discusses using WebQEM, a quantitative evaluation strategy to assess Web site and application quality. Defining and measuring quality indicators can help stakeholders understand and improve Web products. An e-commerce case study illustrates the methodology's utility in systematically assessing attributes that influence product quality",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041945,no,undetermined,0
The use of Kohonen self-organizing maps in process monitoring,"Process monitoring and fault diagnosis have been studied widely in recent years, and the number of industrial applications with encouraging results has grown rapidly. In the case of complex processes a computer aided monitoring enhances operators' possibilities to run the process economically. In this paper a fault diagnosis system is described and some application results from the Outokumpu Harjavalta smelter are discussed. The system monitors process states using neural networks (Kohonen self-organizing maps, SOM) in conjunction with heuristic rules, which are also used to detect equipment malfunctions.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1042576,no,undetermined,0
Towards an impact analysis for component based real-time product line architectures,"In this paper we propose a method for predicting the consequences of adding new components to an existing product line in the real-time systems domain. We refer to such a prediction as an impact analysis. New components are added as new features are introduced in the product line. Adding components to a real-time system may affect the temporal correctness of the system. In our approach to product line architectures, products are constructed by assembling components. By having a prediction enabled component technology as the underlying component technology, we can predict the behavior of an assembly of components. We demonstrate our approach by an example in which temporal correctness and consistency between versions of components is predicted.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046137,no,undetermined,0
Automated software robustness testing - static and adaptive test case design methods,"Testing is essential in the development of any software system. Testing is required to assess a system's functionality and quality of operation in its final environment. This is especially of importance for systems being assembled from many self-contained software components. In this article, we focus on automated testing of software component robustness, which is a component's ability to handle invalid input data or environmental conditions. We describe how large numbers of test cases can effectively and automatically be generated from small sets of test values. However, there is a great demand on ways to efficiently reduce this mass of test cases as actually executing them on a data processing machine would be too time consuming and expensive. We discuss static analytic methods for test case reduction and some of the disadvantages they bring. Finally a more intelligent and efficient approach is introduced, the Adaptive Test Procedure for Software Robustness Testing developed at ABB Corporate Research in Ladenburg. Along with these discussions the need for intelligent test approaches is illustrated by the Ballista methodology for automated robustness testing of software component interfaces. An object-oriented approach based on parameter data types rather than component functionality essentially eliminates the need for function-specific test scaffolding.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046134,no,undetermined,0
Improving the robustness of MPEG-4 video communications over wireless/3G mobile networks,"Two major issues in providing true end-to-end wireless/mobile video capabilities are: interoperability among network platforms and robustness of video compression algorithms in error-prone environments. In this paper, we mainly focus on the second issue and show how error resilience techniques can be used to improve the video quality. We argue that the error resilient tools provided within the MPEG-4 standard are not sufficient to provide acceptable quality in wireless/mobile networks, but that this quality can be significantly improved by the inclusion of hierarchical MPEG-4 video coding techniques. We present a novel hierarchical MPEG-4 video scheme particularly designed for video communications over QoS-capable wireless/mobile network.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045466,no,undetermined,0
Maintenance in joint software development,"The need to combine several efforts in software development has become critical because of the software development requirements and geographically dispersed qualified human resources, background skills, working methods, and software tools among autonomous software enterprises - joint software development. We know that the organization and the development of software depend largely on human initiatives. All human initiatives are subject to change and perpetual evolution. A variety of studies have contributed to highlight the problems arising from the change and the perpetual evolution of the software development. These studies revealed that the majority of the effort spent on the software process is spent on maintenance. To reduce the efforts of software maintenance in joint software development, it is therefore necessary to detect the features of software maintenance processes susceptible to be automatized. The software maintenance problems in joint software development are caused by the changes and perpetual evolutions at the organizational level of an enterprise or at the software development level. The complex nature of changes and perpetual evolutions at the organizational and the development levels includes the following factors: the maintenance contracts among enterprises, the abilities to develop, experiences and background in software development, methodologies of software development, tools available and localization of the organizations. This paper looks into the new software maintenance problems in joint software development by geographically dispersed virtual enterprises",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045152,no,undetermined,0
Tool support for distributed inspection,"Software inspection is one of the best practices for detecting and removing defects early in the software development process. We present a tool to support geographically distributed inspection teams. The tool adopts a reengineered inspection process to minimize synchronous activities and coordination problems, and a lightweight architecture to maximize easy of use and deployment.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045151,no,undetermined,0
A structured approach to handling on-line interface upgrades,"The integration of complex systems out of existing systems is an active area of research and development. There are many practical situations in which the interfaces of the component systems, for example belonging to separate organisations, are changed dynamically and without notification. In this paper we propose an approach to handling such upgrades in a structured and disciplined fashion. All interface changes are viewed as abnormal events and general fault tolerance mechanisms (exception handling, in particular) are applied to dealing with them. The paper outlines general ways of detecting such interface upgrades and recovering after them. An Internet Travel Agency is used as a case study",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045137,no,undetermined,0
Private information retrieval in the presence of malicious failures,"In the application domain of online information services such as online census information, health records and real-time stock quotes, there are at least two fundamental challenges: the protection of users' privacy and the assurance of service availability. We present a fault-tolerant scheme for private information retrieval (FT-PIR) that protects users' privacy and ensure service provision in the presence of malicious server failures. An error detection algorithm is introduced into this scheme to detect the corrupted results from servers. The analytical and experimental results show that the FT-PIR scheme can tolerate malicious server failures effectively and prevent any information of users front being leaked to attackers. This new scheme does not rely on any unproven cryptographic premise and the availability of tamperproof hardware. An implementation of the FT-PIR scheme on a distributed database system suggests just a modest level of performance overhead.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045104,no,undetermined,0
Edge color distribution transform: an efficient tool for object detection in images,"Object detection in images is a fundamental task in many image analysis applications. Existing methods for low-level object detection always perform the color-similarity analyses in the 2D image space. However, the crowded edges of different objects make the detection complex and error-prone. The paper proposes to detect objects in a new edge color distribution space (ECDS) rather than in the image space. In the 3D ECDS, the edges of different objects are segregated and the spatial relation of a same object is kept as well, which make the object detection easier and less error-prone. Since uniform-color objects and textured objects have different distribution characteristics in ECDS, the paper gives a 3D edge-tracking algorithm for the former and a cuboid-growing algorithm for the latter. The detection results are correct and noise-free, so they are suitable for the high-level object detection. The experimental results on a synthetic image and a real-life image are included.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044814,no,undetermined,0
Rejection strategies and confidence measures for a k-NN classifier in an OCR task,"In handwritten character recognition, the rejection of extraneous patterns, like image noise, strokes or corrections, can improve significantly the practical usefulness of a system. In this paper a combination of two confidence measures defined for a k-nearest neighbors (NN) classifier is proposed. Experiments are presented comparing the performance of the same system with and without the new rejection rules.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044806,no,undetermined,0
Application of hazard analysis to software quality modelling,"Quality is a fundamental concept in software and information system development. It is also a complex and elusive concept. A large number of quality models have been developed for understanding, measuring and predicting quality of software and information systems. It has been recognised that quality models should be constructed in accordance to the specific features of the application domain. This paper proposes a systematic method for constructing quality models of information systems. A diagrammatic notation is devised to represent quality models that enclose application specific features. Techniques of hazard analysis for the development and deployment of safety related systems are adapted for deriving quality models from system architectural designs. The method is illustrated by a part of Web-based information systems.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044544,no,undetermined,0
Miro - middleware for mobile robot applications,"Developing software for mobile robot applications is a tedious and error-prone task. Modern mobile robot systems are distributed systems, and their designs exhibit large heterogeneity in terms of hardware, operating systems, communications protocols, and programming languages. Vendor-provided programming environments have not kept pace with recent developments in software technology. Also, standardized modules for certain robot functionalities are beginning to emerge. Furthermore, the seamless integration of mobile robot applications into enterprise information processing systems is mostly an open problem. We suggest the construction and use of object-oriented robot middleware to make the development of mobile robot applications easier and faster, and to foster portability and maintainability of robot software. With Miro, we present such a middleware, which meets the aforementioned requirements and has been ported to three different mobile platforms with little effort. Miro also provides generic abstract services like localization or behavior engines, which can be applied on different robot platforms with virtually no modifications.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044362,no,undetermined,0
An Ant Colony Optimization Algorithm Based on the Nearest Neighbor Node Choosing Rules and the Crossover Operator,"The ant colony optimization algorithm (ACO) has a powerful capacity to find out solutions to combinatorial optimization problems, but it still has two defects, namely, it is slow in convergence speed and is prone to falling in the local optimal solution. Against the deficiencies of this algorithm, in this study we proposed an ACO based on basic ACO algorithm based on well-distributed on the initiation, the nearest neighbor node choosing rules and with crossover operator. In the initiation of the algorithm, the convergence speed of the ACO is increased by distributing the ant colony evenly in all the cities and adopting the nearest neighbor choosing node rule and making crossover computation among better individual ants at the end of each round of cycle when each ant chooses the next city. The experiment results indicate that the ACO proposed in this study is valid.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721704,no,undetermined,0
Software reliability growth with test coverage,"Software test-coverage measures"""" quantify the degree of thoroughness of testing. Tools are now available that measure test-coverage in terms of blocks, branches, computation-uses, predicate-uses, etc. that are covered. This paper models the relations among testing time, coverage, and reliability. An LE (logarithmic-exponential) model is presented that relates testing effort to test coverage (block, branch, computation-use, or predicate-use). The model is based on the hypothesis that the enumerable elements (like branches or blocks) for any coverage measure have various probabilities of being exercised; just like defects have various probabilities of being encountered. This model allows relating a test-coverage measure directly with defect-coverage. The model is fitted to 4 data-sets for programs with real defects. In the model, defect coverage can predict the time to next failure. The LE model can eliminate variables like test-application strategy from consideration. It is suitable for high reliability applications where automatic (or manual) test generation is used to cover enumerables which have not yet been tested. The data-sets used suggest the potential of the proposed model. The model is simple and easily explained, and thus can be suitable for industrial use. The LE model is based on the time-based logarithmic software-reliability growth model. It considers that: at 100% coverage for a given enumerable, all defects might not yet have been found.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044339,no,undetermined,0
Error detection by selective procedure call duplication for low energy consumption,"As commercial off-the-shelf (COTS) components are used in system-on-chip (SoC) design technique that is widely used from cellular phones to personal computers, it is difficult to modify hardware design to implement hardware fault-tolerant techniques and improve system reliability. Two major concerns of this paper are to: (a) improve system reliability by detecting transient errors in hardware, and (b) reduce energy consumption by minimizing error-detection overhead. The objective of this new technique, selective procedure call duplication (SPCD) is to keep the system fault-secured (preserve data integrity) in the presence of transient errors, with minimum additional energy consumption. The basic approach is to duplicate computations and then to compare their results to detect errors. There are 3 choices for duplicate computation: (1) duplicating every statement in the program and comparing results, (2) re-executing procedures through duplicated procedure calls, and comparing results, and (3) re-executing the whole program, and comparing the final results. SPDC combines choices (1) and(2). For a given program, SPCD analyzes procedure-call behavior of the program, and then determines which procedures can have duplicated statements [choice(1)] and which procedure calls can be duplicated [choice (2)] to minimize energy consumption with reasonable error-detection latency. Then, SPCD transforms the original program into a new program that can detect errors with minimum additional energy consumption by re-executing the statements or procedures. SPCD was simulated with benchmark programs; it requires less than 25% additional energy for error detection than previous techniques that do not consider energy consumption.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044337,no,undetermined,0
Fuzzy logic system for fuzzy event tree computing,"The paper presents the authors' contribution in developing a fuzzy logic system for event-tree analysis. The fuzzy event-tree method can be used for the protection and automation of power systems independent safety analysis. The main contribution of the proposed analysis is the evaluation or the general fuzzy conclusion named """"general safety"""" associated to all the paths in the tree. A complex software tool named """"Fuzzy Event Tree Analysis"""" had to be elaborated. The program allows """"general safety"""" fuzzy parameter computing and also protected power system-protection system critical analysis.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044240,no,undetermined,0
Optimizing test strategies during PCB design for boards with limited ICT access,"Engineers have used past experience or subjective preference as a means for assigning test strategies to new products without analyzing the benefits and weaknesses of various different test approaches in a quantitative manner. DFT (design for test) software tools that enable testability analysis during board design allow test engineers to work concurrently with designers. Case study results demonstrate that coverage predicted by DFT software is realistic when compared to actual fault coverage achieved in production. Using DFT software during PCB design to model the fault coverage of different test strategies and make ICT (in-circuit test) access tradeoffs can significantly reduce cost and improve quality. Defect capture rates more than doubled when using alternate test strategies and production line beat rates varied significantly depending on the test strategy chosen. When DFT software enables these decisions early in the product life cycle, both OEMs and EMS providers can win by driving cost reductions through the entire product life cycle from NPI (new product introduction) through manufacturing and warranty.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1032780,no,undetermined,0
Predictable instruction caching for media processors,"The determinism of instruction cache performance can be considered a major problem in multimedia devices which hope to maximise their quality of service. If instructions are evicted from the cache by competing blocks of code, the running application will take significantly longer to execute than if the instructions were present. Since it is difficult to predict when this interference will occur the performance of the algorithm at a given point in time is unclear We propose the use of an automatically configured partitioned cache to protect regions of the application code from each other and hence minimise interference. As well as being specialised to the purpose of providing predictable performance, this cache can be specialised to the application being run, rather than for the average case, using simple compiler algorithms.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1030713,no,undetermined,0
"An integrated approach to flow, thermal and mechanical modeling of electronics devices","The future success of many electronics companies will depend to a large extent on their ability to initiate techniques that bring schedules, performance, tests, support, production, life-cycle-costs, reliability prediction and quality control into the earliest stages of the product creation process. Earlier papers have discussed the benefits of an integrated analysis environment for system-level thermal, stress and EMC prediction. This paper focuses on developments made to the stress analysis module and presents results obtained for an SMT resistor. Lifetime predictions are made using the Coffin-Manson equation. Comparison with the creep strain energy based models of Darveaux (1997) shows the shear strain based method to underestimate the solder joint life. Conclusions are also made about the capabilities of both approaches to predict the qualitative and quantitative impact of design changes.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1012545,no,undetermined,0
Using version control data to evaluate the impact of software tools: a case study of the Version Editor,"Software tools can improve the quality and maintainability of software, but are expensive to acquire, deploy, and maintain, especially in large organizations. We explore how to quantify the effects of a software tool once it has been deployed in a development environment. We present an effort-analysis method that derives tool usage statistics and developer actions from a project's change history (version control system) and uses a novel effort estimation algorithm to quantify the effort savings attributable to tool usage. We apply this method to assess the impact of a software tool called VE, a version-sensitive editor used in Bell Labs. VE aids software developers in coping with the rampant use of certain preprocessor directives (similar to #if/#endif in C source files). Our analysis found that developers were approximately 40 percent more productive when using VE than when using standard text editors.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1019478,no,undetermined,0
Formally verified Byzantine agreement in presence of link faults,"This paper shows that deterministic consensus in synchronous distributed systems with link faults is possible, despite the impossibility result of Gray (1978). Instead of using randomization, we overcome this impossibility by moderately restricting the inconsistency that link faults may cause system-wide. Relying upon a novel hybrid fault model that provides different classes of faults for both nodes and links, we provide a formally verified proof that the m+1-round Byzantine agreement algorithm OMH (Lincoln and Rushby (1993)) requires n > 2f<sub>l</sub><sup>s</sup> + f<sub>l</sub><sup>r</sup> + f<sub>l</sub><sup>ra</sup> + 2(f<sub>a</sub> + f<sub>s</sub>) + f<sub>o</sub> + f<sub>m</sub> + m nodes for transparently masking at most f<sub>l</sub><sup>s</sup> broadcast and f<sub>l</sub><sup>r</sup> receive link faults (including at most f<sub>l</sub><sup>ra</sup> arbitrary ones) per node in each round, in addition to at most f<sub>a</sub>, f<sub>s</sub>, f<sub>o</sub>, f<sub>m</sub> arbitrary, symmetric, omission, and manifest node faults, provided that m  f<sub>a</sub> + f<sub>o</sub> + 1. Our approach to modeling link faults is justified by a number of theoretical results, which include tight lower bounds for the required number of nodes and an analysis of the assumption coverage in systems where links fail independently with some probability p.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1022311,no,undetermined,0
Reliability assessment of network elements using black box testing,"In this paper, we outline a procedure for quality assurance of network elements before their deployment. Software reliability is assessed using two models: a process-centric model (Musa's (1987) basic model) and a product-centric model (proposed by Hoeflin (2000)). Simultaneous use of both approaches is for sensitivity analysis of the results. In addition, we introduce the concept of deployability to measure the degree of confidence on the decision to deploy the equipment in the field.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021796,no,undetermined,0
Solving the consensus problem in a dynamic group: an approach suitable for a mobile environment,"It is now well recognised that the consensus problem is a fundamental problem when one has to implement fault-tolerant distributed services. We extend the consensus paradigm to asynchronous distributed mobile systems prone to disconnection and process crash failures. The paper, first, shows that a consensus problem between mobile hosts is reducible to two agreement problems (a consensus problem and a group membership problem) between fixed hosts. Then, following an approach investigated by Guerraoui and Schiper (see IEEE Transactions on Software Engineering, vol.27, no.1, p.29-41, 2001), the paper uses a genetic consensus service as a basic building block to construct a modular and simple solution.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021697,no,undetermined,0
The application of a distributed system-level diagnosis algorithm in dynamic positioning system,"This paper introduces the application of a distributed system-level fault diagnosis algorithm for detecting and diagnosing faulty processors in dynamic positioning system (DPS) of an offshore vessel. The system architecture of DPS is a loose coupling distributed multiprocessor system, which adopts the technique of Intel's MULTIBUS II and develops a software application on the platform of iRMX OS. In this paper a new approach to the diagnosis problem is presented, including an adaptive PMC model, distributed diagnosis including self-diagnosis and interactive-diagnosis, and system graph-theoretic model. The self-diagnosis fully utilises the individual results of built-in self-tests as a part of diagnosis work. Interactive-diagnosis means that in the system fault-free units perform simple periodic tests on one another under the direction of the graph-theoretic model by interactively communicating, and every unit can only send the diagnosis information to its considered fault-free units. Finally, we illustrate the procedure of diagnosis verification. The results obtained show that the adaptive PMC model is applicable, the distributed system-level diagnosis algorithm is proper, and the applications of diagnosis and verification are reliable and practicable.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021494,no,undetermined,0
Adaptive parameter tuning for relevance feedback of information retrieval,"Relevance feedback is an effective way to improve the performance of an information retrieval system. In practice, the parameters for feedback were usually determined manually without the consideration of the quality of the query. We propose a new concept (adaptiveness) to measure the quality of the query. We built two models to predict the adaptiveness of the query. The parameters for feedback were then determined by the quality of the query. Our experiments on TREC data showed that the performance was improved significantly when compared with blind relevance feedback.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021462,no,undetermined,0
Domain Knowledge Consistency Checking for Ontology-Based Requirement Engineering,"Domain knowledge is one of crucial factors to get a great success in requirements elicitation of high quality. In ontology-based requirements engineering, ontology is used to express domain knowledge, so that the inconsistency of domain knowledge can be found by semantic checking. This paper purposes a new algorithm based on Tableaux algorithm to detecting and resolving inconsistencies of ontology. All kinds of consistency rules of domain knowledge are formally defined at first, and then the semantic checking algorithm is presented to resolve these inconsistencies. Finally, a case study is given to show the process and validate the usability of the algorithm.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722058,no,undetermined,0
Some Metrics for Accessing Quality of Product Line Architecture,"Product line architecture is the most important core asset of software product line. vADL, a product line architecture description languages, can be used for specifying product line architecture, and also provide enough information for measuring quality of product line architecture. In this paper, some new metrics are provided to assess similarity, variability, reusability, and complexity of product line architecture. The main feature of our approach is to assess the quality of product line architecture by analyzing its formal vADL specification, and therefore the process of metric computation can be automated completely.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722101,no,undetermined,0
Predicting TCP throughput from non-invasive network sampling,"In this paper, we wish to derive analytic models that predict the performance of TCP flows between specified endpoints using routinely observed network characteristics such as loss and delay. The ultimate goal of our approach is to convert network observables into representative user and application relevant performance metrics. The main contributions of this paper are in studying which network performance data sources are most reflective of session characteristics, and then in thoroughly investigating a new TCP model based on Padhye et al. (2000) that uses non-invasive network samples to predict the throughput of representative TCP flows between given end-points.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1019259,no,undetermined,0
Error rate estimation for a flight application using the CEU fault injection approach,This paper aims at validating the efficiency of a fault injection approach to predict error rate on applications devoted to operate in radiation environment. Soft error injection experiments and radiation ground testing were performed on software modules using a digital board built on a digital signal processor which is included in a satellite instrument. The analysis of experimental results put in evidence the potentialities offered by the used methodology to predict the error rate of complex applications.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1030218,no,undetermined,0
Bond and electron beam welding quality control of the aluminum stabilized and reinforced CMS conductor by means of ultrasonic phased-array technology,"The Compact Muon Solenoid (CMS) is one of the general-purpose detectors to be provided for the LHC project at CERN. The design field of the CMS superconducting magnet is 4 T, the magnetic length is 12.5 m and the free bore is 6 m. The coils for CNIS are wound of aluminum-stabilized Rutherford type superconductors reinforced with high-strength aluminum alloy. For optimum performance of the conductor a void-free metallic bonding between the high-purity aluminum and the Rutherford type cable as well as between the electron beam welded reinforcement and the high-purity aluminum must be guaranteed. It is the main task of this development work to assess continuously the bond quality over the whole width and the total length of the conductors during manufacture. To achieve this goal we use the ultrasonic phased-array technology. The application of multi-element transducers allows an electronic scanning perpendicular to the direction of production. Such a testing is sufficiently fast in order to allow a continuous analysis of the complete bond. A highly sophisticated software allows the on-line monitoring of the bond and weld quality.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1018616,no,undetermined,0
Heaps and stacks in distributed shared memory,"Software-based distributed shared memory (DSM) systems do usually not provide any means to use shared memory regions as stacks or via an efficient heap memory allocator. Instead DSM users are forced to work with very rudimentary and coarse grain memory (de-)allocation primitives. As a consequence most DSM applications have to ??reinvent the wheel??, that is to implement simple stack or heap semantics within the shared regions. Obviously, this has several disadvantages. It is error-prone, timeconsuming and inefficient. This paper presents an all in software DSM that does not suffer from these drawbacks. Stack and heap organization is adapted to the changed requirements in DSM environments and both, stacks and heaps, are transparently placed in DSM space by the operating system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1016494,no,undetermined,0
Model-based configuration of VPNs,"The design of suitable configurations for virtual private networks (VPNs) is usually difficult and error-prone. The abstract objectives of design are given by high level policies representing various requirements and the designers are often faced with conflicting requirements. Moreover, it is difficult to find a suitable mapping of high level policies to those low level network configurations which correctly and completely implement the abstract objectives. We apply the approach of model-based management where the system itself as well as the management objectives are represented by graphical object instance diagrams. A combination of tool and libraries supports their interactive construction and automated analysis. The implementation of the approach focuses on VPNs which are based on the Linux IPsec software FreeS/WAN.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1015610,no,undetermined,0
Real-time MPEG video encoder with embedded scene change detection and telecine inverse,"This paper describes very cost-effective algorithms to detect scene changes and field repetition in video sequences for a real-time MPEG encoder. It also provides a scheme to support a dynamic GOP structure reflecting the detection outcome on the fly. With these features, the encoder can encode video more efficiently in either quality or bitrate aspect. The proposed detection methods only utilize the existing information, field motion vectors and picture coding type, from MPEG coding and are very suitable for (but not limited to) software-based encoders.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1014014,no,undetermined,0
Asymptotic efficiency of two-stage disjunctive testing,"We adapt methods originally developed in information and coding theory to solve some testing problems. The efficiency of two-stage pool testing of n items is characterized by the minimum expected number E(n, p) of tests for the Bernoulli p-scheme, where the minimum is taken over a matrix that specifies the tests that constitute the first stage. An information-theoretic bound implies that the natural desire to achieve E(n, p) = o(n) as n   can be satisfied only if p(n)  0. Using random selection and linear programming, we bound some parameters of binary matrices, thereby determining up to positive constants how the asymptotic behavior of E(n, p) as n   depends on the manner in which p(n)  0. In particular, it is shown that for p(n) = n<sup>-+o(1)</sup>, where 0 <  < 1, the asymptotic efficiency of two-stage procedures cannot be improved upon by generalizing to the class of all multistage adaptive testing algorithms",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1013122,no,undetermined,0
The SRGM Framework of Integrated Fault Detection Process and Correction Process,"In this paper, the hypothesis that the detected faults will be immediately removed is revised. An integration of fault detection process and correction process is considered. According to the statistic of the faults, the two new frameworks, which are the SRGM framework including repeated faults (CRDW) and the SRGM framework excluding repeated faults (CNRDW), are presented. The above two frameworks, not only can predict the number of cumulative detected faults, but also can predict the number of corrected faults. In this paper, as an example, two reliability models are gained from CNRDW with different detection process and different correction process. The fitting capability and prediction capability of the two models are evaluated by an open software failure data set. The experimental results show that the presented models have a fairly accurate fitting capability and prediction capability compared with other software reliability growth models.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722142,no,undetermined,0
Formal approaches to software testing,"The process of testing software is an important technique for checking and validating the correctness of software. Unfortunately, it is usually difficult, expensive, time consuming and often error prone to achieve both an effective and efficient testing process. Formal methods are a method of specifying and verifying software systems using mathematical and logic approaches. This allows the analysis and reasoning of software systems with precision and rigor. Formal methods target the verification and the proving of correctness, while testing can only show the presence of errors. The use of formal methods can also automate the generation of test cases from formal specifications which can lead to less expensive and less error prone testing process.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1013026,no,undetermined,0
Assessing the quality of Web-based applications via navigational structures,"We study the link validity of a Web site's navigational structure to enhance Web quality. Our approach employs the principle of statistical usage testing to develop an efficient and effective testing mechanism. Some advantages of our approach include generating test scripts systematically, providing coverage metrics, and executing hyperlinks only once.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1022855,no,undetermined,0
Calibration and estimation of redundant signals,This paper presents an adaptive filter for real-time calibration of redundant signals consisting of sensor data and/or analytically derived measurements. The measurement noise covariance matrix is adjusted as a function of the a posteriori probabilities of failure of the individual signals. An estimate of the measured variable is obtained as a weighted average of the calibrated signals. The weighting matrix is recursively updated in real time instead of being fixed a priori. The filter software is presently hosted in a Pentium platform and is portable to other commercial platforms. The filter can be used to enhance the Instrumentation and Control System Software in large-scale dynamical systems.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1023223,no,undetermined,0
Structural geological study of Southern Apennine (Italy) using Landsat 7 imagery,"A structural geological study has been carried out by automatic and visual interpretation of Landsat 7 imagery. The new improved features of ETM+ imagery are tested in the southern part of the Apennines mountain chain, which is characterized by several inverse faults and overthrusting. Spatial information is crucial for structure detection; nevertheless spectral data can also help in the geological interpretation of optical images. In order to combine the spatial and spectral information, panchromatic and multispectral images were fused in synergetic imagery. A lineament analysis was accomplished by visual interpretation and additional processing techniques such as edge detection and morphologic filtering. The combination of different analytical techniques enables the production of a lineament map of the study area. A spatial statistic analysis of the lineaments was performed to analyze their frequency and main direction. The structural geological interpretation of remotely sensed data was compared to the field data collected over some sample areas and structural geological studies carried out by different authors. The features of geological interest detected during the interpretation process were digitized using a raster-based GIS software. A preliminary vector structural geological map was produced.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1024990,no,undetermined,0
Stability analysis for reconfigurable systems with actuator saturation,"Discusses a combined analytic and simulation-based approach to assessing the stability of a control law in a system that may be subject to actuator saturation due to failures and subsequent reconfiguration. The analysis is based on linearized plant dynamics, a linearized state-feedback description of the nonlinear controller dynamics, and a nonlinear actuator model. For systems of this type, a method has previously been developed that provides less conservative estimates of the domain of attraction than other available methods. The domain of attraction estimates are used to guide simulation based stability analysis. The combined analytic and simulation based stability assessment approach is implemented in RASCLE, a software package designed to interface with an arbitrary C, C++, or FORTRAN simulation. Through the combination of analytic stability estimates and automated simulation-based analysis, RASCLE can efficiently provide information about the stability of the full nonlinear system under a wide range of conditions for the purpose of validating a reconfigurable controller.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1025415,no,undetermined,0
Analysis of SEU effects in a pipelined processor,"Modern processors embed features such as pipelined execution units and cache memories that can hardly be controlled by programmers through the processor instruction set. As a result, software-based fault injection approaches are no longer suitable for assessing the effects of SEUs in modern processors, since they are not able to evaluate the effects of SEUs affecting pipelines and caches. In this paper we report an analysis of a commercial processor core where the effects of SEUs located in the processor pipeline and cache memories are studied. Moreover the obtained results are compared with those software-based approaches provide. Experimental results show that software-based approaches may lead to errors during the failure rate estimation of up to 400%.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1030193,no,undetermined,0
Multi-level fault injection experiments based on VHDL descriptions: a case study,"The probability of transient faults increases with the evolution of technologies. There is a corresponding increased demand for an early analysis of erroneous behaviors. This paper reports on results obtained with SEU-like fault injections in VHDL descriptions of digital circuits. Several circuit description levels are considered, as well as several fault modeling levels. These results show that an analysis performed at a very early stage in the design process can actually give a helpful insight into the response of a circuit when a fault occurs.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1030192,no,undetermined,0
Random testing of multi-port static random access memories,"This paper presents the analysis and modeling of random testing for its application to multi-port memories. Ports operate to simultaneously test the memory and detecting multi-port related faults. The state of the memory under test in the presence of inter-port faults has been modeled using Markov state diagrams. In the state diagrams, transition probabilities are established by considering the effects of the memory operations (read and write), the lines involved in the fault (bit and word-lines) as well as the types and number of ports. Test lengths per cell at 99.9% coverage are given.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1029770,no,undetermined,0
Automated algorithm to delineate Z-bands in electron microscopic images of the human skeletal muscle,"The effects of exercise, nutrition, and aging on the development of human skeletal muscles can be observed from the morphological changes of the Z-band under the electron microscope. Quantification of the Z-band damage has provided useful information to exercise physiology research but is usually a labor-intensive process. In this study, an automated image-processing algorithm has been developed to delineate the Z-band with a given start point. The algorithm detects the borders of the Z-band in an incremental fashion along the long axis. At each step of the iteration local border points are detected along radial directions and the centerline is extended toward both ends of the Z-band. The process iterates itself until a stopping criterion is met. The algorithm has been coded in C++ and used in our laboratory for exercise science research. The software has significantly reduced the processing time and provided reliable high-quality data for the study of Z-band damage.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1029401,no,undetermined,0
A compositional approach to monitoring distributed systems,"This paper proposes a specification-based monitoring approach for automatic run-time detection of software errors and failures of distributed systems. The specification is assumed to be expressed in communicating finite state machines based formalism. The monitor observes the external I/O and partial state information of the target distributed system and uses them to interpret the specification. The approach is compositional as it achieves global monitoring by combining the component-level monitoring. The core of the paper describes the architecture and operations of the monitor The monitor includes several independent mechanisms, each tailored to detecting specific kinds of errors or failures. Their operations are described in detail using illustrative examples. Techniques for dealing with nondeterminism and concurrency issues in monitoring a distributed system are also discussed with respect to the considered model and specification. A case study describing the application of the prototype monitor to an embedded system is presented.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1029022,no,undetermined,0
Evaluation of Requirements Analysis Progress Based on Chaos,"Reliable results of requirements engineering are important to the success of a software project. Based on the research of chaotic behavior of requirements analysis, we set up a theoretical model which can be used to lead us to analysis requirements and evaluate the quality of decomposing process. The model shows that analyzing trajectory may consists of three segments i.e. initial segment, middle segment and last segment so long as Requirements Decomposition Rate Parameter is in its stable region. The decomposing process can be taken as normal if all segments or last two segments exist in the trajectory. We may be able to predict the time we need to finish all requirements decomposition in advance. We apply the method in the requirements analysis of home phone service management system, and the initial results show that the method is useful in the evaluation of requirements decomposition.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722048,no,undetermined,0
Modeling and quantification of security attributes of software systems,"Quite often failures in network based services and server systems may not be accidental, but rather caused by deliberate security intrusions. We would like such systems to either completely preclude the possibility of a security intrusion or design them to be robust enough to continue functioning despite security attacks. Not only is it important to prevent or tolerate security intrusions, it is equally important to treat security as a QoS attribute at par with, if not more important than other QoS attributes such as availability and performability. This paper deals with various issues related to quantifying the security attribute of an intrusion tolerant system, such as the SITAR system. A security intrusion and the response of an intrusion tolerant system to the attack is modeled as a random process. This facilitates the use of stochastic modeling techniques to capture the attacker behavior as well as the system's response to a security intrusion. This model is used to analyze and quantify the security attributes of the system. The security quantification analysis is first carried out for steady-state behavior leading to measures like steady-state availability. By transforming this model to a model with absorbing states, we compute a security measure called the """"mean time (or effort) to security failure"""" and also compute probabilities of security failure due to violations of different security attributes.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028941,no,undetermined,0
Detecting processor hardware faults by means of automatically generated virtual duplex systems,"A virtual duplex system (VDS) can be used to increase safety without the use of structural redundancy on a single machine. If a deterministic program P is calculating a given function f, then a VDS contains two variants P<sub>a</sub> and P<sub>b</sub> of P which are calculating the diverse functions f<sub>a</sub> and f<sub>b</sub> in sequence. If no error occurs in the process of designing and executing P<sub>a</sub> and P<sub>b</sub>, then f= f<sub>a</sub>=f<sub>b</sub> holds. A fault in the underlying processor hardware is likely to be detected by the deviation of the results, i.e. f<sub>a</sub>(i)=f<sub>b</sub>(i) for input i. Normally, VDSs are generated by manually applying different diversity techniques. This paper, in contrast, presents a new method and a tool for the automated generation of VDSs with a high detection probability for hardware faults. Moreover, for the first time the diversity techniques are selected by an optimization algorithm rather than chosen intuitively. The generated VDSs are investigated extensively by means of software implemented processor fault injection.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028925,no,undetermined,0
A versatile and modular consensus protocol,"Investigates a modular and versatile approach to solve the consensus problem in asynchronous distributed systems in which up to f processes may crash (f<n/2), but equipped with appropriate oracles. It presents a generic protocol that proceeds by consecutive asynchronous rounds. Each round follows a """"two-phase"""" pattern. The modularity and the versatility of the protocol appear at each phase of a round. The first phase is a selection phase that allows to use any combination merging random oracle, leader oracle and condition. Its aim is to ensure termination by allowing the processes to start the second phase with the same value. The aim of the second phase is to ensure that the agreement property cannot be violated. Its cost depends on the value of f: two communication steps when f<n/2, that reduce to a single communication step when f<n/3. Hence, the behavior of the first phase is mainly ruled by the system additional equipment, while the behavior of the second phase depends on the value of f. It follows that the proposed protocol can be instantiated in different ways according to the oracles the system is equipped with and the actual value of f.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028921,no,undetermined,0
Reliability and availability analysis for the JPL Remote Exploration and Experimentation System,"The NASA Remote Exploration and Experimentation (REE) Project, managed by the Jet Propulsion Laboratory, has the vision of bringing commercial supercomputing technology into space, in a form which meets the demanding environmental requirements, to enable a new class of science investigation and discovery. Dependability goals of the REE system are 99% reliability over 5 years and 99% availability. In this paper we focus on the reliability/availability modeling and analysis of the REE system. We carry out this task using fault trees, reliability block diagrams, stochastic reward nets and hierarchical models. Our analysis helps to determine the ranges of parameters for which the REE dependability goal will be met. The analysis also allows us to assess different hardware and software fault-tolerance techniques.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028918,no,undetermined,0
Experimental evaluation of time-redundant execution for a brake-by-wire application,"This paper presents an experimental evaluation of a brake-by-wire application that tolerates transient faults by temporal error masking. A specially designed real-time kernel that masks errors by triple time-redundant execution and voting executes the application on a fail-stop computer node. The objective is to reduce the number of node failures by masking errors at the computer node level. The real-time kernel always executes the application twice to detect errors, and ensures that a fail-stop failure occurs if there is not enough CPU-time available for a third execution and voting. Fault injection experiments show that temporal error masking reduced the number of fail-stop failures by 42% compared to executing the brake-by-wire task without time redundancy.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028902,no,undetermined,0
Process modelling to support dependability arguments,"Reports work to support dependability arguments about the future reliability of a product before there is direct empirical evidence. We develop a method for estimating the number of residual faults at the time of release from a """"barrier model"""" of the development process, where in each phase faults are created or detected. These estimates can be used in a conservative theory in which a reliability bound can be obtained or can be used to support arguments of fault freeness. We present the work done to demonstrate that the model can be applied in practice. A company that develops safety-critical systems provided access to two projects as well as data over a wide range of past projects. The software development process as enacted was determined and we developed a number of probabilistic process models calibrated with generic data from the literature and from the company projects. The predictive power of the various models was compared.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028892,no,undetermined,0
A software-reliability growth model for N-version programming systems,"This paper presents a NHPP-based SRGM (software reliability growth model) for NVP (N-version programming) systems (NVP-SRGM) based on the NHPP (nonhomogeneous Poisson process). Although many papers have been devoted to modeling NVP-system reliability, most of them consider only the stable reliability, i.e., they do not consider the reliability growth in NVP systems due to continuous removal of faults from software versions. The model in this paper is the first reliability-growth model for NVP systems which considers the error-introduction rate and the error-removal efficiency. During testing and debugging, when a software fault is found, a debugging effort is devoted to remove this fault. Due to the high complexity of the software, this fault might not be successfully removed, and new faults might be introduced into the software. By applying a generalized NHPP model into the NVP system, a new NVP-SRGM is established, in which the multi-version coincident failures are well modeled. A simplified software control logic for a water-reservoir control system illustrates how to apply this new software reliability model. The s-confidence bounds are provided for system-reliability estimation. This software reliability model can be used to evaluate the reliability and to predict the performance of NVP systems. More application is needed to validate fully the proposed NVP-SRGM for quantifying the reliability of fault-tolerant software systems in a general industrial setting. As the first model of its kind in NVP reliability-growth modeling, the proposed NVP SRGM can be used to overcome the shortcomings of the independent reliability model. It predicts the system reliability more accurately than the independent model and can be used to help determine when to stop testing, which is a key question in the testing and debugging phase of the NVP system-development life cycle",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1028403,no,undetermined,0
Automatic detection and exploitation of branch constraints for timing analysis,"Predicting the worst-case execution time (WCET) and best-case execution time (BCET) of a real-time program is a challenging task. Though much progress has been made in obtaining tighter timing predictions by using techniques that model the architectural features of a machine, significant overestimations of WCET and underestimations of GCET can still occur. Even with perfect architectural modeling, dependencies on data values can constrain the outcome of conditional branches and the corresponding set of paths that can be taken in a program. While branch constraint information has been used in the past by some timing analyzers, it has typically been specified manually, which is both tedious and error prone. This paper describes efficient techniques for automatically detecting branch constraints by a compiler and automatically exploiting these constraints within a timing analyzer. The result is significantly tighter timing analysis predictions without requiring additional interaction with a user.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1027799,no,undetermined,0
Numerical methods for beautification of reverse engineered geometric models,"Boundary representation models reconstructed from 3D range data suffer from various inaccuracies caused by noise in the data and the model building software. The quality of such models can be improved in a beautification step, which finds geometric regularities approximately present in the model and tries to impose a consistent subset of these regularities on the model. A framework for beautification and numerical methods to select and solve a consistent set of constraints deduced from a set of regularities are presented. For the initial selection of consistent regularities likely to be part of the model's ideal design priorities, and rules indicating simple inconsistencies between the regularities are employed. By adding regularities consecutively to an equation system and trying to solve it by using quasi-Newton optimization methods, inconsistencies and redundancies are detected. The results of experiments are encouraging and show potential for an expansion of the methods based on degree of freedom analysis.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1027507,no,undetermined,0
Assessing CBD - what's the difference?,"The use of pre-built software components has increased remarkably in the last few years and a lot of companies are now developing software with a component-based approach. However, software developers as well as software process assessors have experienced problems when using existing models in a component-based context. This paper analyses the need for and proposes a set of processes suitable for CBD development and assessment methods and how and and in what way these processes differ from those defined in the ISO/IEC 15504, the international standard for software process assessment. The paper focuses on the processes needed to assemble components in to a larger application and explains the need for a changed process reference model for CBD, which triggers the need for a new software process assessment methodology.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046200,no,undetermined,0
SPiCE in action - experiences in tailoring and extension,"Today the standard ISO/IEC TR 15504: software process assessment commonly known as SPiCE has been in use for more than 5 years, with hundreds of software process assessments performed in organizations around the world. The success of the ISO 15504 approach is demonstrated by its application in and extension to all sectors featuring software development, in particular, space, automotive, finance, healthcare, and electronics. As the current Technical Report makes the transition into an international standard, many initiatives are underway to expand the application of process assessment to areas even outside of software development. This paper reports on experiences in the use of ISO 15504 both in tailoring the standard for particular industrial sectors and in expanding the process assessment approach into new domains. In particular, three projects are discussed: SPiCE for SPACE, a ISO/IEC TR 15504 conformant method of software process assessment developed for the European space industry; SPiCE-9000 for SPACE, an assessment method for space quality management systems, based on ISO 9001:2000; and NOVE-IT, a project of the Swiss federal government to establish and assess processes covering IT procurement, development, operation, and service provision.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046215,no,undetermined,0
What has culture to do with SPI?,"This paper addresses cross-cultural issues in software process improvement (SPI). Cultural factors, which may have a bearing on successful adoption and implementation of software quality management systems, were identified during a field-study in five countries. A self-assessment model, called CODES, has been developed for use by organisations developing software in different parts of the world. The CODES model includes two sub-models. One of the sub-models, called the C.HI.D.DI typology tries to identify the national culture and the second sub-model called the top-down bottom-up model tries to identify the organisational culture and structure. The CODES model investigates to what degree there is a fit between the organisational and the national culture and aims to predict a suitable software quality management system.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046229,no,undetermined,0
Change-oriented requirements traceability. Support for evolution of embedded systems,"Planning of requirements changes is often inaccurate and implementation of changes is time consuming and error prone. One reason for these problems is imprecise and inefficient approaches to analyze the impact of changes. This thesis proposes a precise and efficient impact analysis approach that focuses on functional system requirements changes of embedded control systems. It consists of three parts: (1) a fine-grained conceptual trace model, (2) process descriptions of how to establish traces and how to analyze the impact of changes, and (3) supporting tools. Empirical investigation shows that the approach has a beneficial effect on the effectiveness and efficiency of impact analyses and that it supports a more consistent implementation of changes.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167808,no,undetermined,0
"Automatic failure detection, logging, and recovery for high-availability Java servers","Many systems and techniques exist for detecting application failures. However, previously known generic failure detection solutions are only of limited use for Java applications because they do not take into consideration the specifics of the Java language and the Java execution environment. In this article, we present the application-independent Java Application Supervisor (JAS). JAS can automatically detect, log, and resolve a variety of execution problems and failures in Java applications. In most cases, JAS requires neither modifications nor access to the source code of the supervised application. A set of simple user-specified policies guides the failure detection, logging, and recovery process in JAS. A JAS configuration manager automatically generates default policies from the bytecode of an application. The user can modify these default policies as needed. Our experimental studies show that JAS typically incurs little execution time and memory overhead for the target application. We describe an experiment with a Web proxy that exhibits reliability and performance problems under heavy load and demonstrate an increase in the rate of successful requests to the server by almost 33% and a decrease in the average request processing time by approximately 22% when using JAS.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173217,no,undetermined,0
An overview of industrial software documentation practice,"A system documentation process maturity model and assessment procedure were developed and used to assess 91 projects at 41 different companies over a seven year period. During this time the original version evolved into a total of four versions based on feedback from industry and the experience gained from the assessments. This paper reports the overall results obtained from the assessments which strongly suggest that the practice of documentation is not getting a passing grade in the software industry. The results show a clear maturity gap between documentation practices concerned with defining policy and practices concerned with adherence to those policies. The results further illustrate the need to recognize the importance of improving the documentation process, and to transform the good intentions into explicit policies and actions.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173192,no,undetermined,0
Analytical Modeling Approach to Detect Magnet Defects in Permanent-Magnet Brushless Motors,The paper presents a novel approach to detect magnet faults such as local demagnetization in brushless permanent-magnet motors. We have developed a new form of analytical model that solves the Laplacian/quasi-Poissonian field equations in the machine's air-gap and magnet element regions. We verified the model by using finite-element software in which demagnetization faults were simulated and electromotive force was calculated as a function of rotor position. We then introduced the numerical data of electromotive force into a gradient-based algorithm that uses the analytical model to locate demagnetized regions in the magnet as simulated in the finite-element package. The fast and accurate convergence of the algorithm makes the model useful in magnet fault diagnostics.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4711289,no,undetermined,0
Open source software research activities in AIST towards secure open systems,"National Research Institutes of Advanced Industrial Science and Technology (AIST) is governed by the Ministry of Economy Trade and Industry of Japanese government. The Information Technology Research Institute of AIST has noticed that the open source software approaches are important issues to have high quality and secure software. In this paper, after we have shown four projects of open source software carried out at AIST, we show a typical and simple security problem named """"cross site scripting"""" of Web servers. If the application software for the Web server were opened, this security hole would be quickly fixed because the problem is very simple and the way to fix is quite easy. Then we show several reports on Linux operating system of using governmental computer network infrastructures. We see that a lot of countries are considering using Linux and its application software as their infrastructures. Because of the national securities and the deployment costs AIST is now planning to use Linux office applications in order to assess the feasibility of using open source software as an important infrastructure.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173098,no,undetermined,0
Verifying provisions for post-transaction user input error correction through static program analysis,Software testing is a time-consuming and error-prone process. Automated software verification is an important key to improve software testing. This paper presents a novel approach for the automated approximate verification of provisions of transactions for correcting effects that result from executing database transactions with wrong user inputs. The provision is essential in any database application. The approach verifies the provision through analyzing the source codes of transactions in a database application. It is based on some patterns that in all likelihood exist between the control flow graph of a transaction and the control flow graphs of transactions for correcting some post-transaction user input errors of the former transaction. We have validated the patterns statistically.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173081,no,undetermined,0
Java quality assurance by detecting code smells,"Software inspection is a known technique for improving software quality. It involves carefully examining the code, the design, and the documentation of software and checking these for aspects that are known to be potentially problematic based on past experience. Code smells are a metaphor to describe patterns that are generally associated with bad design and bad programming practices. Originally, code smells are used to find the places in software that could benefit from refactoring. In this paper we investigate how the quality of code can be automatically assessed by checking for the presence of code smells and how this approach can contribute to automatic code inspection. We present an approach for the automatic detection and visualization of code smells and discuss how this approach can be used in the design of a software inspection tool. We illustrate the feasibility of our approach with the development of jCOSMO, a prototype code smell browser that detects and visualizes code smells in JAVA source code. Finally, we show how this tool was applied in a case study.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173068,no,undetermined,0
Relating expectations to automatically recovered design patterns,"At MITRE we are developing tools to aid analysts in assessing the operational usability and quality of object-oriented code. Our tools statically examine source code, automatically recognize the use of design patterns and relate pattern use to software qualities, coding goals, and system engineering expectations about the source code. Thus, through the use of automated design pattern analysis, we are working to reveal originating software design decisions.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173067,no,undetermined,0
Vehicle Design Validation via Remote Vehicle Diagnosis: A feasibility study on battery management system,"In recent years, passenger vehicle product development faces great challenges to maintain high vehicle quality due to the proliferation of Electronics, Control and Software (ECS) features and the resultant system complexity. Quickly detecting and trouble-shooting faults of integrated vehicle systems during the validation stage in a key to enhancing vehicle quality. In this paper, we present a feasibilty study of Vehicle Design Validation via Remote Vehicle Diagnosis (VDV-via-RVD) and its application in the validation of vehicle battery management system. After the discussion of the advantages and challenges of VDV-via-RVD, some preliminary experimental results are presented to demonstrate the concept.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4711431,no,undetermined,0
Teletraffic simulation of cellular networks: modeling the handoff arrivals and the handoff delay,The paper presents an analysis of teletraffic variables in cellular networks. The variables studied are the time between two consecutive handoff arrivals and the handoff delay. These teletraffic variables are characterized by means of an advanced software simulator that models several scenarios assuming fixed channel allocation. Information about the quality of service is also provided. A large set of scenarios has been simulated and the characterization results derived from its study have been presented and analyzed.,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1046536,no,undetermined,0
Testing Web applications,"The rapid diffusion of Internet and open standard technologies is producing a significant growth of the demand of Web sites and Web applications with more and more strict requirements of usability, reliability, interoperability and security. While several methodological and technological proposals for developing Web applications are coining both from industry and academia, there is a general lack of methods and tools to carry out the key processes that significantly impact the quality of a Web application (WA), such as the validation & verification (V&V), and quality assurance. Some open issues in the field of Web application testing are addressed in this paper. The paper exploits an object-oriented model of a WA as a test model, and proposes a definition of the unit level for testing the WA. Based on this model, a method to test the single units of a WA and for the integration testing is proposed. Moreover, in order to experiment with the proposed technique and strategy, an integrated platform of tools comprising a Web application analyzer, a repository, a test case generator and a test case executor, has been developed and is presented in the paper. A case study, carried out with the aim of assessing the effectiveness of the proposed method and tools, produced interesting and encouraging results.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167787,no,undetermined,0
A connected path approach for staff detection on a music score,"The preservation of many music works produced in the past entails their digitalization and consequent accessibility in an easy-to-manage digital format. Carrying this task manually is very time consuming and error prone. While optical music recognition systems usually perform well on printed scores, the processing of handwritten musical scores by computers remain far from ideal. One of the fundamental stages to carry out this task is the staff line detection. In this paper a new method for the automatic detection of music staff lines based on a connected path approach is presented. Lines affected by curvature, discontinuities, and inclination are robustly detected. Experimental results show that the proposed technique consistently outperforms well-established algorithms.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4711927,no,undetermined,0
Modeling the cost-benefits tradeoffs for regression testing techniques,"Regression testing is an expensive activity that can account for a large proportion of the software maintenance budget. Because engineers add tests into test suites as software evolves, over time, increased test suite size makes revalidation of the software more expensive. Regression test selection, test suite reduction, and test case prioritization techniques can help with this, by reducing the number of regression tests that must be run and by helping testers meet testing objectives more quickly. These techniques, however can be expensive to employ and may not reduce overall regression testing costs. Thus, practitioners and researchers could benefit from cost models that would help them assess the cost-benefits of techniques. Cost models have been proposed for this purpose, but some of these models omit important factors, and others cannot truly evaluate cost-effectiveness. In this paper, we present new cost-benefits models for regression test selection, test suite reduction, and test case prioritization, that capture previously omitted factors, and support cost-benefits analyses where they were not supported before. We present the results of an empirical study assessing these models.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167767,no,undetermined,0
Coverage metrics for Continuous Function Charts,"Continuous Function Charts are a diagrammatical language for the specification of mixed discrete-continuous embedded systems, similar to the languages of Matlab/Simulink, and often used in the domain of transportation systems. Both control and data flows are explicitly specified when atomic units of computation are composed. The obvious way to assess the quality of integration test suites is to compute known coverage metrics for the generated code. This production code does not exhibit those structures that would make it amenable to """"relevant"""" coverage measurements. We define a translation scheme that results in structures relevant for such measurements, apply coverage criteria for both control and dataflows at the level of composition of atomic computational units, and argue for their usefulness on the grounds of detected errors.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383123,no,undetermined,0
Elimination of crucial faults by a new selective testing method,"Recent software systems contain a lot of functions to provide various services. According to this tendency, software testing becomes more difficult than before and cost of testing increases so much, since many test items are required. In this paper we propose and discuss such a new selective software testing method that is constructed from previous testing method by simplifying testing specification. We have presented, in the previous work, a selective testing method to perform highly efficient software testing. The selective testing method has introduced an idea of functional priority testing and generated test items according to their functional priorities. Important functions with high priorities are tested in detail, and functions with low priorities are tested less intensively. As a result, additional cost for generating testing instructions becomes relatively high. In this paper in order to reduce its cost, we change the way of giving information, with respect to priorities. The new method gives the priority only rather than generating testing instructions to each test item, which makes the testing method quite simple and results in cost reduction. Except for this change, the new method is essentially the same as the previous method. We applied this new method to actual development of software tool and evaluated its effectiveness. From the result of the application experiment, we confirmed that many crucial faults can be detected by using the proposed method.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166937,no,undetermined,0
The detection of faulty code violating implicit coding rules,"In the field of legacy software maintenance, there unexpectedly arises a large number of implicit coding rules, which we regard as a cancer in software evolution. Since such rules are usually undocumented and each of them is recognized only by a few members in a maintenance team, a person who is not aware of a rule often violates it while doing various maintenance activities such as adding a new functionality or repairing faults. The problem here is not only such a violation introduces a new fault but also the same kind of fault will be generated again and again in the future by different maintainers. This paper proposes a method for detecting code fragments that violate implicit coding rules. In the method, an expert maintainer, firstly, investigates the cause of each failure, described in the past failure reports, and identifies all the implicit coding rules that lie behind the faults. Then, the code patterns violating the rules (which we call """"faulty code patterns"""") are described in a pattern description language. Finally, the potential faulty code fragments are automatically detected by a pattern matching technique. The result of a case study with large legacy software showed that 32.7% of the failures, which have been reported during a maintenance process, were due to the violation of implicit coding rules. Moreover, 152 faults existed in 772 code fragments detected by the prototype matching system, while 111 of them were not reported.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166936,no,undetermined,0
An experimental comparison of checklist-based reading and perspective-based reading for UML design document inspection,"This paper describes an experimental comparison of two reading techniques, namely Checklist-based reading (CBR) and Perspective-based reading (PBR) for Object-Oriented (OO) design inspection. Software inspection is an effective approach to detect defects in the early stages of the software development process. However inspections are usually applied for defect detection in software requirement documents or software code modules, and there is a significant lack of information how inspections should be applied to OO design documents. The comparison was performed in a controlled experiment with 59 subject students. The results of individual data analysis indicate that (a) defect detection effectiveness using both inspection techniques is similar (PBR: 69%, CBR: 70%); (b) reviewers who use PBR spend less time on inspection than reviewers who use CBR; (c) cost per defect of reviewers who use CBR is smaller. The results of 3-person virtual team analysis show that CBR technique is more effective than PBR technique.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166934,no,undetermined,0
A Markov Decision Approach to Optimize Testing Profile in Software Testing,"In this paper, we demonstrate an approach to optimize software testing, minimize the expected cost with given software parameters of concern. Taking software testing process as a Markov decision process, a Markov decision model of software testing is proposed, and by using a learning strategy based on the Cross-Entropy method to optimize the software testing, we obtain the optimal testing profile. Simulation results show that this learning strategy reduces significantly in expected cost comparing with random testing, moreover, this learning strategy is more feasible and significantly in reducing the number of test cases required to detect and revealing a certain number of software defects than random testing.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4709145,no,undetermined,0
Data coverage testing of programs for container classes,"For the testing of container classes and the algorithms or programs that operate on the data in a container, these data have the property of being homogeneous throughout the container. We have developed an approach for this situation called data coverage testing, where automated test generation can systematically generate increasing test data size. Given a program and a test model, it can be theoretically shown that there exists a sufficiently large test data set size N, such that testing with a data set size larger than N does not detect more faults. A number of experiments have been conducted using a set of C++ STL programs, comparing data coverage testing with two other testing strategies: statement coverage and random generation. These experiments validate the theoretical analysis for data coverage, confirming the predicted sufficiently large N for each program.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173244,no,undetermined,0
Genes and bacteria for automatic test cases optimization in the .NET environment,"The level of confidence in a software component is often linked to the quality of its test cases. This quality can in turn be evaluated with mutation analysis: faulty components (mutants) are systematically generated to check the proportion of mutants detected (""""killed"""") by the test cases. But while the generation of basic test cases set is easy, improving its quality may require prohibitive effort. We focus on the issue of automating the test optimization. We looked at genetic algorithms to solve this problem and modeled it as follows: a test case can be considered as a predator while a mutant program is analogous to a prey. The aim of the selection process is to generate test cases able to kill as many mutants as possible. To overcome disappointing experimentation results on the studied .NET system, we propose a slight variation on this idea, no longer at the """"animal"""" level (lions killing zebras) but at the bacteriological level. The bacteriological level indeed better reflects the test case optimization issue: it introduces a memorization function and suppresses the crossover operator. We describe this model and show how it behaves on the case study.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173246,no,undetermined,0
Fault detection capabilities of coupling-based OO testing,"Object-oriented programs cause a shift in focus from software units to the way software classes and components are connected. Thus, we are finding that we need less emphasis on unit testing and more on integration testing. The compositional relationships of inheritance and aggregation, especially when combined with polymorphism, introduce new kinds of integration faults, which can be covered using testing criteria that take the effects of inheritance and polymorphism into account. This paper demonstrates, via a set of experiments, the relative effectiveness of several coupling-based OO testing criteria and branch coverage. OO criteria are all more effective at detecting faults due to the use of inheritance and polymorphism than branch coverage.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173253,no,undetermined,0
Microarchitectural exploration with Liberty,"To find the best designs, architects must rapidly simulate many design alternatives and have confidence in the results. Unfortunately, the most prevalent simulator construction methodology, hand-writing monolithic simulators in sequential programming languages, yields simulators that are hard to retarget, limiting the number of designs explored, and hard to understand, instilling little confidence in the model. Simulator construction tools have been developed to address these problems, but analysis reveals that they do not address the root cause, the error-prone mapping between the concurrent, structural hardware domain and the sequential, functional software domain. This paper presents an analysis of these problems and their solution, the Liberty Simulation Environment (LSE). LSE automatically constructs a simulator from a machine description that closely resembles the hardware, ensuring fidelity in the model. Furthermore, through a strict but general component communication contract, LSE enables the creation of highly reusable component libraries, easing the task of rapidly exploring ever more exotic designs.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1176256,no,undetermined,0
Generating Test Cases of Object-Oriented Software Based on EDPN and Its Mutant,"In object-oriented software testing, a class is considered to be a basic unit of testing. The state of the objects may cause faults that cannot be easily revealed with traditional testing techniques. In this paper, we propose a new technique for class testing by using event-driven Petri nets (EDPN), which is an extended version of Petri Nets, one of techniques having the ability to analyze and test the behavior for the interaction between data members and member functions in class. We demonstrate how to specify a class specification by EDPNs and a given fault model by mutant of EDPNs, which is a theoretical model to describe the dynamic behaviors of EDPNs. A test case generation technique is presented to detect the given faults by analyzing the differences of test scenario in the dynamic behaviors of both EDPNs. The presented algorithm can select a test case that detects errors described in the fault models.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4709130,no,undetermined,0
Partheno-Genetic Algorithm for Test Instruction Generation,"Test case generation is the classic method in finding software defects, and test instruction generation is one of its typical applications in embedded chipset systems.In this paper, the optimized partheno-genetic algorithm(PGA) is proposed after a 0-1 integer programming model is set up for instruction-set test cases generation problem. Based on simulation, the proposed model and algorithm achieve a convincing computational performance, in most cases 50%~70%, instruction-set test cases with better ability of error detecting obtained using this algorithm could save the execution time up to 3 seconds. Besides, it also avoids the problem of using complicated crossover and mutation operations that traditional genetic algorithm shave.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4709142,no,undetermined,0
Race condition and concurrency safety of multithreaded object-oriented programming in Java,"To ensure the reliability and quality, software systems should be safe. The software safety requires the data consistency in the software. In the multithreaded object-oriented programming, the coherency problem, also called a race condition, may destroy the data consistency. In order to overcome the coherency problem Java sets up the """"synchronized"""" mechanism. However, improper use of the """"synchronized"""" mechanism in Java will result in system deadlock, which also violates the requirement of software safety. We find that it is necessary to supplement a new function to the """"synchronized"""" mechanism of Java. Another contribution in the paper is to propose a new approach for detecting system deadlock in Java multithreaded programs with the synchronized mechanism.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1175574,no,undetermined,0
Immune mechanism based computer security design,"Referring to the mechanism of biological immune system, a novel model of computer security system is proposed, which is a dynamic, multi-layered and co-operational system. Through dynamically supervising abnormal behaviors with multi agent immune systems, a two-level defense system is set up for improving the whole performance: one is based on a host and mainly used for. detecting viruses; and the other is based on a network for supervising potential attacks. On the other hand, a pseudo-random technology is adopted for designing the sub-system of data transmission, in order to increase the ability of protecting information against intended interference and monitoring. Simulations on information transmission show that this system has good robustness, error tolerance and self-adaptiveness, although more practice is needed.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1175366,no,undetermined,0
Numerical simulation of car crash analysis based on distributed computational environment,"Automobile CAE software is mainly used to assess the performance quality of vehicles. As the automobile is a product of technology intensive complexity, its design analysis involves a broad range of CAE simulation techniques. An integrated CAE solution of automobiles can include comfort analysis (vibration and noise analysis), safety analysis (car body collision analysis), process-cycle analysis, structural analysis, fatigue analysis, fluid dynamics analysis, test analysis, material data information system and system integration. We put an emphasis on simulation of a whole automobile collision process, which will bring a breakthrough to the techniques of CAE simulation based on high performance computing. In addition, we carry out simulation for a finite-element car model in a distributed computation environment and accomplish coding-and-programming of DAYN3D. We also provide computational examples and a user handbook. Our research collects almost ten numerical automobile models such as Honda, Ford, etc. Moreover, we also deal with different computational scales for the same auto model and some numerical models of air bags are included. Based on the numerical auto model, referring to different physical parameters and work conditions of the auto model, we can control the physical parameters for the numerical bump simulation and analyze the work condition. The result of our attempt conduces to the development of new auto models.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173597,no,undetermined,0
Composition and decomposition of quality of service parameters in distributed component-based systems,"It is becoming increasingly acceptable that component-based development is an effective, efficient and promising approach to develop distributed systems. With components as the building blocks, it is expected that the quality of the end system can be predicted based on the qualities of components in the system. UniFrame is one such framework that facilitates seamless interoperation of heterogeneous distributed software components. As a part of UniFrame, a catalog of quality of service (QoS) parameters has been created to provide a standard method for quantifying the QoS of software components. In this paper, an approach for composition and decomposition of these QoS parameters is proposed. A case study from the financial domain is indicated to validate this model.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173586,no,undetermined,0
A research on multi-level networked RAID based on cluster architecture,"Storage networks is a popular solution to constraint servers in storage field. As described by Gibson's metrics, the performance of multi-level networked RAID (redundant arrays of inexpensive disks) based on cluster is almost the same to that of improved 2D-parity. Compared with other schemes, it is lower cost and easier to realize.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173578,no,undetermined,0
Injecting bit flip faults by means of a purely software approach: a case studied,"Bit flips provoked by radiation are a main concern for space applications. A fault injection experiment performed using a software simulator is described in this paper. Obtained results allow us to predict a low sensitivity to soft errors for the studied application, putting in evidence critical memory elements.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173507,no,undetermined,0
A portable gait analysis and correction system using a simple event detection method,"Microcontrollers are widely used in the area of portable control systems, though they are only beginning to be used for portable, unobtrusive Functional Electrical Stimulation (FES) systems. This paper describes the initial prototyping of such a portable system. This has the intended use of detecting time variant gait anomalies in patients with hemiplegia, and correcting for them. The system is described in two parts. Firstly, the portable hardware implementing two independent communicating microcontrollers for low powered parallel processing and secondly the simplified low power software. Both are designed specifically for long term, stable use and also to communicate with PC based visual software for testing and evaluation. The system operates by using bend sensors to defect the angles of the hip, knee and ankle of both legs. It computes an error signal with which to produce a stimulation wave cycle, that is synchronised and timed for the new gait cycle from that in which the error was observed. This system uses a PID controller to correct for the instability inherent with such a large time delay between observation and correction.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173333,no,undetermined,0
Effect of disturbances on the convergence of failure intensity,"We report a study to determine the impact of four types of disturbances on the failure intensity of a software product undergoing system test. Hardware failures, discovery of a critical fault, attrition in the test team, are examples of disturbances that will likely affect the convergence of the failure intensity to its desired value. Such disturbances are modeled as impulse, pulse, step, and white noise. Our study examined, in quantitative terms, the impact of such disturbances on the convergence behavior of the failure intensity. Results from this study reveal that the behavior of the state model, proposed elsewhere, is consistent with what one might predict. The model is useful in that it provides a quantitative measure of the delay one can expect when a disturbance occurs.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173296,no,undetermined,0
Inter-class mutation operators for Java,"The effectiveness of mutation testing depends heavily on the types of faults that the mutation operators are designed to represent. Therefore, the quality of the mutation operators is key to mutation testing. Mutation testing has traditionally been applied to procedural-based languages, and mutation operators have been developed to support most of their language features. Object-oriented programming languages contain new language features, most notably inheritance, polymorphism, and dynamic binding. Not surprisingly; these language features allow new kinds of faults, some of which are not modeled by traditional mutation operators. Although mutation operators for OO languages have previously been suggested, our work in OO faults indicate that the previous operators are insufficient to test these OO language features, particularly at the class testing level. This paper introduces a new set of class mutation operators for the OO language Java. These operators are based on specific OO faults and can be used to detect faults involving inheritance, polymorphism, and dynamic binding, thus are useful for inter-class testing. An initial Java mutation tool has recently been completed, and a more powerful version is currently under construction.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173287,no,undetermined,0
Mutation of Java objects,"Fault insertion based techniques have been used for measuring test adequacy and testability of programs. Mutation analysis inserts faults into a program with the goal of creating mutation-adequate test sets that distinguish the mutant from the original program. Software testability is measured by calculating the probability that a program will fail on the next test input coming from a predefined input distribution, given that the software includes a fault. Inserted faults must represent plausible errors. It is relatively easy to apply standard transformations to mutate scalar values such as integers, floats, and character data, because their semantics are well understood. Mutating objects that are instances of user defined types is more difficult. There is no obvious way to modify such objects in a manner consistent with realistic faults, without writing custom mutation methods for each object class. We propose a new object mutation approach along with a set of mutation operators and support tools for inserting faults into objects that instantiate items from common Java libraries heavily used in commercial software as well as user defined classes. Preliminary evaluation of our technique shows that it should be effective for evaluating real-world software testing suites.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173285,no,undetermined,0
Worst case reliability prediction based on a prior estimate of residual defects,"In this paper we extend an earlier worst case bound reliability theory to derive a worst case reliability function R(t), which gives the worst case probability of surviving a further time t given an estimate of residual defects in the software N and a prior test time T. The earlier theory and its extension are presented and the paper also considers the case where there is a low probability of any defect existing in the program. For the """"fractional defect"""" case, there can be a high probability of surviving any subsequent time t. The implications of the theory are discussed and compared with alternative reliability models.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173274,no,undetermined,0
A case study using the round-trip strategy for state-based class testing,"A number of strategies have been proposed for state-based class testing. An important proposal made by Chow (1978), that was subsequently adapted by Binder (1999), consists in deriving test sequences covering all round-trip paths in a finite state machine (FSMs). Based on a number of (rather strong) assumptions, and for traditional FSMs, it can be demonstrated that all operation and transfer errors in the implementation can be uncovered. Through experimentation, this paper investigates this strategy when used in the context of UML statecharts. Based on a set of mutation operators proposed for object-oriented code we seed a significant number of faults in an implementation of a specific container class. We then investigate the effectiveness of four test teams at uncovering faults, based on the round-trip path strategy, and analyze the faults that seem to be difficult to detect. Our main conclusion is that the round-trip path strategy is reasonably effective at detecting faults (87% average as opposed to 69% for size-equivalent, random test cases) but that a significant number of faults can only exhibit a high detection probability by augmenting the round-trip strategy with a traditional black-box strategy such as category-partition testing. This increases the number of test cases to run -and therefore the cost of testing- and a cost-benefit analysis weighting the increase of testing effort and the likely gain in fault detection is necessary.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1173268,no,undetermined,0
How much information is needed for usage-based reading? A series of experiments,"Software inspections are regarded as an important technique to detect faults throughout the software development process. The individual preparation phase of software inspections has enlarged its focus from only comprehension to also include fault searching. Hence, reading techniques to support the reviewers on fault detection are needed. Usage-based reading (UBR) is a reading technique, which focuses on the important parts of a software document by using prioritized use cases. This paper presents a series of three UBR experiments on design specifications, with focus on the third. The first experiment evaluates the prioritization of UBR and the second compares UBR against checklist-based reading. The third experiment investigates the amount of information needed in the use cases and whether a more active approach helps the reviewers to detect more faults. The third study was conducted at two different places with a total of 82 subjects. The general result from the experiments is that UBR works as intended and is efficient as well as effective in guiding reviewers during the preparation phase of software inspections. Furthermore, the results indicate that use cases developed in advance are preferable compared to developing them as part of the preparation phase of the inspection.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166932,no,undetermined,0
Comparative study of cognitive complexity measures,"Complexity metrics are used to predict critical information about reliability and maintainability of software systems. Cognitive complexity measure based on cognitive informatics, plays an important role in understanding the fundamental characteristics of software, therefore directly affects the understandability and maintainability of software systems. In this paper, we compared available cognitive complexity measures and evaluated cognitive weight complexity measure in terms of Weyukerpsilas properties.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4717939,no,undetermined,0
An approach to experimental evaluation of software understandability,"Software understandability is an important characteristic of software quality because it can influence cost or reliability of software evolution in reuse or maintenance. However, it is difficult to evaluate software understandability in practice because understanding is an internal process of humans. This paper proposes """"software overhaul"""" as a method for externalizing the process of understanding and presents a probability model to use process data of overhauling to estimate software understandability. An example describes an overhaul tool and its application.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166925,no,undetermined,0
An approach to rapid prototyping of large multi-agent systems,"Engineering individual components of a multi-agent system and their interactions is a complex and error-prone task in urgent need of methods and tools. Prototyping is a valuable technique to help software engineers explore the design space while gaining insight and a """"feel"""" for the dynamics of the system; prototyping also allows engineers to learn more about the relationships among design features and the desired computational behaviour. In this paper we describe an approach to building prototypes of large multi-agent systems with which we can experiment and analyse results. We have implemented an environment embodying our approach. This environment is supported by a distributed platform that helps us achieve controlled simulations.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1114987,no,undetermined,0
Integrating reliability and timing analysis of CAN-based systems,"This paper presents and illustrates a reliability analysis method developed with a focus on controller-area-network-based automotive systems. The method considers the effect of faults on schedulability analysis and its impact on the reliability estimation of the system, and attempts to integrate both to aid system developers. The authors illustrate the method by modeling a simple distributed antilock braking system, and showing that even in cases where the worst case analysis deems the system unschedulable, it may be proven to satisfy its timing requirements with a sufficiently high probability. From a reliability and cost perspective, this paper underlines the tradeoffs between timing guarantees, the level of hardware and software faults, and per-unit cost.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1097745,no,undetermined,0
Towards High-Level Parallel Programming Models for Multicore Systems,"Parallel programming represents the next turning point in how software engineers write software. Multicore processors can be found today in the heart of supercomputers, desktop computers and laptops. Consequently, applications will increasingly need to be parallelized to fully exploit multicore processors throughput gains now becoming available. Unfortunately, writing parallel code is more complex than writing serial code. This is where the threading building blocks (TBB) approach enters the parallel computing picture. TBB helps developers create multithreaded applications more easily by using high-level abstractions to hide much of the complexity of parallel programming, We study the programmability and performance of TBB by evaluating several practical applications. The results show very promising performance but parallel programming with TBB is still tedious and error-prone.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721348,no,undetermined,0
Scenario-based specification and evaluation of architectures for health monitoring of aerospace structures,"HUMS (Health and Usage Monitoring Systems) have been an area of increased research in the recent times due to two main reasons: (a) increase in the occurrences of accidents in the aerospace, and (b) stricter FAA regulations on aircraft maintenance There are several problems associated with the maintenance of aircraft that the HUMS systems can solve through the use of several monitoring technologies. Currently, a variety of maintenance programs are institutionalized by the aircraft carriers that mostly involve visual inspections and hence are error-prone Automatic, continuous health monitoring systems could simplify the maintenance tasks as well as improve the efficiency of the operation, thereby enhancing the safety of air travel and also lowering the total lifecycle costs of aircraft. This paper documents our methodology of employing scenarios in the specification and evaluation of architecture for HUMS. It investigates related works that use scenarios in software development and describes how we use scenarios in our work. Finally, a demonstration of our methods in the development of HUMS is presented.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1052997,no,undetermined,0
Timed Wp-method: testing real-time systems,"Real-time systems interact with their environment using time constrained input/output signals. Examples of real-time systems include patient monitoring systems, air traffic control systems, and telecommunication systems. For such systems, a functional misbehavior or a deviation from the specified time constraints may have catastrophic consequences. Therefore, ensuring the correctness of real-time systems becomes necessary. Two different techniques are usually used to cope with the correctness of a software system prior to its deployment, namely, verification and testing. In this paper, we address the issue of testing real-time software systems specified as a timed input output automaton (TIOA). TIOA is a variant of timed automaton. We introduce the syntax and semantics of TIOA. We present the potential faults that can be encountered in a timed system implementation. We study these different faults based on TIOA model and look at their effects on the execution of the system using the region graph. We present a method for generating timed test cases. This method is based on a state characterization technique and consists of the following three steps: First, we sample the region graph using a suitable granularity, in order to construct a subautomaton easily testable, called grid automaton. Then, we transform the grid automaton into a nondeterministic timed finite state machine (NTFSM). Finally, we adapt the generalized Wp-method to generate timed test cases from NTFSM. We assess the fault coverage of our test cases generation method and prove its ability to detect all the possible faults. Throughout the paper, we use examples to illustrate the various concepts and techniques used in our approach.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1049402,no,undetermined,0
Disaggregating and calibrating the CASE tool variable in COCOMO II,"CASE (computer aided software engineering) tools are believed to have played a critical role in improving software productivity and quality by assisting tasks in software development processes since the 1970s. Several parametric software cost models adopt """"use of software tools"""" as one of the environmental factors that affects software development productivity. Several software cost models assess the productivity impacts of CASE tools based only on breadth of tool coverage without considering other productivity dimensions such as degree of integration, tool maturity, and user support. This paper provides an extended set of tool rating scales based on the completeness of tool coverage, the degree of tool integration, and tool maturity/user support. Those scales are used to refine the way in which CASE tools are effectively evaluated within COCOMO (constructive cost model) II. In order to find the best fit of weighting values for the extended set of tool rating scales in the extended research model, a Bayesian approach is adopted to combine two sources of (expert-judged and data-determined) information to increase prediction accuracy. The extended model using the three TOOL rating scales is validated by using the cross-validation methodologies, data splitting, and bootstrapping. This approach can be used to disaggregate other parameters that have significant impacts on software development productivity and to calibrate the best-fit weight values based on data-determined and expert-judged distributions. It results in an increase in the prediction accuracy in software parametric cost estimation models and an improvement in insights on software productivity investments.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1049401,no,undetermined,0
Requirements in the medical domain: Experiences and prescriptions,"Research shows that information flow in health care systems is inefficient and prone to error. Data is lost, and physicians must repeat tests and examinations because the results are unavailable at the right place and time. Cases of erroneous medication - resulting from misinterpreted, misunderstood, or missing information - are well known and have caused serious health problems and even death. We strongly believe that through effective use of information technology, we can improve both the quality and efficiency of the health sector's work. Introducing a new system might shift power from old to young, from doctor to nurse, or from medical staff to administration. Few people appreciate loss of power, but even fewer will admit that the loss of power is why they resist the new system. Thus, we must work hard to bring this into the open and help people realize that a new system doesn't have to threaten their positions. Again, knowledge and understanding of a hospital's organizational structure, both official and hidden, is necessary if the system's introduction is to be successful.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1049394,no,undetermined,0
Investigating the influence of software inspection process parameters on inspection meeting performance,"The question of whether inspection meetings justify their cost has been discussed in several studies. However, it is still open as to how modern defect detection techniques and team size influence meeting performance, particularly with respect to different classes of defect severity. The influence of software inspection process parameters (defect detection technique, team size, meeting effort) on defect detection effectiveness is investigated, i.e. the number of defects found for 31 teams which inspected a requirements document, to shed light on the performance of inspection meetings. The sets of defects reported by each team after the individual preparation phase (nominal-team performance) and after the team meeting (real-team performance) are compared. The main findings are that nominal teams perform significantly more effectively than real teams for all defect classes. This implies that meeting losses are on average higher than meeting gains. Meeting effort was positively correlated with meeting gains, indicating that synergy effects can only be realised if enough time is available. With regard to meeting losses, existing reports are confirmed that for a given defect, the probability of being lost in a meeting decreases with an increase in the number of inspectors who detected this defect during individual preparation.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1049199,no,undetermined,0
Engineering real-time behavior,"This article presents a process that evaluates an application for real-time correctness throughout development and maintenance. It allows temporal correctness to be designed-in during development, rather than the more typical effort to test-in timing performance at the end of development. It avoids the costly problems that can arise when timing faults are found late in testing or, worse still, after deployment.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1048980,no,undetermined,0
Application of linguistic techniques for Use Case analysis,"The Use Case formalism is an effective way of capturing both business process and functional system requirements in a very simple and easy-to-learn way. Use Cases may be modeled in a graphical way (e.g. using the UML notation), mainly serving as a table of content for Use Cases. System behavior can more effectively be specified by structured natural language (NL) sentences. The use of NL as a way to specify the behavior of a system is however a critical point, due to the inherent ambiguity originating from different interpretations of natural language descriptions. We discuss the use of methods, based on a linguistic approach, to analyze functional requirements expressed by means of textual (NL) Use Cases. The aim is to collect quality metrics and detect defects related to such inherent ambiguity. In a series of preliminary experiments, we applied a number of tools for quality evaluation of NL text (and, in particular, of NL requirements documents) to an industrial Use Cases document. The result of the analysis is a set of metrics that aim to measure the quality of the NL textual description of Use Cases. We also discuss the application of selected linguistic analysis techniques that are provided by some of the tools to semantic analysis of NL expressed Use Case.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1048518,no,undetermined,0
AGORA: attributed goal-oriented requirements analysis method,"This paper presents an extended version of the goal-oriented requirements analysis method called AGORA, where attribute values, e.g. contribution values and preference matrices, are added to goal graphs. An analyst attaches contribution values and preference values to edges and nodes of a goal graph respectively during the process for refining and decomposing the goals. The contribution value of an edge stands for the degree of the contribution of the sub-goal to the achievement of its parent goal, while the preference matrix of a goal represents the preference of the goal for each stakeholder. These values can help an analyst to choose and adopt a goal from the alternatives of the goals, to recognize the conflicts among the goals, and to analyze the impact of requirements changes. Furthermore the values on a goal graph and its structural characteristics allow the analyst to estimate the quality of the resulting requirements specification, such as correctness, unambiguity, completeness etc. The estimated quality values can suggest which goals should be improved and/or refined. In addition, we have applied AGORA to a user account system and assessed it.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1048501,no,undetermined,0
"Text localization, enhancement and binarization in multimedia documents","The systems currently available for content based image and video retrieval work without semantic knowledge, i.e. they use image processing methods to extract low level features of the data. The similarity obtained by these approaches does not always correspond to the similarity a human user would expect. A way to include more semantic knowledge into the indexing process is to use the text included in the images and video sequences. It is rich in information but easy to use, e.g. by key word based queries. In this paper we present an algorithm to localize artificial text in images and videos using a measure of accumulated gradients and morphological post processing to detect the text. The quality of the localized text is improved by robust multiple frame integration. Anew technique for the binarization of the text boxes is proposed. Finally, detection and OCR results for a commercial OCR are presented.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1048482,no,undetermined,0
Reducing No Fault Found using statistical processing and an expert system,"This paper describes a method for capturing avionics test failure results from Automated Test Equipment (ATE) and statistically processing this data to provide decision support for software engineers in reducing No Fault Found (NFF) cases at various testing levels. NFFs have plagued the avionics test and repair environment for years at enormous cost to readiness and logistics support. The costs in terms of depot repair and user exchange dollars that are wasted annually for unresolved cases are graphically illustrated. A diagnostic data model is presented, which automatically captures, archives and statistically processes test parameters and failure results which are then used to determine if an NFF at the next testing level resulted from a test anomaly. The model includes statistical process methods, which produce historical trend patterns for each part and serial numbered unit tested. An Expert System is used to detect statistical pattern changes and stores that information in a knowledge base. A Decision Support System (DSS) provides advisories for engineers and technicians by combining the statistical test pattern with unit performance changes in the knowledge base. Examples of specific F-16 NFF reduction results are provided.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1047966,no,undetermined,0
Binarization of low quality text using a Markov random field model,"Binarization techniques have been developed in the document analysis community for over 30 years and many algorithms have been used successfully. On the other hand, document analysis tasks are more and more frequently being applied to multimedia documents such as video sequences. Due to low resolution and lossy compression, the binarization of text included in the frames is a non-trivial task. Existing techniques work without a model of the spatial relationships in the image, which makes them less powerful. We introduce a new technique based on a Markov random field model of the document. The model parameters (clique potentials) are learned from training data and the binary image is estimated in a Bayesian framework. The performance is evaluated using commercial OCR software.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1047819,no,undetermined,0
Assessing Quality of Policy Properties in Verification of Access Control Policies,"Access control policies are often specified in declarative languages. In this paper, we propose a novel approach, called mutation verification, to assess the quality of properties specified for a policy and, in doing so, the quality of the verification itself. In our approach, given a policy and a set of properties, we first mutate the policy to generate various mutant policies, each with a single seeded fault. We then verify whether the properties hold for each mutant policy. If the properties still hold for a given mutant policy, then the quality of these properties is determined to be insufficient in guarding against the seeded fault, indicating that more properties are needed to augment the existing set of properties to provide higher confidence of the policy correctness. We have implemented Mutaver, a mutation verification tool for XACML, and applied it to policies and properties from a real-world software system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721554,no,undetermined,0
A comprehensive and practical approach for power system security assessment,"This paper proposes a new methodology of the power system dynamic security assessment. It automatically and successively scans contingencies of a power system; furthermore, based on the concept of stability margin, the severity of the contingencies is ranked. Its complement application is on the base of the combination with a dynamic simulation program. The authors demonstrate how to apply this new method to assess the stability security of the real-world network. In the assessment, two types of contingencies (N-1 and N-2) are applied on transformer, generator, bus or line. Assessment results help researchers and operators to make proper adjustment of system operation to ensure system security. It is shown that the new methodology is a comprehensive and practical approach to assess the power system security.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1047202,no,undetermined,0
Automatic test vector generation for bridging faults detection in combinational circuits using false Boolean functions,"The paper presents an automatic test vector generation program for detecting bridging faults in combinational circuits using false Boolean functions. These functions are used for solving the system of equations of the circuit concerning controllability, observability and interconnectivity concepts. The presented model applies to bridging faults that do not change the circuit nature i.e. the combinational free-fault circuit with these interconnectivity faults remains a combinational one.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1105874,no,undetermined,0
No Java without caffeine: A tool for dynamic analysis of Java programs,"To understand the behavior of a program, a maintainer reads some code, asks a question about this code, conjectures an answer, and searches the code and the documentation for confirmation of her conjecture. However, the confirmation of the conjecture can be error-prone and time-consuming because the maintainer has only static information at her disposal. She would benefit from dynamic information. In this paper, we present Caffeine, an assistant that helps the maintainer in checking her conjecture about the behavior of a Java program. Our assistant is a dynamic analysis tool that uses the Java platform debug architecture to generate a trace, i.e., an execution history, and a Prolog engine to perform queries over the trace. We present a usage scenario based on the n-queens problem, and two real-life examples based on the Singleton design pattern and on the composition relationship.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115000,no,undetermined,0
Is prior knowledge of a programming language important for software quality?,"Software engineering is human intensive. Thus, it is important to understand and evaluate the value of different types of experiences, and their relation to the quality of the developed software. Many job advertisements focus on requiring knowledge of specific programming languages. This may seem sensible at first sight, but maybe it is sufficient to have general knowledge in programming and then it is enough to learn a specific language within the new job. A key question is whether prior knowledge actually does improve software quality. This paper presents an empirical study where the programming experience of students is assessed using a survey at the beginning of a course on the Personal Software Process (PSP), and the outcome of the course is evaluated, for example, using the number of defects and development time. Statistical tests are used to analyse the relationship between programming experience and the performance of the students in terms of software quality. The results are mostly unexpected, for example, we are unable to show any significant relation between experience in the programming language used and the number of defects detected.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166922,no,undetermined,0
System testing for object-oriented frameworks using hook technology,"An application framework provides a reusable design and implementation for a family of software systems. If the framework contains defects, the defects will be passed on to the applications developed from the framework. Framework defects are hard to discover at the time the framework is instantiated. Therefore, it is important to remove all defects before instantiating the framework. The problem addressed in this paper is developing an automated state-based test suite generator technique that uses hook technology to produce test suites to test frameworks at the system level. A case study is reported and its results show that the proposed technique is reasonably effective at detecting faults. A supporting tool that automatically produces framework test cases, executes them, and evaluates the results is presented.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115018,no,undetermined,0
An implementation of a distributed algorithm for detection of local knots and cycles in directed graphs based on the CSP model and Java,"Cycles and knots in directed graphs are problems that can be associated with deadlocks in database and communication systems. Many algorithms to detect cycles and knots in directed graphs were proposed. Boukerche and Tropper (1998) have proposed a distributed algorithm that solves the problem in a efficient away. Their algorithm has a message complexity of 2 m vs. (at least) 4 m for the Chandy and Misra algorithm, where m is the number of links in the graph, and requires O (n log n) bits of memory, where n is the number of nodes. We have implemented Boukerche and Tropper's algorithm according to the construction of processes of the CSP model. Our implementation was done using JCSP, an implementation of CSP for Java, and the results are presented.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166900,no,undetermined,0
Review of condition assessment of power transformers in service,"As transformers age, their internal condition degrades, which increases the risk of failure. To prevent these failures and to maintain transformers in good operating condition is a very important issue for utilities. Traditionally, routine preventative maintenance programs combined with regular testing were used. The change to condition-based maintenance has resulted in the reduction, or even elimination, of routine time-based maintenance. Instead of doing maintenance at a regular interval, maintenance is only carried out if the condition of the equipment requires it. Hence, there is an increasing need for better nonintrusive diagnostic and monitoring tools to assess the internal condition of the transformers. If there is a problem, the transformer can then be repaired or replaced before it fails. An extensive review is given of diagnostic and monitoring tests, and equipment available that assess the condition of power transformers and provide an early warning of potential failure.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1161455,no,undetermined,0
Virtual center for renal support: technological approach to patient physiological image,"The patient physiological image (PPI) is a novel concept which manages the knowledge of the virtual center for renal support (VCRS), currently being developed by the Biomedical Engineering Group of the University of Seville. PPI is a virtual """"replica"""" of the patient, built by means of a mathematical model, which represents several physiological subsystems of a renal patient. From a technical point of view, PPI is a component-oriented software module based on cutting-edge modeling and simulation technology. This paper provides a methodological and technological approach to the PPI. Computational architecture of PPI-based VCRS is also described. This is a multi-tier and multi-protocol system. Data are managed by several ORDBMS instances. Communications design is based on the virtual private network (VPN) concept. Renal patients have a minimum reliable access to the VCRS through a public switch telephone network-X.25 gateway. Design complies with the universal access requirement, allowing an efficient and inexpensive connection even in rural environments and reducing computational requirements in the patient's remote access unit. VCRS provides support for renal patients' healthcare, increasing the quality and quantity of monitored biomedical signals, predicting events as hypotension or low dialysis dose, assisting further to avoid them by an online therapy modification and easing diagnostic tasks. An online therapy adjustment experiment simulation is presented. Finally, the presented system serves as a computational aid for research in renal physiology. This is achieved by an open and reusable modeling and simulation architecture which allows the interaction among models and data from different scales and computer platforms, and a faster transference of investigation models toward clinical applications.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1159134,no,undetermined,0
Structural assessment of cost of quality,"Evolving accreditation standards require that software engineering programs, and perhaps even individual courses, demonstrate that students acquire the knowledge and skills necessary to participate effectively in professional practice. To this end, we must be able to assess our students to determine if we have achieved these goals. More problematic, in the assessment realm, is the difference between """"knowledge in the head"""" and """"knowledge in practice"""". We need assessment methods to help us account for not only what is known, but how it is known. Structural assessment may represent a valuable resource in this endeavor. This method assesses a student's knowledge of the relationships among concepts, methodologies, and problems within a particular domain, and may well illuminate the issues that require our attention. Through the use of concept maps, a particular method for representing and conveying structural knowledge, the assessment can be based upon the differences between learner's and expert's structural knowledge. In this paper, we detail our plan to develop and use structural assessment of cost of quality in undergraduate software engineering.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1158640,no,undetermined,0
Using a pulsed supply voltage for delay faults testing of digital circuits in a digital oscillation environment,"High-performance digital circuits with aggressive timing constraints are usually very susceptible to delay faults. Much research done on delay fault detection needs a rather complicated test setup together with precise test clock requirements. In this paper, we propose a test technique based on the digital oscillation test method. The technique, which was simulated in software, consists of sensitizing a critical path in the digital circuit under test and incorporating the path into an oscillation ring. The supply voltage to the oscillation ring is then varied to detect delay and stuck-at faults in the path.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1146804,no,undetermined,0
Learning software engineering principles using open source software,"Traditional lectures espousing software engineering principles hardly engage studentspsila attention due to the fact that students often view software engineering principles as mere academic concepts without a clear understanding of how they can be used in practice. Some of the issues that contribute to this perception include lack of experience in writing and understanding large programs, and lack of opportunities for inspecting and maintaining code written by others. To address these issues, we have worked on a project whose overarching goal is to teach students a subset of basic software engineering principles using source code exploration as the primary mechanism. We attempted to espouse the following software engineering principles and concepts: role of coding conventions and coding style, programming by intention to develop readable and maintainable code, assessing code quality using software metrics, refactoring, and reverse engineering to recover design elements. Student teams have examined the following open source Java code bases: ImageJ, Apache Derby, Apache Lucene, Hibernate, and JUnit. We have used Eclipse IDE and relevant plug-ins in this project.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4720643,no,undetermined,0
Research and Assessment of the Reliability of a Fault Tolerant Model Using AADL,"In order to solve the problem of the assessment of the reliability of the fault tolerant system, the work in this paper is devoted to analyze a subsystem of ATC (air traffic control system), and use AADL (architecture analysis and design language) to build its model. After describing the various software and hardware error states and as well as error propagation from hardware to software, the work builds the AADL error model and convert it to GSPN (general stochastic Petri net). Using current Petri Net technology to assess the reliability of the fault tolerant system which is based on ATC as the background, this paper receives good result of the experiment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721310,no,undetermined,0
A study on a garbage collector for embedded applications,"In general, embedded systems, such as cellular phones and PDAs are provided with small amounts of memory and a low power processor that is slower than desktop ones. Despite these limited resources, present technology allows designers to integrate, in a single chip, an entire system. In this scenario, software development for embedded systems is an error-prone operation. In order to develop better code in less time, Java technology has gained a lot of interest from developers of embedded systems in the last few years, mainly because of its portability, code reuse, and object-oriented paradigm. On the other hand, Java requires an automatic memory management system in Java processors. This paper presents a garbage collection technique based on a software approach for an embedded Java processor. This technique is targeted for applications that are used in portable embedded systems. This paper discusses the most suited algorithm for such applications, showing also some performance overhead results.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137648,no,undetermined,0
The design and implementation of the intel real-time performance analyzer,"Modern PCs support growing numbers of concurrently active independently authored real-time software applications and device drivers. The non realtime nature of PC OSes (Linux, Microsoft Windows, etc.) means that robust real-time software must cope with hold-offs without degradation in user perceivable application quality of service. The open nature of the PC platform necessitates measuring OS interrupt and thread latencies under concurrent load in order to determine with how much hold-off the application must cope. The Intel Real-Time Performance Analyzer is a toolkit for PCs running Microsoft Windows. The toolkit statistically characterizes thread and interrupt latencies plus Windows Deferred Procedure Call (DPC) and kernel Work Item latencies. The toolkit also has facilities for analyzing the causes of long latencies. These latencies can then be incorporated as additional blocking times in a real-time schedulability analysis. An isochronous workload tool is included to model thread and DPC based computation and detect missed deadlines.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137387,no,undetermined,0
Predictive detection methods as the next era in biological signal processing: a case study of ECG analysis,"An idea of a detection method in biological signal processing to predict the possibility of proneness to a disease is described. The goal of this study is to introduce a new methodology in better understanding of biological signals and create new tools for predictive diagnosis rather than detection of the existing defects, so that, make it possible to give a proper preventative procedure/treatment to the patient in advance. Although the complete achievement of such a goal is very far, it is proposed by a prospective study on the ECG signals that the development of such a method is possible. The preliminary results have been encouraging enough to justify our idea. Such a new generation of predictive detection methods would have a profound impact on the medical diagnosis.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1134391,no,undetermined,0
Handling preprocessor-conditioned declarations,"Many software systems are developed with configurable functionality, and for multiple hardware platforms and operating systems. This can lead to thousands of possible configurations, requiring each configuration-dependent programming entity or variable to have different types. Such configuration-dependent variables are often declared inside preprocessor conditionals (e.g., C language). Preprocessor-conditioned declarations may be a source of problems. Commonly used configurations are type-checked by repeated compilation. Rarely used configurations are unlikely to be recently type checked, and in such configurations a variable may have a type not compatible to its use or it may contains uses of variables never defined. This paper proposes an approach to identify all possible types each variable declared in a software system can assume, and under which conditions. Inconsistent variable usages can then be detected for all possible configurations. Impacts of preprocessor-conditioned declaration in 17 different open source software systems are also reported.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1134108,no,undetermined,0
Asymptotics of quickest change detection procedures under a Bayesian criterion,"The optimal detection procedure for detecting changes in independent and identically distributed sequences (i.i.d.) in a Bayesian setting was derived by Shiryaev in the nineteen sixties. However, the analysis of the performance of this procedure in terms of the average detection delay and false alarm probability has been an open problem. In this paper, we investigate the performance of Shiryaev's procedure in an asymptotic setting where the false alarm probability goes to zero. The asymptotic study is performed not only in. the i.d.d. case where the Shiryaev's procedure is optimal but also in a general, non-i.i.d. case. In the latter case, we show that Shiryaev's procedure is asymptotically optimum under mild conditions. We also show that the two popular non-Bayesian detection procedures, namely the Page and Shiryaev-Roberts-Pollak procedures, are not optimal (even asymptotically) under the Bayesian criterion. The results of this study are shown to be especially important in studying the asymptotics of decentralized quickest change detection procedures.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115427,no,undetermined,0
Progress and Quality Modeling of Requirements Analysis Based on Chaos,"It is important and difficult for us to know the progress and quality of requirements analysis. We introduce chaos and software requirements complexity to the description of requirements decomposing, and get a method which can help us to evaluate the progress and quality. The model shows that requirements decomposing procedure has its own regular pattern which we can describe in a equation and track in a trajectory. The requirements analysis process of a software system can be taken as normal if its trajectory coincide with the model. We may be able to predict the time we need to finish all requirements decomposition in advance based on the model. We apply the method in the requirements analysis of homephone service management system, and the initial results show that the method is useful in the evaluation of requirements decomposition.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721324,no,undetermined,0
Combining and adapting software quality predictive models by genetic algorithms,"The goal of quality models is to predict a quality factor starting from a set of direct measures. Selecting an appropriate quality model for a particular software is a difficult, non-trivial decision. In this paper, we propose an approach to combine and/or adapt existing models (experts) in such way that the combined/adapted model works well on the particular system. Test results indicate that the models perform significantly better than individual experts in the pool.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1115031,no,undetermined,0
Design Diverse-Multiple Version Connector: A Fault Tolerant Component Based Architecture,"Component based software engineering (CBSE) is a new archetype to construct the systems by using reusable components ldquoas it isrdquo. To achieve high dependability in such systems, there must be appropriate fault tolerance mechanism in them at the architectural level. This paper presents a fault tolerant component based architecture that relies on the C2 architectural style and is based on design diverse and exception handling fault tolerance strategies. The proposed fault tolerant component architecture employs special-purpose connectors called design diverse-multiple version connectors (DD-MVC). These connectors allow design diverse n-versions of components to run in parallel. The proposed architecture has a fault tolerant connector (FTC), which detects and tolerates different kinds of errors. The proposed architecture adjusts the tradeoff between dependability and efficiency at run time and exhibits the ability to tolerate the anticipated and unanticipated faults effectively. The applicability of proposed architecture is demonstrated with a case study.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721327,no,undetermined,0
Hole analysis for functional coverage data,"One of the main goals of coverage tools is to provide the user with informative presentation of coverage information. Specifically, information on large, cohesive sets of uncovered tasks with common properties is very useful. This paper describes methods for discovering and reporting large uncovered spaces (holes) for crossproduct functional coverage models. Hole analysis is a presentation method for coverage data that is both succinct and informative. Using case studies, we show how hole analysis was used to detect large uncovered spaces and improve the quality of verification.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1012733,no,undetermined,0
Cluster-Based Error Messages Detecting and Processing for Wireless Sensor Networks,"Wireless sensor networks (WSNs) have emerged as a new technology about acquiring and processing messages for a variety of applications. Faults occurring to sensor nodes are common due to lack of power or environmental interference. In order to guarantee the network reliability of service, it is necessary for the WSN to be able to detect and processes the faults and take appropriate actions. In this paper, we propose a novel approach to distinguish and filter the error messages for cluter-based WSNs. The simulation results show that the proposed method not only can avoid frequent re-clustering but also can save the energy of sensor nodes, thus prolong the lifetime of sensor network.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722321,no,undetermined,0
Avoiding faulty privileges in fast stabilizing rings,"Most conventional studies on self-stabilization have been indifferent to the safety under convergence. This paper investigates how mutual exclusion property can be achieved in self-stabilizing rings even for illegitimate configurations. We present a new method which uses a state with a large state space to detect faults. If some faults are detected, every process is reset and not given a privilege. Even if the reset values are different between processes, our protocol mimics the behavior of Dijkstra's K-state protocol (1974). Then we have a fast and safe mutual exclusion protocol. A simulation study also shows its performance",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953655,no,undetermined,0
A probabilistic constructive approach to optimization problems,"We propose a new optimization paradigm for solving intractable combinatorial problems. The technique, named Probabilistic Constructive (PC), combines the advantages of both constructive and probabilistic algorithms. The constructive aspect provides relatively short runtime and makes the technique amenable for the inclusion of insights through heuristic rules. The probabilistic nature facilitates a flexible trade-off between runtime and the quality of solution. In addition to presenting the generic technique, we apply it to the Maximal Independent Set problem. Extensive experimentation indicates that the new approach provides very attractive trade-offs between the quality of the solution and runtime, often outperforming the best previously published approaches.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=968676,no,undetermined,0
FEM simulation and study on rectangular drawing process with flange,"Drawing process is important in manufacturing area. The development of tooling for rectangular drawing can be time-consuming and expensive, for the drawing process is very sensitive to some processing parameters, such as blank holder force, lubrication condition and layout of draw bead. Numerical simulation of the drawing process is a powerful tool for reducing costly trial-and-error loops and shortening development cycle. The finite element method (FEM) can simulate the stress-strain and thickness changes of the sheet, and predict the forming defects such as cracking, wrinkling, and thinning. As an example, numerical simulation of the rectangular drawing process is presented in the paper. A 3D finite element model of the drawing process is developed by the commercial FEM software, DYNAFORM, which is based on dynamic-explicit FEM procedure, LS-DYNA. The geometrical surfaces of tooling and sheet are modeled in CAD software, UG NX. The plastic stress-strain characteristics of the sheet are based on uniaxial tensile test. Simulation results show the distribution of stress, strain, and thickness. Based on the FLD and deformation results, the mould is designed. ManufacturerAs stamping experiments have shown that the agreement between simulation and experiment is good.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4730634,no,undetermined,0
Why is it so hard to predict software system trustworthiness from software component trustworthiness?,"When software is built from components, nonfunctional properties such as security, reliability, fault-tolerance, performance, availability, safety, etc. are not necessarily composed. The problem stems from our inability to know a priori, for example, that the security of a system composed of two components can be determined from knowledge about the security of each. This is because the security of the composite is based on more than just the security of the individual components. There are numerous reasons for this. The article considers only the factors of component performance and calendar time. It is concluded that no properties are easy to compose and some are much harder than others",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969773,no,undetermined,0
Assessing inter-modular error propagation in distributed software,"With the functionality of most embedded systems based on software (SW), interactions amongst SW modules arise, resulting in error propagation across them. During SW development, it would be helpful to have a framework that clearly demonstrates the error propagation and containment capabilities of the different SW components. In this paper, we assess the impact of inter-modular error propagation. Adopting a white-box SW approach, we make the following contributions: (a) we study and characterize the error propagation process and derive a set of metrics that quantitatively represents the inter-modular SW interactions, (b) we use a real embedded target system used in an aircraft arrestment system to perform fault-injection experiments to obtain experimental values for the metrics proposed, (c) we show how the set of metrics can be used to obtain the required analytical framework for error propagation analysis. We find that the derived analytical framework establishes a very close correlation between the analytical and experimental values obtained. The intent is to use this framework to be able to systematically develop SW such that inter-modular error propagation is reduced by design",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969769,no,undetermined,0
Detecting heap smashing attacks through fault containment wrappers,"Buffer overflow attacks are a major cause of security breaches in modern operating systems. Not only are overflows of buffers on the stack a security threat, overflows of buffers kept on the heap can be too. A malicious user might be able to hijack the control flow of a root-privileged program if the user can initiate an overflow of a buffer on the heap when this overflow overwrites a function pointer stored on the heap. The paper presents a fault-containment wrapper which provides effective and efficient protection against heap buffer overflows caused by <e2>C</e2> library functions. The wrapper intercepts every function call to the <e2>C</e2> library that can write to the heap and performs careful boundary checks before it calls the original function. This method is transparent to existing programs and does not require source code modification or recompilation. Experimental results on Linux machines indicate that the performance overhead is small",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969756,no,undetermined,0
A framework for assessing the use of third-party software quality assurance standards to meet FDA medical device software process control guideline's,"The proliferation of medical device software (MDS) potentially increases the risks of patient injury from software defects. The US Food and Drug Administration (FDA) in 1998 updated its MDS regulations, moving away from a product-based regulatory approach toward one more focused on quality assurance processes. However, what constituted acceptable software quality assurance (SQA) processes and whether regulations could be met by the use of third-party standards was not specified. The FDA has implicitly sanctioned using third-party SQA audits in submissions for accelerated review of modifications of existing MDS, but it has neither approved nor rejected their use in submissions for new MDS approval. Suppliers must assess whether adopting a third-party SQA standard assures full or only partial conformance with FDA requirements because they remain potentially liable for damages resulting from software defects. However, substantial differences in the philosophy and organization of FDA requirements and third-party standards make this assessment difficult. This research develops a framework to assess whether third-party SQA standards can meet FDA requirements and then employs the framework to determine if ISO 9000-3 or the Software Engineering Institute's Capability Maturity Model is sufficient to meet such requirements. The authors' research analyzes four SQA categories specified by the FDA guidelines: process management, requirements specification, design control, and change control. Analysis indicates that while neither third-party SQA standard by itself fully meets FDA requirements, either standard is worth adopting and is approximately equivalent in its usefulness",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969424,no,undetermined,0
"Edge, junction, and corner detection using color distributions","For over 30 years (1970-2000) researchers in computer vision have been proposing new methods for performing low-level vision tasks such as detecting edges and corners. One key element shared by most methods is that they represent local image neighborhoods as constant in color or intensity with deviations modeled as noise. Due to computational considerations that encourage the use of small neighborhoods where this assumption holds, these methods remain popular. The research presented models a neighborhood as a distribution of colors. The goal is to show that the increase in accuracy of this representation translates into higher-quality results for low-level vision tasks on difficult, natural images, especially as neighborhood size increases. We emphasize large neighborhoods because small ones often do not contain enough information. We emphasize color because it subsumes gray scale as an image range and because it is the dominant form of human perception. We discuss distributions in the context of detecting edges, corners, and junctions, and we show results for each",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969118,no,undetermined,0
Probability and agents,"To make sense of the information that agents gather from the Web, they need to reason about it. If the information is precise and correct, they can use engines such as theorem provers to reason logically and derive correct conclusions. Unfortunately, the information is often imprecise and uncertain, which means they will need a probabilistic approach. More than 150 years ago, George Boole presented the logic that bears his name. There is concern that classical logic is not sufficient to model how people do or should reason. Adopting a probabilistic approach in constructing software agents and multiagent systems simplifies some thorny problems and exposes some difficult issues that you might overlook if you used purely logical approaches or (worse!) let procedural matters monopolize design concerns. Assessing the quality of the information received from another agent is a major problem in an agent system. The authors describe Bayesian networks and illustrate how you can use them for information quality assessment",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=968836,no,undetermined,0
Pairwise Statistical Significance of Local Sequence Alignment Using Substitution Matrices with Sequence-Pair-Specific Distance,"Pairwise sequence alignment forms the basis of numerous other applications in bioinformatics. The quality of an alignment is gauged by statistical significance rather than by alignment score alone. Therefore, accurate estimation of statistical significance of a pairwise alignment is an important problem in sequence comparison. Recently, it was shown that pairwise statistical significance does better in practice than database statistical significance, and also provides quicker individual pairwise estimates of statistical significance without having to perform time-consuming database search. Under an evolutionary model, a substitution matrix can be derived using a rate matrix and a fixed distance. Although the commonly used substitution matrices like BLOSUM62, etc. were not originally derived from a rate matrix under an evolutionary model, the corresponding rate matrices can be back calculated. Many researchers have derived different rate matrices using different methods and data. In this paper, we show that pairwise statistical significance using rate matrices with sequence-pair-specific distance performs significantly better compared to using a fixed distance. Pairwise statistical significance using sequence-pair-specific distanced substitution matrices also outperforms database statistical significance reported by BLAST.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731306,no,undetermined,0
Dynamic simulator for studying WCDMA based hierarchical cell structures,"A dynamic radio network simulator is implemented for studying WCDMA based hierarchical cell structures. The simulator allows estimation of capacity and quality of service related issues in a two-layer network (microcells and macrocells). The input to the simulator is base station and mobile station information and its output is presented as the blocking and dropping probabilities, handoff rate and capacity of the assumed network. Both uplink and downlink are considered. As an example, the impact of between-layer handover on the capacity is investigated. The whole simulator is based entirely on visual C++ software",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=965487,no,undetermined,0
Research on PolicyBased Collaboration Models in Autonomic Computing,"Autonomic computing is expected to solve system management complexity and cost problems in IT environment by enabling systems to be self-managing. The autonomic computing collaborative work based on policy-driven is studied, including competition and collaboration. The autonomic elements should work collaboratively to implement the complex self-management tasks of autonomic computing system. The competition model is proposed based on the probability policy, and the equilibrium solutions are studied and resolved. It can improve the success rate and utility for practical applications effectively.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731408,no,undetermined,0
Development of the special software tools for the defect/fault analysis in the complex gates from standard cell library,"The development of special software tool named FIESTA (Faults Identification and Estimation of Test Ability) for the defect/fault analysis in the complex gates from industrial cell library is considered. This software tool is destined for the test developers and IC designers and is aimed at: a) probabilistic-based analysis of CMOS physical defects in VLSI circuits: b) facilitation of the work on development of hierarchical probabilistic automatic generation of test patterns; c) improvement of the layout in order to decrease the influence of spot defects on IC manufacturability. We consider the principle concepts of the FIESTA development. They are based on the developed approaches to 1) the identification and estimation of the probability of actual faulty functions resulting from shorts and opens caused by spot defects in the conductive layers of IC layout, and to 2) the evaluation of the effectiveness/usefulness of the test vector components in faults detection",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966791,no,undetermined,0
Optimization Processing in Quadrilateral Meshes Generation Based on Cloud Data,"Quadrilateral meshes generation algorithm based on cloud data is proposed in the paper. Quadrilateral meshes are generated by means of dynamic edges extending without considering the parity of number of points on boundaries to adapt itself to complex boundaries. Collisions detecting of mesh boundaries and an algorithm of optimization processing are set forth. The quality of meshes is improved, and insure rate of the algorithm running. The realization of optimization processing algorithms was introduced in detail, and examples are presented to illustrate the ability of the algorithm.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731688,no,undetermined,0
A software methodology for detecting hardware faults in VLIW data paths,"The proposed methodology aims at providing concurrent hardware fault detection properties in data paths for VLIW processor architectures. The approach, carried out on the application software consists in the introduction of additional instructions for controlling the correctness of the computation with respect to failures in one of the data path functional units. The paper presents the methodology and its application to a set of media benchmarks",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966766,no,undetermined,0
FedEx - a fast bridging fault extractor,"Test pattern generation and diagnosis algorithms that target realistic bridging faults must be provided with a realistic fault list. In this work we describe FedEx, a bridging fault extractor that extracts a circuit from the mask layout, identifies the two-node bridges that can occur, their locations, layers, and relative probability of occurrence. Our experimental results show that FedEx is memory efficient and fast",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966690,no,undetermined,0
Measuring voice readiness of local area networks,"It is well known that company intranets are growing into ubiquitous communications media for everything. As a consequence, network traffic is notoriously dynamic, and unpredictable. In most scenarios, the data network requires tuning to achieve acceptable quality for voice integration. This paper introduces a performance measurement method based on widely used IP protocol elements, which allows measurement of network performance criteria to predict the voice transmission feasibility of a given local area network. The measurement does neither depend on special VoIP (Voice over IP) equipment, nor does it need network monitoring hardware. Rather it uses special payload samples to detect unloaded network conditions to receive reference values. These samples are followed by a typical VoIP application payload to obtain real-world measurement conditions. We successfully validate our method within a local area network and present all captured values that describe important aspects of voice quality",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966227,no,undetermined,0
A dynamic buffer management scheme based on rate estimation in packet-switched networks,"While traffic volume of real-time applications is rapidly increasing, current routers do not guarantee minimum QoS values of fairness and they drop packets in a random fashion. If routers provide a minimum QoS, resulting a less delay, reduced delay-jitter, more fairness, and smooth sending rates, TCP-friendly rate control (TFRC) can be adopted for real-time applications. We propose a dynamic buffer management scheme that meets the requirements described above, and can be applied to TCP flow and to data flow for transfer of real-time applications. The proposed scheme consists of a virtual threshold function, an accurate and stable per-flow rate estimation, and a per-flow exponential drop probability. We discuss how this scheme motivates real-time applications to adopt TCP-friendly rate control",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=966190,no,undetermined,0
An Improving Fault Detection Mechanism in Service-Oriented Applications Based  on Queuing Theory,"SOA has become more and more popular, but fault tolerance is not fully supported in most existing SOA frameworks and solutions provided by various major software companies. SOA implementations with large number of users, services, or traffic, maintaining the necessary performance levels of applications integrated using an ESB presents a substantial challenge, both to the architects who design the infrastructure as well as to IT professionals who are responsible for administration. In this paper, we improve the performance model for analyzing and detecting faults based on the queuing theory. The performance of services of SOA applications is measuring in two categories (individual services and composite services). We improve the model of the individuals services and add the composite services performance measuring.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4730494,no,undetermined,0
Dynamic networking: architecture and prototype systems,"In this paper, we propose anew architecture of the global communication networks, the dynamic networking architecture. The dynamic functions enhance the capabilities of communication networks to deal with various changes detected by human users, applications and networked environment. In this architecture, a new functional layer called flexible network layer (FNL) is introduced between the application layer and the transport layer of the global communication networks. To realize the FNL, we adopt an agent framework to develop and manage various components and related knowledge of agent-based middleware of FNL. We explain the experimental applications of the FNL to discuss the characteristics of the proposed architecture",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=970449,no,undetermined,0
Easy Recommendation Based on Probability Model,"Traditional collaborative filtering recommendation system suffers from some significant limitations, such as scalability and sparsity, which cause the speed and quality of recommendation system is unacceptable. To alleviate these problems, this paper proposes a novel algorithm based on probability model. Our algorithm can directly generate the preference prediction from database and at the same time has the ability to reflect the changes of users' interest incrementally. The effectiveness of the new algorithm is estimated by our experiments.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725958,no,undetermined,0
"Design of integrated software for reconfiguration, reliability, and protection system analysis","Interdependencies among software components for distribution network reconfiguration, reliability and protection system analysis are considered. Software interface specifications are presented. Required functionalities of reconfiguration for restoration are detailed. Two algorithms for reconfiguration for restoration are reviewed and compared. Use of outage analysis data to locate circuit sections in need of reliability improvements and to track predicted improvements in reliability is discussed",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=971385,no,undetermined,0
Integration of remote sensing and geographic information system technology for monitoring changes in the Northwestern Blue Nile Region of Ethiopia,"Environmental degradation has been identified as a major problem in Ethiopia today. Inappropriate use of land management practices has decreased the country's arable and forest lands, drastically deteriorated soil and water quality and severely affected the biodiversity within the environment. Desertification, deforestation, and urbanization are believed to be the primary causes of the loss in biodiversity and global climate change. It is therefore necessary to assess, take inventory, and determine the effect of land use land cover (LULC) change on the environment in this region. Multi-date satellite imagery was obtained to quantify the changes that have occurred. Integration of the results of the imagery analysis and GIS was used to define policies that encourage intelligent use of natural resources. The study site was the northwest part of Ethiopia surrounding the Blue Nile Region of Lake Tana. The primary objective of this project was to use remotely sensed data (i) to quantify the LULC change that has occurred over a 12-year period; (ii) identify the nature and spatial distribution of the change; and (iii) define a management approach that will prevent further environmental degradation. Landsat TM-5 and 7 imagery from 1987 and 1999 respectively, were acquired and each scene was georeferenced and radiometrically corrected. The images were processed using ERDAS Imagine 8.4 (ERDAS Inc., Atlanta, GA) Image Processing software. Comparing results of the unsupervised classification for 1987 and 1999 we observed a major loss of riparian forest along the bank of the Blue Nile River. It was also evident that a considerable amount of land was deforested which may have contribute to the continuing soil loss from the highlands of Ethiopia",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=976188,no,undetermined,0
To a problem on components operation of a distributed system for image processing and analysis,"The modern tasks of processing and analysis of large scale arrays of graphics information demand real-time operation and considerable computational resources. The ideology of developing such systems does not fit the increasing requirements and does not involve the possibilities of these computational resources. Classifications of such systems and their architectures are conducted. The advantages and disadvantages of existing solutions are detected. New goals and tasks are considered, and research directions in the field are pointed out. The alternative system structure for large distributed graphical information array processing, modes of implementation of data exchange processes between separate components of this system, and methods of integration with existing information storage and visualization tools are offered. The main component of the tendered system architecture is the intelligent computing core realizing the principle of an expert system. The feature of its operation is encompassed by using a self-learning mode during operation. It allows not only to improve the quality of automatic processing, and to use expert knowledge, but also to accumulate its own experience. Possible application areas of such a system are considered",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=975080,no,undetermined,0
Dependability analysis of fault-tolerant multiprocessor systems by probabilistic simulation,"The objective of this research is to develop a new approach for evaluating the dependability of fault-tolerant computer systems. Dependability has traditionally been evaluated through combinatorial and Markov modelling. These analytical techniques have several limitations, which can restrict their applicability. Simulation avoids many of the limitations, allowing for more precise representation of system attributes than feasible with analytical modelling. However, the computational demands of simulating a system in detail, at a low abstraction level, currently prohibit evaluation of high-level dependability metrics such as reliability and availability. The new approach abstracts a system at the architectural level, and employs life testing through simulated fault-injection to accurately and efficiently measure dependability. The simulation models needed to implement this approach are derived, in part, from the published results of computer performance studies and low-level fault-injection experiments. The developed probabilistic models of processor, memory and fault-tolerant mechanisms take such properties of real systems, as error propagation, different modes of failures, event dependency and concurrency. They have been integrated with a workload model and statistical analysis module into a generalised software tool. The effectiveness of such an approach was demonstrated through the analysis of several multiprocessor architectures",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=975079,no,undetermined,0
"Framework for modeling software reliability, using various testing-efforts and fault-detection rates","This paper proposes a new scheme for constructing software reliability growth models (SRGM) based on a nonhomogeneous Poisson process (NHPP). The main focus is to provide an efficient parametric decomposition method for software reliability modeling, which considers both testing efforts and fault detection rates (FDR). In general, the software fault detection/removal mechanisms depend on previously detected/removed faults and on how testing efforts are used. From practical field studies, it is likely that we can estimate the testing efforts consumption pattern and predict the trends of FDR. A set of time-variable, testing-effort-based FDR models were developed that have the inherent flexibility of capturing a wide range of possible fault detection trends: increasing, decreasing, and constant. This scheme has a flexible structure and can model a wide spectrum of software development environments, considering various testing efforts. The paper describes the FDR, which can be obtained from historical records of previous releases or other similar software projects, and incorporates the related testing activities into this new modeling approach. The applicability of our model and the related parametric decomposition methods are demonstrated through several real data sets from various software projects. The evaluation results show that the proposed framework to incorporate testing efforts and FDR for SRGM has a fairly accurate prediction capability and it depicts the real-life situation more faithfully. This technique can be applied to wide range of software systems",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=974129,no,undetermined,0
Model integrated computing in robot control to synthesize real-time embedded code,"Manufacturing robots present a class of embedded systems with hard real-time constraints. On the one hand controller software has to satisfy tight timing constraints and rigorous memory requirements. Especially nonlinear dynamics and kinematics models are vital to modern model-based controllers and trajectory planning algorithms. Often this is still realized by manually coding and optimizing the software, a labor intensive and error-prone repetitive process. On the other hand shorter design-cycles and a growing number of customer-specific robots demand more flexibility not just in modeling. This paper presents a model-integrated computing approach to automated code synthesis of dynamics models that satisfies the harsh demands by including domain and problem specific constraints prescribed by the robotics application. It is shown that the use of such tailored formalisms leads to very efficient embedded software, competitive with the hand optimized alternative. At the same time it combines flexibility in model specification and usage with the potential for dynamic adaptation and reconfiguration of the model",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=973961,no,undetermined,0
A graphical class representation for integrated black- and white-box testing,"Although both black- and white-box testing have the same objective, namely detecting faults in a program, they are often conducted separately. In our opinion, the reason is the lack of techniques and tools integrating both strategies, although an integration can substantially decrease testing costs. Specifically, an integrated technique can generate a reduced test suite, as single test cases can cover both specification and implementation at the same time. The paper proposes a new graphical representation of classes, which can be used for integrated class-level black-and white-box testing. Its distinguishing feature from existing representations is that each method of a class is shown from two perspectives, namely the specification and implementation view. Both the specification of a method and its implementation are represented as control flow graphs, which allows black- and white-box testing by structural techniques. Moreover, a test suite reduction technique has been developed for adjusting white-box test cases to black-box testing",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972789,no,undetermined,0
Dynamic and static views of software evolution,"In addition to managing day-to-day maintenance, information system managers need to be able to predict and plan the longer-term evolution of software systems on an objective, quantified basis. Currently this is a difficult task, since the dynamics of software evolution, and the characteristics of evolvable software are not clearly understood. In this paper we present an approach to understanding software evolution. The approach looks at software evolution from two different points of view. The dynamic viewpoint investigates how to model software evolution trends and the static viewpoint studies the characteristics of software artefacts to see what makes software systems more evolvable. The former will help engineers to foresee the actions to be taken in the evolution process, while the latter provides an objective, quantified basis to evaluate the software with respect to its ability to evolve and will help to produce more evolvable software systems",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972776,no,undetermined,0
Bayesian analysis of software cost and quality models,"Due to the pervasive nature of software, software-engineering practitioners have continuously expressed their concerns over their inability to accurately predict the cost, schedule and quality of a software product under development. Thus, one of the most important objectives of the software engineering community has been to develop useful models that constructively explain the software development lifecycle and accurately predict the cost, schedule and quality of developing a software product. Most of the existing parametric models have been empirically calibrated to actual data from completed software projects. The most commonly used technique for empirical calibration has been the popular classical multiple regression approach. This approach imposes a few restrictions often violated by software engineering data and has resulted in the development of inaccurate empirical models that do not perform very well. The focus of this dissertation is to explain the drawbacks of the multiple regression approach for software engineering data and discuss the Bayesian approach which alleviates a few of the problems faced by the multiple regression approach",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972773,no,undetermined,0
Summary of dynamically discovering likely program invariants,"The dissertation dynamically discovering likely program invariants introduces dynamic detection of program invariants, presents techniques for detecting such invariants from traces, assesses the techniques' efficacy, and points the way for future research. Invariants are valuable in many aspects of program development, including design, coding, verification, testing, optimization, and maintenance. They also enhance programmers' understanding of data structures, algorithms, and program operation. Unfortunately, explicit invariants are usually absent from programs, depriving programmers and automated tools of their benefits. The dissertation shows how invariants can be dynamically detected from program traces that capture variable values at program points of interest. The user runs the target program over a test suite to create the traces, and an invariant detector determines which properties and relationships hold over both explicit variables and other expressions. Properties that hold over the traces and also satisfy other tests, such as being statistically justified, not being over unrelated variables, and not being implied by other reported invariants, are reported as likely invariants. Like other dynamic techniques such as testing, the quality of the output depends in part on the comprehensiveness of the test suite. If the test suite is inadequate, then the output indicates how, permitting its improvement. Dynamic analysis complements static techniques, which can be made sound but for which certain program constructs remain beyond the state of the art. Experiments demonstrate a number of positive qualities of dynamic invariant detection and of a prototype implementation, Daikon. Invariant detection is accurate-it rediscovers formal specifications-and useful-it assists programmers in programming tasks. It runs quickly and produces output of modest size. Test suites found in practice tend to be adequate for dynamic invariant detection",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972767,no,undetermined,0
Let The Puppets Move! Automated Testbed Generation for Service-oriented Mobile Applications,"There is a growing interest for techniques and tools facilitating the testing of mobile systems. The movement of nodes is one of the relevant factors of context change in ubiquitous systems and a key challenge in the validation of context-aware applications. An approach is proposed to generate a testbed for service-oriented systems that takes into account a mobility model of the nodes of the network in which the accessed services are deployed. This testbed allows a tester to assess off-line the QoS properties of a service under test, by considering possible variations in the response of the interacting services due to node mobility.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725737,no,undetermined,0
Defect prevention through defect prediction: a case study at Infosys,"This paper is an experience report of a software process model which will help in preventing defects through defect prediction. The paper gives a vivid description of how the model aligns itself to business goals and also achieves various quality and productivity goals by predicting the number and type of defects well in advance and corresponding preventive action taken to reduce the occurrence of defects. Data have been collected from the case study of a live project in INFOSYS Technologies Limited, India. A project team always aims at a zero defect software or a quality product with as few a defects as possible. To deliver a defect free software, it is imperative that in the process of development maximum number of defects are captured and fixed them before we deliver to the customer. In other words our process model should help us detect maximum number of defects possible through various Quality Control activities. Also the process model should be able to predict defects and should help us to detect them quite early. Defects can be reduced in two ways - (i) By detecting it at each and every stage in the project life cycle or (ii) By preventing to occur",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972739,no,undetermined,0
Using code metrics to predict maintenance of legacy programs: a case study,"The paper presents an empirical study on the correlation of simple code metrics and maintenance necessities. The goal of the work is to provide a method for the estimation of maintenance in the initial stages of outsourcing maintenance projects, when the maintenance contract is being prepared and there is very little available information on the software to be maintained. The paper shows several positive results related to the mentioned goal",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972733,no,undetermined,0
Flow analysis to detect blocked statements,"In the context of software quality assessment, the paper proposes two new kinds of data which can be extracted from source code. The first, definitely blocked statements, can never be executed because preceding code prevents the execution of the program. The other data, called possibly blocked statements, may be blocked by blocking code. The paper presents original flow equations to compute definitely and possibly blocked statements in source code. The experimental context is described and results are shown and discussed. Suggestions for further research are also presented",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972712,no,undetermined,0
Causal reasoning for human supervised process reconfiguration,"As safety is becoming an essential concern in industrial automation, an emerging area in automatic control is fault tolerant control. Within the various techniques, reconfiguration employs the redundancy in the plant and its control to make intelligent software that monitors the behavior of the whole. This paper analyses the reconfiguration problem from the point of view of large-scale processes under the responsibility of human operators. As analytical models cannot be envisaged for representing a process with hundred variables and several unpredictable operating modes, reconfiguration is proposed to rely on a simple qualitative model. This model represents the cause-effect relations between variables under the form of a directed graph. The graph is backward searched off line to find the action means on a variable. Then the most relevant remedial actions are selected online after a fault is detected. An example of a nuclear process is used to illustrate the method",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=971490,no,undetermined,0
A case study on reliability improvement of 10 worst performing feeders in Niagara Mohawk Power Corp. (NMPC) service territory,"This case study demonstrates the reliability improvement initiative taken by Niagara Mohawk Power Corporation (NMPC), to analyze 10 of their worst performing feeders. The CYMDIST-RAM (Reliability Assessment Module) from CYME International Inc. was used for the analysis. The program computes system indices (SAIFI, SAIDI, CAIDI etc.) and load point indices (interruption frequency, outage duration etc.) for each zone on the feeder, based on the failure rates and repair times input by the user. Load indices of a zone reflect the trouble areas within a feeder, and are useful for micro-analysis. Variation of different indices along a feeder may be displayed as a color code. Indices may also be reported on the one-line diagram, and as reports in various formats (spreadsheet, Excel, ASCII etc.). Corrective measures such as device addition/relocation, tree-trimming, fault locators were attempted and reliability improvement was assessed in terms of saved CHI (customer-hrs interrupted). Different projects were ranked according to the cost benefit factor (<e1>n</e1>vestment/CHI saved)",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=971387,no,undetermined,0
Accelerating learning from experience: avoiding defects faster,"All programmers learn from experience. A few are rather fast at it and learn to avoid repeating mistakes after once or twice. Others are slower and repeat mistakes hundreds of times. Most programmers' behavior falls somewhere in between: They reliably learn from their mistakes, but the process is slow and tedious. The probability of making a structurally similar mistake again decreases slightly during each of some dozen repetitions. Because of this a programmer often takes years to learn a certain rule-positive or negative-about his or her behavior. As a result, programmers might turn to the personal software process (PSP) to help decrease mistakes. We show how to accelerate this process of learning from mistakes for an individual programmer, no matter whether learning is currently fast, slow, or very slow, through defect logging and defect data analysis (DLDA) techniques",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=965803,no,undetermined,0
Design and evaluation of context-dependent protective relaying approach,"This paper, introduces a new concept of robust protective relays based on unique type of self-organized neural network. An advanced approach for protective relay testing and evaluation is presented as well. The proposed relaying solution detects and subsequently classifies the faults. A new interactive simulation environment based on MATLAB is selected as the main software environment for synthesis and evaluation of complex protection algorithms. Other application programs may be connected with MATLAB and used for simulation of specific power system faults and events",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=964870,no,undetermined,0
Subjective Evaluation of Sound Quality for Mobile Spatial Digital Audio,"In the past decades, technical developments have enabled the delivery of sophisticated mobile spatial audio signals to consumers, over links that range very widely in quality, requiring decisions to be made about the tradeoffs between different aspects of audio quality. It is therefore important to determine the most important spatial quality attributes of reproduced sound fields and to find ways of predicting perceived sound quality on the basis of objective measurements.This paper first briefly reviews several subjective quality measures developed for streaming realtime audio over mobile network communications and in digital audio broadcasts. Then a experimental design on the application of the subjective listening test for mobile spatial audio is described. Finally, the conclusion is analysed and some future research directions are identified.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722334,no,undetermined,0
Modeling and prediction of distribution system voltage distortion caused by nonlinear residential loads,"Electric utilities have expressed concern over increased nonlinear loading of residential power distribution systems. The number and variety of power electronic products found in the typical home continues to grow rapidly, imposing a burden on local power companies to supply reliable, distortion-free service. In order to adequately prepare for the future, utilities must be able to predict the harmonic impact of new power electronic equipment and evaluate the ability of existing power systems to accommodate these nonlinear loads. This paper describes a modeling methodology that uses SPICE simulation software to predict the voltage distortion caused by nonlinear residential loads. Results of applying the methodology to a specific distribution system containing either of two different types of harmonic-rich loads, i.e., variable speed air conditioners or electric vehicle battery chargers, are presented as well",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=956765,no,undetermined,0
"The Exu approach to safe, transparent and lightweight interoperability","Exu is a new approach to automated support for safe, transparent and lightweight interoperability in multilanguage software systems. The approach is safe because it enforces appropriate type compatibility across language boundaries. It is transparent since it shields software developers from the details inherent in low-level language-based interoperability mechanisms. It is lightweight for developers because it eliminates tedious and error-prone coding (e.g., JNI) and lightweight at run-time since it does not unnecessarily incur the performance overhead of distributed, IDL-based approaches. The Exu approach exploits and extends the object-oriented concept of meta-object, encapsulating interoperability implementation in meta-classes so that developers can produce interoperating code by simply using meta-inheritance. An example application of Exu to the development of Java/C++ (i.e., multilanguage) programs illustrates the safety and transparency advantages of the approach. Comparing the performance of the Java/C++ programs produced by Exu to the same set of programs developed using IDL-based approaches provides preliminary evidence of the performance advantages of Exu",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960644,no,undetermined,0
Automated generation of statistical test cases from UML state diagrams,"The adoption of the object-oriented (OO) technology for the development of critical software raises important testing issues. This paper addresses one of these issues: how to create effective tests from OO specification documents? More precisely, the paper describes a technique that adapts a probabilistic method, called statistical functional testing, to the generation of test cases from UML state diagrams, using transition coverage as the testing criterion. Emphasis is put on defining an automatic way to produce both the input values and the expected outputs. The technique is automated with the aid of the Rational Software Corporation's Rose RealTime tool. An industrial case study from the avionics domain, formally specified and implemented in Java, is used to illustrate the feasibility of the technique at the subsystem level. Results of first test experiments are presented to exemplify the fault revealing power of the created statistical test cases",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960618,no,undetermined,0
An End-to-End Content-Aware Congestion Control Approach for MPEG Video Transmission,"In this paper, we present a receiver-based, bandwidth estimation rate control mechanism with content-aware probability retransmission to limit the burden on multimedia transmission congested network. Like the TCP friendly rate control (TFRC) protocol, we compute the sending rate as a function of the loss event rate and round-trip time. Considering the different importance of four distinct types of frames in Standard MPEG encoders, we divide data packets into three grades coarsely and adopt adaptive probability retransmission strategy to assure video playback quality. It is an extension of TCP friendly congestion control. This paper describes the smooth rate algorithm and probability retransmission mechanism. The result of experiments with competing TFRC specification demonstrates the proposed approach reaches a higher throughput and higher PSNR than TFRC especially on the bottleneck links.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4734070,no,undetermined,0
Investigating reinspection decision accuracy regarding product-quality and cost-benefit estimates,"After a software inspection the project manager has to decide whether a product has sufficient quality to pass on to the next software development stage or whether a second inspection cycle, a reinspection, is likely to sufficiently improve its quality. The reinspection decision of recent research focused on the estimation of product quality after inspection, which does not take in to account the effect of a reinspection. Thus we propose to use estimation models for the quality improvement during reinspection and the cost and benefit of a reinspection as basis for the reinspection decision. We evaluate the reinspection decision correctness of these models with time-stamped defect data from a large-scale controlled experiment on the inspection and reinspection of a software requirements document. The main finding of the investigation is that the product quality criterion is likely to force products to be reinspected, if a large number of defects were detected in the first inspection. Further the product-quality, criterion is especially sensitive to an underestimation of the number of defects in the product and will let bad products pass as good. The cost-benefit criterion is less sensitive to estimation error than the product-quality criterion and should in practice be used as second opinion, if a product-quality criterion is applied",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960602,no,undetermined,0
Word shape recognition for image-based document retrieval,"We propose a word shape recognition method for retrieving image-based documents. Document images are segmented at the word level first. Then the proposed method detects local extrema points in word segments to form so-called vertical bar patterns. These vertical bar patterns form the feature vector of a document. The scalar product of two document feature vectors is calculated to measure the pairwise similarity of document images. The proposed method is robust to changing fonts and styles, and is less affected by degradation of document qualities. Three groups of words in different fonts and image qualities were used to test the validity of our method. Real-life document images were also used to test the method's ability of retrieving relevant documents",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=959245,no,undetermined,0
Adaptive algorithms for variable-complexity video coding,"Variable-complexity algorithms provide a means of managing the computational complexity of a software video CODEC. The reduction in computational complexity provided by existing variable-complexity algorithms depends on the video scene characteristics and is difficult to predict. A new approach to variable-complexity encoding is proposed. A variable-complexity DCT algorithm is adaptively updated in order to maintain a near-constant computational complexity. The adaptive update algorithm is shown to be capable of providing a significant, predictable, reduction in computational complexity with only a small loss of video quality. The proposed approach may be particularly useful for software-only video encoding, in applications where processing resources are limited",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=959052,no,undetermined,0
A novel compression algorithm for cell animation images,"We propose a novel compression algorithm for cell animation images. Conventional software algorithms include Cinepak, MPEG-1, Indeo5, etc., but these are for natural images, not for cell animation images. At low bit-rates, these provide relatively very poor image quality. In the proposed method, for intra frame coding, octree-based color quantization, ADPCM, and Golomb-Rice code are used. And for inter frame coding, block-to-block difference information is classified and coded. Therefore computational complexity is relatively low. The proposed methods outperform MPEG-1, FLC, and Indeo 5",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=958527,no,undetermined,0
A short circuit current study for the power supply system of Taiwan railway,"The western Taiwan railway transportation system consists mainly of a mountain route and ocean route. The Taiwan Railway Administration (TRA) has conducted a series of experiments on the ocean route in recent years to identify the possible causes of unknown events which cause the trolley contact wires to melt down frequently. The conducted tests include the short circuit fault test within the power supply zone of the Ho Long substation (Zhu Nan to Tong Xiao) that had the highest probability for the melt down events. Those test results based on the actual measured maximum short circuit current provide a valuable reference for TRA when comparing against the said events. The Le Blanc transformer is the main transformer of the Taiwan railway electrification system. The Le Blanc transformer mainly transforms the Taiwan Power Company (TPC) generated three phase alternating power supply system (69 kV, 60 Hz) into a two single-phase alternating power distribution system (M phase and T phase) (26 kV, 60 Hz) needed for the trolley traction. As a unique winding connection transformer, the conventional software for fault analysis will not be able to simulate its internal current and phase difference between each phase currents. Therefore, besides extracts of the short circuit test results, this work presents a EMTP model based on a Taiwan Railway substation equivalent circuit model with the Le Blanc transformer. The proposed circuit model can simulate the same short circuit test to verify the actual fault current and accuracy of the equivalent circuit model. Moreover, the maximum short circuit current is further evaluated with reference to the proposed equivalent circuit. Preliminary inspection of trolley contact wire reveals the possible causes of melt down events based on the simulation results",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=956727,no,undetermined,0
Electrical integration assessment of wind turbines into industrial power systems: the case of a mining unit,"Onsite diesel-based power generation is a common practice within the industry, not just for supplying private demands but also as a solution to power quality problems. Although diesel generation has benefits, the cost of running such systems is rather high. In favoured locations, especially geographically-remote sites, wind-powered generation is an attractive, cost-effective option. This paper focusses on the development of individual generator models and some simulation results to assess the electrical integration of wind turbines into industrial power systems. Also, an insight into the assessment procedure and the proposed case study are also introduced",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=964848,no,undetermined,0
Evaluating meta-programming mechanisms for ORB middleware,"Distributed object computing middleware, such as CORBA, COM+, and Java RMI, shields developers from many tedious and error-prone aspects of programming distributed applications. It is hard to evolve distributed applications after they are deployed, however, without adequate middleware support for meta-programming mechanisms, such as smart proxies, interceptors, and pluggable protocols. These mechanisms can help improve the adaptability of distributed applications by allowing their behavior to be modified without changing their existing software designs and implementations significantly. This article examines and compares common meta-programming mechanisms supported by DOC middleware. These mechanisms allow applications to adapt more readily to changes in requirements and runtime environments throughout their lifecycles. Some of these meta-programming mechanisms are relatively new, whereas others have existed for decades. This article provides a systematic evaluation of these mechanisms to help researchers and developers determine which are best suited to their application needs",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=956121,no,undetermined,0
Supporting usability through software architecture,"Software engineers should consider usability as a quality attribute in their architectural a designs. Usability determines how effectively and comfortably an end-user can achieve the goals that gave rise to an interactive system. It is an important attribute to consider during all phases of software design, but especially during architectural design because of the expense involved in adding usability aspects after users have tested the system. Since the 1980s, ongoing work on supporting usability through software architectural constructs has focused on the iterative design process for the user interface, which involves initial design, user testing, re-design to correct detected flaws, re-testing, and so on. The traditional software architectural response to repeated and expected modifications to the user interface is to use separation, encapsulation and information hiding to localize the user interface",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=955105,no,undetermined,0
Rational interpolation examples in performance analysis,"The rational interpolation approach has been applied to performance analysis of computer systems previously. In this paper, we demonstrate the effectiveness of the rational interpolation technique in the analysis of randomized algorithms and the fault probability calculation for some real-time systems",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=954515,no,undetermined,0
APD-based measurement technique to estimate the impact of noise emission from electrical appliances on digital communication systems,"This paper describes a technique to measure noise emissions from electrical appliances and study their impact towards digital communication systems by using the amplitude probability distribution (APD) methodology. The APD has been proposed within CISPR for measurement of electromagnetic noise emission. We present a measurement approach which utilizes a programmable digitizer with an analysis software to evaluate the noise APD pattern and probability density function (PDF). A unique noise APD pattern is obtained from each measurement of noise emission from different appliances. The noise PDF is useful for noise modeling and simulation, from which we can estimate the degradation on digital communication performance in terms of bit error probability (BEP). This technique provides a simple platform to examine the effect of other electrical appliances noise emission towards specific digital communication services.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4736019,no,undetermined,0
An object-oriented organic architecture for next generation intelligent reconfigurable mobile networks,"Next generation mobile networks have great potential in providing personalised and efficient quality of service by using re-configurable platforms. The foundation is the concept of software radio where both the mobile terminal and the serving network can be re-configurable. This approach becomes more effective when combined with historic-based prediction strategies that enable the system to learn about application behaviour and predict its resource consumption. We extend that concept by proposing the use of an object-oriented intelligent decision making architecture, which supports general and large-scale applications. The proposed architecture applies the principles of business intelligence and data warehousing, together with the concept of organic viable systems. The architecture is applied to the CAST (configurable radio with advanced software technology) platform",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=954069,no,undetermined,0
Vertical bar detection for gauging text similarity of document images,"A new method for gauging text similarity of image-based documents using word shape recognition is proposed in this paper. Image features are directly extracted instead of using OCR (optical character recognition). The proposed method forms so-called vertical bar patterns by detecting local extrema points in word units extracted by segmenting the document images. These vertical bar patterns form the feature vector of a document. The pair-wise similarity of document images is measured by calculating the scalar product of two document feature vectors. The proposed method is robust to changing fonts and styles, and is less affected by degradation of document qualities. To test the validity of the method, four corpora of document images were used and the ability of the method to retrieve relevant documents is reported",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953868,no,undetermined,0
Probabilistic model for segmentation based word recognition with lexicon,"We describe the construction of a model for off-line word recognizers based on over-segmentation of the input image and recognition of segment combinations as characters in a given lexicon word. One such recognizer, the Word Model Recognizer (WMR), is used extensively. Based on the proposed model it was possible to improve the performance of WMR",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953776,no,undetermined,0
Full-Reference Quality Assessment for Video Summary,"As video summarization techniques have attracted more and more attention for efficient multimedia data management, quality assessment of video summary is required. To address the lack of automatic evaluation techniques, this paper proposes a novel framework including several new algorithms to assess the quality of the video summary against a given reference. First, we partition the reference video summary and the candidate video summary into the sequences of summary unit (SU). Then, we utilize alignment based algorithm to match the SUs in the candidate summary with the SUs in the corresponding reference summary. Third, we propose a novel similarity based 4 C - assessment algorithm to evaluate the candidate video summary from the perspective of coverage, conciseness, coherence, and context, respectively. Finally, the individual assessment results are integrated according to userpsilas requirement by a learning based weight adaptation method. The proposed framework and techniques are experimented on a standard dataset of TRECVID 2007 and show the good performance in automatic video summary assessment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4734018,no,undetermined,0
Scenario-based functional regression testing,"Regression testing has been a popular quality-assurance technique. Most regression testing techniques are based on code or software design. This paper proposes a scenario-based functional regression testing, which is based on end-to-end (E2E) integration test scenarios. The test scenarios are first represented in a template model that embodies both test dependency and traceability. By using test dependency information, one can obtain a test slicing algorithm to detect the scenarios that are affected and thus they are candidates for regression testing. By using traceability information, one can find affected components and their associated test scenarios and test cases for regression testing. With the same dependency and traceability information one can use the ripple effect analysis to identify all affected, including directly or indirectly, scenarios and thus the set of test cases can be selected for regression testing. This paper also provides several alternative test-case selection approaches and a hybrid approach to meet various requirements. A web-based tool has been developed to support these regression testing tasks",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960659,no,undetermined,0
Coupling of design patterns: common practices and their benefits,"Object-oriented (OO) design patterns define collections of interconnected classes that serve a particular purpose. A design pattern is a structural unit in a system built out of patterns, not unlike the way a function is a structural unit in a procedural program or a class is a structural unit in an OO system designed without patterns. When designers treat patterns as structural units, they become concerned with issues such as coupling and cohesion at a new level of abstraction. We examine the notion of pattern coupling to classify how designs may include coupled patterns. We find many examples of coupled patterns; this coupling may be """"tight"""" or """"loose"""", and provides both benefits and costs. We qualitatively assess the goodness of pattern coupling in terms of effects on maintainability, factorability, and reusability when patterns are coupled in various ways",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960670,no,undetermined,0
Calculation of deadline missing probability in a QoS capable cluster interconnect,"The growing use of clusters in diverse applications, many of which have real-time constraints, requires Quality-of-Service (QoS) support from the underlying cluster interconnect. In this paper we propose an analytical model that captures the characteristics of a QoS capable wormhole router which is the basic building block of cluster networks. The model captures the behavior of integrated traffic in a cluster and computes the average deadline missing probability for real-time traffic. The cluster interconnect, considered here, is a hypercube network. Comparison of Deadline Missing Probability (DMP) using the proposed model with that of the simulation shows that our analytical model is accurate and useful",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962514,no,undetermined,0
Analysis of power system transients using wavelets and Prony method,"Transients resulting from switching of capacitor banks in electrical distribution systems affects power quality. The transient overvoltages can theoretically reach peak phase to earth values in the order of 2.0 p.u. High current transients can reach values to ten times the capacitor nominal current with duration of several milliseconds. Another severe operating condition is the switching on a second capacitor bank connected to the same bus. In the work, the characteristics of the transients are analyzed. The time of the beginning of a transient process is detected using a wavelet transform. The frequencies of transient components have been investigated applying the Fourier technique and the Prony model. The investigations show the advantages of the methods basing on the Prony model, over the Fourier technique. A distribution system was simulated using the EMTP software",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=964820,no,undetermined,0
Case study: medical Web service for the automatic 3D documentation for neuroradiological diagnosis,"The case study presents a medical Web service for the automatic analysis of CTA (computer tomography angiography) datasets. It aims at the detection and evaluation of intracranial aneurysms which are malformations of cerebral blood vessels. To obtain a standardized 3D visualization, digital videos are automatically generated. The time-consuming video production caused by the manual delineation of structures, software based volume rendering, and the interactive definition of an optimized camera path is considerably improved with a fully automatic strategy. Therefore, a previously suggested approach (C. Rezk-Salama, 2000) is applied which uses an optimized transfer function as a template and automatically adapts it to an individual dataset. Furthermore, we introduce hardware-accelerated morphologic filtering in order to detect the location of mid-size and giant aneurysms. The actual generation of the video is finally integrated into a hardware accelerated off-screen rendering process based on 3D texture mapping, ensuring fast visualization of high quality. Overall, clinical routine can be considerably assisted by providing a Web based service combining automatic detection and standardized visualization.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=964542,no,undetermined,0
A comparison of algorithm-based fault tolerance and traditional redundant self-checking for SEU mitigation,"The use of an algorithmic, checksum-based """"EDAC"""" (error detection and correction) technique for matrix multiply operations is compared with the more traditional redundant self-checking hardware and retry approach for mitigating single event upset (SEU) or transient errors in soft, radiation tolerant signal processing hardware. Compared with the self-checking approach, the check-sum based EDAC technique offers a number of advantages including lower size, weight, power, and cost. In a manner similar to the SECDED (single error correction/double error detection) EDAC technique commonly used on memory systems, the checksum-based technique can detect and correct errors on the same processing cycle, reducing transient error recovery latency and significantly improving system availability. The paper compares the checksum-based technique with the self-checking technique in terms of failure rates; upset rates coverage, percentage overhead, detection latency, recovery latency, size, weight, power, and cost. The paper also looks at the percentage overhead of the checksum-based technique, which decreases as the size of the matrix increases",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=964242,no,undetermined,0
Automated video chain optimization,"Video processing algorithms found in complex video appliances such as television sets and set top boxes exhibit an interdependency that makes it is difficult to predict the picture quality of an end product before it is actually built. This quality is likely to improve when algorithm interaction is explicitly considered. Moreover, video algorithms tend to have many programmable parameters, which are traditionally tuned in manual fashion. Tuning these parameters automatically rather than manually is likely to speed up product development. We present a methodology that addresses these issues by means of a genetic algorithm that, driven by a novel objective image quality metric, finds high-quality configurations of the video processing chain of complex video products.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=964153,no,undetermined,0
An analysis of the gap between the knowledge and skills learned in academic software engineering course projects and those required in real: projects,"This paper describes how the Software Engineering Body of Knowledge, (SWEBOK) can be used as a guide to assess and improve software engineering courses. A case study is presented in which the guide is applied to a typical undergraduate software engineering course. The lessons learned are presented which the authors believe are generalizable to comparable courses taught at many academic institutions. A novel approach involving largescale software project simulation is also presented a way to overcome some of the course deficiencies identified by the guide",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=963881,no,undetermined,0
Signal processing techniques for Diacoustic(R) analysis of mechanical systems,"It has long been understood that mechanical failures, both extant and incipient, can often be detected by the human ear. A good mechanic can spot many kinds of trouble just by listening and will have a stethoscope in his or her tool box. Diacoustic(R) analysis is a method of achieving the same result, more reliably, using advanced signal processing algorithms. An overall diagnostic system consists of both hardware and software. Depending on the application, the Diacoustic(R) functions will be integrated with other sensors and software for their analysis. For example, temperatures and pressures are frequently indicative of machine health. Modern vehicles use gas sensors (such as the oxygen sensor in an automobile) to ascertain combustion efficiency. All other such indicia must be included with Diacoustic(R) techniques to comprise complete failure detection and warning system. In this paper the author addresses the Diacoustic(R) functions only",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=963354,no,undetermined,0
Effect of code coverage on software reliability measurement,"Existing software reliability-growth models often over-estimate the reliability of a given program. Empirical studies suggest that the over-estimations exist because the models do not account for the nature of the testing. Every testing technique has a limit to its ability to reveal faults in a given system. Thus, as testing continues in its region of saturation, no more faults are discovered and inaccurate reliability-growth phenomena are predicted from the models. This paper presents a technique intended to solve this problem, using both time and code coverage measures for the prediction of software failures in operation. Coverage information collected during testing is used only to consider the effective portion of the test data. Execution time between test cases, which neither increases code coverage nor causes a failure, is reduced by a parameterized factor. Experiments were conducted to evaluate this technique, on a program created in a simulated environment with simulated faults, and on two industrial systems that contained tenths of ordinary faults. Two well-known reliability models, Goel-Okumoto and Musa-Okumoto, were applied to both the raw data and to the data adjusted using this technique. Results show that over-estimation of reliability is properly corrected in the cases studied. This new approach has potential, not only to achieve more accurate applications of software reliability models, but to reveal effective ways of conducting software testing",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=963124,no,undetermined,0
An Interface Matrix Based Detecting Method for the Change of Component,"Component-based software engineering has increased the quality and the efficiency in software development. But the component adaptation is still a crucial issue in Component-based software engineering. In this paper, we focus on a dynamic analysis on the change of component and a method detecting the impact on both the correlative component and the whole system. Firstly, the component model and adaptation principle is described in formal specification. Then the connection matrix of component interface is constructed to help us analyze the relationship of interface. Finally, we propose a new dynamic detecting method based on interface matrix. According to the detecting method expressed in this paper, we have developed a tool CIDT, which is used in CBSE to analyze the impact of the component change.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732165,no,undetermined,0
The development of security system and visual service support software for on-line diagnostics,"Hitachi's CD-SEM achieves the highest tool availability in the industry. However, efforts to further our performance are continuously underway. The proposed on-line diagnostics system can allow senior technical staff to monitor and investigate tool status by connecting the equipment supplier and the device manufacturer sites through the Internet. The advanced security system ensures confidentiality by firewalls, digital certification, and advanced encryption algorithms to protect device manufacturer data from unauthorized access. Service support software, called DDS (defective part diagnosis support system), will analyze the status of mechanical, evacuation, and optical systems. Its advanced overlay function on a timing chart identifies failed components in the tool and allows on-site or remote personnel to predict potential failures prior to their occurrence. Examples of application shows that the proposed system is expected to reduce repair time, improve availability and lower cost of ownership",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962911,no,undetermined,0
A probability-based approach of transaction consistency in mobile environments,"In mobile distributed databases, the communications between sites only provide weak connectivity. To improve the efficiency of transaction processing in mobile computers, lazy replication is used extensively. But this approach either doesn't guarantee serializability and consistency as needed by applications or imposes restrictions on placement of data and which data objects can be updated. The shortcomings make it difficult for lazy replication to be adaptive to the dynamic changes of network connection and configuration in mobile environments. In the paper, we propose a probability-based approach, which guarantees serializability and consistency. We adopt the quality of service specification and achieve transaction consistency dynamically through the collaboration between applications and systems. Probability-based approach is flexible and adaptive to mobile computing environments. The experimental results suggest that probability-based approach outperform ordinary lazy replication",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962639,no,undetermined,0
"On comparisons of random, partition, and proportional partition testing","Early studies of random versus partition testing used the probability of detecting at least one failure as a measure of test effectiveness and indicated that partition testing is not significantly more effective than random testing. More recent studies have focused on proportional partition testing because a proportional allocation of the test cases (according to the probabilities of the subdomains) can guarantee that partition testing will perform at least as well as random testing. We show that this goal for partition testing is not a worthwhile one. Guaranteeing that partition testing has at least as high a probability of detecting a failure comes at the expense of decreasing its relative advantage over random testing. We then discuss other problems with previous studies and show that failure to include important factors (cost, relative effectiveness) can lead to misleading results",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962563,no,undetermined,0
Prioritizing test cases for regression testing,"Test case prioritization techniques schedule test cases for execution in an order that attempts to increase their effectiveness at meeting some performance goal. Various goals are possible; one involves rate of fault detection, a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during testing can provide faster feedback on the system under test and let software engineers begin correcting faults earlier than might otherwise be possible. One application of prioritization techniques involves regression testing, the retesting of software following modifications; in this context, prioritization techniques can take advantage of information gathered about the previous execution of test cases to obtain test case orderings. We describe several techniques for using test execution information to prioritize test cases for regression testing, including: 1) techniques that order test cases based on their total coverage of code components; 2) techniques that order test cases based on their coverage of code components not previously covered; and 3) techniques that order test cases based on their estimated ability to reveal faults in the code components that they cover. We report the results of several experiments in which we applied these techniques to various test suites for various programs and measured the rates of fault detection achieved by the prioritized test suites, comparing those rates to the rates achieved by untreated, randomly ordered, and optimally ordered suites",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962562,no,undetermined,0
Software cost estimation with incomplete data,"The construction of software cost estimation models remains an active topic of research. The basic premise of cost modeling is that a historical database of software project cost data can be used to develop a quantitative model to predict the cost of future projects. One of the difficulties faced by workers in this area is that many of these historical databases contain substantial amounts of missing data. Thus far, the common practice has been to ignore observations with missing data. In principle, such a practice can lead to gross biases and may be detrimental to the accuracy of cost estimation models. We describe an extensive simulation where we evaluate different techniques for dealing with missing data in the context of software cost modeling. Three techniques are evaluated: listwise deletion, mean imputation, and eight different types of hot-deck imputation. Our results indicate that all the missing data techniques perform well with small biases and high precision. This suggests that the simplest technique, listwise deletion, is a reasonable choice. However, this will not necessarily provide the best performance. Consistent best performance (minimal bias and highest precision) can be obtained by using hot-deck imputation with Euclidean distance and a z-score standardization",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962560,no,undetermined,0
Operators for Analyzing Software Reliability with Petri Net,"Reliability is one of the most important indicators for software quality. Among the present researches of software reliability, majority focus on the appliance of probability statistics model for the whole software system. Few work based on software model for analyzing the software reliability is learned. Reliability Petri Net (RPN) is presented in this paper. In RPN, transaction means the function or module of software, and is marked with reliability gene. Based on analyzing the Petri net structure, four reliability operators are developed to perform the relationships between tractions. Reliability formulas are provided respectively for the reduction and decomposition operations of Petri net. Furthermore, priority of these reliability operators is given. With this research, more complex Petri net model could be greatly simplified and the reliability of the system could be evaluated effectively and easily. An example is provided for demonstrating the practicability of this reliability analysis method.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732411,no,undetermined,0
Dependability under malicious agreement in N-modular redundancy-on-demand systems,"In a multiprocessor under normal loading conditions, idle processors offer a natural spare capacity. Previous work attempted to utilize this redundancy to overcome the limitations of classic diagnosability and modular redundancy techniques while providing significant fault tolerance. A common approach is task duplexing. The usefulness of this approach for critical applications, unfortunately, is seriously undermined by its susceptibility to agreement on faulty outcomes (malicious agreement). To assess dependability of duplexing under malicious agreement, we propose a stochastic model which dynamically profiles behavior in the presence of malicious faults. The model uses the so-called policy referred to as NMR on demand (NMROD). Each task in a multiprocessor is duplicated, with additional processors allocated for recovery as needed. NMROD relies on a fault model favoring response correctness over actual fault status, and integrates online repair to provide non-stop operation over an extended period",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962518,no,undetermined,0
"A versatile C++ toolbox for model based, real time control systems of robotic manipulators","Model based technologies form the core of advanced robotic applications such as model predictive control and feedback linearization. More sophisticated models result in higher quality but the use in embedded real-time control systems imposes strict requirements on timing, memory allocation, and robustness. To satisfy these constraints, the model implementation is often optimized by manual coding, an unwieldy and error prone process. The paper presents an approach that exploits code synthesis from high level intuitive and convenient multi-body system (MBS) model descriptions. It relies on an object-oriented C++ library of MBS components tailored to the computations required in robot control such as forward and inverse kinematics, inverse dynamics, and Jacobians. Efficient model evaluation algorithms are developed that apply to multi-body tree structures as well as kinematic loops that are solved analytically for a certain class of loop structures",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=976398,no,undetermined,0
Software architecture for modular self-reconfigurable robots,"Modular, self-reconfigurable robots show the promise of great versatility, robustness and low cost. However, programming such robots for specific tasks, with hundreds of modules and each of which with multiple actuators and sensors, can be tedious and error-prone. The extreme versatility of the modular systems requires a new paradigm in programming. We present a software architecture for this type of robot, in particular the PolyBot, which has been developed through its third generation. The architecture, based on the properties of the PolyBot electro-mechanical design, features a multi-master/multi-slave structure in a multi-threaded environment, with three layers of communication protocols. The architecture is currently being implemented for Motorola PowerPC using vxWorks",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=976422,no,undetermined,0
"The application of remote sensing, geographic information systems, and Global Positioning System technology to improve water quality in northern Alabama","Recently, the water quality status in northern Alabama has been declining due to urban and agricultural growth. Throughout the years, the application of remote sensing and geographic information system technology has undergone numerous modifications and revisions to enhance their ability to control, reduce, and estimate the origin of non-point source pollution. Yet, there is still a considerable amount of uncertainty surrounding the use of this technology as well as its modifications. This research demonstrates how the application of remote sensing, geographic information system, and global positioning system technologies can be used to assess water quality in the Wheeler Lake watershed. In an effort to construct a GIS based water quality database of the study area for future use, a land use cover of the study area will be derived from LANDSAT Thermatic Mapper (TM) imagery using ERDAS IMAGINE image processing software. A Digital Elevation Model of the Wheeler Lake watershed was also from an Environmental Protection Agency Basins database. Physical and chemical properties of water samples including pH, Total Suspended Solids (TSS), Total Fecal Coliform (TC), Total Nitrogen (TN), Total Phosphorus (TP), Biological Oxygen Demand (BOD), Dissolved Oxygen (DO), and selected metal concentrations were measured",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=976822,no,undetermined,0
A Case Retrieval Method for Knowledge-Based Software Process Tailoring Using Structural Similarity,"Reuse of the software development process and its knowledge and experiences is a critical factor for the success of the software project. On the other hand, the software development process needs to be tailored to reflect the specific characteristics of the software project. So, if we can retrieve a similar process to a new project, process tailoring will be less costly and less error-prone because the retrieved process can be tailored to the new case with fewer modifications. In this paper, we propose the case retrieval method based on structural similarity. The structural similarity is calculated by the degree that process elements in a past case are applicable to a new project. By measuring the structural similarity, the retrieved process is ensured to be tailored to the new case with fewer modifications. We validate the usefulness of our method through the experiments using 30 cases.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724531,no,undetermined,0
Modeling the impact of preflushing on CTE in proton irradiated CCD-based detectors,"A software model is described that performs a """"real world"""" simulation of the operation of several types of charge-coupled device (CCD)-based detectors in order to accurately predict the impact that high-energy proton radiation has on image distortion and modulation transfer function (MTF). The model was written primarily to predict the effectiveness of vertical preflushing on the custom full frame CCD-based detectors intended for use on the proposed Kepler Discovery mission, but it is capable of simulating many other types of CCD detectors and operating modes as well. The model keeps track of the occupancy of all phosphorous-silicon (P-V), divacancy (V-V) and oxygen-silicon (O-V) defect centers under every CCD electrode over the entire detector area. The integrated image is read out by simulating every electrode-to-electrode charge transfer in both the vertical and horizontal CCD registers. A signal level dependency on the capture and emission of signal is included and the current state of each electrode (e.g., barrier or storage) is considered when distributing integrated and emitted signal. Options for performing preflushing, preflashing, and including mini-channels are available on both the vertical and horizontal CCD registers. In addition, dark signal generation and image transfer smear can be selectively enabled or disabled. A comparison of the charge transfer efficiency (CTE) data measured on the Hubble space telescope imaging spectrometer (STIS) CCD with the CTE extracted from model simulations of the STIS CCD show good agreement",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003673,no,undetermined,0
FPGA resource and timing estimation from Matlab execution traces,"We present a simulation-based technique to estimate area and latency of an FPGA implementation of a Matlab specification. During simulation of the Matlab model, a trace is generated that can be used for multiple estimations. For estimation the user provides some design constraints such as the rate and bit width of data streams. In our experience the runtime of the estimator is approximately only 1/10 of the simulation time, which is typically fast enough to generate dozens of estimates within a few hours and to build cost-performance trade-off curves for a particular algorithm and input data. In addition, the estimator reports on the scheduling and resource binding used for estimation. This information can be utilized not only to assess the estimation quality, but also as first starting point for the final implementation",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003597,no,undetermined,0
Transparent and Autonomic Rollback-Recovery in Cluster Systems,"Cluster systems provide an excellent environment to run computation hungry applications. However, due to being created using commodity components they are prone to failures. To overcome these failures we propose to use rollback-recovery, which consists of the checkpointing and recovery facilities. Checkpointing facilities have been the focus of many previous studies; however, the recovery facilities have been overlooked. This paper focuses on the requirements, concept and architecture of recovery facilities. The synthesized fault tolerant system was implemented in the GENESIS system and evaluated. The results show that the synthesized system is efficient and scalable.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724363,no,undetermined,0
Predictive distribution reliability analysis considering post fault restoration and coordination failure,"The calculation of predicted distribution reliability indexes can be implemented using a distribution analysis model and the algorithms defined by the """"Distribution System Reliability Handbook"""", EPRI Project 1356-1 Final Report. The calculation of predicted reliability indexes is fairly straightforward until post fault restoration and coordination failure are included. This paper presents the methods used to implement predictive reliability with consideration for post fault restoration and coordination failure into a distribution analysis software model",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1002290,no,undetermined,0
Using simulation to facilitate effective workflow adaptation,"In order to support realistic real-world processes, workflow systems need to be able to adapt to changes. Detecting the need to change and deciding what changes to carry out are very difficult. Simulation analysis can play an important role in this. It can be used in tuning quality of service metrics and exploring """"what-if"""" questions. Before a change is actually made, its possible effects can be explored with simulation. To facilitate rapid feedback, the workflow system (METEOR) and simulation system (JSIM) need to interoperate. In particular, workflow specification documents need to be translated into simulation model specification documents so that the new model can be executed/animated on-the-fly. Fortunately, modern Web technology (e.g., XML, DTD, XSLT) make this relatively straightforward. The utility of using simulation in adapting a workflow is illustrated with an example from a genome workflow.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1000151,no,undetermined,0
Configurable services for mobile users,"Mobile devices, such as cellular phones, personal digital assistants (PDAs), and organizers, are becoming increasingly popular. Due to the high volatility of those devices, the achievable quality-of-service (QoS) for mobile services can hardly be predicted. Even for one particular type of device - say a PDA - the implementation of a mobile service may use different communication interfaces over time (i.e.; wireless LAN, IrDA). Within this paper we present a new approach towards configuration of component-based services for mobile systems. Starting from a XML-based configuration language, which defines a set of rules for component configuration depending on a number of environmental parameters, our approach allows for instantiation and configuration of components. In contrast to many other approaches targeting distributed multimedia-style application on PC-class computers, our framework focuses on the extension of distributed services onto mobile devices. As proof-of-concept scenario we have implemented a configurable distributed video surveillance application on the basis of the Microsoft Distributed Component Object Model on Windows 2000 and on the Windows CE-based Pocket PC platform",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1000049,no,undetermined,0
Assessing multi-version systems through fault injection,"Multi-version design (MVD) has been proposed as a method for increasing the dependability, of critical systems beyond current levels. However, a major obstacle to large-scale commercial usage of this approach is the lack of quantitative characterizations available. We seek to help answer this problem using fault injection. This approach has the potential for yielding highly useful metrics with regard to MVD systems, as well as giving developers a greater insight into the behaviour of each channel within the system. In this research, we develop an automatic fault injection system for multi-version systems called FITMVS. We use this si,stem to test a multi-version system, and then analyze the results produced. We conclude that this approach can yield useful metrics, including metrics related to channel sensitivity, code scope sensitivity, and the likelihood of common-mode failure occurring within a system",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1000042,no,undetermined,0
A simulation based approach for estimating the reliability of distributed real-time systems,"Designers of safety-critical real-time systems are often mandated by requirements on reliability as well as timing guarantees. For guaranteeing timing properties, the standard practice is to use various analysis techniques provided by hard real-time scheduling theory. The paper presents analysis based on simulation, that considers the effects of faults and timing parameter variations on schedulability analysis, and its impact on the reliability estimation of the system. We look at a wider set of scenarios than just the worst case considered in hard real-time schedulability analysis. The ideas have general applicability, but the method has been developed with modelling the effects of external interferences on the controller area network (CAN) in mind. We illustrate the method by showing that a CAN interconnected distributed system, subjected to external interference, may be proven to satisfy its timing requirements with a sufficiently high probability, even in cases when the worst-case analysis has deemed it non-schedulable.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996375,no,undetermined,0
Quantitative analysis of myocardial perfusion and regional left ventricular function from contrast-enhanced power modulation images,"Our goal was to test the feasibility of using power modulation, a new echocardiographic imaging technique, for combined quantitative assessment of myocardial perfusion and regional LV function. Coronary balloon occlusions were performed in 18 anesthetized pigs. Images were obtained during iv contrast infusion at baseline, during coronary occlusion and reperfusion, and analyzed using custom software. At each phase, regional myocardial perfusion was assessed by calculating mean pixel intensity and the rate of contrast replenishment following high-power ultrasound impulses. LV function was assessed by calculating regional fractional area change. All ischemic episodes caused delectable and reversible changes in perfusion and function. Perfusion defects were visualized in real time and confirmed by a significant decrease in pixel intensity in the LAD territory following balloon inflation and reduced rate of contrast replenishment. Fractional area change significantly decreased in ischemic segments, and was restored with reperfusion. Power modulation allows simultaneous on-line assessment of myocardial perfusion and regional LV wall motion",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=977600,no,undetermined,0
An efficient QoS routing algorithm for quorumcast communication,"This paper extends the concept of multicast to quorumcast, a generalized form of multicast communication. The need of quorumcast communication arises in a number of distributed applications. Little work has been done on routing quorumcast messages. The objective of previous research was to construct a minimum cost tree spanning the source and the quorumcast group members. We further consider the path quality of a constructed spanning tree in terms of delay constraints required by applications that use the tree. As the delay-constrained quorumcast routing problem is NP-complete, we propose an efficient heuristic QoS routing algorithm. We also consider how a loop is detected and removed in the course of tree construction and how to deal with members joining/leaving the quorumcast pool. Our simulation study shows that the proposed algorithm performs well and constructs a quorumcast tree whose cost is close to that of the """"optimal"""" routing tree.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=992890,no,undetermined,0
"A case study: validation of guidance control software requirements for completeness, consistency and fault tolerance","We discuss a case study performed for validating a natural language (NL) based software requirements specification (SRS) in terms of completeness, consistency, and fault-tolerance. A partial verification of the Guidance and Control Software (GCS) Specification is provided as a result of analysis using three modeling formalisms. Zed was applied first to detect and remove ambiguity from the GCS partial SRS. Next, Statecharts and Activity-charts were constructed to visualize the Zed description and make it executable. The executable model was used for the specification testing and fault injection to probe how the system would perform under normal and abnormal conditions. Finally, a Stochastic Activity Networks (SANs) model was built to analyze how fault coverage impacts the overall performability of the system. In this way, the integrity of the SRS was assessed. We discuss the significance of this approach and propose approaches for improving performability/fault tolerance",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=992714,no,undetermined,0
Detecting Inconsistent Values Caused by Interaction Faults Using Automatically Located Implicit Redundancies,"This paper addresses the problem of detecting inconsistent values caused by interaction faults originated from an external system.This type of error occurs when a correctly formatted message that is not corrupted during transmission is generated with a field that contains incorrect data.When traditional schemes cannot be used, one alternative is resorting to receiver-based strategies that employ implicit redundancies - relations between events or data, often identified by a human expert.We propose an approach for detecting inconsistent values using implicit redundancies which are automatically located in examples of communications.We show that, even without adding any redundant information to the communication, the proposed approach can achieve a reasonable error detection coverage in fields where sequential relations exist.Other aspects, such as false alarms and latency, are also evaluated.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725289,no,undetermined,0
Rejuvenation and failure detection in partitionable systems,"Certain gateways (e.g., some cable or DSL modems) are known to have low reliability and low availability. Most failures of these devices can however be """"fixed"""" by rejuvenating the device after a failure has been detected. Such a detection based rejuvenation strategy permits increasing the availability of these gateways. In the considered scenario, rejuvenation is non-trivial since a failure of such a gateway will leave it partitioned away from the network. In particular, network operators that want to rejuvenate these gateways are in a different network partition, and can therefore not initiate a remote rejuvenation. In this paper we propose a failure detection based rejuvenation service and a remote detection service. The rejuvenation service detects and fixes """"soft"""" failures automatically (in one partition), and the detection service detects (in another partition) all rejuvenations exactly once, within a bounded amount of time, even when the gateway is rejuvenated consecutively. The detection service also allows the detection of """"hard"""" failures, and filtering of notifications of soft failures",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=992692,no,undetermined,0
Modeling the dependability of N-modular redundancy on demand under malicious agreement,"In a multiprocessor under normal loading conditions, idle processors naturally offer spare capacity. Previous work attempted to utilize this redundancy to overcome the limitations of classic diagnosability and modular redundancy techniques while providing significant fault tolerance. A popular approach is task duplexing. The usefulness of this approach for critical applications, unfortunately, is seriously undermined by its susceptibility to agreement on faulty outcomes (malicious agreement). To assess the dependability of duplexing under malicious agreement, we propose a stochastic model which dynamically profiles behavior in the presence of malicious faults. The model uses a more or less typical policy we call NMR on demand (NMROD). Each task in a multiprocessor is duplicated, with additional processors allocated for recovery as needed. NMROD relies on a fault model favoring response correctness over actual fault status, and integrates online repair to provide nonstop operation over an extended period",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=992682,no,undetermined,0
The linguistic approach to the natural language requirements quality: benefit of the use of an automatic tool,"Natural language (NL) requirements are widely used in the software industry, at least as the first level of description of a system. Unfortunately they are often prone to errors and this is partially caused by interpretation problems due to the use of NL itself. The paper presents a methodology for the analysis of natural language requirements based on a quality model addressing a relevant part of the interpretation problems that can be approached at linguistic level. To provide an automatic support to this methodology a tool called QuARS (quality analyzer of requirement specification) has been implemented. The methodology and the underlying quality model have been validated by analyzing with QuARS several industrial software NL requirement documents showing interesting results",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=992662,no,undetermined,0
Life cycle process knowledge - application during product design,"The demand for high quality, cost-effective and at the same time environmentally conscious products throughout the entire product life has led to a high complexity of the activities involved in the design phase More and more, design partners need to be integrated in the value chain, resulting in a tremendous increase in the scope of information captured during this phase. Only through measures taken at particular stages of the design process is it possible to cope with this situation. A current research project at Technical University Darmstadt, """"SFB 392. Design for Environment-Methods and Tools """", aims at supporting the product designer in minimizing environmental impacts of his products throughout their entire life cycle. A system environment enveloping several tools was developed, in order to assess environmental (and economic) aspects of a product during the design stage Consequences of design decisions on the product life cycle can also be viewed and manipulated, making it possible to forecast product life trends and influence them",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=992337,no,undetermined,0
Reactive objects,"Object-oriented, concurrent, and event-based programming models provide a natural framework in which to express the behavior of distributed and embedded software systems. However, contemporary programming languages still base their I/O primitives on a model in which the environment is assumed to be centrally controlled and synchronous, and interactions with the environment carried out through blocking subroutine calls. The gap between this view and the natural asynchrony of the real world has made event-based programming a complex and error-prone activity, despite recent focus on event-based frameworks and middleware. In thin paper we present a consistent model of event-based concurrency, centered around the notion of reactive objects. This model relieves the object-oriented paradigm from the idea of transparent blocking, and naturally enforces reactivity and state consistency We illustrate our point by a program example that offers substantial improvements in size and simplicity over a corresponding Java-based solution",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003682,no,undetermined,0
End-to-end latency of a fault-tolerant CORBA infrastructure,"This paper presents measured probability density functions (pdfs) for the end-to-end latency, of two-way, remote method invocations from a CORBA client to a replicated CORBA server in a fault-tolerance infrastructure. The infrastructure uses a multicast group-communication protocol based on a logical token-passing ring imposed on a single local-area network. The measurements show that the peaks of the pd/s for the latency are affected by the presence of duplicate messages for active replication, and by the position of the primary server replica on the ring for semi-active and passive replication. Because a node cannot broadcast a user message until it receives the token, up to two complete token rotations can contribute to the end-to-end latency seen by the client for synchronous remote method invocations, depending on the server processing time and the interval between two consecutive client invocations. For semi-active and passive replication, careful placement of the primary server replica is necessary to alleviate this broadcast delay to achieve the best possible end-to-end latency. The client invocation patterns and the server processing time must be considered together to determine the most favorable position for the primary replica. Assuming that an effective sending-side duplicate suppression mechanism is implemented, active replication can be more advantageous than semi-active and passive replication because all replicas compete for sending and, therefore, the replica at the most favorable position will have the opportunity to send first",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003697,no,undetermined,0
Enhancing Real-Time Event Service for synchronization in object oriented distributed systems,"Distributed object computing middleware such as CORBA, RMI, and DCOM have gained wide acceptance and has shielded programmers from many tedious and error-prone aspects of distributed programming. In particular, CORBA event service has been used extensively in embedded systems. We propose an aspect oriented approach to develop synchronization code for distributed systems that use event service as the underlying communication middleware. Our approach is to factor out synchronization as a separate aspect, synthesize synchronization code and then compose it with the functional code. We use high-level """"global invariants"""" to specify the synchronization policies which are then automatically translated into synchronization code for the underlying event service. To implement synchronization efficiently using the event service, we propose enhancements to the semantics of the event service. Specifically, we define the notion of condition events and exactly k semantics. Given these enhancements, we describe a synthesis procedure to translate global invariants into synchronization code based on events. We describe the implementation of the enhancements on the Tao's Real-Time Event Service. We present experimental results to demonstrate that the enhanced event service leads to more efficient implementation of synchronization. We feel that our methodology and the enhanced Real-Time Event Service will lead to more confident use of sophisticated synchronization policies in distributed object oriented systems",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003709,no,undetermined,0
Wait-free objects for real-time systems?,"The aim of this position paper is to promote the use of wait-free implementations for real-time shared objects. Such implementations allow the nonfaulty processes to progress despite the fact the other processes are slow, fast or have crashed. This is a noteworthy property for shared real-time objects. To assess its claim, the paper considers wait-free implementations of three objects: a renaming object, an efficient store/collect object, and a consensus object. On an other side, the paper can also be seen as an introductory survey, to wait-free protocols",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1003807,no,undetermined,0
Reliability and Safety Modeling of Fault Tolerant Control System,"This paper proposes a generalized approach of reliability and safety modeling for fault tolerant control system based on Markov model. The reliability and safety function, computed from the transition probability of the Markov process, provides a proper quantitative measure of the fault tolerant control system because it incorporates the deadline, failure detection and fault isolation, permanent and correlated fault. State transition diagram was established based on the state transition of the system. State transition equation could be obtained by state transition diagram. Different state probability diagrams were acquired with different parameters of failure rate, recovery rate from transient fault, failure detection rate and fault isolation rate.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722384,no,undetermined,0
Investigating the influence of inspector capability factors with four inspection techniques on inspection performance,"We report on a controlled experiment with over 170 student subjects to investigate the influence of inspection process, i.e., the defect detection technique applied, and inspector capability factors on the effectiveness and efficiency of inspections on individual and team level. The inspector capability factors include measures on the inspector's experience, as well as a pre-test with a mini-inspection. We use sampling to quantify the gain of defects detected from selecting the best inspectors according to the pre-test results compared to the performance of an average team of inspectors. Main findings are that inspector development and quality assurance capability and experience factors do not significantly distinguish inspector groups with different inspection performance. On the other hand the mini-inspection pre-test has considerable correlation to later inspection performance. The sampling of teams shows that selecting inspectors according to the mini-inspection pretest considerably improves average inspection effectiveness by up to one third.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011330,no,undetermined,0
Bayesian Network to Construct Interoperability Model of Open Source Software,"There are few topics more heated than the discussion surrounding open source software versus commercial and proprietary software. They are not only in an opposite relation, but also looking for cooperation. Moreover, there are many unresolved problems between them, in which the most typical one is the interoperability. There is a real need for a widely adopted, standardized method to assess the interoperability of open source software. However, few groups or researchers have given the guide up to now. This paper proposed Bayesian Network to construct the structure of interoperability and then learn the condition probability table of the structure. The structure and its condition probability table constitute the interoperability model. The model can be used not only to help user evaluate the interoperability of open source software, but also to guide the software developer to improve the quality of open source software more efficiently. An application showed how to use the model, and the result proved the validity of this model.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722453,no,undetermined,0
Software-based weighted random testing for IP cores in bus-based programmable SoCs,"Presents a software-based weighted random pattern scheme for testing delay faults in IP cores of programmable SoCs. We describe a method for determining static and transition probabilities (profiles) at the inputs of circuits with full-scan using testability metrics based on the targeted fault model, We use a genetic algorithm (GA) based search procedure to determine optimal profiles. We use these optimal profiles to generate a test program that runs on the processor core. This program applies test patterns to the target IP cores in the SoC and analyzes the test responses. This provides the flexibility of applying multiple profiles to the IP core under test to maximize fault coverage. This scheme does not incur the hardware overhead of logic BIST, since the pattern generation and analysis is done by software. We use a probabilistic approach to finding the profiles. We describe our method on transition and path-delay fault models, for both enhanced full-scan and normal full-scan circuits. We present experimental results using the ISCAS 89 benchmarks as IP cores.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1011125,no,undetermined,0
CASCADE - configurable and scalable DSP environment,"As the complexity of embedded systems grows rapidly, it is common to accelerate critical tasks with hardware. Designers usually use off-the-shelf components or licensed IP cores to shorten the time to market, but the hardware/software interfacing is tedious, error-prone and usually not portable. Besides, the existing hardware seldom matches the requirements perfectly, CASCADE, the proposed design environment as an alternative, generates coprocessing datapaths from the executing algorithms specified in C/C++ and attaches these datapaths to the embedded processor with an auto-generated software driver. The number of datapaths and their internal parallel functional units are scaled to fit the application. It seamlessly integrates the design tools of the embedded processor to reduce the re-training/design efforts and maintains short product development time as the pure software approaches. A JPEG encoder is built in CASCADE successfully with an auto-generated four-MAC accelerator to achieve 623% performance boost for our video application.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1010596,no,undetermined,0
The Portable PCB Fault Detector Based on ARM and Magnetic Image,"Traditional fault detector technique can hardly adapt to the modern electronic technique. This paper introduces how to set up military electronic equipmentspsila portable fault detector based on magnetic image, which can fast detect the electronic equipment in the scene. The detector has made use of ARM and uC/OS-II as the developing platform, taking MiniGUI as the figure interface.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722506,no,undetermined,0
Data mining technology for failure prognostic of avionics,Adverse environmental conditions have combined cumulative effects leading to performance degradation and failures of avionics. Classical reliability addresses statistically-generic devices and is less suitable for the situations when failures are not traced to manufacturing but rather to unique operational conditions of particular hardware units. An approach aimed at the accurate assessment of the probability of failure of any avionics unit utilizing the known history-of-abuse from environmental and operational factors is presented herein. The suggested prognostic model utilizes information downloaded from dedicated monitoring systems of flight-critical hardware and stored in a database. Such a database can be established from the laboratory testing of hardware and supplemented with real operational data. This approach results in a novel knowledge discovery from data technology that can be efficiently used in a wide area of applications and provide a quantitative basis for the modern maintenance concept known as service-when-needed. An illustrative numerical example is provided,2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008974,no,undetermined,0
Damaged Mechanism Research of RS232 Interface under Electromagnetic Pulse,"RS232 interface executes the role of Transportation and Communication, which has became the important interface between MCU of embedded system and peripheral equipment. Because RS232 mainly work in bottom of communication protocol, so it is important to protect the infrastructure of RS232. In test, pulse double electromagnetic is pulled into RS232 data transmission lines by coupling clamp, simulated differential-mode pulse voltage, basing on the test data, it will get the damaged mechanism of RS232 interface, meanwhile presenting the Logistic model of injected voltage pulse and probability of damage to the port interface, and getting the performance evaluation of RS232 interface, moreover estimating the voltage pulse range of upper and lower bounds at the port in the normal and damage state.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722537,no,undetermined,0
"Recognizing and responding to """"bad smells"""" in extreme programming","The agile software development process called Extreme Programming (XP) is a set of best practices which, when used, promises swifter delivery of quality software than one finds with more traditional methodologies. In this paper, we describe a large software development project that used a modified XP approach, identifying several unproductive practices that we detected over its two-year life that threatened the swifter project completion we had grown to expect. We have identified areas of trouble in the entire life cycle, including analysis, design, development, and testing. For each practice we identify, we discuss the solution we implemented to correct it and, more importantly, examine the early symptoms of those poor practices (""""bad smells"""") that project managers, analysts, and developers need to look out for in order to keep an XP project on its swifter track.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008006,no,undetermined,0
Experiences in assessing product family software architecture for evolution,"Software architecture assessments are a means to detect architectural problems before the bulk of development work is done. They facilitate planning of improvement activities early in the lifecycle and allow limiting the changes on any existing software. This is particularly beneficial when the architecture has been planned to (or already does) support a whole product family, or a set of products that share common requirements, architecture, components or code. As the family requirements evolve and new products are added, the need to assess the evolvability of the existing architecture is vital. The author illustrates two assessment case studies in the mobile telephone software domain: the Symbian operating system platform and the network resource access control software system. By means of simple experimental data, evidence is shown of the usefulness of architectural assessment as rated by the participating stakeholders. Both assessments have led to the identification of previously unknown architectural defects, and to the consequent planning of improvement initiatives. In both cases, stakeholders noted that a number of side benefits, including improvement of communication and architectural documentation, were also of considerable importance. The lessons learned and suggestions for future research and experimentation are outlined.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008003,no,undetermined,0
Goal-oriented software assessment,"Companies that engage in multi-site, multi-project software development continually face the problem of how to understand and improve their software development capabilities. We have defined and applied a goal-oriented process that enables such a company to assess the strengths and weaknesses of those capabilities. Our goals are to help (a) to decrease the time and cost to develop software, (b) to decrease the time needed to make changes to existing software, (c) to improve software quality, (d) to attract and retain a talented engineering staff, and (e) to facilitate more predictable management of software projects. In response to the variety of product requirements, market needs and development environments, we selected a goal-oriented process, rather than a criteria-oriented process, to advance our strategy and ensure relevance of the results. We describe the design of the process, discuss the results achieved and present vulnerabilities of the methodology. The process includes both interviews with projects' personnel and analysis of change data. Several common issues have emerged from the assessments across multiple projects, enabling strategic investments in software technology. Teams report satisfaction with the outcome in that they act on the recommendations, ask for additional future assessments, and recommend the process to sibling organizations.",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007970,no,undetermined,0
Research on Grouping Strategy in Series Course Projects of Software Engineering,"In order to address such problems as """"random grouping"""" and """"low consistency"""" when our students are conducting their series course projects of software engineering, """"stability factor"""" was put forward to evaluate the stability of a group and to assess the collaboration efficiency of the members in it. A Web-based MIS also was developed to help teachers do real-time supervision during the course projects. When complying with this framework, the quality of practical teaching can be assured, learning outcome will be enhanced, and the scientific encouraging policy will make a positive influence upon cultivating studentspsila team-working capability as well as their collaboration consciousness.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722869,no,undetermined,0
A note on current approaches to extending software engineering with fuzzy logic,"In this paper, we have attempted a study of current approaches carried out in the confluence of the two technologies: fuzzy set theory and software engineering, that could provide a powerful tool for requirements engineering, formal specifications, software quality prediction, object-oriented modeling, and etc. Various requirements analysis and specifications modeling technologies that utilize fuzzy theory are identified, and works related to the use of fuzzy logic for predicting software quality are also outlined",2002,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1006669,no,undetermined,0
Color Reproduction Quality Metric on Printing Images Based on the S-CIELAB Model,"The just-perceived color difference in a pair of printing images, the reproduction and its original, has been confirmed, subjectively by a paired-comparison psychological experiment and objectively by the S-CIELAB color quality metric. For one color image, a total number of 53 pairs of test images, simulating a number of varieties in C, M, Y, and K ink amounts, were produced, and determined their corresponding color difference recognizing probabilities by visual paired-comparison. Also, the image color difference Delta Es values, presented in the S-CIELAB model, for each pairs of images were calculated and correlated with their color difference recognizing probability. The results showed that the just-perceived image color difference Delta Es, when 0.9 color difference recognizing probability being considered as the just-perceived level, was about 1.4 Delta Eab units for the experiment image, being the image color fidelity threshold parameter.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723255,no,undetermined,0
Methodology of the Correct Plate-Making to Keep Consistence of Tone Reproduction,"The process of offset plate copy is controlled through the correct reproduction at the highest and the lowest tone. We propose that we should control the whole tone, and we need the correct control method. Based on the offset control strap, this paper proposes a new detected method of plate resolution and analytic method of the correct copy range of plate. Using these methods and through the experiment, we analyze the resolution and the correct copy range of three types of offset plates. By working in the correct copy range, we can guarantee the consistence of tone reproduction of different types of plates. This is the precondition for color consistence in color printing and convenient to adjust the press.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723278,no,undetermined,0
Practical automated filter generation to explicitly enforce implicit input assumptions,"Vulnerabilities in distributed applications are being uncovered and exploited faster than software engineers can, patch the security holes. All too often these weaknesses result from implicit assumptions made by an application about its inputs. One approach to defending against their exploitation is to interpose a filter between the input source and the application that verifies that the application's assumptions about its inputs actually hold. However, ad hoc design of such filters is nearly as tedious and error-prone as patching the original application itself. We have automated the filter generation process based on a simple formal description of a broad class of assumptions about the inputs to an application. Focusing on the back-end server application case, we have prototyped an easy-to-use tool that generates server-side filtering scripts. These can then be quickly installed on a front-end webs server (either in concert with the application or., when a vulnerability is uncovered), thus shielding the server application from a variety of existing and exploited, attacks, as solutions requiring changes to the applications are developed and tested. Our measurements suggest that input filtering can be done efficiently and should not be a performance concern for moderately loaded web servers. The overall approach may be generalizable to other domains, such as firewall filter generation and API wrapper filter generation.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991551,no,undetermined,0
Information flow analysis of component-structured applications,"Software component technology facilitates the cost-effective development of specialized applications. Nevertheless, due to the high number of principals involved in a component-structured system, it introduces special security problems which have to be tackled by a thorough security analysis. In particular the diversity and complexity of information flows between components hold the danger of leaking information. Since information flow analysis, however, tends to be expensive and error-prone, we apply our object-oriented security analysis and modeling approach. It employs UML-based object-oriented modeling techniques and graph rewriting in order to make the analysis easier and to assure its quality even for large systems. Information flow is modeled based on the decentralized label model (Myers and Liskov, 1997) combining label-based read access policy models and declassification of information with static analysis. We report on the principles of information flow analysis of component-based systems, clarify its application by means of an example, and outline the corresponding tool-support.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991520,no,undetermined,0
Mixture of principal axes registration for change analysis in computer-aided diagnosis,"Non-rigid image registration is a prerequisite for many medical image analysis applications, such as image fusion of multi-modality images and quantitative change analysis of a temporal sequence in computer-aided diagnosis. By establishing the point correspondence of the extracted feature points, it is possible to recover the deformation using nonlinear interpolation methods such as the thin-plate-spline approach. However, it is a difficulty task to establish an exact point correspondence due to the high complexity of the nonlinear deformation existing in medical images. In this paper, a mixture of principal axes registration (mPAR) method is proposed to resolve the correspondence problem through a neural computational approach. The novel feature of mPAR is to align two point sets without needing to establish an explicit point correspondence. Instead, it aligns the two point sets by minimizing the relative entropy between their probability distributions, resulting in a maximum likelihood estimate of the transformation matrix. The registration process consists of: (1) a finite mixture scheme to establish an improved point correspondence and (2) a multilayer perceptron (MLP) neural network to recover the nonlinear deformation. The neural computation for registration used a committee machine to obtain a mixture of piecewise rigid registrations, which gives a reliable point correspondence using multiple extracted objects in a finite mixture scheme. Then the MLP was used to determine the coefficients of a polynomial transform using extracted cross-points of elongated structures as control points. We have applied our mPAR method to a temporal sequence of mammograms from a single patient. The experimental results show that mPAR not only improves the accuracy of the point correspondence but also results in a desirable error-resilience property for control point selection errors",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=991199,no,undetermined,0
Efficient deadlock analysis of clients/server systems with two-way communication,"Deadlocks are a common type of fault in distributed programs. To detect deadlocks in a distributed program P, one approach is to construct the reachability graph (RG) of P, which contains all possible states of P. Since the size of RG(P) is an exponential function of the number of processes in P, the use of RGs for deadlock detection has limited success. The authors present an efficient technique for deadlock analysis of client/server programs with two-way communication, where the server and clients communicate through channels supporting synchronous message-passing. We consider client/server programs in which the server saves the IDs of some clients for future communication. For such a program, we describe how to construct its abstract client/server reachability graph (ACSRG), which contains a significantly smaller number of global states than the corresponding RG. One example is that for a solution to the gas station problem with one pump and six customers, its RG has 25394 states and its ACSRG 74 states. We show that the use of ACSRGs not only greatly reduces the effort for deadlock analysis but also provides a basis for proving freedom from deadlocks for any number of clients.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989476,no,undetermined,0
Analysis of hypergeometric distribution software reliability model,"The article gives detailed mathematical results on the hypergeometric distribution software reliability model (HGDSRM) proposed by Y. Tohma et al. (1989; 1991). In the above papers, Tohma et al. developed the HGDSRM as a discrete-time stochastic model and derived a recursive formula for the mean cumulative number of software faults detected up to the i-th (>0) test instance in testing phase. Since their model is based on only the mean value of the cumulative number of faults, it is impossible to estimate not only the software reliability but also the other probabilistic dependability measures. We introduce the concept of cumulative trial processes, and describe the dynamic behavior of the HGDSRM exactly. In particular, we derive the probability mass function of the number of software faults detected newly at the i-th test instance and its mean as well as the software reliability defined as the probability that no faults are detected up to an arbitrary time. In numerical examples with real software failure data, we compare several HGDSRMs with different model parameters in terms of least squared sum and show that the mathematical results obtained here are very useful to assess the software reliability with the HGDSRM.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989470,no,undetermined,0
Fault tolerant distributed information systems,"Critical infrastructures provide services upon which society depends heavily; these applications are themselves dependent on distributed information systems for all aspects of their operation and so survivability of the information systems is an important issue. Fault tolerance is a mechanism by which survivability can be achieved in these information systems. We outline a specification-based approach to fault tolerance, called RAPTOR, that enables structuring of fault-tolerance specifications and an implementation partially, synthesized from the formal specification. The RAPTOR approach uses three specifications describing the fault-tolerant system, the errors to be detected, and the actions to take to recover from those errors. System specification utilizes an object-oriented database to store the descriptions associated with these large, complex systems. The error detection and recovery specifications are defined using the formal specification notation Z. We also describe an implementation architecture and explore our solution with a case study.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989466,no,undetermined,0
A fault model for subtype inheritance and polymorphism,"Although program faults are widely studied, there are many aspects of faults that we still do not understand, particularly about OO software. In addition to the simple fact that one important goal during testing is to cause failures and thereby detect faults, a full understanding of the characteristics of faults is crucial to several research areas. The power that inheritance and polymorphism brings to the expressiveness of programming languages also brings a number of new anomalies and fault types. This paper presents a model for the appearance and realization of OO faults and defines and discusses specific categories of inheritance and polymorphic faults. The model and categories can be used to support empirical investigations of object-oriented testing techniques, to inspire further research into object-oriented testing and analysis, and to help improve design and development of object-oriented software.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989461,no,undetermined,0
PRASE: An Approach for Program Reliability Analysis with Soft Errors,"Soft errors are emerging as a new challenge in computer applications. Current studies about soft errors mainly focus on the circuit and architecture level. Few works discuss the impact of soft errors on programs. This paper presents a novel approach named PRASE, which can analyze the reliability of a program with the effect of soft errors. Based on the simple probability theory and the corresponding assembly code of a program, we propose two models for analyzing the probabilities about error generation and error propagation. The analytical performance is increased significantly with the help of basic block analysis. The programAs reliability is determined according to its actual execution paths. We propose a factor named PVF (program vulnerability factor), which represents the characteristic of programAs vulnerability in the presence of soft errors. The experimental results show that the reliability of a program has a connection with its structure. Comparing with the traditional fault injection techniques, PRASE has the advantage of faster speed and lower price with more general results.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725302,no,undetermined,0
A Bayesian approach to reliability prediction and assessment of component based systems,"It is generally believed that component-based software development leads to improved application quality, maintainability and reliability. However most software reliability techniques model integrated systems. These models disregard system's internal structure, taking into account only the failure data and interactions with the environment. We propose a novel approach to reliability analysis of component-based systems. Reliability prediction algorithm allows system architects to analyze reliability of the system before it is built, taking into account component reliability estimates and their anticipated usage. Fully integrated with the UML, this step can guide the process of identifying critical components and analyze the effect of replacing them with the more/less reliable ones. Reliability assessment algorithm, applicable in the system test phase, utilizes these reliability predictions as prior probabilities. In the Bayesian estimation. framework, posterior probability of failure is calculated from the priors and test failure data.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989454,no,undetermined,0
An approach to higher reliability using software components,"The general belief that component reuse improves software reliability is based on the assumption that the prior usage has exposed the potential software faults. In reality, this is not necessarily true due to the inherent differences in the environments and usage of the component. To achieve a high reliability for a component-based software system, we need reliable components that interoperate properly in the new environment. In this paper, we present a unified approach to do an evaluation of the interoperablity of components. This involves a generic and systematic capture of the component behavior that expresses the various assumptions made by the designers about components and their interconnections explicitly. With the information captured at a semantic level, this approach can detect potential mismatches between components in the new environment and give guidance on how to resolve the mismatches to fit components in the new context. The capture of this information in an appropriate format and an automated analysis can show serious exposures to reliability in a component-based system, before it is integrated.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989453,no,undetermined,0
A controlled experiment in maintenance: comparing design patterns to simpler solutions,"Software design patterns package proven solutions to recurring design problems in a form that simplifies reuse. We are seeking empirical evidence whether using design patterns is beneficial. In particular, one may prefer using a design pattern even if the actual design problem is simpler than that solved by the pattern, i.e., if not all of the functionality offered by the pattern is actually required. Our experiment investigates software maintenance scenarios that employ various design patterns and compares designs with patterns to simpler alternatives. The subjects were professional software engineers. In most of our nine maintenance tasks, we found positive effects from using a design pattern: either its inherent additional flexibility was achieved without requiring more maintenance time or maintenance time was reduced compared to the simpler alternative. In a few cases, we found negative effects: the alternative solution was less error-prone or required less maintenance time. Overall, we conclude that, unless there is a clear reason to prefer the simpler solution, it is probably wise to choose the flexibility provided by the design pattern because unexpected new requirements often appear. We identify several questions for future empirical research",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988711,no,undetermined,0
Reliability estimation for a software system with sequential independent reviews,"Suppose that several sequential test and correction cycles have been completed for the purpose of improving the reliability of a given software system. One way to quantify the success of these efforts is to estimate the probability that all faults are found by the end of the last cycle, We describe how to evaluate this probability both prior to and after observing the numbers of faults detected in each cycle and we show when these two evaluations would be the same",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988707,no,undetermined,0
Development of a dynamic power system load model,"The paper addresses the issue of measurement based power system load model development. The majority of power system loads respond dynamically to voltage disturbances and such contribute to overall system dynamics. Induction motors represent a major portion of system loads that exhibit dynamic behaviour following the disturbance. In this paper, the dynamic behaviours of an induction motor and a combination of induction motor and static load were investigated under different disturbances and operating conditions in the laboratory. A first order generic dynamic, load model is developed based on the test results. The model proposed is in a transfer function form and it is suitable for direct inclusion in the existing power system stability software. The robustness of the proposed model is also assessed.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988434,no,undetermined,0
"RHIC insertion region, shunt power supply current errors",The Relativistic Heavy Ion Collider (RHIC) was commissioned in 1999 and 2000. RHIC requires power supplies to supply currents to highly inductive superconducting magnets. The RHIC Insertion Region contains many shunt power supplies to trim the current of different magnet elements in a large superconducting magnet circuit. Power Supply current error measurements were performed during the commissioning of RHIC. Models of these power supply systems were produced to predict and improve these power supply current errors using the circuit analysis program MicroCap V by Spectrum Software (TM). Results of the power supply current errors are presented from the models and from the measurements performed during the commissioning of RHIC,2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988211,no,undetermined,0
Mobile location by time advance for GSM,"A method that employs the time advance for locating the position of mobile phone is proposed. The advantage of the proposed technique is that it can work for the current system, such as GSM, without any change in hardware equipment. In this paper, we first add the software into the mobile phone for detecting the time advance, power intensity and quality factor and so on. Then, these parameters can be used to estimate the position of mobile handset. This raw data will be transmitted to the Operator Maintenance Center (OMC) for further processing and applications. The experimental results show that the proposed method can provide an accuracy position to trace the mobile phone in the GSM (Global system for Mobile).",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=985513,no,undetermined,0
Training Security Assurance Teams Using Vulnerability Injection,"Writing secure Web applications is a complex task. In fact, a vast majority of Web applications are likely to have security vulnerabilities that can be exploited using simple tools like a common Web browser. This represents a great danger as the attacks may have disastrous consequences to organizations, harming their assets and reputation. To mitigate these vulnerabilities, security code inspections and penetration tests must be conducted by well-trained teams during the development of the application. However, effective code inspections and testing takes time and cost a lot of money, even before any business revenue. Furthermore, software quality assurance teams typically lack the knowledge required to effectively detect security problems. In this paper we propose an approach to quickly and effectively train security assurance teams in the context of web application development. The approach combines a novel vulnerability injection technique with relevant guidance information about the most common security vulnerabilities to provide a realistic training scenario. Our experimental results show that a short training period is sufficient to clearly improve the ability of security assurance teams to detect vulnerabilities during both code inspections and penetration tests.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725309,no,undetermined,0
Mathematical foundations of minimal cutsets,"Since their introduction in the reliability field, binary decision diagrams have proved to be the most efficient tool to assess Boolean models such as fault trees. Their success increases the need of sound mathematical foundations for the notions that are involved in reliability and dependability studies. This paper clarifies the mathematical status of the notion of minimal cutsets which have a central role in fault-tree assessment. Algorithmic issues are discussed. Minimal cutsets are distinct from prime implicants and they have a great interest from both a computation complexity and practical viewpoint. Implementation of BDD algorithms is explained. All of these algorithms are implemented in the Aralia software, which is widely used. These algorithms and their mathematical foundations were designed to assess efficiently a very large noncoherent fault tree that models the emergency shutdown system of a nuclear reactor",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=983400,no,undetermined,0
Abstracting from failure probabilities,"In fault-tolerant computing, dependability of systems is usually demonstrated by abstracting from failure probabilities (under simplifying assumptions on failure occurrences). In the specification framework Focus, we show under which conditions and to which extent this is sound: We use a specification language that is interpreted in the usual abstract model and in a probabilistic model. We give probability bounds showing the degree of faithfulness of the abstract model wrt. the probabilistic one. These include cases where the usual assumptions are not fulfilled",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981764,no,undetermined,0
Managing the Life-cycle of Industrial Automation Systems with Product Line Variability Models,"The current trend towards component-based software architectures has also influenced the development of industrial automation systems (IAS). Despite many advances, the life-cycle management of large-scale, component-based IAS still remains a big challenge. The knowledge required for the maintenance and runtime reconfiguration is often tacit and relies on individual stakeholders' capabilities - an error-prone and risky strategy in safety critical environments. This paper presents an approach based on product line variability models to manage the lifecycle of IAS and to automate the maintenance and reconfiguration process. We complement the standard IEC 61499 with a variability modeling approach to support both initial deployment and runtime reconfiguration. We illustrate the automated model-based life-cycle management and maintenance process using sample IAS usage scenarios.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725703,no,undetermined,0
Bayesian Inference Approach for Probabilistic Analogy Based Software Maintenance Effort Estimation,"Software maintenance effort estimation is essential for the success of software maintenance process. In the past decades, many methods have been proposed for maintenance effort estimation. However, most existing estimation methods only produce point predictions. Due to the inherent uncertainties and complexities in the maintenance process, the accurate point estimates are often obtained with great difficulties. Therefore some prior studies have been focusing on probabilistic predictions. Analogy Based Estimation (ABE) is one popular point estimation technique. This method is widely accepted due to its conceptual simplicity and empirical competitiveness. However, there is still a lack of probabilistic framework for ABE model. In this study, we first propose a probabilistic framework of ABE (PABE). The predictive PABE is obtained by integrating over its parameter k number of nearest neighbors via Bayesian inference. In addition, PABE is validated on four maintenance datasets with comparisons against other established effort estimation techniques. The promising results show that PABE could largely improve the point estimations of ABE and achieve quality probabilistic predictions.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725294,no,undetermined,0
Feedback control of the software test process through measurements of software reliability,"A closed-loop feedback control model of the software test process (STP) is described. The model is grounded in the well established theory of automatic control. It offers a formal and novel procedure for using product reliability or failure intensity as a basis for closed loop control of the STP. The reliability or the failure intensity of the product is compared against the desired reliability at each checkpoint and the difference fed back to a controller. The controller uses this difference to compute changes necessary in the process parameters to meet the reliability, or failure intensity objective at the terminal checkpoint (the deadline). The STP continues beyond a checkpoint with a revised set of parameters. This procedure is repeated at each checkpoint until the termination of the STP. The procedure accounts for the possibility of changes (during testing), in reliability or failure intensity objective, the checkpoints, and the parameters that characterize the STP. The effectiveness of this procedure was studied using commercial data available in the public domain and also from the data generated through simulation. In all cases, the use of feedback control produces adequate results allowing the achievement of the objectives.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989477,no,undetermined,0
Quality-assuring scheduling-using stochastic behavior to improve resource utilization,"We present a unified model for admission and scheduling, applicable for various active resources such as CPU or disk to assure a requested quality in situations of temporary overload. The model allows us to predict and control the behavior of applications based on given quality requirements. It uses the variations in the execution time, i.e., the time any active resource is needed We split resource requirements into a mandatory part which must be available and an optional part which should be available as often as possible but at least with a certain percentage. In combination with a given distribution for the execution time we can move away from worst-case reservations and drastically reduce the amount of reserved resources for applications which can tolerate occasional deadline misses. This increases the number of admittable applications. For example, with negligible loss of quality our system can admit more than two times the disk bandwidth than a system based on the worst-case. Finally, we validated the predictions of our model by measurements using a prototype real-time system and observed a high accuracy between predicted and measured values.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990603,no,undetermined,0
An empirical evaluation of statistical testing designed from UML state diagrams: the flight guidance system case study,"This paper presents an empirical study of the effectiveness of test cases generated from UML state diagrams using transition coverage as the testing criterion. The test cases production is mainly based on an adaptation of a probabilistic method, called statistical testing based on testing criteria. This technique was automated with the aid of the Rational Software Corporation's Rose RealTime tool. The test strategy investigated combines statistical test cases with (few) deterministic test cases focused on domain boundary values. Its feasibility is exemplified on a research version of an avionics system implemented in Java: the Flight Guidance System case study (14 concurrent state diagrams). Then, the results of an empirical evaluation of the effectiveness of the created test cases are presented. The evaluation was performed using mutation analysis to assess the error detection power of the test cases on more than 1500 faults seeded one by one in the Java source code (115 classes, 6500 LOC). A detailed analysis of the test results allows us to draw first conclusions on the expected strengths and weaknesses of the proposed test strategy.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989479,no,undetermined,0
Dimension recognition and geometry reconstruction in vectorization of engineering drawings,"This paper presents a novel approach for recognizing and interpreting dimensions in engineering drawings. It starts by detecting potential dimension frames, each comprising only the line and text components of a dimension, then verifies them by detecting the dimension symbols. By removing the prerequisite of symbol recognition from detection of dimension sets, our method is capable of handling low quality drawings. We also propose a reconstruction algorithm for rebuilding the drawing entities based on the recognized dimension annotations. A coordinate grid structure is introduced to represent and analyze two-dimensional spatial constraints between entities; this simplifies and unifies the process of rectifying deviations of entity dimensions induced during scanning and vectorization.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990545,no,undetermined,0
The case for resilient overlay networks,"This paper makes the case for Resilient Overlay Networks (RONs), an application-level routing and packet forwarding service that gives end-hosts and applications the ability to take advantage of network paths that traditional Internet routing cannot make use of, thereby improving their end-to-end reliability and performance. Using RON, nodes participating in a distributed Internet application configure themselves into an overlay network and cooperatively forward packets for each other. Each RON node monitors the quality of the links in the underlying Internet and propagates this information to the other nodes; this enables a RON to detect and react to path failures within several seconds rather than several minutes, and allows it to select application-specific paths based on performance. We argue that RON has the potential to substantially improve the resilience of distributed Internet applications to path outages and sustained overload.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990076,no,undetermined,0
Self-tuned remote execution for pervasive computing,"Pervasive computing creates environments saturated with computing and communication capability, yet gracefully integrated with human users. Remote execution has a natural role to play, in such environments, since it lets applications simultaneously leverage the mobility of small devices and the greater resources of large devices. In this paper, we describe Spectra, a remote execution system designed for pervasive environments. Spectra monitors resources such as battery, energy and file cache state which are especially important for mobile clients. It also dynamically balances energy use and quality goals with traditional performance concerns to decide where to locate functionality. Finally, Spectra is self-tuning-it does not require applications to explicitly specify intended resource usage. Instead, it monitors application behavior, learns functions predicting their resource usage, and uses the information to anticipate future behavior.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990062,no,undetermined,0
Using abstraction to improve fault tolerance,"Software errors are a major cause of outages and they are increasingly exploited in malicious attacks. Byzantine fault tolerance allows replicated systems to mask some software errors but it is expensive to deploy. The paper describes a replication technique, BFTA, which uses abstraction to reduce the cost of Byzantine fault tolerance and to improve its ability to mask software errors. BFTA reduces cost because it enables reuse of off-the-shelf service implementations. It improves availability because each replica can be repaired periodically using an abstract view of the state stored by correct replicas, and because each replica can run distinct or non-deterministic service implementations, which reduces the probability of common mode failures. We built an NFS service that allows each replica to run a different operating system. This example suggests that BFTA can be used in practice; the replicated file system required only a modest amount of new code, and preliminary performance results indicate that it performs comparably to the off-the-shelf implementations that it wraps.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990057,no,undetermined,0
QUIM: a framework for quantifying usability metrics in software quality models,"The paper examines current approaches to usability metrics and proposes a new approach for quantifying software quality in use, based on modelling the dynamic relationships of the attributes that affect software usability. The Quality in Use Integrated Map (QUIM) is proposed for specifying and identifying quality in use components, which brings together different factors, criteria, metrics and data defined in different human computer interface and software engineering models. The Graphical Dynamic Quality Assessment (GDQA) model is used to analyse interaction of these components into a systematic structure. The paper first introduces a new classification scheme into a graphical logic based framework using QUIM components (factors, criteria metrics and data) to assess quality in use of interactive systems. Then we illustrate how QUIM and GDQA may be used to assess software usability using subjective measures of quality characteristics as defined in ISO/IEC 9126",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990036,no,undetermined,0
A new tool to analyze ER-schemas,"Cardinality constraints as well as key constraints and functional dependencies are among the most popular classes of constraints in database models. While each constraint class is now well understood, little is done about their interaction. Today, cardinality constraints and key constraints are embedded in most CASE tools, which are usually based on the entity-relationship model. However, these tools do not offer intelligent consistency checking routines for cardinality constraints and they do not consider the global coherence. Conflicts among the constraints are not detected. Our aim is then, to propose a tool for reasoning about a set of cardinality constraints, key and certain functional dependencies in order to help in database design. We treat the global coherence of cardinality constraints. We propose two steps: a syntactical analysis according to our ER Meta-schema and a semantic analysis in order to verify the cardinality constraints and their interactions",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990035,no,undetermined,0
Orderly Random Testing for Both Hardware and Software,"Based on random testing, this paper introduces a new concept of orderly random testing for both hardware and software systems. Random testing, having been employed for years, seems to be inefficient for its random selection of test patterns. Therefore, a new concept of pre-determined distance among test vectors is proposed in the paper to make it more effective in testing. The idea is based on the fact that the larger the distance between two adjacent test vectors in a test sequence, the more the faults will be detected by the test vectors. Procedure of constructing such a testing sequence is presented in detail. The new approach has shown its remarkable advantage of fitting in with both hardware and software testing. Experimental results and mathematical analysis are also given to evaluate the performances of the novel method.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725292,no,undetermined,0
On prediction of cost and duration for risky software projects based on risk questionnaire,"The paper proposes a new approach that can discriminate risky software development projects from smoothly or satisfactorily going projects and give an explanation for the risk. We have already developed a logistic regression model which predicts whether a project becomes risky or not (O. Mizuno et al., 2000). However, the model returned the decision with a calculated probability only. Additionally, a formula was constructed based on the risk questionnaire which includes 23 questions. We therefore try to improve the previous method with respect to accountability and feasibility. In the new approach, we firstly construct a new risk questionnaire including only 9 questions (or risk factors), each of which is concerned with project management. We then apply multiple regression analysis to the actual project data, and clarify a set of factors which contributes essentially to estimate the relative cost error and the relative duration error, respectively. We then apply the constructed formulas to another set of project data. The analysis results show that both the cost and duration of risky projects are estimated fairly well by the formulas. We can thus confirm that our new approach is applicable to software development projects in order to discriminate risky projects from appropriate projects and give reasonable explanations for the risk",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990010,no,undetermined,0
Assurance of conceptual data model quality based on early measures,"The increasing demand for quality information systems (IS), has become quality the most pressing challenge facing IS development organisations. In the IS development field it is generally accepted that the quality of an IS is highly dependent on decisions made early in its development. Given the relevant role that data itself plays in an IS, conceptual data models are a key artifact of the IS design: Therefore, in order to build """"better quality """" IS it is necessary to assess and to improve the quality of conceptual data models based on quantitative criteria. It is in this context where software measurement can help IS designers to make better decision during design activities. We focus this work on the empirical validation of the metrics proposed by Genero et al. for measuring the structural complexity of entity relationship diagrams (ERDs). Through a controlled experiment we will demonstrate that these metrics seem to be heavily correlated with three of the sub-factors that characterise the maintainability of an ERD, such as understandability, analysability and modifiability",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990007,no,undetermined,0
Exception analysis for multithreaded Java programs,"This paper presents a static analysis that estimates uncaught exceptions in multithreaded Java programs. In Java, throwing exceptions across threads is deprecated because of the safely problem. Instead of restricting programmers' freedom, we extend the Java language to support multithreaded exception handling and propose a tool to detect uncaught exceptions in the input programs. Our analysis consists of two steps. The analysis firstly, estimates concurrently evaluated expressions of the multithreads in Java programs by the synchronization relation among the threads. Using this concurrency information, the program's exception flow is derived as set-constraints, whose least model is our analysis result. Both of these two steps are proved safe",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989998,no,undetermined,0
Analysis and implementation method of program to detect inappropriate information leak,"For a program which handles secret information, it is very important to prevent inappropriate information leaks from a program with secret data. D.E. Denning (1976) proposed a mechanism to certify the security of program by statically analyzing information flow, and S. Kuninobu et al. (2000) proposed a more practical analysis framework including recursive procedure handling, although no implementation has been yet made. We propose a method of security analysis implementation, and show a security analysis tool implemented for a procedural language. We extend Kuninobu's algorithm by devising various techniques for analysis of practical programs that have recursive calls and global variables. This method is validated by applying our tools to a simple credit card program, and we confirm that the validation of program security is very useful",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989996,no,undetermined,0
Automated conversion from a requirements document to an executable formal specification,"Many formal specification languages have been developed to engineer complex systems. However natural language (NL) has remained the choice of domain experts to specify the system because formal specification languages are not easy to master. Therefore NL requirements documentation must be reinterpreted by software engineers into a formal specification language. When the system is very complicated, which is mostly the case when one chooses to use formal specification, this conversion is both non-trivial and error-prone, if not implausible. This challenge comes from many factors such as miscommunication between domain experts and engineers. However the major bottleneck of this conversion is from the inborn characteristic of ambiguity of NL and the different level of the formalism between the two domains of NL and the formal specification. This is why there have been very few attempts to automate the conversion from requirements documentation to a formal specification language. This research project is developed as an application of formal specification and linguistic techniques to automate the conversion from a requirements document written in NL to a formal specification language. Contextual Natural Language Processing (CNLP) is used to handle the ambiguity problem in NL and Two Level Grammar (TLG) is used to deal with the different formalism level between NL and formal specification languages to achieve automated conversion from NL requirements documentation into a formal specification (in our case the Vienna Development Method - VDM++). A knowledge base is built from the NL requirements documentation using CNLP by parsing the documentation and storing the syntactic, semantic, and contextual information.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989850,no,undetermined,0
"AGATE, access graph based tools for handling encapsulation","Encapsulation and modularity are supported by various static access control mechanisms that manage implementation hiding and define interfaces adapted to different client profiles. Programming languages use numerous and very different mechanisms, the cumulative application of which is sometimes confusing and hard to predict. Furthermore, understanding and reasoning about access control independently from the programming languages is quite difficult. Tools based on a language-independent model of access control are presented to address these issues. These tools support access control handling via visualisation of access, checking of design requirements on access and source code generation. We believe in the contribution of such tools for improving understanding and enhancing use of access control from design to implementation.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989818,no,undetermined,0
Connectors synthesis for deadlock-free component based architectures,"Nowadays component-based technologies offer straightforward ways of building applications from existing components. Although these technologies might differ in terms of the level of heterogeneity among components they support, e.g. CORBA or COM versus J2EE, they all suffer the problem of dynamic integration. That is, once components are successfully integrated in a uniform context how is it possible to check, control and assess that the dynamic behavior of the resulting application will not deadlock? The authors propose an architectural, connector-based approach to this problem. We compose a system in such a way that it is possible to check whether and why the system deadlocks. Depending on the kind of deadlock, we have a strategy that automatically operates on the connector part of the system architecture in order to obtain a suitably equivalent version of the system which is deadlock-free.",2001,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989803,no,undetermined,0
A Study of Modified Testing-Based Fault Localization Method,"In software development and maintenance, locating faults is generally a complex and time-consuming process. In order to effectively identify the locations of program faults, several approaches have been proposed. Similarity-aware fault localization (SAFL) is a testing-based fault localization method that utilizes testing information to calculate the suspicion probability of each statement. Dicing is also another method that we have used. In this paper, our proposed method focuses on predicates and their influence, instead of on statements in traditional SAFL. In our method, fuzzy theory, matrix calculating, and some probability are used. Our method detects the importance of each predicate and then provides more test data for programmers to analyze the fault locations. Furthermore, programmers will also gain some important information about the program in order to maintain their program accordingly. In order to speed up the efficiency, we also simplified the program. We performed an experimental study for several programs, together with another two testing-based fault localization (TBFL) approaches. These three methods were discussed in terms of different criteria such as line of code and suspicious code coverage. The experimental results show that the proposed method from our study can decrease the number of codes which have more probability of suspicion than real bugs.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725293,no,undetermined,0
Test-adequacy and statistical testing: combining different properties of a test-set,"Dependability assessment of safety-critical or safety-related software components is an important issue for example within the nuclear industry, the avionics sector or the military. Statistical testing is one way of quantifying the dependability of a given software product. The use of sector-specific standards with their suggested test-criteria is another (nonquantitative) way of aiming at employing only components that are """"dependable enough"""". Ideally, both, the acknowledged test criteria and statistical test methods should come into play when assessing software dependability. We want to - in the long-term - move towards this aim. Thus we investigate in this paper a model to combine the fault-detection power of a given test-set (a test-adequacy criterion) with the statistical power of the test-set, i.e. the number of statistical tests within the test-set. With this model we aim at drawing out of any given test-set - whether devised by a plant engineer or a statistician - the overall contribution it can make to dependability assessment.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383115,no,undetermined,0
Smooth ergodic hidden Markov model and its applications in text to speech systems,"In text-to-speech systems, the accuracy of information extraction from text is crucial in producing high quality synthesized speech. In this paper, a new scheme for converting text into its equivalent phonetic spelling is proposed and developed. This method has many advantages over its predecessors and it can complement many other text to speech converting systems in order to get improved performance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1434043,no,undetermined,0
"Towards a unified approach to the representation of, and reasoning with, probabilistic risk information about software and its system interface","Early risk assessment is key in planning the development of systems, including systems that involve software. Such risk assessment needs a combination of the following elements; 1) Severity estimates for the potential effects of failures, and likelihood estimates for their causes; 2) Fault trees that link causes to failures; 3) Efficacy estimates of design and process steps towards reducing risk; 4) Distinctions between preventing, alleviating and detecting (thereafter removing), risks; 5) Risk preventions that have potential side effects of themselves introducing risks. The paper shows a unified approach that accommodates all these elements. The approach combines fault trees (from probabilistic risk assessment methods) with explicit treatment of risk mitigations (a generalization of the notion of a """"detection"""" seen in FMECA analyses). Fault trees capture the causal relationships by which failure mechanisms may combine to lead to failure modes. Risk mitigations encompass (and distinguish among) options to prevent risks, detect risks, and alleviate risks (i.e., decrease their impact should they occur). This approach has been embodied in extensions to a JPL-developed risk assessment tool, and is illustrated here on software risk assessment information drawn from an actual project's software system FMECA (failure modes, effects and criticality analysis). Since its elements are typical of risk assessment of software and its system interface, the findings should be relevant to a wide range of software systems.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383134,no,undetermined,0
Program Model Checking Using Design-for-Verification: NASA Flight Software Case Study,"Model checking is a verification technique developed in the 1980s that has a history of industrial application in hardware verification and verification of communications protocol specifications. Program model checking is a technique for model checking software in which the program itself is the model to be checked. Program model checking has shown potential for detecting software defects that are extremely difficult to detect through traditional testing. The technique has been the subject of research and relatively small-scale applications but faces several barriers to wider deployment. This paper is a report on continuing work applying Java PathFinder (JPF), a program model checker developed at NASA Ames Research Center, to the shuttle abort flight management system, a situational awareness application originally developed for the space shuttle. The paper provides background on the model checking tools that were used and the target application, and then focuses on the application of a """"design for verification"""" (D4V) principle and its effect on model checking. The case study helps validate the applicability of program model checking technology to real NASA flight software. A related conclusion is that application of D4V principles can increase the efficiency of model checking in detecting subtle software defects. The paper is oriented toward software engineering technology transfer personnel and software practitioners considering introducing program model checking technology into their organizations.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4161597,no,undetermined,0
Algorithmic Differentiation: Application to Variational Problems in Computer Vision,"Many vision problems can be formulated as minimization of appropriate energy functionals. These energy functionals are usually minimized, based on the calculus of variations (Euler-Lagrange equation). Once the Euler-Lagrange equation has been determined, it needs to be discretized in order to implement it on a digital computer. This is not a trivial task and, is moreover, error- prone. In this paper, we propose a flexible alternative. We discretize the energy functional and, subsequently, apply the mathematical concept of algorithmic differentiation to directly derive algorithms that implement the energy functional's derivatives. This approach has several advantages: First, the computed derivatives are exact with respect to the implementation of the energy functional. Second, it is basically straightforward to compute second-order derivatives and, thus, the Hessian matrix of the energy functional. Third, algorithmic differentiation is a process which can be automated. We demonstrate this novel approach on three representative vision problems (namely, denoising, segmentation, and stereo) and show that state-of-the-art results are obtained with little effort.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4204161,no,undetermined,0
Accurate Software-Related Average Current Drain Measurements in Embedded Systems,"Performing accurate average current drain measurements of digital programmable components (e.g., microcontrollers, digital signal processors, System-on-Chip, or wireless modules) is a critical and error-prone measurement problem for embedded system manufacturers due to the impulsive time-varying behavior of the current waveforms drawn from a battery in real operating conditions. In this paper, the uncertainty contributions affecting the average current measurements when using a simple and inexpensive digital multimeter are analyzed in depth. Also, a criterion to keep the standard measurement uncertainty below a given threshold is provided. The theoretical analysis is validated by means of meaningful experimental results",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4201005,no,undetermined,0
Composite Event Detection in Wireless Sensor Networks,"Sensor networks can be used for event alarming applications. To date, in most of the proposed schemes, the raw or aggregated sensed data is periodically sent to a data consuming center. However, with this scheme, the occurrence of an emergency event such as a fire is hardly reported in a timely manner which is a strict requirement for event alarming applications. In sensor networks, it is also highly desired to conserve energy so that the network lifetime can be maximized. Furthermore, to ensure the quality of surveillance, some applications require that if an event occurs, it needs to be detected by at least k sensors where k is a user-defined parameter. In this work, we examine the timely energy-efficient k-watching event detection problem (TEKWEO). A topology-and-routing-supported algorithm is proposed which constructs a set of detection sets that satisfy the short notification time, energy conservation, and tunable quality of surveillance requirements for event alarming applications. Simulation results are shown to validate the proposed algorithm.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197939,no,undetermined,0
Scientific programming with Java classes supported with a scripting interpreter,"jLab environment provides a Matlab/Scilab like scripting language that is executed by an interpreter, implemented in the Java language. This language supports all the basic programming constructs and an extensive set of built in mathematical routines that cover all the basic numerical analysis tasks. Moreover, the toolboxes of jLab can be easily implemented in Java and the corresponding classes can be dynamically integrated to the system. The efficiency of the Java compiled code can be directly utilised for any computationally intensive operations. Since jLab is coded in pure Java, the build from source process is much cleaner, faster, platform independent and less error prone than the similar C/C++/Fortran-based open source environments (e.g. Scilab and Octave). Neuro-Fuzzy algorithms can require enormous computation resources and at the same time an expressive programming environment. The potentiality of jLab is demonstrated by describing the implementation of a Support Vector Machine toolkit and by comparing its performance with a C/C++ and a Matlab version and across different computing platforms (i.e. Linux, Sun/Solaris and Windows XP)",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197558,no,undetermined,0
Estimation and Evaluation of Common Cause Failures,"Success of many modern applications is highly dependent on the correct functioning of complex computer based systems. In some cases, failures in these systems may cause serious consequences in terms of loss of human life. Systems in which failure could endanger human life are termed safety-critical. The SIS (safety instrumented system) should be designed to meet the required safety integrity level as defined in the safety requirement specification (safety requirement allocation). Moreover, the SIS design should be performed in a way that minimizes the potential for common mode or common cause failures (CCF). A CCF occurs when a single fault result in the corresponding failure of multiple components. Thus, CCFs can result in the SIS failing to function when there is a process demand. Consequently, CCFs have to be identified during the design process and the potential impact on the SIS functionality have to be understood. This paper gives details about the estimation and evaluation of common failures and assesses a loo2 system. It is a survey paper that presents the newest developments in common cause failure analysis.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4196343,no,undetermined,0
A Study on the Application of Patient Location Data for Ubiquitous Healthcare System based on LBS,"Ubiquitous means the environment that users can receive the medical treatment regardless of the location and time. As the quality of the life has been improved, we are more focusing on our health and people want to be treated with the arising trend of the ubiquitous. Along with the circumstance, the interest of the remote-treatment has been increasing. So, systems are developing that can check their health status and treat them in a distance in a real time. Now, we are asking more services that can detect patients location and utilize this information. We studied the """"health care system"""" that can be operated by detecting the location of the patient in an urgent situation with the previous remote-treatment system that applying """"location based services"""". This system is the service which can help to detect the location of people or things through the portable-equipment based on wireless communication network. With this system we can process and manage data at the hospital or emergency room in a distance by transferring bio-data such as ECG data and pulse data as well as the user's location information.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4494212,no,undetermined,0
Determining Criteria for Selecting Software Components: Lessons Learned,"Software component selection is growing in importance. Its success relies on correctly assessing the candidate components' quality. For a particular project, you can assess quality by identifying and analyzing the criteria that affect it. Component selection is on the suitability and completeness of the criteria used for evaluation. Experiences from determining criteria for several industrial projects provide important lessons. For a particular selection process, you can organize selection criteria into a criteria catalog. A CC is built for a scope, which can be either a domain (workflow systems, mail servers, antivirus tools, and so on) or a category of domains (communication infrastructure, collaboration software, and so on). Structurally, a CC arranges selection criteria in a hierarchical tree-like structure. The higher-level selection criteria serve to classify more concrete selection criteria, usually allowing some overlap. They also serve to leverage the CC",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4163034,no,undetermined,0
NDT. A Model-Driven Approach for Web Requirements,"Web engineering is a new research line in software engineering that covers the definition of processes, techniques, and models suitable for Web environments in order to guarantee the quality of results. The research community is working in this area and, as a very recent line, they are assuming the Model-Driven paradigm to support and solve some classic problems detected in Web developments. However, there is a lack in Web requirements treatment. This paper presents a general vision of Navigational Development Techniques (NDT), which is an approach to deal with requirements in Web systems. It is based on conclusions obtained in several comparative studies and it tries to fill some gaps detected by the research community. This paper presents its scope, its most important contributions, and offers a global vision of its associated tool: NDT-Tool. Furthermore, it analyzes how Web Engineering can be applied in the enterprise environment. NDT is being applied in real projects and has been adopted by several companies as a requirements methodology. The approach offers a Web requirements solution based on a Model-Driven paradigm that follows the most accepted tendencies by Web engineering.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497213,no,undetermined,0
Model Based Requirements Specification and Validation for Component Architectures,"Requirements specification is a major component of the system development cycle. Mistakes and omissions in requirements documents lead to ambiguous or wrong interpretation by engineers and, in turn, cause errors that trickle down in design and implementation with consequences on the overall development cost. In this paper we describe a methodology for requirements specification that aims to alleviate the above issues and that produces models for functional requirements that can be automatically validated for completeness and consistency. This methodology is part of the Requirements Driven Design Automation framework (RDDA) that we develop for component-based system development. The RDDA framework uses an ontology-based language for semantic description of functional product requirements, UM- L/SysML structure diagrams, component constraints, and Quality of Service. The front end method for requirements specification is the SysML editor in Rhapsody. A requirements model in OWL is converted from SysML XMI representation. The specification is validated for completeness and consistency with a ruled-based system implemented in Prolog. With our methodology, omissions and several types of consistency errors present in the requirements specification are detected early on, before the design stage.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4519001,no,undetermined,0
"Using Parallel Processing Tools to Predict Rotorcraft Performance, Stability, and Control","This paper discusses the development of the High Performance Computing (HPC) Collaborative Simulation and Test (CST) portfolio CST-03 program, one of the projects in the Common HPC Software Support Initiative (CHSSI) portfolio. The objective of this development was to provide computationally scalable tools to predict rotorcraft performance, stability, and control. The ability to efficiently predict and optimize vehicle performance, stability, and control from high fidelity computer models would greatly enhance the design and testing process and improve the quality of systems acquisition. Through this CHSSI development, the US Navy Test Pilot School performance, stability, and control test procedures were fully implemented in a high performance parallel computing environment. These Navy flight test support options were parallelized, implemented, and validated in the FLIGHTLAB comprehensive, multidisciplinary modeling environment. These tools were designed to interface with other CST compatible models and a standalone version of the tools (FLIGHTLAB-ASPECT) was delivered for use independent of the FLIGHTLAB development system. Tests on the MAUI Linux cluster indicated that there was over 25 times speedup using 32 CPUs. The tests also met the accuracy criteria as defined for the Beta trial.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4161583,no,undetermined,0
Fault Injection Campaign for a Fault Tolerant Duplex Framework,"Software based fault tolerance may allow the use of COTS digital electronics in building a highly reliable computing system for spacecraft. In this work we present the results of a fault injection campaign we conducted on the Duplex Framework (DF). The DF is a software developed by the UCLA group [1], [2] that allows to run two copies (or replicas) of the same program on two different nodes of a commercial off-the-shelf (COTS) computer cluster. By the means of a third process (comparator) running on a different node that constantly monitors the results computed by the two replicas, the DF is able to restart the two replica processes if an inconsistency in their computation is detected. In order to test the reliability of the DF we wrote a simple fault injector that injects faults in the virtual memory of one of the replica process to simulate the effects of radiation in space. These faults occasionally cause the process to crash or produce erroneous outputs. For this study we used two different applications, one that computes an encryption of a input file using the RSA algorithm, and another that optimizes the trade-off between time spent and the fuel consumption for a low-thrust orbit transfer. But the DF is generic enough that any application written in C or Fortran could be used with little or no modification of the original source code. Our results show the potential of such approach in detecting and recovering from radiation induced random errors. This approach is very cost efficient compared to hardware implemented duplex operations and can be adopted to control processes on spacecrafts where the fault rate produced by cosmic rays is not very high.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4161526,no,undetermined,0
Is That a Fish in Your Ear? A Universal Metalanguage for Multimedia,"Developing the code to parse and generate multimedia bitstreams has traditionally been a repetitive and error-prone task. It has also been an area of application development that defied the goal of software reuse. In contrast, BSDL abstracts the minutiae of bitstream parsing out of software code, into an interoperable data file (the BSDL schema), allowing developers to concentrate on the functionality of their particular application. BSDL's approach has demonstrated applications at numerous points in the multimedia delivery chain. In the future, this approach may be extended to still other processing tasks, such as transcoding and transmoding, or to types of binary data other than multimedia",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4160281,no,undetermined,0
Fault Detection Using Differential Flatness in Flight Guidance Systems,"In this paper, flight guidance dynamics are shown to be implicit differentially flat with respect to the inertial position of an aircraft. This proves the existence of a set of relations between these flat outputs and the state variables representative of flight guidance dynamics and between these flat outputs and the basic inputs to flight guidance dynamics. A neural network is introduced to obtain, from the actual trajectory, nominal flight parameters which can be compared with actual values to detect abnormal behaviour",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4159728,no,undetermined,0
A New Grid Resource Management Mechanism Based on D-S Theory,"There may be some malicious nodes in the Grid environment because the resource of nodes can access the Grid system freely. The number and qualities of this resource can be change dramatically and optionally, so it is possible to affect the utilization of Grid resource. Hence, the trust mechanism is widely used in the management of Grid. This paper proposes a new method to detect the behaviors of resource providers in Grid environments based on D-S theory. Through simulating experiments, this mechanism can record the behaviors exactly and prevent the malicious ones. Then it gives a strong support for resource scheduling in the Grid.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4505778,no,undetermined,0
Resource Optimization for 60 GHz Indoor Networks Using Dynamic Extended Cell Formation,"With the advent of opening up 60 GHz Radio with 5 GHz of available spectrum, many bandwidth-hungry applications can be supported. The immediate concern, however, is the constraint on the line-of-sight transmission as well as the short transmission range of signals. As a result, in an indoor network, a mobile user might experience frequent breaks or losses of connection when one moves from one cell to another. To mitigate this problem, the Extended Cell (EC) architecture is proposed. In this paper, a dynamic Extended Cell formation algorithm is proposed based on the actual floor plan and the traffic situation under the network. Moreover, by applying this dynamic EC formation algorithm, we show that the call blocking probability is reduced. The dynamic EC formation also eases the deployment and maintenance cost due to its adaptability to the changing environment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4517420,no,undetermined,0
Improving Usability of Software Refactoring Tools,"Post-deployment maintenance and evolution can account for up to 75% of the cost of developing a software system. Software refactoring can reduce the costs associated with evolution by improving system quality. Although refactoring can yield benefits, the process includes potentially complex, error-prone, tedious and time-consuming tasks. It is these tasks that automated refactoring tools seek to address. However, although the refactoring process is well-defined, current refactoring tools do not support the full process. To develop better automated refactoring support, we have completed a usability study of software refactoring tools. In the study, we analysed the task of software refactoring using the ISO 9241-11 usability standard and Fitts' List of task allocation. Expanding on this analysis, we reviewed 11 collections of usability guidelines and combined these into a single list of 38 guidelines. From this list, we developed 81 usability requirements for refactoring tools. Using these requirements, the software refactoring tools Eclipse 3.2, Condenser 1.05, RefactorIT 2.5.1, and Eclipse 3.2 with the Simian UI 2.2.12 plugin were studied. Based on the analysis, we have selected a subset of the requirements that can be incorporated into a prototype refactoring tool intended to address the full refactoring process.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4159683,no,undetermined,0
Patching Processor Design Errors with Programmable Hardware,"Equipping processors with programmable hardware to patch design errors lets manufacturers release regular hardware patches, avoiding costly chip recalls and potentially speeding time to market. For each error detected, the manufacturer creates a fingerprint, which the customer uses to program the hardware. The hardware watches for error conditions; when they arise, it takes action to avoid the error. Overall, our scheme enables an exciting new environment where hardware design errors can be handled as easily as system software bugs, by applying a patch to the hardware",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4205120,no,undetermined,0
Automatic Instruction-Level Software-Only Recovery,"Software-only reliability techniques protect against transient faults without the overhead of hardware techniques. Although existing low-level software-only fault-tolerance techniques detect faults, they offer no recovery assistance. This article describes three automatic, instruction-level, software-only recovery techniques representing different trade-offs between reliability and performance",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4205122,no,undetermined,0
An evaluation method for aspectual modeling of distributed software architectures,"Dealing with crosscutting requirements in software development usually makes the process more complex. Modeling and analyzing of these requirements in the software architecture facilitate detecting architectural risks early. Distributed systems have more complexity and so these facilities are much useful in development of such systems. Aspect oriented Architectural Description Languages (ADD) have emerged to represent solutions for discussed problems; nevertheless, imposing radical changes to existing architectural modeling methods is not easily acceptable by architects. Software architecture analysis methods, furthermore, intend to verify that the quality requirements have been addressed properly. In this paper, we enhance ArchC# through utilization of aspect features with an especial focus on Non-Functional Requirements (NFR). ArchC# is mainly focused on describing architecture of distributed systems; in addition, it unifies software architecture with an object- oriented implementation to make executable architectures. Moreover, in this paper, a comparative analysis method is presented for evaluation of the result. All of these materials are illustrated along with a case study.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493639,no,undetermined,0
An Approach to Automated Agent Deployment in Service-Based Systems,"In service-based systems, services from various providers can be integrated following specific workflows to achieve users' goals. These workflows are often executed and coordinated by software agents, which invoke appropriate services based on situation changes. These agents need to be deployed on underlying platforms with respect to various requirements, such as access permission of agents, real-time requirements of workflows, and reliability of the overall system. Deploying these agents manually is often error-prone and time-consuming. Furthermore, agents need to migrate from hosts to hosts at runtime to satisfy deployment requirements. Hence, an automated agent deployment mechanism is needed. In this paper, an approach to automated agent deployment in service-base systems is presented. In this approach, the deployment requirements are represented as deployment policies, and techniques are developed for generating agent deployment plans by solving the constraints specified in deployment policies, and for generating executable code for runtime agent deployment and migration.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4208852,no,undetermined,0
Decision Reuse in an Interactive Model Transformation,"Propagating incremental changes and maintaining traceability are challenges for interactive model transformations, i.e. ones that combine automation with user decisions. After evolutionary changes to the source models the transformations have to be rerun. Earlier decisions cannot be used directly, because they may have been affected by the changes. Re-doing or verifying each decision manually is error-prone and burdensome. We present a way to model user interaction for transformations that are well (but not fully) understood. We model each decision as a set of options and their consequences. Also, we model the decision context, i.e. the circumstances (including model elements) affecting the decision. When a transformation is run, user decisions and their context are recorded. After a model change, a decision can be safely reused without burdening the user, if its context has not changed. The context maps source model elements to a decision, and thus provides traceability across the decision.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493307,no,undetermined,0
Efficient Analysis of Systems with Multiple States,A multistate system is a system in which both the system and its components may exhibit multiple performance levels (or states) varying from perfect operation to complete failure. Examples abound in real applications such as communication networks and computer systems. Analyzing the probability of the system being in each state is essential to the design and tuning of dependable multistate systems. The difficulty in analysis arises from the non-binary state property of the system and its components as well as dependence among those multiple states. This paper proposes a new model called multistate multivalued decision diagrams (MMDD) for the analysis of multistate systems with multistate components. The computational complexity of the MMDD-based approach is low due to the nature of the decision diagrams. An example is analyzed to illustrate the application and advantages of the approach.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4220955,no,undetermined,0
Fault Detection and Recovery in a Transactional Agent Model,"Servers can be fault-tolerant through replication and checkpointing technologies in the client server model. However, application programs cannot be performed and servers might block in the two-phase commitment protocol due to the client fault. In this paper, we discuss the transactional agent model to make application programs fault-tolerant by taking advantage of mobile agent technologies where a program can move from a computer to another computer in networks. Here, an application program on a faulty computer can be performed on another operational computer by moving the program. A transactional agent moves to computers where objects are locally manipulated. Objects manipulated have to be held until a transactional agent terminates. Some sibling computers which the transactional gent has visited might be faulty before the transactional agent terminates. The transactional agent has to detect faulty sibling computers and makes a decision on whether it commits/aborts or continues the computation by skipping the faulty computers depending on the commitment condition. For example, a transactional agent has to abort in the atomic commitment if a sibling computer is faulty. A transactional agent can just drop a faulty sibling computer in the at-least-one commitment. We evaluate the transactional agent model in terms of how long it takes for the transactional agent to treat faulty sibling computers .",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4220885,no,undetermined,0
Reengineering Idiomatic Exception Handling in Legacy C Code,"Some legacy programming languages, e.g., C, do not provide adequate support for exception handling. As a result, users of these legacy programming languages often implement exception handling by applying an idiom. An idiomatic style of implementation has a number of drawbacks: applying idioms can be fault prone and requires significant effort. Modern programming languages provide support for structured exception handling (SEH) that makes idioms largely obsolete. Additionally, aspect-oriented programming (AOP) is believed to further reduce the effort of implementing exception handling. This paper investigates the gains that can be achieved by reengineering the idiomatic exception handling of a legacy C component to these modern techniques. First, we will reengineer a C component such that its exception handling idioms are almost completely replaced by SEH constructs. Second, we will show that the use of AOP for exception handling can be beneficial, even though the benefits are limited by inconsistencies in the legacy implementation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493308,no,undetermined,0
Visual Detection of Design Anomalies,"Design anomalies, introduced during software evolution, are frequent causes of low maintainability and low flexibility to future changes. Because of the required knowledge, an important subset of design anomalies is difficult to detect automatically, and therefore, the code of anomaly candidates must be inspected manually to validate them. However, this task is time- and resource-consuming. We propose a visualization-based approach to detect design anomalies for cases where the detection effort already includes the validation of candidates. We introduce a general detection strategy that we apply to three types of design anomaly. These strategies are illustrated on concrete examples. Finally we evaluate our approach through a case study. It shows that performance variability against manual detection is reduced and that our semi-automatic detection has good recall for some anomaly types.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493326,no,undetermined,0
Modularity-Oriented Refactoring,"Refactoring, in spite of widely acknowledged as one of the best practices of object-oriented design and programming, still lacks quantitative grounds and efficient tools for tasks such as detecting smells, choosing the most appropriate refactoring or validating the goodness of changes. This is a proposal for a method, supported by a tool, for cross-paradigm refactoring (e.g. from OOP to AOP), based on paradigm and formalism-independent modularity assessment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493330,no,undetermined,0
Network provisioning over IP networks with call admission control schemes,"Multimedia applications are migrating to IP networks imposing high challenges on network planners. Challenges arise due to the stringent quality of service (QoS) requirements of multimedia applications that cannot be met over enterprise IP networks unless advanced techniques and strategies are applied. Example techniques are traffic differentiation, capacity evaluation and reservation, and call admission control. In this work, we assume practical call admission control (CAC) schemes and study by simulations the distribution of traffic inside the network. We show how capacity needs of traffic are affected when various CAC schemes are employed. Finally, we identify a procedure to evaluate the link capacity share for realtime traffic and study the resulting tradeoff between capacity needs and QoS parameters such as packet loss and blocking probability.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493558,no,undetermined,0
Optimizing jobs timeouts on clusters and production grids,"This paper presents a method to optimize the timeout value of computing jobs. It relies on a model of the job execution time that considers the job management system latency through a random variable. It also takes into account a proportion of outliers to model either reliable clusters or production grids characterized by faults causing jobs loss. Job management systems are first studied considering classical distributions. Different behaviors are exhibited, depending on the weight of the tail of the distribution and on the amount of outliers. Experimental results are then shown based on the latency distribution and outlier ratios measured on the EGEE grid infrastructure<sup>1</sup>. Those results show that using the optimal timeout value provided by our method reduces the impact of outliers and leads to a 1.36 speed-up even for reliable systems without outliers.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4215371,no,undetermined,0
OTSX: An extended transaction service in support of FT-CORBA standard,The FT-CORBA standard that has been adopted by OMG in recent years introduces mechanisms in support of increasing the availability of systems. This standard provides an infrastructure to detect faults and replicate distributed objects. In this paper we are going to share our experiences on implementing an extended transaction service (OTSX) which provides a set of specific features in support of FT-CORBA standard. These extensions allow distributed applications developed on top of FT-CORBA to run atomic operations on multiple object groups and ignore any faults that may occur in any object. The role of some affecting parameters like object size and failure rate is also studied and reported in this paper.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493569,no,undetermined,0
Design Fault Directed Test Generation for Microprocessor Validation,"Functional validation of modern microprocessors is an important and complex problem. One of the problems in functional validation is the generation of test cases that has higher potential to find faults in the design. We propose a model based test generation framework that generates tests for design fault classes inspired from software validation. There are two main contributions in this paper. Firstly, we propose a microprocessor modeling and test generation framework that generates test suites to satisfy modified condition decision coverage (MCDC), a structural coverage metric that detects most of the classified design faults as well as the remaining faults not covered by MCDC. Secondly, we show that there exists good correlation between types of design faults proposed by software validation and the errors/bugs reported in case studies on microprocessor validation. We demonstrate the framework by modeling and generating tests for the microarchitecture of VESPA, a 32-bit microprocessor. In the results section, we show that the tests generated using our framework's coverage directed approach detects the fault classes with 100% coverage, when compared to model-random test generation",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211892,no,undetermined,0
Automatic Application Specific Floating-point Unit Generation,"This paper describes the creation of custom floating point units (FPUs) for application specific instruction set processors (ASIPs). ASIPs allow the customization of processors for use in embedded systems by extending the instruction set, which enhances the performance of an application or a class of applications. These extended instructions are manifested as separate hardware blocks, making the creation of any necessary floating point instructions quite unwieldy. On the other hand, using a predefined FPU includes a large monolithic hardware block with considerable number of unused instructions. A customized FPU will overcome these drawbacks, yet the manual creation of one is a time consuming, error prone process. This paper presents a methodology for automatically generating floating-point units (FPUs) that are customized for specific applications at the instruction level. Generated FPUs comply with the IEEE754 standard, which is an advantage over FP format customization. Custom FPUs were generated for several Mediabench applications. Area savings over a fully-featured FPU without resource sharing of 26%-80% without resource sharing and 33%-87% with resource sharing, were obtained. Clock period increased in some cases by up to 9.5% due to resource sharing",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211840,no,undetermined,0
A Smooth Refinement Flow for Co-designing HW and SW Threads,"Separation of HW and SW design flows represents a critical aspect in the development of embedded systems. Co-verification becomes necessary, thus implying the development of complex co-simulation strategies. This paper presents a refinement flow that delays as much as possible the separation between HW and SW concurrent entities (threads), allowing their differentiation, but preserving an homogeneous simulation environment. The approach relies on SystemC as the unique reference language. However, SystemC threads, corresponding to the SW application, are simulated outside the control of the SystemC simulation kernel to exploit the typical features of multi-threading real-time operating systems running on embedded systems. On the contrary HW threads maintain the original simulation semantics of SystemC. This allows designers to effectively tune the SW application before HW/SW partitioning, leaving to an automatic procedure the SW generation, thus avoiding error-prone and time-consuming manual conversions",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211780,no,undetermined,0
An Analysis of Performance Interference Effects in Virtual Environments,"Virtualization is an essential technology in modern datacenters. Despite advantages such as security isolation, fault isolation, and environment isolation, current virtualization techniques do not provide effective performance isolation between virtual machines (VMs). Specifically, hidden contention for physical resources impacts performance differently in different workload configurations, causing significant variance in observed system throughput. To this end, characterizing workloads that generate performance interference is important in order to maximize overall utility. In this paper, we study the effects of performance interference by looking at system-level workload characteristics. In a physical host, we allocate two VMs, each of which runs a sample application chosen from a wide range of benchmark and real-world workloads. For each combination, we collect performance metrics and runtime characteristics using an instrumented Ken hypervisor. Through subsequent analysis of collected data, we identify clusters of applications that generate certain types of performance interference. Furthermore, we develop mathematical models to predict the performance of a new application from its workload characteristics. Our evaluation shows our techniques were able to predict performance with average error of approximately 5%",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4211036,no,undetermined,0
"OOPS for Motion Planning: An Online, Open-source, Programming System","The success of sampling-based motion planners has resulted in a plethora of methods for improving planning components, such as sampling and connection strategies, local planners and collision checking primitives. Although this rapid progress indicates the importance of the motion planning problem and the maturity of the field, it also makes the evaluation of new methods time consuming. We propose that a systems approach is needed for the development and the experimental validation of new motion planners and/or components in existing motion planners. In this paper, we present the online, open-source, programming system for motion planning (OOPS<sub>MP</sub>), a programming infrastructure that provides implementations of various existing algorithms in a modular, object-oriented fashion that is easily extendible. The system is open-source, since a community-based effort better facilitates the development of a common infrastructure and is less prone to errors. We hope that researchers will contribute their optimized implementations of their methods and thus improve the quality of the code available for use. A dynamic Web interface and a dynamic linking architecture at the programming level allows users to easily add new planning components, algorithms, benchmarks, and experiment with different parameters. The system allows the direct comparison of new contributions with existing approaches on the same hardware and programming infrastructure",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4209665,no,undetermined,0
Independent Model-Driven Software Performance Assessments of UML Designs,"In many software development projects, performance requirements are not addressed until after the application is developed or deployed, resulting in costly changes to the software or the acquisition of expensive high-performance hardware. To remedy this, researchers have developed model-driven performance analysis techniques for assessing how well performance requirements are being satisfied early in the software lifecycle. In some cases, companies may not have the expertise to perform such analysis on their software; therefore they have an independent assessor perform the analysis. This paper describes an approach for conducting independent model-driven software performance assessments of UML 2.0 designs and illustrates this approach using a real-time signal generator as a case study",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4208857,no,undetermined,0
Obstacles to Comprehension in Usage Based Reading,"Usage based reading (UBR) is a recent approach to object oriented software inspections. Like other scenario based reading (SBR) techniques it proposes a prescriptive reading procedure. However, the impact of such procedures upon comprehension is not well known, and consideration has not been given to established software cognition theories. This paper describes a study examining software comprehension in UBR inspections. Participants traced the events of a UML sequence diagram through Java source code while thinking aloud. An electronic interface collected real-time data, allowing the identification of """"points of interest"""", which were categorised according to issues affecting participants' performance. Together with indicators of participants' cognitive processes, this suggests that adherence to UBR scenarios is non-trivial. While UBR can detect more critical defects, we argue that a re-think of its prescriptive nature, including the use of cognition support, is required before it can become a practical reading technique.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4159676,no,undetermined,0
A Model-Based Approach for Testing GUI Using Hierarchical Predicate Transition Nets,"Testing graphical user interface (GUI) has shown to be costly and difficult. Existing approaches for testing GUI are event-driven. In this paper, we propose a model based testing method to test the structural representation of GUIs specified in high class of Petri nets known as hierarchical predicate transitions nets (HPrTNs). In order to detect early design faults and fully benefit from HPrTNmodels, we have extended the original coverage criteria proposed for HPrTNs by event-based criteria defined for GUI testing",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4151711,no,undetermined,0
Detecting VLIW Hard Errors Cost-Effectively through a Software-Based Approach,"Research indicates that as technology scales, hard errors such as wear-out errors are increasingly becoming a critical challenge for microprocessor design. While hard errors in memory structures can be efficiently detected by error correction code, detecting hard errors for functional units cost-effectively is a challenging problem. In this paper, we propose to exploit the idle cycles of the under-utilized VLIW functional units to run test instructions for detecting wear-out errors without increasing the hardware cost or significantly impacting performance. We also explore the design space of this software-based approach to balance the error detection latency and the performance for VLIW architectures. Our experimental results indicate that such a software-based approach can effectively detect hard errors with minimum impact on performance for VLIW processors, which is particularly useful for reliable embedded applications with cost constraints.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221157,no,undetermined,0
Agent-based Human-computer-interaction for Real-time Monitoring Systems in the Trucking Industry,"Auto ID systems can replace time-consuming, costly and error-prone processes of human data entry and produce detailed real time information. However, they would add value only to the extent that data is presented in a user-friendly manner. As model-based decision support is not always adequate, an agent-based approach is often chosen. Real life entities such as orders and trucks are represented by agents, which negotiate in order to solve planning problems. For the respective data representation at least two forms can be distinguished, focusing either on (1) resources (account-based) or (2) orders (order-centric). Applying cognitive fit theory we describe how the different interfaces affect decision making. The hypotheses would be tested in a laboratory experiment. The intended contribution should support that order-centric interfaces have higher user-friendliness and are especially beneficial to low-analytics and planners working under time pressure",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076424,no,undetermined,0
A Comparison of Static Architecture Compliance Checking Approaches,"The software architecture is one of the most important artifacts created in the lifecycle of a software system. It enables, facilitates, hampers, or interferes directly the achievement of business goals, functional and quality requirements. One instrument to determine how adequate the architecture is for its intended usage is architecture compliance checking. This paper compares three static architecture compliance checking approaches (reflexion models, relation conformance rules, and component access rules) by assessing their applicability in 13 distinct dimensions. The results give guidance on when to use which approach.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077029,no,undetermined,0
Optimisation of wirebond interconnects by automated parameter variation,"A numerical optimisation strategy for interconnections in electronic packaging is demonstrated. The method is based on a toolbox for the parametric generation of finite- element models of package types such as Chip Scale Package (CSP), Micro Lead Package (MLP) or Ball Grid Array (BGA). The novelty of this work is the combination of this modeling toolbox with an optimisation software for automatic parameter variation. Resulting in a convenient tool to investigate the influence of geometry on the relevant quality characteristics of the device. Users can set the parameters to be varied, the ranges of parameter variation and the number of iterations. The optimisation software automatically generates the parameter sets depending on the number of iterations. The generation of a finite-element model for each parameter set, the meshing and the implementation of the required material properties are also automated by the toolbox. Thereafter, the simulation of the desired load conditions results in quality characteristics such as the maximum mechanical stress for each set. After completion of all iterations, the optimisation software provides a user interface for statistical analysis and graphic visualisation of the results. The wirebond geometry is also included in the toolbox. Influence on maximum mechanical stress and fatigue properties under thermal loads is examined during this study. As an example, the effect of the bonding tool geometry on the locations and the value of the maximum mechanical stress in the wirebond material during thermal shocking is determined. This combination of parametric finite-element model generation and automatic parameter variation represents a powerful tool for design automation in packaging technology and product development. The effects of several geometrical parameters on the thermal and mechanical behaviour of packaging interconnects can be predicted. In a virtual product-development process, time- and cost-intensive prototyping and testin- - g can thus be reduced.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4525044,no,undetermined,0
Inside Architecture Evaluation: Analysis and Representation of Optimization Potential,"The share of software in embedded systems has been growing permanently in the recent years. Thus, software architecture as well as its evaluation has become an important part of embedded systems design to define, assess, and assure architecture and system quality. Furthermore, design space exploration can be based on architecture evaluation. To achieve an efficient exploration process, architectural decisions need to be well considered. In this paper, analysis of architecture evaluation is performed to uncover dependencies of the quality attributes which are the first class citizens of architecture evaluation. With an explicit representation of such dependencies, valuable changes of an architecture can be calculated. Next to the exploration support, the analysis results help to document architecture knowledge and make architectural decisions explicit and traceable. The development process can now be based on dependable and well documented architectural decisions. Effects of changes become more predictable. Time and costs can be saved by avoiding suboptimal changes.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077020,no,undetermined,0
An Innovative Approach to Tackling the Boundary Effect in Adaptive Random Testing,"Adaptive random testing (ART) is an effective improvement of random testing (RT) in the sense that fewer test cases are needed to detect the first failure. It is based on the observation that failure-causing inputs are normally clustered in one or more contiguous regions in the input domain. Hence, it has been proposed that test case generation should refer to the locations of successful test cases (those that do not reveal failures) to ensure that all test cases are far apart and evenly spread in the input domain. Distance-based ART and restricted random testing are the first two previous attempts. However, test cases generated by these attempts are far apart but not necessarily evenly spread, since more test cases are generated near the boundary of the input domain. This paper analyzes the cause of this phenomenon and proposes an enhanced implementation based on the concept of virtual images of the successful test cases. The results of simulations show that the test cases generated by our enhanced implementation are not only far apart but also evenly spread in the input domain. Furthermore, the fault detection capability of ART for high-dimensional input domains is also enhanced",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076912,no,undetermined,0
Reconciling Manual and Automated Testing: The AutoTest Experience,"Software can be tested either manually or automatically. The two approaches are complementary: automated testing can perform a large number of tests in little time, whereas manual testing uses the knowledge of the testing engineer to target testing to the parts of the system that are assumed to be more error-prone. Despite this complementarity, tools for manual and automatic testing are usually different, leading to decreased productivity and reliability of the testing process. AutoTest is a testing tool that provides a """"best of both worlds"""" strategy: it integrates developers' test cases into an automated process of systematic contract-driven testing. This allows it to combine the benefits of both approaches while keeping a simple interface, and to treat the two types of tests in a unified fashion: evaluation of results is the same, coverage measures are added up, and both types of tests can be saved in the same format",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076909,no,undetermined,0
Practical Combinatorial Testing: Beyond Pairwise,"With new algorithms and tools, developers can apply high-strength combinatorial testing to detect elusive failures that occur only when multiple components interact. In pairwise testing, all possible pairs of parameter values are covered by at least one test, and good tools are available to generate arrays with the value pairs. In the past few years, advances in covering-array algorithms, integrated with model checking or other testing approaches, have made it practical to extend combinatorial testing beyond pairwise tests. The US National Institute of Standards and Technology (NIST) and the University of Texas, Arlington, are now distributing freely available methods and tools for constructing large t-way combination test sets (known as covering arrays), converting covering arrays into executable tests, and automatically generating test oracles using model checking (http://csrc.nist.gov/acts). In this review, we focus on real-world problems and empirical results from applying these methods and tools.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4525537,no,undetermined,0
Ontology Driven Requirements Query,"Use cases are commonly used to represent customers' requirements during systems development. In a large software development environment, finding a relevant use case from a large use case library created in the past or related projects is a complex, error-prone and expensive task. Based on the semantic Web approach, we propose an ontological methodology to support this task. We use ontology to augment use cases with semantic information. This ontology is derived from ResearchCyc ontology. We also propose the augmentation of queries used to retrieve use cases with this ontology. We present this approach to better capture, reuse and query use cases",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076779,no,undetermined,0
A Methodology for Performance Modeling and Simulation Validation of Parallel 3-D Finite Element Mesh Refinement With Tetrahedra,"The design and implementation of parallel finite element methods (FEMs) is a complex and error-prone task that can benefit significantly by simulating models of them first. However, such simulations are useful only if they accurately predict the performance of the parallel system being modeled. The purpose of this contribution is to present a new, practical methodology for validation of a promising modeling and simulation approach for parallel 3-D FEMs. To meet this goal, a parallel 3-D unstructured mesh refinement model is developed and implemented based on a detailed software prototype and parallel system architecture parameters in order to simulate the functionality and runtime behavior of the algorithm. Estimates for key performance measures are derived from these simulations and are validated with benchmark problem computations obtained using the actual parallel system. The results illustrate the potential benefits of the new methodology for designing high performance parallel FEM algorithms.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526979,no,undetermined,0
Transistor-Level Synthesis for Low-Power Applications,"An important factor which greatly affects the power consumption and the delay of a circuit is the input capacitance of its gates. High input capacitances increase the power consumption as well as the time for charging and discharging the inputs. Current approaches address this problem either through gate-level only resynthesis and optimization, or indirectly through transistor-level synthesis aimed for transistor count reduction. In this paper a method is presented to synthesize complex gates at the transistor level with explicit consideration of the switching activity profile for the gate. The method finds a power efficient implementation by giving priority to transistor inputs with higher switching activity, while keeping the overall number of required transistors low. Experimental results demonstrate the benefit of the approach",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4149102,no,undetermined,0
TopoLayout: Multilevel Graph Layout by Topological Features,"We describe TopoLayout, a feature-based, multilevel algorithm that draws undirected graphs based on the topological features they contain. Topological features are detected recursively inside the graph, and their subgraphs are collapsed into single nodes, forming a graph hierarchy. Each feature is drawn with an algorithm tuned for its topology. As would be expected from a feature-based approach, the runtime and visual quality of TopoLayout depends on the number and types of topological features present in the graph. We show experimental results comparing speed and visual quality for TopoLayout against four other multilevel algorithms on a variety of data sets with a range of connectivities and sizes. TopoLayout frequently improves the results in terms of speed and visual quality on these data sets",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069239,no,undetermined,0
An Effective PSO-Based Memetic Algorithm for Flow Shop Scheduling,"This paper proposes an effective particle swarm optimization (PSO)-based memetic algorithm (MA) for the permutation flow shop scheduling problem (PFSSP) with the objective to minimize the maximum completion time, which is a typical non-deterministic polynomial-time (NP) hard combinatorial optimization problem. In the proposed PSO-based MA (PSOMA), both PSO-based searching operators and some special local searching operators are designed to balance the exploration and exploitation abilities. In particular, the PSOMA applies the evolutionary searching mechanism of PSO, which is characterized by individual improvement, population cooperation, and competition to effectively perform exploration. On the other hand, the PSOMA utilizes several adaptive local searches to perform exploitation. First, to make PSO suitable for solving PFSSP, a ranked-order value rule based on random key representation is presented to convert the continuous position values of particles to job permutations. Second, to generate an initial swarm with certain quality and diversity, the famous Nawaz-Enscore-Ham (NEH) heuristic is incorporated into the initialization of population. Third, to balance the exploration and exploitation abilities, after the standard PSO-based searching operation, a new local search technique named NEH_1 insertion is probabilistically applied to some good particles selected by using a roulette wheel mechanism with a specified probability. Fourth, to enrich the searching behaviors and to avoid premature convergence, a simulated annealing (SA)-based local search with multiple different neighborhoods is designed and incorporated into the PSOMA. Meanwhile, an effective adaptive meta-Lamarckian learning strategy is employed to decide which neighborhood to be used in SA-based local search. Finally, to further enhance the exploitation ability, a pairwise-based local search is applied after the SA-based search. Simulation results based on benchmarks demonstrate the effectiveness of- - the PSOMA. Additionally, the effects of some parameters on optimization performances are also discussed",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4067067,no,undetermined,0
PocketPad: Using Handhelds and Digital Pens to Manage Data in Mobile Contexts,"PocketPad is an information management system geared toward university students. The system is designed to support the capture, storage, browsing, editing and organization of handwritten notes via the complementary use of digital pens to capture information, handheld computers to browse and store prior information, and digital pens and handheld computers in combination to edit and organize information. Desktop computer software synchronizes the information from the digital pen and Pocket PC during editing and reorganization. Through the combined use of different hardware components supported by our software, we describe a system that bridges the paper-electronic information divide in mobile contexts. The system relies on human-in-the-loop techniques coupled with stroke timing to simplify coordination of content from different sources, rather than resorting to complex and often error-prone computer document recognition algorithms.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4063774,no,undetermined,0
Preventing Lithography-Induced Maverick Yield Events With A Dispense System Advanced Equipment Control Method,"As semiconductor manufacturers march to the drum beat of Moore's law there is very little room for yield mavericks, especially those that can be prevented. Critical process errors are costly and photolithography is one of the few processes in semiconductor manufacturing where there is an opportunity to correct errors. Small changes in photo resist dispensed volume may have severe impact on film thickness uniformity and can ultimately affect patterning. It is important to monitor photo-dispense conditions to detect real-time events that may have a direct negative impact on process yield and be able to react to these events as quickly as possible. This paper presents an evaluation of the IntelliGenreg Mini, a photo resist dispense system manufactured by Entegris, Inc. This system utilizes advanced equipment control software, known as dispense confirmation, to detect variations in photo dispense. These variations, caused by bubbles in the dispense line, valve errors, and accidentally changed chemistries can all create maverick yield events that can go undetected until metrology, defect inspection, or wafer final test. The ability of an advanced dispense system to detect events and create alerts is a very powerful tool, but it can be most effective when that information is collected and analyzed by an automated system. In a modern fabricator this is most likely a statistical process control chart that is monitoring a track's progress and is ready to stop the track when a maverick event occurs or alert personnel to trends they may not otherwise catch with other inline metrology data. Dispense confirmation, when combined with networking capabilities, can meet this need. After a brief description of the pump, data from simulated yield-affecting events will be examined to evaluate the IntelliGenreg Mini's ability to detect them. This discussion will conclude with a brief analysis of the ultimate time and cost savings of utilizing dispense confirmation with networking cap- abilities to detect and eliminate poorly coated wafers.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529002,no,undetermined,0
Detecting Ventricular Fibrillation by Time-Delay Methods,"A pivotal component in automated external defibrillators (AEDs) is the detection of ventricular fibrillation (VF) by means of appropriate detection algorithms. In scientific literature there exists a wide variety of methods and ideas for handling this task. These algorithms should have a high detection quality, be easily implementable, and work in realtime in an AED. Testing of these algorithms should be done by using a large amount of annotated data under equal conditions. For our investigation we simulated a continuous analysis by selecting the data in steps of 1 s without any preselection. We used the complete BIH-MIT arrhythmia database, the CU database, and files 7001-8210 of the AHA database. For a new VF detection algorithm we calculated the sensitivity, specificity, and the area under its receiver operating characteristic curve and compared these values with the results from an earlier investigation of several VF detection algorithms. This new algorithm is based on time-delay methods and outperforms all other investigated algorithms",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4034001,no,undetermined,0
Scene Parsing Using Region-Based Generative Models,"Semantic scene classification is a challenging problem in computer vision. In contrast to the common approach of using low-level features computed from the whole scene, we propose """"scene parsing"""" utilizing semantic object detectors (e.g., sky, foliage, and pavement) and region-based scene-configuration models. Because semantic detectors are faulty in practice, it is critical to develop a region-based generative model of outdoor scenes based on characteristic objects in the scene and spatial relationships between them. Since a fully connected scene configuration model is intractable, we chose to model pairwise relationships between regions and estimate scene probabilities using loopy belief propagation on a factor graph. We demonstrate the promise of this approach on a set of over 2000 outdoor photographs, comparing it with existing discriminative approaches and those using low-level features",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032602,no,undetermined,0
Data Mining Static Code Attributes to Learn Defect Predictors,"The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of """"McCabes versus Halstead versus lines of code counts"""" for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027145,yes,undetermined,0
Three-phase four-wire DSTATCOM based on a three-dimensional PWM algorithm,"A modified voltage space vector pulse-width modulated (PWM) algorithm for three-phase four-wire distribution static compensator (DSTATCOM) is described in this paper. The mathematical model of shunt-connected three-leg inverter in three-phase four-wire system is studied in a-b-c frame. The p-q-o theory based on the instantaneous reactive power theory is applied to detect the reference current. A fast and generalized applicable three-dimensional space vector modulation (3DSVM) is proposed for controlling a three-leg inverter. The reference voltage vector is decomposed into an offset vector and a two-level vector. So identification of neighboring vectors and dwell times calculation are all settled by a general two-level 3DSVM control. This algorithm can also be applied to multilevel inverter. The zero-sequence component of each vector is considered in order to implement the neutral current compensation. The simulation is performed by EMTDC/PSCAD software. The neutral current, harmonics current, unbalance current and reactive current can be compensated. The result shows that the validity of the proposed 3DSVM can be applied to compensate power quality problems in three-phase four-wire system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4523748,no,undetermined,0
"Fighting bugs: remove, retry, replicate, and rejuvenate","Even if software developers don't fully understand the faults or know their location in the code, software rejuvenation can help avoid failures in the presence of aging-related bugs. This is good news because reproducing and isolating an aging-related bug can be quite involved, similar to other Mandelbugs. Moreover, monitoring for signs of software aging can even help detect software faults that were missed during the development and testing phases. If, on the other hand, a developer can detect a specific aging-related bug in the code, fixing it and distributing a software update might be worthwhile. In the case of the Patriot missile-defense system, a modified version of the software was indeed prepared and deployed to users. It arrived at Dhahran on 26 February 1991 - a day after the fatal incident.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4085640,no,undetermined,0
Generalized Discrete Software Reliability Modeling With Effect of Program Size,"Generalized methods for software reliability growth modeling have been proposed so far. But, most of them are on continuous-time software reliability growth modeling. Many discrete software reliability growth models (SRGM) have been proposed to describe a software reliability growth process depending on discrete testing time such as the number of days (or weeks); the number of executed test cases. In this paper, we discuss generalized discrete software reliability growth modeling in which the software failure-occurrence times follow a discrete probability distribution. Our generalized discrete SRGMs enable us to assess software reliability in consideration of the effect of the program size, which is one of the influential factors related to the software reliability growth process. Specifically, we develop discrete SRGMs in which the software failure-occurrence times follow geometric and discrete Rayleigh distributions, respectively. Moreover, we derive software reliability assessment measures based on a unified framework for discrete software reliability growth modeling. Additionally, we also discuss optimal software release problems based on our generalized discrete software reliability growth modeling. Finally, we show numerical examples of software reliability assessment by using actual fault-counting data",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4100777,no,undetermined,0
A new power quality detection device based on embedded technique,"A new kind of monitoring device on power quality based on digital signal processing (DSP) chip and advanced RISC machine (ARM) microprogrammed control unit (MCU) is introduced in this thesis. And its power quality detection arithmetic and its hardware realization and software realization are expatiated on. The device use a kind of reformative FFT to measure harmonic frequency difference, with a digital filter method to survey voltage fluctuation and flicker, in a wavelet transform to detect transient power quality disturbance, on an embedded Linux operation system to develop management software, by DSP chip to implement power quality calculations and through ARM MCU to perform data management, so that the device can implement real-time, comprehensive and high-precision monitoring and management for all power quality parameters. Thus, the device not only structures a self-governed power quality monitoring system, but also acts as front placed part or local area network of wide area power quality monitoring system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4523667,no,undetermined,0
IPOG: A General Strategy for T-Way Software Testing,"Most existing work on t-way testing has focused on 2-way (or pairwise) testing, which aims to detect faults caused by interactions between any two parameters. However, faults can also be caused by interactions involving more than two parameters. In this paper, we generalize an existing strategy, called in-parameter-order (IPO), from pairwise testing to t-way testing. A major challenge of our generalization effort is dealing with the combinatorial growth in the number of combinations of parameter values. We describe a t-way testing tool, called FireEye, and discuss design decisions that are made to enable an efficient implementation of the generalized IPO strategy. We also report several experiments that are designed to evaluate the effectiveness of FireEye",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148973,no,undetermined,0
System Level Performance Assessment of SOC Processors with SystemC,"This paper presents a system level methodology for modeling, and analyzing the performance of system-on-chip (SOC) processors. The solution adopted focuses on minimizing assessment time by modeling processors behavior only in terms of the performance metrics of interest. Formally, the desired behavior is captured through a C/C++ executable model, which uses finite state machines (FSM) as the underlying model of computation (MOC). To illustrate and validate our methodology we applied it to the design of a 16-bit reduced instruction set (RISC) processor. The performance metrics used to assess the quality of the design considered are power consumption and execution time. However, the methodology can be extended to any performance metric. The results obtained demonstrate the robustness of the proposed method both in terms of assessment time and accuracy",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148970,no,undetermined,0
Application of Bayesian Networks to Architectural Optimisation,"The field of optimisation covers a great multitude of principles, methods and frameworks aimed at maximisation of an objective under constraints. However, the classical optimisation can not be easily applied in the context of computer-based systems architecture as there is not enough knowledge concerning the dependencies between non-functional qualities of the system. Out approach is based on the simulation optimisation methodology where the system simulation is first created to assess the current state of the design with respect to the objectives. The results of the simulation are used to construct a Bayesian belief network which effectively becomes a base for an objective function and serves as the main source of the decision support pertaining to the guidance of the optimisation process. The potential effects of each proposed change or combination of changes is then examined by updating and re-evaluating the system simulation",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148917,no,undetermined,0
Realization of an Adaptive Distributed Sound System Based on Global-Time-Based Coordination and Listener Localization,"This paper discusses the benefits of exploiting 1) the principle of global-time-based coordination of distributed computing actions (TCoDA) and 2) a high-level component-/object-based programming approach in developing real-time embedded computing software. The benefits are discussed in the context of a concrete case study. A new major type of distributed multimedia processing applications, called Adaptive Distributed Sound Systems (ADSSs), is presented here to show the compelling nature of the TCoDA exploitation. High-quality ADSSs impose stringent real-time distributed computing requirements. They require a global-time base with precision better than 100 mus. For efficient implementation, the TMO programming scheme and associated tools are shown to be highly useful. In addition, a prototype TMO-based ADSS has been developed and its most important quality attributes have been empirically evaluated. The prototype ADSS has also turned out to be a cost-effective tool for assessing the quality of service of a TMO execution engine.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4519565,no,undetermined,0
Verification and Validation of (Real Time) COTS Products using Fault Injection Techniques,"With the goal of reducing time to market and project costs, the current trend of real time business and mission critical systems is evolving from the development of custom made applications to the use of commercial off the shelf (COTS) products. Obviously, the same confidence and quality of the custom made software components is expected from the commercial applications. In most cases, such products (COTS) are not designed with stringent timing and/or safety requirements as priorities. Thus, to decrease the gap between the use of custom made components and COTS components, this paper presents a methodology for evaluating COTS products in the scope of dependable, real time systems, through the application of fault injection techniques at key points of the software engineering process. By combining the use of robustness testing (fault injection at interface level) with software fault injection (using educated fault injection operators), a COTS component can be assessed in the context of the system it will belong to, with special emphasis given to timing and safety constraints that are usually imposed by the target real time dependable environment. In the course of this work, three case studies have been performed to assess the methodology using realistic scenarios that used common COTS products. Results for one case study are presented",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127318,no,undetermined,0
Uncertainty Explicit Assessment of Off-the-Shelf Software: Selection of an Optimal Diverse Pair,"Assessment of software COTS components is an essential part of component-based software development. Sub-optimal selection of components may lead to solutions with low quality. The assessment is based on incomplete knowledge about the COTS components themselves and other aspects, which may affect the choice such as the vendor's credentials, etc. We argue in favor of assessment methods in which uncertainty is explicitly represented (`uncertainty explicit' methods) using probability distributions. We have adapted a model (developed elsewhere by Littlewood, B. et al. (2000)) for assessment of a pair of COTS components to take account of the fault (bug) logs that might be available for the COTS components being assessed. We also provide empirical data from a study we have conducted with off-the-shelf database servers, which illustrate the use of the method",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127304,no,undetermined,0
Matrix converter: improvement on the start-up and the switching behavior,"The matrix converter (MC) presents a promising topology that needs to overcome certain barriers (protection systems, durability, the development of converters for real applications, etc.) in order to gain a foothold in the market. Taking into consideration that the great majority of efforts are being oriented towards control algorithms and modulation, this article focuses on MC hardware. In order to improve the switching speed of the MC and thus obtain signals with less harmonic distortion, several different IGBT excitation circuits are being studied. Below, the appropriate topology is selected for the MC and a configuration is presented which reduces the excursion range of the drivers and optimizes the switching speed of the IGBTs. Inadequate driver control can lead to the destruction of the MC due to its low ride-through capability. Moreover, this converter is specially sensitive during start-up, as at that moment there are high overcurrents and overvoltages. With the aim of finding a solution for starting-up the MC, a circuit is presented (separate from the control software) which ensures a correct sequencing of supplies, thus avoiding a short-circuit between input phases. Moreover, it detects overcurrent, connection/disconnection, and converter supply faults. Faults cause the circuit to protect the MC by switching off all the IGBT drivers without latency. All this operability is guaranteed even when the supply falls below the threshold specified by the manufacturers for the correct operation of the circuits. All these features are demonstrated with experimental results. For all these reasons, it can be said that the techniques proposed in this article substantially improve the MC start-up cycle, representing a step forward towards the development of reliable matrix converters for real applications.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4522946,no,undetermined,0
Workshop: Assessing the Quality of a Business Process Implemented across Systems of Systems,"A team at the SEI has developed a framework that has shown promise in assessing the survivability of a business process in a system of systems environment. Assessment outputs in pilot use included survivability requirements and gaps among interoperable systems. It is anticipated that the framework can be of use in the evaluation of other quality attributes. Researchers and practitioners in software design, architecture, quality assurance, and requirements validation are asked to participate in this workshop to review the work done for survivability and assist in the determination of its broader applicability",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127288,no,undetermined,0
Failure Time Based Reliability Growth in Product Development and Manufacturing,"The failure-in-time (FIT) rate is widely used to quantify the reliability of a electronic component. It fails to indicate the portion of the failures due to either environmental or electrical stresses or issues that are related to process/handing, manufacturing and applications. To meet this end, FIT-based corrective action driven metrics are proposed to link the failure mode (FM) with components and non-component faults. First the conventional failure mode pareto is reviewed and its deficiency is discussed. Then a new index called the failure mode rate (FMR) is introduced to monitor the FM trend and evaluate the effectiveness of corrective actions (C/As). Based on the FMR, the FIT rate is extended to non-component failure mode and further to individual failure mode in predicting the reliability of electronic products. The extended FIT rate enables product designers to narrow down the root-cause of the failure, identify the C/A ownership, and estimate the MTBF improvement. The new metrics provide a guideline for prioritizing resources to attack the critical failures with the minimum cost.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4126400,no,undetermined,0
Assessing Diagnostic Techniques for Fault Tolerance in Software,"One of the main concerns in software safety critical applications is to ensure sufficient reliability if one cannot prove the absence of faults. Fault tolerance (FT) provides a plausible method for improving reliability claims in the presence of systematic failures in software. It is plausible that some software FT techniques offer increased protection than others. However, the extent of claims that can be made for different FT software architectures remains unclear. We investigate an approach to FT that integrates data diversity (DD) assertions and traditional assertions (TA). We also present the principles of a method to assess the effectiveness of the approach. The aim of this approach is to make it possible to evolve more powerful FT and thereby improve reliability. This is a step towards the aim of understanding the effectiveness of FT safety-critical applications and thus making it easier to use FT in safety arguments",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4126355,no,undetermined,0
On The Development Of Fault Injection Profiles,"The impact of hardware failures on software has attracted substantial attention in the study of dependable systems. Fault injection techniques have emerged as a major means to evaluate software behavior in the presence of hardware failures. However, due to the lack of knowledge of the fault distribution information, the fault location and time are randomly selected. One major drawback of this approach is that the injected faults do not represent the system's operational situation, thus software reliability cannot be credibly assessed. This paper aims at extending the use of fault injection to the reliability prediction of hardware faults. To do so, we have developed a set of analytical and simulation based methods capable of statistically reproducing the underlying physics and phenomena leading to hardware failures in a given system operational context. Such distributions are referred to as fault injection profiles, and are the basis to extend the fault injection technique with fault models that represent the actual conditions under which hardware faults occur",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4126354,no,undetermined,0
Adaptive runtime fault management for service instances in component-based software applications,"The Trust4All project aims to define an open, component-based framework for the middleware layer in high-volume embedded appliances that enables robust and reliable operation, upgrading and extension. To improve the availability of each individual application in a Trust4All system, a runtime configurable fault management mechanism (FMM) is proposed, which detects deviations from given service specifications by intercepting interface calls. When repair is necessary, FMM picks a repair action that incurs the best tradeoff between the success rate and the cost of repair. Considering that it is rather difficult to obtain sufficient information about third party components during their early stage of usage, FMM is designed to be able to accumulate knowledge and adapts its capability accordingly",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4124010,no,undetermined,0
Interactive Image Repair with Assisted Structure and Texture Completion,"Removing image defects in an undetectable manner has been studied for its many useful and varied applications. In many cases the desired result may be ambiguous from the image data alone and needs to be guided by a user's knowledge of the intended result. This paper presents a framework for interactively incorporating user guidance into the filling-in process, more effectively using user input to fill in damaged regions in an image. This framework contains five main steps: first, the scratch or defect is detected; second, the edges outside the defect are detected; third, curves are fit to the detected edges; fourth, the structure is completed across the damaged region; and finally, texture synthesis constrained by the previously computed curves is used to fill in the intensities in the damaged region. Scratch detection, structure completion, and texture synthesis are influenced or guided by user input when given. Results include removal of defects from images that contain structure, texture, or both structure and texture. Users can complete images with ambiguous structure in multiple ways by gesturing the cursor in the direction of the desired structure completion",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118740,no,undetermined,0
An Operation-Centered Approach to Fault Detection in Symmetric Cryptography Ciphers,"One of the most effective ways of attacking a cryptographic device is by deliberate fault injection during computation, which allows retrieving the secret key with a small number of attempts. Several attacks on symmetric and public-key cryptosystems have been described in the literature and some dedicated error-detection techniques have been proposed to foil them. The proposed techniques are ad hoc ones and exploit specific properties of the cryptographic algorithms. In this paper, we propose a general framework for error detection in symmetric ciphers based on an operation-centered approach. We first enumerate the arithmetic and logic operations included in the cipher and analyze the efficacy and hardware complexity of several error-detecting codes for each such operation. We then recommend an error-detecting code for the cipher as a whole based on the operations it employs. We also deal with the trade-off between the frequency of checking for errors and the error coverage. We demonstrate our framework on a representative group of 11 symmetric ciphers. Our conclusions are supported by both analytical proofs and extensive simulation experiments",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118667,no,undetermined,0
Systematic t-Unidirectional Error-Detecting Codes over Zm,"Some new classes of systematic t-unidirectional error-detecting codes over Z<sub>m</sub> are designed. It is shown that the constructed codes can detect two errors using two check digits. Furthermore, the constructed codes can detect up to m<sup>r-2</sup> + r-2 errors using r ges 3 check bits. A bound on the maximum number of detectable errors using r check digits is also given.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118666,no,undetermined,0
Assessment of Code Quality through Classification of Unit Tests in VeriNeC,"Unit testing is a tool for assessing code quality. Unit tests check the correctness of code fragments like methods, loops and conditional statements. Usually, every code fragment is involved in different tests. We propose a classification of tests, depending on the tested features, which delivers a higher detailed feedback than unclassified tests. Unclassified tests only deliver a feedback whether they failed or succeeded. The detailed feedback from the classified tests help to do a better code quality assessment and can be incorporated in tools helping to improve code quality. We demonstrate the power of this approach doing unit tests on network configuration.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221056,no,undetermined,0
Making Embedded Software Development More Efficient with SOA,"SOA has been one of the most fascinating design paradigms for enterprise-level application software during the recent years. Key to its success has been the inherent support of reusability and scalability. This has brought forward significant advancements in the efficiency of SOA based software during development, deployment and runtime. As a result of the ongoing increase of computational power on embedded devices, and the ever-increasing connectivity of these, SOA has become relevant also for devices with medium computational capabilities like WiFi routers. Extrapolation suggests that SOA will soon be seen on typical embedded systems like sensors and actuators. In this paper we make a survey to outline the potential of SOA to become a key factor in embedded software development. We believe that by embracing this paradigm, current obstacles in the embedded development process can be addressed more effectively, leading to an efficient and less error prone design flow. Although some efforts in this direction have already been made, there are still areas open for research in order to optimize the development process for embedded SOA.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221179,no,undetermined,0
Code Generation on Steroids: Enhancing COTS Code Generators via Generative Aspects,"Commercial of-the-shelf (COTS) code generators have become an integral part of modern commercial software development. Programmers use code generators to facilitate many tedious and error-prone software development tasks including language processing, XML data binding, graphical component creation, and middleware deployment. Despite the convenience offered by code generators, the generated code is not always adequate for the task at hand. This position paper proposes an approach to address this problem. We utilize the power of aspect oriented programming (AOP) to enhance the functionality of generated code. Furthermore, our approach enables the programmer to specify these enhancements through an intuitive graphical interface. Our proof of concept software tool provides event-handling aspect/aspects that enhance the functionality of the XML processing classes automatically generated by a commercial of- the-shelf code generator, Castor.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273234,no,undetermined,0
Distance Relay With Out-of-Step Blocking Function Using Wavelet Transform,Out-of-step blocking function in distance relays is required to distinguish between a power swing and a fault. Speedy and reliable detection of symmetrical faults during power swings presents a challenge. This paper introduces wavelet transform to reliably and quickly detect power swings as well as detect any fault during a power swing. The total number of dyadic wavelet levels of voltage/current waveforms and the choice of particular levels for such detection are carefully studied. A logic block based on the wavelet transform is developed. The output of this block is combined with the output of the conventional digital distance relay to achieve desired performance during power swings. This integrated relay is extensively tested on a simulated system using PSCAD/ EMTDC<sup>reg</sup> software.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4265653,no,undetermined,0
Recovering Workflows from Multi Tiered E-commerce Systems,"A workflow is a computerized specification of a business process. A workflow describes how tasks are executed and ordered following business policies. E-commerce systems implement the workflows of the daily operations of an organization. Organizations must continuously modify their e-commerce systems in order to accommodate workflow changes. However, e-commerce systems are often designed and developed without referring to the workflows. Modifying e-commerce systems is a time consuming and error prone task. In order to correctly perform this task, developers require an in-depth understanding of multi tiered e-commerce systems and the workflows that they implement. In this paper, we present an approach which automatically recovers workflows from three tier e-commerce systems. Given the starting UI page of a particular workflow, the approach traces the flow of control throughout the different tiers of the e-commerce system in order to recover that workflow. We demonstrate the effectiveness of our approach through experiments on an open source e-commerce system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4268254,no,undetermined,0
Differentiation of Wireless and Congestion Losses in TCP,"TCP is the most commonly used data transfer protocol. It assumes every packet loss to be congestion loss and reduces the sending rate. This will decrease the sender's throughput when there is an appreciable rate of packet loss due to link error and not due to congestion. This issue is significant for wireless links. We present an extension of TCP-Casablanca, which improves TCP performance over wireless links. A new discriminator is proposed that not only differentiates congestion and wireless losses, but also identifies the congestion level in the network, i.e., whether the network is lightly congested or heavily congested and throttles the sender's rate according to the congestion level in the network.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4268134,no,undetermined,0
Impact of Retransmission Delays on Multilayer Video Streaming over IEEE 802.1le Wireless Networks,"In this paper, we seek to establish probabilistic bounds of retransmission delays for transporting multilayer video frames over IEEE 802.11e QAP/QSTA with enhanced MAC distributed coordination function (EDCF). We consider an end-to-end multilayer video streaming that uses hybrid FEC/ARQ error detection and control. Under multiple priority levels of IEEE 802.11e MAC EDCF, we first establish steady-state collision probabilities and contention resolution delays, given the number of nodes. We introduce a time-varying Rayleigh slow-fading channel error model and studying its effect on MAC EDCF transmissions. For video transmissions, we model the expected waiting time of EDCF MAC video queue using head-of-line (HOL) priority queueing discipline using the MAC delay distribution derived earlier as service distribution. The total MAC EDCF video (base layer) queueing delay is the sum of expected waiting time of high-priority voice frames, service residual of best-effort data and the expected waiting time of video frames at HOL queue. Next, we model video retransmission events at receiver as renewal-reward process of frame(s) identified for retransmission to establish the """"spread""""-time between successful renewal events. The """"spread""""-time is indeed the probabilistic retransmission bound that we seek for a single video frame identified for retransmission. We verify our model and analytical bounds using an in-house multimedia mobile communication platform (MMCP), written entirely in software to study the cross-layer interworking between MAC and transport for IEEE 802.11 and 802.11e MAC. MMCP currently supports MPEG4 single-layer and FGS two-layer with concurrent voice and video streaming capabilities. Our model, when combined with a receiver-based channel feedback, can yield a jitter-free, rate-adaptive and guaranteed """"base"""" video quality.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4268053,no,undetermined,0
An Empirical Study into Use of Dependency Injection in Java,"Over the years many guidelines have been offered as to how to achieve good quality designs. We would like to be able to determine to what degree these guidelines actually help. To do that, we need to be able to determine when the guidelines have been followed. This is often difficult as the guidelines are often presented as heuristics or otherwise not completely specified. Nevertheless, we believe it is important to gather quantitative data on the effectiveness of design guidelines wherever possible. In this paper, we examine the use of """"dependency injection"""", which is a design principle that is claimed to increase software design quality attributes such as extensibility, modifiability, testability, and reusability. We develop operational definitions for it and analysis techniques for detecting its use. We demonstrate these techniques by applying them to 34 open source Java applications.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483212,no,undetermined,0
On Modeling and Developing Self-Healing Web Services Using Aspects,"Like any computing application, Web services are subject to failure and unavailability due to multiple reasons like Web service faulty-code and unreliable communication-infrastructure. A manual correction of Web services failure is error-prone and time-consuming. An effective Web services environment should be able to monitor its state, diagnosis faults, and automatically recover from failures. This process is known as self-healing. In this paper, we address self-healing issues of Web services using Aspect-Oriented Programming (AOP). AOP supports separation of self-healing concerns from Web services code and promotes maintenance and reusability.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4267981,no,undetermined,0
Checklist Based Reading's Influence on a Developer's Understanding,"This paper addresses the influence the checklist based reading inspection technique has on a developer's ability to modify inspected code. Traditionally, inspections have been used to detect defects within the development life cycle. This research identified a correlation between the number of defects detected and the successful code extensions for new functionality unrelated to the defects. Participants reported that having completed a checklist inspection, modifying the code was easier because the inspection had given them an understanding of the code that would not have existed otherwise. The results also showed a significant difference in how developers systematically modified code after completing a checklist inspection when compared to those who had not performed a checklist inspection. This study has shown that applying software inspections for purposes other than defect detection include software understanding and comprehension.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483238,no,undetermined,0
Automated Usability Testing Using HUI Analyzer,"In this paper, we present an overview of HUI Analyzer, a tool intended for automating usability testing. The tool allows a user interface's expected and actual use to be captured unobtrusively, with any mismatches indicating potential usability problems being highlighted. HUI Analyzer also supports specification and checking of assertions governing a user interface's layout and actual user interaction. Assertions offer a low cost means of detecting usability defects and are intended to be checked iteratively during a user interface's development. Hotspot analysis is a feature that highlights the relative use of GUI components in a form. This is useful in informing form layout, for example to collocate heavily used components thereby reducing unnecessary scrolling or movement. Based on evaluation, we have found HUI Analyzer's performance in detecting usability defects to be comparable to conventional formal user testing. However the time taken by HUI Analyzer to automatically process and analyze user interactions is much less than that for formal user testing.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483248,no,undetermined,0
Probabilistic Field Coverage using a Hybrid Network of Static and Mobile Sensors,"Providing field coverage is a key issue in many sensor network applications. For a field with unevenly distributed static sensors, a quality coverage with acceptable network lifetime is often difficult to achieve. We propose a hybrid network that consists of both static and mobile sensors, and we suggest that it can be a cost-effective solution for held coverage. The main challenges of designing such a hybrid network are, first, determining necessary coverage contributions from each type of sensors; and second, scheduling the sensors to achieve the desired coverage contributions, which includes activation scheduling for static sensors and movement scheduling for mobile sensors. In this paper, we offer an analytical study on the above problems, and the results also lead to a practical system design. Specifically, we present an optimal algorithm for calculating the contributions from different types of sensors, which fully exploits the potentials of the mobile sensors and maximizes the network lifetime. We then present a random walk model for the mobile sensors. The model is distributed with very low control overhead. Its parameters can be fine-tuned to match the moving capability of different mobile sensors and the demands from a broad spectrum of applications. A node collaboration scheme is then introduced to further enhance the system performance. We demonstrate through analysis and simulation that, in our hybrid design, a small set of mobile sensors can effectively address the uneven distribution of the static sensors and significantly improve the coverage quality.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4262452,no,undetermined,0
The Reduction of Simulation Software Execution Time for Models of Integrated Electric Propulsion Systems through Partitioning and Distribution,"Software time-domain simulation models are useful to the naval engineering community both for the system design of future vessels and for the in-service support of existing vessels. For future platforms, the existence of a model of the vessel's electrical power system provides a means of assessing the performance of the system against defined requirements. This could be at the stage of requirements definition, bid assessment or any subsequent stage in the design process. For in-service support of existing platforms, the existence of a model of the vessel's electrical power system provides a means of assessing the possible cause and effect of operational defects reported by ship's staff, or of assessing the possible future implications of some change in the equipment line-up or operating conditions for the vessel. Detailed high fidelity time-domain simulation of systems, however, can be problematic due to extended execution time. This arises from the model's mathematically stiff nature: models of Integrated Electric Propulsion systems can also require significant computational resource. A conventional time-domain software simulation model is only able to utilize a single computer processor at any one time. The duration of time required to obtain results from a software model could be significantly reduced if more computer processors were utilized simultaneously. This paper details the development of a distributed simulation environment. This environment provides a mechanism for partitioning a time-domain software simulation model and running it on a cluster of computer processors. The number of processors utilized in the cluster ranges between four and sixteen nodes. The benefit of this approach is that reductions in simulation duration are achievable by an appropriate choice of model partitioning. From an engineering perspective, any net timing reduction translates to an increase in the availability of data, from which more efficient analysis and design follows.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4233799,no,undetermined,0
A Straightforward Approach to Introduce FDC-Methods for Wet-Process-Equipment,"In this paper the benefits from introducing FDC- methods for the monitoring of wet-chemistry batch- processors will be outlined. As an example, a typical spray-process equipment shall be used to demonstrate the benefits of using fault detection and classification. These include monitoring of process parameters and frequency of equipment alarms and as result indicating the current state of the equipment. Batch processors usually have a high throughput and in the case of wet-chemistry equipment the process result may be monitored only indirectly. This makes the knowledge about the state of the equipment obligatory for minimizing scrap. Aim of FDC-methods is detecting tool degrading through recording and interpreting the process-parameters during every run and, if necessary, triggering preventive maintenance. Process engineers, maintenance engineers and the equipment operator need to have all the information they need for their decisions delivered in a way they can easily access. To achieve this, Infineon Technologies has implemented various software-tools which all share the same date-base.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4259262,no,undetermined,0
Integrating RTL IPs into TLM Designs Through Automatic Transactor Generation,"Transaction Level Modeling (TLM) is an emerging design practice for overcoming increasing design complexity. It aims at simplifying the design flow of embedded systems by designing and verifying a system at different abstraction levels. In this context, transactors play a fundamental role since they allow communication between the system components, implemented at different abstraction levels. Reuse of RTL IPs into TLM systems is a meaningful example of key advantage guaranteed by exploiting transactors. Nevertheless, transactors implementation is still manual, tedious and error-prone, and the effort spent to verify their correctness often overcomes the benefits of the TLM-based design flow. In this paper we present a methodology to automatically generate transactors for RTL IPs. We show how the transactor code can be automatically generated by exploiting the testbench of any RTL IP.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484653,no,undetermined,0
Thermal Balancing Policy for Streaming Computing on Multiprocessor Architectures,"As feature sizes decrease, power dissipation and heat generation density exponentially increase. Thus, temperature gradients in multiprocessor systems on chip (MPSoCs) can seriously impact system performance and reliability. Thermal balancing policies based on task migration have been proposed to modulate power distribution between processing cores to achieve temperature flattening. However, in the context of MPSoC for multimedia streaming computing, where timeliness is critical, the impact of migration on quality of service must be carefully analyzed. In this paper we present the design and implementation of a lightweight thermal balancing policy that reduces on-chip temperature gradients via task migration. This policy exploits run-time temperature and load information to balance the chip temperature. Moreover, we assess the effectiveness of the proposed policy for streaming computing architectures using a cycle-accurate thermal-aware emulation infrastructure. Our results using a real-life software defined radio multitask benchmark show that our policy achieves thermal balancing while keeping migration costs bounded.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484766,no,undetermined,0
A digital signal processing approach for modulation quality assessment in WiMAX systems,"Modulation quality in WiMAX systems that rely on OFDM modulation is dealt with. A digital signal processing approach is, in particular, proposed, aimed at assessing the performance of transmitters in terms of standard parameters. At present, WiMAX technology deployment is at the very beginning. Also, measurement instrumentation mandated to assist WiMAX apparatuses production and installation is not completely mature, and entitled to be significantly improved in terms of functionality and performance. In particular, no dedicated instrument is already present on the market, but the available solutions are arranged complementing existing hardware, such as real-time spectrum analyzers and vector signal analyzers, with a proper analysis software. Differently from the aforementioned solutions, the proposed approach is independent of the specific hardware platform mandated to the demodulation of the incoming WiMAX signal. It can operate, in fact, with any hardware capable of achieving and delivering the baseband I and Q components of the signal under analysis. Moreover, being open-source it can be improved or upgraded according to future needs.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258506,no,undetermined,0
Software Protection Mechanisms for Dependable Systems,"We expect that in future commodity hardware will be used in safety critical applications. But the used commodity microprocessors will become less reliable because of decreasing feature size and reduced power supply. Thus software-implemented approaches to deal with unreliable hardware will be required. As one basic step to software- implemented hardware-fault tolerance (SIHFT) we aim at providing failure virtualization by turning arbitrary value failures caused by erroneous execution into crash failures which are easier to handle. Existing SIHFT approaches either are not broadly applicable or lack the ability to reliably deal with permanent hardware faults. In contrast, Forin [7] introduced the Vital Coded Microprocessor which reliably detects transient and permanent hardware errors but is not applicable to arbitrary programs and requires special hardware. We discuss different approaches to generalize Forin's approach and make it applicable to modern infrastructures.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484802,no,undetermined,0
Fault Tolerant Signal Processing for Masking Transient Errors in VLSI Signal Processors,"This paper proposes fault tolerant signal processing strategies for achieving reliable performance in VLSI signal processors that are prone to transient errors due to increasingly smaller feature dimensions and supply voltages. The proposed methods are based on residue number system (RNS) coding, involving either hardware redundancy or multiple execution redundancy (MER) strategies designed to identify and overcome transient errors. RNS techniques provide powerful low-redundancy fault tolerance properties that must be introduced at VLSI design levels, whereas MER strategies generally require higher degrees of redundancy that can be introduced at software programming levels.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4253202,no,undetermined,0
A Divergence-measure Based Classification Method for Detecting Anomalies in Network Traffic,"We present 'D-CAD,' a novel divergence-measure based classification method for anomaly detection in network traffic. The D-CAD method identifies anomalies by performing classification on features drawn from software sensors that monitor network traffic. We compare the performance of the D-CAD method with two classifier based anomaly detection methods implemented using supervised Bayesian estimation and supervised maximum-likelihood estimation. Results show that the area under receiver operating characteristic curve (AUC) of the D-CAD method is as high as 0.9524, compared to an AUC value of 0.9102 of the supervised maximum-likelihood estimation based anomaly detection method and to an AUC value of 0.8887 of the supervised Bayesian estimation based anomaly detection method.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4239021,no,undetermined,0
Efficient Belief Propagation for Vision Using Linear Constraint Nodes,"Belief propagation over pairwise connected Markov random fields has become a widely used approach, and has been successfully applied to several important computer vision problems. However, pairwise interactions are often insufficient to capture the full statistics of the problem. Higher-order interactions are sometimes required. Unfortunately, the complexity of belief propagation is exponential in the size of the largest clique. In this paper, we introduce a new technique to compute belief propagation messages in time linear with respect to clique size for a large class of potential functions over real-valued variables. We demonstrate this technique in two applications. First, we perform efficient inference in graphical models where the spatial prior of natural images is captured by 2 times 2 cliques. This approach shows significant improvement over the commonly used pairwise-connected models, and may benefit a variety of applications using belief propagation to infer images or range images. Finally, we apply these techniques to shape-from-shading and demonstrate significant improvement over previous methods, both in quality and in flexibility.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270119,no,undetermined,0
Experiments with Analogy-X for Software Cost Estimation,"We developed a novel method called Analogy-X to provide statistical inference procedures for analogy- based software effort estimation. Analogy-X is a method to statistically evaluate the relationship between useful project features and target features such as effort to be estimated, which ensures the dataset used is relevant to the prediction problem, and project features are selected based on their statistical contribution to the target variables. We hypothesize that this method can be (1) easily applied to a much larger dataset, and (2) also it can be used for incorporating joint effort and duration estimation into analogy, which was not previously possible with conventional analogy estimation. To test these two hypotheses, we conducted two experiments using different datasets. Our results show that Analogy-X is able to deal with ultra large datasets effectively and provides useful statistics to assess the quality of the dataset. In addition, our results show that feature selection for duration estimation differs from feature selection for joint-effort duration estimation. We conclude Analogy-X allows users to assess the best procedure for estimating duration given their specific requirements and dataset.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483211,no,undetermined,0
Using Model-Driven Development in Time-Constrained Course Projects,"Educational software development processes, used in course projects, must exercise practices and artifacts comparable to similar industry-level processes, while achieving acceptable productivity and quality, and, at the same time, complying with constraints on available student time. Here, we discuss our experience with a specific model-driven development process, applied in a time-constrained software engineering course. The course projects are developed in iterations, each delivering a subset of the product functions. These, specified as use cases, undergo a sequence of model transformations, until they become tested code. Transformation steps are verified using standardized quality gates (inspections, tests, and audits), which serve three purposes: teaching verification, validation and quality assurance; helping to assess and grade projects; and providing feedback for process improvement. Size, effort and defect data is recorded in standardized reports. Collected data show that the quality gates proved effective to ensure compliance with the prescribed process, and that using a balanced reusable framework is necessary to achieve satisfactory productivity and quality.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271599,no,undetermined,0
Statistical QoS Guarantee and Energy-Efficiency in Web Server Clusters,"In this paper we study the soft real-time web cluster architecture needed to support e-commerce and related applications. Our testbed is based on an industry standard, which defines a set of Web interactions and database transactions with their deadlines, for generating real workload and bench-marking e-commerce applications. In these soft real-time systems, the quality of service (QoS) is usually defined as the fraction of requests that meet the deadlines. When this QoS is measured directly, regardless of whether the request missed the deadline by an epsilon amount of time or by a large difference, the result is always the same. For this reason, only counting the number of missed requests in a period avoids the observation of the real state of the system. Our contributions are theoretical propositions of how to control the QoS, not measuring the QoS directly, but based on the probability distribution of the tardiness in the completion time of the requests. We call this new QoS metric tardiness quantile metric (TQM). The proposed method provides fine-grained control over the QoS so that we can make a closer examination of the relation between QoS and energy efficiency. We validate the theoretical results showing experiments in a multi-tiered e-commerce web cluster implemented using only open-source software solutions.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271683,no,undetermined,0
Momentum-Based Motion Detection Methodology for Handoff in Wireless Networks,"This paper presents a novel motion detection scheme by using the momentum of received signal sstrength (MRSS) to improve the quality of handoff in a general wireless network. MRSS can detect the motion state of a mobile node (MN) without assistance of any positioning service. Although MRSS is sensitive in detecting user's motion, it is static and fails to detect quickly the motion changes of users. Thus, a novel motion state dependent MRSS scheme called dynamic MRSS (DMRSS) algorithm is proposed to address this issue. Extensive simulation experiments were conducted to study performance of our presented algorithms. The simulation results show that MRSS and DMRSS can be used to assist a handoff algorithm in substantially reducing unnecessary handoff and saving power.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4482789,no,undetermined,0
On the Quality of Service of Crash-Recovery Failure Detectors,"In this paper, we study and model a crash-recovery target and its failure detector's probabilistic behavior. We extend quality of service (QoS) metrics to measure the recovery detection speed and the proportion of the detected failures of a crash-recovery failure detector. Then the impact of the dependability of the crash-recovery target on the QoS bounds for such a crash-recovery failure detector is analysed by adopting general dependability metrics such as MTTF and MTTR. In addition, we analyse how to estimate the failure detector's parameters to achieve the QoS from a requirement based on Chen's NFD-S algorithm. We also demonstrate how to execute the configuration procedure of this crash-recovery failure detector. The simulations are based on the revised NFD-S algorithm with various MTTF and MTTR. The simulation results show that the dependability of a recoverable monitored target could have significant impact on the QoS of such a failure detector and match our analysis results.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273025,no,undetermined,0
A Reinforcement Learning Approach to Automatic Error Recovery,"The increasing complexity of modern computer systems makes fault detection and localization prohibitively expensive, and therefore fast recovery from failures is becoming more and more important. A significant fraction of failures can be cured by executing specific repair actions, e.g. rebooting, even when the exact root causes are unknown. However, designing reasonable recovery policies to effectively schedule potential repair actions could be difficult and error prone. In this paper, we present a novel approach to automate recovery policy generation with reinforcement learning techniques. Based on the recovery history of the original user-defined policy, our method can learn a new, locally optimal policy that outperforms the original one. In our experimental work on data from a real cluster environment, we found that the automatically generated policy can save 10% of machine downtime.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273024,no,undetermined,0
Uniformity by Construction in the Analysis of Nondeterministic Stochastic Systems,"Continuous-time Markov decision processes (CTMDPs) are behavioral models with continuous-time, nondeterminism and memoryless stochastics. Recently, an efficient timed reachability algorithm for CTMDPs has been presented, allowing one to quantify, e. g., the worst-case probability to hit an unsafe system state within a safety critical mission time. This algorithm works only for uniform CTMDPs -- CTMDPs in which the sojourn time distribution is unique across all states. In this paper we develop a compositional theory for generating CTMDPs which are uniform by construction. To analyze the scalability of the method, this theory is applied to the construction of a fault-tolerant workstation cluster example, and experimentally evaluated using an innovative implementation of the timed reachability algorithm. All previous attempts to model-check this seemingly well-studied example needed to ignore the presence of nondeterminism, because of lacking support for modelling and analysis.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273023,no,undetermined,0
Experimental Risk Assessment and Comparison Using Software Fault Injection,"One important question in component-based software development is how to estimate the risk of using COTS components, as the components may have hidden faults and no source code available. This question is particularly relevant in scenarios where it is necessary to choose the most reliable COTS when several alternative components of equivalent functionality are available. This paper proposes a practical approach to assess the risk of using a given software component (COTS or non-COTS). Although we focus on comparing components, the methodology can be useful to assess the risk in individual modules. The proposed approach uses the injection of realistic software faults to assess the impact of possible component failures and uses software complexity metrics to estimate the probability of residual defects in software components. The proposed approach is demonstrated and evaluated in a comparison scenario using two real off-the-shelf components (the RTEMS and the RTLinux real-time operating system) in a realistic application of a satellite data handling application used by the European Space Agency.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273002,no,undetermined,0
A Tunable Add-On Diagnostic Protocol for Time-Triggered Systems,"We present a tunable diagnostic protocol for generic time-triggered (TT) systems to detect crash and send/receive omission faults. Compared to existing diagnostic and membership protocols for TT systems, it does not rely on the single-fault assumption and tolerates malicious faults. It runs at the application level and can be added on top of any TT system (possibly as a middleware component) without requiring modifications at the system level. The information on detected faults is accumulated using a penalty/reward algorithm to handle transient faults. After a fault is detected, the likelihood of node isolation can be adapted to different system configurations, including those where functions with different criticality levels are integrated. Using actual automotive and aerospace parameters, we experimentally demonstrate the transient fault handling capabilities of the protocol.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272968,no,undetermined,0
Dependability Assessment of Grid Middleware,"Dependability is a key factor in any software system due to the potential costs in both time and money a failure may cause. Given the complexity of grid applications that rely on dependable grid middleware, tools for the assessment of grid middleware are highly desirable. Our past research, based around our fault injection technology (FIT) framework and its implementation, WS-FIT, has demonstrated that network level fault injection can be a valuable tool in assessing the dependability of traditional Web services. Here we apply our FIT framework to globus grid middleware using grid-FIT, our new implementation of the FIT framework, to obtain middleware dependability assessment data. We conclude by demonstrating that grid-FIT can be applied to globus grid systems to assess dependability as part of a fault removal mechanism and thus allow middleware dependability to be increased.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272963,no,undetermined,0
Analysis of Timing Requirements for Intrusion Detection System,"An intrusion detection system (IDS) is a collection of sensors (often in the form of mobile agents) that collect data (security related events), classify them and trigger an alarm when unwanted manipulations to regular network behaviour is detected. Activities of attackers and network are time dependent. In the paper, fault trees with time dependencies (FTTD) are used to describe intrusions with emphasis put on timing properties. In FTTD, events and gates are characterized by time parameters. FTTD are used in verification whether the IDS reacts sufficiently quick on the intrusions. As an example, """"the victim trusts the intruder"""" attack is analysed.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272920,no,undetermined,0
Error Recovery Problems,"The paper deals with the problem of handling detected faults in computer systems. We present software procedures targeted at fault detection, fault masking and error recovery. They are discussed in the context of standard PC Windows and Linux environments. Various aspects of checkpointing and recovery policies are studied. The presented considerations are illustrated with some experimental results obtained in our fault injection testbench.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272919,no,undetermined,0
Evaluation of MDA/PSM database model quality in the context of selected non-functional requirements,"Conceptual, logical, and physical database models can be regarded as PIM, PSM<sub>1</sub>, and PSM<sub>2</sub> data models within MDA architecture, respectively. Many different logical database models can be derived from a given conceptual database model by applying a set of transformations rules. To choose a logical database model for further transformation (to physical database model at PSM<sub>2</sub> level) some selection criteria based on quality demands, e.g. database efficiency, easy maintainability or portability, should be established. To evaluate quality of the database models some metrics should be provided. We present metrics for measuring two selected, conflicted quality characteristics (efficiency and maintainability), next we analyse correlation between the metrics, and finally propose how to assess quality of logical database models.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272887,no,undetermined,0
Evaluating the Combined Effect of Vulnerabilities and Faults on Large Distributed Systems,"On large and complex distributed systems hardware and software faults, as well as vulnerabilities, exhibit significant dependencies and interrelationships. Being able to assess their actual impact on the overall system dependability is especially important. The goal of this paper is to propose a unifying way of describing a complex hardware and software system, in order to assess the impact of both vulnerabilities and faults by means of the same underlying reasoning mechanism, built on a standard Prolog inference engine. Some preliminary experimental results show that a prototype tool based on these techniques is both feasible and able to achieve encouraging performance levels on several synthetic test cases.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272886,no,undetermined,0
SOSRAID-6: A Self-Organized Strategy of RAID-6 in the Degraded Mode,"The distinct benefit of RAID-6 is that it provides higher reliability than the other RAID levels for tolerating double disk failures. Whereas, when a disk fails the read/write operations on the failed disk will be redirected to all the surviving disks, which will increase the burden of the surviving disks, the probability of the disk failure and the energy consumption along with the degraded performance issue. In this paper, we present a Self-Organized Strategy (SOS) to improve the performance of RAID-6 in the degraded mode. SOS organizes the data on the failed disks to the corresponding parity locations on first access. Then the later accesses to the failed disks will be redirected to the parity locations rather than all the surviving disks. Besides the performance improvement the SOSRAID-6 reduces the failure probability of the survived disks and is more energy efficient compared with the Traditional RAID-6. With the theoretical evaluation we find that the SOSRAID-6 is more powerful than the TRAID-6.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4482907,no,undetermined,0
Visualization of Growth Curve Data from Phenotype Microarray Experiments,"Phenotype microarrays provide a technology to simultaneously survey the response of an organism to nearly 2,000 substrates, including carbon, nitrogen and potassium sources; varying pH; varying salt concentrations; and antibiotics. In order to more quickly and easily view and compare the large number of growth curves produced by phenotype microarray experiments, we have developed software to produce and display color images, each of which corresponds to a set of 96 growth curves. Using color images to represent growth curves data has proven to be a valuable way to assess experiment quality, compare replicates, facilitate comparison of the responses of different organisms, and identify significant phenotypes. The color images are linked to traditional plots of growth versus time, as well as to information about the experiment, organism, and substrate. In order to share and view information and data project-wide, all information, plots, and data are accessible using only a Web browser.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272032,no,undetermined,0
Distributed algorithm for change detection in satellite images for Grid Environments,"This paper presents a solution for real-time satellite image processing. The focus is on the detection of changes in MODIS images. We present a distributed algorithm for change detection which is based on extracting relevant parameters from MODIS spectral bands. The algorithm detects the changes between two images of the same geographical area at different time moments. The algorithm, able to run in a Grid system, is scalable, fault-tolerant. We present the experimental results of this algorithm considering three spectral bands and different input images. We also propose a method to integrate applications based on this algorithm into the MedioGRID architecture.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271931,no,undetermined,0
Toward the Use of Automated Static Analysis Alerts for Early Identification of Vulnerability- and Attack-prone Components,"Extensive research has shown that software metrics can be used to identify fault- and failure-prone components. These metrics can also give early indications of overall software quality. We seek to parallel the identification and prediction of fault- and failure-prone components in the reliability context with vulnerability- and attack-prone components in the security context. Our research will correlate the quantity and severity of alerts generated by source code static analyzers to vulnerabilities discovered by manual analyses and testing. A strong correlation may indicate that automated static analyzers (ASA), a potentially early technique for vulnerability identification in the development phase, can identify high risk areas in the software system. Based on the alerts, we may be able to predict the presence of more complex and abstract vulnerabilities involved with the design and operation of the software system. An early knowledge of vulnerability can allow software engineers to make informed risk management decisions and prioritize redesign, inspection, and testing efforts. This paper presents our research objective and methodology.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271764,no,undetermined,0
Standard Tools for Hardware-in-the-Loop (HIL) Modeling and Simulation,"This paper presents a cost effective hardware-in-the-loop (HIL) environment integrated into a Simulink real-time simulation. Using an inexpensive national instruments data acquisition card in conjunction with Simulink real-time simulator, a Schweitzer engineering laboratory (SEL) relay was integrated into a Simulink power system fault transient simulation. The sequence of events, from the fault initialization to the breaker opening, was captured through the Simulink model. During the simulation, the relay was fed generator terminal voltages and currents from the Simulink simulation. When the relay detects a fault condition, it sends an open breaker signal back to the simulation. The simulation opens the virtual breaker until the fault is cleared and the open-breaker signal from the relay ceases.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4233811,no,undetermined,0
Simulation of Multi-Speed Vehicular Communication with Code Division Multiple Access,"In this paper, a mathematical model of vehicular communication UMTS (Universal Mobile Telecommunications System) is implemented in the software package MATLAB 6. The mathematical modeling allows the quality factor of the used code to be predicted. The accuracy of implementation is demonstrated by performing a sample simulation with a generator of narrow-band noises. The model remains sufficiently simple and efficient.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4233287,no,undetermined,0
Efficient Top-k Query Evaluation on Probabilistic Data,"Modern enterprise applications are forced to deal with unreliable, inconsistent and imprecise information. Probabilistic databases can model such data naturally, but SQL query evaluation on probabilistic databases is difficult: previous approaches have either restricted the SQL queries, or computed approximate probabilities, or did not scale, and it was shown recently that precise query evaluation is theoretically hard. In this paper we describe a novel approach, which computes and ranks efficiently the top-k answers to a SQL query on a probabilistic database. The restriction to top-k answers is natural, since imprecisions in the data often lead to a large number of answers of low quality, and users are interested only in the answers with the highest probabilities. The idea in our algorithm is to run in parallel several Monte-Carlo simulations, one for each candidate answer, and approximate each probability only to the extent needed to compute correctly the top-k answers.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221737,no,undetermined,0
On Sufficiency of Mutants,"Mutation is the practice of automatically generating possibly faulty variants of a program, for the purpose of assessing the adequacy of a test suite or comparing testing techniques. The cost of mutation often makes its application infeasible. The cost of mutation is usually assessed in terms of the number of mutants, and consequently the number of """"mutation operators"""" that produce them. We address this problem by finding a smaller subset of mutation operators, called """"sufficient"""", that can model the behaviour of the full set. To do this, we provide an experimental procedure and adapt statistical techniques proposed for variable reduction, model selection and nonlinear regression. Our preliminary results reveal interesting information about mutation operators.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222686,no,undetermined,0
Determining Configuration Probabilities of Safety-Critical Adaptive Systems,"This article presents a novel technique to calculate the probability that an adaptive system assumes a configuration. An important application area of dynamic adaptation is the cost-efficient development of dependable embedded systems. Dynamic adaptation exploits implicitly available redundancy, reducing the need for hardware redundancy, to make systems more available, reliable, survivable and, ultimately, more safe. Knowledge of configuration probabilities of a system is an essential requirement for the optimization of safety efforts in development. In perspective, it is also a prerequisite for dependability assessment. Our approach is based on a modeling language for complex reconfiguration behavior. We transform the adaptation model into a probabilistic target model that combines a compositional fault tree with Markov chains. This hybrid model can be evaluated efficiently using a modified BDD-based algorithm. The approach is currently being implemented in an existing reliability modeling tool.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4224161,no,undetermined,0
Interaction Analysis of Heterogeneous Monitoring Data for Autonomic Problem Determination,"Autonomic systems require continuous self-monitoring to ensure correct operation. Available monitoring data exists in a variety of formats, including log files, performance counters, traces, and state and configuration parameters. Such heterogeneity, together with the extremely large volume of data that could be collected, makes analysis very complex. To allow for more-effective problem determination, there is a need for a comprehensive integration of management data. In addition, monitoring should be adaptive to the current perceived operation of the system. In this paper we present an architecture to meet the above goals. We leverage an open-source XML-based format for data integration and describe an approach to automatically adjust monitoring for diagnosis when anomalies are detected. We have implemented a partial prototype using an Eclipse-based open-source platform. We show the effectiveness of our prototype based on fault-injection experiments. We also study issues of disparity of data formats, information overload, scalability, and automated problem determination.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4224159,no,undetermined,0
A Bayesian network based trust model for improving collaboration in mobile ad hoc networks,"Functioning as fully decentralised distributed systems, without the need of predefined infrastructures, mobile ad hoc networks provide interesting solutions when setting up dynamic and flexible applications. However, these systems also bring up some problems. In such open environments, it is difficult to discover among the nodes, which are malicious and which are not, in order to be able to choose good partners for cooperation. One solution for this to be possible, is for the entities to be able to evaluate the trust they have in each other and, based on this trust, determine which entities they can cooperate with. In this paper, we present a trust model adapted to ad hoc networks and, more generally, to distributed systems. This model is based on Bayesian networks, a probabilistic tool which provides a flexible means of dealing with probabilistic problems involving causality. The model evaluates the trust in a server according, both, to direct experiences with the server and recommendations concerning its service. We show, through a simulation, that the proposed model can determine the best server out of a set of eligible servers offering a given service. Such a trust model, when applied to ad hoc networks, tends to increase the QoS of the various services used by a host. This, when applied to security related services thus increases the overall security of the hosts.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4223066,no,undetermined,0
Languages for Safety-Critical Software: Issues and Assessment,"Safety-critical systems (whose anomalous behavior could have catastrophic consequences such as loss of human life) are becoming increasingly prevalent; standards such as DO-178B, originally developed for the certification of commercial avionics, are attracting attention in other communities. The requirement to comply with such standards imposes constraints (on quality assurance, traceability, etc.) much beyond what is typical for Commercial-Off-The-Shelf Software. One of the major decisions that affects the development of safety-critical software is the choice of programming language(s). Specific language features, either by their presence of absence, may make certification easier or harder. Indeed, full genera-lpurpose languages are almost always too complex, and restricted subsets are required. This tutorial compares several languages currently in use or under consideration for safety-critical systems --C (and also C++), Ada, and Java -- and assesses them with respect to their suitability to be constrained for use for such purposes. It specifically examines the MISRA C subset, SPARK, and the in-progress effort to develop a safety-critical profile of the Real-Time Specification for Java. The tutorial also identifies the challenges that Object Oriented Programming imposes on safety certification and indicates possible future directions.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222735,no,undetermined,0
A Quality-Driven Approach to Enable Decision-Making in Self-Adaptive Software,"Self-adaptive software is a closed-loop system aims at altering itself in response to changes at runtime. Such a system, normally, requires monitoring, detecting (analyzing), deciding (planning), and acting (effecting) processes to fulfill adaptation requirements. This research mainly focuses on developing a quality-driven framework to facilitate realizing the deciding process. The framework is required to capture goals of adaptation, utility information, and domain characteristics in a knowledge-base.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222701,no,undetermined,0
Fault-Based Web Services Testing,"Web services are considered a new paradigm for building software applications that has many advantages over the previous paradigms; however, Web services are still not widely used because Service Requesters do not trust Web services that were built by others. Testing can participate in solving this problem because it can be used to assess the quality attributes of Web services and hence increase the requesters' trustworthiness. This paper proposes an approach that can be used to test the robustness and other related attribute of Web services, and that can be easily enhanced to assess other quality attributes. The framework is based on rules for test case generation that are designed by, firstly, analyzing WSDL document to know what faults could affect the robustness quality attribute of Web services, and secondly, using the fault-based testing techniques to detect such faults. A proof of concept tool that depends on these rules has been implemented in order to assess the usefulness of the rules in detecting robustness faults in different Web services platforms.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4492524,no,undetermined,0
Adaptive Probabilistic Model for Ranking Code-Based Static Analysis Alerts,"Software engineers tend to repeat mistakes when developing software. Automated static analysis tools can detect some of these mistakes early in the software process. However, these tools tend to generate a significant number of false positive alerts. Due to the need for manual inspection of alerts, the high number of false positives may make an automated static analysis tool too costly to use. In this research, we propose to rank alerts generated from automated static analysis tools via an adaptive model that predicts the probability an alert is a true fault in a system. The model adapts based upon a history of the actions the software engineer has taken to either filter false positive alerts or fix true faults. We hypothesize that by providing this adaptive ranking, software engineers will be more likely to act upon highly ranked alerts until the probability that remaining alerts are true positives falls below a subjective threshold.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222694,no,undetermined,0
DECIMAL and PLFaultCAT: From Product-Line Requirements to Product-Line Member Software Fault Trees,"PLFaultCAT is a tool for software fault tree analysis (SFTA) during product-line engineering. When linked with DECIMAL, a product-line requirements verification tool, the enhanced version of PLFaultCAT provides traceability between product- line requirements and SFTA hazards as well as semi-automated derivation of the SFTA for each new product-line system previously verified by DECIMAL. The combined tool reduces the effort needed to safely reuse requirements and customize the product-line SFTA as each new system is constructed.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222675,no,undetermined,0
Abnormal Process Condition Prediction (Fault Diagnosis) Using G2 Expert System,"Abnormal operating conditions (faults) in industrial processes have the potential to cause loss of production, loss of life and/or damage to environment. The accidents, which could cost industry billons of dollars per year, can be prevented if abnormal process condition is predicted and controlled in advance. Due to the increased process complexity and instability in operating conditions, the existing control system may have a limited ability to provide practical assistance to both operators and engineers. Advanced software applications, based on expert system, has the potential to assist engineers in monitoring, detecting, diagnosing abnormal condition and thus providing safe guards against unexpected process conditions.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4233034,no,undetermined,0
STRADA: A Tool for Scenario-Based Feature-to-Code Trace Detection and Analysis,"Software engineers frequently struggle with understanding the relationships between the source code of a system and its requirements or high-level features. These relationships are commonly referred to as trace links. The creation and maintenance of trace links is a largely manual, time-consuming, and error- prone process. This paper presents STRADA (Scenario-based TRAce Detection and Analysis) - a tool that helps software engineers explore traces links to source code through testing. While testing is predominantly done to ensure the correctness of a software system, STRADA demonstrates a vital secondary benefit: by executing source code during testing it can be linked to requirements and features, thus establishing traceability automatically.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222671,no,undetermined,0
Enhancing Software Testing by Judicious Use of Code Coverage Information,"Recently, tools for the analysis and visualization of code coverage have become widely available. At first glance, their value in assessing and improving the quality of automated test suites seems to be obvious. Yet, experimental studies as well as experience from projects in industry indicate that their use is not without pitfalls. We found these tools in a number of recent projects quite beneficial. Therefore, we set out to gather code coverage information from one of these projects. In this experience report, first the system under scrutiny as well as our methodology is described. Then, four major questions concerning the impact and benefits of using these tools are discussed. Furthermore, a list of ten lessons learned is derived. The list may help developers judiciously use code coverage tools, in order to reap a maximum of benefits.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222622,no,undetermined,0
Company-Wide Implementation of Metrics for Early Software Fault Detection,"To shorten time-to-market and improve customer satisfaction, software development companies commonly want to use metrics for assessing and improving the performance of their development projects. This paper describes a measurement concept for assessing how good an organization is at finding faults when most cost-effective, i. e. in most cases early. The paper provides results and lessons learned from applying the measurement concept widely at a large software development company. A major finding was that on average, 64 percent of all faults found would have been more cost effective to find during unit tests. An in-depth study of a few projects at a development unit also demonstrated how to use the measurement concept for identifying which parts in the fault detection process that needs to be improved to become more efficient (e.g. reduce the amount of time spent on rework).",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222617,no,undetermined,0
Predicting Faults from Cached History,"We analyze the version history of 7 software systems to predict the most fault prone entities and files. The basic assumption is that faults do not occur in isolation, but rather in bursts of several related faults. Therefore, we cache locations that are likely to have faults: starting from the location of a known (fixed) fault, we cache the location itself, any locations changed together with the fault, recently added locations, and recently changed locations. By consulting the cache at the moment a fault is fixed, a developer can detect likely fault-prone locations. This is useful for prioritizing verification and validation resources on the most fault prone files or entities. In our evaluation of seven open source projects with more than 200,000 revisions, the cache selects 10% of the source code files; these files account for 73%-95% of faults - a significant advance beyond the state of the art.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222610,no,undetermined,0
Algorithm-Based Fault Tolerance for Fail-Stop Failures,"Fail-stop failures in distributed environments are often tolerated by checkpointing or message logging. In this paper, we show that fail-stop process failures in ScaLAPACK matrix-matrix multiplication kennel can be tolerated without checkpointing or message logging. It has been proved in previous algorithm-based fault tolerance that, for matrix-matrix multiplication, the checksum relationship in the input checksum matrices is preserved at the end of the computation no mater which algorithm is chosen. From this checksum relationship in the final computation results, processor miscalculations can be detected, located, and corrected at the end of the computation. However, whether this checksum relationship can be maintained in the middle of the computation or not remains open. In this paper, we first demonstrate that, for many matrix matrix multiplication algorithms, the checksum relationship in the input checksum matrices is not maintained in the middle of the computation. We then prove that, however, for the outer product version algorithm, the checksum relationship in the input checksum matrices can be maintained in the middle of the computation. Based on this checksum relationship maintained in the middle of the computation, we demonstrate that fail-stop process failures (which are often tolerated by checkpointing or message logging) in ScaLAPACK matrix-matrix multiplication can be tolerated without checkpointing or message logging.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4492768,no,undetermined,0
A Technique for Enabling and Supporting Debugging of Field Failures,"It is difficult to fully assess the quality of software in- house, outside the actual time and context in which it will execute after deployment. As a result, it is common for software to manifest field failures, failures that occur on user machines due to untested behavior. Field failures are typically difficult to recreate and investigate on developer platforms, and existing techniques based on crash reporting provide only limited support for this task. In this paper, we present a technique for recording, reproducing, and minimizing failing executions that enables and supports in- house debugging of field failures. We also present a tool that implements our technique and an empirical study that evaluates the technique on a widely used e-mail client.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222588,no,undetermined,0
Assessment of Package Cohesion and Coupling Principles for Predicting the Quality of Object Oriented Design,"In determining the quality of design two factors are important, namely coupling and cohesion. This paper highlights the principles of package architecture from cohesion and coupling point of view and discusses the method for extracting metric associated with them. The method is supported with the help of case study. The results arrived at from the case study are discussed further for utilizing them for predicting the quality of software.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221858,no,undetermined,0
An Integrated Framework for Assessing and Mitigating Risks to Maritime Critical Infrastructure,"Maritime security poses daunting challenges of protecting thousands of potential targets, fixed and mobile, dispersed across US coasts and waterways. We describe an integrated approach to systematically assessing risks and prospective strategies for mitigating those risks. The maritime security risk analysis model (MSRAM) quantifies maritime risk in terms of threats, vulnerabilities, and consequences. We are extending MSRAM with novel """"what-if"""" behavioral simulation software that projects the likely reduction of risks over time from adopting strategies for allocating existing security assets and investing in new security capabilities. Projected outcomes (and costs) can then be compared to identify the most robust strategies for mitigating risk. This decision support method can be re-applied over time as strategies are executed, to re-validate and adjust them in response to changing conditions and terrorist behaviors. This dynamic portfolio-based approach improves confidence, consistency, and quality of risk management decisions. It is extensible beyond the maritime domain to address other critical risk areas in homeland security.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4227817,no,undetermined,0
RF2ID: A Reliable Middleware Framework for RFID Deployment,"The reliability of RFID systems depends on a number of factors including: RF interference, deployment environment, configuration of the readers, and placement of readers and tags. While RFID technology is improving rapidly, a reliable deployment of this technology is still a significant challenge impeding widespread adoption. This paper investigates system software solutions for achieving a highly reliable deployment that mitigates inherent unreliability in RFID technology. We have developed (1) a virtual reader abstraction to improve the potentially error-prone nature of reader generated data (2) a novel path abstraction to capture the logical flow of information among virtual readers. We have designed and implemented an RFID middleware: RF<sup>2</sup>ID (reliable framework for radio frequency identification) to organize and support queries over data streams in an efficient manner. Prototype implementation using both RFID readers and simulated readers using an empirical model of RFID readers show that RF<sup>2</sup>ID is able to provide high reliability and support path-based object detection.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4227984,no,undetermined,0
Fast Failure Detection in a Process Group,"Failure detectors represent a very important building block in distributed applications. The speed and the accuracy of the failure detectors is critical to the performance of the applications built on them. In a common implementation of failure detector based on heartbeats, there is a tradeoff between speed and accuracy so it is difficult to be both fast and accurate. Based on the observation that in many distributed applications, one process takes a special role as the leader, we propose a fast failure detection (FFD) algorithm that detects the failure of the leader both fast and accurately. Taking advantage of spatial multiple timeouts, FFD detects the failure of the leader within a time period of just a little more than one heartbeat interval, making it almost the fastest detection algorithm possible based on heartbeat messages. FFD could be used stand alone in a static configuration where the leader process is fixed at one site. In a dynamic setting, where the role of leader has to be assumed by another site if the current leader fails, FFD could be used in collaboration with a leader election algorithm to speed up the process of electing a new leader.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228024,no,undetermined,0
Identifying and Addressing Uncertainty in Architecture-Level Software Reliability Modeling,"Assessing reliability at early stages of software development, such as at the level of software architecture, is desirable and can provide a cost-effective way of improving a software system's quality. However, predicting a component's reliability at the architectural level is challenging because of uncertainties associated with the system and its individual components due to the lack of information. This paper discusses representative uncertainties which we have identified at the level of a system's components, and illustrates how to represent them in our reliability modeling framework. Our preliminary evaluation indicates promising results in our framework's ability to handle such uncertainties.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228252,no,undetermined,0
Using Maintainability Based Risk Assessment and Severity Analysis in Prioritizing Corrective Maintenance Tasks,"A software product spends more than 65% of its lifecycle in maintenance. Software systems with good maintainability can be easily modified to fix faults. We define maintainability-based risk as a product of two factors: the probability of performing maintenance tasks and the impact of performing these tasks. In this paper, we present a methodology for assessing maintainability-based risk in the context of corrective maintenance. The proposed methodology depends on the architectural artifacts and their evolution through the life cycle of the system. In order to prioritize corrective maintenance tasks, we combine components' maintainability- based risk with the severity of a failure that may happen as a result of unfixed fault. We illustrate the methodology on a case study using UML models.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4230939,no,undetermined,0
Construct Metadata Model based on Coupling Information to Increase the Testability of Component-based Software,"A software component must be tested every time it is reused, to guarantee the quality of both the component itself and the system in which it is to be integrated. So how to increase testability of component has become a key technology in the software engineering community. This paper introduces a method to increase component testability. Firstly we analyze the meanings of component testability and the effective ways to increase testability. Then we give some definitions on component coupling testing criterion. And we further give the definitions of DU-I(definition-use information) and OP- Vs (observation-point values). Base on these, we introduce a definition-use table, which includes DU-I and OP-Vs item, to help component testers understanding and observing the component better. Then a framework of testable component based on above DU-table is given. These facilities provide ways to detect errors, to observe state variables by observation-points based monitor mechanism. And we adopt coupling-based testing using information DU-table provided. Lastly, we applied the method to our application software developed before, and generate some test cases. And our method is compared with Orso method and Kan method using the same example, presenting the comparison results. The relevant results illustrate the validity of our method, effectively generating test cases and killing more mutants.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4230935,no,undetermined,0
An Approach for Specification-based Test Case Generation for Web Services,"Web services applications are built by the integration of many loosely coupled and reusable services using open standards. Testing Web service is important in detecting faults and assessing quality attributes. A difficulty in testing Web services applications is the unavailability of the source code for both the application builder and the broker. This paper propose a solution to this problem by providing a formal, specification-based approach for automatically generating test cases for web services based on the WSDL input messages parts' XML Schema datatypes. Examples of using this approach are then given in order to give evidence of its usefulness. The role of the application builders and the brokers in using this approach to test Web services is also described.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4230934,no,undetermined,0
Local and Global Recency Weighting Approach to Bug Prediction,"Finding and fixing software bugs is a challenging maintenance task, and a significant amount of effort is invested by software development companies on this issue. In this paper, we use the Eclipse project's recorded software bug history to predict occurrence of future bugs. The history contains information on when bugs have been reported and subsequently fixed.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228670,no,undetermined,0
Predicting Defects and Changes with Import Relations,Lowering the number of defects and estimating the development time of a software project are two important goals of software engineering. To predict the number of defects and changes we train models with import relations. This enables us to decrease the number of defects by more efficient testing and to assess the effort needed in respect to the number of changes.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228668,no,undetermined,0
Identifying Changed Source Code Lines from Version Repositories,"Observing the evolution of software systems at different levels of granularity has been a key issue for a number of studies, aiming at predicting defects or at studying certain phenomena, such as the presence of clones or of crosscutting concerns. Versioning systems such as CVS and SVN, however, only provide information about lines added or deleted by a contributor: any change is shown as a sequence of additions and deletions. This provides an erroneous estimate of the amount of code changed. This paper shows how the evolution of changes at source code line level can be inferred from CVS repositories, by combining information retrieval techniques and the Levenshtein edit distance. The application of the proposed approach to the ArgoUML case study indicates a high precision and recall.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228651,no,undetermined,0
Spam Filter Based Approach for Finding Fault-Prone Software Modules,"Because of the increase of needs for spam e-mail detection, the spam filtering technique has been improved as a convenient and effective technique for text mining. We propose a novel approach to detect fault-prone modules in a way that the source code modules are considered as text files and are applied to the spam filter directly. In order to show the applicability of our approach, we conducted experimental applications using source code repositories of Java based open source developments. The result of experiments shows that our approach can classify more than 75% of software modules correctly.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228641,no,undetermined,0
CiCUTS: Combining System Execution Modeling Tools with Continuous Integration Environments,"System execution modeling (SEM) tools provide an effective means to evaluate the quality of service (QoS) of enterprise distributed real-time and embedded (DRE) systems. SEM tools facilitate testing and resolving performance issues throughout the entire development life-cycle, rather than waiting until final system integration. SEM tools have not historically focused on effective testing. New techniques are therefore needed to help bridge the gap between the early integration capabilities of SEM tools and testing so developers can focus on resolving strategic integration and performance issues, as opposed to wrestling with tedious and error-prone low-level testing concerns. This paper provides two contributions to research on using SEM tools to address enterprise DRE system integration challenges. First, we evaluate several approaches for combining continuous integration environments with SEM tools and describe CiCUTS, which combines the CUTS SEM tool with the CruiseControl.NET continuous integration environment. Second, we present a case study that shows how CiCUTS helps reduce the time and effort required to manage and execute integration tests that evaluate QoS metrics for a representative DRE system from the domain of shipboard computing. The results of our case study show that CiCUTS helps developers and testers ensure the performance of an example enterprise DRE system is within its QoS specifications throughout development, instead of waiting until system integration time to evaluate QoS.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4492388,no,undetermined,0
Semantic Dependencies and Modularity of Aspect-Oriented Software,"Modularization of crosscutting concerns is the main benefit provided by aspect-oriented constructs. In order to rigorously assess the overall impact of this kind of modularization, we use design structure matrixes (DSMs) to analyze different versions (OO and AO) of a system. This is supported by the concept of semantic dependencies between classes and aspects, leading to a more faithful notion of coupling for AO systems. We also show how design rules can make those dependencies explicit and, consequently, yield a more modular design.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228633,no,undetermined,0
Assessing Module Reusability,"We propose a conceptual framework for assessing the reusability of modules. To do so, we define reusability of a module as the product of its functionality and its applicability. We then generalize the framework to the assessment of modularization techniques.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228629,no,undetermined,0
An Evolutionary Approach to Software Modularity Analysis,"Modularity determines software quality in terms of evolvability, changeability, maintainability, etc. and a module could be a vertical slicing through source code directory structure or class boundary. Given a modularized design, we need to determine whether its implementation realizes the designed modularity. Manually comparing source code modular structure with abstracted design modular structure is tedious and error-prone. In this paper, we present an automated approach to check the conformance of source code modularity to the designed modularity. Our approach uses design structure matrices (DSMs) as a uniform representation; it uses existing tools to automatically derive DSMs from the source code and design, and uses a genetic algorithm to automatically cluster DSMs and check the conformance. We applied our approach to a small canonical software system as a proof of concept experiment. The results supported our hypothesis that it is possible to check the conformance between source code structure and design structure automatically, and this approach has the potential to be scaled for use in large software systems.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228628,no,undetermined,0
Towards Assessing Modularity,"It's noted in this workshop's call for papers that despite the emergence of a large number of """"modularisation techniques"""" (e.g., aspects, design patterns, and so on), there are no standard approaches or """"rules of thumb"""" for assessing the benefits and drawbacks of using these techniques in the construction of real software systems. In this paper we argue that the first step in assessing such techniques should be to determine their effect on modularity. Only then can we be sure that they have even been correctly classified as """"modularisation techniques"""".",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228625,no,undetermined,0
Implementing Adaptive Performance Management in Server Applications,"Performance and scalability are critical quality attributes for server applications in Internet-facing business systems. These applications operate in dynamic environments with rapidly fluctuating user loads and resource levels, and unpredictable system faults- Adaptive (autonomic) systems research aims to augment such server applications with intelligent control logic that can detect and react to sudden environmental changes. However, developing this adaptive logic is complex in itself. In addition, executing the adaptive logic consumes processing resources, and hence may (paradoxically) adversely effect application performance. In this paper we describe an approach for developing high-performance adaptive server applications and the supporting technology. The Adaptive Server Framework (ASF) is built on standard middleware services, and can be used to augment legacy systems with adaptive behavior without needing to change the application business logic. Crucially, ASF provides built-in control loop components to optimize the overall application performance, which comprises both the business and adaptive logic. The control loop is based on performance models and allows systems designers to tune the performance levels simply by modifying high level declarative policies. We demonstrate the use of ASF in a case study.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228612,no,undetermined,0
A Probabilistic Approach to Measuring Robustness in Computing Systems,"System builders are becoming increasingly interested in robust design. We believe that a methodology for generating robustness metrics helps the robust design research efforts and, in general, is an important step in the efforts to create robust computing systems. The purpose of the research in this paper is to quantify the robustness of a resource allocation, with the eventual objective of setting a standard that could easily be instantiated for a particular computing system to generate robustness metric. We present our theoretical foundation for robustness metric and give its instantiation for a particular system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4228308,no,undetermined,0
Software Configuration Management for Product Derivation in Software Product Families,"A key process in software product line (SPL) engineering is product derivation, which is the process of building software products from a base set of core assets. During product derivation, the components in both core assets and derived software products are modified to meet needs for different functionality, platforms, quality attributes, etc. However, existing software configuration management (SCM) systems do not sufficiently support the derivation process in SPL. In this paper, we introduce a novel SCM system that is well-suited for product derivation in SPL. Our tool, MoSPL handles version management at the component level via its product versioning and data models. It explicitly manages logical constraints and derivation relations among components in both core assets and derived products, thus enabling the automatic propagation of changes in the core assets to their copies in derived products and vice versa. The system can also detect conflicting changes to different copies of components in software product lines.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4492408,no,undetermined,0
ATP-Based Automated Fault Simulation,"As a free Electromagnetic Transient (EMT) simulation program, the Alternative Transient Program (ATP) cannot simulate in batch mode. Manual operation is very boring and prone to error when thousands of faults are to be simulated. In order to automate the process, based on close observation and analysis of the operation mechanism of the ATP, the letter concludes some useful rules and develops a software package to automate the ATP-based EMT simulation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529108,no,undetermined,0
Hardening XDS-Based Architectures,"Healthcare is an information-intensive domain and therefore information technologies are playing an ever-growing role in this sector. They are expected to increase the efficiency of the delivery of healthcare services in order to both improve the quality and reduce the costs. In this context, security has been identified as a priority although several gaps still exist. This paper reports on the results of assessing the threats to XDS-based architectures. Accordingly, an architectural solution to the identified threats is presented.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529316,no,undetermined,0
FEDC: Control Flow Error Detection and Correction for Embedded Systems without Program Interruption,"This paper proposes a new technique called CFEDC to detect and correct control flow errors (CFEs) without program interruption. The proposed technique is based on the modification of application software and minor changes in the underlying hardware. To demonstrate the effectiveness of CFEDC, it has been implemented on an OpenRISC 1200 as a case study. Analytical results for three workload programs show that this technique detects all CFEs and corrects on average about 81.6% of CFEs. These figures are achieved with zero error detection /correction latency. According to the experimental results, the overheads are generally low as compared to other techniques; the performance overhead and the memory overhead are on average 8.5% and 9.1%, respectively. The area overhead is about 4% and the power dissipation increases by the amount of 1.5% on average.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529318,no,undetermined,0
Extending CSCM to support Interface Versioning,"Software component has been a main stream technology used to tackle issues such as software reuse, software quality and, software development complexity. In spite of the proliferation of component models (CORBA, .Net, JavaBeans), certain issues and limitations inherent to components are still not addressed adequately. For instance, composing software components especially those provided by different suppliers may result in faulty behavior. This behavior might be the result of incompatibilities between aging components and/or freshly released components and their respective interfaces. This paper, present an approach to tackle component interface incompatibilities via the use of a component and interface versioning scheme. This approach is designed as an extension to the compositional structured component model (CSCM), an ongoing research project. The implementation of this extension makes use of code annotations to provide interface versioning information useful in detecting interface incompatibilities",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4078924,no,undetermined,0
Blocking vs. Non-Blocking Coordinated Checkpointing for Large-Scale Fault Tolerant MPI,"A long-term trend in high-performance computing is the increasing number of nodes in parallel computing platforms, which entails a higher failure probability. Fault programming environments should be used to guarantee the safe execution of critical applications. Research in fault tolerant MPI has led to the development of several fault tolerant MPI environments. Different approaches are being proposed using a variety of fault tolerant message passing protocols based on coordinated checkpointing or message logging. The most popular approach is with coordinated checkpointing. In the literature, two different concepts of coordinated checkpointing have been proposed: blocking and non-blocking. However they have never been compared quantitatively and their respective scalability remains unknown. The contribution of this paper is to provide the first comparison between these two approaches and a study of their scalability. We have implemented the two approaches within the MPICH environments and evaluate their performance using the NAS parallel benchmarks",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4090192,no,undetermined,0
An Intelligent Error Detection Model for Reliable QoS Constraints Running on Pervasive Computing,"We propose an intelligence predictive model for reliable QoS constraints running on pervasive computing. FTA is a system that is suitable for detecting and recovering software error based on pervasive computing environment as RCSM(Reconfigurable Context-Sensitive Middleware) by using software techniques. One of the methods to detect error for session's recovery inspects process database periodically. But this method has a weak point of inspecting all processes without regard to session. Therefore, we propose FTA. This method detects error by inspecting by hooking method. If an error is found, FTA informs GSM of the error. GSM informs Daemon or SA-SMA of the error. Daemon creates SA-SMA and so on. SA-SMA creates Video Service Provide Instance and so on.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4089304,no,undetermined,0
The Use of Intra-Release Product Measures in Predicting Release Readiness,"Modern business methods apply micro management techniques to all aspects of systems development. We investigate the use of product measures during the intra-release cycles of an application as a means of assessing release readiness. The measures include those derived from the Chidamber and Kemerer metric suite and some coupling measures of our own. Our research uses successive monthly snapshots during systems re-structuring, maintenance and testing cycles over a two year period on a commercial application written in C++. We examine the prevailing trends which the measures reveal at both component class and application level. By applying criteria to the measures we suggest that it is possible to evaluate the maturity and stability of the application thereby facilitating the project manager in making an informed decision on the application's fitness for release.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539550,no,undetermined,0
Unsupervised Contextual Keyword Relevance Learning and Measurement using PLSA,"In this paper, we have developed a probabilistic approach using PLSA for the discovery and analysis of contextual keyword relevance based on the distribution of keywords across a training text corpus. We have shown experimentally, the flexibility of this approach in classifying keywords into different domains based on their context. We have developed a prototype system that allows us to project keyword queries on the loaded PLSA model and returns keywords that are closely correlated. The keyword query is vectorized using the PLSA model in the reduce aspect space and correlation is derived by calculating a dot product. We also discuss the parameters that control PLSA performance including a) number of aspects, b) number of EM iterations c) weighting functions on TDM (pre-weighting). We have estimated the quality through computation of precision-recall scores. We have presented our experiments on PLSA application towards document classification",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4086258,no,undetermined,0
StegoBreaker: Audio Steganalysis using Ensemble Autonomous Multi-Agent and Genetic Algorithm,"The goal of steganography is to avoid drawing suspicion to the transmission of a hidden message in multi-medium. This creates a potential problem when this technology is misused for planning criminal activities. Differentiating anomalous audio document (stego audio) from pure audio document (cover audio) is difficult and tedious. Steganalytic techniques strive to detect whether an audio contains a hidden message or not. This paper investigates the use of genetic algorithm (GA) to aid autonomous intelligent software agents capable of detecting any hidden information in audio files, automatically. This agent would make up the detection agent in an architecture comprising of several different agents that collaborate together to detect the hidden information. The basic idea is that, the various audio quality metrics (AQMs) calculated on cover audio signals and on stego-audio signals vis-a-vis their denoised versions, are statistically different. GA employs these AQMs to steganalyse the audio data. The overall agent architecture will operate as an automatic target detection (ATD) system. The architecture of ATD system is presented in this paper and it is shown how the detection agent fits into the overall system. The design of ATD based audio steganalyzer relies on the choice of these audio quality measures and the construction of a GA based rule generator, which spawns a set of rules that discriminates between the adulterated and the untouched audio samples. Experimental results show that the proposed technique provides promising detection rates",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4086242,no,undetermined,0
Ant Agent-Based Multicast Routing with QoS Guarantees,"This paper designs a novel ant agent-based multicast routing algorithm with bandwidth and delay guarantees, called QMRA, which works for packet-switching networks where the state information is imprecise. In our scheme, an ant uses the probability that a link satisfies QoS requirements and the cost of a path instead of the ant's trip time or age to determine the amount of pheromone to deposit, so that it has a simpler migration process, less control parameters and can tolerate the imprecision of state information. In this paper, the proof of correctness and complexity analysis of QMRA are given. And that, experimental results show our algorithm can achieve low routing blocking ratio, low average packet delay and fast convergence when the network state information is imprecise",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4079154,no,undetermined,0
Automated Discovery of Human Activities inside Pervasive Living Spaces,"The recognition and detection of human activities constitutes a very important step towards the fulfilment of the notion of pervasive environments. By detecting patterns on those behaviours, an environment can adapt and respond to the inhabitants' needs, thus improving the quality of life. This paper presents a framework in which those ideas can be applied and tested. It includes a system using a temporal neural-network driven embedded agent working with online, real-time data from a network of unobtrusive low-level sensors situated in either a simulated environment or a fully fitted real environment such as a whole flat",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4079091,no,undetermined,0
Harmonization of usability measurements in ISO9126 software engineering standards,"The measurement of software usability is recommended in ISO 9126-2 to assess the external quality of software by allowing the user to test its usefulness before it is delivered to the client. Later, during the operation and maintenance phases, usability should be maintained, otherwise the software will have to be retired. This then raises harmonization issues about the proper positioning of the usability characteristic: does usability really belong to the external quality view of ISO 9126-2 and should the external quality characteristic of usability be harmonized with that of the quality in use model defined in ISO 9126-1 and ISO 9126-4? This paper analyzes these two questions: first, we identify and analyze the subset of ISO 9126-2 quality subcharacteristics and measures of usability that can be useful for quality in use, and then we recommend improvements to the harmonization of these ISO 9126 models",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4078913,no,undetermined,0
TTCN-3 Testing of Hoorn-Kersenboogerd Railway Interlocking,"Railway control systems are safety-critical, so we have to ensure that they are designed and implemented correctly. Testing these systems is a key issue. Prior to system testing, the software of a railway control system is tested separately from the hardware. The interlocking is a layer of railway control systems that guarantees safety. It allows to execute commands given by a user only if they are safe; unsafe commands are rejected. Railway interlockings are central to efficient and safe traffic management for railway infrastructure managers and operators. European integration requires new standards for specification and testing interlockings. Here we propose an approach to testing interlockings with TTCN-3 and give an example for its application. The code of interlockings is simulated during test execution. For assessing the quality of the tests, we propose an approach inspired by the classification tree method",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4055023,no,undetermined,0
The Adaptive Detection And Application Of Weak Signal Based On Stochastic Resonance,"In this paper, we study how to use the stochastic resonance principle to detect the weak intermediate and low frequency signals under the condition of intensive noise in mechanical fault diagnosis. According to the relationship among system outputs, system parameters and input signals, we've studied the algorithm and the technology of the software self-adaptive control of the nonlinear bistable system. Using this method, we can obtain output with adequate Signal-to-Noise ratio (SNR) without solving complex differential equations. The simulation results show the affectivity of this method. This study shows a way to detect the weak signals under the condition of intensive noise",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077796,no,undetermined,0
New Tools for Blackout Prevention,"Recent power system blackouts have heightened the concern for power system security and, therefore, reliability. However, potential security improvements which could be achieved through transmission system facility reinforcement and generation supply expansion are long-term, costly, and uncertain. As a result, more immediate and cost effective solutions to the security issue have been pursued including the development of specialized software tools which can assess power system security in near-real-time and assist operators in maintaining adequate security margins at all times thus lowering the risk of blackouts. Such software tools have been implemented in numerous power systems world-wide and are gaining popularity as a result of demonstrated benefits to system performance",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4075765,no,undetermined,0
Analysis of Distributed Intelligent Agent Model for QoS Dynamic Scheme in GSM/GPRS Network,"In this paper we study dynamic quality of service scheme in GSM/GPRS wireless network. A load balancing architecture constructed by distributed intelligent agent has been presented to support real time or burst data services. Fuzzy neural network was employed to predict GPRS traffic by learning examples. Meanwhile, we have presented a traffic estimation algorithm and a simple decision mechanism to deal with special applications such as burst data transmission. The simulation shows that distributed intelligent agent architecture could significantly reduce packet delay, route cost and relieve GPRS bottleneck",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4072149,no,undetermined,0
Optimal QoS-aware Sleep/Wake Scheduling for Time-Synchronized Sensor Networks,"We study the sleep/wake scheduling problem in the context of clustered sensor networks. We conclude that the design of any sleep/wake scheduling algorithm must take into account the impact of the synchronization error. Our work includes two parts. In the first part, we show that there is an inherent tradeoff between energy consumption and message delivery performance (defined as the message capture probability in this work). We formulate an optimization problem to minimize the expected energy consumption, with the constraint that the message capture probability should be no less than a threshold. In the first part, we assume the threshold is already given. However, by investigating the unique structure of the problem, we transform the non-convex problem into a convex equivalent, and solve it using an efficient search method. In the second part, we remove the assumption that the capture probability threshold is already given, and study how to decide it to meet the quality of services (QoS) requirement of the application. We observe that in many sensor network applications, a group of sensors collaborate to perform common task(s). Therefore, the QoS is usually not decided by the performance of any individual node, but by the collective performance of all the related nodes. To achieve the collective performance with minimum energy consumption, intuitively we should provide differentiated services for the nodes and favor more important ones. We thus formulate an optimization problem, which aims to set the capture probability threshold for messages from each individual node such that the expected energy consumption is minimized, while the collective performance is guaranteed. The problem turns out to be non-convex and hard to solve exactly. Therefore, we use approximation techniques to obtain a suboptimal solution that approximates the optimum. Simulations show that our approximate solution significantly outperforms a scheme without differentiated treatment of the nodes.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4067940,no,undetermined,0
Triple Modular Redundancy with Standby (TMRSB) Supporting Dynamic Resource Reconfiguration,"A fault tolerance model called triple modular redundancy with standby (TMRSB) is developed which combines the two popular fault tolerance techniques of triple modular redundancy (TMR) and standby (SB) fault tolerance. In TMRSB systems, each module of a TMR arrangement has access to several independent standby configurations. When a fault is detected in a module's active configuration, the physical resources within that module are re-mapped to restore the desired fault-free functionality by reconfiguring the resource pool to one of the standby configurations. A mathematic model for TMRSB systems is developed for field programmable gate array (FPGA) logic devices. Simulation of the model was also performed using the BlockSim reliability software tool which takes into account the reconfiguration time overheads and an imperfect switching mechanism. With component time-to-failure following an exponential distribution throughout long mission duration, the range of operation over which TMRSB is superior to a standby system and a TMR system is shown.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062463,no,undetermined,0
Semantic-Based Workflow Composition for Video Processing in the Grid,"We outline the problem of automatic video processing for the EcoGrid. This poses many challenges as there is a vast amount of raw data that need to be analysed effectively and efficiently. Furthermore, ecological data are subject to environmental changes and are exception-prone, hence their qualities vary. As manual processing by humans can be time and labour intensive, video and image processing tools can go some way to addressing such problems since they are computationally fast. However, most video analyses that utilise a combination of these tools are still done manually. We propose a semantic-based hybrid workflow composition method that strives to provide automation to speed up this process. The requirements for such a system are presented, whereby we aim for a solution that best satisfies these requirements and that overcomes the limitations of existing grid workflow composition systems",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4061355,no,undetermined,0
Improvement in Reliability and Energy Yield Prediction of Thin-Film CdS/CdTe PV Modules,"In this work, we illustrate improvement in thin-film PV module durability via process optimization. Data are presented from large installations, allowing accurate estimation of failure rates and distributions of failure modes. Improvement in product quality is also described; we show that recent thin-film products can meet industry expectations for consistent power output. In addition, results of product characterization performed at First Solar are presented. Dependence of module output on irradiance and temperature is illustrated, and we show that common assumptions about such dependence (based on experience with conventional PV technology) may not hold for thin-film modules. Predicted behavior of module output (as computed with PV system modeling software) and real-world data are compared. It is shown that adjustment of the parametric description of the module can be used to successfully reduce discrepancy between predicted and actual module behaviors",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4060096,no,undetermined,0
"Parallel Genomic Sequence-Searching on an Ad-Hoc Grid: Experiences, Lessons Learned, and Implications","The Basic local alignment search tool (BLAST) allows bioinformaticists to characterize an unknown sequence by comparing it against a database of known sequences. The similarity between sequences enables biologists to detect evolutionary relationships and infer biological properties of the unknown sequence. mpiBLAST, our parallel BLAST, decreases the search time of a 300 KB query on the current NT database from over two full days to under 10 minutes on a 128-processor cluster and allows larger query files to be compared. Consequently, we propose to compare the largest query available, the entire NT database, against the largest database available, the entire NT database. The result of this comparison can provide critical information to the biology community, including insightful evolutionary, structural, and functional relationships between every sequence and family in the NT database. Preliminary projections indicated that to complete the task in a reasonable length of time required more processors than were available to us at a single site. Hence, we assembled GreenGene, an ad-hoc grid that was constructed """"on the fly"""" from donated computational, network, and storage resources during last year's SC|05. GreenGene consisted of 3048 processors from machines that were distributed across the United States. This paper presents a case study of mpiBLAST on GreenGene - specifically, a pre-run characterization of the computation, the hardware and software architectural design, experimental results, and future directions",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4090196,no,undetermined,0
Qualitative Modeling for Requirements Engineering,"Acquisition of """"quantitative"""" models of sufficient accuracy to enable effective analysis of requirements tradeoffs is hampered by the slowness and difficulty of obtaining sufficient data. """"Qualitative"""" models, based on expert opinion, can be built quickly and therefore used earlier. Such qualitative models are nondeterminate which makes them hard to use for making categorical policy decisions over the model. The nondeterminacy of qualitative models can be tamed using """"stochastic sampling"""" and """"treatment learning"""". These tools can quickly find and set the """"master variables"""" that restrain qualitative simulations. Once tamed, qualitative modeling can be used in requirements engineering to assess more options, earlier in the life cycle",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4090240,no,undetermined,0
UML Activity Diagram Based Testing of Java Concurrent Programs for Data Race and Inconsistency,"Data race occurs when multiple threads simultaneously access shared data without appropriate synchronization, and at least one is write. System with a data race is nondeterministic and may generate different outputs even with the same input, according to different interleaving of data access. We present a model-based approach for detecting data races in concurrent Java programs. We extend UML Activity diagrams with data operation tags, to model program behavior. Program under test (PUT) is instrumented according to the model. It is then executed with random test cases generated based on path analysis of the model. Execution traces are reverse engineered and used for post-mortem verification. First, data races are identified by searching the time overlaps of entering and exiting critical sections of different threads. Second, implementation could be inconsistent with the design. The problem may tangle with race condition and makes it hard to detect races. We compare the event sequences with the behavior model for consistency checking. Identified inconsistencies help debuggers locate the defects in the PUT. A prototype tool named tocAj implements the proposed approach and was successfully applied to several cases studies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539547,no,undetermined,0
Bio - Inspired & Traditional Approaches to Obtain Fault Tolerance,"Applying some observable phenomena from cells, focused on their organization, function, control and healing mechanisms, a simple fault tolerant implementation can be obtained. Traditionally, fault tolerance has been added explicitly to a system by including redundant hardware and/or software, which takes over when an error has been detected. These concepts and ideas have been applied before with the triple modular redundancy. Our approach is to design systems where redundancy was incorporated implicitly into the hardware and to mix bio-inspired and traditional approaches to deal with fault tolerance. These ideas are shown using a discrete cosine transform (application) as organ, its MAC (function) interconnected as cell and parity redundancy checker (error detector) as immune system to obtain a fault tolerance design",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4099982,no,undetermined,0
Research of Transient Stability Margin Affected by Single-phase Reclosing,"The reliability can be enhanced by using single-phase reclosing, but when reclose to permanent faults, it is another surge on system. So there is necessity to study on the influence of reclosing sequence on transient stability margin. In the case that transient and permanent fault is not effectively identified, the influence of sequence and time of single-phase reclosing on transient stability margin in Southern China Power Grid of 2005 year is studied using software FASTEST when single-phase fault is occurred on 500 kV Anshun - Tianshengqiao transmission line in this paper. Simulation results indicate that when the terminal without fault uses non-voltage detecting and recloses first instead of the two terminals use non-voltage detecting in turn, the transient angle, voltage and frequency stability margin can all be enhanced and the working condition of breaker can be improved. A method combined with fault location algorithm to modify the sequence of single-phase reclosing online to enhance transient stability and lower the surge when reclose to permanent fault is proposed.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4116144,no,undetermined,0
A Transmission Line Unit Protection Technique Based Combination Modulus by Using Mathematical Morphology,This paper presents a concept of combination modulus (CM) to solve the problem that the transient-based protection techniques may fail to detect faults under certain conditions. The CM features are analyzed and discussed in detail. A novel transmission line unit protection scheme is proposed by comparing transient current CM polarity. The mathematical morphology (MM) technique is used to extract the polarity features from fault-generated current wave signals propagating along transmission lines during a post-fault period. The simulation results of the ATP/EMTP software show that the reliability of the protection scheme proposed has been considerably improved.,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4116115,no,undetermined,0
Software Quality in Ladder Programming,"This paper aims to measure the software quality for programmable logic controllers (PLCs) especially in ladder programming. The proposed quality metrics involve the criteria of simplicity, reconfigurability, reliability, and flexibility. A fuzzy inference algorithm is developed to select the best controller design among different ladder programs for the same application. A single tone membership function is used to represent the quality metric per each controller. The fitness of each controller is represented by the minimum value of all evaluated criteria. Thereafter, a min-max fuzzy inference is applied to take the decision (which controller is the best). The developed fuzzy assessment algorithm is applied to a conveyor belt module connected to a PLC to perform a repeated sequence. The decision making to select the best ladder program is obtained using the fuzzy assessment algorithm. The obtained results affirmed the potential of the proposed algorithm to assess the quality of the designed ladder programs",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4115500,no,undetermined,0
Traffic and Network Engineering in Emerging Generation IP Networks: A Bandwidth on Demand Model,"This paper assesses the performance of a network management scheme where network engineering (NE) is used to complement traffic engineering (TE) in a multi-layer setting where a data network is layered above an optical network. We present a TE strategy which is based on a multi-constraint optimization model consisting of finding bandwidth-guaranteed IP tunnels subject to contention avoidance minimization and bandwidth usage maximization constraints. The TE model is complemented by a NE model which uses a bandwidth trading mechanism to rapidly re-size and re-optimize the established tunnels (LSPs/lambdaSPs) under quality of service (QoS) mismatches between the traffic carried by the tunnels and the resources available for carrying the traffic. The resulting TE+NE strategy can be used to achieve bandwidth on demand (BoD) in emerging generation IP networks using a (G)MPLS- like integrated architecture in a cost effective way. We evaluate the performance of this hybrid strategy when routing, re-routing and re-sizing the tunnels carrying the traffic offered to a 23-node test network.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4114884,no,undetermined,0
Service Quality of Information Systems,"Information system development is an expensive process, and usually fall behind the expectations of user and implementation is achieved much later than expected if ever. The designers of information systems and programmers often begin designing and programming the system too early, before they actually understand the users' or stakeholders' requirements. Since designing and programming systems are very expensive, ill-defined requirements cause projects to fall behind schedule and over budget. Correctly assessing customer needs and requirements is very important for information systems development. Over 50% of development errors occur during the requirements analysis phase of the development cycle. In this paper, following the statement of the problem, possible causes are discussed. The conclusion is made by providing some guidelines for the design of information systems that meet or exceed user requirements",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4114676,no,undetermined,0
On the Predictability of Random Tests for Object-Oriented Software,"Intuition suggests that random testing of object-oriented programs should exhibit a significant difference in the number of faults detected by two different runs of equal duration. As a consequence, random testing would be rather unpredictable. We evaluate the variance of the number of faults detected by random testing over time. We present the results of an empirical study that is based on 1215 hours of randomly testing 27 Eiffel classes, each with 30 seeds of the random number generator. Analyzing over 6 million failures triggered during the experiments, the study provides evidence that the relative number of faults detected by random testing over time is predictable but that different runs of the random test case generator detect different faults. The study also shows that random testing quickly finds faults: the first failure is likely to be triggered within 30 seconds.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539534,no,undetermined,0
A Delay Fault Model for At-Speed Fault Simulation and Test Generation,"We describe a transition fault model, which is easy to simulate under test sequences that are applied at-speed, and provides a target for the generation of at-speed test sequences. At-speed test application allows a circuit to be tested under its normal operation conditions. However, fault simulation and test generation for the existing fault models become significantly more complex due to the need to handle faulty signal-transitions that span multiple clock cycles. The proposed fault model alleviates this shortcoming by introducing unspecified values into the faulty circuit when fault effects may occur. Fault detection potentially occurs when an unspecified value reaches a primary output. Due to the uncertainty that an unspecified value propagated to a primary output will be different from the fault free value, an inherent requirement in this model is that a fault would be potentially detected multiple times in order to increase the likelihood of detection. Experimental results demonstrate that the model behaves as expected in terms of fault coverage and numbers of detections of target faults. A variation of an n-detection test generation procedure for stuck-at faults is used for generating test sequences under this model",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4110158,no,undetermined,0
Traffic-aware Stress Testing of Distributed Real-Time Systems Based on UML Models in the Presence of Time Uncertainty,"In a previous work, we reported and experimented with a stress testing methodology to detect network traffic- related real-time (RT) faults in distributed real-time systems (DRTSs) based on the design UML models. The stress methodology, referred to as time-shifting stress test methodology (TSSTM), aimed at increasing chances of discovering RT faults originating from network traffic overloads in DRTSs. The TSSTM uses the UML 2.0 model of a system under test (SUT), augmented with timing information, and is based on an analysis of the control flow in UML sequence diagrams. In order to devise deterministic test requirements (from time point of view) that yield the maximum stress test scenario in terms of network traffic in a SUT, the TSSTM methodology requires that the timing information of messages in sequence diagrams is available and as precise as possible. In reality, however, the timing information of messages is not always available and precise. As we demonstrate using a case study in this work, the effectiveness of the stress test cases generated by TSSTM is very sensitive to such time uncertainty. In other words, TSSTM might generate imprecise and not necessarily maximum stressing test cases in the presence of such time uncertainty and, thus, it might not be very effective in revealing RT faults. To address the above limitation of TSSTM, we present in this article a modified testing methodology which can be used to stress test systems when the timing information of messages is imprecise or unpredictable. The stress test results of applying the new test methodology to a prototype DRTS indicate that, in the presence of uncertainty in timing information of messages, the new methodology is more effective in detecting RT faults when compared to our previous methodology (i.e., TSSTM) and also test cases based on an operational profile.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539536,no,undetermined,0
A New Approach for Induction Motor Broken Bar Diagnosis by Using Vibration Spectrum,"Different methods for detecting broken bars in induction motors can be found in literature. Many of these methods are based on evaluating special frequency magnitudes in machine signals spectrums. Current, power, flux, etc are among these signals. Frequencies related to broken rotor fault are dependent on slip; therefore, correct diagnosis of fault depends on accurate determination of motor velocity and slip. The traditional methods typically require several sensors that should be pre-installed in some cases. A practical diagnosis method should be easily performed in site and does not take too much time. This paper presents a diagnosing method based on only a vibration sensor. Motor velocity oscillation due to broken rotor causes frequency components at twice slip frequency difference around speed frequency in vibration spectrum. Speed frequency and its harmonics as well as twice supply frequency, can easily and accurately be found in vibration spectrum, therefore the motor slip can be computed. Now components related to rotor fault can be found. According to this method, an apparatus consisting necessary hardware and software has been designed. Experimental tests have confirmed the efficiency of the method",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4108512,no,undetermined,0
Development of Defect Classification Algorithm for POSCO Rolling Strip Surface Inspection System,"Surface inspection system (SIS) is an integrated hardware-software system which automatically inspects the surface of the steel strip. It is equipped with several cameras and illumination over and under the steel strip roll and automatically detects and classifies defects on the surface. The performance of the inspection algorithm plays an important role in not only quality assurance of the rolled steel product, but also improvement of the strip production process control. Current implementation of POSCO SIS has good ability to detect defects, however, classification performance is not satisfactory. In this paper, we introduce POSCO SIS and suggest a new defect classification algorithm which is based on support vector machine technique. The suggested classification algorithm shows good classification ability and generalization performance",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4108062,no,undetermined,0
2D Frequency Selective Extrapolation for Spatial Error Concealment in H.264/AVC Video Coding,"The frequency selective extrapolation extends an image signal beyond a limited number of known samples. This problem arises in image and video communication in error prone environments where transmission errors may lead to data losses. In order to estimate the lost image areas, the missing pixels are extrapolated from the available correctly received surrounding area which is approximated by a weighted linear combination of basis functions. In this contribution, we integrate the frequency selective extrapolation into the H.264/AVC coder as spatial concealment method. The decoder reference software uses spatial concealment only for I frames. Therefore, we investigate the performance of our concealment scheme for I frames and its impact on following P frames caused by error propagation due to predictive coding. Further, we compare the performance for coded video sequences in TV quality against the non-normative concealment feature of the decoder reference software. The investigations are done for slice patterns causing chequerboard and raster scan losses enabled by flexible macroblock ordering (FMO).",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4107009,no,undetermined,0
Multiple Description Scalar Quantization Based 3D Mesh Coding,"In this paper, we address the problem of 3D model transmission over error-prone channels using multiple description coding (MDC). The objective of MDC is to encode a source into multiple bitstreams, called descriptions, supporting multiple quality levels of decoding. Compared to layered coding techniques, each description can be decoded independently to approximate the model. In the proposed approach, the mesh geometry is compressed using multiresolution geometry compression. Then multiple descriptions are obtained by applying multiple description scalar quantization (MDSQ) to the obtained wavelet coefficients. Experimental results show that, the proposed approach achieves competitive compression performance compared with existing multiple description methods.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4106589,no,undetermined,0
On the Use of Behavioral Models for the Integrated Performance and Reliability Evaluation of Fault-Tolerant Avionics Systems,"In this paper, the authors propose an integrated methodology for the reliability and performance analysis of fault-tolerant systems. This methodology uses a behavioral model of the system dynamics, similar to the ones used by control engineers when designing the control system, but incorporates additional artifacts to model the failure behavior of the system components. These artifacts include component failure modes (and associated failure rates) and how those failure modes affect the dynamic behavior of the component. The methodology bases the system evaluation on the analysis of the dynamics of the different configurations the system can reach after component failures occur. For each of the possible system configurations, a performance evaluation of its dynamic behavior is carried out to check whether its properties, e.g., accuracy, overshoot, or settling time, which are called performance metrics, meet system requirements. After all system configurations have been evaluated, the values of the performance metrics for each configuration and the probabilities of going from the nominal configuration (no component failures) to any other configuration are merged into a set of probabilistic measures of performance. To illustrate the methodology, and to introduce a tool that the authors developed in MATLAB/SIMULINKreg that supports this methodology, the authors present a case-study of a lateral-directional flight control system for a fighter aircraft",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4106294,no,undetermined,0
A Software Factory for Air Traffic Data,"Modern enterprise architecture requires a flexible, scalable and upgradeable infrastructure that allows communication, and subsequently collaboration, between heterogeneous information processing and computing environments. Heterogeneous systems often use different data representations for the same data items, limiting collaboration. Although this problem is conceptually straightforward, the process of data conversion is error prone, often dramatically underestimated, and surprisingly complex. The complexity is often the result of the non-standard data representations that are used by computing systems in the aviation domain. This paper describes some of the work that is being done by Boeing Advanced Air Traffic Management to address this challenge. A prototype software factory for air traffic data management is being built and evaluated. The software factory provides the capability for a user such as a Systems Engineer or an Air Traffic Domain Expert to create an interface model. The model will allow the user to specify entities such as data items, scaling, units, headers and footers, representation, and coding. The factory automatically creates a machine usable interface. A prototype for a Domain Specific Language to assist in this task is being developed",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4106235,no,undetermined,0
Application of Static Transfer Switch for Feeder Reconfiguration to Improve Voltage at Critical Locations,"The main objective of this work was to assess and evaluate the performance of static transfer switch (STS) for feeder reconfiguration. Two particular network feeders namely preferred and alternate were selected for simulation studies. Both feeders belong to IESCO system (Islamabad Electric Supply Company, Pakistan). The sensitive loads are fed by preferred feeder but in case of disturbances, the loads are transferred to alternate feeder. Different simulation cases were performed for optimum installation of STS to obtain the required voltage quality. The simulations are performed using the PSCAD/EMTDC package",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4104661,no,undetermined,0
"Uniform Crime Report """"SuperClean"""" Data Cleaning Tool","The analysis of UCR data provides a basis for crime prevention in the United States as well as a sound decision making tool for policy makers. The decisions made with the use of UCR data range from major funding for resource allocation down to patrol distribution by local police departments. The FBI collects and maintains the database of the Uniform Crime Reports (UCR), from 18,000 reporting police agencies nationwide. However, many of these data sets have missing, incomplete, or incorrect data points that render crime analysis less effective. UCR experts have stated that in the current form UCR data is unreliable and sporadic. Efforts have previously been made to design a software application to correct these necessary problems, but the application was deemed insufficient due to limited portability and usability. Software requirements restricted potential users and the user interface was ineffective. However, this previous work describes the functions needed to effectively clean and assess UCR data. This paper describes the design of an application used to clean, process, and correct UCR data so that ideal policy decisions can be made. Erroneous portions of the data will be found using the outlier detection function that is based on a statistical model of anomalous behavior. These methods incorporate sponsor specifications and user requirements. This project builds upon the GRASP (geospatial repository for analysis and safety planning) project's goal of sharing information between law enforcement agencies. Eventually this application could be integrated with GRASP to form a single repository for UCR and spatial crime data. This paper describes how the new stand alone application will allow users to clean, correct, and process UCR data in an efficient, user-friendly manner. Formal testing provides a basis to assess the effectiveness of the application based on the metrics of time, cost, and quality. The results are the basis for improvements to the application",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4055105,no,undetermined,0
Security Design Patterns: Survey and Evaluation,"Security design patterns have been proposed recently as a tool for the improvement of software security during the architecture and design phases. Since the appearance of this research topic in 1997, several catalogs have emerged, and the security pattern community has produced significant contributions, with many related to design. In this paper, we survey major contributions in the state of the art in the field of security design patterns and assess their quality in the context of an established classification. From our results, we determined a classification of inappropriate pattern qualities. Using a six sigma approach, we propose a set of desirable properties that would prevent flaws in new design patterns, as well as a template for expressing them",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4055006,no,undetermined,0
Effective RTL Method to Develop On-Line Self-Test Routine for the Processors Using the Wavelet Transform,"In this paper, we introduce a new efficient register transfer level (RTL) method to develop on-line self- test routines. We consider some prioritizations to select the components and instructions of the processor. In addition, we choose test patterns based on spectral RTL test pattern generation (TPG) strategy. For the purpose of spectral analysis, we use the wavelet transform. Also, we use a few extra instructions for the purpose of the signature monitoring to detect control flow errors. We demonstrate that the combination of these three strategies is effective for developing small test programs with high fault coverage in a small test development time. In this case, we only need the instruction set architecture (ISA) and RTL information. Our method not only provides a simple and fast algorithm for on-line self-test applications, also gains the advantages of utilizing lower memory and reducing the test generation time complexities in comparison with proposed methods so far. We focus on the application of this approach for Parwan processor. We develop a self-test routine using our proposed method for Parwan processor and demonstrate the effectiveness of our proposed methodology for on-line testing by presenting experimental results for Parwan processor.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529795,no,undetermined,0
On the Assessment of the Mean Failure Frequency of Software in Late Testing,"We propose an approach for assessing the mean failure frequency of a program, based on the statistical test of hypotheses. The approach can be used to establish stopping rules and evaluate the quality of a program based on its mean failure frequency during the late testing phases. Our proposal shows how to set and satisfy conservative bounds for the minimum number of test executions that are needed to achieve a target mean failure frequency with a specified level of statistical significance, based on the quality goal of testing and the specific test execution profile chosen. We relax a few assumptions of the literature, so our approach can be used in a larger set of real-life cases.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031800,no,undetermined,0
Correctness-preserving synthesis for real-time control software,"Formal theories for real-time systems (such as timed process algebra, timed automata and timed Petri nets) have gained great success in the modelling of concurrent timing behavior and in the analysis of real-time properties. However, due to the ineliminable timing differences between a model and its realization, synthesising a software realization from a model in a correctness-preserving way is still a challenging research topic. In this paper, we tackle this problem by solving a set of sub-problems. First, we introduce property relations between real-time systems on the basis of their absolute and relative timing differences. Second, we bridge the timing differences between a model and its realization by a sequence of (absolute and relative) timing differences. Third, we propose two parameterised hypotheses to capture the timing differences between the model and its realization. The parameters of both hypotheses are used to predict the real-time properties of the realization from those of the model. Finally, we introduce a synthesis tool, which shows that the two hypotheses can be satisfied during software synthesis",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032270,no,undetermined,0
Collaborative Target Detection in Wireless Sensor Networks with Reactive Mobility,"Recent years have witnessed the deployments of wireless sensor networks in a class of mission-critical applications such as object detection and tracking. These applications often impose stringent QoS requirements including high detection probability, low false alarm rate and bounded detection delay. Although a dense all-static network may initially meet these QoS requirements, it does not adapt to unpredictable dynamics in network conditions (e.g., coverage holes caused by death of nodes) or physical environments (e.g., changed spatial distribution of events). This paper exploits reactive mobility to improve the target detection performance of wireless sensor networks. In our approach, mobile sensors collaborate with static sensors and move reactively to achieve the required detection performance. Specifically, mobile sensors initially remain stationary and are directed to move toward a possible target only when a detection consensus is reached by a group of sensors. The accuracy of final detection result is then improved as the measurements of mobile sensors have higher signal-to-noise ratios after the movement. We develop a sensor movement scheduling algorithm that achieves near-optimal system detection performance within a given detection delay bound. The effectiveness of our approach is validated by extensive simulations using the real data traces collected by 23 sensor nodes.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539679,no,undetermined,0
Bootstrapping Performance and Dependability Attributes ofWeb Services,"Web services gain momentum for developing flexible service-oriented architectures. Quality of service (QoS) issues are not part of the Web service standard stack, although non-functional attributes like performance, dependability or cost and payment play an important role for service discovery, selection, and composition. A lot of research is dedicated to different QoS models, at the same time omitting a way to specify how QoS parameters (esp. the performance related aspects) are assessed, evaluated and constantly monitored. Our contribution in this paper comprises: a) an evaluation approach for QoS attributes of Web services, which works completely service-and provider independent, b) a method to analyze Web service interactions by using our evaluation tool and extract important QoS information without any knowledge about the service implementation. Furthermore, our implementation allows assessing performance specific values (such as latency or service processing time) that usually require access to the server which hosts the service. The result of the evaluation process can be used to enrich existing Web service descriptions with a set of up-to-date QoS attributes, therefore, making it a valuable instrument for Web service selection",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032029,no,undetermined,0
A New Intelligent Traffic Shaper for High Speed Networks,"In this paper, a new intelligent traffic shaper is proposed to obtain a reasonable utilization of bandwidth while preventing traffic overload in other part of the network and as a result, reducing total number of packet dropping in the whole network. This approach trains an intelligent agent to learn an appropriate value for token generation rate of a Token Bucket at various states of the network. This method shows satisfactory results in simulations from the aspects of keeping dropping probability low while injecting as many packets as possible into the network by minimization of used buffer size at each router in order to keep the delay occurred by packets waiting in long buffers to be sent, as small as possible",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031945,no,undetermined,0
Using RDL to Facilitate Customization of Variability Points,"Reusable software assets have variability points, which are locations for customization. Reusable asset consumers, i.e. reusers, must become knowledgeable about the techniques used to implement variability, and the activities required to customize the variability points. Moreover reuse activities must be constrained to a specific sequence to avoid a lengthy error-prone reuse process and inconsistencies in the final application. Specifying reuse activities required to customize variability points using a programming language is a valuable contribution to the reuse process, given the way reuse is clearly exposed. Moreover reuse scripts described in such a language can be input to a reuse environment for automatic or semi-automatic assistance to the reuse process. In this work we discuss software reuse activities and illustrate how RDL (Reuse Description Language) can be used to facilitate variability pointAs customization.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031827,no,undetermined,0
Application of Computational Redundancy in Dangling Pointers Detection,"Many programmers manipulate dynamic data improperly, which may produce dynamic memory problems, such as dangling pointer. Dangling pointers can occur when a function returns a pointer to an automatic variable, or when trying to access a deleted object. The existence of dangling pointers causes the programs to behave incorrectly. Dangling pointers are common defect that are easy to commit, but are difficult to discover. In this paper we propose a dynamic approach that detects dangling pointers in computer programs. Redundant computation is an execution of a program statement(s) that does not contribute to the program output. The notion of redundant computation is introduced as a potential indicator of defects in programs. We investigate the application of redundant computation in dangling pointers detection. The results of the initial experiment show that, the redundant computation detection can help the debuggers to localize the source(s) of the dangling pointers. During the experiment, we find that, our approach may be capable of detecting other types of dynamic memory problems such as memory leaks and inaccessible objects, which we plan to investigate in our future research.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031815,no,undetermined,0
A system architecture for collaborative environmental modeling research,"This relates to early stage research that aims to build an integrated toolbox of instruments that can be used for environmental modeling tasks. The application area described is grape growing and wine production. A comparative study including data gathered in both New Zealand and Chile is described. Using both passive and sensor technology data is gathered from atmosphere, vines, and soil. Human sensory perceptions relating to wine taste and quality is also gathered. The project proposes a synthesizer which collects and analyzes data in real time. Computational neural network modeling methods and geographic information systems are used for result depiction. This convergence of computational techniques and information processing methods is proposed as being an example of software and systems collaboration. The project called Eno-Humanas is so named because of the bled of the precise enological data and less qualitative human perception data. It is expected that the discrete input elements of the architecture here will be demonstrably dependency-related and derived from correlation values once data gathering instruments and analytical software have been developed. At this stage of the project, these tools and methods are being built and tested. This is the first stage of the project and the proposed research that will come from it in order to answer wide questions such as the ordinal set of data values necessarily present to predict climate conditions, the relationship between vine sap rise and dew point calibrations, towards addressing the popular question of 'what makes for a good year for wine'. In addition to the bringing together of various technologies, methods and kinds of data, (geo-referential, climatic, atmospheric, terrain, plant biological and qualitative sensory expressions), the paper also describes an international research collaboration and its parameters.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4543910,no,undetermined,0
Configuring Processes and Business Documents - An Integrated Approach to Enterprise Systems Collaboration,"Enterprise systems (ES) provide standardized, off-the-shelf support for operations and management within organizations. With the advent of ES based on a service-oriented architecture (SOA) and an increasing demand of IT-supported interorganizational collaboration, implementation projects face paradigmatically new challenges. The configuration of ES is costly and error-prone. Dependencies between business processes and business documents are hardly explicit and foster component proliferation instead of reuse. Configurative modeling can support the problem in two ways: First, conceptual modeling abstracts from technical details and provides more intuitive access and overview. Second, configuration allows the projection of variants from master models providing manageable variants with controlled flexibility. We aim at tackling the problem by proposing an integrated model-based framework for configuring both, processes and business documents, on an equal basis; as together, they constitute the core business components of an ES",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031697,no,undetermined,0
Software Defect Content Estimation: A Bayesian Approach,"Software inspection is a method to detect errors in software artefacts early in the development cycle. At the end of the inspection process the inspectors need to make a decision whether the inspected artefact is of sufficient quality or not. Several methods have been proposed to assist in making this decision like capture recapture methods and Bayesian approach. In this study these methods have been analyzed and compared and a new Bayesian approach for software inspection is proposed. All of the estimation models rely on an underlying assumption that the inspectors are independent. However, this assumption of independence is not necessarily true in practical sense, as most of the inspection teams interact with each other and share their findings. We, therefore, studied a new Bayesian model where the inspectors share their findings, for defect estimate and compared it with Bayesian models in the literature, where inspectors examine the artefact independently. The simulations were carried out under realistic software conditions with a small number of difficult defects and a few inspectors. The models were evaluated on the basis of decision accuracy and median relative error and our results suggest that the dependent inspector assumption improves the decision accuracy (DA) over the previous Bayesian model and CR models",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4054576,no,undetermined,0
Enabling executable architecture by improving the foundations of DoD architecting,"Architecture is an intrinsic quality or property of a system. It consists of the arrangement and inter-relationships, both static and dynamic, among the components of the system as well as their externally visible properties. We commonly think of this property as the structure or form of the system. Through the practice of architecting, we seek to make apparent the architecture of a system through the creation of architecture descriptions. Architecture descriptions are representations or conceptualizations of the form of a system. In architecting, our goal is to understand, affect, predict, or manage this architecture property in order to achieve other system properties that are dependent upon it. In creating such descriptions, we often employ an architecture framework as a way of conceptualizing the form of the system. A framework consists of a set of assumptions, concepts, values, and practices that constitutes a way of viewing reality. Applying an architecture framework results in creation of a representation of the system that is at least two steps removed from the reality of the system: first, through our interpretation of that reality, and second, through the application of a framework to shape our interpretation. Most of the architecture descriptions produced by DoD architects are static. They portray system properties at a single point in time. However, system properties may in fact change over time due to the interaction of components of the system's architecture via their established relationships. Successfully achieving the goal of creating executable system models at various phases of a system's life cycle is principally dependent upon expressing such models at a sufficient level of formality and characterization.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4543979,no,undetermined,0
Data Processing Workflow Automation in Grid Architecture,"Because of the poor performance and the expensive license cost, traditional relational database management systems are no longer good choices for processing huge amount of data. Grid computing is replacing the place of RDBMS in data processing. Traditionally a workflow is generated by data experts, which is time consuming, labor intensive and error prone. More over, it becomes the bottleneck of the overall performance of data processing in the grid architecture. This paper proposes a multi-layer workflow automation strategy that can automatically generate a workflow from a business language. A prototype has been implemented and a simulation has been designed",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031551,no,undetermined,0
Extending an SSI Cluster for Resource Discovery in Grid Computing,"Grid technologies enable large-scale sharing of resources within formal or informal consortia of individuals and/or virtual organizations. In these settings, the discovery, characterization, and monitoring of resources, services, and computations can be challenging due to the considerable diversity, large numbers, dynamic behavior, and geographical distribution of the entities in which a user might be interested. Hence, information services are a vital part of any grid software infrastructure, providing fundamental mechanisms for discovery and monitoring, and thus for planning and adapting application behavior. This paper proposes a resource discovery system for grid computing with fault-tolerant capabilities starting from an SSI clustering operating system. The proposed system uses dynamic leader-determination and registration mechanisms to automatically recover from nodes and network failures. The system is centralized and uses dynamic (or soft-state) registration to detect and recover from failures. Provisional or backup leader determination provides tolerance and recovery in the event of the leader node failing. The system was tested against a control network modeled after existing grid computing resource discovery components, such as Globus monitoring and discovery system (MDS). In various failure scenarios, the proposed system showed better resilience and performance than the control system",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031470,no,undetermined,0
Formal proofs for QoS-oriented Transformations,"The methodology of Model Driven Architecture (MDA) has been a popular area of research in recent years. To cater for the increasing awareness of the importance in software Quality of Service (QoS), some have suggested MDA as a solution. However, unlike functional properties, QoS displayed in a development cycle is prone to changes after deployment due to a non-constant runtime environment and usage. A possible solution is to provide a monitoring framework to ensure that QoS violations are always detectable. However, as with any MDA based approach it is dangerous to simply assume that transformations will do exactly as specified. This paper describes an approach for producing formal proofs for our particular QoS-oriented transformational system[1], based on the proof-as-programs methodology.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031301,no,undetermined,0
Requirements Traceability and Transformation Conformance in Model-Driven Development,"The variety of design artefacts (models) produced in a model-driven design process results in an intricate relationship between requirements and the various models. This paper proposes a methodological framework that simplifies management of this relationship. This framework is a basis for tracing requirements, assessing the quality of model transformation specifications, metamodels, models and realizations. We propose a notion of conformance between application models which reduces the effort needed for assessment activities. We discuss how this notion of conformance can be integrated with model transformations",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031222,no,undetermined,0
A Model Driven Exception Management Framework for Developing Reliable Software Systems,"Programming languages provide exception handling mechanisms to structure fault tolerant activities into software systems. However, the use of exceptions at this low level of abstraction can be error-prone and complex leading to new programming errors. In this paper, we present a model-driven framework to support the iterative development of reliable software systems. This framework is comprised of UML-based modeling notations and a transformation engine that supports the automated generation of exception management features for a software system. It leverages domain specific exception modeling languages, patterns, modeling tools and framework libraries. The feasibility of this approach is demonstrated through the development of a case study business application, known as Project Tracker",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031218,no,undetermined,0
Worqbench: An Integrated Framework for e-Science Application Development,"With the proliferation of Grid computing, potentially vast computational resources are available for solving complex problems in science and engineering. However, writing, deploying, and testing e-Science applications over highly heterogeneous and distributed infrastructure are complex and error prone. Further complicating matters, programmers may need to target a variety of different Grid middleware packages. This paper presents the design and implementation of Worqbench, an integrated, modular and middleware neutral framework for e- Science application development on the Grid. Worqbench can be incorporated into a number of existing Integrated Development Environments, further leveraging the advantages of such systems. We illustrate one such implementation in the Eclipse environment.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031024,no,undetermined,0
The Role of Stability Testing in Heterogeneous Application Environment,"This paper presents an approach to system stability tests performed in Motorola private radio networks (PRN) department. The stability tests are among crucial elements of the department's testing strategy, such as functional testing, regression testing and stress testing. The gravity of the subject is illustrated with an example of a serious system defect: memory leak, whicht was detected in Solaris operating system during system stability tests. The paper provides technical background essential to understand the problem, as well as emphasizes the role of the tests in the problem solving. The following approaches to memory leaks detection are discussed: code review, memory debugging and system stability tests. The article presents several guidelines on stability test implementation and mentions the crucial elements of the PRN Department testing strategy: load definition, testing period and the system monitoring method.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539571,no,undetermined,0
Quality Assessment of Mutation Operators Dedicated for C# Programs,"The mutation technique inserts faults in a program under test in order to assess or generate test cases, or evaluate the reliability of the program. Faults introduced into the source code are defined using mutation operators. They should be related to different, also object-oriented features of a program. The most research on OO mutations was devoted to Java programs. This paper describes analytical and empirical study performed to evaluate the quality of advanced mutation operators for C# programs. Experimental results demonstrate effectiveness of different mutation operators. Unit tests suites and functional tests were used in experiments. A detailed analysis was conducted on mutation operators dealing with delegates and exception handling",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032289,no,undetermined,0
Generating Optimal Test Set for Neighbor Factors Combinatorial Testing,"Combinatorial testing is a specification-based testing method, which can detect the faults triggered by interaction of factors. For one kind of software in which the interactions only exist between neighbor factors, this paper proposes the concept of neighbor factors combinatorial testing, presents the covering array generation algorithms for neighbor factors pair-wise (N=2) coverage, neighbor factors N-way (Nges2) coverage and variable strength neighbor factors coverage, and proves that the covering arrays generated by these three algorithms are optimal. Finally we analyze an application scenario, which shows that this approach is very practical",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032293,no,undetermined,0
Probabilistic Adaptive Random Testing,"Adaptive random testing (ART) methods are software testing methods which are based on random testing, but which use additional mechanisms to ensure more even and widespread distributions of test cases over an input domain. Restricted random testing (RRT) is a version of ART which uses exclusion regions and restricts test case generation to outside of these regions. RRT has been found to perform very well, but its use of strict exclusion regions (from within which test cases cannot be generated) has prompted an investigation into the possibility of modifying the RRT method such that all portions of the input domain remain available for test case generation throughout the duration of the algorithm. In this paper, we present a probabilistic approach, probabilistic ART (PART), and explain two different implementations. Preliminary empirical data supporting the methods is also examined",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032295,no,undetermined,0
Integrating Processes of Logistics Outsourcing Risk Management in e-Business,"Logistics outsourcing has been recognized to have important potential benefits, including reduced costs, improved quality, the ability to focus on core competencies and access to new technologies. Most prior studies have articulated the advantages of logistics outsourcing and paid little attention to the risks in e-business environments. The main purpose of this study is to present how the current logistics outsourcing risk management process can be integrated and improved through the use of new e-business applications",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053310,no,undetermined,0
A Predictive Method for Providing Fault Tolerance in Multi-agent Systems,"The growing importance of multi-agent applications and the need for a higher quality of service in these systems justify the increasing interest in fault-tolerant multi-agent systems. In this article, we propose an original method for providing dependability in multi- agent systems through replication. Our method is different from other works because our research focuses on building an automatic, adaptive and predictive replication policy where critical agents are replicated to avoid failures. This policy is determined by taking into account the criticality of the plans of the agents, which contain the collective and individual behaviors of the agents in the application. The set of replication strategies applied at a given moment to an agent is then fine-tuned gradually by the replication system so as to reflect the dynamicity of the multi-agent system. We report on experiments assessing the efficiency of our approach.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052925,no,undetermined,0
Verification of Intelligent Agents with ACTL for Epistemic Reasoning,"Verification of multi-agent systems (MAS) is a huge challenge, especially for those systems where security and safety are of major importance. Verification detects faults, defects and drawbacks in an early stage of software development. Here, we give a formal model for verification of MAS by means of model checking technique. We extend the existing action computation tree logic (ACTL) with epistemic operators in order to reason about knowledge properties of MAS. We introduce new operators for manipulation on agent's actions with data. We explain their syntax and semantics for our ACTL-er (ACTL for epistemic reasoning), and provide a case study for a MAS system of foraging bees.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052705,no,undetermined,0
A Best Practice Guide to Resources Forecasting for the Apache Webserver,"Recently, measurement based studies of software systems proliferated, reflecting an increasingly empirical focus on system availability, reliability, aging and fault tolerance. However, it is a non-trivial, error-prone, arduous, and time-consuming task even for experienced system administrators and statistical analysis to know what a reasonable set of steps should include to model and successfully predict performance variables or system failures of a complex software system. Reported results are fragmented and focus on applying statistical regression techniques to captured numerical system data. In this paper, we propose a best practice guide for building empirical models based on our experience with forecasting Apache Web server performance variables and forecasting call availability of a real world telecommunication system. To substantiate the presented guide and to demonstrate our approach step-by-step we model and predict the response time and the amount of free physical memory of an Apache Web server system. Additionally, we present concrete results for a) variable selection where we cross benchmark three procedures, b) empirical model building where we cross benchmark four techniques and c) sensitivity analysis. This best practice guide intends to assist in configuring modeling approaches systematically for best estimation and prediction results",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041903,no,undetermined,0
An Evaluation of Similarity Coefficients for Software Fault Localization,"Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important technique for the development of dependable software. In this paper we study different similarity coefficients that are applied in the context of a program spectral approach to software fault localization (single programming mistakes). The coefficients studied are taken from the systems diagnosis/automated debugging tools Pinpoint, Tarantula, and AMPLE, and from the molecular biology domain (the Ochiai coefficient). We evaluate these coefficients on the Siemens Suite of benchmark faults, and assess their effectiveness in terms of the position of the actual fault in the probability ranking of fault candidates produced by the diagnosis technique. Our experiments indicate that the Ochiai coefficient consistently outperforms the coefficients currently used by the tools mentioned. In terms of the amount of code that needs to be inspected, this coefficient improves 5% on average over the next best technique, and up to 30% in specific cases",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041886,no,undetermined,0
An Evaluation of Two Bug Pattern Tools for Java,"Automated static analysis is a promising technique to detect defects in software. However, although considerable effort has been spent for developing sophisticated detection possibilities, the effectiveness and efficiency has not been treated in equal detail. This paper presents the results of two industrial case studies in which two tools based on bug patterns for Java are applied and evaluated. First, the economic implications of the tools are analysed. It is estimated that only 3-4 potential field defects need to be detected for the tools to be cost-efficient. Second, the capabilities of detecting field defects are investigated. No field defects have been found that could have been detected by the tools. Third, the identification of fault-prone classes based on the results of such tools is investigated and found to be possible. Finally, methodological consequences are derived from the results and experiences in order to improve the use of bug pattern tools in practice.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539552,no,undetermined,0
Phoenix: Detecting and Recovering from Permanent Processor Design Bugs with Programmable Hardware,"Although processor design verification consumes ever-increasing resources, many design defects still slip into production silicon. In a few cases, such bugs have caused expensive chip recalls. To truly improve productivity, hardware bugs should be handled like system software ones, with vendors periodically releasing patches to fix hardware in the field. Based on an analysis of serious design defects in current AMD, Intel, IBM, and Motorola processors, this paper proposes and evaluates Phoenix - novel field-programmable on-chip hardware that detects and recovers from design defects. Phoenix taps key logic signals and, based on downloaded defect signatures, combines the signals into conditions that flag defects. On defect detection, Phoenix flushes the pipeline and either retries or invokes a customized recovery handler. Phoenix induces negligible slowdown, while adding only 0.05% area and 0.48% wire overheads. Phoenix detects all the serious defects that are triggered by concurrent control signals. Moreover, it recovers from most of them, and simplifies recovery for the rest. Finally, we present an algorithm to automatically size Phoenix for new processors",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041833,no,undetermined,0
Predicting the Viterbi Score Distribution for a Hidden Markov Model and Application to Speech Recognition,"Hidden Markov models are used in many important applications such as speech recognition and handwriting recognition. Finding an effective HMM to fit the data is important for successful operation. Typically, the Baum-Welch algorithm is used to train an HMM, and seeks to maximize the likelihood probability of the model, which is closely related to the Viterbi score. However, random initialization causes the final model quality to vary due to locally optimum solutions. Conventionally, in speech recognition systems, models are selected from a collection of already trained models using some performance criterion. In this paper, we investigate an alternative method of selecting models using the Viterbi score distribution. A method to determine the Viterbi score distribution is described based on Viterbi score variation with respect to the number of states (N) in the model. Our tests show that the distribution is approximately Gaussian when the number of states is greater than 3. The paper also investigates the relationship between performance and the Viterbi score's percentile value and discusses several interesting implications for Baum-Welch training",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041064,no,undetermined,0
An Empirical Study on Bayesian Network-based Approach for Test Case Prioritization,"A cost effective approach to regression testing is to prioritize test cases from a previous version of a software system for the current release. We have previously introduced a new approach for test case prioritization using Bayesian Networks (BN) which integrates different types of information to estimate the probability of each test case finding bugs. In this paper, we enhance our BN-based approach in two ways. First, we introduce a feedback mechanism and a new change information gathering strategy. Second, a comprehensive empirical study is performed to evaluate the performance of the approach and to identify the effects of using different parameters included in the technique. The study is performed on five open source Java objects. The obtained results show relative advantage of using feedback mechanism for some objects in terms of early fault detection. They also provide insight into costs and benefits of the various parameters used in the approach.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539555,no,undetermined,0
Software Release Time Management: How to Use Reliability Growth Models to Make Better Decisions,"In late years, due to the significance of software application, professional testing of software becomes an increasingly important task. Once all detected faults are removed, project managers can begin to determine when to stop testing. Software reliability has important relations with many aspects of software, including the structure, the operational environment, and the amount of testing. Actually, software reliability analysis is a key factor of software quality and can be used for planning and controlling the testing resources during development. Over the past three decades, many software reliability growth models (SRGMs) have been proposed. For most traditional SRGMs, one common assumption is that the fault detection rate is a constant over time. However, the fault detection process in the operational phase is different from that in the testing phase. Thus, in this paper, we use the testing compression factor (TCF) to reflect the fact and describe the possible phenomenon. In addition, sometimes the one-to-one mapping relationship between failures and faults may not be realistic. Therefore, we also incorporate the concept of quantified ratio, not equal to 1, of faults to failures into software reliability growth modeling. We estimate the parameters of the proposed model based on real software failure data set and give a fair comparison with other SRGMs. Finally, we show how to use the proposed model to conduct software release time management",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4037099,no,undetermined,0
Recovering from Distributable Thread Failures with Assured Timeliness in Real-Time Distributed Systems,"We consider the problem of recovering from failures of distributable threads with assured timeliness. When a node hosting a portion of a distributable thread fails, it causes orphans - i.e., thread segments that are disconnected from the thread's root. We consider a termination model for recovering from such failures, where the orphans must be detected and aborted, and failure-exception notification must be delivered to the farthest, contiguous surviving thread segment for resuming thread execution. We present a realtime scheduling algorithm called AUA, and a distributable thread integrity protocol called TP-TR. We show that AUA and TP-TR bound the orphan cleanup and recovery time, thereby bounding thread starvation durations, and maximize the total thread accrued timeliness utility. We implement AUA and TP-TR in a real-time middleware that supports distributable threads. Our experimental studies with the implementation validate the algorithm/protocol's time-bounded recovery property and confirm their effectiveness",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032488,no,undetermined,0
Hidden Markov Models as a Support for Diagnosis: Formalization of the Problem and Synthesis of the Solution,"In modern information infrastructures, diagnosis must be able to assess the status or the extent of the damage of individual components. Traditional one-shot diagnosis is not adequate, but streams of data on component behavior need to be collected and filtered over time as done by some existing heuristics. This paper proposes instead a general framework and a formalism to model such over-time diagnosis scenarios, and to find appropriate solutions. As such, it is very beneficial to system designers to support design choices. Taking advantage of the characteristics of the hidden Markov models formalism, widely used in pattern recognition, the paper proposes a formalization of the diagnosis process, addressing the complete chain constituted by monitored component, deviation detection and state diagnosis. Hidden Markov models are well suited to represent problems where the internal state of a certain entity is not known and can only be inferred from external observations of what this entity emits. Such over-time diagnosis is a first class representative of this category of problems. The accuracy of diagnosis carried out through the proposed formalization is then discussed, as well as how to concretely use it to perform state diagnosis and allow direct comparison of alternative solutions",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032486,no,undetermined,0
Solving Consensus Using Structural Failure Models,"Failure models characterise the expected component failures in fault-tolerant computing. In the context of distributed systems, a failure model usually consists of two parts: a functional part specifying in what way individual processing entities may fail and a structural part specifying the potential scope of failures within the system. Such models must be expressive enough to cover all relevant practical situations, but must also be simple enough to allow uncomplicated reasoning about fault-tolerant algorithms. Usually, an increase in expressiveness complicates formal reasoning, but enables more accurate models that allow to improve the assumption coverage and resilience of solutions. In this paper, we introduce the structural failure model class DiDep that allows to specify directed dependent failures, which, for example, occur in the area of intrusion tolerance and security. DiDep is a generalisation of previous classes for undirected dependent failures, namely the general adversary structures, the fail-prone systems, and the core and survivor sets, which we show to be equivalent. We show that the increase in expressiveness of DiDep does not significantly penalise the simplicity of corresponding models by giving an algorithm that transforms any consensus algorithm for undirected dependent failures into a consensus algorithm for a DiDep model. We characterise the improved resilience obtained with DiDep and show that certain models even allow to circumvent the famous FLP impossibility result",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032483,no,undetermined,0
Test Instrumentation and Pattern Matching for Automatic Failure Identification,"An increasing emphasis on test automation presents test teams with a growing set of results to review and investigate. The life time of a given failure typically spans multiple automation runs making the necessary due-diligence effort time consuming, labor intensive, error prone and often redundant. When faced with this condition within their teams, the authors addressed the problem from two approaches; pattern recognition and instrumentation. Pattern recognition sought to automate some of the manual processes of failure identification while instrumentation provided a detailed and consistent description of a failure. In practice, the problem divides into 3 domains; instrumentation, annotation, and recognition. Instrumentation forms the basis for the solution. Annotation provides the ability to refine a given pattern based on the investigation results. Recognition forms the basis for identification. The effectiveness of the solution in a production environment will be discussed. Best practices and lessons learned will be shared.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539565,no,undetermined,0
A Semi-empirical Model of Test Quality in Symmetric Testing: Application to Testing Java Card APIs,"In the smart card quality assurance field, software testing is the privileged way of increasing the confidence level in the implementation correctness. When testing Java Card application programming interfaces (APIs), the tester has to deal with the classical oracle problem, i.e. to find a way to evaluate the correctness of the computed output. In this paper, we report on an experience in testing methods of the Oberthur Card Systems Cosmo 32 RSA Java Card APIs by using the Symmetric Testing paradigm. This paradigm exploits user-defined symmetry properties of Java methods as test oracles. We propose an experimental environment that combines random testing and symmetry checking for (on-card) cross testing of several API methods. We develop a semi-empirical model (a model fed by experimental data) to help deciding when to stop testing and to assess test quality",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032302,no,undetermined,0
Voltage Sag Study for a Practical Industrial Distribution Network,"Voltage sags have become one of the major power quality concerns in recent years. To decide the suitability of mitigation methods, knowledge is needed about the expected number of sags as a function of special characteristics; this knowledge can be obtained through voltage sag analysis software, which is developed in the paper based on the method of fault position. The software has many functions; a detailed voltage sag study for a practical industrial distribution network by using it has been done in this paper. In order to assess the degree of fault position influence on voltage sags of the power system, an index named fault position sag coefficient (FPSC) is defined in the paper. This coefficient can be calculated out based on the results of voltage sag analysis software. The dangerous fault positions which cause serious voltage sags can be determined.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4116226,no,undetermined,0
Forming Groups for Collaborative Learning in Introductory Computer Programming Courses Based on Students' Programming Styles: An Empirical Study,"This paper describes and evaluates an approach for constructing groups for collaborative learning of computer programming. Groups are formed based on students' programming styles. The style of a program is characterized by simple well known metrics, including length of identifiers, size and number of modules (functions/procedures), and numbers of indented, commented and blank lines. A tool was implemented to automatically assess the style of programs submitted by students. For evaluating the tool and approach used to construct groups, some experiments were conducted involving information systems students enrolled in a course on algorithms and data structures. The experiments showed that collaborative learning was very effective for improving the programming style of students, particularly for students that worked in heterogeneous groups (formed by students with different levels of knowledge of programming style)",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4116907,no,undetermined,0
Automated Error-Prevention and Error-Detection Tools for Assembly Language in the Educational Environment,"Automated tools for error prevention and error detection exist for many high-level languages, but have been nonexistent for assembly-language programs, embedded programs in particular. We present new tools that improve the quality and reliability of assembly-language programs by helping the educator automate the arduous tasks of exposing and correcting common errors and oversights. These tools give the educator a user-friendly, but powerful means of completely testing student programs. The new tools that we have developed are the result of years of research and experience by the authors in testing and debugging students' programming assignments. During this time, we created a few preliminary versions of these automated tools, allowing us to test our students' projects in one fell swoop. These tools gave us the ability to catch stack errors and memory-access errors that we would not have been able to detect with normal testing. These tools considerably shortened the amount of testing time and allowed us to detect a larger group of errors",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4117183,no,undetermined,0
Preventing Cross Site Request Forgery Attacks,"The Web has become an indispensable part of our lives. Unfortunately, as our dependency on the Web increases, so does the interest of attackers in exploiting Web applications and Web-based information systems. Previous work in the field of Web application security has mainly focused on the mitigation of cross site scripting (XSS) and SQL injection attacks. In contrast, cross site request forgery (XSRF) attacks have not received much attention. In an XSRF attack, the trust of a Web application in its authenticated users is exploited by letting the attacker make arbitrary HTTP requests on behalf of a victim user. The problem is that Web applications typically act upon such requests without verifying that the performed actions are indeed intentional. Because XSRF is a relatively new security problem, it is largely unknown by Web application developers. As a result, there exist many Web applications that are vulnerable to XSRF. Unfortunately, existing mitigation approaches are time-consuming and error-prone, as they require manual effort to integrate defense techniques into existing systems. In this paper, we present a solution that provides a completely automatic protection from XSRF attacks. More precisely, our approach is based on a server-side proxy that detects and prevents XSRF attacks in a way that is transparent to users as well as to the Web application itself. We provide experimental results that demonstrate that we can use our prototype to secure a number of popular open-source Web applications, without negatively affecting their behavior",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4198791,no,undetermined,0
A Failure/Fault Diagnoser Model for Autonomous Agents under Presence of Disturbances,"In this paper, we provide our preliminary results on failure/fault analysis for an autonomous agent whose behavior is influenced by disturbances from its environment. It is assumed that failures are unobservable and only detected through some observable symptoms. Furthermore, faults may be observable directly, but the conditions leading to them are unknown a priori. Both the agent and its environment may contribute to any given failure or fault. The main results of this paper relate to the development of a diagnoser model which is built by a parallel composition of Petri net models of the underlying components. This model includes both the normative and disruptive behavior of the agent and its perception of the environment. The uncontrollability of the environment leads to a non-deterministic diagnosis behavior. The likelihood of each possible state of the agent's diagnoser is updated every time that a new controllable action is taken or new sensory information is received. At each updating, if the likelihood of a failure or fault state is higher than a specified threshold, then an alarm is raised indicating a disruptive event.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274411,no,undetermined,0
Experimental Evaluation of Three Concurrent Error Detection Mechanisms,"This paper presents an experimental evaluation of the effectiveness of three hardware-based control flow checking mechanisms, using software-implemented fault injection (SWIFI) method. The fault detection technique uses reconfigurable of the shelf FPGAs to concurrently check the execution flow of the target program. The technique assigns signatures to the target program in the compile time and verifies the signatures using a FPGA as a watchdog processor to detect possible violation caused by the transient faults. A total of 3000 faults were injected in the experimental embedded system, which is based on an 8051 microcontroller, to measure the error detection coverage. The experimental results show that these mechanisms detect about 90% of transient errors, injected by software implemented method.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4243649,no,undetermined,0
Improved Assertion Lifetime via Assertion-Based Testing Methodology,"Assertions-based verification (ABV) has been widely used in digital design validation. Assertions are HDL-syntaxed representation of design specification and used as a functional error detection mechanism. During the process of designing with HDLs, assertions are imported which could fire in case of violation during testbench run. Although these assertions are mostly used during simulation and for verifying the functional correctness of the design, but as they illustrate the specifications of a design, it is likely that their lifetime could be extended by embedding them in the chip to detect low level faults like stuck-at faults. In this paper, we introduce a new automatable assertion-based on-line testing methodology. Experimental results show that the synthesis of assertions into a chip, and then using them for online testing, can provide an acceptable coverage for stuck-at faults.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4243645,no,undetermined,0
Admon: ViSaGe Administration And Monitoring Service For Storage Virtualization in Grid Environment,"This work is part of the ViSaGe project. This project concerns the storage virtualization applied in the grid environment. Its goal is to create and manage virtual storage resources by aggregating geographically distributed physical storage resources shared on a Grid. To these shared resources, a quality of service will be associated and related to the data storage performance. In a Grid environment, sharing resources may improve performances if these resources are well managed and if the management software obtains sufficient knowledge about the grid resources workload (computing nodes, storage nodes and links). The grid resources workload is mainly perceived by a monitoring system. Several existing monitoring systems are available for monitoring the grid resources and applications. Each one provides informations according to its aim. For example, Network Weather Service [6] is designed to monitor grid computing nodes and links. On the other hand, Netlogger [8] monitors grid resources and applications in order to detect application's bottleneck. These systems are useful for a post mortem analysis. However, in ViSaGe, we need a system that analyzes the necessary nodes state during execution time. This system is represented by the ViSaGe Administration and monitoring service """"Admon"""". In this paper, we present our scalable distributed system: Admon. Admon traces applications, and calculates the system resources consumption (CPU, Disks, Links). Its originality consists in providing a new opportunity to improve virtual data storage performance by using workload's constraints (e.g., maximum CPU usage percentage). Many constraints will be discussed (such as time's constraint). These constraints allow performance improvement by assigning nodes to the ViSaGe's jobs in an effective manner.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4530214,no,undetermined,0
Power Quality And Harmonic Loads,This paper presents the results of a research done on the harmonic distortion of load current and voltage in low voltage distribution networks owing to the extensive use of peak-detecting type capacitor-filter rectifier systems (often the front-end of non linear electronic loads). A classical case to be studied is the compact fluorescent lamps (CFLs) without power factor correction being implemented. It has become evident that the use of CFLs alone is more or less accountable to the non-linear loads generated in rural electrification schemes in Sri Lanka. A case study was also done by the authors on a chosen low voltage distribution network to analyze the impact of CFL loads and the capabilities of the network with regard to IEEE 519 regulations. The methods of analysis and the results are presented in this paper and PSCAD software has been used to simulate the system. The commercial aspects of using CFLs and energy saving have been discussed. Harmonic effects on the distribution transformers are studied as applied to the line segment considered in the study,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4216554,no,undetermined,0
The Application of the System Parameter Fusion Principle to Assessing University Electronic Library Performance,"Modern technology provides a great amount of information. But for computer monitoring systems or computer control systems, in order to have the situation in hand, we need to reduce the number of variables to one or two parameters, which express the quality and/or security of the whole system. In this paper, the authors introduce the system parameter fusion principle put forward by the third author and present how to apply it to assessing university electronic library performance combining with the Delphi technique and AHP",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4216535,no,undetermined,0
Distribution Network Restoration Using Sequential Monte Carlo Approach,"The paper describes a developed procedure for modeling restoration after a fault and calculating associated switching time using sequential Monte-Carlo approach. The procedure is based on the experience and knowledge of distribution system operators, thus represents an attempt in regarding the realistic issues influencing the duration of fault isolation and customer restoration, i.e. the actual switching and eventually repair time needed for restoration of each affected customer. In addition to expected duration of switching, the associated probability density functions can be computed. Further more, Visual Basic software using the proposed procedure has been developed and some examples of switching time expectations and probability distributions calculations are presented",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4202279,no,undetermined,0
Transient Error Detection in Embedded Systems Using Reconfigurable Components,"In this paper, a hardware control flow checking technique is presented and evaluated. This technique uses re configurable of the shelf FPGA in order to concurrently check the execution flow of the target micro processor. The technique assigns signatures to the main program in the compile time and verifies the signatures using a FPGA as a watchdog processor to detect possible violation caused by the transient faults. The main characteristic of this technique is its ability to be applied to any kind of processor architecture and platforms. The low imposed hardware and performance overhead by this technique makes it suitable for those applications in which cost is a major concern, such as industrial applications. The proposed technique is experimentally evaluated on an 8051 microcontroller using software implemented fault injection (SWIFI). The results show that this technique detects about 90% of the injected control flow errors. The watchdog processor occupied 26% of an Altera Max-7000 FPGA chip logic cells. The performance overhead varies between 42% and 82% depending on the workload used.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197508,no,undetermined,0
"Relationships between Test Suites, Faults, and Fault Detection in GUI Testing","Software-testing researchers have long sought recipes for test suites that detect faults well. In the literature, empirical studies of testing techniques abound, yet the ideal technique for detecting the desired kinds of faults in a given situation often remains unclear. This work shows how understanding the context in which testing occurs, in terms of factors likely to influence fault detection, can make evaluations of testing techniques more readily applicable to new situations. We present a methodology for discovering which factors do statistically affect fault detection, and we perform an experiment with a set of test-suite- and fault-related factors in the GUI testing of two fielded, open-source applications. Statement coverage and GUI-event coverage are found to be statistically related to the likelihood of detecting certain kinds of faults.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539528,no,undetermined,0
Debug Support for Scalable System-on-Chip,"On-chip debug is an important technique to detect and locate the faults in the practical software applications. Scalability and reusability are the essential features of system-on-chip (SoC). Therefore, the debug architecture should meet the requirement of those features. Furthermore, it is necessary for applications developers to communication with the SoC chip on-line. In this paper, we present the novel debug architecture to solve above problems. The debug architecture has been implemented in a typical SoC chip. The results of performance analysis show that the debug architecture has high performance at the cost of few resources and area.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197226,no,undetermined,0
Challenges in System on Chip Verification,"The challenges of system on a chip (SoC) verification is becoming increasingly complex as submicron process technology shrinks die size, enabling system architects to include more functionality in a single chip solution. A functional defect refers to the feature sets, protocols or performance parameters not conforming to the specifications of the SoC. Some of the functional defects can be solved by software workarounds but some require revisions of silicon. The revision of silicon not only costs millions of dollars but also impacts time to market, quality, customer commitments. Working silicon for the first revision of the SoC requires a robust module, chip and system verification strategy to uncover the logical and timing defects before tapeout. Different techniques are needed at each level (module, chip and system) to complete verification. In addition verification should quantify with a metric at every hierarchy to assess functional holes and address it. Verification metric can be a combination of code coverage, functional coverage, assertion coverage, protocol coverage, interface coverage and system coverage. A successful verification strategy also requires the test bench to be scalable, configurable, support reuse of functional tests, integration with tools and finally linkage to validation. The scope of this paper will discuss the verification strategy and pitfalls used in verification strategy and finally make recommendations for successful strategy.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4197222,no,undetermined,0
The 3LGM2-Tool to Support Information Management in Health Care,"In industrialized as well as in developing countries the driving force for healthcare has recently been the trend towards a better coordination of care. The focus has been changed from isolated procedures in a single healthcare institution (e.g. a hospital or a general practice) to the patient-oriented care process spreading over institutional boundaries. This should lead to a shift towards better integrated and shared care. Health care professionals in different departments of a hospital but moreover in a region - and in many cases even worldwide - have to cooperate in order to achieve health for the patient. [1] Cooperation needs an adequate system for communicating and processing of information, i.e., an information system, which is that socio-technical subsystem of a (S et of) health care institution(s), which presents information at the right time, in the right place to the right people [2, 3]. Hospital Information Systems (HIS) as well as regional Health Information Systems (rHIS) (consisting of different institutional information systems) are constructed like a (complex of) building(s) out of different and probably heterogeneous bricks and components. Thus cooperation depends especially on the availability of adequate communication links between the institutional information systems and their components. Besides technical problems of communication links there are a lot of complex problems of connecting heterogeneous software components of different vendors and with different database schemata to be solved. Especially the proper application of communication standards like HL7 and DICOM [4-6] needs proper planning and supervision as part of a systematic information management. Like an architect the information manager needs a blueprint or model for the information system's architecture respectively the enterprise architecture [7-9]. In [10] we proposed the 3LGM<sup>2</sup> as a meta model for modeling Information Systems (IS). 3LGM<sup>2</sup> has been designe- d to describe IS by concepts on three layers. The domain layer consists of enterprise functions and entity types, the logical tool layer focuses on application components and the physical tool layer describes physical data processing components. In contrast to other approaches a lot of inter-layer-relationships exist. 3LGM<sup>2</sup> is defined using the Unified Modeling Language (UML). The meta model has been supplemented by the 3LGM<sup>2</sup> tool [12]. Using 3LGM<sup>2</sup> as the ontological basis this tool enables information managers to graphically design even complex IS. It assists information managers similarly to Computer Aided Design tools (CAD) supporting architects. The tool provides means for analyzing a HIS model and thus for assessing the HIS quality. The talk will focus on the 3LGM<sup>2</sup> tool and its most important features. It will be shown, how a model can be created by graphical user interaction as well as by importing data from other sources. It will be illustrated how the tool's analyzing features support information managers doing their job. Examples will be taken from 3LGM<sup>2</sup> models of the information system of the Leipzig University Hospital and the regional health information system of Saxony, a federal state of Germany.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4196513,no,undetermined,0
Fault Tolerance Management for a Hierarchical GridRPC Middleware,"The GridRPC model is well suited for high performance computing on grids thanks to efficiently solving most of the issues raised by geographically and administratively split resources. Because of large scale, long range networks and heterogeneity, Grids are extremely prone to failures. GridRPC middleware are usually managing failures by using 1) TCP or other link network layer provided failure detector, 2) automatic checkpoints of sequential jobs and 3) a centralized stable agent to perform scheduling. Most recent developments have provided some new mechanisms like the optimal Chandra & Toueg & Aguillera failure detector, most numerical libraries now providing their own optimized checkpoint routine and distributed scheduling GridRPC architectures. In this paper we aim at adapting to these novelties by providing the first implementation and evaluation in a grid system of the optimal fault detector, a novel and simple checkpoint API allowing to manage both service provided checkpoint and automatic checkpoint (even for parallel services) and a scheduling hierarchy recovery algorithm tolerating several simultaneous failures. All those mechanisms are implemented and evaluated on a real grid in the DIET middleware.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4534253,no,undetermined,0
Research and Development of Print Quality Inspection System of Biochips,"In order to automatically detect the print defects of PET board of a kind of biochip, integrated electro-mechanical method was employed to design the experimental platform for the recognition and marker of PET board, therefore the automatic control of mechanical inspection system was realized; CCD sensor was used to get the image of PET board, and specific computer software was developed to achieve the image processing and recognition of PET board; thus, the automatic inspection system of print quality of biochips was constructed. Practical results indicated that this system can precisely and effectively achieve real-time detection.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4535620,no,undetermined,0
Probabilistic ISOCS Uncertainty Estimator: Application to the Segmented Gamma Scanner,"Traditionally, high resolution gamma-ray spectroscopy (HRGS) has been used as a very powerful tool to determine the radioactivity of various items, such as samples in the laboratory, waste assay containers, or large items in-situ. However, in order to properly interpret the quality of the result, an uncertainty estimate must be made. This uncertainty estimate should include the uncertainty in the efficiency calibration of the instrument, as well as many other operational and geometrical parameters. Efficiency calibrations have traditionally been made using traceable radioactive sources. More recently, mathematical calibration techniques have become increasingly accurate and more convenient in terms of time and effort, especially for complex or unusual configurations. Whether mathematical or source-based calibrations are used, any deviations between the as-calibrated geometry and the as-measured geometry contribute to the total measurement uncertainty (TMU). Monte Carlo approaches require source, detector, and surrounding geometry inputs. For non-trivial setups, the Monte Carlo approach is time consuming both in terms of geometry input and CPU processing. Canberra Industries has developed a tool known as In-Situ Object Calibration Software (ISOCS) that utilizes templates for most common real life setups. With over 1000 detectors in use with this product, the ISOCS software has been well validated and proven to be much faster and acceptably accurate for many applications. A segmented gamma scanner (SGS) template is available within ISOCS and we use it here to model this assay instrument for the drummed radioactive waste. Recently, a technique has been developed which uses automated ISOCS mathematical calibrations to evaluate variations between reasonably expected calibration conditions and those that might exist during the actual measurement and to propagate them into an overall uncertainty on the final efficiency. This includes variations in container wall thickness - and diameter, sample height and density, sample non-uniformity, sample-detector geometry, and many other variables, which can be specified according to certain probability distributions. The software has a sensitivity analysis mode which varies one parameter at a time and allows the user to identify those variables that have the largest contribution to the uncertainty. There is an uncertainty mode which uses probabilistic techniques to combine all the variables and compute the average efficiency and the uncertainty in that efficiency, and then to propagate those values with the gamma spectroscopic analysis into the final result. In the areas of waste handling and environmental protection, nondestructive assay by gamma ray scanning can provide a fast, convenient, and reliable way of measuring many radionuclides in closed items. The SGS is designed to perform accurate quantitative assays on gamma emitting nuclides such as fission products, activation products, and transuranic nuclides. For the SGS, this technique has been applied to understand impacts of the geometry variations during calibration on the efficiency and to estimate the TMU.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4179189,no,undetermined,0
The Monitoring Data Archiving Service for ATLAS,"ATLAS is one the four experiments being assembled at the CERN Large Hadron Collider (LHC). The complexity of the ATLAS experiment and the high event rate make the monitoring system an essential tool to assess the status of the hardware and the quality of the data while they are being acquired. It is important that all the monitoring data, mainly ROOT histograms, are saved to a permanent storage system, so that they can be used later for studying the time evolution of the experimental conditions or as reference for the future runs. The presentation will show the solution proposed to this issue within the ATLAS Trigger and Data Acquisition (TDAQ) project. Many GB of monitoring data are expected per run. At the end of each run, the Monitoring Data Archiving service (MDA) retrieves all the available histograms from the Online Histogramming service (a temporary storage provided within the ATLAS TDAQ software framework) and writes them into ROOT files, which in turn will be stored on tapes. The Collection and Cache service (CoCa) manages a disk based cache in order to guarantee a fast access to the histograms produced during the last runs. Furthermore, it collects many small files and produces big archives to be stored on tape, thus enhancing the efficiency in the tape usage. Initially meant as a component of MDA, CoCa has evolved as an independent package that can be used by any ATLAS online application.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4178939,no,undetermined,0
Task Graph Generation,"There are many intelligent design tools available, which are being used at the highest level of abstraction. These tools are very effective in solving the hardware/software co-synthesis problems. These tools require the input specification of the problem to be in the form of one or more task graph. Currently, one major problem is that many real time embedded system designs are specified in high level programming languages, not task graphs. The designer can manually transform the input specification from the used computer language to a task graph form, but this job has tedious and error prone problems. The task graph generation described in this paper reduces the potential for error and time required by automating the task graph process",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275158,no,undetermined,0
Multi-processor system design with ESPAM,"For modern embedded systems, the complexity of embedded applications has reached a point where the performance requirements of these applications can no longer be supported by embedded system architectures based on a single processor. Thus, the emerging embedded System-on-Chip platforms are increasingly becoming multiprocessor architectures. As a consequence, two major problems emerge, i.e., how to design and how to program such multiprocessor platforms in a systematic and automated way in order to reduce the design time and to satisfy the performance needs of applications executed on these platforms. Unfortunately, most of the current design methodologies and tools are based on Register Transfer Level (RTL) descriptions, mostly created by hand. Such methodologies are inadequate, because creating RTL descriptions of complex multiprocessor systems is error-prone and time consuming. As an efficient solution to these two problems, in this paper we propose a methodology and techniques implemented in a tool called ESPAM for automated multiprocessor system design and implementation. ESPAM moves the design specification from RTL to a higher, so called system level of abstraction. We explain how starting from system level platform, application, and mapping specifications, a multiprocessor platform is synthesized and programmed in a systematic and automated way. Furthermore, we present some results obtained by applying our methodology and ESPAM tool to automatically generate multiprocessor systems that execute a real-life application, namely a Motion-JPEG encoder.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278517,no,undetermined,0
Evolving GA Classifiler for Audio Steganalysis based on Audio Quality Metrics,"Differentiating anomalous audio document (Stego audio) from pure audio document (cover audio) is difficult and tedious. Steganalytic techniques strive to detect whether an audio contains a hidden message or not. This paper presents a genetic algorithm (GA) based approach to audio steganalysis, and the software implementation of the approach. The basic idea is that, the various audio quality metrics calculated on cover audio signals and on stego-audio signals vis-a-vis their denoised versions, are statistically different. GA is employed to derive a set of classification rules from audio data using these audio quality metrics, and fitness function is used to judge the quality of each rule. The generated rules are then used to detect or classify the audio documents in a real-time environment. Unlike most existing GA-based approaches, because of the simple representation of rules and the effective fitness function, the proposed method is easier to implement while providing the flexibility to generally detect any new steganography technique. The implementation of the GA based audio steganalyzer relies on the choice of these audio quality metrics and the construction of a two-class classifier, which will discriminate between the adulterated and the untouched audio samples. Experimental results show that the proposed technique provides promising detection rates.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4286072,no,undetermined,0
Ensuring Numerical Quality in Grid Computing,"Certain numerically intensive applications executed within a grid computing environment crucially depend on the properties of floating-point arithmetic implemented on the respective platform. Differences in these properties may have drastic effects. This paper identifies the central problems related to this situation. We propose an approach which gives the user valuable information on the various platforms available in a grid computing environment in order to assess the numerical quality of an algorithm run on each of these platforms. In this manner, the user will at least have very strong hints whether a program will perform reliably in a grid before actually executing it. Our approach extends the existing IeeeCC754 test suite by two """"grid-enabled"""" modes: The first mode calculates a """"numerical checksum"""" on a specific grid host and executes the job only if the checksum is identical to a locally generated one. The second mode provides the user with information on the reliability and IEEE 754-conformity of the underlying floating-point implementation of various platforms. Furthermore, it can help to find a set of compiler options to optimize the application's performance while retaining numerical stability.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402410,no,undetermined,0
Software Reliability Measurement and Prediction,This chapter contains sections titled: <br> Why Study and Measure Software Reliability? <br> What Is Reliability? <br> Faults and Failures <br> Failure Severity Classes <br> Failure Intensity <br> The Cost of Reliability <br> Software Reliability Theory <br> Reliability Models <br> Failure Arrival Rates <br> But When Do I Ship? <br> System Configurations: Probability and Reliability <br> Answers to Initial Question <br> Summary <br> Problems <br> Project <br> References,2006,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=5989154.pdf&bkn=5988896&pdfType=chapter,no,undetermined,0
Decade experience of the ecological risk assessment for water ecosystem and human population in Russia,Generalization of environmental risk applications as an integral safety criterion for water ecosystems and human population shows good promise of this approach to local and regional level environmental quality assessment. The major advantage of the environmental risk assessment approach is the timely detection of negative environmental quality trends prior to the toxic substance concentration reaching the maximum permissible concentration in water and air. The existing models of spatial and temporal water poisonous material concentration variation do not provide for authentic results due to their erroneous axiomatic assumptions. A new concept has been suggested for environmental risk assessment for marine ecosystems and human population due to dumped chemical weapons. Implementation of this new concept requires: Actual data on the effect of short-term biogeochemical processes on the behavior of poisonous materials and their decay products both in the water and at its boundaries with seabed deposits and the air; Software package for assessing the effect of nonlinear processes on poisonous material decay and transport rates in seabed water systems.,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7266163,no,undetermined,0
Semantic reliability of multi-agent intelligent systems,"Generally the concept of reliability has been interpreted as applied to hardware and software and has been based upon the assumption that a system can be decomposed into subsystems or components to which success or failure probabilities can be assigned assuming perfect semantic transactions between them, i.e., with no consideration to variation in the interpretation of the meanings of messages between various components. In multi-agent intelligent systems, where the agents interact with each other in capacities other than merely sending and receiving messages, cooperative decisions are made based upon beliefs, desires, intentions, and the autonomy of individual agents. In such cases, even if the components as well as the interconnections are error-free in the classical sense, there can be serious failures due to semantic variability and consequently the concept of reliability needs to be extended to semantics as well. This paper attempts to establish this new concept of semantic reliability and explore its relationship to the system reliability and information extraction processes. Here we examine the communication between agents and semantic error modes in multi-agent systems using Rao and Georgeff's belief-desire-intention (BDI) model of intelligent agents to decompose the semantic variation into its contributing parts from various subsystems comprising the agents. From this, the impact and the risk management strategies including fault tolerance are evolved. World representation, domain ontologies, and knowledge representation are brought out as important determinants of error control. A fault tolerance design based on goal hierarchy is suggested.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6768566,no,undetermined,0
Adaptive protection setting and coordination for power distribution systems,"In this paper, a protection system using a Multi-Agent concept for power distribution networks is proposed. Every digital over current relay (OCR) is developed as an agent by adding its own intelligence, self-tuning and communication ability. In order to cope with frequent changes in the network operation condition and faults, an OCR agent, suggested in this paper, is able to detect a fault or a change in the network and find its optimal parameters of the protection relays in an autonomous manner considering information of the whole network obtained by communication between other agents. Simulations in a simple distribution network show the effectiveness of the suggested scheme.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372371,no,undetermined,0
Interface faults injection for component-based integration testing,"This paper presents a simple and improved technique of interface fault insertion for conducting component integration testing through the use of aspect-oriented software development (AOSD). Taking the advantage of aspect's cross-cutting features, this technique only requires additional codes written in AspectJ rather than having a separate tool to perform this operation. These aspect codes act as wrappers around interface services and perform operations such as disabling the implementation of the interface services, raising exceptions or corrupting the inputs and outputs of interface services. Interface faults are inserted into the system under test to evaluate the quality of the test cases by ensuring not only that they detect errors due to the interactions between components, but they are also able to handle exceptions raised when interface faults are triggered.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5276516,no,undetermined,0
Operational Fault Detection in cellular wireless base-stations,"The goal of this work is to improve availability of operational base-stations in a wireless mobile network through non-intrusive fault detection methods. Since revenue is generated only when actual customer calls are processed, we develop a scheme to minimize revenue loss by monitoring real-time mobile user call processing activity. The mobile user call load profile experienced by a base-station displays a highly non-stationary temporal behavior with time-of-day, day-of-the-week and time-of-year variations. In addition, the geographic location also impacts the traffic profile, making each base-station have its own unique traffic patterns. A hierarchical base-station fault monitoring and detection scheme has been implemented in an IS-95 CDMA Cellular network that can detect faults at - base station level, sector level, carrier level, and channel level. A statistical hypothesis test framework, based on a combination of parametric, semi-parametric and non-parametric test statistics are defined for determining faults. The fault or alarm thresholds are determined by learning expected deviations during a training phase. Additionally, fault thresholds have to adapt to spatial and temporal mobile traffic patterns that slowly changes with seasonal traffic drifts over time and increasing penetration of mobile user density. Feedback mechanisms are provided for threshold adaptation and self-management, which includes automatic recovery actions and software reconfiguration. We call this method, Operational Fault Detection (OFD). We describe the operation of a few select features from a large family of OFD features in Base Stations; summarize the algorithms, their performance and comment on future work.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4798311,no,undetermined,0
Reverse Engineering XML,"A great number of existing XML documents in various domain such as electrical business have to be maintained in order to constantly adapt to a dynamically changing environment to keep pace with business needs. A DTD or XML schema in its current textual form commonly lacks clarity and readability, which makes the maintenance process tedious and error-prone. This paper presents an approach to reverse engineering the XML documents to conceptual model, which makes the XML documents more close to real world and business needs, let the designers quickly gain a picture of the overall structure of XML documents in order to improve its quality, increase the maintainability and reusability. In this paper, the conceptual model is described by UML class diagram, a three-level model is defined, and a novel approach for extracting various structure and semantic information from existing DTD is given, especially the inheritance structure can be inferred from the DTD structure",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4673747,no,undetermined,0
Software Planned Learning and Recognition Based on the Sequence Learning and NARX Memory Model of Neural Network,"In traditional way, software plans are represented explicitly by some semantic schemas. However, semantic contents, constrains and relations of plans are hard for explicit presentation. Besides, it is a heavy and error-prone work to build such a library of plans. Algorithms of recognition of such plans demand exact matching by which semantic denotation is obvious itself. We thus present a novel approach of applying neural network in the presentation and recognition of plans via asymmetric Hebbian plasticity and non-linear auto-regressive with exogenous inputs (NARX) to learn and recognize plans. Semantics of plans are represented implicitly and error-tolerant. The recognition procedure is also error-tolerant because it tends to match fuzzily like human. Models and relevant limitations are illustrated and analyzed in this article",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4673743,no,undetermined,0
An Ontology-Based Approach for Domain Requirements Elicitation and Analysis,"Domain requirements are fundamental for software reuse and are the product of domain analysis. This paper presents an ontology based approach to elicit and analyze domain requirements. An ontology definition is given out. Problem domain is decomposed into several sub problem domains by using subjective decomposition method. The top-down refinement method is used to refine each sub problem domain into primitive requirements. Abstract stakeholders are used instead of real ones when decomposing problem domain and domain primitive requirements are represented by ontology. Not only domain commonality, variability and qualities are presented, but also reasoning logic is used to detect and handle incompleteness and inconsistency of domain requirements. In addition, a case of 'spot and futures transaction' domain is used to illustrate the approach",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4673732,no,undetermined,0
Improved automated quantification of left ventricular size and function from cardiac magnetic resonance images,"Assessment of left ventricular (LV) size and function from cardiac magnetic resonance (CMR) images requires manual tracing of LV borders on multiple 2D slices, which is subjective, experience dependent, tedious and time-consuming. We tested a new method for automated dynamic segmentation of CMR images based on a modified region-based model, in which a level set function minimizes a functional containing information regarding the probability density distribution of the gray levels. Images (GE 1.5T FIESTA) obtained in 9 patients were analyzed to automatically detect LV endocardial boundaries and calculate LV volumes and ejection fraction (EF). These measurements were validated against manual tracing. The automated calculation ofLV volumes and EF was completed in each patient in <3 min and resulted in high level of agreement with no significant bias and narrow limits of agreement with the reference technique. The proposed technique allows fast automated detection of endocardial boundaries as a basis for accurate quantification of LV size and function from CMR images.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4511786,no,undetermined,0
Inline VPD-TXRF for Contamination Control: Reality or Myth? Experience in a 300mm R&D MirrorBit Flash Memory Fab,Recent advances in TXRF hardware and software and highly successful integration of automated VPD modules to TXRF have made inline vapor phase decomposition-total reflection X-ray fluorescence (VPD-TXRF) a reality. In this paper we describe the use of and application of a fully integrated VPD-TXRF system capable of analyzing 200 mm and 300 mm diameter silicon wafers during the critical conversion period of an R&D MirrorBit Flash Fabrication facility to 300 mm. Rapid deployment of such an automated system during the conversion period has enabled us to 1) efficiently bench mark and qualify newly installed processing tools 2) assess the validity of existing protocols and procedures 3) respond very quickly to incidences where protocols have been breached and 4) identify process tools that heavily contaminate the backside of wafers.,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493127,no,undetermined,0
Noise Makers Need to Know Where to be Silent  Producing Schedules That Find Bugs,"A noise maker is a tool that seeds a concurrent program with conditional synchronization primitives, such as yield(), for the purpose of increasing the likelihood that a bug manifest itself. We introduce a novel fault model that classifies locations as """"good"""", """"neutral"""", or """"bad,"""" based on the effect of a thread switch at the location. Using the model, we explore the terms under which an efficient search for real-life concurrent bugs can be conducted. We accordingly justify the use of probabilistic algorithms for this search and gain a deeper insight of the work done so far on noise- making. We validate our approach by experimenting with a set of programs taken from publicly available multi-threaded benchmarks. Our empirical evidence demonstrates that real-life behavior is similar to one derived from the model.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463749,no,undetermined,0
"Formal Security Analysis in Industry, at the Example of Electronic Distribution of Aircraft Software (EDS)","Summary form only given. When developing products or solutions in industry and assessing their quality, formal methods provide the most rigorous tools for checking for safety and security flaws. In this talk we share our first-hand general experience in this area, and furthermore provide some details of a project specifying and modeling electronic distribution software (EDS). We comment on the motivation, practice, and impact of applying formal methods in industry, including the role of evaluation and certification according to the common criteria. Second, we give an overview of which modeling and verification techniques we have found useful so far, for which reasons. Third, we present some ongoing work on specifying and modeling EDS. The aim of EDS is to alleviate the burden of distributing initial and update versions of software in modern airplanes. By now this is done physically using disks, which is becoming unbearable with the amount of software steadily increasing. EDS is currently under standardization in the ARINC 666 committee, which includes the main players Boeing and Airbus, as well as their maintenance partners. Obviously, electronic shipment via cable-based and wireless connections faces severe security threats, such that one should better check with maximal scrutiny whether the mechanisms actually fulfill the security goals required, in particular integrity and authenticity.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463685,no,undetermined,0
Presenting A Method for Benchmarking Application in the Enterprise Architecture Planning Process Based on Federal Enterprise Architecture Framework,"One of the main challenges of the enterprise architecture planning process is its time consuming and to some extend having unrealistic results from this process under heading target architecture products. Getting best practices in this area can be to a large extent effective in speed up and quality enhanced of the results of enterprise architecture planning. Utilization of best practices in most methodologies and the enterprise architecture planning process guidelines namely EAP Methodology presented by Steven Spewak [14] also BSP Methodology produced by IBM [15], have been recommended. However there have been no presentation of any process or a specific method which would lead to benchmarking at enterprise architectural planning level. In this paper, a systematic and documented approach to employ benchmarking in the enterprise architecture planning process is being presented which can be used to assess the equally successful enterprises as best practices in target architecture documentation or by building a transition plan , utilize the enterprise architecture planning process. No doubt in order to have a basic and specific framework and also because of its vast application in governmental and nongovernmental organizations, federal enterprise architect reference models are utilized, though other frameworks and their presented reference models can also be used. Results obtained from proposed approach are indicative of reduced enterprise architecture planning process time especially the target architecture documentation, also risks reduction in this process and increased reliability in production.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529988,no,undetermined,0
Computerized Detection of Lung Tumors in PET/CT Images,"More and more hybrid PET/CT machines are being installed in medical centers across the country as combining computer tomography (CT) and positron emission tomography (PET) provides powerful and unique means in tumor diagnosis. Visual inspection of the images is a tedious and error-prone task and in many clinics the attenuation-uncorrected PET images are not examined by the physician, potentially missing an important source of information, especially for subtle tumors. We are developing a computer aided diagnosis software prototype that simultaneously processes the CT, attenuation-corrected PET, and attenuation-uncorrected PET volumes to detect tumors in the lungs. The system applies optimal thresholding and multiple gray-level thresholding with volume criterion to extract the lungs and to detect tumor candidates, respectively. A fuzzy logic based approach is used to reduce false-positive tumors. The remaining set of tumor candidates are ranked according to their likelihood of being actual tumors. We show the preliminary results of a retrospective evaluation of clinical PET/CT images",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4462257,no,undetermined,0
Sensor Validation within a Pervasive Medical Environment,"Pervasive patient sensing devices generate large quantities of wireless data. This data needs to be transmitted to central medical servers or mobile practitioners for real-time analysis. Various factors can affect the """"quality"""" of our patient data. These include: wireless interference (e.g. access point or radio failure) and/or Sensor failure. Vital patient data packets may be lost resulting in an incorrect diagnosis. Patient sensor failure is a reality. It is imperative that sensor failure is detected as soon as possible to ensure a higher QoS is provided. Presented is a Data Management System-Validation Model (DMS-VM). It is designed to manage wireless interference and sensor failure in a controlled and intelligent manner. The DMS-VM samples multiple patient vital sign readings and intelligently filters this data to verify its integrity based on an agent middleware platform. This novel approach provides higher QoS within context aware medical environments. The DMS-VM experimental prototype is presented.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4178781,no,undetermined,0
Web-based colorimetric sensing for food quality monitoring,"The work presented in this paper outlines a novel technique for remote food quality control over the Internet by using web based image processing. The colour change of a colorimetric sensor was captured with a wireless camera and the data was transmitted to a PC, which uploaded the information to the web. A software system for colour analysis was developed to process the data locally. Quantitative colour information which reflects the quality of the food product can be deduced remotely using this technique. This novel technique was applied to the monitoring of fish spoilage in packaged fish. The on-package sensors detected the release of spoilage products, typically the amines, from the fish and gave a visible colour change which was captured by the wireless camera. The colour information obtained through the web, when processed remotely using the in-house developed software, accurately reflected the state of the product. This technology reduces the labour requirements in food quality monitoring and can be applied to all colorimetric sensors.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4178754,no,undetermined,0
Using Data Confluences in a Distributed Network with Social Monitoring to Identify Fault Conditions,This paper discusses the potential benefits of socially attentive monitoring in multi-agent systems. A multi-agent system with this feature is shown to detect and identify when an individual within the network fails to operate correctly. The system that has been developed is capable of detecting a range of common faults such as stuck at zero by allowing communication between peers within a software agent network. Further adaption to the model allows an improvement in system response without introduction of specific control design algorithms,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4178180,no,undetermined,0
Improving software reliability and productivity via mining program source code,"A software system interacts with third-party libraries through various APIs. Insufficient documentation and constant refactorings of third-party libraries make API library reuse difficult and error prone. Using these library APIs often needs to follow certain usage patterns. These patterns aid developers in addressing commonly faced programming problems such as what checks should precede or follow API calls, how to use a given set of APIs for a given task, or what API method sequence should be used to obtain one object from another. Ordering rules (specifications) also exist between APIs, and these rules govern the secure and robust operation of the system using these APIs. These patterns and rules may not be well documented by the API developers. Furthermore, usage patterns and specifications might change with library refactorings, requiring changes in the software that reuse the library. To address these issues, we develop novel techniques (and their supporting tools) based on mining source code, assisting developers in productively reusing third party libraries to build reliable and secure software.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536384,no,undetermined,0
Development of Intelligent Visual Inspection System (IVIS) for Bottling Machine,"This paper presents a research on developing an intelligent visual inspection system (IVIS) for bottling machine, focusing on the development of image processing framework for defect detection. The objective of the research is to contribute a method on modeling, integrating and enhancing IVIS for the process of quality control in industrial area. IVIS application for quality control was studied using plastic bottles on a production line simulation. An experiment had done by using developed software and special equipments such as conveyor belt, lighting source, and a Web camera (Webcam) to capture the image. The experiment result shows that the system is accurate enough to detect moving object on the speed at 106 rpm with the accuracy of the image acquisition is 94.264%.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4142152,no,undetermined,0
The Fault Diagnosis of a Class of Nonlinear Stochastic Time-delay systems,"This paper presents a new fault detection algorithm for a class of nonlinear stochastic time-delay systems. Different from the classical fault detection design, a fault detection filter with an output observer and a consensus filter is constructed for fault detecting. Simulations are provided to show the efficiency of the proposed approach.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4141923,no,undetermined,0
24/7 Software Development in Virtual Student Exchange Groups: Redefining the Work and Study Week,"A concept of time zone driven, 24/7-week software development in a Virtual Student Exchange (VSX) environment is being defined, developed and applied to explore reliable and efficient continuous modes of work/study processes. The overall goal is to assess the suitability and benefits of this innovative approach to teaching and learning in order to increase the efficiency and effectiveness of these processes. This new methodology aims to address industry needs for training in international teaming, to enrich students' experience, and to improve the quality of education in the participating institutions. The techniques and tools discussed here create an integrated framework for international collaboration among teaming groups of students in practice and team oriented engineering education. This paper also aims to justify the need, merits, and feasibility of the virtual collaboration student exchange teaching program between educational institutions separated by three 8- hour time zones: the Faculty of Electronic Engineering of the Wroclaw University of Technology in Poland (WUT), the Faculty of Electrical and Computer Engineering at the University of Arizona, Tucson, USA (UA) and the Faculty of Engineering, Software Engineering Group at University of Technology, Sydney, Australia (UTS). The paper defines the proposed methodology, reviews the tools and processes involved, and finally reports preliminary results.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4141698,no,undetermined,0
A Method for Detecting and Measuring Architectural Layering Violations in Source Code,"The layered architecture pattern has been widely adopted by the developer community in order to build large software systems. The layered organization of software modules offers a number of benefits such as reusability, changeability and portability to those who are involved in the development and maintenance of such software systems. But in reality as the system evolves over time, rarely does the actual source code of the system conform to the conceptual horizontal layering of modules. This in turn results in a significant degradation of system maintainability. In order to re-factor such a system to improve its maintainability, it is very important to discover, analyze and measure violations of layered architecture pattern. In this paper we propose a technique to discover such violations in the source code and quantitatively measure the amount of non-conformance to the conceptual layering. The proposed approach evaluates the extent to which the module dependencies across layers violate the layered architecture pattern. In order to evaluate the accuracy of our approach, we have applied this technique to discover and analyze such violations to a set of open source applications and a proprietary business application by taking the help of domain experts wherever possible.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4137415,no,undetermined,0
"Fault Tolerance using """"Parallel Shadow Image Servers (PSIS)"""" in Grid Based Computing Environment","This paper presents a critical review of the existing fault tolerance mechanism in grid computing and the overhead involved in terms of reprocessing or rescheduling of jobs, if in case a fault arisen. For this purpose we suggested the parallel shadow image server (PSIS) copying techniques in parallel to the resource manager for having the check points for rescheduling of jobs from the nearest flag, if in case the fault is detected. The job process is to be scheduled from the resource manager node to the worker nodes and then its' submitted back by the worker nodes in serialized form to the parallel shadow image servers from the worker nodes after the pre-specified amount of time, which we call the recent spawn or the flag check point for rescheduling or reprocessing of job. If the fault is arisen then the rescheduling is done from the recent check point and submitted to the worker node from where the job was terminated. This will not only save time but will improve the performance up to major extent",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4136949,no,undetermined,0
Fault Tolerant Job Scheduling in Computational Grid,"In large-scale grids, the probability of a failure is much greater than in traditional parallel systems [I]. Therefore, fault tolerance has become a crucial area in grid computing. In this paper, we address the problem of fault tolerance in term of resource failure. We devise a strategy for fault tolerant job scheduling in computational grid. Proposed strategy maintains history of the fault occurrence of resource in grid information service (GIS). Whenever a resource broker has job to schedule it uses the resource fault occurrence history information from GIS and depending on this information use different intensity of check pointing and replication while scheduling the job on resources which have different tendency towards fault. Using check pointing proposed scheme can make grid scheduling more reliable and efficient. Further, it increases the percentage of jobs executed within specified deadline and allotted budget, hence helping in making grid trustworthy. Through simulation we have evaluated the performance of the proposed strategy. The experimental results demonstrate that proposed strategy effectively schedule the grid jobs in fault tolerant way in spite of highly dynamic nature of grid",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4136898,no,undetermined,0
Experience-driven selective scan for 802.11 networks,"The current use of the IEEE 802.11 protocol does not fully meet the full requirements of real-time applications. During the handoff period the STA cannot receive traffic and the quality of these applications is therefore reduced. A significant cause of this latency is that the STA normally scans all possible channels before synchronizing one of them. Considering that normally several channels are empty and that the 802.11 infrastructure architecture provides for fixed access points, it is possible to reduce the overall latency by reducing the number of channels to be scanned. In this paper we show an algorithm for selectively scanning just one probable channel. If such a channel is not available, the algorithm selects a second candidate until it scans all channels. These probabilities depend on the movements inside that LAN of previous STA. We show motivations and specifications of the algorithm",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4129891,no,undetermined,0
Binary Alpha-plane Assisted Motion Estimation of MPEG-4 Arbitrarily Shaped Video Objects,"In this paper, we propose a fast motion estimation algorithm of arbitrarily shaped video object in MPEG-4. The proposed algorithm incorporates the binary alpha-plane and the extended contour to predict accurately the motion vectors of boundary macroblocks so that the conventional fast motion estimation algorithms can be employed to search the motion vectors of opaque macro-blocks using the motion vectors of the neighboring boundary macro-blocks as the initial center. Experimental results show that the proposed algorithm requires low computation complexity while provides good motion compensation quality",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4129104,no,undetermined,0
Fault Injection-based Test Case Generation for SOA-oriented Software,"The concept of service oriented architecture (SOA) implies a rapid construction of a software system with components as published Web services. How to effectively and efficiently test and assess available Web services with similar functionalities published by different service providers remains a challenge. In this paper, we present a step-by-step fault injection-based automatic test case generation approach. Preliminary test results are also reported",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4125735,no,undetermined,0
Modifying weber fraction law to postprocessing and edge detection applications,"A postprocessing scheme is proposed to enhance compressed images. The main objective is to obtain improvements that are pertinent to the properties of human visual system. The proposed scheme implements Weber fraction (also called contrast sensitivity) to enhance the appearance of the current block by incorporating information from adjacent blocks. The ratio DeltaI/I is found between the mean of a line of pixels in the current block and two points, each resides on the boundary of an adjacent block, that are the continuation of the chosen line. To avoid biasing toward low intensity values and to preserve the symmetry of the sensitivity curve, I was replaced by the maximum of the actual mean value and the corresponding value of the negative image or simply max(mean, 255-mean). If DeltaI/I is less than a threshold, the chosen line is replaced with a one fitting the original data and the two boundary points. Although PSNR improvement is <0.3 dB, the resultant image is visually more pleasing as will be demonstrated experimentally. The algorithm can be easily modified to perform as an edge detection scheme by finding DeltaI/I between any pixel and its 8 neighbours. The maximum is then taken. A new histogram thresholding is then applied to discriminate the edge pixels. Experimental results indicate a superior capability of the proposed scheme to detect edges of objects that are close in intensity to their background. Some comparisons with Sobel operator are also demonstrated.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4537346,no,undetermined,0
A Proposal and Empirical Validation of Metrics to Evaluate the Maintainability of Software Process Models,"Software measurement is essential for understanding, defining, managing and controlling the software development and maintenance processes and it is not possible to characterize the various aspects of development in a quantitative way without having a deep understanding of software development activities and their relationships The current competitive marketplace calls for the continuous improvement of processes and as consequence companies have to change their processes in order to adapt to these new emerging needs. It implies the continuous change of their software process models and therefore, it is fundamental to facilitate the evolution of these models by evaluating its easiness of maintenance (maintainability). In this paper we introduce a set of metrics for software process models and discuss how these can be used as maintainability indicators. In particular, we report the results of a family of experiments that assess relationships between the structural properties, as measured by the metrics, of the process models and their maintainability. As a result a set of useful metrics to evaluate the software process models maintainability, have been obtained",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4124506,no,undetermined,0
Mobile Computing instead of paper based documentation in German Rheumatology,"Our objective was the integration of mobile computing in our web based documentation software DocuMed.rh to improve process organisation and quality of care. We focused on self-administered standardized patient questionnaires which are implemented in DocuMed.rh. Validity of online obtained data and the capability of disabled patients to handle a Tablet PC were assigned. On a regularly scheduled visit 117 patients completed prearranged sets of self-administered questionnaires as a paper-pencil and an electronic version using a Tablet PC in a cross-over design. Patients experiences with the Tablet PC and history of computer/internet use were assessed. Positive ethics approval and signed patients consents were obtained. Though only 65% of the patients reported computer experiences no major problems with the Tablet PC occurred. Scores obtained by direct data entry on the Tablet PC did not differ significantly from the scores obtained by the paper-pencil questionnaires in the complete group and in subgroups. Application of self-administered questionnaires on the new medium Tablet PC is efficient and capable in patients with inflammatory rheumatic disease. Mobile obtained data are rapidly available and can easily be merged with clinical data, thereby contributing intensely to improved patient care.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4124123,no,undetermined,0
Building Statistical Test-Cases for Smart Device Software-An Example,"Statistical testing (ST) of software or logic-based components can produce dependability information on such components by yielding an estimate for their probability of failure on demand. An example of software-based components that are increasingly used within safety-related systems e.g. in the nuclear industry, are smart devices. Smart devices are devices with intelligence, capable of more than merely representing correctly a sensed quantity but of functionality such as processing data, self-diagnosis and possibly exchange of data with other devices. Examples are smart transmitters or smart sensors. If such devices are used in a safety-related context, it is crucial to assess whether they fulfil the dependability requirements posed on them to ensure they are dependable enough to be used within the specific safety-related context. This involves making a case for the probability of systematic failure of the smart device. This failure probability is related to faults present in the logic or software-based part of the device. In this paper we look at a technique that can be used to establish a probability of failure for the software part of a smart monitoring unit. This technique is """"statistical testing"""" (ST). Our aim is to share our own experience with ST and to describe some of the issues we have encountered so far on the way to perform ST on this device software.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123715,no,undetermined,0
Software Estimation- An Introdution,"In many cases a successful project is one that meets a time and cost target that has been estimated by someone somewhere. The quality of this estimate will set the underlying probability of successfully completing the project. This presentation will discuss the basic factors that effect software estimation, and aims to show the benefits of formalising the estimation process to increase the understanding of key decisions that form the basis of an estimate. It will introduce the critical factors that ensure that we base our projects on information and assumptions that are clearly defined; easily understood and credible. It will also introduce the basic laws that govern software development, and the effect on project schedule.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123431,no,undetermined,0
Pilot testing the DigiQUAL<sup>TM</sup> protocol: lessons learned,"The association of research libraries (ARL) is developing the DigiQUALtrade protocol to assess the service quality provided by digital libraries (DLs). In 2005, statements about DL service quality were put through a two-step validation process with DL developers and then with users in an online survey.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4119184,no,undetermined,0
PLP: Towards a realistic and accurate model for communication performances on hierarchical cluster-based systems,"Today, due to many reasons, such as the inherent heterogeneity, the diversity, and the continuous evolving of actual computational supports, writing efficient parallel applications on such systems represents a great challenge. One way to answer this problem is to optimize communications of such applications. Our objective within this work is to design a realistic model able to accurately predict the cost of communication operations on execution environments characterized by both heterogeneity and hierarchical structure. We principally aim to guarantee a good quality of prediction with a neglected additional overhead. The proposed model was applied on point-to-point and collective communication operations and showed by achieving experiments on a hierarchical cluster-based system with heterogeneous resources that the predicted performances are close to measured ones.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536486,no,undetermined,0
Efficient Mutant Generation for Mutation Testing of Pointcuts in Aspect-Oriented Programs,"Fault-based testing is an approach where the designed test data is used to demonstrate the absence of a set of prespecified faults, typically being frequently occurring faults. Mutation testing is a fault-based testing technique used to inject faults into an existing program, i.e., a variation of the original program and see if the test suite is sensitive enough to detect common faults. Aspect-oriented programming (AOP) provides new modularization of software systems by encapsulating crosscutting concerns. AspectJ, a language designed to support AOP uses abstractions like pointcuts, advice, and aspects to achieve AOP's primary functionality. Developers tend to write pointcut expressions with incorrect strength, thereby selecting additional events than intended to or leaving out necessary events. This incorrect strength causes aspects, the set of crosscutting concerns, to fail. Hence there is a need to test the pointcuts for their strength. Mutation testing of pointcuts includes two steps: creating effective mutants (variations) of a pointcut expression and testing these mutants using the designed test data. The number of mutants for a pointcut expression is usually large due to the usage of wildcards. It is tedious to manually identify effective mutants that are of appropriate strength and resemble closely the original pointcut expression. Our framework automatically generates mutants for a pointcut expression and identifies mutants that resemble closely the original expression. Then the developers could use the test data for the woven classes against these mutants to perform mutation testing.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4144722,no,undetermined,0
A Distributed Fault-Tolerant Algorithm for Event Detection Using Heterogeneous Wireless Sensor Networks,"Distributed event detection using wireless sensor networks has received growing interest in recent years. In such applications, a large number of inexpensive and unreliable sensor nodes are distributed in a geographical region to make firm and accurate local decisions about the presence or absence of specific events based on their sensor readings. However, sensor readings can be unreliable, due to either noise in the sensor readings or hardware failures in the devices, and may cause nodes to make erroneous local decisions. We present a general fault-tolerant event detection scheme that allows nodes to detect erroneous local decisions based on the local decisions reported by their neighbors. This detection scheme does not assume homogeneity of sensor nodes and can handle cases where nodes have different accuracy levels. We prove analytically that the derived fault-tolerant estimator is optimal under the maximum a posteriori (MAP) criterion. An equivalent weighted voting scheme is also derived. Further, we describe two new error models that take into account the neighbor distance and the geographical distributions of the two decision quorums. These models are particularly suitable for detection applications where the event under consideration is highly localized. Our fault-tolerant estimator is simulated using a network of 1024 nodes deployed randomly in a square region and assigned random probability of failures",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4177877,no,undetermined,0
Assessment of Data Diversity Methods for Software Fault Tolerance Based on Mutation Analysis,"One of the main concerns in safety-critical software is to ensure sufficient reliability because proof of the absence of systematic failures has proved to be an unrealistic goal. fault-tolerance (FT) is one method for improving reliability claims. It is reasonable to assume that some software FT techniques offer more protection than others, but the relative effectiveness of different software FT schemes remains unclear. We present the principles of a method to assess the effectiveness of FT using mutation analysis. The aim of this approach is to observe the power of FT directly and use this empirical process to evolve more powerful forms of FT. We also investigate an approach to FT that integrates data diversity (DD) assertions and TA. This work is part of a longer term goal to use FT in quantitative safety arguments for safety critical systems.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4144725,no,undetermined,0
"Service replication in Grids: Ensuring consistency in a dynamic, failure-prone environment","A major challenge in a service-oriented environment as a Grid is fault tolerance. The more resources and services involved, the more complicated and error-prone becomes the system. Migol (Luckow and Schnor, 2008) is a Grid middleware, which addresses the fault tolerance of Grid applications and services. Migol's core component is its registry service called application information service (AIS). To achieve fault tolerance and high availability the AIS is replicated on different sites. Since a registry is a stateful Web service, the replication of the AIS is no trivial task. In this paper, we present our concept for active replication of Grid services. Migol's Replication Service uses a token-based algorithm and certificate-based security to provide secure group communication. Further, we show in different experiments that active replication in a real Grid environment is feasible.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536211,no,undetermined,0
Hiddenness Control of Hidden Markov Models and Application to Objective Speech Quality and Isolated-Word Speech Recognition,"Markov models are a special case of hidden Markov models (HMM). In Markov models the state sequence is visible, whereas in a hidden Markov model the underlying state sequence is hidden and the sequence of observations is visible. Previous research on objective techniques for output-based speech quality (OBQ) showed that the state transition probability matrix A of a Markov model is capable of capturing speech quality information. On the other hand similar experiments using HMMs showed that the observation symbol probability matrix B is more effective at capturing the speech quality information. This shows that the speech quality information in A matrix of a Markov model shifts to the B matrix of an HMM. An HMM can have varying degrees of hiddenness, which can be intuitively guessed from the entries of its observation probability matrix B for the discrete models. In this paper, we propose a visibility measure to assess the hiddenness of a given HMM, and also a method to control the hiddenness of a discrete HMM. We test the advantage of implementing hiddenness control in output-based objective speech quality (OBQ) and isolated-word speech recognition. Our test results suggest that hiddenness control improves the performance of HMM-based OBQ and might be useful for speech-recognition as well.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4176728,no,undetermined,0
Model-based fault localization in large-scale computing systems,"We propose a new fault localization technique for software bugs in large-scale computing systems. Our technique always collects per-process function call traces of a target system, and derives a concise execution model that reflects its normal function calling behaviors using the traces. To find the cause of a failure, we compare the derived model with the traces collected when the system failed, and compute a suspect score that quantifies how likely a particular part of call traces explains the failure. The execution model consists of a call probability of each function in the system that we estimate using the normal traces. Functions with low probabilities in the model give high anomaly scores when called upon a failure. Frequently-called functions in the model also give high scores when not called. Finally, we report the function call sequences ranked with the suspect scores to the human analyst, narrowing further manual localization down to a small part of the overall system. We have applied our proposed method to fault localization of a known non-deterministic bug in a distributed parallel job manager. Experimental results on a three-site, 78-node distributed environment demonstrate that our method quickly locates an anomalous event that is highly correlated with the bug, indicating the effectiveness of our approach.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536310,no,undetermined,0
Workflow Quality of Service Management using Data Mining Techniques,"Organizations have been aware of the importance of quality of service (QoS) for competitiveness for some time. It has been widely recognized that workflow systems are a suitable solution for managing the QoS of processes and workflows. The correct management of the QoS of workflows allows for organizations to increase customer satisfaction, reduce internal costs, and increase added value services. In this paper we show a novel method, composed of several phases, describing how organizations can apply data mining algorithms to predict the QoS for their running workflow instances. Our method has been validated using experimentation by applying different data mining algorithms to predict the QoS of workflow",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4155473,no,undetermined,0
Investigating software Transactional Memory on clusters,"Traditional parallel programming models achieve synchronization with error-prone and complex-to-debug constructs such as locks and barriers. Transactional Memory (TM) is a promising new parallel programming abstraction that replaces conventional locks with critical sections expressed as transactions. Most TM research has focused on single address space parallel machines, leaving the area of distributed systems unexplored. In this paper we introduce a flexible Java Software TM (STM) to enable evaluation and prototyping of TM protocols on clusters. Our STM builds on top of the ProActive framework and has as an underlying transactional engine the state-of-the-art DSTM2. It does not rely on software or hardware distributed shared memory for the execution. This follows the transactional semantics at object granularity level and its feasibility is evaluated with non-trivial TM-specific benchmarks.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536340,no,undetermined,0
P2D-3 Objective Performance Testing and Quality Assurance of Medical Ultrasound Equipment,"The goal of this study was to develop a test protocol that contains the minimum set of performance measurements for predicting the clinical performance of ultrasound equipment and that is based on objective assessments by computerized image analysis. The post-processing look-up-table (LUT) is measured and linearized. The elevational focus (slice thickness) of the transducer is estimated and the in plane transmit focus is positioned at the same depth. The developed tests are: echo level dynamic range (dB), contrast resolution (i.e., """"gamma"""" of display, #gray levels/dB) and -sensitivity, overall system sensitivity, lateral sensitivity profile, dead zone, spatial resolution, and geometric conformity of display. The concept of a computational observer is used to define the lesion signal-to-noise ratio, SNR<sub>L</sub> (or Mahalanobis distance), as a measure for contrast sensitivity. The whole performance measurement protocol has been implemented in software. Reports are generated that contain all the information about the measurements and results, such as graphs, images and numbers. The software package may be viewed and a run-time version downloaded at the website: http://www.qa4us.eu",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4152267,no,undetermined,0
WLC12-5: A TDMA-Based MAC Protocol for Industrial Wireless Sensor Network Applications using Link State Dependent Scheduling,"Existing TDMA-based MAC protocols for wireless sensor networks are not specifically built to consider the harsh conditions of industrial environments where the communication channel is prone to signal fading. We propose a TDMA-based MAC protocol for wireless sensor networks built for industrial applications that uses link state dependent scheduling. In our approach, nodes gather samples of the channel quality and generate prediction sets from the sample sets in independent slots. Using the prediction sets, nodes only wake up to transmit/receive during scheduled slots that are predicted to be clear and sleep during scheduled slots that may potentially cause a transmitted signal to fade. We simulate our proposed protocol and compare its performance with a general non-link state dependent TDMA protocol and a CSMA protocol. We found that our protocol significantly improves packet throughput as compared to both the general non-link state dependent TDMA protocol and CSMA protocol. We also found that in conditions which are not perfect under our assumptions, the performance of our protocol degrades gracefully.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4151314,no,undetermined,0
QRPp1-1: User-Level QoS Assessment of a Multipoint-to-Multipoint TV Conferencing Application over IP Networks,"This paper studies a multipoint-to-multipoint TV conferencing application over IP networks and assesses its user-level QoS with two types of QoS mapping. In utilizing the application, the user perceives quality of the communication with every other conferee; we refer to the quality as individual user-level QoS to a conferee. According to the individual user-level QoS, he/she totally judges the quality of the application, which is refer to as overall user-level QoS for the user. The overall user-level QoS of the application can be affected by the individual one to each conferee; therefore, it is difficult to clarify QoS parameters which affect the overall user-level QoS. This paper tackles the problem by utilizing two types of QoS mapping: mapping between the two kinds of user-level QoS and that between user-level QoS and application-level QoS. In this paper, an experiment with a simple task by three conferees was carried out. The user-level QoS is assessed by one of the psychometric methods. As a result of the two types of QoS mapping, we find two interesting results. First, when a user communicates with the other two conferees, the lower individual user-level QoS has more effect on the overall user-level QoS than the higher one. Second, the individual user-level QoS can depend on not only its application-level QoS but also that of the other conferees.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4151094,no,undetermined,0
QRP01-6: Resource Optimization Subject to a Percentile Response Time SLA for Enterprise Computing,"We consider a set of computer resources used by a service provider to host enterprise applications subject to service level agreements. We present an approach for resource optimization in such an environment that minimizes the total cost of computer resources used by a service provider for an enterprise application while satisfying the QoS metric that the response time for executing service requests is statistically bounded. That is, gamma% of the time the response time is less than a pre-defined value. This QoS metric is more realistic than the mean response time typically used in the literature. Numerical results show the applicability of the approach and validate its accuracy.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4151053,no,undetermined,0
OPN09-03: GMPLS Signaling Feedback for Encompassing Physical Impairments in Transparent Optical Networks,"Next generation GMPLS networks will be characterized by domains of transparency, in which the end-to-end optical signal quality has to be guaranteed. Currently GMPLS does not take into account the evaluation of physical impairments. Thus just limited size domains of transparency, where physical impairments can be neglected, are practically achievable. This study utilizes GMPLS signaling protocol extensions to encompass the optical layer physical impairments. The proposed approach allows to detect during the signaling phase whether lightpaths cannot be set up because of unacceptable optical signal quality. In this case successive set up attempts are performed, selecting the alternative routes with three schemes which exploit the feedback information of the signaling messages. Numerical results show that the proposed extensions are able to significantly decrease the lightpath blocking probability due to physical impairments in both static and dynamic conditions.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4151038,no,undetermined,0
NGL03-4: An Interoperability Mechanism for Seamless Interworking between WLAN and UMTS-HSDPA Networks,"Future wireless communication systems are expected to provide seamless inter-working between existing and 3G radio networks providing the user with a wide variety of services, while maintaining a large area of coverage and minimum user QoS requirements. In this paper, a new mechanism that implements interoperability between HSDPA and WLAN systems is proposed. The proposed interoperability mechanism is activated via the optimization of a suitably defined cost function, which takes into account all the appropriate system level parameters that trigger the interoperability process. The performance evaluation of the proposed scheme is assessed by means of a software - based simulation platform. A number of simulations have been carried out in order to demonstrate the performance enhancements achieved by the proposed mechanism in terms of user throughput, handovers statistics, and system throughput.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4150888,no,undetermined,0
Detection and Interpretation of Text Information in Noisy Video Sequences,"Text superimposed on the video frames provides supplemental but important information for video indexing and retrieval. The detection and recognition of text from video is thus an important issue in automated content-based indexing of visual information in video archives. Text of interest is not limited to static text. They could be scrolling in a linear motion where only part of the text information is available during different frames of the video. The problem is further complicated if the video is corrupted with noise. An algorithm is proposed to detect, classify and segment both static and simple linear moving text in complex noisy background. The extracted texts are further processed using averaging to attain a quality suitable for text recognition by commercial optical character recognition (OCR) software",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4150265,no,undetermined,0
Error and Rate Joint Control for Wireless Video Streaming,"In this paper, a precise error-tracking scheme for robust transmission of real-time video streaming over wireless IP network is presented. By utilizing negative acknowledgements from feedback channel, the encoder can precisely calculate and track the propagated errors by examining the backward motion dependency. With this precise tracking, the error-propagation effects can be terminated completely by INTRA refreshing the affected macroblocks. In addition, due to lots of INTRA macroblocks refresh will entail a large increase of the output bit rate of a video encoder, several bit rate reduction techniques are proposed. They can be jointly used with INTRA refresh scheme to obtain uniform video quality performance instead of only changing the quantization scale. The simulations show that both control strategies yield significant video quality improvements in error-prone environments",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4149487,no,undetermined,0
Media Streaming via TFRC: An Analytical Study of the Impact of TFRC on User-Perceived Media Quality,"<div style=""""font-variant: small-caps; font-size: .9em;"""">First Page of the Article</div><img class=""""img-abs-container"""" style=""""width: 95%; border: 1px solid #808080;"""" src=""""/xploreAssets/images/absImages/04146897.png"""" border=""""0"""">",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4146897,no,undetermined,0
Exploiting Reference Frame History in H.264/AVC Motion Estimation,"Motion estimation is the most crucial and time-consuming part of the H.264/AVC video compression standard. The introduction of motion search of variable block sizes in multiple reference frames has significantly increased the computational complexity. This paper proposes a fast motion estimation algorithm, most used reference first (MURF), based on the usage history of reference frames. The algorithm rearranges the search order of the reference frames based on the selection probability of the reference frames in coding the current frame. The experimental results show that the proposed algorithm, when compared to the best algorithm in H.264 reference software, achieves on average 60% reduction in search points and 52% reduction in motion estimation time with comparable video quality and negligible increase in bit-rate and memory",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145418,no,undetermined,0
Finding Errors in Interoperating Components,"Two or more components (e.g., objects, modules, or programs) interoperate when they exchange data, such as XML data. Currently, there is no approach that can detect a situation at compile time when one component modifies XML data so that it becomes incompatible for use by other components, delaying discovery of errors to runtime. Our solution, a verifier for interoperating components for finding logic faults (Viola) builds abstract programs from the source code of components that exchange XML data. Viola symbolically executes these abstract programs thereby obtaining approximate specifications of the data that would be output by these components. The computed and expected specifications are compared to find errors in XML data exchanges between components. We describe our approach, implementation, and give our error checking algorithm. We used Viola on open source and commercial systems and discovered errors that were not detected during their design and testing.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273229,no,undetermined,0
Greensand Mulling Quality Determination Using Capacitive Sensors,"Cast iron foundries typically use molds made from a mixture of sand, clay, and water called greensand. The mixing of the clay and water into the sand requires the clay to be smeared around the sand particles in a very thin layer, which allows the mold to have strength while allowing gas to escape. This mixing is called mulling and takes place in a machine called a muller. At present, the industry uses electrical resistance measurements to determine water quantity in the muller. The resistance measurements can not accurately predict the quality of mulling due to binding of the water to sodium and calcium ions in the clay. Poorly mixed greensand has a high resistance when the water is concentrated in a few areas, then a medium mixed greensand has a lower resistance because the water present between the sensors, and a well mulled sand has a higher resistance when the clay binds the water. This paper investigates the feasibility of using capacitive sensors to measure mulling quality using the simulation software Ansoft Maxwell. A second investigation of this paper is to find the ability of capacitance sensors to determine the drying effect of molds delayed in the casting process.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4480245,no,undetermined,0
Adaptive Kalman Filtering for anomaly detection in software appliances,"Availability and reliability are often important features of key software appliances such as firewalls, web servers, etc. In this paper we seek to go beyond the simple heartbeat monitoring that is widely used for failover control. We do this by integrating more fine grained measurements that are readily available on most platforms to detect possible faults or the onset of failures. In particular, we evaluate the use of adaptive Kalman Filtering for automated CPU usage prediction that is then used to detect abnormal behaviour. Examples from experimental tests are given.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544581,no,undetermined,0
Technique Integration for Requirements Assessment,"In determining whether to permit a safety-critical software system to be certified and in performing independent verification and validation (IV&V) of safety- or mission-critical systems, the requirements traceability matrix (RTM) delivered by the developer must be assessed for accuracy. The current state of the practice is to perform this work manually, or with the help of general-purpose tools such as word processors and spreadsheets Such work is error-prone and person-power intensive. In this paper, we extend our prior work in application of Information Retrieval (IR) methods for candidate link generation to the problem of RTM accuracy assessment. We build voting committees from five IR methods, and use a variety of voting schemes to accept or reject links from given candidate RTMs. We report on the results of two experiments. In the first experiment, we used 25 candidate RTMs built by human analysts for a small tracing task involving a portion of a NASA scientific instrument specification. In the second experiment, we randomly seeded faults in the RTM for the entire specification. Results of the experiments are presented.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384177,no,undetermined,0
Early Models for System-Level Power Estimation,"Power estimation and verification have become important aspects of System-on-Chip (SoC) design flows. However, rapid and effective power modeling and estimation technologies for complex SoC designs are not widely available. As a result, many SoC design teams focus the bulk of their efforts on using detailed low-level models to verify power consumption. While such models can accurately estimate power metrics for a given design, they suffer from two significant limitations: (1) they are only available late in the design cycle, after many architectural features have already been decided, and (2) they are so detailed that they impose severe limitations on the size and number of workloads that can be evaluated. While these methods are useful for power verification, architects require information much earlier in the design cycle, and are therefore often limited to estimating power using spreadsheets where the expected power dissipation of each module is summed up to predict total power. As the model becomes more refined, the frequency that each module is exercised may be added as an additional parameter to further increase the accuracy. Current spreadsheets, however, rely on aggregate instruction counts and do not incorporate either time or input data and thus have inherent inaccuracies. Our strategy for early power estimation relies on (i) measurements from real silicon, (ii) models built from those measurements models that predict power consumption for a variety of processor micro-architectural structures and (iii)FPGA-based implementations of those models integrated with an FPGA-based performance simulator/emulator. The models will be designed specifically to be implemented within FPGAs. The intention is to integrate the power models with FPGA-based full-system, functional and performance simulators/emulators that will provide timing and functional information including data values. The long term goal is to provide relative power accuracy and power trends useful to arc- - hitects during the architectural phase of a project, rather than precise power numbers that would require far more information than is available at that time. By implementing the power models in an FPGA and driving those power models with a system simulator/emulator that can feed the power models real data transitions generated by real software running on top of real operating systems, we hope to both improve the quality of early stage power estimation and improve power simulation performance.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620146,no,undetermined,0
The Design of a Multimedia Protocol Analysis Software Environment,"We have developed a variant of Estelle, called Time-Estelle which is able to express multimedia quality of service (QoS) parameters, synchronisation scenarios, and time-dependent and probabilistic behaviours of multimedia protocols. We have developed an approach to verifying a multimedia protocol specified in Time-Estelle. To predict the performance of a multimedia system, we have also developed a method for the performance analysis of a multimedia protocol specified in Time-Estelle. However, without the support of a software environment to automate the processes, verification and performance analysis methods would be very time-consuming. This paper describes the design of such a software environment.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385206,no,undetermined,0
Hardware Failure Virtualization Via Software Encoded Processing,"In future, the decreasing feature size will make it much more difficult to built reliable microprocessors. Economic pressure will most likely result in the reliability of microprocessors being tuned for the commodity market. Dedicated reliable hardware is very expensive and usually slower than commodity hardware. Thus, software implemented hardware fault tolerance (SIHFT) will become essential for building safe systems. Existing SIHFT approaches either are not broadly applicable or lack the ability to reliably deal with permanent hardware faults. In contrast, Forin (1989) introduced the vital coded microprocessor which reliably detects transient and permanent hardware failures, but is not applicable to arbitrary programs. It requires a dedicated development process and special hardware. We extend Forin's Vital Code, so that it is applicable to arbitrary binary code which enables us to apply it to existing binaries or automatically during compile time. Furthermore, our approach does not require special purpose hardware.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384907,no,undetermined,0
Discovering Web Services Using Semantic Keywords,"With the increasing growth in popularity of Web services, the discovery of relevant services becomes a significant challenge. In order to enhance the service discovery is necessary that both the Web service description and the request for discovering a service explicitly declare their semantics. Some languages and frameworks have been developed to support rich semantic service descriptions and discover using ontology concepts. However, the manual creation of such concepts is tedious and error-prone and many users accustomed to automatic tools might not want to invert his time in obtaining this knowledge. In this paper we propose a system that assists to both service producers and service consumers in the discovery of semantic keywords which can be used to describe and discover Web services respectively. First, our system enhances semantically the list of keywords extracted from the elements that comprise the description of a Web service and the user keywords used for discover a service. Second, an ontology matching process is used to discovers matchings between the ontological terms of a service description and a request for service selection. Third, a subsumption reasoning algorithm tries to find service description(s) which match the user request.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384863,no,undetermined,0
Developing Intentional Systems with the PRACTIONIST Framework,"Agent-based systems have become a very attractive approach for dealing with the complexity of modern software applications and have proved to be useful and successful in some industrial domains. However, engineering such systems is still a challenge due to the lack of effective tools and actual implementations of very interesting and fascinating theories and models. In this area the so-called intentional stance of systems can be very helpful to efficiently predict, explain, and define the behaviour of complex systems, without having to understand how they actually work, but explaining them in terms of some mental qualities or attitudes, rather than their physical or design stance. In this paper we present the PRACTIONIST framework, that supports the development of PRACTIcal reasONIng sySTems according to the BDI model of agency, which uses some mental attitudes such as beliefs, desires, and intentions to describe and specify the behaviour of system components. We adopt a goal-oriented approach and a clear separation between the deliberation phase and the means-ends reasoning, and consequently between the states of affairs to pursue and the way to do it. Moreover, PRACTIONIST allows developers to implement agents that are able to reason about their beliefs and the other agents' beliefs, expressed by modal logic formulas.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384847,no,undetermined,0
Statistical Assessment of Global and Local Cylinder Wear,"Assessment of cylindricity has been traditionally performed on the basis of cylindrical crowns containing a set of points that are supposed to belong to a controlled cylinder. As such, all sampled points must lie within a crown. In contrast, the present paper analyzes the cylindricity for wear applications, in which a statistical trend is assessed, rather than to assure that all points fall within a given tolerance. Principal component analysis is used to identify the central axis of the sampled cylinder, allowing to find the actual (expected value of the) radius and axis of the cylinder. Application of k-cluster and transitive closure algorithms allow to identify particular areas of the cylinder which are specially deformed. For both, the local areas and the global cylinder, a quantile analysis allows to numerically grade the degree of deformation of the cylinder. The algorithms implemented are part of the CYLWEAR<sup>copy</sup> system and used to assess local and global wear cylinders.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384788,no,undetermined,0
QoS Proxy Architecture for Real Time RPC with Traffic Prediction,"Currently, there are many research works focused at the creation of architectures that support QoS for real time multimedia applications guarantees. However, those architectures do not support the traffic of RT-RPCs whose requirements (i.e. priority and deadline) are harder than those of multimedia applications. The aim of this work is to propose an architecture of QoS proxy for RT-RPCs that uses Box-Jenkins time series models in order to predict future traffic characteristics of RT-RPCs that pass through the proxy, allowing the anticipated allocation of the necessary resources to attend the predicted demand and the choice of policies aimed at the adaptation of the proxy to the states of its network environment.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384556,no,undetermined,0
Estimating the Required Code Inspection Team Size,"Code inspection is considered an efficient method for detecting faults in software code documents. The number of faults not detected by inspection should be small. Several methods have been suggested for estimating the number of undetected faults. These methods include the fault injection method that is considered to be quite laborious, capture recapture methods that avoid the problems of code injection and the Detection Profile Method for cases where capture recapture methods do not provide sufficient accuracy. The Kantorowitz estimator is based on a probabilistic model of the inspection process and enables the estimating the number of inspectors required to detect a specified fraction of all the faults of a document as well as the number of undetected faults. This estimator has proven to be satisfactory in inspection of user requirements documents. The experiments reported in this study suggest that it is also useful for code inspection.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384090,no,undetermined,0
Performance under failures of high-end computing,"Modern high-end computers are unprecedentedly complex. Occurrence of faults is an inevitable fact in solving large-scale applications on future Petaflop machines. Many methods have been proposed in recent years to mask faults. These methods, however, impose various performance and production costs. A better understanding of faults' influence on application performance is necessary to use existing fault tolerant methods wisely. In this study, we first introduce some practical and effective performance models to predict the application completion time under system failures. These models separate the influence of failure rate, failure repair, checkpointing period, checkpointing cost, and parallel task allocation on parallel and sequential execution times. To benefit the end users of a given computing platform, we then develop effective fault-aware task scheduling algorithms to optimize application performance under system failures. Finally, extensive simulations and experiments are conducted to evaluate our prediction models and scheduling strategies with actual failure trace.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5348807,no,undetermined,0
AIMS: An agent-based information management system in JBI-like environments,"One of the challenges for many mission-critical applications is how to determine the quality of information from distributed and heterogeneous data sources. In this paper we consider joint battlespace infosphere (JBI) or JBI-like collaborative information sharing environments, where the quality of published information often depends on the quality of the contributing information and of the sources that publish them. However, as the environment becomes larger and more diverse, it is becoming increasingly difficult for human operators to assess the quality of information from various data sources. To address this challenge, we develop AIMS, an agent-based information management system, to manage the quality of information in JBI-like environments, e.g., information trustworthiness. In our approach, each operator is associated with a software agent, called client agent. The client agent enables its operator to interact with the JBI repository via the services of query, publish, and subscribe. Moreover, the client agent collaborates with other agents to assess and learn the trustworthiness of information and the reliabilities of corresponding data sources from its pedigree and the feedback from operators.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4621767,no,undetermined,0
Wavelet application for determining missing cylinders,"In this article, wavelet analysis to analyze signals in the engine faults diagnose is studied. Taking advantage of the wavelet analysis tools of MATLAB 6.0 software, it calculates the wavelet transform coefficients of the normal and abnormal signals, analyzes differences between normal and abnormal signals by using the parameters, such as average evolution root, kurtosis factor and average index. Moreover, it put forward a new method, in which the missing cylinder fault can be detected by the wavelet analysis.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4798570,no,undetermined,0
Automated Model-Based Configuration of Enterprise Java Applications,"The decentralized process of configuring enterprise applications is complex and error-prone, involving multiple participants/roles and numerous configuration changes across multiple files, application server settings, and database decisions. This paper describes an approach to automated enterprise application configuration that uses a feature model, executes a series of probes to verify configuration properties, formalizes feature selection as a constraint satisfaction problem, and applies constraint logic programming techniques to derive a correct application configuration. To validate the approach, we developed a configuration engine, called Fresh, for enterprise Java applications and conducted experiments to measure how effectively Fresh can configure the canonical Java Pet Store application. Our results show that Fresh reduces the number of lines of hand written XML code by up to 92% and the total number of configuration steps by up to 72%.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384002,no,undetermined,0
Providing Support for Model Composition in Metamodels,"In aspect-oriented modeling (AOM), a design is described using a set of design views. It is sometimes necessary to compose the views to obtain an integrated view that can be analyzed by tools. Analysis can uncover conflicts and interactions that give rise to undesirable emergent behavior. Design models tend to have complex structures and thus manual model composition can be arduous and error- prone. Tools that automate significant parts of model composition are needed if AOM is to gain industrial acceptance. One way of providing automated support for composing models written in a particular language is to define model composition behavior in the metamodel defining the language. In this paper we show how this can be done by extending the UML metamodel with behavior describing symmetric, signature-based composition of UML model elements. We also describe an implementation of the metamodel that supports systematic composition of UML class models.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383998,no,undetermined,0
Assessing the Real Worth of Software Tools to Check the Healthiness Conditions of Automotive Software,"There are a number of software-controlled features in today's automotive vehicles making them comfortable, safer, entertaining, informative and even greener! The number of features is rapidly growing and so is the software content of automotive vehicles to meet these requirements. The software code that realises any one feature is, nowadays, often distributed across several Electronic Control Units (ECUs) as well. In order to produce highly reliable automotive vehicles, their increasingly complex software has to be of high-quality and this requires sophisticated tools and techniques within the automotive industry. One such category of tools statically check whether the software developed hold certain properties (healthiness conditions), such as checking that a variable is set before it is read, and that arithmetic operations do not lead to overflow. These tools typically generate a list of issues, which highlight potential areas of the code, where the healthiness conditions being checked might fail. In practice, the list of generated issues typically contains a significant number of false positives; i.e. issues that cannot lead to a genuine failure of a healthiness condition. This paper discusses the design of objective experiments and the initial stages of an ongoing automotive industry study to assess the real worth of such tools. Towards this end, relevant concepts such as healthiness conditions for software are explained and the various criteria used for the objective experiments are defined giving their rationale.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383626,no,undetermined,0
Enabling Architecture Changes in Distributed Web-Applications,"Engineering methods for Web applications that do not take changes of the system environment into account are in danger of planning across purposes with reality. Modern Web applications are characterized by dynamically evolving architectures of loosely coupled content sources, components and services from multiple organizations. The evolution of such ecosystems poses a problem to management and maintenance. Up-to-date architectural information about the components and their relationships is required in different places within the system. However, this is problematic, because, manual propagation of changes in system descriptions is both costly and error-prone. In this paper, we therefore describe how the publish-subscribe principle can be applied to automate the handling of architecture changes via a loosely-coupled event mechanism. We investigate relevant architecture changes and propose a concrete system of subscription topics and event compositions. The practicality of the approach is demonstrated by means of an implemented support system that is compliant with the WS-notification specification.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383163,no,undetermined,0
A Requirement Level Modification Analysis Support Framework,"Modification analysis is an essential phase of most software maintenance processes, requiring decision makers to perform and predict potential change impacts, feasibility and costs associated with a potential modification request. The majority of existing techniques and tools supporting modification analysis focusing on source code level analysis and require an understanding of the system and its implementation. In this research, we present a novel approach to support the identification of potential modification and re-testing efforts associated with a modification request, without the need for analyzing or understanding the system source code. We combine Use Case Maps with Formal Concept Analysis to provide a unique modification analysis framework that can assist decision makers during modification analysis at the requirements level. We demonstrate the applicability of our approach on a telephony system case study.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383100,no,undetermined,0
Towards Automatic Measurement of Probabilistic Processes,In this paper we propose a metric for finite processes in a probabilistic extension of CSP. The kernel of the metric corresponds to trace equivalence and most of the operators in the process algebra is shown to satisfy non-expansiveness property with respect to this metric. We also provide an algorithm to calculate the distance between two processes to a prescribed discount factor in polynomial time. The algorithm has been implemented in a tool that helps us to measure processes automatically.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385480,no,undetermined,0
Nondeterministic Testing with Linear Model-Checker Counterexamples,"In model-based testing, software test-cases are derived from a formal specification. A popular technique is to use traces created by a model-checker as test-cases. This approach is fully automated and flexible with regard to the structure and type of test-cases. Nondeterministic models, however, pose a problem to testing with model-checkers. Even though a model-checker is able to cope with nondeterminism, the traces it returns make commitments at non- deterministic transitions. If a resulting test-case is executed on an implementation that takes a different, valid transition at such a nondeterministic choice, then the test-case would erroneously detect a fault. This paper discusses the extension of available model-checker based test-case generation methods so that the problem of nondeterminism can be overcome.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385486,no,undetermined,0
Detecting Double Faults on Term and Literal in Boolean Expressions,"Fault-based testing aims at selecting test cases to guarantee the detection of certain prescribed faults in programs. The detection conditions of single faults have been studied and used in areas like developing test case selection strategies, establishing relationships between faults and investigating the fault coupling effect. It is common, however, for programmers to commit more than one fault. Our previous studies on the detection conditions of faults in Boolean expressions show that (1) some test case selection strategies developed for the detection of single faults can also detect all double faults related to terms, but (2) these strategies cannot guarantee to detect all double faults related to literals. This paper supplements our previous studies and completes our series of analysis of the detection condition of all double fault classes in Boolean expressions. Here we consider the fault detection conditions of combinations of two single faults, in which one is related to term and the other is related to literal. We find that all such faulty expressions, except two, can be detected by some test case selection strategies for single fault detection. Moreover, the two exception faulty expressions can be detected by existing strategies when used together with a supplementary strategy which we earlier developed to detect double literal faults.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385487,no,undetermined,0
Synthesizing Component-Based WSN Applications via Automatic Combination of Code Optimization Techniques,"Wireless sensor network (WSN) applications sense events in-situ and compute results in-network. Their software components should run on platforms with stringent constraints on node resources. Developers often design their programs by trial-and-error with a view to meeting these constraints. Through numerous iterations, they manually measure and estimate how far the programs cannot fulfill the requirements, and make adjustments accordingly. Such manual process is time-consuming and error-prone. Automated support is necessary. Based on an existing task view that treats a WSN application as tasks and models resources as constraints, we propose a new component view that associates components with code optimization techniques and constraints. We develop algorithms to synthesize components running on nodes, fulfilling the constraints, and thus optimizing their quality. We evaluate our proposal by a simulation study adapted from a real-life WSN application. Keywords: Wireless sensor network, adaptive software design, resource constraint, code optimization technique.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385494,no,undetermined,0
FT-CoWiseNets: A Fault Tolerance Framework for Wireless Sensor Networks,"In wireless sensor networks (WSNs), faults may occur through malfunctioning hardware, software errors or by external causes such as fire and flood. In business applications where WSNs are applied, failures in essential parts of the sensor network must be efficiently detected and automatically recovered. Current approaches proposed in the literature do not cover all the requirements of a fault tolerant system to be deployed in an enterprise environment and therefore are not suitable for such applications. In this paper we investigate these solutions and present FT-CoWiseNets, a framework designed to improve the availability of heterogeneous WSNs through an efficient fault tolerance support. The proposed framework satisfies the requirements and demonstrates to be more adequate to business scenarios than the current approaches.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4394936,no,undetermined,0
Analysis of Anomalies in IBRL Data from a Wireless Sensor Network Deployment,"Detecting interesting events and anomalous behaviors in wireless sensor networks is an important challenge for tasks such as monitoring applications, fault diagnosis and intrusion detection. A key problem is to define and detect those anomalies with few false alarms while preserving the limited energy in the sensor network. In this paper, using concepts from statistics, we perform an analysis of a subset of the data gathered from a real sensor network deployment at the Intel Berkeley Research Laboratory (IBRL) in the USA, and provide a formal definition for anomalies in the IBRL data. By providing a formal definition for anomalies in this publicly available data set, we aim to provide a benchmark for evaluating anomaly detection techniques. We also discuss some open problems in detecting anomalies in energy constrained wireless sensor networks.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4394914,no,undetermined,0
Statistical Hypothesis Testing for Assessing Monte Carlo Estimators: Applications to Image Synthesis,"Image synthesis algorithms are commonly compared on the basis of running times and/or perceived quality of the generated images. In the case of Monte Carlo techniques, assessment often entails a qualitative impression of convergence toward a reference standard and severity of visible noise; these amount to subjective assessments of the mean and variance of the estimators, respectively. In this paper we argue that such assessments should be augmented by well-known statistical hypothesis testing methods. In particular, we show how to perform a number of such tests to assess random variables that commonly arise in image synthesis such as those estimating irradiance, radiance, pixel color, etc. We explore five broad categories of tests: 1) determining whether the mean is equal to a reference standard, such as an analytical value, 2) determining that the variance is bounded by a given constant, 3) comparing the means of two different random variables, 4) comparing the variances of two different random variables, and 5) verifying that two random variables stem from the same parent distribution. The level of significance of these tests can be controlled by a parameter. We demonstrate that these tests can be used for objective evaluation of Monte Carlo estimators to support claims of zero or small bias and to provide quantitative assessments of variance reduction techniques. We also show how these tests can be used to detect errors in sampling or in computing the density of an importance function in MC integrations.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4392721,no,undetermined,0
An Integrated Design Of Fast LSP Data Plane Failure Detection In MPLS-OAM,"One desirable application of BFD (Bi-directional Forwarding Detection) in MPLS-OAM (Operation, Administration, and Maintenance ) is to detect MPLS ( Multple Protocol Label Switching) LSP (Label Switching Path) data plane failures. Besides detectiong failures, LSP-Ping can furtherly verify the LSP data plane against the control plane. However, the control plane processing required for BFD control packets is relatively smaller than that for LSP-Ping messages. In this paper,we will propose how to combinate LSP-Ping and BFD to provide faster data plane failure detection, which possiblely operates on a greater number of LSPs.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4392677,no,undetermined,0
Trace Based Mobility Model for Ad Hoc Networks,"Mobility of the nodes in a mobile ad hoc network poses a challenge in determining stable routes in the network. It is often difficult to predict the mobility of the nodes, as they tend to be random in nature. However, a non-random component would also exist in many scenarios. It is this non-random behavior that we consider in this paper to identify the movement trace of the mobile nodes. An algorithm is proposed to model the regular movement of a node as a trace containing a list of stable positions and their associated time. We call this model as a trace based mobility model (TBMM). The effectiveness of this model is shown by predicting the accuracy of a node movement by performing a ten fold cross validation. We also show the applicability of the trace information in the routing protocol to provide quality of service (QoS).",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4390875,no,undetermined,0
Implementation of Hybrid Automata in Scicos,"Hybrid automaton is a standard model for describing a hybrid system. A hybrid automaton is a state machine augmented with differential equations and is generally represented by a graph composed of vertices and edges where vertices represent continuous activities and edges represent discrete transitions. Modeling a hybrid automaton with large number of vertices may be difficult, time-consuming and error prone using standard modules in modeling and simulation environments such as Scicos. In this paper, we present the new Scicos automaton block used for modeling and simulation of hybrid automata.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389334,no,undetermined,0
A Phase-Locked Loop for the Synchronization of Power Quality Instruments in the Presence of Stationary and Transient Disturbances,"Power quality instrumentation requires accurate fundamental frequency estimation and signal synchronization, even in the presence of both stationary and transient disturbances. In this paper, the authors present a synchronization technique for power quality instruments based on a single-phase software phase-locked loop (PLL), which is able to perform the synchronization, even in the presence of such disturbances. Moreover, PLL is able to detect the occurrence of a transient disturbance. To evaluate if and how the synchronization technique is adversely affected by the application of stationary and transient disturbing influences, appropriate testing conditions have been developed, taking into account the requirements of the in-force standards and the presence of the voltage transducer.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389091,no,undetermined,0
Using Numerical Model to Predict Hydrocephalus Based on MRI Images,"Abnormal flow of cerebrospinal fluid (CSF) may lead to a hydrocephalus condition as a result of birth defects, accident or infection. In this paper we use raw MRI data to work out the """"tissue classification segmentation"""" image and then import this realistic brain geometry into a finite element software so as to simulate CSF velocity and pressure distributions throughout the brain tissue in hydrocephalus. Such procedure is summarized using a 2D case-study.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4387685,no,undetermined,0
Abstraction in Assertion-Based Test Oracles,"Assertions can be used as test oracles. However, writing effective assertions of right abstraction levels is difficult because on the one hand, detailed assertions are preferred for thorough testing (i.e., to detect as many errors as possible), but on the other hand abstract assertions are preferred for readability, maintainability, and reusability. As assertions become a practical tool for testing and debugging programs, this is an important and practical problem to solve for the effective use of assertions. We advocate the use of model variables - specification-only variables of which abstract values are given as mappings from concrete program states - to write abstract assertions for test oracles. We performed a mutation testing experiment to evaluate the effectiveness of the use of model variables in assertion-based test oracles. According to our experiment, assertions written in terms of model variables are as effective as assertions written without using model variables in detecting (injected) faults, and the execution time overhead of model variables are negligible. Our findings are applicable to other use of runtime checkable assertions.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385528,no,undetermined,0
Statistical Metamorphic Testing Testing Programs with Random Output by Means of Statistical Hypothesis Tests and Metamorphic Testing,"Testing software with random output is a challenging task as the output corresponding to a given input differs from execution to execution. Therefore, the usual approaches to software testing are not applicable to randomized software. Instead, statistical hypothesis tests have been proposed for testing those applications. To apply these statistical hypothesis tests, either knowledge about the theoretical values of statistical characteristics of the program output (e. g. the mean) or a reference implementation (e. g. a legacy system) are required to apply statistical hypothesis tests. But often, both are not available. In the present paper, it is discussed how a testing method called Metamorphic Testing can be used to construct statistical hypothesis tests without knowing exact theoretical characteristics or having a reference implementation. For that purpose, two or more independent output sequences are generated by the implementation under test (IUT). Then, these sequences are compared according to the metamorphic relation using statistical hypothesis tests.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385527,no,undetermined,0
Recovery of fault-tolerant real-time scheduling algorithm for tolerating multiple transient faults,"The consequences of missing deadline of hard real time system tasks may be catastrophic. Moreover, in case of faults, a deadline can be missed if the time taken for recovery is not taken into account during the phase when tasks are submitted or accepted to the system. However, when faults occur tasks may miss deadline even if fault tolerance is employed. Because when an erroneous task with larger execution time executes up to end of its total execution time even if the error is detected early, this unnecessary execution of the erroneous task provides no additional slack time in the schedule to mitigate the effect of error by running additional copy of the same task without missing deadline. In this paper, a recovery mechanism is proposed to augment the fault-tolerant real-time scheduling algorithm RM-FT that achieves node level fault tolerance (NLFT) using temporal error masking (TEM) technique based on rate monotonic (RM) scheduling algorithm. Several hardware and software error detection mechanisms (EDM), i.e. watchdog processor or executable assertions, can detect an error before an erroneous task finishes its full execution, and can immediately stops execution. In this paper, using the advantage of such early detection by EDM, a recovery algorithm RM-FT-RECOVERY is proposed to find an upper bound, denoted by Edm Bound, on the execution time of the tasks, and mechanism is developed to provide additional slack time to a fault-tolerant real-time schedule so that additional task copies can be scheduled when error occurs.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579369,no,undetermined,0
Uniform Selection of Feasible Paths as a Stochastic Constraint Problem,"Automatic structural test data generation is a real challenge of software testing. Statistical structural testing has been proposed to address this problem. This testing method aims at building an input probability distribution to maximize the coverage of some structural criteria. Under the all paths testing objective, statistical structural testing aims at selecting each feasible path of the program with the same probability. In this paper, we propose to model a uniform selector of feasible paths as a stochastic constraint program. Stochastic constraint programming is an interesting framework which combines stochastic decision problem and constraint solving. This paper reports on the translation of uniform selection of feasible paths problem into a stochastic constraint problem. An implementation which uses the library PCC(FD) of SICStus Prolog designed for this problem is detailed. First experimentations, conducted over a few academic examples, show the interest of our approach.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385508,no,undetermined,0
Refinement of a Tool to Assess the Data Quality in Web Portals,"The Internet is now firmly established as an environment for the administration, exchange and publication of data. To support this, a great variety of Web applications have appeared, among these web portals. Numerous users worldwide make use of Web portals to obtain information for different purposes. These users, or data consumers, need to ensure that this information is suitable for the use to which they wish to put it. PDQM (portal data quality model) is a model for the assessment of portal data quality. It has been implemented in the PoDQA tool (portal data quality assessment tool), which can be accessed at http://podqa.webportalqualitv.com. In this paper we present the various refinements that it has been necessary to make in order to obtain a tool which is stable and able to make accurate and efficient calculations of the elements needed to assess the quality of the data of a Web portal.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385501,no,undetermined,0
Compiler-assisted architectural support for program code integrity monitoring in application-specific instruction set processors,"(ASIPs) are being increasingly used in mobile embedded systems, the ubiquitous networking connections have exposed these systems under various malicious security attacks, which may alter the program code running on the systems. In addition, soft errors in microprocessors can also change program code and result in system malfunction. At the instruction level, all code modifications are manifested as bit flips. In this work, we present a generalized methodology for monitoring code integrity at run-time in ASIPs, where both the instruction set architecture (ISA) and the underlying microarchitecture can be customized for a particular application domain. Based on the microoperation-based monitoring architecture that we have presented in previous work, we propose a compiler-assisted and application-controlled management approach for the monitoring architecture. Experimental results show that compared with the OS-managed scheme and other compiler-assisted schemes, our approach can detect program code integrity compromises with much less performance degradation.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601899,no,undetermined,0
Automatic Quality Assessment of SRS Text by Means of a Decision-Tree-Based Text Classifier,"The success of a software project is largely dependent upon the quality of the Software Requirements Specification (SRS) document, which serves as a medium to communicate user requirements to the technical personnel responsible for developing the software. This paper addresses the problem of providing automated assistance for assessing the quality of textual requirements from an innovative point of view, namely through the use of a decision- tree-based text classifier, equipped with Natural Language Processing (NLP) tools. The objective is to apply the text classification technique to build a system for the automatic detection of ambiguity in SRS text based on the quality indicators defined in the quality model proposed in this paper. We believe that, with proper training, such a text classification system will prove to be of immense benefit in assessing SRS quality. To the authors' best knowledge, ours is the first documented attempt to apply the text classification technique for assessing the quality of software documents.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385497,no,undetermined,0
Defining Software Evolvability from a Free/Open-Source Software,"This paper studies various sources of information to identify factors that influence the evolvability of Free and Open-Source Software (FIOSS) endeavors. The sources reviewed to extract criteria are (1) interviews with FIOSS integrators, (2) the scientific literature, and (3) existing standard, norms as well as (4) three quality assessment methodologies specific to FIOSS , namely, QSOS, OpenBRR and Open Source Maturity Model. This effort fits in the larger scope of QUALOSS, a research project funded by the European Commission, whose goal is to develop a methodology to assess the evolvability and robustness of FIOSS endeavors.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4383094,no,undetermined,0
Application development on hybrid systems,"Hybrid systems consisting of a multitude of different computing device types are interesting targets for high-performance applications. Chip multiprocessors, FPGAs, DSPs, and GPUs can be readily put together into a hybrid system; however, it is not at all clear that one can effectively deploy applications on such a system. Coordinating multiple languages, especially very different languages like hardware and software languages, is awkward and error prone. Additionally, implementing communication mechanisms between different device types unnecessarily increases development time. This is compounded by the fact that the application developer, to be effective, needs performance data about the application early in the design cycle. We describe an application development environment specifically targeted at hybrid systems, supporting data-flow semantics between application kernels deployed on a variety of device types. A specific feature of the development environment is the availability of performance estimates (via simulation) prior to actual deployment on a physical system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5348809,no,undetermined,0
HMM Based Channel Status Predictor for Cognitive Radio,"Nowadays, many researchers are interested in cognitive radio (CR) technology. We can say that the CR is the extended technology of software defined radio. Both technologies seem to be similar but there is an important difference. That is a sensing and channel management function. CR always senses incumbent users (IU) (or primary users) appearing on the channel the CR has been used and the CR must evacuate from that channel for preventing IU from interferences. For this purpose, CR should include a functionality of being able to find new relevant channel to move. So, CR must evaluate the quality of empty channels. From the point of view, we propose the HMM based channel status predictor, which helps the CR evaluate the quality. We will implement a HMM channel predictor and it will predict next channel status based on past channel states.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554696,no,undetermined,0
A User-centric Applications Sharing Model on Pervasive Computing,"Pervasive computing is booming research field, in which innovative techniques and applications are continuously forming to provide users with high quality ambient and personalized services. Applications available for end-users have become increasingly abundant, distributed and heterogeneous in pervasive computing environment. How to share and utilize these applications is a significant research problem in pervasive computing environment. This paper puts forward a User-centric Applications Sharing Model (U-ASM). The U-ASM mainly focuses on abstracting the applications from service providers and end-users, and then utilizes virtualization technology to encapsulate applications and uses ontology organize distributed applications as a logic unit with semantic relationships so that it can provide better quality of service for end-users. The research result has been applied in R&D Infrastructure and Facility Development of Ministry of Science and Technology and has great flexibility and extensibility.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4365491,no,undetermined,0
Remote health-care monitoring using Personal Care Connect,"Caring for patients with chronic illnesses is costlynearly $1.27 trillion today and predicted to grow much larger. To address this trend, we have designed and built a platform, called Personal Care Connect (PCC), to facilitate the remote monitoring of patients. By providing caregivers with timely access to a patient's health status, they can provide patients with appropriate preventive interventions, helping to avoid hospitalization and to improve the patient's quality of care and quality of life. PCC may reduce health-care costs by focusing on preventive measures and monitoring instead of emergency care and hospital admissions. Although PCC may have features in common with other remote monitoring systems, it differs from them in that it is a standards-based, open platform designed to integrate with devices from device vendors and applications from independent software vendors. One of the motivations for PCC is to create and propagate a working environment of medical devices and applications that results in innovative solutions. In this paper, we describe the PCC remote monitoring system, including our pilot tests of the system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5386592,no,undetermined,0
A Fault Detection Mechanism for Fault-Tolerant SOA-Based Applications,"Fault tolerance is an important capability for SOA-based applications, since it ensures the dynamic composition of services and improves the dependability of SOA-based applications. Fault detection is the first step of fault detection, so this paper focuses on fault detection, and puts forward a fault detection mechanism, which is based on the theories of artificial neural network and probability change point analysis rather than static service description, to detect the services that fail to satisfy performance requirements at runtime. This paper also gives reference model of fault-tolerance control center of enterprise services bus.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370804,no,undetermined,0
An Unsupervised Intrusion Detection Method Combined Clustering with Chaos Simulated Annealing,"Keeping networks security has never been such an imperative task as today. Threats come from hardware failures, software flaws, tentative probing and malicious attacks. In this paper, a new detection method, Intrusion Detection based on Unsupervised Clustering and Chaos Simulated Annealing algorithm (IDCCSA), is proposed. As a novel optimization technique, chaos has gained much attention and some applications during the past decade. For a given energy or cost function, by following chaotic ergodic orbits, a chaotic dynamic system may eventually reach the global optimum or its good approximation with high probability. To enhance the performance of simulated annealing which is to find a near-optimal partitioning clustering, simulated annealing algorithm is proposed by incorporating chaos. Experiments with KDD cup 1999 show that the simulated annealing combined with chaos can effectively enhance the searching efficiency and greatly improve the detection quality.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370702,no,undetermined,0
Power-Aware Control Flow Checking Compilation: Using Less Branches to Reduce Power Dissipation,"Satellite-borne embedded systems require the properties of low-powered and reliability in the spatial radiation environment. The control flow checking is an effective way for the running systems to prevent the broken-down caused by Single Event Upsets. Traditional software control flow checking uses a great deal of branch instructions to detect errors, thus brings great overhead in power dissipation. In this paper, a partition method of basic block is suggested. In this partition method, branch instructions are reduced greatly, while the high error detection coverage remain ensure. The simulated results show that compared with the traditional Control Flow Checking by Software Signatures(CFCSS) control flow checking algorithm, the Improved algorithm can reduce total branch instructions by over 10%, reduce the power dissipation by nearly 9%, without decreasing the error detection coverage.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370659,no,undetermined,0
Study and Applicaion of FTA Software System,"Fault tree analysis (FTA) is an effective technique to analyze the reliability of a complex system. Through selecting the reasonable top events, putting out the fault tree of the system,and carrying out quantitative analysis about the fault tree, the information of the system under study can be systematically understood. First, this paper studies how to calculate the occurrence probability of the top event and compute the importance degree of a basic event. Then this paper put forwards the realized arithmetic of FTA software system. Also, it analyzed and designed the class of fault tree, calculation module of minimal cut, graph drawing of tree, and calculation class of reliably characteristic. On the basis of the above study, the software system of Fault Tree Analysis is established. Making use of the fault tree analysis system, the reliability of the intelligent electrical apparatus is analyzed. The fault tree of intelligent electrical apparatus system is established according to its composing and the relating test data. It can find the minimal cut set, and gain the probability configuration importance degree. These studies are helpful for the reliability design of the intelligent electrical apparatus.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370345,no,undetermined,0
Partial Disk Failures: Using Software to Analyze Physical Damage,"A good understanding of disk failures is crucial to ensure a reliable storage of data. There have been numerous studies characterizing disk failures under the common assumption that failed disks are generally unusable. Contrary to this assumption, partial disk failures are very common, e.g., caused by a head crash resulting in a small number of inaccessible disk sectors. Nevertheless, the damage can sometimes be catastrophic if the file system meta-data were among the affected sectors. As disk density rapidly increases, the likelihood of losing data also rises. This paper describes our experience in analyzing partial disk failures using the physical locations of damaged disk sectors to assess the extent and characteristics of the damage on disk platter surfaces. Based on our findings, we propose several fault-tolerance techniques to proactively guard against permanent data loss due to partial disk failures.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4367973,no,undetermined,0
Distributed Diagnosis of Failures in a Three Tier E-Commerce System,"For dependability outages in distributed Internet infrastructures, it is often not enough to detect a failure, but it is also required to diagnose it, i.e., to identify its source. Complex applications deployed in multi-tier environments make diagnosis challenging because of fast error propagation, black-box applications, high diagnosis delay, the amount of states that can be maintained, and imperfect diagnostic tests. Here, we propose a probabilistic diagnosis model for arbitrary failures in components of a distributed application. The monitoring system (the Monitor) passively observes the message exchanges between the components and, at runtime, performs a probabilistic diagnosis of the component that was the root cause of a failure. We demonstrate the approach by applying it to the Pet Store J2EE application, and we compare it with Pinpoint by quantifying latency and accuracy in both systems. The Monitor outperforms Pinpoint by achieving comparably accurate diagnosis with higher precision in shorter time.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4365695,no,undetermined,0
An Activity-Based Quality Model for Maintainability,"Maintainability is a key quality attribute of successful software systems. However, its management in practice is still problematic. Currently, there is no comprehensive basis for assessing and improving the maintainability of software systems. Quality models have been proposed to solve this problem. Nevertheless, existing approaches do not explicitly take into account the maintenance activities, that largely determine the software maintenance effort. This paper proposes a 2-dimensional model of maintainability that explicitly associates system properties with the activities carried out during maintenance. The separation of activities and properties facilitates the identification of sound quality criteria and allows to reason about their interdependencies. This transforms the quality model into a structured and comprehensive quality knowledge base that is usable in industrial project environments. For example, review guidelines can be generated from it. The model is based on an explicit quality metamodel that supports its systematic construction and fosters preciseness as well as completeness. An industrial case study demonstrates the applicability of the model for the evaluation of the maintainability of Matlab Simulink models that are frequently used in model-based development of embedded systems.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362631,no,undetermined,0
"The Andres Project: Analysis and Design of Run-Time Reconfigurable, Heterogeneous Systems","Today's heterogeneous embedded systems combine components from different domains, such as software, analogue hardware and digital hardware. The design and implementation of these systems is still a complex and error-prone task clue to the different Models of Computations (MoCs), design languages and tools associated with each of the domains. Though making such systems adaptive is technologically feasible, most of the current design methodologies do not explicitely support adaptive architectures. This paper present the ANDRES project. The main objective of ANDRES is the development of a seamless design flow for adaptive heterogeneous embedded systems (AHES) based on the modelling language SystemC. Using domain-specific modelling extensions and libraries, ANDRES will provide means to efficiently use and exploit adaptivity in embedded system design. The design flow is completed by a methodology and tools for automatic hardware and software synthesis for adaptive architectures.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380679,no,undetermined,0
Evaluation of Semantic Interference Detection in Parallel Changes: an Exploratory Experiment,"Parallel developments are becoming increasingly prevalent in the building and evolution of large-scale software systems. Our previous studies of a large industrial project showed that there was a linear correlation between the degree of parallelism and the likelihood of defects in the changes. To further study the relationship between parallel changes and faults, we have designed and implemented an algorithm to detect """"direct"""" semantic interference between parallel changes. To evaluate the analyzer's effectiveness in fault prediction, we designed an experiment in the context of an industrial project. We first mine the change and version management repositories to find sample versions sets of different degrees of parallelism. We investigate the interference between the versions with our analyzer. We then mine the change and version repositories to find out what faults were discovered subsequent to the analyzed interfering versions. We use the match rate between semantic interference and faults to evaluate the effectiveness of the analyzer in predicting faults. Our contributions in this evaluative empirical study are twofold. First, we evaluate the semantic interference analyzer and show that it is effective in predicting faults (based on """"direct"""" semantic interference detection) in changes made within a short time period. Second, the design of our experiment is itself a significant contribution and exemplifies how to mine software repositories rather than use artificial cases for rigorous experimental evaluations.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362620,no,undetermined,0
Mining the Lexicon Used by Programmers during Sofware Evolution,"Identifiers represent an important source of information for programmers understanding and maintaining a system. Self-documenting identifiers reduce the time and effort necessary to obtain the level of understanding appropriate for the task at hand. While the role of the lexicon in program comprehension has long been recognized, only a few works have studied the quality and enhancement of the identifiers and no works have studied the evolution of the lexicon. In this paper, we characterize the evolution of program identifiers in terms of stability metrics and occurrences of renaming. We assess whether an evolution process similar to the one occurring for the program structure exists for identifiers. We report data and results about the evolution of three large systems, for which several releases are available. We have found evidence that the evolution of the lexicon is more limited and constrained than the evolution of the structure. We argue that the different evolution results from several factors including the lack of advanced tool support for lexicon construction, documentation, and evolution.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362614,no,undetermined,0
Historical Risk Mitigation in Commercial Aircraft Avionics as an Indicator for Intelligent Vehicle Systems,"How safety is perceived in conjunction with consumer products has much to do with its presentation to the buying public and the company reputation for performance and safety. As the automobile industry implements integrated vehicle safety and driver aid systems, the question of public perception of the true safety benefits would seem to parallel the highly automated systems of commercial aircraft, a market in which perceived benefits of flying certainly outweigh concerns of safety. It is suggested that the history of critical aircraft systems provides a model for the wide-based implementation of automated systems in automobiles. The requirement for safety in aircraft systems as an engineering design parameter takes on several forms such as wear-out, probability of catastrophic failure and mean time between replacement or repair (MTBR). For automobile systems as in aircraft, it is a multidimensional topic encompassing a variety of hardware and software functions, fail-safe or fail-operational capability and operator and control interaction. As with critical flight systems, the adherence to specific federal safety requirements is also a cost item to which all manufacturers must adhere, but that also provides a common baseline to which all companies must design. Long a requirement for the design of systems for military and commercial aircraft control, specific safety standards have produced methodologies for analysis and system mechanization that would suggest the operational safety design methods needed for automobiles. Ultimately, tradeoffs must be completed to attain an acceptable level of safety when compared to the cost for developing and selling the system. As seen with commercial aircraft, acceptance of product safety by the public is not based on understanding strict technical requirements but is primarily the result of witnessing many hours of fault free operation, and seeking opinions of those they feel are knowledgeable. This brief study will use data from p- reliminary concept studies for the Automated Highway System and developments by human factors analysts and sociologists concerning perceptions of risk to present an evaluation of the technological methods historically used to mitigate risk in critical aircraft systems and how they might apply to automation in automobiles.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362216,no,undetermined,0
An Observation-Based Approach to Performance Characterization of Distributed n-Tier Applications,"The characterization of distributed n-tier application performance is an important and challenging problem due to their complex structure and the significant variations in their workload. Theoretical models have difficulties with such wide range of environmental and workload settings. Experimental approaches using manual scripts are error-prone, time consuming, and expensive. We use code generation techniques and tools to create and run the scripts for large-scale experimental observation of n-tier benchmarking application performance measurements over a wide range of parameter settings and software/hardware combinations. Our experiments show the feasibility of experimental observations as a sound basis for performance characterization, by studying in detail the performance achieved by (up to 3) database servers and (up to 12) application servers in the RUBiS benchmark with a workload of up to 2700 concurrent users.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362192,no,undetermined,0
High-available grid services through the use of virtualized clustering,"Grid applications comprise several components and web-services that make them highly prone to the occurrence of transient software failures and aging problems. This type of failures often incur in undesired performance levels and unexpected partial crashes. In this paper we present a technique that offers high-availability for Grid services based on concepts like virtualization, clustering and software rejuvenation. To show the effectiveness of our approach, we have conducted some experiments with OGSA-DAI middleware. One of the implementations of OGSA-DAI makes use of use of Apache Axis V1.2.1, a SOAP implementation that suffers from severe memory leaks. Without changing any bit of the middleware layer we have been able to anticipate most of the problems caused by those leaks and to increase the overall availability of the OGSA-DAI Application Server. Although these results are tightly related with this middleware it should be noted that our technique is neutral and can be applied to any other Grid service that is supposed to be high-available.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4354113,no,undetermined,0
Sources of Mistakes in PFD Calculations for Safety-Related Loop Typicals,"In order to prevent any harm for human beings and environment, IEC 61511 imposes strict requirements on safety instrumented functions (SIFs) in chemical and pharmaceutical production plants. As measure of quality a safety integrity level (SIL) of 1, 2, 3 or 4 is postulated for the SIF. In this context for every SIF realization, i.e. safety-related loop, a SIL-specific probability of failure on demand (PFD) must be proven. Usually, the PFD calculation is performed based on the failure rates of each loop component aided by commercial software tools. But this bottom-up approach suffers from many uncertainties. Especially a lack of reliable failure rate data causes many problems. Reference data for different environmental conditions are available to solve this situation. However, this pragmatism leads to a PFD bandwidth, not to a single PFD value as desired. In order to make a decision for a numerical value appropriate for plant applications in chemical industry, a data ascertainment has been initiated by the European NAMUR within its member companies. Combined with statistical methods their results display large deficiencies for the bottom-up approach. As one main source of mistakes the distribution of the loop PFD has been identified. The well known percentages for sensor, logic solver and final element part often cited in literature could not be confirmed.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4354006,no,undetermined,0
Evaluation of Medical Image Watermarking with Tamper Detection and Recovery (AW-TDR),"This paper will study and evaluate watermarking technique by Zain and Fauzi. Recommendations will then be made to enhance the technique especially in the aspect of recovery or reconstruction rate for medical images. A proposal will also be made for a better distribution of watermark to minimize the distortion of the region of interest (ROI). The final proposal will enhance AW-TDR in three aspects; firstly the image quality in the ROI will be improved as the maximum change is only 2 bits in every 4 pixels, or embedding rate of 0.5 bits/pixel. Secondly the recovery rate will also be better since the recovery bits are located outside the region of interest. The disadvantage in this is that, only manipulation done in the ROI will be detected. Thirdly the quality of the reconstructed image will be enhanced since the average of 2 x 2 pixels would be used to reconstruct the tampered image.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4353631,no,undetermined,0
Defect prevention and detection in software for automated test equipment,"Software for automated test equipment can be tedious and monotonous making it just as error-prone as other types of software. Active defect prevention and detection are important for test applications. Incomplete or unclear requirements, a cryptic syntax, variability in syntax or structure, and changing requirements are among the problems encountered in test applications for one tester. These issues increase the probability of error introduction during test application development. This paper describes a test application development tool designed to address these issues for the PT3800 tester, a continuity and insulation resistance tester. The tool was designed with powerful built-in defect prevention and detection capabilities. A reduction in rework and a two-fold increase in productivity are the results. The defect prevention and detection capabilities are described along with lessons learned and their applicability to other test equipment software.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4374223,no,undetermined,0
Automatic segmentation and band detection of protein images based on the standard deviation profile and its derivative,"Gel electrophoresis has significantly influenced the progress achieved in genetic studies over the last decade. Image processing techniques that are commonly used to analyze gel electrophoresis images require mainly three steps: band detection, band matching, and quantification and comparison. Although several techniques have been proposed to fully automate all steps, errors in band detection and, hence, in quantification are still important issues to address. In order to detect bands, many techniques were used, including image segmentation. In this paper, we present two novel, fully-automated techniques based on the standard deviation and its derivative to perform segmentation and to detect protein bands. Results show that even for poor quality images with faint bands, segmentation and detection are highly accurate.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4374497,no,undetermined,0
Are Two Heads Better than One? On the Effectiveness of Pair Programming,"Pair programming is a collaborative approach that makes working in pairs rather than individually the primary work style for code development. Because PP is a radically different approach than many developers are used to, it can be hard to predict the effects when a team switches to PP. Because projects focus on different things, this article concentrates on understanding general aspects related to effectiveness, specifically project duration, effort, and quality. Not unexpectedly, our meta-analysis showed that the question of whether two heads are better than one isn't precise enough to be meaningful. Given the evidence, the best answer is """"it depends"""" - on both the programmer's expertise and the complexity of the system and tasks to be solved. Two heads are better than one for achieving correctness on highly complex programming tasks. They might also have a time gain on simpler tasks. Additional studies would be useful. For example, further investigation is clearly needed into the interaction of complexity and programmer experience and how they affect the appropriateness of a PP approach; our current understanding of this phenomenon rests chiefly on a single (although large) study. Only by understanding what makes pairs work and what makes them less efficient can we take steps to provide beneficial work conditions, to avoid detrimental conditions, and to avoid pairing altogether when conditions are detrimental. With the right cooks and the right combination of ingredients, the broth has the potential to be very good indeed.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4375233,no,undetermined,0
Using Software Reliability Growth Models in Practice,"The amount of software in consumer electronics has grown from thousands to millions of lines of source code over the past decade. Up to a million of these products are manufactured each month for a successful mobile phone or television. Development organizations must meet two challenging requirements at the same time: be predictable to meet market windows and provide nearly fault-free software. Software reliability is the probability of failure-free operation for a specified period of time in a specified environment. The process of finding and removing faults to improve the software reliability can be described by a mathematical relationship called a software reliability growth model (SRGM). Our goal is to assess the practical application of SRGMs during integration and test and compare them with other estimation methods. We empirically validated SRGMs' usability in a software development environment. During final test phases for three embedded software projects, software reliability growth models predicted remaining faults in the software, supporting management's decisions.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4375247,no,undetermined,0
Multi-Processor System-Level Synthesis for Multiple Applications on Platform FPGA,"Multiprocessor systems-on-chip (MPSoC) are being developed in increasing numbers to support the high number of applications running on modern embedded systems. Designing and programming such systems prove to be a major challenge. Most of the current design methodologies rely on creating the design by hand, and are therefore error-prone and time-consuming. This also limits the number of design points that can be explored. While some efforts have been made to automate the flow and raise the abstraction level, these are still limited to single-application designs. In this paper, we present a design methodology to generate and program MPSoC designs in a systematic and automated way for multiple applications. The architecture is automatically inferred from the application specifications, and customized for it. The flow is ideal for fast design space exploration (DSE) in MPSoC systems. We present results of a case study to compute the buffer-throughput trade-offs in real-life applications, H263 and JPEG decoders. The generation of the entire project takes about 100 ms, and the whole DSE was completed in 45 minutes, including the FPGA mapping and synthesis.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380631,no,undetermined,0
Improving Usability of Web Pages for Blinds,"Warranting the access to Web contents to any citizen, even to people with physical disabilities, is a major concern of many government organizations. Although guidelines for Web developers have been proposed by international organisations (such as the W3C) to make Web site contents accessible, the wider part of today's Web sites are not completely usable by peoples with sight disabilities. In this paper, two different approaches for dynamically transforming Web pages into aural Web pages, i.e. pages that are optimised for blind peoples, will be presented. The approaches exploit heuristic techniques for summarising Web pages contents and providing them to blind users in order to improve the usability of Web sites. The techniques have been validated in an experiment where usability metrics have been used to assess the effectiveness of the Web page transformation techniques.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380250,no,undetermined,0
A WSAD-Based Fact Extractor for J2EE Web Projects,"This paper describes our implementation of a fact extractor for J2EE Web applications. Fact extractors are part of each reverse engineering toolset; their output is used by reverse engineering analyzers and visualizers. Our fact extractor has been implemented on top of IBM's Websphere Application Developer (WSAD). The extractor's schema has been defined with the Eclipse Modeling Framework (EMF) using a graphical modeling approach. The extractor extensively reuses functionality provided by WSAD, EMF, and Eclipse, and is an example of component-based development. In this paper, we show how we used this development approach to accomplish the construction of our fact extractor, which, as a result, could be realized with significantly less code and in shorter time compared to a homegrown extractor implemented from scratch. We have assessed our extractor and the produced facts with a table- based and a graph-based visualizer. Both visualizers are integrated with Eclipse.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380245,no,undetermined,0
Automatic Test Case Generation for Multi-tier Web Applications,"Testing multi-tier Web applications is challenging yet critical. First, because of inter-tier interactions, a fault in one tier may propagate to the others. Second, Web applications are often continuously evolving. Testing such emerging applications must efficiently generate test cases to catch up with fast-paced evolution and effectively capture cross-tier faults. We present a technique based on an inter-connection dependence model to generate sequences of Web pages that are potentially fault prone. To ensure that these sequences of Web pages will be exercised as designated, the path condition for each execution path is computed and used to determine the domain of each input parameter and database state. Input data for each Web page can then be automatically generated by using boundary value analysis. The test suite generated by our technique guarantees that inter-tier interactions will be adequately tested.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380242,no,undetermined,0
Empirical Validation of a Web Fault Taxonomy and its usage for Fault Seeding,"The increasing demand for reliable Web applications gives a central role to Web testing. Most of the existing works are focused on the definition of novel testing techniques, specifically tailored to the Web. However, no attempt was carried out so far to understand the specific nature of Web faults. This is of fundamental importance to assess the effectiveness of the proposed Web testing techniques. In this paper, we describe the process followed in the construction of a Web fault taxonomy. After the initial, top- down construction, the taxonomy was subjected to four iterations of empirical validation aimed at refining it and at understanding its effectiveness in bug classification. The final taxonomy is publicly available for consultation and editing on a Wiki page. Testers can use it in the definition of test cases that target specific classes of Web faults. Researchers can use it to build fault seeding tools that inject artificial faults which resemble the real ones.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380241,no,undetermined,0
Protection of Induction Motor Using PLC,"The goal of this paper is to protect induction motors against possible failures by increasing the reliability, the efficiency, and the performance. The proposed approach is a sensor-based technique. For this purpose, currents, voltages, speed and temperature values of the induction motor were measured with sensors. When any fault condition is detected during operation of the motor, PLC controlled on-line operation system activates immediately. The performance of the protection system proposed is discussed by means of application results. The motor protection achieved in the study can be faster than the classical techniques and applied to larger motors easily after making small modifications on both software and hardware.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4380120,no,undetermined,0
Harbor: Software-based Memory Protection For Sensor Nodes,"Many sensor nodes contain resource constrained microcontrollers where user level applications, operating system components, and device drivers share a single address space with no form of hardware memory protection. Programming errors in one application can easily corrupt the state of the operating system or other applications. In this paper, we propose Harbor, a memory protection system that prevents many forms of memory corruption. We use software based fault isolation (""""sandboxing"""") to restrict application memory accesses and control flow to protection domains within the address space. A flexible and efficient memory map data structure records ownership and layout information for memory regions; writes are validated using the memory map. Control flow integrity is preserved by maintaining a safe stack that stores return addresses in a protected memory region. Run-time checks validate computed control flow instructions. Cross domain calls perform low-overhead control transfers between domains. Checks are introduced by rewriting an application's compiled binary. The sand- boxed result is verified on the sensor node before it is admitted for execution. Harbor's fault isolation properties depend only on the correctness of this verifier and the Harbor runtime. We have implemented and tested Harbor on the SOS operating system. Harbor detected and prevented memory corruption caused by programming errors in application modules that had been in use for several months. Harbor's overhead, though high, is less than that of application-specific virtual machines, and reasonable for typical sensor workloads.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4379694,no,undetermined,0
Finding Two Optimal Positions of a Hand-Held Camera for the Best Reconstruction,"This paper proposes an experimental study to find the two optimal positions of a hand-held digital camera for the capture of the geometry and texture of an object. Using our improved 3D reconstruction pipeline based on a semi-dense matching between a pair of uncalibrated images, a layout of twenty-five camera positions is tested in real conditions of image acquisition and also completely simulated. The reconstruction quality is measured by assessing the accuracy of the final 3D structure in accordance with a ground truth. The results provide the optimal capturing layout. Another interesting conclusion is that the accuracy of the reconstruction does not change much in the nearby area around the best position, which enables to the hand-held capture to not strictly respect this configuration.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4379439,no,undetermined,0
Detecting Hidden Messages Using Image Power Spectrum,"In this paper we present a study of the effects of data hiding on the power spectra of digital images. Several imperceptible data hiding techniques have been proposed that provide strong visual security and robustness. Although imperceptible to the human visual system, the hidden data affects the natural qualities of the image, such as the image power spectrum. In this study, we classify a large image database into a number of categories. For each category, we calculate the slope of the power spectra for the marked and unmarked images. We note that in the case of spatial data hiding the average slope of the power spectra of marked images is 54.93% higher compared to that of the unmarked images. Also in the cases of transform domain data hiding we note that the average slope of the power spectra of the images marked using a discrete cosine (wavelet) transform (DC(W)T) based technique is higher by 9.12% (38.39%). We also test a commercially available data hiding software namely Digimarc Corp.'s MyPictureMarc 2005 VI.0. In this case the average power spectra of the marked images is 35.99% higher. Hence we see that the proposed scheme is a tool for universal steganalysis with varying degrees of success depending on the type of embedding.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378981,no,undetermined,0
Compensated Signature Embedding Based Multimedia Content Authentication System,"Digital content authentication and preservation is an extremely challenging task in realizing decentralized digital libraries. The concept of compensated signature embedding is proposed to develop an effective multimedia content authentication system. The proposed system does not require any third party reference or side information. Towards this end, a content-based fragile signature is derived and embedded into the media using a robust watermarking technique. Since the embedding process introduces distortion in the media, it may lead to authentication failure. We propose to adjust the media samples iteratively or using a closed form process to compensate for the embedding distortion. Using an example image authentication system, we show that the proposed scheme is highly effective in detecting even minor modifications to the media.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378974,no,undetermined,0
Detecting and Exploiting Symmetry in Discrete-State Markov Models,"Dependable systems are usually designed with multiple instances of components or logical processes, and often possess symmetries that may be exploited in model-based evaluation. The problem of how best to exploit symmetry in models has received much attention from the modeling community, but no solution has garnered widespread support, primarily because each solution is limited in terms of either the types of symmetry that can be exploited, or the difficulty of translating from the system description to the model formalism. We propose a new method for detecting and exploiting model symmetry in which 1) models retain the structure of the system, and 2) all symmetry inherent in the structure of the model can be detected and exploited for the purposes of state-space reduction. Composed models are constructed from models through specification of connections between models that correspond to shared state fragments. The composed model is interpreted as an undirected graph, and results from group theory, and graph theory are used to develop procedures for automatically detecting, and exploiting all symmetries in the composed model. We discuss the necessary algorithms to detect and exploit model symmetry, and provide a proof that the theory generates an equivalent model. After a thorough analysis of the added complexity, a state-space generator which implements these algorithms within Mobius is then presented.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378409,no,undetermined,0
A Best Practice Guide to Resource Forecasting for Computing Systems,"Recently, measurement-based studies of software systems have proliferated, reflecting an increasingly empirical focus on system availability, reliability, aging, and fault tolerance. However, it is a nontrivial, error-prone, arduous, and time-consuming task even for experienced system administrators, and statistical analysts to know what a reasonable set of steps should include to model, and successfully predict performance variables, or system failures of a complex software system. Reported results are fragmented, and focus on applying statistical regression techniques to monitored numerical system data. In this paper, we propose a best practice guide for building empirical models based on our experience with forecasting Apache web server performance variables, and forecasting call availability of a real-world telecommunication system. To substantiate the presented guide, and to demonstrate our approach in a step by step manner, we model, and predict the response time, and the amount of free physical memory of an Apache web server system, as well as the call availability of an industrial telecommunication system. Additionally, we present concrete results for a) variable selection where we cross benchmark three procedures, b) empirical model building where we cross benchmark four techniques, and c) sensitivity analysis. This best practice guide intends to assist in configuring modeling approaches systematically for best estimation, and prediction results.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378407,no,undetermined,0
Automatic Document Logo Detection,"Automatic logo detection and recognition continues to be of great interest to the document retrieval community as it enables effective identification of the source of a document. In this paper, we propose a new approach to logo detection and extraction in document images that robustly classifies and precisely localizes logos using a boosting strategy across multiple image scales. At a coarse scale, a trained Fisher classifier performs initial classification using features from document context and connected components. Each logo candidate region is further classified at successively finer scales by a cascade of simple classifiers, which allows false alarms to be discarded and the detected region to be refined. Our approach is segmentation free and lay-out independent. We define a meaningful evaluation metric to measure the quality of logo detection using labeled groundtruth. We demonstrate the effectiveness of our approach using a large collection of real-world documents.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4377038,no,undetermined,0
Assessing and Improving the Quality of Document Images Acquired with Portable Digital Cameras,"Professionals and students of many different areas start to use portable digital cameras to take photos of documents, instead of photocopying them. This article analyses the quality of such documents for optical character recognition and proposes ways of improving their transcription and readability.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376979,no,undetermined,0
Symbolic Generation of Models for Microwave Software Tools,"In this paper, we present a use of computer algebra systems (CAS) to derive the scattering parameter description of equivalent networks which are models of linear microwave devices, such as planar transmission line discontinuities. In this case, the use of an automated symbolic technique is a natural choice because manual derivation of scattering parameters is practically impossible, error-prone, and a fatiguing labour. Benefits of the presented symbolic approach are highlighted from the viewpoint of a microwave software tool. We exemplify our original symbolic algorithm by analyzing a four-port network that represents a microstrip cross-junction.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4375976,no,undetermined,0
Ultrasonic motor driving method for EMI-free image in MR image-guided surgical robotic system,"Electromagnetic interference (EMI) between magnetic resonance (MR) imager and surgical manipulator is a severe problem, that degrades the image quality, in MR image- guided surgical robotic systems. We propose a novel motor driving method to acquire noise-free image. Noise generation accompanied by motor actuation is permitted only during the """"dead time"""" when the MR imager stops signal acquisition to wait for relaxation of protons. For the synchronized control between MR imager and motor driving system, we adopted a radio-frequency pulse signal detected by a special antenna as a synchronous trigger. This method can be applied widely because it only senses a part of the scanning signal and requires neither hardware nor software changes to the MR imager. The evaluation results showed the feasibility of RF pulse as a synchronous trigger and the availability of sequence-based noise reduction method.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4399391,no,undetermined,0
A holistic test procedure for security systems software: An experience report,"This paper presents a comprehensive holistic software test procedure for security systems applications. The method evaluates the functional requirements of a security system. Based on the functional requirements and the hardware solution that satisfies the requirements, a test procedure was developed. The test procedure evaluates the correctness, robustness, efficiency, portability, integrity, verifiability and validation and ease of use. An impact assessment is done to assess the cost effectiveness, compatibility and reusability of the software with existing hardware. The study found that the impact of software on the business process is the most important test for any new software prior to deployment.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4401471,no,undetermined,0
Quantified Impacts of Guardband Reduction on Design Process Outcomes,"The value of guardband reduction is a critical open issue for the semiconductor industry. For example, due to competitive pressure, foundries have started to incent the design of manufacturing-friendly ICs through reduced model guardbands when designers adopt layout restrictions. The industry also continuously weighs the economic viability of relaxing process variation limits in the technology roadmap [2]. Our work gives the first-ever quantification of the impact of modeling guardband reduction on outcomes from the synthesis, place and route (SP&R) implementation flow. We assess the impact of model guard- band reduction on various metrics of design cycle time and design quality, using open-source cores and production (specifically, ARM/TSMC) 90 nm and 65 nm technologies and libraries. Our experimental data clearly shows the potential design quality and turnaround time benefits of model guardband reduction. For example, we typically (i.e., on average) observe 13% standard-cell area reduction and 12% routed wirelength reduction as the consequence of a 40% reduction in library model guardband; 40% is the amount of guardband reduction reported by IBM for a variation-aware timing methodology [8]. We also assess the impact of guardband reduction on design yield. Our results suggest that there is justification for the design, EDA and process communities to enable guardband reduction as an economic incentive for manufacturing-friendly design practices.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479839,no,undetermined,0
Enhancing the ESIM (Embedded Systems Improving Method) by Combining Information Flow Diagram with Analysis Matrix for Efficient Analysis of Unexpected Obstacles in Embedded Software,"In order to improve the quality of embedded software, this paper proposes an enhancement to the ESIM (embedded systems improving method) by combining an IFD (information flow diagram) with an Analysis Matrix to analyze unexpected obstacles in the software. These obstacles are difficult to predict in the software specification. Recently, embedded systems have become larger and more complicated. Theoretically therefore, the development cycle of these systems should be longer. On the contrary, in practice the cycle has been shortened. This trend in industry has resulted in the oversight of unexpected obstacles, and consequently affected the quality of embedded software. In order to prevent the oversight of unexpected obstacles, we have already proposed two methods for requirements analysis: the ESIM using an Analysis Matrix and a method that uses an IFD. In order to improve the efficiency of unexpected obstacle analysis at reasonable cost, we now enhance the ESIM by combining an IFD with an Analysis Matrix. The enhancement is studied from the following three viewpoints. First, a conceptual model comprising both the Analysis Matrix and IFD is defined. Then, a requirements analysis procedure is proposed, that uses both the Analysis Matrix and IFD, and assigns each specific role to either an expert or non-expert engineer. Finally, to confirm the effectiveness of this enhancement, we carry out a description experiment using an IFD.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425871,no,undetermined,0
A SLA-Oriented Management of Containers for Hosting Stateful Web Services,"Service-oriented architectures provide integration of interoperability for independent and loosely coupled services. Web services and the associated new standards such as WSRF are frequently used to realise such service-oriented architectures. In such systems, autonomic principles of self-configuration, self-optimisation, self-healing and self- adapting are desirable to ease management and improve robustness. In this paper we focus on the extension of the self management and autonomic behaviour of a WSRF container connected by a structured P2P overlay network to monitor and rectify its QoS to satisfy its SIAs. The SLA plays an important role during two distinct phases in the life-cycle of a WSRF container. Firstly during service deployment when services are assigned to containers in such a way as to minimise the threat of SLA violations, and secondly during maintenance when violations are detected and services are migrated to other containers to preserve QoS. In addition, as the architecture has been designed and built using standardised modern technologies and with high levels of transparency, conventional Web services can be deployed with the addition of a SLA specification.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426875,no,undetermined,0
Fault Detection System Using Directional Classified Rule-Based in AHU,"Monitoring systems used at present to operate air handling unit (AHU) optimally do not have a function that enables to detect faults properly when there are faults of such as operating plants or performance falling, so they are unable to manage faults rapidly and operate optimally. In this paper, we have developed a fault detection system, directional classified rule-based, which can be used in AHU system. In order to experiment this algorithm, it was applied to AHU system which is installed inside environment chamber(EC), verified its own practical effect, and confirmed its own applicability to the related field in the future.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426285,no,undetermined,0
A Survey of System Software for Wireless Sensor Networks,"Wireless sensor networks (WSNs) are increasingly used as an enabling technology in a variety of domains. They operate unattended in an autonomous way, are inherently error prone and have limited resources like energy and bandwidth. Due to these special features and constraints, the management of sensor nodes in WSNs has its own unique technical challenges as well. For example, system software for WSNs needs to be specially tailored and additionally support specific quality of service (QoS) properties such as context-awareness, application-knowledge, reconfiguration, QoS-awareness, scalability and efficiency. A proper classification and comparison of notable system software for WSNs seems to be in order for the literature on this subject. In doing so, in this paper we try to present a survey on such system software with the objective of classifying them especially with regard to aforementioned QoS considerations in the middleware layer. Some classes are identified: agent-based, service-based, query-based, and event-based ones.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426270,no,undetermined,0
A Model-Driven Simulation for Performance Evaluation of 1xEV-DO,"Finding an appropriate approach to evaluate the capacity of a radio access technology with respect to a wide range of parameters (radio signal quality, quality of service, user mobility, network resources) has become increasingly important in today's wireless network planning. In this paper, we propose a model- based simulation to assess the capabilities of the IxEV- DO radio access technology. Results are expressed in terms of throughputs and signal quality (C/I and Ec/Io) for the requested services and applications.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426113,no,undetermined,0
Automatic Test Case Generation from UML Sequence Diagram,"This paper presents a novel approach of generating test cases from UML design diagrams. Our approach consists of transforming a UML sequence diagram into a graph called the sequence diagram graph (SDG) and augmenting the SDG nodes with different information necessary to compose test vectors. These information are mined from use case templates, class diagrams and data dictionary. The SDG is then traversed to generate test cases. The test cases thus generated are suitable for system testing and to detect interaction and scenario faults.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425952,no,undetermined,0
Improving Dependability Using Shared Supplementary Memory and Opportunistic Micro Rejuvenation in Multi-tasking Embedded Systems,"We propose a comprehensive solution to handle memory-overflow problems in multitasking embedded systems thereby improving their reliability and availability. In particular, we propose two complementary techniques to address two significant causes of memory-overflow problems. The first cause is errors in estimating appropriate stack and heap memory requirement. Our first technique, called shared supplementary memory (SSM), exploits the fact that the probability of multiple tasks requiring more than their estimated amount of memory concurrently is low. Using analytical model and simulations, we show that reliability can be considerably improved when SSM is employed. Furthermore, for the same reliability, SSM reduces total memory requirement by as much as 29.31% The second cause is the presence of coding Mandelbugs, which can cause abnormal memory requirement. To address this, we propose a novel technique, called opportunistic micro-rejuvenation, which when combined with SSM, provide several advantages: preventing critical-time outage, resource frugality and dependability enhancement.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459665,no,undetermined,0
An Approach for Specifying Access Control Policy in J2EE Applications,"Most applications based on J2EE platform use role- based access control as an efficient mechanism to achieve security. The current approach for specifying access rule is based on methods of Enterprise JavaBeans (EJBs). In large-scale systems, where a large number of EJBs are used and the interactions between EJBs are complex, direct use of this method- based approach is error-prone and difficult to maintain. We propose an alternative approach for specifying access control policy based on the concept of business function.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425883,no,undetermined,0
Injecting security as aspectable NFR into Software Architecture,"Complexity of the software development process is often increased by actuality of crosscutting concerns in software requirements; moreover, software security as a particular non-functional requirement of software systems is often addressed late in the software development process. Modeling and analyzing of these concerns and especially security in the software architecture facilitate detecting architectural vulnerabilities, decrease costs of the software maintenance, and reduce finding tangled and complex components in the ultimate design. Aspect oriented ADLs have emerged to overcome this problem; however, imposing radical changes to existing architectural modeling methods is not easily acceptable by architects. In this paper, we present a method to enhance conventional software architecture description languages through utilization of aspect features with special focuses on security. To achieve the goal, aspectable NFRs have been clarified; then, for their description in the software architecture, an extension to xADL 2.0 [E.M. Dashofy, 2005] has been proposed; finally, we illustrate this material along with a case study.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425869,no,undetermined,0
An Approach for Assessment of Reliability of the System Using Use Case Model,"Existing approaches on reliability assessment of the system entirely depends on expertise, knowledge of system analysts and computation of usage probability of different user operations. Existing approach is therefore can not be taken as accurate, particularly to deal with new or unfamiliar systems. Further modern systems are very large and complex to manipulate without any automation. Addressing these issues, we propose an analytical technique in our work. In this paper, we propose a novel approach to assess reliability of the system using use case model. We consider a metric to assess the reliability of a system under development.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4418306,no,undetermined,0
Study of distributed generation type and islanding impact on the operation of radial distribution systems,"Nowadays, power generation from renewable energy source is a preferred option and will continue to grow during the coming years. Distributed generation (DG) technologies include photovoltaic, wind turbine and fuel cell, etc., which use renewable energy sources. Most DG units use two generation types i.e. induction generator and synchronous generator. It is crucial that the power system impacts be assessed accurately so that these DG units can be applied in a manner that avoids causing degradation of power quality, reliability and control of the utility system. This paper presents the dynamic modelling of electrical network as well as DG units in order to study islanding behaviour in steady state and transient simulation. The impact of different DG types has been studied on the IEEE 34-bus system using a commercial software tool.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4469125,no,undetermined,0
Model calibration of a real petroleum reservoir using a parallel real-coded genetic algorithm,"An application of a Real-coded Genetic Algorithm (GA) to the model calibration of a real petroleum reservoir is presented. In order to shorten the computation time, the possible solutions generated by the GA are evaluated in parallel on a group of computers. This required the GA to be adapted to a multi-processor structure, so that the scalability of the computation is maximised. The best solutions of each run enter the ensemble of calibrated models, which is finally analysed using a clustering algorithm. The aim is to identify the optimal regions contained in the ensemble and thus to reveal the distinct types of reservoir models consistent with the historic production data, as a way to assess the uncertainty in the Reservoir Characterisation due to the limited reliability of optimisation algorithms. The developed methodology is applied to the characterisation of a real petroleum reservoir. Results show a large improvement with respect to previous studies on that reservoir in terms of the quality and diversity of the obtained calibrated models. Our main conclusion is that, even with regularisation, many distinct calibrated models are possible, which highlights the importance of applying optimisation methods capable of identifying all such solutions.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425034,no,undetermined,0
Estimation of distribution algorithms for testing object oriented software,"One of the main tasks software testing involves is the generation of the test cases to be used during the test. Due to its expensive cost, the automation of this task has become one of the key issues in the area. While most of the work on test data generation has concentrated on procedural software, little attention has been paid to object oriented programs, even so they are a usual practice nowadays. We present an approach based on estimation of distribution algorithms (EDAs) for dealing with the test data generation of a particular type of objects, that is, containers. This is the first time that an EDA has been applied to testing object oriented software. In addition to automated test data generation, the EDA approach also offers the potential of modelling the fitness landscape defined by the testing problem and thus could provide some insight into the problem. Firstly, we show results from empirical evaluations and comment on some appealing properties of EDAs in this context. Next, a framework is discussed in order to deal with the generation of efficient tests for the container classes. Preliminary results are provided as well.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424504,no,undetermined,0
Assess the impact of photovoltaic generation systems on low-voltage network: software analysis tool development,"The integration of photovoltaic generation systems into power networks can cause both benefits and drawbacks. However, utilities have to control and operate their systems properly, in order to assure the availability and quality power supply to the users. Therefore, utilities should consider technical constraints and existing regulation in order to assess the impact of photovoltaic systems and limit their integration. On the other hand, regulation includes few operation constraints and they are not implemented in current software analysis tools. In the present paper a tool for assessing the impact of PV integration on low-voltage networks is described. The voltage fluctuation, the inversion of the power flow and the increase of short-circuit capacity are the problems considered in the proposed tool. Further work will focus on harmonics distortion.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424183,no,undetermined,0
"Multiple signal processing techniques based power quality disturbance detection, classification, and diagnostic software","This work presents the development steps of the software PQMON, which targets power quality analysis applications. The software detects and classifies electric system disturbances. Furthermore, it also makes diagnostics about what is causing such disturbances and suggests line of actions to mitigate them. Among the disturbances that can be detected and analyzed by this software are: harmonics, sag, swell and transients. PQMON is based on multiple signal processing techniques. Wavelet transform is used to detect the occurrence of the disturbances. The techniques used to do such feature extraction are: fast Fourier transform, discrete Fourier transform, periodogram, and statistics. Adaptive artificial neural network is also used due to its robustness in extracting features such as fundamental frequency and harmonic amplitudes. The probable causes of the disturbances are contained in a database, and their association to each disturbance is made through a cause-effect relationship algorithm, which is used to diagnose. The software also allows the users to include information about the equipments installed in the system under analysis, resulting in the direct nomination of any installed equipment during the diagnostic phase. In order to prove the effectiveness of software, simulated and real signals were analyzed by PQMON showing its excellent performance.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424176,no,undetermined,0
Transactional Memory Execution for Parallel Multithread Programming without Lock,"With the increasing popularity of shared-memory programming model, especially at the advent of multicore processors, applications need to become more concurrent to take advantage of the increased computational power provided by chip level multiprocessing. Traditionally locks are used to enforce data dependence and timing constraints between the various threads. However locks are error- prone, and often leading to unwanted race conditions, priority inversion, or deadlock. Therefore, recent waves of research projects are exploring transaction memory systems as an alternative synchronization mechanism to locks. This paper presents a software transactional memory execution model for parallel multithread programming without lock.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4420173,no,undetermined,0
Assessing tram schedules using a library of simulation components,"Assessing tram schedules is important to assure an efficient use of infrastructure and for the provision of a good quality service. Most existing infrastructure modeling tools provide support to assess an individual aspect of rail systems in isolation, and do not provide enough flexibility to assess many aspects that influence system performance at once. We propose a library of simulation components that enable rail designers to assess different system configurations. In this paper we show how we implemented some basic safety measures used in rail systems such as: reaction to control objects (e.g. traffic lights), priority rules, and block safety systems.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419815,no,undetermined,0
A New Context-aware Application Validation Method Based on Quality-driven Petri Net Models,"There is a problem that the crash in devices or service components may occur during executing context-aware applications when developing multiple context-aware applications in context-aware middleware. It will lead to the abnormal execution of context-aware applications. In this paper, a method based on quality-driven Petri nets (QPN) derived from Petri nets is proposed to solve it. In this method, a context-aware application is qualified, simulated, and validated based on QPN. Through QPN, we can simulate the execution process of context-aware applications. In QPN, Petri Nets' behavioral properties involved reachability. Reachability will detect and reflect the crash in devices or component services before executing context-aware applications.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4427845,no,undetermined,0
A PC-Based System for Automated Iris Recognition under Open Environment,"This paper presents an entirely automatic system designed to realize accurate and fast personal identification from the iris images acquired under open environment. The acquisition device detects the appearance of a user at any moment using an ultrasonic transducer, guides the user in positioning himself in the acquisition range and acquires the best iris image of the user through quality evaluation. Iris recognition is done using the bandpass characteristic of wavelets and wavelet transform principles for detecting singularities to extract iris features and adopting Hamming distance to match iris codes. The authentication service software can enroll a user's iris image into a database and perform verification of a claimed identity or identification of an unknown entity. The identification rate is high and the recognition result is available about 6 s starting at iris image acquisition. This system is promising to be used in applications requiring personal identification.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4428237,no,undetermined,0
ANDES: an Anomaly Detection System for Wireless Sensor Networks,"In this paper, we propose ANDES, a framework for detecting and finding the root causes of anomalies in operational wireless sensor networks (WSNs). The key novelty of ANDES is that it correlates information from two sources: one in the data plane as a result of regular data collection in WSNs, the other in the management plane implemented via a separate routing protocol, making it resilient to routing anomaly in the data plane. Evaluation using a 32-node sensor testbed shows that ANDES is effective in detecting fail-stop failures and most routing anomalies with negligible computing and storage overhead.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4428636,no,undetermined,0
A probabilistic approach of mobile router selection algorithm in dynamic moving networks,"A dynamic moving network with unspecified multiple mobile routers or dynamic mobile routers may causes difficult problems in terms of stability and reliability in the moving networks. We assume that a user terminal such as cellular phone can be a mobile router in a dynamic moving network. The selection and maintenance of mobile routers should be very important issue like in this scenario in order to support reliable communication channels to internal nodes. In the selection of representative mobile routers, the staying time of a mobile router should be one of essential factors, e.g. battery power and network capacity. It, however, is not easy to estimate the staying time of each mobile router and candidate to be selected. We propose the staying time prediction algorithm of a dynamic mobile router according to the variable status of dynamic moving network. In this approach, the proposed algorithm is based on gathered statistical data as a probabilistic approach in order to estimate the staying time of a mobile router. Finally simulation results will show the accuracy of the probability.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4429115,no,undetermined,0
Predicting performance of software systems during feasibility study of software project management,"Software performance is an important nonfunctional attribute of software systems for producing quality software. Performance issues must be considered throughout software project development. Predicting performance early in the life cycle is addressed by many methodologies, but the data collected during feasibility study not considered for predicting performance. In this paper, we consider the data collected (technical and environmental factors) during feasibility study of software project management to predict performance. We derive an algorithm to predict the performance metrics and simulate the results using a case study on banking application.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4449845,no,undetermined,0
Using design based binning to improve defect excursion control for 45nm production,"For advanced device (45 nm and below), we proposed a novel method to monitor systematic and random excursion. By integrating design information and defect inspection results into automated software (DBB), we can identify design/process marginality sites with defect inspection tool. In this study, we applied supervised binning function (DBC) and defect criticality index (DCI) to identify systematic and random excursion problems on 45 nm SRAM wafers. With established SPC charts, we will be able to detect future excursion problem in manufacturing line early.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4446790,no,undetermined,0
Efficient Development Methodology for Multithreaded Network Application,"Multithreading is becoming increasingly important for modern network programming. In inter-process communication platform, multithreaded applications have much benefit especially to improve application's throughput, responsiveness and latency. However, developing good quality of multithreaded codes is difficult, because threads may interact with each others in unpredictable ways. Although modern compilers can manage threads well, but in practice, synchronization errors (such as: data race errors, deadlocks) required careful management and good optimization method. The goal of this work is to discover common pitfalls in multithreaded network applications, present a software development technique to detect errors and optimize efficiently multithreaded applications. We compare performance of a single threaded network application with multithreaded network applications, use tools called Intelreg VTunetrade Performance Analyzer, Intelreg Thread Checker and our method to efficiently fix errors and optimize performance. Our methodology is divided into three phases: First phase is performance analysis using Intelreg VTunetrade Performance Analyzer with the aim is to identify performance optimization opportunities and detect bottlenecks. In second phase, with Intelreg Thread Checker we allocate data races, memory leakage and debug the multithreaded applications. In third phase, we apply tuning and optimization to the multithreaded applications. With the understanding of the common pitfalls in multithreaded network applications, through the development and debugging methodology aforementioned above, developers are able to optimize and tune their applications efficiently.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4451394,no,undetermined,0
A policy controlled IPv4/IPv6 network emulation environment,"In QoS enabled IP-based networks, QoS signaling and policy control are used to control the access to network resources and their usage. The IETF proposed standard protocol for policy control is Common Open Policy Service (COPS) protocol, which has also been adopted in 3GPP IP Multimedia Subsystem (IMS) Release 5. This paper presents a prototype for policy-controlled IPv4/IPv6 network emulation environment, in which it is possible to specify the policy control and emulate, over a period of time, QoS parameters such as bandwidth, packet delay, jitter, and packet discard probability for media flows within an IP multimedia session. The policy control is handled by COPS, and IP channel emulation uses two existing network emulation tools, NIST Net and ChaNet, supporting IPv4 and IPv6 protocols, respectively. The scenario-based approach allows reproducible performance measurements and running various experiments by using the same network behavior. A graphical user interface has been developed to make the scenario specification more user-friendly. We demonstrate the functionality of the prototype emulation environment for IPv6 and analyze its performance.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4446098,no,undetermined,0
Variable block size error concealment scheme based on H.264/AVC non-normative decoder,"As the newest video coding standard, H.264/AVC can achieve high compression efficiency. At the same time, due to the high-efficiently predictive coding and the variable length entropy coding, it is more sensitive to transmission errors. So error concealment (EC) in H.264 is very important when compressed video sequences are transmitted over error-prone networks and erroneously received. To achieve higher EC performance, this paper proposes variable block size error concealment scheme (VBSEC) by utilizing the new concept of variable block size motion estimation (VBSME) in H.264 standard. This scheme provides four EC modes and four sub-block partitions. The whole corrupted macro-block (MB) will be divided into variable block size adaptively according to the actual motion. More precise motion vectors (MV) will be predicted for each sub-block. We also produce a more accurate distortion function based on spatio-temporal boundary matching algorithm (STBMA). By utilizing VBSEC scheme based on our STBMA distortion function, we can reconstruct the corrupted MB in the inter frame more accurately. The experimental results show that our proposed scheme can obtain maximum PSNR gain up to 1.72 dB and 0.48 dB, respectively compared with the boundary matching algorithm (BMA) adopted in the JM11.0 reference software and STBMA.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4445834,no,undetermined,0
Formal safety verification for TTP/C network in Drive-by-wire system,"TTP/C is a member of the time-triggered protocol (TTP) family that satisfies Society of Automotive Engineers Class C requirements for hard real-time fault-tolerant communication. As a communication network designed for safety-critical system, it is essential to verify its safety depending on formal methods. We investigate the fault-tolerant and fault-avoidance strategies of TTP/C network used in Drive-by-wire system, with Markov modeling techniques, and evaluate the failure rate subject to different failure modes, taking into account both transit and permanent physical failures. Generalized Stochastic Petri Net (GSPN) is selected to model concurrency, non-determinism properties and calculate Markov model automatically. A model with 157 states and 78 transitions is built. The result of experiments shows that failure probability of TTP/C network in 7-nodes DBW system varies from 10<sup>-6</sup> to 10<sup>-10</sup> with different configuration. And diagnose mistakes are proved to be a critical factor for the success of membership service.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4456390,no,undetermined,0
Multi-Agent System-based Protection Coordination of Distribution Feeders,"A protection system adopting multi-agent concept for power distribution systems is proposed. A device agent, embedded in a protective device, detects faults or adapts in the network in an autonomous manner and changes its operating parameters to new operation conditions by collaborating with other protection agents. Simulations of the agents and their operations for a sample distribution system with a variety of cases show the feasibility of the agent-based adaptive protection of distribution networks.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4441686,no,undetermined,0
A parallel controls software approach for PEP II: AIDA & MATLAB middle layer,"The controls software in use at PEP II (Stanford control program - SCP) had originally been developed in the eighties. It is very successful in routine operation but due to its internal structure it is difficult and time consuming to extend its functionality. This is problematic during machine development and when solving operational issues. Routinely, data has to be exported from the system, analyzed offline, and calculated settings have to be reimported. Since this is a manual process, it is time consuming and error-prone. Setting up automated processes, as is done for MIA (model independent analysis), is also time consuming and specific to each application. Recently, there has been a trend at light sources to use MATLAB [1] as the platform to control accelerators using a """"MATLAB middle layer"""" [2] (MML), and so called channel access (CA) programs to communicate with the low level control system (LLCS). This has proven very successful, especially during machine development time and trouble shooting. A special CA code, named AIDA (Accelerator Independent Data Access [3]), was developed to handle the communication between MATLAB, modern software frameworks, and the SCP. The MML had to be adapted for implementation at PEP II. Colliders differ significantly in their designs compared to light sources, which poses a challenge. PEP II is the first collider at which this implementation is being done. We will report on this effort, which is still ongoing.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4440280,no,undetermined,0
Identification of Relational Discrepancies between Database Schemas and Source-Code in Enterprise Applications,"As enterprise applications become more and more complex, the understanding and quality assurance of these systems become an increasingly important issue. One specific concern of data reverse engineering, a necessary process for this type of applications which tackles the mentioned aspects, is to retrieve constraints which are not explicitly declared in the database schema but verified in the code. In this paper we propose a novel approach for detecting the relational discrepancies between database schemas and source-code in enterprise applications, as part of the data reverse engineering process. Detecting and removing these discrepancies allows us to ensure the accuracy of the stored data as well as to increase the level of understanding of the data involved in an enterprise application.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4438084,no,undetermined,0
A practical approach to comprehensive system test & debug using boundary scan based test architecture,"In this paper, we present a boundary scan based system test approach for large and complex electronic systems. Using the multi-drop architecture, a test bus is extended through the backplane and the boundary scan chain of every board is connected to this test bus through a gateway device. We present a comprehensive system test method using this test architecture to achieve high quality, reliability and efficient diagnosis of structural defects and some functional errors. This test architecture enables many advanced test methods like, embedded test application for periodic system maintenance, high quality backplane test for efficient diagnosis of structural defects on the backplane, in-system remote programming of programmable devices in the field. Finally, we present a novel fault injection method to detect and diagnose various functional errors in the system software of an electronic system. These methods were implemented in various systems and we present some implementation data to show the effectiveness of these advanced test methods.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437663,no,undetermined,0
Presentation of Information Synchronized with the Audio Signal Reproduced by Loudspeakers using an AM-based Watermark,Reproducing stego audio signal via a loudspeaker and detecting embedded data from a recorded sound from a microphone are challenging with respect to the application of data hiding. A watermarking technique using subband amplitude modulation was applied to a system that displays text information synchronously with the watermarked audio signal transmitted in the air. The robustness of the system was evaluated by a computer simulation in terms of the correct rate of data transmission under reverberant and noisy conditions. The results showed that the performance of detection and the temporal precision of synchronization were sufficiently high. Objective measurement of the watermarked audio quality using the PEAQ method revealed that the mean objective difference grade obtained from 100 watermarked music samples exhibited an intermediate value between the mean ODGs of 96-kbps and 128-kbps MP3 encoded music samples.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4457704,no,undetermined,0
Functional testing of digital microfluidic biochips,"Dependability is an important attribute for microfluidic biochips that are used for safety-critical applications such as point-of-care health assessment, air-quality monitoring, and food-safety testing. Therefore, these devices must be adequately tested after manufacture and during bioassay operations. Known techniques for biochip testing are all function-oblivious, i.e., while they can detect and locate defect sites on a microfluidic array, they cannot be used to ensure correct operation of functional units. In this paper, we introduce the concept of functional testing of microfluidic biochips. We address fundamental biochip operations such as droplet dispensing, droplet transportation, mixing, splitting, and capacitive sensing. Long electrode actuation times are avoided to ensure that there is no electrode degradation during testing. We evaluate the proposed test methods using simulations as well as experiments for a fabricated biochip.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437614,no,undetermined,0
Software-based BIST for Analog to Digital Converters in SoC,"Embedded software based self testing has recently become focus of intense research for microprocessor and memories in SoC. In this paper, we used the testing microprocessor and memory for developing software-based self-testing of analog to digital converters in SoC. The advantage of this methodology include at speed testing, low cost, and small test time. Simulation results show that the proposed method can detect not only catastrophic faults but also some parametric faults.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437457,no,undetermined,0
Fault Detection System Activated by Failure Information,"We propose a fault detection system activated by an application when the application recognizes the occurrence of a failure, in order to realize self managing systems that automatically find the source of a failure. In existing detection systems, there are three issues for constructing self managing applications: i) the detection results are not sent to the applications, ii) they can not identify the source failure from all of the detected failures, and iii) configuring the detection system for networked system is hard work. For overcoming these issues, the proposed system takes three approaches: i) the system receives failure information from an application and returns a result set to the application, ii) the system identifies the source failure using relationships among errors, and Hi) the system obtains information of the monitored system from a database. The relationship is expressed by a tree. This tree is called error relationship tree. The database provides information which are system entities such as hardware devices, software object, and network topology. When the proposed system starts looking for the source of a failure, causal relations from an error relation tree are referred to, and the correspondence of error definitions and actual objects is derived using the database. We show the design of the detection operation activated by the failure information and the architecture of the proposed system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459634,no,undetermined,0
Fisher information-based evaluation of image quality for time-of-flight PET,"The use of time-of-flight (TOF) information during reconstruction is generally considered to improve the image quality. In this work we quantified this improvement using two existing methods: (1) a very simple analytical expression only valid for a central point in a large uniform disk source, and (2) efficient analytical approximations for post-filtered maximum likelihood expectation maximization (MLEM) reconstruction with a fixed target resolution, predicting the image quality in a pixel or in a small region based on the Fisher information matrix. The image quality was investigated at different locations in various software phantoms. Simplified as well as realistic phantoms, measured both with TOF positron emission tomography (PET) systems and with a conventional PET system, were simulated. Since the time resolution of the system is not always accurately known, the effect on the image quality of using an inaccurate kernel during reconstruction was also examined with the Fisher information- based method. First, we confirmed with this method that the variance improvement in the center of a large uniform disk source is proportional to the disk diameter and inversely proportional to the time resolution. Next, image quality improvement was observed in all pixels, but in eccentric and high-count regions the contrast-to-noise ratio (CNR) increased slower than in central and low- or medium-count regions. Finally, the CNR was seen to decrease when the time resolution was inaccurately modeled (too narrow or too wide) during reconstruction. Although the optimum is rather flat, using an inaccurate TOF kernel might introduce artifacts in the reconstructed image.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4437031,no,undetermined,0
The research status of complex system integrated health management system (CSIHM) architecture,"Complex system integrated health management (CSIHM) technology is developed from the fault detecting, isolation and reconfiguration (FDIR) technology; which is the integrated application of the advanced reasoning technology, artificial intelligence technology, sensor technology and information management technology. CSIHM can effectively manage the health states of many kinds of complex system for reducing the maintenance cost of an overall system and make sure the accomplishment of mission and the safety of crews by detecting the system malfunctions earlier and make deal with them automatically. Recent years, most of presented health management models, such as ISHM, IVHM and PHM etc. can be put in the category of CSIHM. This paper firstly presents the goal which CSIHM must be achieved and its activities, and then has an analysis and comparison of some representative CSIHM architectures combining with some typical cases emerged from their development course, at last presents that use the technique of concurrent engineering to conduct the design of CSIHM and the design of complex system managed by CSIHM simultaneously, the design of user interface and the development of core engine is the difficulties and hotspot problems in the research of CSIHM architecture.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419524,no,undetermined,0
Automatic Test Case Generation from UML Models,"This paper presents a novel approach of generating test cases from UML design diagrams. We consider use case and sequence diagram in our test case generation scheme. Our approach consists of transforming a UML use case diagram into a graph called use case diagram graph (UDG) and sequence diagram into a graph called the sequence diagram graph (SDG) and then integrating UDG and SDG to form the system testing graph (STG). The STG is then traversed to generate test cases. The test cases thus generated are suitable for system testing and to detect operational, use case dependency, interaction and scenario faults.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4418295,no,undetermined,0
Service-Oriented Business Process Modeling and Performance Evaluation based on AHP and Simulation,"With the evolution of grid technologies and the application of service-oriented architecture (SOA), more and more enterprises are integrated and collaborated with each other in a loosely coupled environment. A business process in that environment, i.e., the service-oriented business process (SOBP), shows highly flexibility for its free selection and composition of different services. The performance of the business process usually has to be evaluated and predicted before its being implemented. And it has special features since it includes both business-level and IT-level attributes. However, the existing modeling and performance evaluation methods of business process are mainly concentrated on business-level performance. And the researches on service selection and composition are usually limited to the IT-level metrics. An extended activity-network-based SOBP Model, its three-level performance metrics, and the corresponding calculation algorithm are proposed to fulfill these requirements. The advantages of our method in SOBP modeling and performance evaluation are highlighted also.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402135,no,undetermined,0
A Structural Complexity Metric for Software Components,"At present, the number of components increases largely, and component-based software development (CBSD) is becoming a new effective software development paradigm, how to measure their reliability, maintainability and complexity attracts more and more attentions. This paper presents a metric to assess the structural complexity of components. Moreover it proves that the metric satisfies some good properties.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402665,no,undetermined,0
Fast implementation of a l<sub></sub>- l/<sub>1</sub> penalized sparse representations algorithm: applications in image denoising and coding,"Sparse representation techniques have become an important tool in image processing in recent years, for coding, de-noising and in-painting purposes, for instance. They generally rely on an lscr<sub>2</sub>-lscr<sub>1</sub> penalized criterion and fast algorithms have been proposed to speed up the applications. We propose to replace the lscr<sub>2</sub>-part of the criterion, which has been chosen both for its easy implementation and its relation to the PSNR quality measure, by a lscr<sub></sub>-part. We present a new fast way to minimize a lscr<sub></sub>- lscr<sub>1</sub> penalized criterion and assess its potential benefits for image De-noising and coding.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4487264,no,undetermined,0
Behavioral Fault Modeling for Model-based Safety Analysis,"Recent work in the area of model-based safety analysis has demonstrated key advantages of this methodology over traditional approaches, for example, the capability of automatic generation of safety artifacts. Since safety analysis requires knowledge of the component faults and failure modes, one also needs to formalize and incorporate the system fault behavior into the nominal system model. Fault behaviors typically tend to be quite varied and complex, and incorporating them directly into the nominal system model can clutter it severely. This manual process is error-prone and also makes model evolution difficult. These issues can be resolved by separating the fault behavior from the nominal system model in the form of a """"fault model"""", and providing a mechanism for automatically combining the two for analysis. Towards implementing this approach we identify key requirements for a flexible behavioral fault modeling notation. We formalize it as a domain-specific language based on Lustre, a textual synchronous dataflow language. The fault modeling extensions are designed to be amenable for automatic composition into the nominal system model.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404742,no,undetermined,0
"Scalable, Adaptive, Time-Bounded Node Failure Detection","This paper presents a scalable, adaptive and time-bounded general approach to assure reliable, real-time node-failure detection (NFD) for large-scale, high load networks comprised of commercial off-the-shelf (COTS) hardware and software. Nodes in the network are independent processors which may unpredictably fail either temporarily or permanently. We present a generalizable, multilayer, dynamically adaptive monitoring approach to NFD where a small, designated subset of the nodes are communicated information about node failures. This subset of nodes are notified of node failures in the network within an interval of time after the failures. Except under conditions of massive system failure, the NFD system has a zero false negative rate (failures are always detected with in a finite amount of time after failure) by design. The NFD system continually adjusts to decrease the false alarm rate as false alarms are detected. The NFD design utilizes nodes that transmit, within a given locality, """"heartbeat"""" messages to indicate that the node is still alive. We intend for the NFD system to be deployed on nodes using commodity (i.e. not hard-real-time) operating systems that do not provide strict guarantees on the scheduling of the NFD processes. We show through experimental deployments of the design, the variations in the scheduling of heartbeat messages can cause large variations in the false-positive notification behavior of the NFD subsystem. We present a per-node adaptive enhancement of the NFD subsystem that dynamically adapts to provide run-time assurance of low false-alarm rates with respect to past observations of heartbeat scheduling variations while providing finite node-failure detection delays. We show through experimentation that this NFD subsystem is highly scalable and uses low resource overhead.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404740,no,undetermined,0
How can Previous Component Use Contribute to Assessing the Use of COTS?,"The intuitive notion exists in industry and among regulators that successful use of a commercially available software-based component over some years and within different application environments must imply some affirmative statement about the quality of the component and - in terms of a safety-case - that it should provide evidence to support a specific safety claim for usage of the component in a specific new environment. Yet, so far a method is lacking to investigate quantitatively how such evidence can inform and influence an estimate for example of the component's probability of failure per demand or per hour, and thus the evidence is not used. Currently there is no blueprint to show us what such evidence contributes to meeting a safety claim. In this paper a route is explored that may allow to make use of such prior evidence and combine it with fresh statistical test data pertaining to the new usage environment. The model proposed is an initial model but it is hoped that it can help to develop over time a framework that can be practically used by regulators and safety assessors to inform a safety case for COTS components containing a software part.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404734,no,undetermined,0
One in a baker's dozen: debugging debugging,"In the work of Voas (1993), they outlined 13 major software engineering issues needing further research: (1) what is software quality? (2) what are the economic benefits behind existing software engineering techniques?, (3) does process improvement matter?, (4) can you trust software metrics and measurement?, (5) why are software engineering standards confusing and hard to comply with, (6) are standards interoperable, (7) how to decommission software?, (8) where are reasonable testing and debugging stoppage criteria?, (9) why are COTS components so difficult to compose?, (10) why are reliability measurement and operational profile elicitation viewed suspiciously, (11) can we design in the """"ilities"""" both technically and economically, (12) how do we handle the liability issues surrounding certification, and (13) is intelligence and autonomic computing feasible? This paper focuses on a simple and easy to understand metric that addresses the eighth issue, a testing and debugging testing stoppage criteria based on expected probability of failure graphs.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404729,no,undetermined,0
Cognitive radio Research and Implementation Challenges,"Future mobile terminals will be able to communicate with various heterogeneous systems which are different by means of the algorithms used to implement baseband processing and channel coding. This represents many challenges in designing flexible and energy efficient architectures. Using the sensing phase, the mobile can sense its environment and detect the spectrum holes and use them to communicate. Current research are investigating different techniques of using cognitive radio to reuse locally unused spectrum to increase the total system capacity. They aim also to develop efficient algorithm able to maximize the quality of service (QoS) for the secondary (unlicensed) users while minimizing the interference to the primary (licensed) users. However, there are many challenges across all layers of a cognitive radio system design, from its application to its implementation.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4487323,no,undetermined,0
Mapping CMMI Project Management Process Areas to SCRUM Practices,"Over the past years, the capability maturity model (CMM) and capability maturity model integration (CMMI) have been broadly used for assessing organizational maturity and process capability throughout the world. However, the rapid pace of change in information technology has caused increasing frustration to the heavyweight plans, specifications, and other documentation imposed by contractual inertia and maturity model compliance criteria. In light of that, agile methodologies have been adopted to tackle this challenge. The aim of our paper is to present mapping between CMMI and one of these methodologies, Scrum. It shows how Scrum addresses the Project Management Process Areas of CMMI. This is useful for organizations that have their plan-driven process based on the CMMI model and are planning to improve its processes toward agility or to help organizations to define a new project management framework based on both CMMI and Scrum practices.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402760,no,undetermined,0
Component Based Proactive Fault Tolerant Scheduling in Computational Grid,"Computational Grids have the capability to provide the main execution platform for high performance distributed applications. Grid resources having heterogeneous architectures, being geographically distributed and interconnected via unreliable network media are extremely complex and prone to different kinds of errors, failures and faults. Grid is a layered architecture and most of the fault tolerant techniques developed on grids use its strict layering approach. In this paper, we have proposed a cross-layer design for handling faults proactively. In a cross-layer design, the top- down and bottom-up approach is not strictly followed, and a middle layer can communicate with the layer below or above it [1]. At each grid layer there would be a monitoring component that would decide on predefined factors that the reliability of that particular layer is high, medium or low. Based on Hardware Reliability Rating (HRR) and Software Reliability Rating (SRR), the Middleware Monitoring Component / Cross- Layered Component (MMC/CLC) would generate a Combined Rating (CR) using CR calculation matrix rules. Each grid participating node will have a CR value generated through cross layered communication using HMC, MMC/CLC and SMC. All grid nodes will have their CR information in the form of a CR table and high rated machines would be selected for job execution on the basis of minimum CPU load along with different intensities of check pointing. Handling faults proactively at each layer of grid using cross communication model would result in overall improved dependability and increased performance with less overheads of check pointing.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4516328,no,undetermined,0
Work in progress - Developing transversal skills in an online theoretical engineering subject,"In some concentration subjects of online programs, such as theoretical scientific subjects (Mathematics, Physics, etc), where most of the problems have only one solution and there are few chances for discussion, the development of transversal skills, such as team-working, leadership, or oral and written expression is forgotten. Typically, only specific competences about the subject are acquired. Due to the lack of discussions, students usually work in an autonomous way, only a few days before the exam, and this causes a high ratio of desertion. To avoid this, teamwork has been planned. In it, lecturers and students assess the correctness of the solutions, the presentation of the reports, the written expression, the pedagogical quality of their explanations, and some teamwork skills such as organization, distribution of the work, the use of e-learning tools for communication, etc. To collect and analyze the marks which students grade each other's work a simple tool has been created. This way, in a preliminary analysis it is shown that students are closer to their mates, they develop transversal skills, and they study during all the year, increasing the ratio of success.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4418114,no,undetermined,0
IR thermographic detection of defects in multi-layered composite materials used in military applications,"Multi-layered composites are frequently used in many military applications as constructional materials and light armours protecting personnel and armament against fragments and bullets. Material layers can be very different by their physical properties. Therefore, such materials represent a difficult inspection task for many traditional techniques of non-destructive testing (NDT). Typical defects of composite materials are delaminations, a lack of adhesives, condensations and crumpling. IR thermographic NDT is considered as a candidate technique to detect such defects. In order to determine the potential usefulness of the thermal methods, specialized software has been developed for computing 3D (three- dimensional) dynamic temperature distributions in anisotropic six-layer solid bodies with subsurface defects. In this paper, both modeling and experimental results which illustrate advantages and limitations of IR thermography in inspecting composite materials will be presented.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4516626,no,undetermined,0
Using In-Process Testing Metrics to Estimate Post-Release Field Quality,"In industrial practice, information on the software field quality of a product is available too late in the software lifecycle to guide affordable corrective action. An important step towards remediation of this problem lies in the ability to provide an early estimation of post-release field quality. This paper evaluates the Software Testing and Reliability Early Warning for Java (STREW-J) metric suite leveraging the software testing effort to predict post-release field quality early in the software development phases. The metric suite is applicable for software products implemented in Java for which an extensive suite of automated unit test cases are incrementally created as development proceeds. We validated the prediction model using the STREW-J metrics via a two-phase case study approach which involved 27 medium-sized open source projects, and five industrial projects. The error in estimation and the sensitivity of the predictions indicate the STREW-J metric suite can be used effectively to predict post-release software field quality.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402212,no,undetermined,0
A Multi-Agent Fault Detection System for Wind Turbine Defect Recognition and Diagnosis,This paper describes the use of a combination of anomaly detection and data-trending techniques encapsulated in a multi-agent framework for the development of a fault detection system for wind turbines. Its purpose is to provide early error or degradation detection and diagnosis for the internal mechanical components of the turbine with the aim of minimising overall maintenance costs for wind farm owners. The software is to be distributed and run partly on an embedded microprocessor mounted physically on the turbine and on a PC offsite. The software will corroborate events detected from the data sources on both platforms and provide information regarding incipient faults to the user through a convenient and easy to use interface.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4538286,no,undetermined,0
Using Machine Learning to Support Debugging with Tarantula,"Using a specific machine learning technique, this paper proposes a way to identify suspicious statements during debugging. The technique is based on principles similar to Tarantula but addresses its main flaw: its difficulty to deal with the presence of multiple faults as it assumes that failing test cases execute the same fault(s). The improvement we present in this paper results from the use of C4.5 decision trees to identify various failure conditions based on information regarding the test cases' inputs and outputs. Failing test cases executing under similar conditions are then assumed to fail due to the same fault(s). Statements are then considered suspicious if they are covered by a large proportion of failing test cases that execute under similar conditions. We report on a case study that demonstrates improvement over the original Tarantula technique in terms of statement ranking. Another contribution of this paper is to show that failure conditions as modeled by a C4.5 decision tree accurately predict failures and can therefore be used as well to help debugging.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402205,no,undetermined,0
Prioritization of Regression Tests using Singular Value Decomposition with Empirical Change Records,"During development and testing, changes made to a system to repair a detected fault can often inject a new fault into the code base. These injected faults may not be in the same files that were just changed, since the effects of a change in the code base can have ramifications in other parts of the system. We propose a methodology for determining the effect of a change and then prioritizing regression test cases by gathering software change records and analyzing them through singular value decomposition. This methodology generates clusters of files that historically tend to change together. Combining these clusters with test case information yields a matrix that can be multiplied by a vector representing a new system modification to create a prioritized list of test cases. We performed a post hoc case study using this technique with three minor releases of a software product at IBM. We found that our methodology suggested additional regression tests in 50% of test runs and that the highest-priority suggested test found an additional fault 60% of the time.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402199,no,undetermined,0
Towards Self-Protecting Enterprise Applications,"Enterprise systems must guarantee high availability and reliability to provide 24/7 services without interruptions and failures. Mechanisms for handling exceptional cases and implementing fault tolerance techniques can reduce failure occurrences, and increase dependability. Most of such mechanisms address major problems that lead to unexpected service termination or crashes, but do not deal with many subtle domain dependent failures that do not necessarily cause service termination or crashes, but result in incorrect results. In this paper, we propose a technique for developing selfprotecting systems. The technique proposed in this paper observes values at relevant program points. When the technique detects a software failure, it uses the collected information to identify the execution contexts that lead to the failure, and automatically enables mechanisms for preventing future occurrences of failures of the same type. Thus, failures do not occur again after the first detection of a failure of the same type.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402195,no,undetermined,0
Software Reliability Modeling with Test Coverage: Experimentation and Measurement with A Fault-Tolerant Software Project,"As the key factor in software quality, software reliability quantifies software failures. Traditional software reliability growth models use the execution time during testing for reliability estimation. Although testing time is an important factor in reliability, it is likely that the prediction accuracy of such models can be further improved by adding other parameters which affect the final software quality. Meanwhile, in software testing, test coverage has been regarded as an indicator for testing completeness and effectiveness in the literature. In this paper, we propose a novel method to integrate time and test coverage measurements together to predict the reliability. The key idea is that failure detection is not only related to the time that the software experiences under testing, but also to what fraction of the code has been executed by the testing. This is the first time that execution time and test coverage are incorporated together into one single mathematical form to estimate the reliability achieved. We further extend this method to predict the reliability of fault- tolerant software systems. The experimental results with multi-version software show that our reliability model achieves a substantial estimation improvement compared with existing reliability models.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402193,no,undetermined,0
Preliminary Models of the Cost of Fault Tolerance,"Software cost estimation and overruns continue to plague the software engineering community, especially in the area of safety-critical systems. We provide some preliminary models to predict the cost of adding fault detection, fault-tolerance, or fault isolation techniques to a software system or subsystem if the cost of originally developing the system or subsystem is known. Since cost is a major driver in the decision to develop new safety-critical systems, such models will be useful to requirements engineers, systems engineers, decision makers, and those intending to reuse systems and components in safety-critical environments where fault tolerance is critical.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404763,no,undetermined,0
Adding Autonomic Capabilities to Network Fault Management System,"In this paper, we propose an adaptive framework for adding the most desired aspects of autonomic capabilities into the critical components of a network fault management system. The aspects deemed as the most desirable are those that have a significant impact on system dependability, which include self-monitoring, self-healing, self-adjusting, and self-configuring. Self-monitoring oversees the environmental conditions and system behavior, building a consciousness ground to support self-awareness capabilities. It is responsible for monitoring the system states and environmental conditions, analyzing them and thus detecting and identifying system faults/failures. Upon detection, self-healing operations is enabled to respond (i.e. take proper actions) to the identified faults /failures. These actions are usually accomplished by self-configuring and self- adjusting the corresponding system configurations and operations. Together, all self-*approaches complete an adaptive framework and offer a sound solution towards high system assurance.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404764,no,undetermined,0
Soft-error induced system-failure rate analysis in an SoC,"This paper proposes an analytical method to assess the soft-error rate (SER) in the early stages of a System-on-Chip (SoC) platform-based design methodology. The proposed method gets an executable UML model of the SoC and the raw soft-error rate of different parts of the platform as its inputs. Soft-errors on the design are modelled by disturbances on the value of attributes in the classes of the UML model and disturbances on opcodes of software cores. This Architectural Vulnerability Factor (AVF) and the raw soft-error rate of the components in the platform are used to compute the SER of cores. Furthermore, the SER and the severity of error in each core in the SoC are used to compute the System-Failure Rate (SFR) of the SoC.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4481075,no,undetermined,0
Conquering Complexity,"In safety-critical systems, the potential impact of each separate failure is normally studied in detail and remedied by adding backups. Failure combinations, though, are rarely studied exhaustively; there are just too many of them, and most have a low probability of occurrence. Defect detection in software development is usually understood to be a best effort at rigorous testing just before deployment. But defects can be introduced in all phases of software design, not just in the final coding phase. Defect detection therefore shouldn't be limited to the end of the process, but practiced from the very beginning. In a rigorous model-based engineering process, each phase is based on the construction of verifiable models that capture the main decisions.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404823,no,undetermined,0
Hard-to-detect errors due to the assembly-language environment,"Most programming errors are reasonably simple to understand and detect. One of the benefits of a high-level language is its encapsulation of error-prone concepts such as memory access and stack manipulation. Assembly-language programmers do not have this luxury. In our quest to create automated error-prevention and error-detection tools for assembly language, we need to create a comprehensive list of possible errors. We are not considering syntax errors or algorithmic errors. Assemblers, simple testing, and automated testing can detect those errors. We want to deal with design errors that are direct byproducts of the assembly-language environment or result from a programmer's lack of understanding of the assembly-language environment. Over many years of assembly-language instruction, we have come across a plethora of errors. Understanding the different types of errors and how to prevent and detect them is essential to our goal of creating automated error-prevention and error-detection tools. In this paper we list and explain the types of errors we have cataloged.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4418030,no,undetermined,0
Neural network controlled voltage disturbance detector and output voltage regulator for Dynamic Voltage Restorer,"This paper describes the high power DVR (Dynamic Voltage Restorer) with the neural network controlled voltage disturbance detector and output voltage regulator. Two essential parts of DVR control are how to detect the voltage disturbance such as voltage sag and how to compensate it as fast as possible respectively. The new voltage disturbance detector was implemented by using the delta rule of the neural network control. Through the proposed method, we can instantaneously track the amplitude of each phase voltage under the severe unbalanced voltage conditions. Compared to the conventional synchronous reference frame method, the proposed one shows the minimum time delay to determine the instance of the voltage disturbance event. Also a modified d-q transformed voltage regulator for single phase inverter was adopted to obtain the fast dynamic response and the robustness, where three independent single phase inverters are controlled by using the amplitude of source voltage obtained by neural network controller. By using the proposed voltage regulator, the voltage disturbance such as voltage sag can be compensated quickly to the nominal voltage level. The proposed disturbance detector and the voltage regulator were applied to the high power DVR (1000 kV A@440 V) that was developed for the application of semiconductor manufacture plant. The performances of the proposed DVR control were verified through computer simulation and experimental results. Finally, conclusions are given.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4417587,no,undetermined,0
A Failure Tolerating Atomic Commit Protocol for Mobile Environments,"In traditional fixed-wired networks, standard protocols like 2-Phase-Commit are used to guarantee atomicity for distributed transactions. However, within mobile networks, a higher probability of failures including node failures, message loss, and even network partitioning makes the use of these standard protocols difficult or even impossible. To use traditional database applications within a mobile scenario, we need an atomic commit protocol that reduces the chance of infinite blocking. In this paper, we present an atomic commit protocol called multi coordinator protocol (MCP) that uses a combination of the traditional 2-Phase-Commit, 3-Phase-Commit, and consensus protocols for mobile environments. Simulation experiments comparing MCP with 2PC show how MCP enhances stability for the coordination process by involving multiple coordinators, and that the additional time needed for the coordination among multiple coordinators is still reasonable.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4417138,no,undetermined,0
UML-based safety analysis of distributed automation systems,"HAZOP (hazard and operability) studies are carried out to analyse complex automated systems, especially large and distributed automated systems. The aim is to systematically assess the automated system regarding possibly negative effects of deviations from standard operation on safety and performance. Today, HAZOP studies require significant manual effort and tedious work of several costly experts. The authors of this paper propose a knowledge-based approach to support the HAZOP analysis and to reduce the required manual effort. The main ideas are (1) to incorporate knowledge about typical problems in automation systems, in combination with their causes and their effects, in a rule base, and (2) to apply this rule base by means of a rule engine on the description of the automated system under consideration. This yields a list of possible dangers regarding safety risks and performance reductions. These results can be used by the automation experts to improve the system's design. Within this paper, the general approach is presented, and an example application is dealt with where the system design is given in the form of a UML class diagram, and the HAZOP study is focused on hazards caused by faulty communication within the distributed system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4416901,no,undetermined,0
Extensible Virtual Environment Systems Using System of Systems Engineering Approach,"The development of Virtual Environment (VE) systems is a challenging endeavor with a complex problem domain. The experience in the past decade has helped contribute significantly to various measures of software quality of the resulting VE systems. However, the resulting solutions remain monolithic in nature without addressing successfully the issue of system interoperability and software aging. This paper argues that the problem resides in the traditional system centric approach and that an alternative approach based on system of systems engineering is necessary. As a result, the paper presents a reference architecture based on layers, where only the core is required for deployment and all others are optional. The paper also presents an evaluation methodology to assess the validity of the resulting architecture, which was applied to the proposed core layer and involving individual sessions with 12 experts in developing VE systems.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4414620,no,undetermined,0
Cooperative mixed strategy for service selection in service oriented architecture,"In service oriented architecture (SOA), service brokers could find many service providers which offer same function with different quality of service (QoS). Under this condition, users may encounter difficulty to decide how to choose from the candidates to obtain optimal service quality. This paper tackles the service selection problem (SSP) of time-sensitive services using the theory of games creatively. Pure strategies proposed by current studies are proved to be improper to this problem because the decision conflicts among the users result in poor performance. A novel cooperative mixed strategy (CMS) with good computability is developed in this paper to solve such inconstant-sum non-cooperative n-person dynamic game. Unlike related researches, CMS offers users an optimized probability mass function instead of a deterministic decision to select a proper provider from the candidates. Therefore it is able to eliminate the fluctuation of queue length, and raise the overall performance of SOA significantly. Furthermore, the stability and equilibrium of CMS are proved by simulations.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4413993,no,undetermined,0
Database Isolation and Filtering against Data Corruption Attacks,"Various attacks (e.g., SQL injections) may corrupt data items in the database systems, which decreases the integrity level of the database. Intrusion detections systems are becoming more and more sophisticated to detect such attacks. However, more advanced detection techniques require more complicated analyses, e.g, sequential analysis, which incurs detection latency. If we have an intrusion detection system as a filter for all system inputs, we introduce a uniform processing latency to all transactions of the database system. In this paper, we propose to use a """"unsafe zone"""" to isolate user's SQL queries from a """"safe zone"""" of the database. In the unsafe zone, we use polyinstantiations and flags for the records to provide an immediate but different view from that of the safe zone to the user. Such isolation has negligible processing latency from the user's view, while it can significantly improve the integrity level of the whole database system and reduce the recovery costs. Our techniques provide different integrity levels within different zones. Both our analytical and experimental results confirm the effectiveness of our isolation techniques against data corruption attacks to the databases. Our techniques can be applied to database systems to provide multizone isolations with different levels of QoS.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4412980,no,undetermined,0
9D-6 Signal Analysis in Scanning Acoustic Microscopy for Non-Destructive Assessment of Connective Defects in Flip-Chip BGA Devices,"Failure analysis in industrial applications often require methods working non-destructively for allowing a variety of tests at a single device. Scanning acoustic microscopy in the frequency range above 100 MHz provides high axial and lateral resolution, a moderate penetration depth and the required non-destructivity. The goal of this work was the development of a method for detecting and evaluating connective defects in densely integrated flip-chip ball grid array (BGA) devices. A major concern was the ability to automatically detect and differentiate the ball-connections from the surrounding underfill and the derivation of a binary classification between void and intact connection. Flip chip ball grid arrays with a 750 mum silicon layer on top of the BGA were investigated using time resolved scanning acoustic microscopy. The microscope used was an Evolution II (SAM TEC, Aalen, Germany) in combination with a 230 MHz transducer. Short acoustic pulses were emitted into the silicon through an 8 mm liquid layer. In receive mode reflected signals were recorded, digitized and stored at the SAM's internal hard drive. The off-line signal analysis was performed using custom-made MATLAB (The Mathworks, Natick, USA) software. The sequentially working analysis characterized echo signals by pulse separation to determine the positions of BGA connectors. Time signals originated at the connector interface were then investigated by wavelet- (WVA) and pulse separation analysis (PSA). Additionally the backscattered amplitude integral (BAI) was estimated. For verification purposes defects were evaluated by X-ray- and scanning electron microscopy (SEM). It was observed that ball connectors containing cracks seen in the SEM images show decreased values of wavelet coefficients (WVC). However, the relative distribution was broader compared to intact connectors. It was found that the separation of pulses originated at the entrance and exit of the ball array corresponded to the condition of- the connector. The success rate of the acoustic method in detecting voids was 96.8%, as verified by SEM images. Defects revealed by the acoustic analysis and confirmed by SEM could be detected by X-ray microscopy only in 64% of the analysed cases. The combined analyses enabled a reliable and non destructive detection of defect ball-grid array connectors. The performance of the automatically working acoustical method seemed superior to X-ray microscopy in detecting defect ball connectors.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4409782,no,undetermined,0
Computer-Assisted Instruction in Probability and Statistics,"Because of some traditional modes of teaching, our understanding for Computer-Assisted Instruction (CAI) in probability and statistics appears narrow, making application of CAI in probability and statistics superficial. Four aspects of CAI in probability and statistics are researched mainly to reform traditional teaching and enhance teaching quality and effect of probability and statistics. Firstly, the paper defines traditional teaching of probability and statistics. Secondly, the paper addresses some skills of doing multimedia courseware of probability and statistics. Thirdly, the paper considers application and development of statistical software of CAI in probability and statistics. At last, the paper stresses the leading role of teacher in CAI.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4409339,no,undetermined,0
Visual Support In Automated Tracing,Automated traceability facilitates the dynamic generation of candidate links between requirements and other software artifacts. It provides an alternative option to the arduous and error-prone process of manually creating and maintaining a trace matrix. However the result set contains both true and false links which must therefore be evaluated by an analyst. Current approaches display the candidate links to the user in a relatively bland textual format. This position paper proposes several visualization techniques for helping analysts to evaluate sets of candidate links. The techniques are illustrated using examples from the Ice Breaker System.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4473004,no,undetermined,0
Tackling feedback design complexity,"Feedback control dealt with the application of cheap commercial off-the-shelf (COTS) hardware to control problems. What does not come cheap, however, is the design, in terms of software and people skills. Engineering design was mostly based on hand calculations 30 years ago, and scientific calculators from the likes of Hewlett Packard and Texas Instruments were all the rage with designers. The feedback loop was expressed as an equation G = A/(1-AB), which still forms the basis of most simulation and design software today, where G is the gain of the system, with A representing the feed-forward element, typically an amplifier, and B representing the feedback element. The minus sign indicates negative feedback. The input to the equation is typically by means of a matrix, which is the foundation of several of today's leading software packages, including, amongst others, Math Works Matlab (matrix laboratory) and Simulink, and National Instruments (NI) MatrixX and Lab View. Both companies are working relentlessly to complete the design cycle from design and simulation through to testing and commissioning in hardware, and even in silicon. The Math Works has recently launched Embedded Matlab, which allows users to generate embeddable C code directly from Matlab programs, avoiding the common, time-consuming and error-prone process of rewriting Matlab algorithms in C. Embedded Matlab supports many high-level Matlab language features, such as multidimensional arrays, real and complex numbers, structures, flow control and subscripting. The conversion to C code is performed by Real-Time Workshop 7. If Simulink is used, synthesisable Verilog and VHDL can also be generated.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4475492,no,undetermined,0
Surface Remeshing on Triangular Domain for CAD Applications,"In this paper a systematic remeshing method of triangle mesh surface is proposed for CAD applications. A density-sensitive edge marking function is proposed for surface resampling. Surface energy minimization is used to redistribute vertices for face geometry regularization. A simulated annealing method is proposed for vertex connectivity optimization. During each stage, the deviations from original mesh are prevented, and the boundaries and features are well preserved. While all modifications are performed locally, the error-prone global parameterization is entirely avoided. The advantage and robustness of this technique are verified by many examples from real-world product design.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407893,no,undetermined,0
Robustness of a Spoken Dialogue Interface for a Personal Assistant,"Although speech recognition systems have become more reliable in recent years, they are still highly error-prone. Other components of a spoken language dialogue system must then be robust enough to handle these errors effectively, to avoid recognition errors from adversely affecting the overall performance of the system. In this paper, we present the results of a study focusing on the robustness of our agent-based dialogue management approach. We found that while the speech recognition software produced serious errors, the dialogue manager was generally able to respond reasonably to users' utterances.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407272,no,undetermined,0
AutoGrid: Towards an Autonomic Grid Middleware,"Computer grids have drawn great attention of academic and enterprise communities, becoming an attractive alternative for the execution of applications that demand huge computational power, allowing the integration of computational resources spread through different administrative domains. However, grids exhibit high variation of resource availability, node instability, variations on load distribution, and heterogeneity of computational devices and network technology. Due to those characteristics, grid management and configuration is error-prone and almost impracticable to be performed solely by human beings. This paper describes AutoGrid, an autonomic grid middleware built using Adapta reconfiguration framework and runtime system. AutoGrid introduces self-managing capabilities to the Integrade grid middleware, such as: context-awareness, self- healing, self-optimization and self-configuration. This paper also presents insights and experiments that show the benefits towards an autonomic grid infrastructure.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407158,no,undetermined,0
A Combinatorial Approach to Quantify Stochastic Failure of Complex Component-Based Systems_The Case of an Advanced Railway Level Crossing Surveillance System,"There are different approaches to quantify stochastic failures of complex component-based systems. Methods successfully applied such as fault tree analysis, event tree analysis, Markov analysis and failure mode and effect analysis are recommended to certain extend according to their applicability to component-based systems. A combinatorial model of fault tree analysis and Markov analysis is developed in this paper to estimate the safety state of an advanced railway level crossing surveillance system which will be implemented by Taiwan Railways Administration in the future. Based on observations of an existing level crossing system, the combinatorial model is used to determine an instantaneous risk probability function, which is dependent on the system state. The results demonstrate that the advanced railway level crossing surveillance system has a higher safety state probability than the existing one.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4406401,no,undetermined,0
Towards Automatic Grading of Nuclear Cataract,"Objective quantification of lens images is essential for cataract assessment and treatment. In this paper, bottom-up and top-down strategies are combined to detect the lens contour from the slit-lamp images. The center of the lens is localized by horizontal and vertical intensity profile clustering and the lens contour is estimated by fitting an ellipse. A modified active shape model (ASM) is further applied to detect the contour of the lens. The average intensity inside the lens is employed as the indicator of nuclear opacity. The relationship between our automated nuclear cataract assessment and the clinical grading is analyzed. The preliminary study of forty images shows that the difference between automatic grading and clinical grading is acceptable.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4353454,no,undetermined,0
Varieties of interoperability in the transformation of the health-care information infrastructure,"Health-care costs are rising dramatically. Errors in medical delivery are associated with an alarming number of preventable, often fatal adverse events. A promising strategy for reversing these trends is to modernize and transform the health-care information exchange (HIE), that is, the mobilization of health-care information electronically across organizations within a region or community. The current HIE is inefficient and error-prone; it is largely paper-based, fragmented, and therefore overly complex, often relying on antiquated IT (information technology). To address these weaknesses, projects are underway to build regional and national HIEs which provide interoperable access to a variety of data sources, by a variety of stakeholders, for a variety of purposes. In this paper we present a technologist's guide to health-care interoperability. We define the stakeholders, roles, and activities that comprise an HIE solution; we describe a spectrum of interoperability approaches and point out their advantages and disadvantages; and we look in some detail at a set of real-world scenarios, discussing the interoperability approaches that best address the needs. These examples are drawn from IBM experience with real-world HIE engagements.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5386593,no,undetermined,0
Experience at Italian National Institute of Health in the quality control in telemedicine: tools for gathering data information and quality assessing,"The authors proposed a set of tools and procedures to perform a Telemedicine Quality Control process (TM-QC) to be submitted to the telemedicine (TM) manufacturers. The proposed tools were: the Informative Questionnaire (InQu), the Classification Form (ClFo), the Technical File (TF), the Quality Assessment Checklist (QACL). The InQu served to acquire the information about the examined TM product/service; the ClFo allowed to classify a TM product/service as belonging to one application area of TM. The TF was intended as a technical dossier of product and forced the TM supplier to furnish the only requested documentation of its product, so to avoid redundant information. The QACL was a checklist of requirements, regarding all the essential aspects of the telemedical applications, that each TM products/services must be met. The final assessment of the TM product/service was carried out via the QACL, by computing the number of agreed requirements: on the basis of this computation, a Quality Level (QL) was assigned to the telemedical application. Seven levels were considered, ranging from the Basic Quality Level (QL1- B) to the Excellent Quality Level (QL7-E). The TM-QC process resulted a powerful tool to perform the quality control of the telemedical applications and should be a guidance to all the TM practitioners, from the manufacturers to the expert evaluators. The quality control process procedures proposed thus could be adopted in future as routine procedures and could be useful in the assessing the TM delivering into the National Health Service versus the traditional face to face healthcare services.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352911,no,undetermined,0
Measuring and Assessing Software Reliability Growth through Simulation-Based Approaches,"In the past decade, several rate-based simulation approaches were proposed to predict software failure process. But most of them did not take the number of available debuggers into consideration and this may not be reasonable. In practice, the number of debuggers is always limited and controlled. If all debuggers or developers are busy, the new detected faults should be willing to wait (for a long time to be corrected and removed). Besides, practical experiences also show that the fault removal time is non-negligible and the number of removed faults generally lags behind the total number of detected faults. Based on these facts, in this paper, we will apply queueing theory to describe and explain the possible debugging behavior during software development. Two simulation procedures are developed based on G/G/ infin and G/G/m queueing models. The proposed methods will be illustrated with real software failure data. Experimental results will be analyzed and discussed in detail. The results we obtained will greatly help to understand the influence of size of debugger teams on the software failure correction activities and other related reliability assessments.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291036,no,undetermined,0
Failure and Coverage Factors Based Markoff Models: A New Approach for Improving the Dependability Estimation in Complex Fault Tolerant Systems Exposed to SEUs,"Dependability estimation of a fault tolerant computer system (FTCS) perturbed by single event upsets (SEUs) requires obtaining first the probability distribution functions for the time to recovery (TTR) and the time to failure (TTF) random variables. The application cross section (sigmaAP) approach does not give directly all the required information. This problem can be solved by means of the construction of suitable Markoff models. In this paper, a new method for constructing such models based on the system's failure and coverage factors is presented. Analytical dependability estimation is consistent with fault injection experiments performed in a fault tolerant operating system developed for a complex, real time data processing system.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291722,no,undetermined,0
Exposing Digital Forgeries in Interlaced and Deinterlaced Video,"With the advent of high-quality digital video cameras and sophisticated video editing software, it is becoming increasingly easier to tamper with digital video. A growing number of video surveillance cameras are also giving rise to an enormous amount of video data. The ability to ensure the integrity and authenticity of these data poses considerable challenges. We describe two techniques for detecting traces of tampering in deinterlaced and interlaced video. For deinterlaced video, we quantify the correlations introduced by the camera or software deinterlacing algorithms and show how tampering can disturb these correlations. For interlaced video, we show that the motion between fields of a single frame and across fields of neighboring frames should be equal. We propose an efficient way to measure these motions and show how tampering can disturb this relationship.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291562,no,undetermined,0
Automated Testing EJB Components Based on Algebraic Specifications,Algebraic testing is an automated software testing method based on algebraic formal specifications. It has the advantages of highly automated testing process and independence of the software's implementation details. This paper applies the method to software components. An automated testing tool called CASCAT for Java components is presented. A case study of the tool shows the high fault detecting ability.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291199,no,undetermined,0
Designing for Recovery: New Challenges for Large-Scale Complex IT Systems,"Summary form only given. Since the 1980s, the object of design for dependability has been to avoid, detect or tolerate system faults so that these do not result in failures that are detectable outside the system. Whilst this is potentially achievable in medium size systems that are controlled by a single organisations, it is now practically impossible to achieve in large-scale systems of systems where different parts of the system are owned and controlled by different organisations. Therefore, we must accept the inevitability of failure and re-orient our system design strategies to recover from those failures at minimal cost and as quickly as possible. This talk will discuss why such recovery strategies cannot be purely technical but must be socio-technical in nature and argue that design for recovery will require a better understanding of how people recover from failure and the information they need during that recovery process. I will argue that supporting recovery should be a fundamental design objective of systems and explore what this means for current approaches to large-scale systems design.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4464003,no,undetermined,0
"Requirements, Plato's Cave, and Perceptions of Reality","Software developers build systems in response to agreed-upon requirements as if those requirements were absolutely perfect. Those of us in the requirements field know that the process of creating and documenting requirements is extremely error- prone. In fact, so error prone that we wonder why developers (a) accept them as truth, and then to make matters worse, (b) make it so difficult to change them when problems are eventually discovered. This paper draws an analogy between the process of requirements determination and Plato's allegory of the cave. Specifically, it describes requirements problems that arise when our perceptions of reality differ from actual reality.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291166,no,undetermined,0
"Quality Metrics for Internet Applications: Developing """"New"""" from """"Old""""","This discussion concerns 'metrics'. More specifically, we discuss quantitative metrics for evaluating Internet applications: what should we quantify, monitor and analyse in order to characterise, evaluate and develop Internet applications based on reusing existing Internet applications,which is widely available in the Internet. Due to the distinctive evolution nature of Internet applications, assessing quality of software will provide ease and higher accuracy for Web developers. However, there is a great gap between the rapid development of Internet applications and the slow speed of developing corresponding metric measures. To tackle this issue, we look into measuring the quality of Internet applications and enable Web developers to enhance the quality of their programs and identify reusable components from Internet-based resources.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291159,no,undetermined,0
Experience Report on the Construction of Quality Models for Some Content Management Software Domains,"In previous work, we proposed the use of software quality models for driving the formulation of requirements in the context of software package selection. Now, we report two related projects of construction of software quality models in the domains of document management, entreprise content management and Web content management. These domains may be considered particular cases of a more general category sometimes labeled as content management. The goals of these projects are several. First, to assess the scalability of our methods and artifacts. Second, to investigate the degree of reusability when working on domains so closely related. Third, in relation to the previous one, to gain more knowledge of the adequacy and effectiveness of our notion of software domains taxonomy. Fourth, to evaluate the suitability and usability of our DesCOTS system proposed as tool-support for these activities.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4464010,no,undetermined,0
An Architectural Framework for the Design and Analysis of Autonomous Adaptive Systems,"Autonomous adaptive systems (AAS) have been proposed as a solution to effectively (re)design software so that it can respond to changes in execution environments, without human intervention. In the software engineering community, alternative approaches to the design of AAS have been proposed including solutions based on component technology, design patterns, and resource allocation techniques. A key limitation of the currently available approaches is that they detect constraint violations, but they do not support the prediction of constraint violations. In this work we propose an architectural framework for the design and analysis of autonomous adaptive systems, hereafter referred to as KAROO, which provides a key, new contribution: the capability to predict when a system needs to adapt itself. The results of extensive experimental evaluation of a KAROO-based system are excellent: 100% of the violations are predicted; the system is able to avoid the violations by adapting itself almost 98% of the time. The framework is a novel integration of control-theory-based adaptation, multi-criteria decision making and component-based software engineering techniques.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291014,no,undetermined,0
A Traceability Link Model for the Unified Process,"Traceability links are widely accepted as efficient means to support an evolutionary software development. However, their usage in analysis and design is effort consuming and error prone due to lacking or missing methods and tools for their creation, update and verification. In this paper we analyse and classify Unified Process artefacts to establish a traceability link model for this process. This model defines all required links between the artefacts. Furthermore, it provides a basis for the (semi)-automatic establishment and the verification of links in Unified Process development projects. We also define a first set of rules as step towards an efficient management of the links. In the ongoing project the rule set is extended to establish a whole framework of methods and rules.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287940,no,undetermined,0
Analysis of Conflicts among Non-Functional Requirements Using Integrated Analysis of Functional and Non-Functional Requirements,"Conflicts among non-functional requirements are often identified subjectively and there is a lack of conflict analysis in practice. Current approaches fail to capture the nature of conflicts among non-functional requirements, which makes the task of conflict resolution difficult. In this paper, a framework has been provided for the analysis of conflicts among non-functional requirements using the integrated analysis of functional and non-functional requirements. The framework identifies and analyzes conflicts based on relationships among quality attributes, functionalities and constraints. Since, poorly structured requirement statements usually result in confusing specifications; we also developed canonical forms for representing non-functional requirements. . The output of our framework is a conflict hierarchy that refines conflicts among non-functional requirements level by level. Finally, a case study is provided in which the proposed framework was applied to analyze and detect conflicts among the non-functional requirements of a search engine.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291007,no,undetermined,0
A Model-based Object-oriented Approach to Requirement Engineering (MORE),"Most requirement documents were written in ambiguous natural languages which are less formal and imprecise. Without modeling the requirement documents, the knowledge of the requirement is hard to be kept in a way, which can be analyzed and integrated with artifacts in other phases of software life cycle, e.g. UML diagrams in analysis and design phases. Therefore, maintaining the traceability and consistency of requirement documents and software artifacts in other phases is costly and error prone. In this paper, we propose a model-based object-oriented approach to requirement engineering (MORE). Applying modeling and OO technologies to requirement phases, the domain knowledge can be captured in a well-defined model, so the completeness, consistency, traceability and reusability of requirement and its integration with the artifacts of other phases can be cost effectively improved. A case study has shown the promise of our approach.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4290998,no,undetermined,0
Defining and Detecting Bad Smells of Aspect-Oriented Software,"Bad smells are software patterns that are generally associated with bad design and bad programming. They can be removed by using the refactoring technique which improves the quality of software. Aspect-oriented (AO) software development, which involves new notions and the different ways of thinking for developing software and solving the crosscutting problem, possibly introduces different kinds of design flaws. Defining bad smells hidden in AO software in order to point out bad design and bad programming is then necessary. This paper proposes the definition of new AO bad smells. Moreover, appropriate existing AO refactoring methods for eliminating each bad smell are presented. The proposed bad smells are validated. The results show that after removing the bad smells by using appropriate refactoring methods, the software quality is increased.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4290985,no,undetermined,0
The Study of Noise-rejection by Using Pulse Time-delay Identification Method and the Analysis of PD Data Obtained in the Field,"Based on the mechanism of partial discharges occurring in the stator winding insulations of generators, a set of on-line PD measurement with double sensors is developed in this paper. After comparing the different types of measurement, the noise suppressing method is focused on the technique of """"pulse time-delay identification"""" which is based on installing double sensors for every phase. The principle of the method is introduced in this paper. The software program designed by Labview provides the power frequency graph, N-Q and N- A two dimension diagram, three dimension diagram and maximum quantity of PD and NQN tendency diagram to make a further analysis of the PD data. In addition, the dada obtained in the field is analyzed here, including an insulation fault detected in a plant successfully.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4290885,no,undetermined,0
Petrifying Worm Cultures: Scalable Detection and Immunization in Untrusted Environments,"We present and evaluate the design of a new and comprehensive solution for automated worm detection and immunization. The system engages a peer-to-peer network of untrusted machines on the Internet to detect new worms and facilitate rapid preventative response. We evaluate the efficacy and scalability of the proposed system through large-scale simulations and assessments of a functional real-world prototype. We find that the system enjoys scalability in terms of network coverage, fault- tolerance, security, and maintainability. It proves effective against new worms, and supports collaboration among among mutually mistrusting parties.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288910,no,undetermined,0
Application of Extreme Value Theory to the Analysis of Wireless Network Traffic,"It is important to study the traffic in the wireless network control and management. This paper proposes the use of the EVT (extreme value theory) for the analysis of wireless network traffic. The role of EVT is to allow the development of procedures that are scientifically and statistically rational to estimate the extreme behavior of random processes. We have performed extensive simulation experiments by taking traffic data that is greater than a given threshold value. The results of our experiments and analysis show the wireless network traffic model obtained through the EVT fits well with the empirical distribution of traffic. Meanwhile, we can obtain EVT model has the lowest """"average deviation"""" compared with other popular distribution model such as exponential, lognormal, gamma, Weibull. Thus illustrates EVT is more suitable than other distributions to model the traffic and it has a good application foreground in the analysis of wireless network traffic.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288757,no,undetermined,0
Assessing What Information Quality Means in OTS Selection Processes,"OTS selection plays a crucial role in the deployment of software systems. One of its main current problems is how to deal with the vast amount of unstructured, incomplete, evolvable and widespread information that highly increases the risks of taking a wrong decision. The goal of our research is to tackle these information quality problems for facilitating the collection, storage, retrieval, analysis and reuse of OTS related information. An essential issue in this endeavor is to assess what OTS selectors mean by Information Quality and their needs to perform an informed selection. Therefore, we are putting forward an on-line survey to get empirical data supporting our approach.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4464033,no,undetermined,0
New Protection Techniques Against SEUs for Moving Average Filters in a Radiation Environment,"Single event effects (SEEs) caused by radiation are a major concern when working with circuits that need to operate in certain environments, like for example in space applications. In this paper, new techniques for the implementation of moving average filters that provide protection against SEEs are presented, which have a lower circuit complexity and cost than traditional techniques like triple modular redundancy (TMR). The effectiveness of these techniques has been evaluated using a software fault injection platform and the circuits have been synthesized for a commercial library in order to assess their complexity. The main idea behind the presented approach is to exploit the structure of moving average filter implementations to deal with SEEs at a higher level of abstraction.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291783,no,undetermined,0
How Business Goals Drive Architectural Design,"This paper illustrates how business goals can significantly impact a software management system's architecture without necessarily affecting its functionality. These goals include 1) supporting hardware devices from different manufacturers, 2) considering language, culture, and regulations of different markets, 3) assessing tradeoffs and risks to determine how the product should support these goals, 4) refining goals such as scaling back on intended markets, depending on the company's comfort level with the tradeoffs and risks. More importantly, these business goals correspond to quality attributes the end system must exhibit. The system must be modifiable to support a multitude of hardware devices and consider different languages and cultures. Supporting different regulations in different geographic markets requires the system to respond to life-threatening events in a timely manner performance requirement.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4292022,no,undetermined,0
High-Level Application Development is Realistic for Wireless Sensor Networks,"Programming wireless sensor network (WSN) applications is known to be a difficult task. Part of the problem is that the resource limitations of typical WSN nodes force programmers to use relatively low-level techniques to deal with the logical concurrency and asynchronous event handling inherent in these applications. In addition, existing general-purpose, node-level programming tools only support the networked nature of WSN applications in a limited way and result in application code that is hardly portable across different software platforms. All of this makes programming a single device a tedious and error-prone task. To address these issues we propose a high-level programming model that allows programmers to express applications as hierarchical state machines and to handle events and application concurrency in a way similar to imperative synchronous languages. Our program execution model is based on static scheduling what allows for standalone application analysis and testing. For deployment, the resulting programs are translated into efficient sequential C code. A prototype compiler for TinyOS has been implemented and its evaluation in described in this paper.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4292873,no,undetermined,0
Detecting and Reducing Partition Nodes in Limited-routing-hop Overlay Networks,"Many Internet applications use overlay networks as their basic facilities, like resource sharing, collaborative computing, and so on. Considering the communication cost, most overlay networks set limited hops for routing messages, so as to restrain routing within a certain scope. In this paper we describe partition nodes in such limited- routing-hop overlay networks, whose failure may potentially lead the overlay topology to be partitioned so that seriously affect its performance. We propose a proactive, distributed method to detect partition nodes and then reduce them by changing them into normal nodes. The results of simulations on both real-trace and generated topologies, scaling from 500 to 10000 nodes, show that our method can effectively detect and reduce partition nodes and improve the connectivity and fault tolerance of overlay networks.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293756,no,undetermined,0
Exploring Genetic Programming and Boosting Techniques to Model Software Reliability,"Software reliability models are used to estimate the probability that a software fails at a given time. They are fundamental to plan test activities, and to ensure the quality of the software being developed. Each project has a different reliability growth behavior, and although several different models have been proposed to estimate the reliability growth, none has proven to perform well considering different project characteristics. Because of this, some authors have introduced the use of Machine Learning techniques, such as neural networks, to obtain software reliability models. Neural network-based models, however, are not easily interpreted, and other techniques could be explored. In this paper, we explore an approach based on genetic programming, and also propose the use of boosting techniques to improve performance. We conduct experiments with reliability models based on time, and on test coverage. The obtained results show some advantages of the introduced approach. The models adapt better to the reliability curve, and can be used in projects with different characteristics.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4298229,no,undetermined,0
Automated Wireless Sensor Network Testing,"The design of distributed, wireless, and embedded system is a tedious and error-prone process. Experiences from previous real-world wireless sensor network (WSN) deployments strongly indicate that it is vital to follow a systematic design approach to satisfy all design requirements including robustness and reliability. Such a design methodology needs to include an end-to-end testing methodology. The proposed framework for WSN testing allows to apply distributed unit testing concepts in the development process. The tool flow decreases test time and allows for monitoring the correctness of the implementation throughout the development process.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297445,no,undetermined,0
Model-Based Gaze Direction Estimation in Office Environment,"In this paper, we present a model-based approach for gaze direction estimation in office environment. An overlapped elliptical model is used in detection of head, and Bayesian network model is used in estimation of gaze direction. The head consists of two regions which are face and hair region, and it can be represented by two overlapped ellipses. We use its spatial layout based on relative angle of two ellipses and size ratio of two ellipses as prior information for gaze direction estimation. In an image, the face regions are detected based on color and shape information, the hair regions are detected based on color information. The head is tracked by mean shift algorithm and adjustment method for image sequence. The performance of the proposed approach is illustrated on various image sequences obtained from office environment, and we show goodness of gaze direction estimation quality.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459595,no,undetermined,0
Evaluation and Application of MVFs in Coverage for Coverage-Based NHPP SRGM Frameworks,Many non-homogeneous Poisson process software reliability growth models are characterized by their mean value functions. Mean value functions of coverage-based models are usually obtained as composite functions of the coverage growth function and the function relating the number of detected faults to the coverage. This paper performs empirical evaluation of the relationships between the number of detected faults and the coverage embedded in the coverage- based software reliability growth models. It is also illustrated that integration of well-performing coverage growth functions and relationships between the number of detected faults and the coverage produces well-performing mean value functions.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296962,no,undetermined,0
Analysing source code: looking for useful verb-direct object pairs in all the right places,"The large time and effort devoted to software maintenance can be reduced by providing software engineers with software tools that automate tedious, error-prone tasks. However, despite the prevalence of tools such as IDEs, which automatically provide program information and automated support to the developer, there is considerable room for improvement in the existing software tools. The authors' previous work has demonstrated that using natural language information embedded in a program can significantly improve the effectiveness of various software maintenance tools. In particular, precise verb information from source code analysis is useful in improving tools for comprehension, maintenance and evolution of object-oriented code, by aiding in the discovery of scattered, action-oriented concerns. However, the precision of the extraction analysis can greatly affect the utility of the natural language information. The approach to automatically extracting precise natural language clues from source code in the form of verb- direct object (DO) pairs is described. The extraction process, the set of extraction rules and an empirical evaluation of the effectiveness of the automatic verb-DO pair extractor for Java source code are described.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4460892,no,undetermined,0
Towards an Automated Test Generation with Delayed Transitions for Timed Systems,"In this paper we analyze the influence of the urgency in the timed transitions, and as consequence, in the test suite generation. As result, we formalize rules to generate sequences where the messages exchanged may be instantaneous or delayed. In addition, the generated scenarios are able to detect timing faults. For test generation, we use a prototype tool called HJ2IF. It is based on a test purpose algorithm, called hit-or-jump and it is applied for systems specified using Intermediate Format language (IF).",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296856,no,undetermined,0
Automating Embedded Software Testing on an Emulated Target Board,"An embedded system consists of heterogeneous layers including hardware, HAL (hardware abstraction layer), OS kernel and application layer. Interactions between these layers are the software interfaces to be tested in an embedded system. The identified interfaces are important criterion that selects test cases and monitors the test results in order to detect faults and trace their causes. In this paper, we propose an automated scheme of embedded software interface test based on the emulated target board. The automated scheme enables to identify the location of interface in the source code to be tested, to generate test cases, and to determine 'pass' or fail' on the interface. We implemented the test tool called 'Justitia' based on the proposed scheme. As a case study, we applied the 'Justitia' to mobile embedded software on the S3C2440 microprocessor and Linux kernel v2.4.20.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296720,no,undetermined,0
Automated Test Data Generation using Search Based Software Engineering,"Generating test data is a demanding process. Without automation, the process is slow, expensive and error-prone. However, techniques to automate test data generation must cater for a bewildering variety of functional and non-functional test adequacy criteria and must either implicitly or explicitly solve problems involving state propagation and constraint satisfaction. This talk will show how optimisation techniques associated with search based software engineering (SBSE) have been used to automate test data generation. The talk will survey the area and present the results of recent work on characterising, transforming and eliding test data search landscapes.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296713,no,undetermined,0
Length and readability of structured software engineering abstracts,"Attempts to perform systematic literature reviews have identified a problem with the quality of software engineering abstracts for papers describing empirical studies. Structured abstracts have been found useful for improving the quality of abstracts in many other disciplines. However, there have been no studies of the value of structured abstracts in software engineering. Therefore this paper aims to assess the comparative length and readability of unstructured abstracts and structured versions of the same abstract. Abstracts were obtained from all empirical conference papers from the Evaluation and Assessment in Software Engineering Conference (EASE04 and EASE06) that did not have a structured abstract (23 in total). Two novice researchers created structured versions of the abstracts, which were checked by the papers' authors (or a surrogate). Web tools were used to extract the length in words and readability in terms of the Flesch reading ease index and automated readability index (ARI) for the structured and unstructured abstracts. The structured abstracts were on average 142.5 words longer than the unstructured abstracts (p < 0.001). The readability of the structured abstracts was better by 8.5 points on the Flesch index (p < 0.001) and 1.8 points on the ARI (p < 0.003). The results are consistent with previous studies, although the increase in length and the increase in readability are both greater than the previous studies. Future work will consider whether structured abstracts increase the content and quality of abstracts.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4460893,no,undetermined,0
A Display Simulation Toolbox for Image Quality Evaluation,"The output of image coding and rendering algorithms are presented on a diverse array of display devices. To evaluate these algorithms, image quality metrics should include more information about the spatial and chromatic properties of displays. To understand how to best incorporate such display information, we need a computational and empirical framework to characterize displays. Here we describe a set of principles and an integrated suite of software tools that provide such a framework. The display simulation toolbox (DST) is an integrated suite of software tools that help the user characterize the key properties of display devices and predict the radiance of displayed images. Assuming that pixel emissions are independent, the DST uses the sub-pixel point spread functions, spectral power distributions, and gamma curves to calculate display image radiance. We tested the assumption of pixel independence for two liquid crystal device (LCD) displays and two cathode-ray tube (CRT) displays. For the LCD displays, the independence assumption is reasonably accurate. For the CRT displays it is not. The simulations and measurements agree well for displays that meet the model assumptions and provide information about the nature of the failures for displays that do not meet these assumptions.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463754,no,undetermined,0
ModelML: a Markup Language for Automatic Model Synthesis,"Domain-specific modeling has become a popular way of designing and developing systems. It generally involves a systematic use of a set of object-oriented models to represent various facets of a domain. However, manually creating instances of these models is time-consuming and error-prone when a system in the domain is complex. Automatic model synthesis tools are thus usually developed to free users from the model creation process. In practice, most of these tools would hard code knowledge about the domain specific models in the program. A biggest problem with these tools is that their source code needs to be changed whenever the knowledge changes. In this paper, we define a model markup language (ModelML) to facilitate the development of automatic model synthesis tools. The language provides a complete self-describing representation of object-oriented models to be synthesized. Unlike other XML-based representations of models, ModelML reflects the structure of the models directly in the nesting of elements in the XML-based syntax. This feature allows the knowledge about the domain specific models to be decoupled from model synthesis tools. To demonstrate the usefulness of the markup language, we have developed a generic automatic model synthesis tool which is based on ModelML inputs.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296640,no,undetermined,0
Reuse Strategy based on Quality Certification of Reusable Components,"There are some barriers that prevent effective and systematic reuse. These barriers are produced by the need of introducing new methods for reuse development and especially by the distrust of developers in the components to be reused. One form of promoting reuse and reducing risks is guaranteeing the quality of these components. This can be achieved by assessing quality attributes and characteristics for each type of component. In this paper we present a reuse strategy based on quality certification. The strategy advantages are: the introduction of reuse throughout the software development process; incentive to reuse within the development team and the achievement of level three and four of the software reuse maturity model. The main result from this work is a strategy that encompasses the best practices of reuse and quality certification, which was validated through a survey, submitted to experts in the reuse and software engineering areas.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296611,no,undetermined,0
Extended Fault Detection Techniques for Systems-on-Chip,"The adoption of systems-on-chip (SoCs) in different types of applications represents an attracting solution. However, the high integration level of SoCs increases the sensitivity to transient faults and consequently introduces some reliability concerns. Several solutions have been proposed to attack this issue, mainly intended to face faults in the processor or in the memory. In this paper, we propose a solution to detect transient faults affecting data transmitted between the microprocessor and the communication peripherals embedded in a SoC. This solution combines some modifications of the source code at high level with the introduction of an Infrastructure IP (I-IP) to increase the dependability of the SoC.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4295254,no,undetermined,0
Layout to Logic Defect Analysis for Hierarchical Test Generation,"As shown by previous studies, shorts between the interconnect wires should be considered as the predominant cause of failures in CMOS circuits. Fault models and tools for targeting these defects, such as the bridging fault test pattern generators have been available for a long time. However, this paper proposes a new hierarchical approach based on critical area extraction for identifying the possible shorted pairs of nets on the basis of the chip layout information, combined with logic-level test pattern generation for bridging faults. Experiments on real design layouts will show that only a fraction of all the possible pairs of nets have non-zero shorting probabilities. Furthermore, it will also be proven at the logic-level that nearly all such bridging faults can be tested by a simple and robust one-pattern logic test. The methods proposed in this paper are supported by a design flow implementing existing commercial and academic CAD software.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4295251,no,undetermined,0
March DSS: A New Diagnostic March Test for All Memory Simple Static Faults,"Diagnostic march tests are powerful tests that are capable of detecting and identifying faults in memories. Although march SS was published for detecting simple static faults, no test has been published for identifying all faults possibly present in memory cells. In this paper, we target all published simple static faults. We identify faults that cannot be distinguished due to their analog behavior. We present a new methodology for generating irredundant diagnostic march tests for any desired subset of the simple static faults using the necessary and sufficient conditions for fault detection. Using that methodology, along with a verification tool, and trial and error, we were able to build a new diagnostic test for all distinguishable faults named march DSS. March DSS is the first test that is capable of identifying all distinguishable memory static faults. Compared to the latest most comprehensive published diagnostic march test, march DSS provides significant improvement in terms of fault coverage, time complexity, and power consumption. By targeting the same faults, we were able to provide a new test equivalent to the latest published test with 46% improvement in time complexity.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4294027,no,undetermined,0
On the Customization of Components: A Rule-Based Approach,"Realizing the quality-of-service (QoS) requirements for a software system continues to be an important and challenging issue in software engineering. A software system may need to be updated or reconfigured to provide modified QoS capabilities. These changes can occur at development time or at runtime. In component-based software engineering, software systems are built by composing components. When the QoS requirements change, there is a need to reconfigure the components. Unfortunately, many components are not designed to be reconfigurable, especially in terms of QoS capabilities. It is often labor-intensive and error-prone work to reconfigure the components, as developers need to manually check and modify the source code. Furthermore, the work requires experienced senior developers, which makes it costly. The limitations motivate the development of a new rule-based semiautomated component parameterization technique that performs code analysis to identify and adapt parameters and changes components into reconfigurable ones. Compared with a number of alternative QoS adaptation approaches, the proposed rule-based technique has advantages in terms of flexibility, extensibility, and efficiency. The adapted components support the reconfiguration of potential QoS trade-offs among time, space, quality, and so forth. The proposed rule-based technique has been successfully applied to two substantial libraries of components. The F-measure or balanced F-score results for the validation are excellent, that is, 94 percent. Index Terms-Performance measures, rule-based processing, representations.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288145,no,undetermined,0
A Business Modeled Approach for Trust Management in P2P,"P2P communities, is a method for arranging large numbers of peers in a self configuring peer relationship based on declared attributes (or interests) of the participating peers. This method is expected to have an impact in sharing of resources and pruning of search spaces based on the interests of the clients. Current peer- to-peer systems are targeted for information sharing, file storage, searching and indexing often using an overlay network. In this paper we expand the scope of peer-to-peer systems to include the concept of a business environment analogous to a """"stock market"""". Our work focuses on efficient methods to discover trustworthy peers in the P2P network. We investigate the behavior of randomly created relationships formed during transaction between Vendors and Emptors. Discovering services on the fly is essential to being able to identify profitable oriented transactions. In addition, efficient Vendor/Emptor based algorithms allow us to manage quickly changing market trends. Moreover the inclusion of the concept of trading policies among business communities enhances the probability of mutual gain.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287932,no,undetermined,0
A Robust Tool to Compare Pre- and Post-Surgical Voice Quality,"Assessing voice quality by means of objective parameters is of great relevance for clinicians. A large number of indexes have been proposed in literature and in commercially available software tools. However, clinicians commonly resort to a small subset of such indexes, due to difficulties in managing set up options and understanding their meaning. In this paper, the analysis has been limited to few but effective indexes, devoting great effort to their robust and automatic evaluation. Specifically, fundamental frequency (F<sub>0</sub>), along with its irregularity (jitter (J) and relative average perturbation (RAP)), noise and formant frequencies, are tracked on voiced parts of the signal only. Mean and std values are also displayed. The underlying high-resolution estimation procedure is further strengthened by an adaptive estimation of the optimal length of signal frames for analysis, linked to varying signal characteristics. Moreover, the new tool allows for automatic analysis of any kind of signal, both as far as F<sub>0</sub> range and sampling frequency are concerned, no manual setting being required to the user. This makes the tool feasible for application by non-expert users, also thanks to its simple interface. The proposed approach is applied here to patients suffering from cysts and polyps that underwent micro-laryngoscopic direct exeresis (MLSD).",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352853,no,undetermined,0
Online Applications of Wavelet Transforms to Power System Relaying - Part II,"Recent wavelet developments in power engineering applications, include detection, localization, classification, identification, storage, compression, and network/system analysis of the power quality disturbance signals, to very recently, power system relaying [1,2,3,4]. This paper assesses the online use of wavelet analysis to power system relaying. The paper presents a novel technique for transmission-line fault detection and classification using the DWT for which an optimal selection of mother wavelet and data window size based on the minimum entropy criterion has been performed. The paper starts with the review of recent work within the field of wavelet analysis and its applications to power systems engineering. Then, the theoretical background of the technique is presented and the proposed method is described in detail. Finally, the effect of different parameters on the algorithm are examined in order to highlight its performance. Typical fault conditions on a practical 220 kV power system as generated by ATP/EMTP is analyzed with Daubechies wavelets. The performance of the fault classifier is tested using MATLAB software. The feasibility of using wavelet analysis to detect and classify faults is investigated. Finally it discusses the results, limitations and possible improvement. It is found that the use of wavelet transforms together with an effective classification procedure is considered to be straightforward, fast, computationally efficient and allow for real-time accurate applications in monitoring and classifying techniques in power engineering.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275213,no,undetermined,0
Continuous SPA: Continuous Assessing and Monitoring Software Process,"In the past ten years many assessment approaches have been proposed to help manage software process quality. However, few of them are configurable and real-time in practice. Hence, it is advantageous to find ways to monitor the current status of software processes and detect the improvement opportunities. In this paper, we introduce a web- based prototype system (Continuous SPA) on continuous assessing and monitoring software process, and perform a practical study in one process area: project management. Our study results are positive and show that features such as global management, well-defined responsibility and visualization can be integrated in process assessment to help improve the software process management.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278791,no,undetermined,0
Web Service decision-making model based on uncertain-but-bounded attributes,"Web services have become one of the most popular technologies of Web application. But the quality of Web services is not as stable as traditional software components' because of the uncertainty of network. Based on non-probability-set, the convex method is used to judge the range of performance affected by uncertain-but-bounded attributes. This method only requires the highest and lowest value of the uncertain attribute values and need not to know their probability distribution. This paper proposes the metric algorithm of the change of web service quality, and proposes the Web service decisionmaking algorithm based on the theory of multiple attributes decision by TOPSIS (technique for order preference by similarity to idea solution) in operations research.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278781,no,undetermined,0
Resource Allocation Based On Workflow For Enhancing the Performance of Composite Service,"Under SOA, multiple services can be aggregated to create a new composite service based on some predefined workflow. The QoS of this composite service is determined by the cooperation of all these Web services. With workflow pipelining, it is unreasonable to improve the overall service performance by only considering individual services without considering the relationship among them. In this paper, we propose to allocate resources by tracing and predicting workloads dynamically with the pipelining of service requests in workflow graph. At any moment, there are a number of service requests being handled by different services. Firstly, we predict future workloads for any requests as soon as they arrive at any service in the workflow. Secondly, we allocate resources for the predicted workloads to enhance the performance by replicating more services to resources. Our target is to maximize the number of successful requests with the constraints of limited resources. Experiment shows that our dynamic resource allocation mechanism is more efficient for enhancing the global performance of composite service than static resource allocation mechanism in general.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278703,no,undetermined,0
A Bayesian network based Qos assessment model for web services,"Quality of service (QoS) plays a key role in Web services. In an open and volatile environment, a provider may not deliver the QoS it declared. Hence, it's necessary to provide a QoS assessment model to determine the likely behavior of a provider. Although many researches have been done to develop models and techniques to assist users in QoS assessment, most of them ignore various QoS requirements of users, which are great important to evaluate a provider adopting the policy based on service differentiation. In this paper, we propose an approach, called Bayesian network based QoS assessment model, to QoS assessment. Through online learning, it supports to update the corresponding Bayesian network dynamically. The salient feature of this model is that it can correctly predict the provider's capability in various combinations of users' QoS requirements, especially to the provider with different service levels. Experimental results show that the proposed QoS assessment model is effective.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4278696,no,undetermined,0
Jitter-Buffer Management for VoIP over Wireless LAN in a Limited Resource Device,VoIP over WLAN is a promising technology as a powerful replacement for current local wireless telephony systems. Packet timing Jitter is a constant issue in QoS of IEEE802.11 networks and exploiting an optimum jitter handling algorithm is an essential part of any VoIP over WLAN (VoWiFi) devices especially for the low cost devices with limited resources. In this paper two common algorithms using buffer as a method for Jitter handling are analyzed with relation to different traffic patterns. The effect of different buffer sizes on the quality of voice will be assessed for these patterns. Various traffic patterns were generated using OPNET and Quality of output voice was evaluated based on ITU PESQ method. It was shown that an optimum voice quality can be attained using a circular buffer with a size of around twice that of a voice packet.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4476541,no,undetermined,0
Modeling Distribution Overcurrent Protective Devices for Time-Domain Simulations,"Poor overcurrent protective device coordination can cause prolonged and unnecessary voltage variation problems. The coordination of many protective devices can be a difficult task and unfortunately, device performance and accuracy are not evaluated once the device settings are chosen and deployed. Ideally, an automated system would interrogate system data at the substation and estimate voltage variations and assess protective device performance. To test the accuracy of the automated system, a simulation model is developed to generate test data. The time-domain overcurrent protective device models can be used to estimate the duration of voltage sag during utility fault clearing operation as well. This paper presents the modeling of overcurrent protective device models created in a time-domain power system simulator. The radial distribution simulation, also made in the same time-domain software, allows testing of different overcurrent protection device settings and placement.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275922,no,undetermined,0
A Web-Based Fuel Management Software System for a Typical Indian Coal based Power Plant,"The fuel management system forms an integral part of the management process in a power plant and hence is one of the most critical areas. It deals with the management of commercial, operational and administrative functions pertaining to estimating fuel requirements, selection of fuel suppliers, fuel quality check, transportation and fuel handling, payment for fuel received, consumption and calculation of fuel efficiency. The results are then used for cost benefit analysis to suggest further plant improvement. At various levels, management information reports need to be extracted to communicate the required information across various levels of management. The core processes of fuel management involve a huge amount of paper work and manual labour, which makes it tedious, time-consuming and prone to human errors. Moreover, the time taken at each stage as well as the transparency of the relevant information has a direct bearing on the economics and efficient operation of the power plant. Both system performance and information transparency can be enhanced by the introduction of Information Technology in managing this area. This paper reports on the development of Web-based Fuel Management System Software, based on 3-tiered J2EE architecture, which aims at systematic functioning of the Core Business Processes of Fuel Management of a typical coal-fired thermal power plant in the Indian power scenario.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4275713,no,undetermined,0
Analysis of System-Failure Rate Caused by Soft-Errors using a UML-Based Systematic Methodology in an SoC,"This paper proposes an analytical method to assess the soft-error rate (SER) in the early stages of a System-on-Chip (SoC) platform-based design methodology. The proposed method gets an executable UML (Unified Modeling Language) model of the SoC and the raw soft- error rate of different parts of the platform as its inputs. Soft-errors on the design are modeled by disturbances on the value of attributes in the classes of the UML model and disturbances on opcodes of software cores. The Dynamic behavior of each core is used to determine the propagation probability of each variable disturbance to the core outputs. Furthermore, the SER and the execution time of each core in the SoC and a Failure Modes and Effects Analysis (FMEA) that determines the severity of each failure mode in the SoC are used to compute the System-Failure Rate (SFR) of the SoC.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274851,no,undetermined,0
Protocol Engineering Principles for Cryptographic Protocols Design,"Design of cryptographic protocols especially authentication protocols remains error-prone, even for experts in this area. Protocol engineering is a new notion introduced in this paper for cryptographic protocol design, which is derived from software engineering idea. We present and illustrate protocol engineering principles in three groups: cryptographic protocol security requirements analysis principles, detailed protocol design principles and provable security principles. Furthermore, we illustrate that some of the well-known Abadi and Needham's principles are ambiguous. This paper is useful in that it regards cryptographic protocol design as system engineering, hence it can efficiently indicate implicit assumptions behind cryptographic protocol design, and present operational principles on uncovering these subtleties. Although our principles are informal, but they are practical, and we believe that they will benefit other researchers.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287930,no,undetermined,0
A Rapid Fault Injection Approach for Measuring SEU Sensitivity in Complex Processors,"Processors are very common components in current digital systems and to assess their reliability is an essential task during the design process. In this paper a new fault injection solution to measure SEU sensitivity in processors is presented. It consists in a hardware-implemented module that performs fault injection through the available JTAG-based On-Chip Debugger (OCD). It can be widely applicable to different processors since JTAG standard is an extended interface and OCDs are usually available in current processors. The hardware implementation avoids the communication between the target system and the software debugging tool. The method has been applied to a complex processor, the ARM7TDMI. Results illustrate the approach is a fast, efficient and cost-effective solution.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274827,no,undetermined,0
Refactoring--Does It Improve Software Quality?,"Software systems undergo modifications, improvements and enhancements to cope with evolving requirements. This maintenance can cause their quality to decrease. Various metrics can be used to evaluate the way the quality is affected. Refactoring is one of the most important and commonly used techniques of transforming a piece of software in order to improve its quality. However, although it would be expected that the increase in quality achieved via refactoring is reflected in the various metrics, measurements on real life systems indicate the opposite. We analyzed source code version control system logs of popular open source software systems to detect changes marked as refactorings and examine how the software metrics are affected by this process, in order to evaluate whether refactoring is effectively used as a means to improve software quality within the open source community.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273477,no,undetermined,0
Integrated Management of Company Processes and Standard Processes: A Platform to Prepare and Perform Quality Management Appraisals,"Business processes have been introduced in many companies during the last years. But it was not clear how to measure the quality of these processes. ISO/IEC 15504 and CMMI have filled this gap and provide measurement frameworks to assess the maturity of processes. However, introducing and adapting processes to comply with these standards is difficult and error-prone. Especially the integration of the requirements of the standards into the company processes is hard to accomplish. In this paper we propose an integrated process modeling approach that is able to bridge the gap between business processes and requirements of the standards. Building on this integrated model we are able to produce reports that systematically uncover and display weaknesses in the process.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273472,no,undetermined,0
Robust Estimation of Timing Yield with Partial Statistical Information on Process Variations,"This paper illustrates the application of distributional robustness theory to compute the worst-case timing yield of a circuit. Our assumption is that the probability distribution of process variables are unknown and only the intervals of the process variables and their class of distributions are available. We consider two practical classes to group potential distributions. We then derive conditions that allow applying the results of the distributional robustness theory to efficiently and accurately estimate the worst-case timing yield for each class. Compared to other recent works, our approach can model correlations among process variables and does not require knowledge of exact function form of the joint distribution function of process variables. While our emphasis is on robust timing yield estimation, our approach is also applicable to other types of parametric yield.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479718,no,undetermined,0
Experiences from Representing Software Architecture in a Large Industrial Project Using Model Driven Development,"A basic idea of model driven development (MDD) is to capture all important design information in a set of formal or semi formal models that are automatically kept consistent by tools. This paper reports on industrial experience from use of MDD and shows that the approach needs improvements regarding the architecture since there are no suggested ways to formalize design rules which are an important part of the architecture. Instead, one has to rely on time consuming and error prone manual interpretations, reviews and reworkings to keep the system consistent with the architecture. To reap the full benefits of MDD it is therefore important to find ways of formalizing design rules to make it possible to allow automatic enforcement of the architecture on the system model.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273346,no,undetermined,0
A Workflow-Based Non-intrusive Approach for Enhancing the Survivability of Critical Infrastructures in Cyber Environment,"The focus of this paper is on vulnerabilities which exist in supervisory control and data acquisition (SCADA) systems. Cyber attacks targeting weaknesses in these systems can seriously degrade the survivability of a critical system. Detailed here is a non-intrusive approach for improving the survivability of these systems without interruption of their normal process flow. In a typical SCADA system, unsafe conditions are avoided by including interlocking logic code on the base system. This prevents conflicting operations from starting at inappropriate times, and provides corrective action or graceful shut-down of the system when a potentially unsafe condition is detected. If this code or these physical devices are manipulated remotely, the system can fail with unpredictable results. In the proposed approach, a workflow is constructed on a system outside of the attack path and separate from the process under control. The workflow is a combination of the functional behavior of a SCADA system and a model generated by cyber attack scenarios in that system. A cause and effect relationship of commands processed by the SCADA system is simulated in the workflow to help detect malicious operations. The workflow then contain functional and survivability knowledge of the underlying system. Failures induced by the introduction of malicious logic will be predicted by simulating the fault in the workflow. Modeling these modes of failure will be valuable in implementing damage control. This model is event driven and conducts simulation externally, hence does not interfere with normal functionality of the underlying systems.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273330,no,undetermined,0
An On-Demand Test Triggering Mechanism for NoC-Based Safety-Critical Systems,"As embedded and safety-critical applications begin to employ many-core SoCs using sophisticated on-chip networks, ensuring system quality and reliability becomes increasingly complex. Infrastructure IP has been proposed to assist system designers in meeting these requirements by providing various services such as testing and error detection, among others. One such service provided by infrastructure IP is concurrent online testing (COLT) of SoCs. COLT allows system components to be tested in-field and during normal operation of the SoC However, COLT must be used judiciously in order to minimize excessive test costs and application intrusion. In this paper, we propose and explore the use of an anomaly-based test triggering unit (ATTU) for on-demand concurrent testing of SoCs. On-demand concurrent testing is a novel solution to satisfy the conflicting design constraints of fault-tolerance and performance. Ultimately, this ensures the necessary level of design quality for safety-critical applications. To validate this approach, we explore the behavior of the ATTU using a NoC-based SoC simulator. The test triggering unit is shown to trigger tests from test infrastructure IP within 1 ms of an error occurring in the system while detecting 81% of errors, on average. Additionally, the ATTU was synthesized to determine area and power overhead.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479723,no,undetermined,0
On the Contributions of an End-to-End AOSD Testbed,"Aspect-Oriented Software Development (AOSD) techniques are gaining increased attention from both academic and industrial organisations. In order to promote a smooth adoption of such techniques it is of paramount importance to perform empirical analysis of AOSD to gather a better understanding of its benefits and limitations. In addition, the effects of aspect-oriented (AO) mechanisms on the entire development process need to be better assessed rather than just analysing each development phase in isolation. As such, this paper outlines our initial effort on the design of a testbed that will provide end-to-end systematic comparison of AOSD techniques with other mainstream modularisation techniques. This will allow the proponents of AO and non- AO techniques to compare their approaches in a consistent manner. The testbed is currently composed of: (i) a benchmark application, (ii) an initial set of metrics suite to assess certain internal and external software attributes, and (in) a """"repository"""" of artifacts derived from AOSD approaches that are assessed based on the application of (i) and (ii). This paper mainly documents a selection of techniques that will be initially applied to the benchmark. We also discuss the expected initial outcomes such a testbed will feed back to the compared techniques. The applications of these techniques are contributions from different research groups working on AOSD.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279204,no,undetermined,0
Probabilistic QoS and soft contracts for transaction based Web services,"Web services orchestrations and choreographies require establishing quality of service (QoS) contracts with the user. This is achieved by performing QoS composition, based on contracts established between the orchestration and the called Web services. These contracts are typically stated in the form of hard guarantees (e.g., response time always less than 5 msec). In this paper we propose using soft contracts instead. Soft contracts are characterized by means of probability distributions for QoS parameters. We show how to compose such contracts, to yield a global contract (probabilistic) for the orchestration. Our approach is implemented by the TOrQuE tool. Experiments on TOrQuE show that overly pessimistic contracts can be avoided and significant room for safe overbooking exists.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279591,no,undetermined,0
A Declarative Approach to Enhancing the Reliability of BPEL Processes,"Currently, BPEL is the de-facto standard for the Web service composition. Because Web services are autonomous and loosely coupled, BPEL processes are susceptible to a wide variety of faults. However, BPEL only provides limited constructs for handling faults, which makes fault handling a time-consuming and error-prone task. In this paper, we propose a declarative approach to enhancing the reliability of BPEL processes. Our solution specifies fault handling logic through a set of event- condition-action (ECA) rules which build on an extensible set of fault-tolerant patterns. These ECA rules are integrated with normal business logic before deployment to generate a fault-tolerant BPEL process. We also develop a GUI tool to assist designers to specify ECA rules. Experiments show our approach is feasible.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279609,no,undetermined,0
Utility-based QoS Brokering in Service Oriented Architectures,"Quality of service (QoS) is an important consideration in the dynamic service selection in the context of service oriented architectures. This paper extends previous work on QoS brokering for SOAs by designing, implementing, and experimentally evaluating a service selection QoS broker that maximizes a utility function for service consumers. Utility functions allow stakeholders to ascribe a value to the usefulness of a system as a function of several attributes such as response time, throughput, and availability. This work assumes that consumers of services provide to a QoS broker their utility functions and their cost constraints on the requested services. Service providers register with the broker by providing service demands for each of the resources used by the services provided and cost functions for each of the services. Consumers request services from the QoS broker, which selects a service provider that maximizes the consumer's utility function subject to its cost constraint. The QoS broker uses analytic queuing models to predict the QoS values of the various services that could be selected under varying workload conditions. The broker and services were implemented using a J2EE/Weblogic platform and experiments were conducted to evaluate the broker's efficacy. Results showed that the broker adequately adapts its selection of service providers according to cost constraints.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279627,no,undetermined,0
Dynamic coupling measurement of object oriented software using trace events,"Software metrics are increasingly playing a central role in the planning and control of software development projects. Coupling measures have important applications in software development and maintenance. They are used to reason about the structural complexity of software and have been shown to predict quality attributes such as fault-proneness, ripple effects of changes and changeability. Coupling or dependency is the degree to which each program module relies on each one of the other modules. Coupling measures characterize the static usage dependencies among the classes in an object-oriented system. Traditional coupling measures take into account only """"static"""" couplings. They do not account for """"dynamic"""" couplings due to polymorphism and may significantly underestimate the complexity of software and misjudge the need for code inspection, testing and debugging. This is expected to result in poorer predictive accuracy of the quality models that utilize static coupling measurement. In this paper, We propose dynamic coupling measurement techniques. First the source code is introspected and all the functions are added with some trace events. Then the source code is compiled and allowed to run. During runtime the trace events are logged. This log report provides the actual function call information (AFCI) during the runtime. Based on AFCI the source code is filtered to arrive the actual runtime used source code (ARUSC). The ARUSC is then given for any standard coupling technique to get the dynamic coupling.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4469179,no,undetermined,0
An Efficient K-means Clustering Algorithm Based on Influence Factors,"Clustering has been one of the most widely studied topics in data mining and pattern recognition, k-means clustering has been one of the popular, simple and faster clustering algorithms, but the right value of k is unknown and selecting effectively initial points is also difficult.In view of this, a lot of work has been done on various versions of k-means,which refines initial points and detects the number of clusters. In this paper, we present a new algorithm, called an efficient k-means clustering based on influence factors,which is divided into two stages and can automatically achieve the actual value of k and select the right initial points based on the datasets characters. Propose influence factor to measure similarity of two clusters,using it to determine whether the two clusers should be merged into one.In order to obtain a faster algorithms theorem is proposed and proofed,using it to accelerate the algorithm. Experimental results from Gaussian datasets were generated as in Pelleg and Moore (2000) show the algorithm has high quality and obtains a satisfying result.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287794,no,undetermined,0
Quality Assessment of Beef Based of Computer Vision and Electronic Nose,"Current techniques for beef quality evaluations rely on sensory methods. These procedures are subjective, prone to error, and difficult to quantify. Automated evaluation of color and odor is desirable to reduce subjectivity and discrepancies and assist with the creation of standards for inspectors worldwide. The objectives of this study were to develop color machine vision techniques for visual evaluation and to test electronic nose sensors for odor raw and beef. A color machine vision system was developed to analyze the color of beef samples. The system was able to analyze the color of samples with non-uniform color surfaces. An electronic nose sensors was used to measure odors of beef and beef stored at different temperatures, with different levels of spoilage. Discriminant function analysis was used as the pattern recognition technique to differentiate samples based on odors. Results showed that the electronic nose could discriminate differences in odor due to storage time and spoilage levels for beef. Results also showed good correlation of sensor reading with sensory scores overall, the electronic nose showed good sensitivity and accuracy. Results from this work could lead to methodologies that will assist in the objective and repeatable quality evaluation of beef. These methods have potential in industrial and regulatory application where rapid response, no sample preparation, and no need for chemicals are required.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287759,no,undetermined,0
Recognizing Humans Based on Gait Moment Image,"This paper utilizes the periodicity of swing distances to estimate gait period. It shows good adaptability to low quality silhouette images. Gait moment image (GMI) is implemented based on the estimated gait period. GMI is the gait probability image at each key moment in gait period. It reduces the noise of the silhouettes extracted from low quality videos by gait probability distribution at each key moment. Moment deviation image (MDI) is generated by using silhouette images and GMIs. As a good complement of gait energy image (GEI), MDI provides more motion features than the basic GEI. MDI is utilized together with GEI to represent a subject. The nearest neighbor classifier is adopted to recognize subjects. The proposed algorithm is evaluated on the USF gait database, and the performance is compared with the baseline algorithm and two other algorithms. Experimental results show that this algorithm achieves a higher total recognition rate than the other algorithms.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287755,no,undetermined,0
A Novel Algorithm for Detecting Air Holes in Steel Pipe Welding Based on Hopfield Neural Network,"The paper segment x-ray images of steel pipe welding to assess the quality of welding. Image segmentation is posed as an optimization problem, and is correlated with the energy function of the multistage Hopfield neural network. The algorithm for optimization and the principle of selecting coefficient are also given. The algorithm is easy to be programmed. As an application, we successfully segment some real industrial welding x-ray images.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287478,no,undetermined,0
The Influence of Defect Distribution Function Parameters on Test Patterns Generation,This paper describes the analysis of influence of yield loss model parameters on the test patterns generation. The probability of shorts between conducting paths as well as the estimations of yield loss are presented on the example gates from industrial standard cell library in 0.8 mum CMOS technology.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4286222,no,undetermined,0
Challenges and Opportunities in Information Quality,"Summary form only given. Each year companies are spending hundreds of thousands of dollars in data cleansing and other activities to improve the quality of information they use to conduct business. The hidden cost of bad data - lost opportunities, low productivity, waste, and myriads of other consequences - is believed to be much higher than these direct costs. One study estimates this combined cost due to bad data to be over U$30 billion in year 2006 alone. As business operations rely more and more on computerized systems, this cost is bound to increase at an alarming rate. Information quality (or data quality) has been an integral part of various enterprise systems such as master data management, customer data integration, and ETL (extraction, transform, and load). We are witnessing trends of renewed awareness and efforts, both in research and practice, to address information quality collectively as an independent value in enterprise computing. International organizations such as EPC Global and the International Standardization Organization (ISO) have launched working groups to study and possibly introduce standards that can be used to define, assess, and enhance information quality throughout the supply chain. Issues in information quality range over multiple disciplines including software engineering, databases, statistics, organizational operations, and accounting. The scope and goal of information quality management would depend on the organization's objectives and business models. Assessing the impact of data quality is a complex task involving key business performance indexes such as sales, profitability, and customer satisfaction. Methods of assuring data quality must address operational processes as well as supporting technologies. This panel, with input from experts from both academia and industry, explores the challenges and opportunities in information quality in the dynamic environment of today's enterprise computing.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285254,no,undetermined,0
Deployment of Accountability Monitoring Agents in Service-Oriented Architectures,"Service-oriented architecture (SOA) provides a flexible paradigm to compose dynamic service processes using individual services. However, service processes can be vastly complex, involving many service partners, thereby giving rise to difficulties in terms of pinpointing the service(s) responsible for problematic outcomes. In this research, we study efficient and effective mechanisms to deploy agents to monitor and detect undesirable services in a service process. We model the agent deployment problem as the classic weighted set covering (WSC) problem and present agent selection solutions at different stages of service process deployment. We propose the MASS (merit based agent and service selection) algorithm that considers agent cost during QoS-based service composition by using a meritagent_cost heuristic metric. We also propose the IGA (incremental greedy algorithm) to achieve fast agent selection when a service process is reconfigured after service failures. The performance study shows that our proposed algorithms are effective on saving agent cost and efficient on execution time.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285228,no,undetermined,0
A Queueing-Theory-Based Fault Detection Mechanism for SOA-Based Applications,"SOA has become more and more popular, but fault tolerance is not supported in most SOA-based applications yet. Although fault tolerance is a grand challenge for enterprise computing, we can partially resolve this problem by focusing on its some aspect. This paper focuses on fault detection and puts forward a queueing-theory-based fault detection mechanism to detect the services that fail to satisfy performance requirements. This paper also gives a reference service model and reference architecture of fault-tolerance control center of Enterprise Services Bus for SOAbased applications.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285223,no,undetermined,0
A Queueing-Theory-Based Fault Detection Mechanism for SOA-Based Applications,"SOA has become more and more popular, but fault tolerance is not supported in most SOA-based applications yet. Although fault tolerance is a grand challenge for enterprise computing, we can partially resolve this problem by focusing on its some aspect. This paper focuses on fault detection and puts forward a queueing-theory-based fault detection mechanism to detect the services that fail to satisfy performance requirements. This paper also gives a reference service model and reference architecture of fault-tolerance control center of Enterprise Services Bus for SOA- based applications.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285211,no,undetermined,0
VLSI Oriented Fast Multiple Reference Frame Motion Estimation Algorithm for H.264/AVC,"In H.264/AVC standard, motion estimation can be processed on multiple reference frames (MRF) to improve the video coding performance. For the VLSI real-time encoder, the heavy computation of fractional motion estimation (FME) makes the integer motion estimation (IME) and FME must be scheduled in two macro block (MB) pipeline stages, which makes many fast MRF algorithms inefficient for the computation reduction. In this paper, two algorithms are provided to reduce the computation of FME and IME. First, through analyzing the block's Hadamard transform coefficients, all-zero case after quantization can be accurately detected. The FME processing in the remaining frames for the block, detected as all-zero one, can be eliminated. Second, because the fast motion object blurs its edges in image, the effect of MRF to aliasing is weakened. The first reference frame is enough for fast motion MBs and MRF is just processed on those slow motion MBs with a small search range. The computation of IME is also highly reduced with this algorithm. Experimental results show that 61.4%-76.7% computation can be saved with the similar coding quality as the reference software. Moreover, the provided fast algorithms can be combined with fast block matching algorithms to further improve the performance.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4285047,no,undetermined,0
Distortion-Based Partial Distortion Search for Fast Motion Estimation,"Block motion estimation with full search is computationally complex. To reduce this complexity, different methods have been proposed, including partial distortion, which can reduce the computational complexity with no loss of image quality. We propose a distortion-based partial distortion search (DPDS) based on the magnitude of distortion and adaptive update of the matching order. We calculate absolute differences for all pixels in the predicted block point. Pixels are then sorted by the amount of distortion in a descending order for the matching process, which produces a scanning map. The sum of the absolute differences (SAD) of other candidate positions is then computed from this matching order. We also use an update of the scanning map by checking the increase in the number of absolute differences for the SAD value. The proposed DPDS algorithm improves the computational efficiency, compared with the original PDS scheme, because the accumulated value of the absolute pixel differences can rapidly reach the current minimum SAD value. The proposed algorithm is 4-13 times faster than the full search method with the same visual quality.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4284971,no,undetermined,0
Local Reference with Early Termination in H.264 Motion Estimation,"Multiple reference frames and variable block sizes improve compression efficiency of H.264, however, they also increase the encoder complexity and motion estimation time. This paper proposes a new algorithm, called local reference with early termination (LRET) to reduce the H.264 motion estimation time without adding to the encoder complexity. The LERT algorithm rearranges the search order of the reference frames based on the selection probability of the reference frames in the current frame. The experimental results show that the LERT achieves up to 59% reduction in motion estimation time with comparable video quality and negligible increase in bit-rate, as compared to the best algorithm in H.264 reference software.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4284667,no,undetermined,0
A Formal Model for Quality of Service Measurement in e-Government,"Quality is emerging as a promising approach to promote the development of services in e-Government. A proper Quality of Services is mandatory in order to satisfy citizens and firms' needs and to accept the use of ICT in our life. This paper describes our ongoing research on QoS run-time measurement and preliminary ideas on the appliance of run-time monitoring to guarantee assurance of service applications. For this purpose, we also define and describe a formal model based on a set of quality parameters for e-Government services. These parameters can be useful both as a basis for understanding and assessing competing services, and as a way to determine what improvements are needed to assure citizens and firms satisfaction.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283865,no,undetermined,0
Data Flow between Tools: Towards a Composition-Based Solution for Learning Design,"Data flow between tools cannot be specified using the current IMS Learning Design specification (LD). Nevertheless by specifying this data flow between tools, several degrees of activity automation may augment the system intervention opportunities for data flow management. Service automation, data flow automation and data flow validation may enhance the continuity of the learning design realization, reduce the student's cognitive load and obtain system-support for error prone situations. In this paper a novel approach based on the composition of LD and a standard workflow technology is proposed. Unlike other current approaches, our approach maintains interoperability with both LD and workflow standards. Then an architectural solution based on the composition approach is presented.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4281034,no,undetermined,0
Detecting Primary Transmitters via Cooperation and Memory in Cognitive Radio,"Effective detection of the activity of primary transmitters is known to be one of the major challenges to the implementation of cognitive radio systems. In this paper, we investigate the use of cooperation and memory (using tools from change detection) as means to enhance primary detection. We focus on the simple case of two secondary users and one primary source. Numerical results show the relevant performance benefits of both cooperation and memory.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4298330,no,undetermined,0
On the Evaluation of Header Compression for VoIP Traffic over DVB-RCS,"The transition towards the 'All IP' environment was long ago foreseen, but the actual shift towards this goal did not happen until recently. This inevitable convergence is naturally coupled with a number of significant advantages, but also introduces some problems that were not present before. One of these, which is the focus of this paper, is the efficient transport of voice traffic over IP (VoIP). Since voice streams consist of numerous but small packets, the overhead that is caused by the RTP/UDP/IP headers is comparable - if not higher than the capacity required for the actual payload. This handicap becomes even more severe in the case of radio communications, where the scarcity of bandwidth demands an as efficient as possible utilization. The above facts make the introduction of header compression algorithms necessary, in order to mitigate the problem of overwhelming overhead. This paper describes the design and implementation of a software platform that aims to evaluate the performance of a well known header compression scheme within the context of DVB-RCS. More specifically, the focus of this work is to assess quantitatively the gains in capacity and the degradation of quality of service, when the Compressed RTP header compression scheme is employed in this satellite environment. The presented testbed models the impairments of the satellite channel and applies the header compression mechanism on real VoIP traffic.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299089,no,undetermined,0
Managing Behaviour Trust in Grids Using Statistical Methods of Quality Assurance,"In this paper, an approach for managing behaviour trust of participants in Grid computing environments is presented. By considering the interaction process among participants in Grid environments similar to an industrial production process, we argue that through the use of statistical methods of quality assurance it is possible to monitor the behaviour of Grid participants and discover deviations in order to assess the behaviour trust of the participants.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299793,no,undetermined,0
Chip Multiprocessor Architecture:Techniques to Improve Throughput and Latency,"Chip multiprocessors - also called multi-core microprocessors or CMPs for short - are now the only way to build high-performance microprocessors, for a variety of reasons. Large uniprocessors are no longer scaling in performance, because it is only possible to extract a limited amount of parallelism from a typical instruction stream using conventional superscalar instruction issue techniques. In addition, one cannot simply ratchet up the clock speed on today's processors, or the power dissipation will become prohibitive in all but water-cooled systems. Compounding these problems is the simple fact that with the immense numbers of transistors available on today's microprocessor chips, it is too costly to design and debug ever-larger processors every year or two. CMPs avoid these problems by filling up a processor die with multiple, relatively simpler processor cores instead of just one huge core. The exact size of a CMP's cores can vary from very simple pipelines to moderately compl x superscalar processors, but once a core has been selected the CMP's performance can easily scale across silicon process generations simply by stamping down more copies of the hard-to-design, high-speed processor core in each successive chip generation. In addition, parallel code execution, obtained by spreading multiple threads of execution across the various cores, can achieve significantly higher performance than would be possible using only a single core. While parallel threads are already common in many useful workloads, there are still important workloads that are hard to divide into parallel threads. The low inter-processor communication latency between the cores in a CMP helps make a much wider range of applications viable candidates for parallel execution than was possible with conventional, multi-chip multiprocessors; nevertheless, limited parallelism in key applications is the main factor limiting acceptance of CMPs in some types of systems. After a discussion of the basi pros and cons of CMPs when they are compared with conventional uniprocessors, this book examines how CMPs can best be designed to handle two radically different kinds of workloads that are likely to be used with a CMP: highly parallel, throughput-sensitive applications at one end of the spectrum, and less parallel, latency-sensitive applications at the other. Throughput-sensitive applications, such as server workloads that handle many independent transactions at once, require careful balancing of all parts of a CMP that can limit throughput, such as the individual cores, on-chip cache memory, and off-chip memory interfaces. Several studies and example systems, such as the Sun Niagara, that examine the necessary tradeoffs are presented here. In contrast, latency-sensitive applications - many desktop applications fall into this category - require a focus on reducing inter-core communication latency and applying techniques to help programmers divide their programs into multiple threads s easily as possible. This book discusses many techniques that can be used in CMPs to simplify parallel programming, with an emphasis on research directions proposed at Stanford University. To illustrate the advantages possible with a CMP using a couple of solid examples, extra focus is given to thread-level speculation (TLS), a way to automatically break up nominally sequential applications into parallel threads on a CMP, and transactional memory. This model can greatly simplify manual parallel programming by using hardware - instead of conventional software locks - to enforce atomic code execution of blocks of instructions, a technique that makes parallel coding much less error-prone. Contents: The Case for CMPs / Improving Throughput / Improving Latency Automatically / Improving Latency using Manual Parallel Programming / A Multicore World: The Future of CMPs",2007,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6812697.pdf&bkn=6812696&pdfType=book,no,undetermined,0
Hybrid Intelligent and Adaptive Sensor Systems with Improved Noise Invulnerability by Dynamically Reconfigurable Matched Sensor Electronics,"Hybrid intelligent sensor systems and networks are composed of modules of tightly co-operating software and hardware components. Bio-inspired information processing is embodied in algorithms as well as dedicated electronics for intelligent processing and system adaptation. This paper focuses on the challenges imposed on the small yet irreplaceable analog and mixed signal components in such a sensor system, which are prone to deviation and degradations. Novel architectures combine issues of rapid-prototyping, trimming, fault-tolerance, and self- repair. However, the common reconfiguration approaches cannot deal efficiently with real-world noise problems. This paper adapts effective solution strategies to advanced sensor electronics for hybrid intelligent and adaptive sensor systems in a 0.35 mum CMOS technology and reports on the design of a novel generic chip.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344069,no,undetermined,0
Hardness for Explicit State Software Model Checking Benchmarks,"Directed model checking algorithms focus computation resources in the error-prone areas of concurrent systems. The algorithms depend on some empirical analysis to report their performance gains. Recent work characterizes the hardness of models used in the analysis as an estimated number of paths in the model that contain an error. This hardness metric is computed using a stateless random walk. We show that this is not a good hardness metric because models labeled hard with a stateless random walk metric have easily discoverable errors with a stateful randomized search. We present an analysis which shows that a hardness metric based on a stateful randomized search is a tighter bound for hardness in models used to benchmark explicit state directed model checking techniques. Furthermore, we convert easy models into hard models as measured by our new metric by pushing the errors deeper in the system and manipulating the number of threads that actually manifest an error.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343941,no,undetermined,0
Testing conformance on Stochastic Stream X-Machines,"Stream X-machines have been used to specify real systems requiring to represent complex data structures. One of the advantages of using stream X-machines to specify a system is that it is possible to produce a test set that, under certain conditions, detects all the faults of an implementation. In this paper we present a formal framework to test temporal behaviors in systems where temporal aspects are critical. Temporal requirements are expressed by means of random variables and affect the duration of actions. Implementation relations are presented as well as a method to determine the conformance of an implementation with respect to a specification by applying a test set.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343939,no,undetermined,0
Static Testing,This chapter contains sections titled: <br> Introduction <br> Goal of Static Testing <br> Candidate Documents for Static Testing <br> Static Testing Techniques <br> Tracking Defects Detected by Static Testing <br> Putting Static Testing in Perspective,2007,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=5989497.pdf&bkn=5201507&pdfType=chapter,no,undetermined,0
Comparison of Outlier Detection Methods in Fault-proneness Models,"In this paper, we experimentally evaluated the effect of outlier detection methods to improve the prediction performance of fault-proneness models. Detected outliers were removed from a fit dataset before building a model. In the experiment, we compared three outlier detection methods (Mahalanobis outlier analysis (MOA), local outlier factor method (LOFM) and rule based modeling (RBM)) each applied to three well-known fault-proneness models (linear discriminant analysis (LDA), logistic regression analysis (LRA) and classification tree (CT)). As a result, MOA and RBM improved Fl-values of all models (0.04 at minimum, 0.17 at maximum and 0.10 at mean) while improvements by LOFM were relatively small (-0.01 at minimum, 0.04 at maximum and 0.01 at mean).",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343779,no,undetermined,0
An Approach to Global Sensitivity Analysis: FAST on COCOMO,"There are various models in software engineering that are used to predict quality-related aspects of the process or artefacts. The use of these models involves elaborate data collection in order to estimate the input parameters. Hence, an interesting question is which of these input factors are most important. More specifically, which factors need to be estimated best and which might be removed from the model? This paper describes an approach based on global sensitivity analysis to answer these questions and shows its applicability in a case study on the COCOMO application at NASA.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343772,no,undetermined,0
Characterizing Software Architecture Changes: An Initial Study,"With today's ever increasing demands on software, developers must produce software that can be changed without the risk of degrading the software architecture. Degraded software architecture is problematic because it makes the system more prone to defects and increases the cost of making future changes. The effects of making changes to software can be difficult to measure. One way to address software changes is to characterize their causes and effects. This paper introduces an initial architecture change characterization scheme created to assist developers in measuring the impact of a change on the architecture of the system. It also presents an initial study conducted to gain insight into the validity of the scheme. The results of this study indicated a favorable view of the viability of the scheme by the subjects, and the scheme increased the ability of novice developers to assess and adequately estimate change effort.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343769,no,undetermined,0
Evaluating Software Project Control Centers in Industrial Environments,"Many software development organizations still lack support for detecting and reacting to critical project states in order to achieve planned goals. One means to institutionalize project control, systematic quality assurance, and management support on the basis of measurement and explicit models is the establishment of so-called software project control centers. However, there is only little experience reported in the literature with respect to setting up and applying such control centers in industrial environments. One possible reason is the lack of appropriate evaluation instruments (such as validated questionnaires and appropriate analysis procedures). Therefore, we developed an initial measurement instrument to systematically collect experience with respect to the deployment and use of control centers. Our main research goal was to develop and evaluate the measurement instrument. The instrument is based on the technology acceptance model (TAM) and customized to project controlling. This article illustrates the application and evaluation of this measurement instrument in the context of industrial case studies and provides lessons learned for further improvement. In addition, related work and conclusions for future work are given.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343759,no,undetermined,0
A New Data Hiding Scheme with Quality Control for Binary Images Using Block Parity,"Data hiding is usually achieved by alternating some nonessential information in the host message. A more challenging problem is to hide data in a two - color binary image. Hiding is difficult for the binary image since each of its black or white pixels requires only one bit representation. So that, changing a pixel can be easily detected. In this paper, we propose a new data hiding scheme using the parity of blocks. The original image is partitioned into mxn blocks. The new scheme ensures that for any bit that is modified in the host image, the bit must be adjacent to another bit that has the same value as the former's new value. Thus, the existence of secret information in the host image is difficult to detect. The invisible effect will be achieved by sacrificing some data hiding space, but the new scheme still offers a good data hiding ratio. Specifically, for each m x n block of host image, we will hide one bit of secret data by changing either one bit or without changing any bits in the block.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299818,no,undetermined,0
Controlling Energy Demand in Mobile Computing Systems,"This lecture provides an introduction to the problem of managing the energy demand of mobile devices. Reducing energy consumption, primarily with the goal of extending the lifetime of battery-powered devices, has emerged as a fundamental challenge in mobile computing and wireless communication. The focus of this lecture is on a systems approach where software techniques exploit state-of-the-art architectural features rather than relying only upon advances in lower-power circuitry or the slow improvements in battery technology to solve the problem. Fortunately, there are many opportunities to innovate on managing energy demand at the higher levels of a mobile system. Increasingly, device components offer low power modes that enable software to directly affect the energy consumption of the system. The challenge is to design resource management policies to effectively use these capabilities. The lecture begins by providing the necessary foundations, including basic energy terminology and widely accepted metrics, system models of how power is consumed by a device, and measurement methods and tools available for experimental evaluation. For components that offer low power modes, management policies are considered that address the questions of when to power down to a lower power state and when to power back up to a higher power state. These policies rely on detecting periods when the device is idle as well as techniques for modifying the access patterns of a workload to increase opportunities for power state transitions. For processors with frequency and voltage scaling capabilities, dynamic scheduling policies are developed that determine points during execution when those settings can be changed without harming quality of service constraints. The interactions and tradeoffs among the power management policies of multiple devices are discussed. We explore how the effective power management on one component of a system may have either a positive or negative impact on over ll energy consumption or on the design of policies for another component. The important role that application-level involvement may play in energy management is described, with several examples of cross-layer cooperation. Application program interfaces (APIs) that provide information flow across the application-OS boundary are valuable tools in encouraging development of energy-aware applications. Finally, we summarize the key lessons of this lecture and discuss future directions in managing energy demand.",2007,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6813185.pdf&bkn=6813184&pdfType=book,no,undetermined,0
Predicting software defects in varying development lifecycles using Bayesian nets,"An important decision in software projects is when to stop testing. Decision support tools for this have been built using causal models represented by Bayesian Networks (BNs), incorporating empirical data and expert judgement. Previously, this required a custom BN for each development lifecycle. We describe a more general approach that allows causal models to be applied to any lifecycle. The approach evolved through collaborative projects and captures significant commercial input. For projects within the range of the models, defect predictions are very accurate. This approach enables decision-makers to reason in a way that is not possible with regression-based models.",2007,https://pdfs.semanticscholar.org/508a/ce5aebc44213ae84307b4bd83ecf33a706bc.pdf,yes,undetermined,0
Identifying and characterizing change-prone classes in two large-scale open-source products,"Developing and maintaining open-source software has become an important source of profit for many companies. Change-prone classes in open-source products increase project costs by requiring developers to spend effort and time. Identifying and characterizing change-prone classes can enable developers to focus timely preventive actions, for example, peer-reviews and inspections, on the classes with similar characteristics in the future releases or products. In this study, we collected a set of static metrics and change data at class level from two open-source projects, KOffice and Mozilla. Using these data, we first tested and validated Pareto's Law which implies that a great majority (around 80%) of change is rooted in a small proportion (around 20%) of classes. Then, we identified and characterized the change-prone classes in the two products by producing tree-based models. In addition, using tree-based models, we suggested a prioritization strategy to use project resources for focused preventive actions in an efficient manner. Our empirical results showed that this strategy was effective for prioritization purposes. This study should provide useful guidance to practitioners involved in development and maintenance of large-scale open-source products.",2007,http://www.sciencedirect.com/science/article/pii/S0164121206001622,yes,undetermined,0
Evaluating the Impact of Adaptive Maintenance Process on Open Source Software Quality,"The paper focuses on measuring and assessing the relation of adaptive maintenance process and quality of open source software (OSS). A framework for assessing adaptive maintenance process is proposed and applied. The framework consists of six sub- processes. Five OSSs with considerable number of releases have been studied empirically. Their main evolutionary and quality characteristics have been measured. The main results of the study are the following:. 1) Software maintainability is affected mostly by the activities of the 'analysis' maintenance sub-process. 2) Software testability is affected by the activities of all maintenance sub-processes. 3) Software reliability is affected mostly by the activities of the 'design' and 'delivery' maintenance sub- processes. 4) Software complexity is affected mostly by the activities of the 'problem identification', design', 'implementation' and 'test' sub-processes. 5) Software flexibility is affected mostly by the activities of the 'delivery' sub-process.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343746,no,undetermined,0
Overhead Analysis of Scientific Workflows in Grid Environments,"Scientific workflows are a topic of great interest in the grid community that sees in the workflow model an attractive paradigm for programming distributed wide-area grid infrastructures. Traditionally, the grid workflow execution is approached as a pure best effort scheduling problem that maps the activities onto the grid processors based on appropriate optimization or local matchmaking heuristics such that the overall execution time is minimized. Even though such heuristics often deliver effective results, the execution in dynamic and unpredictable grid environments is prone to severe performance losses that must be understood for minimizing the completion time or for the efficient use of high-performance resources. In this paper, we propose a new systematic approach to help the scientists and middleware developers understand the most severe sources of performance losses that occur when executing scientific workflows in dynamic grid environments. We introduce an ideal model for the lowest execution time that can be achieved by a workflow and explain the difference to the real measured grid execution time based on a hierarchy of performance overheads for grid computing. We describe how to systematically measure and compute the overheads from individual activities to larger workflow regions and adjust well-known parallel processing metrics to the scope of grid computing, including speedup and efficiency. We present a distributed online tool for computing and analyzing the performance overheads in real time based on event correlation techniques and introduce several performance contracts as quality-of-service parameters to be enforced during the workflow execution beyond traditional best effort practices. We illustrate our method through postmortem and online performance analysis of two real-world workflow applications executed in the Austrian grid environment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359427,no,undetermined,0
Provable Protection against Web Application Vulnerabilities Related to Session Data Dependencies,"Web applications are widely adopted and their correct functioning is mission critical for many businesses. At the same time, Web applications tend to be error prone and implementation vulnerabilities are readily and commonly exploited by attackers. The design of countermeasures that detect or prevent such vulnerabilities or protect against their exploitation is an important research challenge for the fields of software engineering and security engineering. In this paper, we focus on one specific type of implementation vulnerability, namely, broken dependencies on session data. This vulnerability can lead to a variety of erroneous behavior at runtime and can easily be triggered by a malicious user by applying attack techniques such as forceful browsing. This paper shows how to guarantee the absence of runtime errors due to broken dependencies on session data in Web applications. The proposed solution combines development-time program annotation, static verification, and runtime checking to provably protect against broken data dependencies. We have developed a prototype implementation of our approach, building on the JML annotation language and the existing static verification tool ESC/Java2, and we successfully applied our approach to a representative J2EE-based e-commerce application. We show that the annotation overhead is very small, that the performance of the fully automatic static verification is acceptable, and that the performance overhead of the runtime checking is limited.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359468,no,undetermined,0
Comparing Model Generated with Expert Generated IV&V Activity Plans,"An IV&V activity plan describes what assurance activities to perform, where to do them, when, and to what extent. Meaningful justification for an IV&V budget and evidence that activities performed actually provide high assurance has been difficult to provide from plans created (generally ad hoc) by experts. JAXA now uses the """"strategic IV&V planning and cost model"""" to addresses these issues and complement expert planning activities. This research presents a grounded empirical study that compares plans generated by the strategic model to those created by experts on several past IV&V projects. Through this research, we found that the model generated plan typically is a superset of the experts ' plan. We found that experts tended to follow the most cost-effective route but had a bias in their particular activity selections. Ultimately we found increased confidence in both expert and model based planning and now have new tools for assessing and improving them.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343734,no,undetermined,0
"On power control for wireless sensor networks: System model, middleware component and experimental evaluation","In this paper, we investigate strategies for radio power control for wireless sensor networks that guarantee a desired packet error probability. Efficient power control algorithms are of major concern for these networks, not only because the power consumption can be significantly decreased but also because the interference can be reduced, allowing for higher throughput. An analytical model of the Received Signal Strength Indicator (RSSI), which is link quality metric, is proposed. The model relates the RSSI to the Signal to Interference plus Noise Ratio (SINR), and thus provides a connection between the powers and the packet error probability. Two power control mechanisms are studied: a Multiplicative-Increase Additive-Decrease (MIAD) power control described by a Markov chain, and a power control based on the average packet error rate. A component-based software implementation using the Contiki operating system is provided for both the power control mechanisms. Experimental results are reported for a test-bed with Telos motes.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7069054,no,undetermined,0
How good are your testers? An assessment of testing ability,"During our previous research conducted in the Sheffield Software Engineering Observatory [11], we found that test first programmers spent a higher percentage of their time testing than those testing after coding. However as the team allocation was based on subjects ' academic records and their preference, it was unclear if they were simply better testers. Thus this paper proposes two questionnaires to assess the testing ability of subjects, in order to reveal the factors that contribute to the previous findings. Preliminary results show that the testing ability of subjects, as measured by the survey, varies based on their professional skill level.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344103,no,undetermined,0
Practical strategies to improve test efficiency,"This paper introduces strategies to detect software bugs in earlier life cycle stage in order to improve test efficiency. Static analysis tool is one of the effective methods to reveal software bugs during software development. Three popular static analysis tools are introduced, two of which, PolySpace and Splint, are compared with each other by analyzing a set of test cases generatedd by the authors. PolySpace can reveal 60% bugs with 100% R/W ratio (ratio of real bugs and total warnings), while Splint reveal 73.3% bugs with 44% R/W ratio. And they are good at finding different categories of bugs. Two strategies are concluded to improve test efficiency, under the guideline that static analysis tools should be used in finding different categories of bugs according to their features. The first one aims at finding bugs as many as possible, while the second concentrates to reduce the average time on bug revelation. Experimental data shows the first strategy can find 100% bugs with 60% R/W ratio, the second one find 80% bugs with 66.7% R/W ratio. Experiment results prove that these two strategies can improve the test efficiency in both fault coverage and testing time.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6074060,no,undetermined,0
An Empirical Evaluation of the MuJava Mutation Operators,"Mutation testing is used to assess the fault-finding effectiveness of a test suite. Information provided by mutation testing can also be used to guide the creation of additional valuable tests and/or to reveal faults in the implementation code. However, concerns about the time efficiency of mutation testing may prohibit its widespread, practical use. We conducted an empirical study using the MuClipse automated mutation testing plug-in for Eclipse on the back end of a small web-based application. The first objective of our study was to categorize the behavior of the mutants generated by selected mutation operators during successive attempts to kill the mutants. The results of this categorization can be used to inform developers in their mutant operator selection to improve the efficiency and effectiveness of their mutation testing. The second outcome of our study identified patterns in the implementation code that remained untested after attempting to kill all mutants.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4344124,no,undetermined,0
Monitoring Cole-Cole Parameters During Haemodialysis (HD),"The investigation of the hydration process during the haemodialysis treatment sessions is very important for the development of methods for predicting the unbalanced fluid shifts and hypotension crisis hence improving the quality of the haemodialysis procedure. Bioimpedance measurements can give valuable information about the tissue under measurement, therefore characterizing the tissue.In this work we propose a non-invasive method based on local multifrequency bioimpedance measurements that allow us to determine the fluid distribution and variations during haemodialysis. Clinical measurements were done using 10 HD patients during 60 HD sessions. Bioimpedance data, ultrafiltration volume, blood volume and blood heamatocrit variations were recorded continuously during the HD sessions. Bioimpedance of the local tissue was measured with a 4-elctrode impedance system using surface electrodes with sampling rate of 1 meas./4 min. at 6 different frequencies. The measured impedances were fitted into Cole-Cole model and the Cole-Cole parameters were continuously determined for each measurement point during the HD session. The 4 Cole-Cole parameters (R<sub>infin</sub>R<sub>0</sub>Fc<sub>alpha</sub>) and their variations were evaluated. Impedance values at infinite and zero (R<sub>infin</sub>R<sub>0</sub>) frequencies were extrapolated from Cole-Cole mathematical model. These values are assumed to represent the impedance of total tissue fluid and the impedance of the extracellular space respectively.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352770,no,undetermined,0
Robust Nonparametric Segmentation of Infarct Lesion from Diffusion-Weighted MR Images,"Magnetic Resonance Imaging (MRI) is increasingly used for the diagnosis and monitoring of neurological disorders. In particular Diffusion-Weighted MRI (DWI) is highly sensitive in detecting early cerebral ischemic changes in acute stroke. Cerebral infarction lesion segmentation from DWI is accomplished in this work by applying nonparametric density estimation. The quality of the class boundaries is improved by including an edge confidence map, that is the confidence of truly being in the presence of a border between adjacent regions. The adjacency graph, that is constructed with the label regions, is analyzed and pruned to merge adjacent regions. The method was applied to real images, keeping all parameters constant throughout the process for each data set. The combination of region segmentation and edge detection proved to be a robust automatic technique of segmentation from DWI images of cerebral infarction regions in acute ischemic stroke. In a comparison with the reference infarct lesions segmentation, the automatic segmentation presented a significant correlation (r = 0.935), and an average Tanimoto index of 0.538.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352736,no,undetermined,0
Wavelet based approach for posture transition estimation using a waist worn accelerometer,The ability to rise from a chair is considered to be important to achieve functional independence and quality of life. This sit-to-stand task is also a good indicator to assess condition of patients with chronic diseases. We developed a wavelet based algorithm for detecting and calculating the durations of sit-to-stand and stand-to-sit transitions from the signal vector magnitude of the measured acceleration signal. The algorithm was tested on waist worn accelerometer data collected from young subjects as well as geriatric patients. The test demonstrates that both transitions can be detected by using wavelet transformation applied to signal magnitude vector. Wavelet analysis produces an estimate of the transition pattern that can be used to calculate the transition duration that further gives clinically significant information on the patients condition. The method can be applied in a real life ambulatory monitoring system for assessing the condition of a patient living at home.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352683,no,undetermined,0
Determination of simple thresholds for accelerometry-based parameters for fall detection,"The increasing population of elderly people is mainly living in a home-dwelling environment and needs applications to support their independency and safety. Falls are one of the major health risks that affect the quality of life among older adults. Body attached accelerometers have been used to detect falls. The placement of the accelerometric sensor as well as the fall detection algorithms are still under investigation. The aim of the present pilot study was to determine acceleration thresholds for fall detection, using triaxial accelerometric measurements at the waist, wrist, and head. Intentional falls (forward, backward, and lateral) and activities of daily living (ADL) were performed by two voluntary subjects. The results showed that measurements from the waist and head have potential to distinguish between falls and ADL. Especially, when the simple threshold-based detection was combined with posture detection after the fall, the sensitivity and specificity of fall detection were up to 100 %. On the contrary, the wrist did not appear to be an optimal site for fall detection.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352552,no,undetermined,0
Exploration of Quantitative Scoring Metrics to Compare Systems Biology Modeling Approaches,"In this paper, we report a focused case study to assess whether quantitative metrics are useful to evaluate molecular-level system biology models on cellular metabolism. Ideally, the bio-modeling community shall be able assess systems biology models based on objective and quantitative metrics. This is because metric-based model design not only can accelerate the validation process, but also can improve the efficacy of model design. In addition, the metric will enable researchers to select models with any desired quality standards to study biological pathway. In this case study, we compare popular systems biology modeling approaches such as Michaelis-Menten kinetics and generalized mass action and flux balance analysis to examine the difficulties in developing quantitative metrics for bio-model assessment. We created a set of guidelines in evaluating the efficacy of various bio-modeling approaches and system analysis in several """";bio-systems of interest"""";. We found that quantitative scoring metrics are essential aids for (i) model adopters and users to determine fundamental distinctions among bio-models, and (ii) model developers to improve key areas in bio-modeling. Eventually, we want to extend this evaluation practice to broad systems biology modeling.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352493,no,undetermined,0
Machine Simulation for Workflow Integration Testing,"This paper addresses the problems of modeling and simulating physical machines, part of complex industrial production lines. The direct execution of industrial workflows on production line machines before integration testing can be very expensive and may lead to improper machine operation and even to non recoverable faults. In order to simulate the execution of industrial workflow models for integration testing purposes, we propose a physical machine simulator based on nondeterministic, probability- based state machines. For each physical machine a behavioral model is constructed using operational scenarios followed by its translation into a state machine representation. The proposed simulator was used for a sausage preparing production line in the context of the food trace project.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352160,no,undetermined,0
Silicon Debug for Timing Errors,"Due to various sources of noise and process variations, assuring a circuit to operate correctly at its desired operational frequency has become a major challenge. In this paper, we propose a timing-reasoning-based algorithm and an adaptive test-generation algorithm for diagnosing timing errors in the silicon-debug phase. We first derive three metrics that are strongly correlated to the probability of a candidate's being an actual error source. We analyze the problem of circuit timing uncertainties caused by delay variations and test sampling. Then, we propose a candidate-ranking heuristic, which is robust with respect to such sources of timing uncertainty. Based on the initial ranking result and the timing information, we further propose an adaptive path-selection and test-generation algorithm to generate additional diagnostic patterns for further improvement of the first-hit-rate. The experimental results demonstrate that combining the ranking heuristic and the adaptive test-generation method would result in a very high resolution for timing diagnosis.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352014,no,undetermined,0
Performance Analysis of CORBA Replication Models,"Active and passive replication models constitute an effective way to achieve the availability objectives of distributed real-time (DRE) systems. These two models have different impacts on the application performance. Although these models have been commonly used in practical systems, a systematic quantitative evaluation of their influence on the application performance has not been conducted. In this paper we describe a methodology to analyze the application performance in the presence of active and passive replication models. For each one of these models, we obtain an analytical expression for the application response time in terms of the model parameters. Based on these analytical expressions, we derive the conditions under which one replication model has better performance over the other. Our results indicate that the superiority of one replication model over the other is governed not only by the model parameters but also by the application characteristics. We illustrate the value of the analytical expressions to assess the influence of the parameters of each model on the application response time and for a comparative analysis of the two models.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4351394,no,undetermined,0
Building a Self-Healing Operating System,"User applications and data in volatile memory are usually lost when an operating system crashes because of errors caused by either hardware or software faults. This is because most operating systems are designed to stop working when some internal errors are detected despite the possibility that user data and applications might still be intact and recoverable. Techniques like exception handling, code reloading, operating system component isolation, micro-rebooting, automatic system service restarts, watchdog timer based recovery and transactional components can be applied to attempt self-healing of an operating system from a wide variety of errors. Fault injection experiments show that these techniques can be used to continue running user applications after transparently recovering the operating system in a large percentage of cases. In cases where transparent recovery is not possible, individual process recovery can be attempted as a last resort.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4351383,no,undetermined,0
Understanding and Building Spreadsheet Tools,"Spreadsheets are among the most widely used programming systems. Unfortunately, there is a high incidence of errors within spreadsheets that are employed for a wide variety of computations. Some of these errors have a huge impact on individuals and organizations. As part of our research on spreadsheets, we have developed several approaches that are targeted at helping end-user programmers prevent, detect, and correct faults within their spreadsheets. In this tutorial, we explain fundamental principles on which spreadsheet tools can be based. We then illustrate how some simple inference mechanisms and visualization techniques that are based on these principles can be derived to detect errors or anomalous areas within spreadsheets. We also introduce a flexible framework for the quick prototype development of such spreadsheet tools and visualizations.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4351318,no,undetermined,0
Numerical Simulation of the Temperature Distribution in SiC Sublimation Growth System,"Although serious attempts have been developed silicon carbide bulk crystal growth technology to an industrial process during the last years, the quality of crystal remains deficient. One of the major problems is that the thermal field of SiC growth systems is not fully understood. Numerical simulation is considered as an important tool for the investigation of the thermal field distribution inside the growth crucible system involved with SiC bulk growth. We employ the finite-element software package ANSYS to provide additional information on the thermal field distribution. A two-dimensional model has been developed to simulate the axisymmetric growth system consist of a cylindrical susceptor (graphite crucible), a graphite felt insulation, and a copper inductive coil. The modeling field is coupled electromagnetic heating and thermal transfer. The induced magnetic field is used to predict heat generation due to magnetic induction. Conduction, convection and radiation in various components of the system are accounted for the heat transfer ways. The thermal field in SiC sublimation growth system was provided.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4350761,no,undetermined,0
Out-of-bounds array access fault model and automatic testing method study,"Out-of-bounds array access(OOB) is one of the fault models commonly employed in the objectoriented programming language. At present, the technology of code insertion and optimization is widely used in the world to detect and fix this kind of fault. Although this method can examine some of the faults in OOB programs, it cannot test programs thoroughly, neither to find the faults correctly. The way of code insertion makes the test procedures so inefficient that the test becomes costly and time-consuming. This paper, uses a kind of special static test technology to realize the fault detection in OOB programs. We first establish the fault models in OOB program, and then develop an automatic test tool to detect the faults. Some experiments have exercised and the results show that the method proposed in the paper is efficient and feasible in practical applications.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6074018,no,undetermined,0
Study on Software of VXIbus Boundary Scan Test Generation,"The goal of this paper is to develop a set of software of boundary-scan test (BST) generation using some test generation algorithms and test data. In order to get the test data quickly and effectively, a new innovative method of establishing test project description (TPD) file is presented. During the testing of two different boundary-scan circuit boards, all faults can be detected, indicating that the expected design objective is achieved.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4350620,no,undetermined,0
Using Simulation to Evaluate the Impact of New Requirements Analysis Tools,"Summary form only given. Adopting new tools and technologies on a development process can be a risky endeavor. Will the project accept the new technology? What will be the impact? Far too often the project is asked to adopt the new technology without planning how it will be applied on the project or evaluating the technology's potential impact. In this paper we provide a case study evaluating one new technology. Specifically we assess the merits of an automated defect detection tool. Using process simulation, we find situations where the use of this new technology is useful and situations where the use of this new technology is useless for large-scale NASA projects that utilize a process similar to the IEEE 12207 systems development lifecycle. We also calculate the value of the tool when implementing at different point in the process. This can help project manager to decide whether it would be worthwhile to invest in this new tool. The method can be applied to assessing the impact (including Return on Investment), break even point and the over-all value of applying any tool on a project.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349581,no,undetermined,0
A Study on Performance Measurement of a Plastic Packaging Organization's Manufacturing System by AHP Modeling,"By the effect of globalization, products, services, capital, technology, and people began to circulate more freely in the world. As a conclusion, in order to achieve and gain an advantage against competitors, manufacturing firms had to adopt themselves to changing conditions and evaluate their critical performance criteria. In this study, the aim is to determine general performance criteria and their characteristics and classifications from previous studies and evaluate performance criteria for a plastic packaging organization by utilizing analytic hierarchy process (AHP) modeling. A specific manufacturing organization, operating in the Turkish plastic packaging sector has been selected and the manufacturing performance criteria have been determined for that specific organization. Finally, the selected criteria have been assessed according to their relative importance by utilizing AHP approach and expert choice (EC) software program. As a result of this study, operating managers chose cost, quality, customer satisfaction and time factors as criteria for this organization. As the findings of the study indicate, the manufacturing organization operating in the plastic packaging sector, overviews its operations and measures its manufacturing performance basically on those four criteria and their sub criteria. Finally, relative importance of those main measures and their sub criteria are determined in consideration to plastic packaging sector.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349449,no,undetermined,0
Defect Detection Efficiency: Test Case Based vs. Exploratory Testing,"This paper presents a controlled experiment comparing the defect detection efficiency of exploratory testing (ET) and test case based testing (TCT). While traditional testing literature emphasizes test cases, ET stresses the individual tester's skills during test execution and does not rely upon predesigned test cases. In the experiment, 79 advanced software engineering students performed manual functional testing on an open-source application with actual and seeded defects. Each student participated in two 90-minute controlled sessions, using ET in one and TCT in the other. We found no significant differences in defect detection efficiency between TCT and ET. The distributions of detected defects did not differ significantly regarding technical type, detection difficulty, or severity. However, TCT produced significantly more false defect reports than ET. Surprisingly, our results show no benefit of using predesigned test cases in terms of defect detection efficiency, emphasizing the need for further studies of manual testing.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343733,no,undetermined,0
Test Inspected Unit or Inspect Unit Tested Code?,"Code inspection and unit testing are two popular fault- detecting techniques at unit level. Organizations where inspections are done generally supplement it with unit testing, as both are complementary. A natural question is the order in which the two techniques should be exercised as this may impact the overall effectiveness and efficiency of the verification process. In this paper, we present a controlled experiment comparing the two execution-orders, namely, code inspection followed by unit testing (CI-UT) and unit testing followed by code inspection (UT-CI), performed by a group of fresh software engineers in a company. The subjects inspected program-units by traversing a set of usage scenarios and applied unit testing by writing JUnit tests for the same. Our results showed that unit testing can be more effective, as well as more efficient, if applied after code inspection whereas the later is unaffected of the execution- order. Overall results suggest that sequence CI-UT performs better than UT-CI in time-constrained situations.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343732,no,undetermined,0
"Assessing, Comparing, and Combining Statechart- based testing and Structural testing: An Experiment","Although models have been proven to be helpful in a number of software engineering activities there is still significant resistance to model-driven development. This paper investigates one specific aspect of this larger problem. It addresses the impact of using statecharts for testing class clusters that exhibit a state-dependent behavior. More precisely, it reports on a controlled experiment that investigates their impact on testing fault-detection effectiveness. Code-based, structural testing is compared to statechart-based testing and their combination is investigated to determine whether they are complementary. Results show that there is no significant difference between the fault detection effectiveness of the two test strategies but that they are significantly more effective when combined. This implies that a cost-effective strategy would specify statechart-based test cases early on, execute them once the source code is available, and then complete them with test cases based on code coverage analysis.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4343731,no,undetermined,0
Fault Detection Structures for the Montgomery Multiplication over Binary Extension Fields,"Finite field arithmetic is used in applications like cryptography, where it is crucial to detect the errors. Therefore, concurrent error detection is very beneficial to increase the reliability in such applications. Multiplication is one of the most important operations and is widely used in different applications. In this paper, we target concurrent error detection in the Montgomery multiplication over binary extension fields. We propose error detection schemes for two Montgomery multiplication architectures. First, we present a new concurrent error detection scheme using the time redundancy and apply it on semi-systolic array Montgomery multipliers. Then, we propose a parity based error detection scheme for the bit-serial Montgomery multiplier over binary extension Fields.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4318983,no,undetermined,0
Node-Replacement Policies to Maintain Threshold-Coverage in Wireless Sensor Networks,"With the rapid deployment of wireless sensor networks, there are several new sensing applications with specific requirements. Specifically, target tracking applications are fundamentally concerned with the area of coverage across a sensing site in order to accurately track the target. We consider the problem of maintaining a minimum threshold-coverage in a wireless sensor network, while maximizing network lifetime and minimizing additional resources. We assume that the network has failed when the sensing coverage falls below the minimum threshold-coverage. We develop three node-replacement policies to maintain threshold-coverage in wireless sensor networks. These policies assess the candidature of each failed sensor node for replacement. Based on different performance criteria, every time a sensor node fails in the network, our replacement policies either replace with a new sensor or ignore the failure event. The node-replacement policies replace a failed node according to a node weight. The node weight is assigned based on one of the following parameters: cumulative reduction of sensing coverage, amount of energy increase per node, and local reduction of sensing coverage. We also implement a first-fail-first-replace policy and a no-replacement policy to compare the performance results. We evaluate the different node-replacement polices through extensive simulations. Our results show that given a fixed number of replacement sensor nodes, the node-replacement policies significantly increase the network lifetime and the quality of coverage, while keeping the sensing-coverage about a pre-set threshold.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4317909,no,undetermined,0
Feature Extraction System for Contextual Classification within Security Imaging Applications,"Throughout security imaging applications, there is a persistent need for accurate contextual classification of objects within the scene so proper subsequent decisions can be made. To generate a set of scene attributes necessary for this analysis, this paper presents a novel feature extraction system composed of three divisions: an edge detection system, a segmentation system, and a recognition system. System inputs are considered to be low resolution, low quality images, often collected from inexpensive security imaging cameras. This work concentrates on enhancing the accuracy of the detected boundaries and edge pixel locations within the edge detection system as a pre-processing step for the segmentation and recognition systems. The edge detection described here is based on Boolean derivatives, calculated using partial derivatives of Boolean functions in combination with fusion and binarization steps. This edge detection system allows overall subsequent improvements in the segmentation and recognition systems, producing a stronger overall feature extraction system for processing data within security imaging applications.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4304298,no,undetermined,0
Enlarging Instruction Streams,"Web applications are widely adopted and their correct functioning is mission critical for many businesses. At the same time, Web applications tend to be error prone and implementation vulnerabilities are readily and commonly exploited by attackers. The design of countermeasures that detect or prevent such vulnerabilities or protect against their exploitation is an important research challenge for the fields of software engineering and security engineering. In this paper, we focus on one specific type of implementation vulnerability, namely, broken dependencies on session data. This vulnerability can lead to a variety of erroneous behavior at runtime and can easily be triggered by a malicious user by applying attack techniques such as forceful browsing. This paper shows how to guarantee the absence of runtime errors due to broken dependencies on session data in Web applications. The proposed solution combines development-time program annotation, static verification, and runtime checking to provably protect against broken data dependencies. We have developed a prototype implementation of our approach, building on the JML annotation language and the existing static verification tool ESC/Java2, and we successfully applied our approach to a representative J2EE-based e-commerce application. We show that the annotation overhead is very small, that the performance of the fully automatic static verification is acceptable, and that the performance overhead of the runtime checking is limited.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4302707,no,undetermined,0
Dynamic Detection of COTS Component Incompatibility,"The development of COTS-based systems shifts the focus of testing and verification from single components to component integration. Independent teams and organizations develop COTS components without referring to specific systems or interaction patterns. Developing systems that reuse COTS components (even high-quality ones) therefore presents new compatibility problems. David Garlan, Robert Allen, and John Ockerbloom (1995) reported that in their experience, integrating four COTS components took 10 person-years (rather than the one planned person-year), mainly because of integration problems. According to Barry Boehm and Chris Abts (1999), three of the four main problems with reusing COTS products are absence of control over their functionality, absence of control over their evolution, and lack of design for interoperability. Our proposed technique, called behavior capture and test, detects COTS component incompatibilities by dynamically analyzing component behavior. BCT incrementally builds behavioral models of components and compares them with the behavior the components display when reused in new contexts. This lets us identify incompatibilities, unexpected interactions, untested behaviors, and dangerous side effects.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4302690,no,undetermined,0
Efficient Quality Impact Analyses for Iterative Architecture Construction,"In this paper, we present an approach that supports efficient quality impact analyses in the context of iteratively constructed architectures. Since the number of established architectural strategies and the number of inter-related models heavily increase during iterative architecture construction, the impact analysis of newly introduced quality strategies during later stages becomes highly effort-intensive and error-prone. With our approach we mitigate the effort needed for such quality impact analyses by enabling efficient separation of concerns. For achieving efficiency, we developed an aspect-oriented approach that enables the automatic weaving of quality strategies into architectural artifacts. By doing so, we are able to conduct selective quality impact evaluations with significantly reduced effort.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459140,no,undetermined,0
Automatic Rule Derivation for Adaptive Architectures,"This paper discusses on-going work in adaptive architectures concerning automatic adaptation rule derivation. Adaptation is rule-action based but deriving rules that meet the adaptation goals are tedious and error prone. We present an approach that uses model-driven derivation and training for automatically deriving adaptation rules, and exemplify this in an environment for scientific computing.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459179,no,undetermined,0
Early Software Product Improvement with Sequential Inspection Sessions: An Empirical Investigation of Inspector Capability and Learning Effects,"Software inspection facilitates product improvement in early phases of software development by detecting defects in various types of documents, e.g., requirements and design specifications. Empirical study reports show that usage-based reading (UBR) techniques can focus inspectors on most important use cases. However, the impact of inspector qualification and learning effects in the context of inspecting a set of documents in several sessions is still not well understood. This paper contributes a model for investigating the impact of inspector capability and learning effects on inspection effectiveness and efficiency in a large-scale empirical study in an academic context. Main findings of the study are (a) the inspection technique UBR better supported the performance inspectors with lower experience in sequential inspection cycles (learning effect) and (b) when inspecting objects of similar complexity significant improvements of defect detection performance could be measured.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4301086,no,undetermined,0
Quality-of-Service Management in ASEMA System for Unicast MPEG4 Video Transmissions,"The Active Service Environment Management system (ASEMA) provides best possible multimedia service experience to end user. One of the main aspects of the ASEMA is to provide a variable live streaming service to the end users of the ASEMA. The ASEMA implements this through dynamic change in the properties of the video stream delivered to the end user's end device. The live video stream used in the ASEMA is delivered to the end users is in MPEG 4 video format. The video itself is streamed on top of the real time protocol (RTP) and the parameters are negotiated with the real time streaming protocol (RTSP) before the streaming commences. The research problem of this is to provide easy solution for QoS measurements in networks that are closed nature. The research is based on the constructive method of the related publications and the results are deducted from the constructed quality of service management of the ASEMA system. The management is founded on the measuring of the receiving and sending bit rate values in both ends, in the sending end and in the receiving end. If a fluctuation in the values are detected the video stream's properties are changed dynamically.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4301080,no,undetermined,0
Using a Configurator for Predictable Component Composition,"Predicting the properties of a component composition has been studied in component-based engineering. However, most studies do not address how to find a component composition that satisfies given functional and nonfunctional requirements. This paper proposes that configurable products and configurators can be used for solving this task. The proposed solution is based on the research on traditional, mechanical configurable products. Due to the computational complexity, the solution should utilise existing techniques from the field of artificial intelligence. The applicability of the approach is demonstrated with KumbangSec, which is a conceptualisation, a language and a configurator tool for deriving component compositions with given functional and security requirements. KumbangSec configurator utilises existing inference engine models to ensure efficiency.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4301064,no,undetermined,0
Programming Approaches and Challenges for Wireless Sensor Networks,"Wireless sensor networks (WSNs) constitute a new pervasive and ubiquitous technology. They have been successfully used in various application areas and in future computing environments, WSNs will play an increasingly important role. However, programming sensor networks and applications to be deployed in them is extremely challenging. It has traditionally been an error-prone task since it requires programming individual nodes, using low-level programming issues and interfacing with the hardware and the network. This aspect is currently changing as different high-level programming abstractions and middleware solutions are coming into the arena. Nevertheless, many research challenges are still open. This paper presents a survey of the current state-of-the-art in the field, establishing a classification and highlighting some likely research challenges and future directions.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4300008,no,undetermined,0
Beyond Total Cost of Ownership: Applying Balanced Scorecards to Open-Source Software,"Potential users of Open Source Software (OSS) face the problem of evaluating OSS, in order to assess the convenience of adopting OSS instead of commercial software, or to choose among different OSS proposals. Different metrics were defined, addressing different OSS properties: the Total Cost of Ownership (TCO) addresses the cost of acquiring, adapting and operating OSS; the Total Account Ownership (TAO) represents the degree of freedom of the user with respect to the technology provider; indexes like the Open Business Quality Rating (Open BQR) assess the quality of the software with respect to the user's needs. However, none of the proposed methods and models addresses all the aspects of OSS in a balanced and complete way. For this purpose, the paper explores the possibility of adapting the Balanced Scorecard (BSC) technique to OSS. A preliminary definition of the BSC for OSS is given and discussed.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299954,no,undetermined,0
Redundant Coupling Detection Using Dynamic Dependence Analysis,"Most of the software engineers realize the importance of avoiding coupling between programs modules in order to achieve the advantages of modules reusability. However, most of them practice modules' coupling in many cases without necessities. Many techniques have been proposed to detect module's coupling in computer programs. However, developers desire to automatically identify unnecessary module's coupling that can be eliminated with minimum effort, which we refer to as redundant coupling. Redundant coupling is the module coupling that does not contribute to the output of the program. In this paper we introduce an automated approach that uses the dynamic dependence analysis to detect the redundant coupling between program's modules. Such technique guides the developers to avoid redundant coupling when testing their programs.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299924,no,undetermined,0
A Novel Framework for Test Domain Reduction using Extended Finite State Machine,"Test case generation is an expensive, tedious, and error- prone process in software testing. In this paper, test case generation is accomplished using an Extended Finite State Machine (EFSM). The proper domain representative along the specified path is selected based on fundamental calculus approximation. The pre/post-conditions of class behavior is derived from a continuous or piece-wise continuous function whose values are chosen from partitioned subdomains. Subsequent test data for the designated class can be generated from the selected test frames. In so doing, the domain is partitioned wherein reduced test cases are generated, yet insuring complete test coverage of the designated test plan. The proposed modeling technique will be conducive toward a new realm of test domain analysis. Its validity can also be procedurally proved by straightforward mathematical principles.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299923,no,undetermined,0
Identification Of Software Performance Bottleneck Components In Reuse based Software Products With The Application Of Acquaintanceship Graphs,Component-based software engineering provides an opportunity for better quality and increased productivity in software development by using reusable software components [9]. Also performance is a make-or-break quality for software. The systematic application of software performance engineering techniques throughout the development process can help to identify design alternatives that preserve desirable qualities such as extensibility and reusability while meeting performance objectives [1]. Implementing the effective performance-based management of software intensive projects has proven to be challenging task now a days. This paper aims at identifying the major reasons of software performance failures in terms of the component communication path. My work focused on one of the applications of discrete mathematics namely graph theory. This study makes an attempt to predict the most used components to the least used with the help of acquaintanceship graphs and also the shortest communication path between any two components with the help of adjacency matrix. Experiments are conducted with four components and the result shows a promising approach towards component utilization and bottleneck determination that describe the major areas in the component communication to concentrate in achieving success in cost-effective development of highly performance software.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299916,no,undetermined,0
Assessing the Effectiveness of a Distributed Method for Code Inspection: A Controlled Experiment,"We propose a distributed inspection method that tries to minimise the synchronous collaboration among team members to identify defects in software artefacts. The approach consists of identifying conflicts on the potential defects and then resolving them using an asynchronous discussion before performing a traditional synchronous meeting. This approach has been implemented in a Web based tool and assessed through a controlled experiment with master students in Computer Science at the University of Salerno. The tool presented provides automatic merge and conflict highlighting functionalities to support the inspectors during the pre-meeting refinement phase and provides the moderator with information about the inspection progress as a decision support. The tool also supports a synchronous inspection meeting to discuss about unsolved conflicts. However, by analysing the data collected during a controlled experiment we found that this phase can often be skipped due to the fact that asynchronous discussion resolved most of the conflicts.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299861,no,undetermined,0
Montgomery Multiplication with Redundancy Check,"This paper presents a method of adding redundant code to the Montgomery multiplication algorithm, to ensure that a fault attack during its calculation can be detected. This involves having checksums on the input variables that are then used to calculate a valid checksum for the output variable, in a similar manner to that proposed by Walter. However, it is shown that the proposed method is more secure than the previous work, as all the variables required to calculate Montgomery multiplication are protected.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4318982,no,undetermined,0
A comparative study of SPI Approaches with ProPAM,"Software process improvement (SPI) is one of the main software development challenges. Unfortunately, process descriptions generally do not correspond to the processes actually performed during software development projects. They just represent high-level plans and do not contain the information necessary for the concrete software projects. This deficient alignment between the process and project is caused by processes that are unrelated to project activities and failure in detecting project changes to improve the process. Process and project alignment is essential to really find out how process management is important to achieve an organization's strategic objectives. Considering this approach, this paper presents a comparative study of some of the most recognized SPI approaches and a new software process improvement methodology proposed, designed by Process and Project Alignment Methodology (ProPAM). Our intention is to show the problems observed in existing SPI approach and recognize that further research in process and project alignment based on actor oriented approaches is required.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4335238,no,undetermined,0
Using the Conceptual Cohesion of Classes for Fault Prediction in Object-Oriented Systems,"High cohesion is a desirable property of software as it positively impacts understanding, reuse, and maintenance. Currently proposed measures for cohesion in Object-Oriented (OO) software reflect particular interpretations of cohesion and capture different aspects of it. Existing approaches are largely based on using the structural information from the source code, such as attribute references, in methods to measure cohesion. This paper proposes a new measure for the cohesion of classes in OO software systems based on the analysis of the unstructured information embedded in the source code, such as comments and identifiers. The measure, named the Conceptual Cohesion of Classes (C3), is inspired by the mechanisms used to measure textual coherence in cognitive psychology and computational linguistics. This paper presents the principles and the technology that stand behind the C3 measure. A large case study on three open source software systems is presented which compares the new measure with an extensive set of existing metrics and uses them to construct models that predict software faults. The case study shows that the novel measure captures different aspects of class cohesion compared to any of the existing cohesion measures. In addition, combining C3 with existing structural cohesion metrics proves to be a better predictor of faulty classes when compared to different combinations of structural cohesion metrics.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384505,no,undetermined,0
A Study of a Transactional Parallel Routing Algorithm,"Transactional memory proposes an alternative synchronization primitive to traditional locks. Its promise is to simplify the software development of multi-threaded applications while at the same time delivering the performance of parallel applications using (complex and error prone) fine grain locking. This study reports our experience implementing a realistic application using transactional memory (TM). The application is Lee's routing algorithm and was selected for its abundance of parallelism but difficulty of expressing it with locks. Each route between a source and a destination point in a grid can be considered a unit of parallelism. Starting from this simple approach, we evaluate the exploitable parallelism of a transactional parallel implementation and explore how it can be adapted to deliver better performance. The adaptations do not introduce locks nor alter the essence of the implemented algorithm, but deliver up to 20 times more parallelism. The adaptations are derived from understanding the application itself and TM. The evaluation simulates an abstracted TM system and, thus, the results are independent of specific software or hardware TM implemented, and describe properties of the application.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4336228,no,undetermined,0
Software Reliability Analysis and Measurement Using Finite and Infinite Server Queueing Models,"Software reliability is often defined as the probability of failure-free software operation for a specified period of time in a specified environment. During the past 30 years, many software reliability growth models (SRGM) have been proposed for estimating the reliability growth of software. In practice, effective debugging is not easy because the fault may not be immediately obvious. Software engineers need time to read, and analyze the collected failure data. The time delayed by the fault detection & correction processes should not be negligible. Experience shows that the software debugging process can be described, and modeled using queueing system. In this paper, we will use both finite, and infinite server queueing models to predict software reliability. We will also investigate the problem of imperfect debugging, where fixing one bug creates another. Numerical examples based on two sets of real failure data are presented, and discussed in detail. Experimental results show that the proposed framework incorporating both fault detection, and correction processes for SRGM has a fairly accurate prediction capability.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385745,no,undetermined,0
Real-Time Frame-Layer H.264 Rate Control for Scene-Transition Video at Low Bit Rate,"An abrupt scene-transition frame is one that is hardly correlated with the previous frames. In that case, because an intra-coded frame has less distortion than an inter-coded one, almost all macroblocks are encoded in intra mode. This breaks up the rate control flow and increases the number of bits used. Since the reference software for H.264 takes no special action for a scene-transition frame, several studies have been conducted to solve the problem using the quadratic R-D model. However, since this model is more suitable for inter frames, they are unsuitable for computing the QP of the scene-transition intra frame. In this paper, a modified algorithm for detecting scene transitions is presented, and a real-time rate control scheme accounting for the characteristics of intra coding is proposed for scene- transition frames. The proposed scheme was validated using 16 test sequences. The results showed that the proposed scheme performed better than the existing H.264 rate control schemes. The PSNR was improved by an average of 0.4-0.6 dB and a maximum of 1.1-1.6 dB. The PSNR fluctuation was also improved by an average of 18.6 %.<sup>1</sup>",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341589,no,undetermined,0
A Sliced Coprocessor for Native Clifford Algebra Operations,"Computer graphics applications require efficient tools to model geometric objects. The traditional approach based on compute-intensive matrix calculations is error-prone due to a lack of integration between geometric reasoning and matrix-based algorithms. Clifford algebra offers a solution to these issues since it permits specification of geometry at a coordinate-free level. The best way to exploit the symbolic computing power of geometric (Clifford) algebra is supporting its data types and operators directly in hardware. This paper outlines the architecture of S-CliffoSor (Sliced Clifford coprocessor), a parallelizable embedded coprocessor that executes native Clifford algebra operations. S-CliffoSor is a sliced coprocessor that can be replicated for parallel execution of concurrent Clifford operations. A single slice has been designed, implemented and tested on the Celoxica Inc. RC1000 board. The experimental results show the potential to achieve a 3times speedup for Clifford sums and 4times speedup for Clifford products compared to against the analogous operations in the software library generator GAIGEN.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341505,no,undetermined,0
An Efficient Binary-Decision-Diagram-Based Approach for Network Reliability and Sensitivity Analysis,"Reliability and sensitivity analysis is a key component in the design, tuning, and maintenance of network systems. Tremendous research efforts have been expended in this area, but two practical issues, namely, imperfect coverage (IPC) and common-cause failures (CCF), have generally been missed or have not been fully considered in existing methods. In this paper, an efficient approach for fully incorporating both IPC and CCF into network reliability and sensitivity analysis is proposed. The challenges are to allow multiple failure modes introduced by IPC and to cope with multiple dependent faults caused by CCF simultaneously in the analysis. Our methodology for addressing the aforementioned challenges is to separate the consideration of both IPC and CCF from the combinatorics of the solution, which is based on reduced ordered binary decision diagrams (ROBDD). Due to the nature of the ROBDD and the separation of IPC and CCF from the solution combinatorics, our approach has a low computational complexity and is easy to implement. A sample network system is analyzed to illustrate the basics and advantages of our approach. A software tool that we developed for fault-tolerant network reliability and sensitivity analysis is also presented.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404062,no,undetermined,0
Functional Verification of RTL Designs driven by Mutation Testing metrics,"The level of confidence in a VHDL description directly depends on the quality of its verification. This quality can be evaluated by mutation-based test, but the improvement of this quality requires tremendous efforts. In this paper, we propose a new approach that both qualifies and improves the functional verification process. First, we qualify test cases thanks to the mutation testing metrics: faults are injected in the design under verification (DUV) (making DUV's mutants) to check the capacity of test cases to detect theses mutants. Then, a heuristic is used to automatically improve IPs validation data. Experimental results obtained on RTL descriptions from ITC'99 benchmark show how efficient is our approach.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341472,no,undetermined,0
Functional Test-Case Generation by a Control Transaction Graph for TLM Verification,"Transaction level modeling allows exploring several SoC design architectures leading to better performance and easier verification of the final product. Test cases play an important role in determining the quality of a design. Inadequate test-cases may cause bugs to remain after verification. Although TLM expedites the verification of a hardware design, the problem of having high coverage test cases remains unsettled at this level of abstraction. In this paper, first, in order to generate test-cases for a TL model we present a Control-Transaction Graph (CTG) describing the behavior of a TL Model. A Control Graph is a control flow graph of a module in the design and Transactions represent the interactions such as synchronization between the modules. Second, we define dependent paths (DePaths) on the CTG as test-cases for a transaction level model. The generated DePaths can find some communication errors in simulation and detect unreachable statements concerning interactions. We also give coverage metrics for a TL model to measure the quality of the generated test-cases. Finally, we apply our method on the SystemC model of AMBA-AHB bus as a case study and generate test-cases based on the CTG of this model.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4341464,no,undetermined,0
Usability Evaluation of B2C Web Site,"Web site usability is a critical metric for assessing the quality of the B2C Web site. A measure of usability must not only provide a rating for a specific Web site, but also should it illuminate the specific strengths and weaknesses about site design. In this paper, the usability and usability evaluation of B2C Web site are described. A comprehensive set of usability guidelines developed by Microsoft (MUG) is revised and utilized. The index and sub index comprising these guidelines are present firstly. The weights of each indexes and sub indexes are decided by AHP(Analytical Hierarchy Process). Base on the investigation data, a mathematic arithmetic is proposed to calculate the grade of each B2C Web site. The illustrated example shows that the evaluation approach of this paper is very effective.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4340724,no,undetermined,0
Study on Collaborative Arithmetic of Technology Support and Customer Visit of E-Business Website,"While traditional e-business Website revenue research emphasizes customer marketing such as price adjustment and individuation service, it fails to consider the collaborative dimensions of technology support capability and customer visit quantity, their interactions with a system, Website revenue is a function of customer visit quantity, and technology support capability is the base of customer visit quantity, at the same time, it will also influences the Website's cost and the QoS, and influences the revenue in the end. Our purpose is to find a collaborative arithmetic to maximize the revenue under a certain condition. Our research begins with a CBMG of a typical small-scale e-business Website, via the probability statistics of visit data; we get the relation of revenue and customer visit quantity, then we establish an QN model of the hardware structure by the queuing network theorem, via the analysis of QN model, we get the relation of technology support capability and customer visit quantity. Base on the two relations, we propose an intelligent optimization arithmetic to calculate the revenue, the calculating result can shows as a curve, and we can easy get the information of potential capability of revenue, customer quantity and the QoS from the curve. And all these information will help the Website's manager to confirm the optimal customer quantity and the only hardware investment at a certain stage.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4340716,no,undetermined,0
Video Error Concealment Using Spatio-Temporal Boundary Matching and Partial Differential Equation,"Error concealment techniques are very important for video communication since compressed video sequences may be corrupted or lost when transmitted over error-prone networks. In this paper, we propose a novel two-stage error concealment scheme for erroneously received video sequences. In the first stage, we propose a novel spatio-temporal boundary matching algorithm (STBMA) to reconstruct the lost motion vectors (MV). A well defined cost function is introduced which exploits both spatial and temporal smoothness properties of video signals. By minimizing the cost function, the MV of each lost macroblock (MB) is recovered and the corresponding reference MB in the reference frame is obtained using this MV. In the second stage, instead of directly copying the reference MB as the final recovered pixel values, we use a novel partial differential equation (PDE) based algorithm to refine the reconstruction. We minimize, in a weighted manner, the difference between the gradient field of the reconstructed MB in current frame and that of the reference MB in the reference frame under given boundary condition. A weighting factor is used to control the regulation level according to the local blockiness degree. With this algorithm, the annoying blocking artifacts are effectively reduced while the structures of the reference MB are well preserved. Compared with the error concealment feature implemented in the H.264 reference software, our algorithm is able to achieve significantly higher PSNR as well as better visual quality.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407506,no,undetermined,0
Universal Adaptive Differential Protection for Regulating Transformers,"Since regulating transformers have proved to be efficient in controlling the power flow and regulating the voltage, they are more and more widely used in today's environment of energy production, transmission and distribution. This changing environment challenges protection engineers as well to improve the sensitivity of protection, so that low-current faults could be detected (like turn-to-turn short circuits in transformer windings) and a warning message could be given. Moreover, the idea of an adaptive protection that adjusts the operating characteristics of the relay system in response to changing system conditions has became much more promising. It improves the protection sensitivity and simplifies its conception. This paper presents an adaptive adjustment concept in relation to the position change of the on load tap changer for universal differential protection of regulating transformers; such a concept provides a sensitive and cost-efficient protection for regulating transformers. Various simulations are carried out with the Electro-Magnetic Transients Program/Alternative Transients Program. The simulation results indicate the functional efficiency of the proposed concept under different fault conditions; the protection is sensitive to low level intern faults. The paper concludes by describing the software implementation of the algorithm on a test system based on a digital signal processor.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4454455,no,undetermined,0
A Low Pass Filter Traffic Shaping Mechanism for the Arrival of Traffic,"To decrease the traffic burstiness, general method of traditional shaping mechanisms is to smooth the traffic rate or the packet interarrival time of existing flows. However, traffic burstiness is also induced by the gusty arrivals of new flows. According to our knowledge, there is no research which attempts to decrease the burstiness caused by the arrivals of new flows. To cope with this, a shaping mechanism to the interarrival of flow named interarrival low pass filter (ILPF) fore-shaping mechanism is proposed in this paper. By the low pass filter, the ILPF fore-shaping mechanism smoothes the flow interarrival time so as to decrease the traffic burstiness. The ILPF fore- shaping mechanism is well suitable for the real-time applications which can endure some delay for accessing. According to theoretic analysis and computer simulation, it is proved that the ILPF fore-shaping mechanism significantly improves the utilization of the network as well as providing QoS guarantees in terms of probability.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4340039,no,undetermined,0
"A Risk-Based, Value-Oriented Approach to Quality Requirements","When quality requirements are elicited from stakeholders, they're often stated qualitatively, such as """"the response time must be fast"""" or """"we need a highly available system"""". However, qualitatively represented requirements are ambiguous and thus difficult to verify. The value-oriented approach to specifying quality requirements uses a range of potential representations chosen on the basis of assessing risk instead of quantifying everything.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4455629,no,undetermined,0
A Compound PRM Method for Path Planning of the Tractor-Trailer Mobile Robot,This paper researches the path planning problem for the tractor-trailer mobile robot and presents a novel environment modelling method called Compound PRM which builds the global compound roadmap by combining the local regular roadmap with the universal probabilistic roadmap. Path planning based on the Compound PRM roadmap can improve the quality of the local routes near obstacles and lower the complexity of the planning computation. Also the loss of feasible space is avoided during path planing. The simulation experiment shows that this method is very efficient in the use of tractor-trailer mobile robot path planning.,2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4338880,no,undetermined,0
Traffic Object Tracking Based on Increased-step Motion History Image,"A new image-based method for tracking moving vehicles is purposed using the layered step-down grey value silhouette of increased-step motion history image. Real-time tracking of moving vehicles appeared in the traffic scene can be achieved by segmentation and marking the motion silhouette regions in the increased-step motion history images. High quality segmentation is realized by improve the basic motion history image. Background of the traffic scene is subtracted from the traffic video frames. At last, experiments are done with crossroad traffic videos. Moving objects is segmented effectively. The results show that the method is robust against the disturbance of changeful environment, high detecting rate and fast real-time processing speed.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4338584,no,undetermined,0
Subjective Evaluation of Techniques for Proper Name Pronunciation,"Automatic pronunciation of unknown words of English is a hard problem of great importance in speech technology. Proper names constitute an especially difficult class of words to pronounce because of their variable origin and uncertain degree of assimilation of foreign names to the conventions of the local speech community. In this paper, we compare four different methods of proper name pronunciation for English text-to-speech (TTS) synthesis. The first (intended to be used as the primary strategy in a practical TTS system) uses a set of manually supplied pronunciations, referred to as the ldquodictionaryrdquo pronunciations. The remainder are pronunciations obtained from three different data-driven approaches (intended as candidates for the back-up strategy in a real system) which use the dictionary of ldquoknownrdquo proper names to infer pronunciations for unknown names. These are: pronunciation by analogy (PbA), a decision tree method (CART), and a table look-up method (TLU). To assess the acceptability of the pronunciations to potential users of a TTS system, subjective evaluation was carried out, in which 24 listeners rated 1200 synthesized pronunciations of 600 names by the four methods using a five-point (opinion score) scale. From over 50 000 proper names and their pronunciations, 150 so-called one-of-a-kind pronunciations were selected for each of the four methods (600 in total). A one-of-a-kind pronunciation is one for which one of the four methods disagrees with the other three methods, which agree among themselves. Listener opinions on one-of-a-kind pronunciations are argued to be a good measure of the overall quality of a particular method. For each one-of-a-kind pronunciation, there is a corresponding so-called rest pronunciation (another 600 in total), on which the remaining three competitor methods agree, for which listener opinions are taken to be indicative of the general quality of the competition. Nonparametric tests of significance of mean opin- on scores show that the dictionary pronunciations are rated superior to the automatically inferred pronunciations with little difference between the data-driven methods for the one-of-a-kind pronunciations, but for the rest pronunciations there is suggestive evidence that PbA is superior to both CART and TLU, which perform at approximately the same level.",2007,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4337635,no,undetermined,0
A Fault Tolerant and Multi-Paradigm Grid Architecture for Time Constrained Problems. Application to Option Pricing in Finance.,"This paper introduces a Grid software architecture offering fault tolerance, dynamic and aggressive load balancing and two complementary parallel programming paradigms. Experiments with financial applications on a real multi-site Grid assess this solution. This architecture has been designed to run industrial and financial applications, that are frequently time constrained and CPU consuming, feature both tightly and loosely coupled parallelism requiring generic programming paradigm, and adopt client-server business architecture.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031022,no,undetermined,0
Online hardening of programs against SEUs and SETs,"Processor cores embedded in systems-on-a-chip (SoCs) are often deployed in critical computations, and when affected by faults they may produce dramatic effects. When hardware hardening is not cost-effective, software implemented hardware fault tolerance (SIHFT) can be a solution to increase SoCs' dependability. However, SIHFT increases the time for running the hardened application, and the memory occupation. In this paper we propose a method that eliminates the memory overhead, using a new approach to instruction hardening and control flow checking during the execution of the application, without the need for introducing any change in its source code. The proposed method is also non-intrusive, since it does not require any modification in the main processor's architecture. The method is suitable for hardening SoCs against transient faults and also for detecting permanent faults",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030939,no,undetermined,0
Mathematical Modelling for the Design of an Edge Router,"This paper presents the formulation, modelling and analysis of network traffic for the purpose of designing an edge router, catering for a large campus. Presuming packet arrival from Poissonpsilas process, and departure or service time distribution to be exponential, a Markov model has been developed. The stochastic characteristics of the traffic has been monitored and studied over long durations through different times of a day and different days of a week. Design parameters like buffer capacity, link speed and various other key parameters for an edge router have been derived. Additionally, an algorithm has been suggested to manipulate the weights of the queues dynamically in a weighted round robin scheduling, thereby changing the packet departure rate of the flows. The algorithm eventually provides a control over the load factors and the probability of packet.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617373,no,undetermined,0
Motion analysis of the international and national rank squash players,"In this paper, we present a study on squash player work-rate during the squash matches of two different quality levels. To assess work-rate, the measurement of certain parameters of player motion is needed. The computer vision based software application was used to automatically obtain player motion data from the digitized video recordings of 22 squash matches. The matches were played on two quality levels - international and Slovene national players. We present the results of work-rate comparison between these two groups of players based on game duration and distance covered by the players. We found that the players on the international quality level on average cover significantly larger distances, which is partially caused by longer average game durations.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521312,no,undetermined,0
A collaborative filtering algorithm embedded BP network to ameliorate sparsity issue,"Collaborative filtering technologies are facing two major challenges: scalability and recommendation quality. Sparsity of source data sets is one major reason causing the poor recommendation quality. To reduce sparsity, we design a collaborative filtering algorithm who firstly selects users whose non-null ratings intersect the most as candidates of nearest neighbors, and then builds up backpropagation neural networks to predict values of the null ratings in the candidates. Experimental results show that this algorithm can increase the accuracy of nearest neighbors, resulting in improving recommendation quality of the recommendation system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1527245,no,undetermined,0
The art and science of software release planning,"Incremental development provides customers with parts of a system early, so they receive both a sense of value and an opportunity to provide feedback early in the process. Each system release is thus a collection of features that the customer values. Furthermore, each release serves to fix defects detected in former product variants. Release planning (RP) addresses decisions related to selecting and assigning features to create a sequence of consecutive product releases that satisfies important technical, resource, budget, and risk constraints.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524914,no,undetermined,0
Error Modeling in Dependable Component-Based Systems,"Component-based development (CBD) of software, with its successes in enterprise computing, has the promise of being a good development model due to its cost effectiveness and potential for achieving high quality of components by virtue of reuse. However, for systems with dependability concerns, such as real-time systems, a major challenge in using CBD consists of predicting dependability attributes, or providing dependability assertions, based on the individual component properties and architectural aspects. In this paper, we propose a framework which aims to address this challenge. Specifically, we present a revised error classification together with error propagation aspects, and briefly sketch how to compose error models within the context of component-based systems (CBS). The ultimate goal is to perform the analysis on a given CBS, in order to find bottlenecks in achieving dependability requirements and to provide guidelines to the designer on the usage of appropriate error detection and fault tolerance mechanisms.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591772,no,undetermined,0
Operational experience with intelligent software agents for shipboard diesel and gas turbine engine health monitoring,"The power distribution network aboard future navy warships are vital to reliable operations and survivability. Power distribution involves delivering electric power from multiple generation sources to a dynamic set of load devices. Advanced power electronics, intelligent controllers, and a communications infrastructure form a shipboard power distribution network, much like the domestic electric utility power grid. Multiple electric generation and storage devices distributed throughout the ship will eliminate dependence on any single power source through dynamic load management and power grid connectivity. Although new technologies are under development, gas turbine and diesel generators remain as the likely near-term power sources for the future all-electric ship integrated power system (IPS). Health monitoring of these critical IPS power sources are essential to achieving reliability and survivability goals. System complexity, timing constraints, and manning constraints will shift both control and equipment health monitoring functions from human operators to intelligent machines. Drastic manning reductions coupled with a large increase in the number of sensor monitoring points makes automated condition-based maintenance (CBM) a stringent requirement. CBM has traditionally been labor intensive and expensive to implement, relying on human experiential knowledge, interactive data processing, information management, and cognitive processing. The diagnostic robustness and accuracy of these embedded software agents are essential, as false or missed diagnostic calls have severe ramifications within the intelligent, automated control environment. This paper presents some of the first reported results of intelligent diagnostic software agents operating in real-time onboard naval ships with gas turbine and diesel machinery plants. The agents are shown to perform a substantial amount of CBM-related data processing and analysis that would not otherwise be performed by the cre- , including real-time, neural network diagnostic inferencing. The agents are designed to diagnose existing system faults and to predict machinery problems at their earliest stage of development. The results reported herein should be of particular interest to those involved with future all-electric ship designs that includes both gas turbine and diesel engines as primary electrical power sources.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524673,no,undetermined,0
An Efficient and Practical Defense Method Against DDoS Attack at the Source-End,"Distributed Denial-of-Service (DDoS) attack is one of the most serious threats to the Internet. Detecting DDoS at the source-end has many advantages over defense at the victim-end and intermediate-network. One of the main problems for source-end methods is the performance degradation brought by these methods, which discourages Internet service providers (ISPs) to deploy the defense system. We propose an efficient detection approach, which only requires limited fixed-length memory and low computation overhead but provides satisfying detection results. The low cost of defense is expected to attract more ISPs to join the defense. The experiments results show our approach is efficient and feasible for defense at the source-end",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524303,no,undetermined,0
Pre-layout physical connectivity prediction with application in clustering-based placement,"In this paper, we introduce a structural metric, logic contraction, for pre-layout physical connectivity prediction. For a given set of nodes forming a cluster in a netlist, we can predict their proximity in the final layout based on the logic contraction value of the cluster. We demonstrate a very good correlation of our pre-layout measure with the post-layout physical distances between those nodes. We show an application of the logic contraction to circuit clustering. We compare our seed-growth clustering algorithm with the existing efficient clustering techniques. Experimental results demonstrate the effectiveness of our new clustering method.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524126,no,undetermined,0
Fast Motion Estimation by Motion Vector Merging Procedure for H. 264,"In this paper, a fast motion estimation algorithm for variable block-size by using a motion vector merging procedure is proposed for H.264. The motion vectors of adjacent small blocks are merged to predict the motion vectors of larger blocks for reducing the computation. Experimental results show that our proposed method has lower computational complexity than full search, fast full search and fast motion estimation of the H.264 reference software JM93 with slight quality decrease and little bit-rate increase",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521703,no,undetermined,0
On the Use of Specification-Based Assertions as Test Oracles,"The """"oracle problem' is a well-known challenge for software testing. Without some means of automatically computing the correct answer for test cases, testers must instead compute the results by hand, or use a previous version of the software. In this paper, we investigate the feasibility of revealing software faults by augmenting the code with complete, specification-based assertions. Our evaluation method is to (1) develop a formal specification, (2) translate this specification into assertions, (3) inject or identify existing faults, and (4) for each version of the assertion-enhanced system containing a fault, execute it using a set of test inputs and check for assertion violations. Our goal is to determine whether specification-based assertions are a viable method of revealing faults, and to begin to assess the extent to which their cost-effectiveness can be improved. Our evaluation is based on two case studies involving real-world software systems. Our results indicate that specification-based assertions can effectively reveal faults, as long as they adversely affect the program state. We describe techniques that we used for translating high-level specifications into code-level assertions. We also discuss the costs associated with the approach, and potential techniques for reducing these costs",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521219,no,undetermined,0
Performance analysis of random access channel in OFDMA systems,"The random access channel (RACH) in OFDMA systems is an uplink contention-based transport channel that is mainly used for subscriber stations to make a resource request to base stations. In this paper we focus on analyzing the performance of RACH in OFDMA systems such that the successful transmission probability, correctly detectable probability and throughput of RACH are analyzed. We also choose an access mechanism with binary exponential backoff delay procedure similar to that in IEEE 802.11. Based on the mechanism, we derive the delay and the blocking probability of RACH in OFDMA systems.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515514,no,undetermined,0
Code Normal Forms,"Because of their strong economic impact, complexity and maintainability are among the most widely used terms in software engineering. But, they are also among the most weakly understood. A multitude of software metrics attempts to analyze complexity and a proliferation of different definitions of maintainability can be found in text books and corporate quality guide lines. The trouble is that none of these approaches provides a reliable basis for objectively assessing the ability of a software system to absorb future changes. In contrast to this, relational database theory has successfully solved very similar difficulties through normal forms. In this paper, we transfer the idea of normal forms to code. The approach taken is to introduce semantic dependencies as a foundation for the definition of code normal form criteria",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521198,no,undetermined,0
System Availability Analysis Considering Hardware/Software Failure Severities,"Model-based analysis is a well-established approach to assess the influence of several factors on system availability within the context of system structure. Prevalent availability models in the literature consider all failures to be equivalent in terms of their consequences on system services. In other words, all the failures are assumed to be of the same level of severity. In practice, failures are typically classified into multiple severity levels, where failures belonging to the highest severity level cause a complete loss of service, while failures belonging to levels below the highest level enable the system to operate in a degraded mode. This makes it necessary to consider the influence of failure severities on system availability. In this paper we present a Markov model which considers failure severities of the components of the system in conjunction with its structure. The model also incorporates the repair of the components. Based on the model, we derive a closed form expression which relates system availability to the failure and repair parameters of the components. The failure parameters in the model are estimated based on the data collected during acceptance testing of a satellite system. However, since adequate data are not available to estimate the repair parameters, the closed form expressions are used to assess the sensitivity of the system availability to the repair parameters",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521193,no,undetermined,0
"Is My Software """"Good Enough"""" to Release? - A Probabilistic Assessment","We present the basics of a probabilistic methodology to assess the overall quality of software preparatory to its release through the evaluation of process and product evidence, the """" 'good enough' to release"""" (GETR) methodology, in this paper. GETR methodology has three main elements: a model whose elements represent activities and artifacts identified in the literature as being effective assessors of software quality, a process for populating certain parts of the model, and methods for analyzing the importance of contributions made by individual evidence to the determination of overall system quality. First, the methodology's components are briefly introduced. A demonstration of how the methodology can be applied is then given through two case studies reviewing release assessments for in-house developed analytical tools. The robustness of the model is also illustrated by the results of the case studies",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521189,no,undetermined,0
Research on double-end cooperative congestion control mechanism of random delay network,"Propagation delay is the main factor of affecting the network performance. Random propagation delay has an adverse impact on the stability of feedback control mechanism. We argue that some existing schemes which try to control the node-end-systems queue donpsilat work well when random propagation delay acts on the models. To find a solution to this problem, a double-end cooperative congestion control mechanism is proposed and analyzed. We have studied the performance of the control mechanism via simulations on OPNET software. Simulations show that the mechanism can improve network performance under random propagation delay. And in the node-end-systems, the probability of cells discard is lesser.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594176,no,undetermined,0
Stroke detection and reconstruction of characters pressed on metal label,"In order to detect features of protuberant characters, a novel stroke detection method based on Gabor filters is proposed. First, the gray images of protuberant characters were preprocessed using morphological algorithm. Next, a set of Gabor filters is used to break down an image of protuberant characters into four directional images, which contain the stroke information of four directions. Then, a reconstruction experiment is carried out with the Gabor characters. The results show that the Gabor representation has strong reconstruction power. Finally, A BP neural network is introduced to classify the Gabor features and the experiment results tell that the Gabor features have good separate capability. All of the above proves that the proposed method can be reliably used for feature extraction of pressed characters in low-quality images.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594474,no,undetermined,0
Composition assessment metrics for CBSE,"The objective of this paper is the formal definition of composition assessment metrics for CBSE, using an extension of the CORBA component model metamodel as the ontology for describing component assemblies. The method used is the representation of a component assembly as an instantiation of the extended CORBA component model metamodel. The resulting meta-objects diagram can then be traversed using object constraint language clauses. These clauses are a formal and executable definition of the metrics that can be used to assess quality attributes from the assembly and its constituent components. The result is the formal definition of context-dependent metrics that cover the different composition mechanisms provided by the CORBA component model and can be used to compare alternative component assemblies; a metamodel extension to capture the topology of component assemblies. The conclusion is that providing a formal and executable definition of metrics for CORBA component assemblies is an enabling precondition to allow for independent scrutiny of such metrics which is, in turn, essential to increase practitioners confidence on predictable quality attributes.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517732,no,undetermined,0
An iterative hardware Gaussian noise generator,"The quality of generated Gaussian noise samples plays a crucial role when evaluating the bit error rate performance of communication systems. This paper presents a new approach for the field-programmable gate array (FPGA) realization of a high-quality Gaussian noise generator (GNG). The datapath of the GNG can be configured differently based on the required accuracy of the Gaussian probability density function (PDF). Since the GNG is often most conveniently implemented on the same FPGA as the design under evaluation, the area efficiency of the proposed GNG is important. For a particular configuration, the proposed design utilizes only 3% of the configurable slices and two on-chip block memories of a Virtex XC2V4000-6 FPGA to generate Gaussian samples within up to 6.55, where  is the standard deviation, and can operate at up to 132 MHz.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517373,no,undetermined,0
Predictive compression of dynamic 3D meshes,"An efficient algorithm for compression of dynamic time-consistent 3D meshes is presented. Such a sequence of meshes contains a large degree of temporal statistical dependencies that can be exploited for compression using DPCM. The vertex positions are predicted at the encoder from a previously decoded mesh. The difference vectors are further clustered in an octree approach. Only a representative for a cluster of difference vectors is further processed providing a significant reduction of data rate. The representatives are scaled and quantized and finally entropy coded using CABAC, the arithmetic coding technique used in H.264/MPEG4-AVC. The mesh is then reconstructed at the encoder for prediction of the next mesh. In our experiments we compare the efficiency of the proposed algorithm in terms of bit-rate and quality compared to static mesh coding and interpolator compression indicating a significant improvement in compression efficiency.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1529827,no,undetermined,0
Error concealment for slice group based multiple description video coding,"This paper develops error concealment methods for multiple description video coding (MDC) in order to adapt to error prone packet networks. The three-loop slice group MDC approach of D. Wang et al. (2005) is used. MDC is very suitable for multiple channel environments, and especially able to maintain acceptable quality when some of these channels fail completely, i.e. in an on-off MDC environment, without experiencing any drifting problem. Our MDC scheme coupled with the proposed concealment approaches proved to be suitable not only for the on-off MDC environment case (data from one channel fully lost), but also for the case where only some packets are lost from one or both channels. Copying video and using motion vectors from correct descriptions are combined together for concealment prior to applying traditional methods. Results are compared to the traditional error concealment method proposed in the H.264 reference software, showing significant improvements for both the balanced and unbalanced channel cases.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1529864,no,undetermined,0
CredEx: user-centric credential management for grid and Web services,"User authentication is a crucial security component for most computing systems. But since the security needs of different systems vary widely, authentication mechanisms are similarly diverse. In particular, independently-managed Web and grid services vary with regard to the type of security token (credential) used to prove user identity (username/password, X.509 signing, Kerberos, etc.). Forcing users to manage and present credentials manually for each service is tedious, error-prone and potentially insecure. In contrast, we present CredEx, an open-source, standards-based Web service that facilitates the secure storage of credentials and enables the dynamic exchange of different credential types using the WS-Trust token exchange protocol. With CredEx, a user can achieve single sign-on by acquiring a single (default) credential then dynamically exchanging that credential as needed for services that authenticate a different way. We describe the design and implementation of CredEx by focusing on its use in bridging password-based Web services and PKI-based grid services, illustrating how interoperability between these realms can be based upon the WS-Security and WS-Trust specifications.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1530793,no,undetermined,0
Modelling inter-organizational workflow security in a peer-to-peer environment,"The many conflicting technical, organizational, legal and domain-level constraints make the implementation of secure, inter-organizational workflows a very complex task, which is bound to low-level technical knowledge and error prone. The SECTINO project provides a framework for the realization and the high-level management of security-critical workflows based on the paradigm of model driven security. In our case the models are translated into runtime artefacts that configure a target reference architecture based on Web services technologies. In this paper we focus on the global workflow model, which captures the message exchange protocol between partners cooperating in a distributed environments well as basic security patterns. We show how the model maps to workflow and security components of the hosting environments at the partner nodes.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1530844,no,undetermined,0
A model of soft error effects in generic IP processors,"When designing reliability-aware digital circuits, either hardware or software techniques may be adopted to provide a certain degree of failure detection/tolerance, caused by either hardware faults or soft-errors. These techniques are quite well established when working at a low abstraction level, whereas are currently under investigation when moving to higher abstraction levels, in order to cope with the increasing complexity of the systems being designed. This paper presents a model of soft error effects to be adopted when defining software-only techniques to achieve fault detection capabilities. The work identifies on a generic IP processor the misbehaviors caused by soft errors, classifies and analyzes them with respect to the possibility of detecting them by means of previously published approaches. An experimental validation of the proposed model is carried out on the Leon2 processor.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544532,no,undetermined,0
A software-based concurrent error detection technique for power PC processor-based embedded systems,"This paper presents a behavior-based error detection technique called control flow checking using branch trace exceptions for powerPC processors family (CFCBTE). This technique is based on the branch trace exception feature available in the powerPC processors family for debugging purposes. This technique traces the target addresses of program branches at run-time and compares them with reference target addresses to detect possible violations caused by transient faults. The reference target addresses are derived by a preprocessor from the source program. The proposed technique is experimentally evaluated on a 32-bit powerPC microcontroller using software implemented fault injection (SWIFI). The results show that this technique detects about 91% of the injected control flow errors. The memory overhead is 39.16% on average, and the performance overhead varies between 110% and 304% depending on the workload used. This technique does not modify the program source code.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544525,no,undetermined,0
Analyzing BPEL Compositionality Based on Petri Nets,"Process of service composition is complex and error-prone, which makes a formal modeling and analysis method highly desirable. This paper presents a Petri net-based approach to analyzing the soundness and compositionality of services in BPEL. A set of translation rules is proposed to transform BPEL processes into Petri nets, by which behaviors of the BPEL processes are articulated. The instantiation net of target services are used to capture all of the possible implementation flows of composition processes. Based on theories of Petri nets, the principles for analyzing soundness and compositionality of Web services are provided. A detailed example is given to demonstrate the applicability of our method.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591584,no,undetermined,0
Metamodeling Autonomic System Management Policies - Ongoing Works,"Autonomic computing is recognized as one of the most promising solution to address the increasingly complex task of distributed environments' administration. In this context, many projects relied on software components and architectures to organize such an autonomic management software. However, we observed that the interfaces of a component model are too low-level, difficult to use and still error prone. Therefore, we introduced higher-level languages for the modeling of deployment and management policies. These domain specific languages enhance simplicity and consistency of the policies. Our current work is to formally describe the metamodels and the semantics associated with these languages.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591729,no,undetermined,0
Studying the fault-detection effectiveness of GUI test cases for rapidly evolving software,"Software is increasingly being developed/maintained by multiple, often geographically distributed developers working concurrently. Consequently, rapid-feedback-based quality assurance mechanisms such as daily builds and smoke regression tests, which help to detect and eliminate defects early during software development and maintenance, have become important. This paper addresses a major weakness of current smoke regression testing techniques, i.e., their inability to automatically (re)test graphical user interfaces (GUIs). Several contributions are made to the area of GUI smoke testing. First, the requirements for GUI smoke testing are identified and a GUI smoke test is formally defined as a specialized sequence of events. Second, a GUI smoke regression testing process called daily automated regression tester (DART) that automates GUI smoke testing is presented. Third, the interplay between several characteristics of GUI smoke test suites including their size, fault detection ability, and test oracles is empirically studied. The results show that: 1) the entire smoke testing process is feasible in terms of execution time, storage space, and manual effort, 2) smoke tests cannot cover certain parts of the application code, 3) having comprehensive test oracles may make up for not having long smoke test cases, and 4) using certain oracles can make up for not having large smoke test suites.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542069,no,undetermined,0
Checking inside the black box: regression testing by comparing value spectra,"Comparing behaviors of program versions has become an important task in software maintenance and regression testing. Black-box program outputs have been used to characterize program behaviors and they are compared over program versions in traditional regression testing. Program spectra have recently been proposed to characterize a program's behavior inside the black box. Comparing program spectra of program versions offers insights into the internal behavioral differences between versions. In this paper, we present a new class of program spectra, value spectra, that enriches the existing program spectra family. We compare the value spectra of a program's old version and new version to detect internal behavioral deviations in the new version. We use a deviation-propagation call tree to present the deviation details. Based on the deviation-propagation call tree, we propose two heuristics to locate deviation roots, which are program locations that trigger the behavioral deviations. We also use path spectra (previously proposed program spectra) to approximate the program states in value spectra. We then similarly compare path spectra to detect behavioral deviations and locate deviation roots in the new version. We have conducted an experiment on eight C programs to evaluate our spectra-comparison approach. The results show that both value-spectra-comparison and path-spectra-comparison approaches can effectively expose program behavioral differences between program versions even when their program outputs are the same, and our value-spectra-comparison approach reports deviation roots with high accuracy for most programs.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542068,no,undetermined,0
Refactoring the aspectizable interfaces: an empirical assessment,"Aspect oriented programming aims at addressing the problem of the crosscutting concerns, i.e., those functionalities that are scattered among several modules in a given system. Aspects can be defined to modularize such concerns. In this work, we focus on a specific kind of crosscutting concerns, the scattered implementation of methods declared by interfaces that do not belong to the principal decomposition. We call such interfaces aspectizable. All the aspectizable interfaces identified within a large number of classes from the Java Standard Library and from three Java applications have been automatically migrated to aspects. To assess the effects of the migration on the internal and external quality attributes of these systems, we collected a set of metrics and we conducted an empirical study, in which some maintenance tasks were executed on the two alternative versions (with and without aspects) of the same system. In this paper, we report the results of such a comparison.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542065,no,undetermined,0
OBDD-based evaluation of reliability and importance measures for multistate systems subject to imperfect fault coverage,"Algorithms for evaluating the reliability of a complex system such as a multistate fault-tolerant computer system have become more important. They are designed to obtain the complete results quickly and accurately even when there exist a number of dependencies such as shared loads (reconfiguration), degradation, and common-cause failures. This paper presents an efficient method based on ordered binary decision diagram (OBDD) for evaluating the multistate system reliability and the Griffith's importance measures which can be regarded as the importance of a system-component state of a multistate system subject to imperfect fault-coverage with various performance requirements. This method combined with the conditional probability methods can handle the dependencies among the combinatorial performance requirements of system modules and find solutions for multistate imperfect coverage model. The main advantage of the method is that its time complexity is equivalent to that of the methods for perfect coverage model and it is very helpful for the optimal design of a multistate fault-tolerant system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542055,no,undetermined,0
Determining how much software assurance is enough? A value-based approach,"A classical problem facing many software projects is how to determine when to stop testing and release the product for use. On the one hand, we have found that risk analysis helps to address such """"how much is enough?"""" questions, by balancing the risk exposure of doing too little with the risk exposure of doing too much. In some cases, it is difficult to quantify the relative probabilities and sizes of loss in order to provide practical approaches for determining a risk-balanced """"sweet spot"""" operating point. However, we have found some particular project situations in which tradeoff analysis helps to address such questions. In this paper, we provide a quantitative approach based on the COCOMO II cost estimation model and the COQUALMO qualify estimation model. We also provide examples of its use under the differing value profiles characterizing early startups, routine business operations, and high-finance operations in marketplace competition situation. We also show how the model and approach can assess the relative payoff of value-based testing compared to value-neutral testing based on some empirical results. Furthermore, we propose a way to perform cost/schedule/reliability tradeoff analysis using COCOMO II to determine the appropriate software assurance level in order to finish the project on time or within budget.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541826,no,undetermined,0
Towards a Process Maturity Model for Open Source Software,"For traditional software development, process maturity models (CMMI, SPICE) have long been used to assess product quality and project predictability. For OSS, on the other hand, these models are generally perceived as inadequate. In practice, though, many OSS communities are well-organized, and there is evidence of process maturity in OSS projects. This position paper presents work in progress on developing a process maturity model specifically for OSS projects. 1.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591752,no,undetermined,0
System test case prioritization of new and regression test cases,"Test case prioritization techniques have been shown to be beneficial for improving regression-testing activities. With prioritization, the rate of fault detection is improved, thus allowing testers to detect faults earlier in the system-testing phase. Most of the prioritization techniques to date have been code coverage-based. These techniques may treat all faults equally. We build upon prior test case prioritization research with two main goals: (1) to improve user-perceived software quality in a cost effective way by considering potential defect severity and (2) to improve the rate of detection of severe faults during system-level testing of new code and regression testing of existing code. We present a value-driven approach to system-level test case prioritization called the prioritization of requirements for test (PORT). PORT prioritizes system test cases based upon four factors: requirements volatility, customer priority, implementation complexity, and fault proneness of the requirements. We conducted a PORT case study on four projects developed by students in advanced graduate software testing class. Our results show that PORT prioritization at the system level improves the rate of detection of severe faults. Additionally, customer priority was shown to be one of the most important prioritization factors contributing to the improved rate of fault detection.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541815,no,undetermined,0
"Multispectral multidimensional multiplexed data: the more, the merrier","The ability to detect multiple molecular species at once is becoming increasingly important. Multispectral imaging systems can be used to capture multiplexed molecular signals, and can be applied to the analysis of chromogenically stained slides in brightfield mode and of samples stained with a variety of light-emitting dyes (from the visible to the NIR range) in fluorescence mode. Quantum dots make a particularly good match with this imaging technology, which is also extremely helpful for the identification and elimination of interfering autofluorescence. The ability to accurately determine the spectral qualities of dyes in-situ is also valuable. Multispectral imaging has proven to be useful for multicolor FISH, for resolving multiple species of GFP with overlapping emission spectra and for resolving red/brown double-labeled histopathology stains. The uses of spectral imaging in clinical pathology are still being explored and need to be matched to appropriate software tools. Appropriately constrained linear unmixing algorithms and novel automated tools have recently been developed to provide simple, accurate analysis procedures. Conventional hematoxylin-and-eosin-or Papanicolaou-stained pathology sections can have sufficient spectral content to allow the classification of cells of different lineage or to separate normal from neoplastic cells. Analysis of such specimens may succeed using spectral """"signatures"""" and simple segmentation algorithms. The rich data sets also reward the use of more advanced analysis techniques. These can include a number of approaches pioneered for remote sensing purposes, such as spectral similarity mapping, automated clustering algorithms in n dimensions, principal component analysis, as well as other more sophisticated techniques.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1540641,no,undetermined,0
ssahaSNP - a polymorphism detection tool on a whole genome scale,"We present a software package which can detect homozygous SNPs and indels on a eukaryotic genome scale from millions of shotgun reads. Matching seeds of a few kmer words are found to locate the position of the read on the genome. Full sequence alignment is performed to detect base variations. Quality values of both variation bases and neighbouring bases are checked to exclude possible sequence base errors. To analyze polymorphism level in the genome, we used the package to detect indels from 20 million WGS reads against the draft WGS assembly. From the dataset, we detected a total number of 663,660 indels, giving an estimated average indel density at about one indel every 2.48 kilobases. Distribution of indels length and variation of indel mapped times are also analyzed.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1540618,no,undetermined,0
Distributed integrity checking for systems with replicated data,"This work presents a new comparison-based diagnosis model and a new algorithm, called Hi-Dif, based on this model. The algorithm is used for checking the integrity of systems with replicated data, for instance, detecting unauthorized Web page modifications. Fault-free nodes running Hi-Dif send a task to two other nodes and the task results are compared. If the comparison produces a match, the two nodes are classified in the same set. On the other hand, if the comparison results in a mismatch, the two nodes are classified in different sets, according to their task results. One of the sets always contains all fault-free nodes. One fundamental difference of the proposed model to previously published models is that the new model allows the task outputs of two faulty nodes to be equal to each other. Considering a system of N nodes, we prove that the algorithm has latency equal to log<sub>2</sub>N testing rounds in the worst case; that the maximum number of tests required is O(N<sup>2</sup>); and, that the algorithm is (N-1)-diagnosable. Experimental results obtained by simulation and by the execution of a tool implemented applied to the Web are presented.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531151,no,undetermined,0
Improving the Quality of GNU/Linux Distributions,"The widespread adoption of free and open source software (FOSS) has lead to a freer and more agile marketplace where there is a higher number of components that can be used to build systems in many original and often unforeseen ways. One of the most prominent examples of complex systems built with FOSS components are GNU/Linux-based distributions. In this paper we present some tools that aim at helping distribution editors with maintaining the huge package bases associated with these distributions, and improving their quality, by detecting errors and inconsistencies in an effective, fast and automatic way.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591758,no,undetermined,0
Extending ATAM to assess product line architecture,"Software architecture is a core asset for any organization that develops software-intensive systems. Unsuitable architecture can precipitate disaster because the architecture determines the structure of the project. To prevent this, software architecture must be evaluated. The current evaluation methods, however, focus on single product architecture, and not product line architectures and they hardly consider the characteristics of the product lines, such as the variation points. This paper describes the extension of a scenario-based analysis technique for software product architecture-called EATAM, which not only analyzes the variation points of the quality attribute using feature modeling but also creates variability scenarios for the derivation of the variation points using the extended PLUC tag approach. This is a method that aims to consider the tradeoffs in the variability scenarios of the software product family architecture. The method has been validated through a case study involving microwave oven software product line in the appliance domain.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594775,no,undetermined,0
A new priority based congestion control protocol for Wireless Multimedia Sensor Networks,"New applications made possible by the rapid improvements and miniaturization in hardware has motivated recent developments in wireless multimedia sensor networks (WMSNs). As multimedia applications produce high volumes of data which require high transmission rates, multimedia traffic is usually high speed. This may cause congestion in the sensor nodes, leading to impairments in the quality of service (QoS) of multimedia applications. Thus, to meet the QoS requirements of multimedia applications, a reliable and fair transport protocol is mandatory. An important function of the transport layer in WMSNs is congestion control. In this paper, we present a new queue based congestion control protocol with priority support (QCCP-PS), using the queue length as an indication of congestion degree. The rate assignment to each traffic source is based on its priority index as well as its current congestion degree. Simulation results show that the proposed QCCP-PS protocol can detect congestion better than previous mechanisms. Furthermore it has a good achieved priority close to the ideal and near-zero packet loss probability, which make it an efficient congestion control protocol for multimedia traffic in WMSNs. As congestion wastes the scarce energy due to a large number of retransmissions and packet drops, the proposed QCCP-PS protocol can save energy at each node, given the reduced number of retransmissions and packet losses.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594816,no,undetermined,0
An integrated approach for increasing the soft-error detection capabilities in SoCs processors,"Software implemented hardware fault tolerance (SIHFT) techniques are able to detect most of the transient and permanent faults during the usual system operations. However, these techniques are not capable to detect some transient faults affecting processor memory elements such as state registers inside the processor control unit, or temporary registers inside the arithmetic and logic unit. In this paper, we propose an integrated (hardware and software) approach to increase the fault detection capabilities of software techniques by introducing a limited hardware redundancy. Experimental results are reported showing the effectiveness of the proposed approach in covering soft-errors affecting the processor memory elements and escaping to purely software approaches.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544544,no,undetermined,0
Scheduling tasks with Markov-chain based constraints,"Markov-chain (MC) based constraints have been shown to be an effective QoS measure for a class of real-time systems, particularly those arising from control applications. Scheduling tasks with MC constraints introduces new challenges because these constraints require not only specific task finishing patterns but also certain task completion probability. Multiple tasks with different MC constraints competing for the same resource further complicates the problem. In this paper, we study the problem of scheduling multiple tasks with different MC constraints. We present two scheduling approaches which (i) lead to improvements in """"overall"""" system performance, and (ii) allow the system to achieve graceful degradation as system load increases. The two scheduling approaches differ in their complexities and performances. We have implemented our scheduling algorithms in the QNX real-time operating system environment and used the setup for several realistic control tasks. Data collected from the experiments as well as simulation all show that our new scheduling algorithms outperform algorithms designed for window-based constraints as well as previous algorithms designed for handling MC constraints.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508457,no,undetermined,0
ZenFlow: a visual Web service composition tool for BPEL4WS,"Web services have become a very powerful technology to build service oriented architectures and standardize the access to legacy services. Through Web service composition new added value Web services can be created out of existing ones. Examples of these compositions are virtual organizations, outsourcing, enterprise application integration, business process definitions and business to business inter/intra-enterprise relationships. In order to enable the construction of business processes as composite Web services, a number of composition languages has been proposed by the software industry. However, the handiwork of specifying a business process with these languages through simple text or XML editors is tough, complex and error prone. Visual support can ease the definition of business processes. In this paper, we describe ZenFlow, a visual composition tool for Web services written in BPEL4WS. ZenFlow provides several visual facilities to ease the definition of a business process such as multiple views of a process, syntactic and semantic awareness, filtering, logical zooming capabilities and hierarchical representations.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509502,no,undetermined,0
SDTV Quality Assessment Using Energy Distribution of DCT Coefficients,"The VQM (Video Quality Measurement) scheme is a methodology that measures the difference of quality between the distorted video signal and the reference video signal. In this paper, we propose a novel video quality measurement method that extracts features in DCT (Discrete Cosine Transform) domain of H.263 SDTV. Main idea of the proposed method is to utilize the texture pattern and edge oriented information that is generated in DCT domain. For this purpose, the energy distribution of the reodered DCT coefficients is considered to obtain unique information of each video file. Then, we measure the difference of probability distribution of context information between original video and distorded one. The simulation results show that the proposed algorithm can represent correctly the video quality and give a high correlation with the video DMOS.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595547,no,undetermined,0
Measurement-driven dashboards enable leading indicators for requirements and design of large-scale systems,"Measurement-driven dashboards provide a unifying mechanism for understanding, evaluating, and predicting the development, management, and economics of large-scale systems and processes. Dashboards enable interactive graphical displays of complex information and support flexible analytic capabilities for user customizability and extensibility. Dashboards commonly include software requirements and design metrics because they provide leading indicators for project size, growth, and stability. This paper focuses on dashboards that have been used on actual large-scale projects as well as example empirical relationships revealed by the dashboards. The empirical results focus on leading indicators for requirements and design of large-scale systems. In the first set of 14 projects focusing on requirements metrics, the ratio of software requirements to source-lines-of code averaged 1:46. Projects that far exceeded the 1:46 requirements-to-code ratio tended to be more effort-intensive and fault-prone during verification. In the second set of 16 projects focusing on design metrics, the components in the top quartile of the number of component internal states had 6.2 times more faults on average than did the components in the bottom quartile, after normalization by size. The components in the top quartile of the number of component interactions had 4.3 times more faults on average than did the components in the bottom quartile, after normalization by size. When the number of component internal states was in the bottom quartile, the component fault-proneness was low even when the number of component interactions was in the upper quartiles, regardless of size normalization. Measurement-driven dashboards reveal insights that increase visibility into large-scale systems and provide feedback to organizations and projects",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509300,no,undetermined,0
An industrial case study of implementing and validating defect classification for process improvement and quality management,"Defect measurement plays a crucial role when assessing quality assurance processes such as inspections and testing. To systematically combine these processes in the context of an integrated quality assurance strategy, measurement must provide empirical evidence on how effective these processes are and which types of defects are detected by which quality assurance process. Typically, defect classification schemes, such as ODC or the Hewlett-Packard scheme, are used to measure defects for this purpose. However, we found it difficult to transfer existing schemes to an embedded software context, where specific document- and defect types have to be considered. This paper presents an approach to define, introduce, and validate a customized defect classification scheme that considers the specifics of an industrial environment. The core of the approach is to combine the software engineering know-how of measurement experts and the domain know-how of developers. In addition to the approach, we present the results and experiences of using the approach in an industrial setting. The results indicate that our approach results in a defect classification scheme that allows classifying defects with good reliability, that allows identifying process improvement actions, and that can serve as a baseline for evaluating the impact of process improvements",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509297,no,undetermined,0
Assessing the impact of coupling on the understandability and modifiability of OCL expressions within UML/OCL combined models,"Diagram-based UML notation is limited in its expressiveness thus producing a model that would be severely underspecified. The flaws in the limitation of the UML diagrams are solved by specifying UML/OCL combined models, OCL being an essential add-on to the UML diagrams. Aware of the importance of building precise models, the main goal of this paper is to carefully describe a family of experiments we have undertaken to ascertain whether any relationship exists between object coupling (defined through metrics related to navigations and collection operations) and two maintainability sub-characteristics: understandability and modifiability of OCL expressions. If such a relationship exists, we will have found early indicators of the understandability and modifiability of OCL expressions. Even though the results obtained show empirical evidence that such a relationship exists, they must be considered as preliminaries. Further validation is needed to be performed to strengthen the conclusions and external validity",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509292,no,undetermined,0
Visualization of self-stabilizing distributed algorithms,"In this paper, we present a method to build an homogeneous and interactive visualization of self-stabilizing distributed algorithms using Visidia platform. The approach developed in this work allows to simulate the transient failures and their correction mechanism. We use local computations to encode self-stabilizing algorithms like the distributed algorithms implemented in Visidia. The resulting interface is able to select some processes and incorrectly change their states to show the transient failures. The system detects and corrects these transient failures by applying correction rules. Many examples of self-stabilizing distributed algorithms are implemented.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509129,no,undetermined,0
A BIST approach for testing FPGAs using JBITS,"This paper explores the built-in self test (BIST) concepts to test the configurable logic blocks (CLBs) of static RAM (SRAM) based FPGAs using Java Bits (JBits). The proposed technique detects and diagnoses single and multiple stuck-at faults in the CLBs while significantly reducing the time taken to perform the testing. Previous BIST approaches for testing FPGAs use traditional CAD tools which lack control over configurable resources, resulting in the design being placed on the hardware in a different way than intended by the designer. In this paper, the design of the logic BIST architecture is done using JBits 2.8 software for Xilinx Virtex family of devices. The test requires seven configurations and two test sessions to test the CLBs. The time taken to generate the entire BIST logic in both the sessions is approximately 77 seconds as compared with several minutes to hours in traditional design flow.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508546,no,undetermined,0
Multiple views to support engineering change management for complex products,"Unforeseen change propagation can have a major impact on products and design processes and cause project delays and excessive costs. However, current change management depends heavily on individual designers' typically limited product overview. For complex products, this approach is error-prone because the amount of data that is necessary to properly assess the risk of changes is too large. The information has to be broken down into smaller chunks so that it is easier to cope with. On the other hand, an overview over the entire product must be provided in order to be able to predict changes resulting from changes in other components. In this paper, we discuss the CPM (change prediction method) tool that incorporates a multiple view strategy to visualise complex change data and allows designers to run what-if scenarios in order to assess the implications of changing components in a complex product during the design process.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508219,no,undetermined,0
Enhancing transparency and adaptability of dynamically changing execution environments for mobile agents,"Mobile agent-based distributed systems become obtaining significant popularity as a potential vehicle to allow software components to be executed on heterogeneous environments despite mobility of users and computations. However as these systems generally force mobile agents to use only common functionalities provided in every execution environment, the agents may not access environment-specific resources. In this paper, we propose a new framework using aspect oriented programming technique to accommodate a variety of static resources as well as dynamic ones whose amount is continually changed at runtime even in the same execution environment. Unlike previous works, this framework divides roles of software developers into three groups to relieve application programmers from the complex and error-prone parts of implementing dynamic adaptation and allowing each developer to only concentrate on his own part. Also, the framework enables policy decision makers to apply various adaptation policies to dynamically changing environments for adjusting mobile agents to the change of their resources.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515479,no,undetermined,0
On monitoring concurrent systems with TLA: an example,"We present an approach for producing oracles from TLA (temporal logic of action) specification of a system. Such oracles are useful, for monitoring purposes, to detect temporal faults by checking a running implementation of a system against a verified behavioral model. We use the Ben-Ari classical incremental garbage collection algorithm for illustration.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508128,no,undetermined,0
Using POMDP-based state estimation to enhance agent system survivability,"A survivable agent system depends on the incorporation of many recovery features. However, the optimal use of these recovery features requires the ability to assess the actual state of the agent system accurately at a given time. This paper describes an approach for the estimation of the state of an agent system using partially-observable Markov decision processes (POMDPs). POMDPs are dependent on a model of the agent system - components, environment, sensors, and the actuators that can correct problems. Based on this model, we define a state estimation for each component (asset) in the agent system. We model a survivable agent system as a POMDP that takes into account both environmental threats and observations from sensors. We describe the process of updating the state estimation as time passes, as sensor inputs are received, and as actuators affect changes. This state estimation process has been deployed within the Ultralog application and successfully tested using Ultralog's survivability tests on a full-scale (1000+) agent system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1507043,no,undetermined,0
Empirical case studies in attribute noise detection,"The problem of determining the noisiest attribute(s) from a set of domain-specific attributes is of practical importance to domain experts and the data mining community. Data noise is generally of two types: attribute noise and mislabeling errors (class noise). For a given domain-specific dataset, attributes that contain a significant amount of noise can have a detrimental impact on the success of a data mining initiative, e.g., reducing the predictive ability of a classifier in a supervised learning task. Techniques that provide information about the noise quality of an attribute are useful tools for a data mining practitioner when performing analysis on a dataset or scrutinizing the data collection processes. Our technique for detecting noisy attributes uses an algorithm that we recently proposed for the detection of instances with attribute noise. This paper presents case studies that confirm our recent work done on detecting noisy attributes and further validates that our technique is indeed able to detect attributes that contain noise.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1506475,no,undetermined,0
Evaluating the Effectiveness of Random and Partition Testing by Delivered Reliability,"The software engineering literature is full of test data selection and adequacy strategies. However, It is still a question whether these adequacy strategies are effective or not. So it is necessary to research how to evaluate the effectiveness of test strategy. The effectiveness of random and subdomain testing methodology is normally evaluated by failure detecting ability. However, detecting more failures does not guarantee that the software is more reliable because those failures detected maybe small and subtle ones that will seldom occur in reality. So in this paper, delivered reliability which presents the reliability of software after testing is introduced to evaluate their effectiveness. The better method delivers higher reliability after all test failures have been eliminated.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595602,no,undetermined,0
Towards Embedded Artificial Intelligence Based Security for Computer Systems,"This paper presents experiments using Artificial Intelligence (AI) algorithms for online monitoring of integrated computer systems, including System-on-Chip based embedded systems. This new framework introduces an AI-lead infrastructure that is intended to operate in parallel with conventional monitoring and diagnosis techniques. Specifically, an initial application is presented, where each of the systempsilas software tasks are characterised online during their execution by a combination of novel hardware monitoring circuits and background software. These characteristics then stimulate a Self-Organising Map based classifier which is used to detect abnormal system behaviour, as caused by failure and malicious tampering including viruses. The approach provides a system-level perspective and is shown to detect subtle anomalies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595800,no,undetermined,0
A study on the application of digital cameras to derive audio information from the TVs on trains,"Digital camera can help people take videos anytime with ease. For this reason, application of digital video camera's technology is the hottest topic right now. Due to the fact that the TV on the train is in a public place it can't give audio information. We were trying to find a way to get the audio information by second time (ST) detection video only. """"Second time"""" (ST) means the user uses their digital camera to take video from the original video. We propose a new way to use digital watermarking by embedding some pointer symbols into the source video, so passengers can use this kind of video to detect high quality audio in ST situations. In order to show other possible application for this process in the future, in this study we also use VB programming to design simulation software """"DDAS"""" (directly detection audio system). The software was designed to handle uncompressed AVl files. We tested this software using ST video. In the Experiments of this software successfully detected audio information and audio file types covering AVI, MPEG, MIDI and WAV file type. According to the questionnaires from the users, DDAS system's output audio file was given a 3.8 MOS level which shows the system has enough audio embedded ability. From the questionnaires, we also found ST video's screen size affects the detected audio's MOS quality. We found the best MOS quality was located in 1:11:4/3 screen size. We also use this kind of technology for study and we found that using multimedia improves the student's grades and the grades about 6 average grades.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1502362,no,undetermined,0
Development of customized distribution automation system (DAS) for secure fault isolation in low voltage distribution system,"This paper presents the development of customized distribution automation system (DAS) for secure fault isolation at the low voltage (LV) down stream, 415/240 V by using the Tenaga Nasional Berhad (TNB) distribution system. It is the first DAS research work done on customer side substation for operating and controlling between the consumer side system and the substation in an automated manner. Most of the work is focused on developing very secure fault isolation whereby the fault is detected, identified, isolated and remedied in few seconds. Supervisory Control and Data Acquisition (SCADA) techniques has been utilized to build Human Machine Interface (HMI) that provides a graphical operator interface functions to monitor and control the system. Microprocessor based Remote Monitoring Devices have been used for customized software to be downloaded to the hardware. Power Line Carrier (PLC) has been used as communication media between the consumer and the substation. As result, complete DAS fault isolation system has been developed for cost reduction, maintenance time saving and less human intervention during faults.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595984,no,undetermined,0
A seamless handoff approach of mobile IP based on dual-link,"Mobile IP protocol solves the problem of mobility support for hosts connected to Internet anytime and anywhere, and makes the mobility transparent to the higher layer applications. But the handoff latency in Mobile IP affects the quality of communication. This paper proposes a seamless handoff approach based on dual-link and link layer trigger, using information from link layer to predict and trigger the dual-link handoff. During the handoff, MN keeps one link connected with the current network while it handoff another link to the new network. In this paper we develop a model system based on this approach and provide experiments to evaluate the performance of this approach. The experimental results show that this approach can ensure the seamless handoff of Mobile IP.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509638,no,undetermined,0
A software tool to relate technical performance to user experience in a mobile context,"Users in todaypsilas mobile ICT environment are confronted with more and more innovations and an ever increasing technical quality, which makes them more demanding and harder to please. It is often hard to measure and to predict the user experience during service consumption. This is nevertheless a very important dimension that should be taken into account while developing applications or frameworks. In this paper we demonstrate a software tool that is integrated in a wireless living lab environment in order to validate and quantify actual user experience. The methodology to assess the user experience combines both technological and social assets. User experience of a Wineguide application on a PDA is related to signal strength, monitored during usage of the applications. Higher signal strengths correspond with a better experience (e.g. speed). Finally, difference in the experience among users will be discussed.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594902,no,undetermined,0
Power transmission control using distributed max-flow,Existing maximum flow algorithms use one processor for all calculations or one processor per vertex in a graph to calculate the maximum possible flow through a graph's vertices. This is not suitable for practical implementation. We extend the max-flow work of Goldberg and Tarjan to a distributed algorithm to calculate maximum flow where the number of processors is less than the number of vertices in a graph. Our algorithm is applied to maximizing electrical flow within a power network where the power grid is modeled as a graph. Error detection measures are included to detect problems in a simulated power network. We show that our algorithm is successful in executing quickly enough to prevent catastrophic power outages.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510028,no,undetermined,0
Gompertz software reliability model and its application,"In this article, we propose a stochastic model called the Gompertz software reliability model based on the familiar non-homogeneous Poisson process. It is shown that the proposed model can be derived from the well-known statistical theory of extreme-value and has the quite similar asymptotic property to the classical Gompertz curve. In a numerical example with the software failure data observed in a real software development project, we apply the Gompertz software reliability model to assess the software reliability and to predict the number of initial fault contents. We empirically conclude that our new model may function better than the existing models and is attractive in terms of goodness-of-fit test based on information criteria and mean squared error.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510058,no,undetermined,0
Two approaches for the improvement in testability of communication protocols,"Protocols have grown larger and more complex with the advent of computer and communication technologies. As a result, the task of conformance testing of protocol implementation has also become more complex. The study of design for testability (DFT) is a research area in which researchers investigate design principles that will help to overcome the ever increasing complexity of testing distributed systems. Testability metrics are essential for evaluating and comparing designs. In a previous paper, we introduce a new metric for testability of communication protocols, based on the detection probability of a default. We demonstrate the usefulness of the metric for identifying faults there are more difficult to detect. In this paper, we present two approaches for improved testing of a protocol implementation once those faults that are difficult to detect are identified.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515464,no,undetermined,0
A reflective practice of automated and manual code reviews for a studio project,"In this paper, the target of code review is project management system (PMS), developed by a studio project in a software engineering master's program, and the focus is on finding defects not only in view of development standards, i.e., design rule and naming rule, but also in view of quality attributes of PMS, i.e., performance and security. From the review results, a few lessons are learned. First, defects which had not been found in the test stage of PMS development could be detected in this code review. These are hidden defects that affect system quality and that are difficult to find in the test. If the defects found in this code review had been fixed before the test stage of PMS development, productivity and quality enhancement of the project would have been improved. Second, manual review takes much longer than an automated one. In this code review, general check items were checked by automation tool, while project-specific ones were checked by manual method. If project-specific check items could also be checked by automation tool, code review and verification work after fixing the defects would be conducted very efficiently. Reflecting on this idea, an evolution model of code review is studied, which eventually seeks fully automated review as an optimized code review.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1515372,no,undetermined,0
Conflict classification and analysis of distributed firewall policies,"Firewalls are core elements in network security. However, managing firewall rules, particularly, in multifirewall enterprise networks, has become a complex and error-prone task. Firewall filtering rules have to be written, ordered, and distributed carefully in order to avoid firewall policy anomalies that might cause network vulnerability. Therefore, inserting or modifying filtering rules in any firewall requires thorough intrafirewall and interfirewall analysis to determine the proper rule placement and ordering in the firewalls. In this paper, we identify all anomalies that could exist in a single- or multifirewall environment. We also present a set of techniques and algorithms to automatically discover policy anomalies in centralized and distributed firewalls. These techniques are implemented in a software tool called the """"Firewall Policy Advisor"""" that simplifies the management of filtering rules and maintains the security of next-generation firewalls.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1514536,no,undetermined,0
Analysis of a method for improving video quality in the Internet with inclusion of redundant essential video data,"This paper presents a variant of a method for combating the burst-loss problem of multicast video packets. This variant uses redundant essential video data to offer biased protection towards all critical video information from loss, especially from contiguous loss when a burst loss of multiple consecutive packets occurs. Simulation results indicate that inclusion of redundant critical video data improves performance of our previously proposed method for solving the burst-loss problem. Furthermore, probability models, created for evaluating the effectiveness of the proposed method and its variant, give results which are consistent with those obtained from simulations.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512871,no,undetermined,0
Applications of Global Positioning System (GPS) in geodynamics: with three examples from Turkey,"Global Positioning System (GPS) has been very useful tool for the last two decades in the area of geodynamics because or the validation of the GPS results by the Very Long Baseline Interferometry (VLBI) and Satellite Laser Ranging (SLR) observations. The modest budget requirement and the high accuracy relative positioning availability of GPS increased the use of it in determination of crustal and/or regional deformations. Since the civilian use the GPS began in 1980, the development on the receiver and antenna technology with the ease of use software packages reached to a well known state, which may be named as a revolution in the Earth Sciences among other application fields. Analysis of a GPS network can also give unknown information about the fault lines that can not be seen from the ground surface. Having information about the strain accumulation along the fault line may allow us to evaluate future probabilities of regional earthquake hazards and develop earthquake scenarios for specific faults. In this study, the use of GPS in geodynamical studies will be outlined throughout the instrumentation, the measurements, and the methods utilized. The preliminary results of three projects, sponsored by the Scientific & Technical Research Council of Turkey (TUBITAK) and Istanbul Technical University (ITU) which have been carried out in Turkey using GPS will be summarized. The projects are mainly aimed to determine the movements along the fault zones. Two of the projects have been implemented along the North Anatolian Fault Zone. (NAFZ), one is in Mid-Anatolia region, and the ther is in Western Marmara region. The third project has been carried out in the Fethiye-Burdur region. The collected GPS data were processed by the GAMIT/GLOBK software The results are represented as velocity vectors obtained using the yearly combinations of the daily measured GPS data.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512597,no,undetermined,0
Detecting application-level failures in component-based Internet services,"Most Internet services (e-commerce, search engines, etc.) suffer faults. Quickly detecting these faults can be the largest bottleneck in improving availability of the system. We present Pinpoint, a methodology for automating fault detection in Internet services by: 1) observing low-level internal structural behaviors of the service; 2) modeling the majority behavior of the system as correct; and 3) detecting anomalies in these behaviors as possible symptoms of failures. Without requiring any a priori application-specific information, Pinpoint correctly detected 89%-96% of major failures in our experiments, as compared with 20%-70% detected by current application-generic techniques.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510707,no,undetermined,0
Scalable architecture for context-aware activity-detecting mobile recommendation systems,"One of the main challenges in building multi-user mobile information systems for real-world deployment lies in the development of scalable systems. Recent work on scaling infrastructure for conventional web services using distributed approaches can be applied to the mobile space, but limitations inherent to mobile devices (computational power, battery life) and their communication infrastructure (availability and quality of network connectivity) challenge system designers to carefully design and optimize their software architectures. Additionally, notions of mobility and position in space, unique to mobile systems, provide interesting directions for the segmentation and scalability of mobile information systems. In this paper we describe the implementation of a mobile recommender system for leisure activities, codenamed Magitti, which was built for commercial deployment under stringent scalability requirements. We present concrete solutions addressing these scalability challenges, with the goal of informing the design of future mobile multi-user systems.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4594884,no,undetermined,0
Utilization of extended firewall for object-oriented regression testing,"A testing firewall involves identifying various components that are dependent upon changed elements, but are just one level away from the modified elements. This paper investigates situations when data flow paths are longer, and the mechanism of thorough testing of components one level away from the changed elements may not detect certain regression faults caused by the change; this research has led to the notion of an extended firewall that takes these longer data paths into account. Empirical studies are reported that show the extent to which an extended firewall can detect more faults and how much more testing is required to achieve this increased detection.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510176,no,undetermined,0
Facilitating the implementation and evolution of business rules,"Many software systems implement, amongst other things, a collection of business rules. However, the process of evolving the business rules associated with a system is both time consuming and error prone. In this paper, we propose a novel approach to facilitating business rule evolution through capturing information to assist with the evolution of rules at the point of implementation. We analyse the process of rule evolution, in order to determine the information that must be captured. Our approach allows programmers to implement rules by embedding them into application programs (giving the required performance and genericity), while still easing the problems of evolution.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510156,no,undetermined,0
An empirical study of software maintenance of a Web-based Java application,"This paper presents empirical study detailing software maintenance for Web-based Java applications to aid in understanding and predicting the software maintenance category and effort. The specific application described is a Java Web-based administrative application in the e-Government arena. The application is based on the design of an open source model-view-controller framework. The domain factors, which also provide contextual references, are described based on Kitchenham et al.'s software maintenance ontology. This paper characterizes the number of fault reports and maintenance effort for each maintenance category. This study finds that the distribution of software maintenance effort in this observed Web-based Java application is similar to the distribution in previous software maintenance studies, which analyzed non object-oriented and non Web-based applications.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510151,no,undetermined,0
Test prioritization using system models,"During regression testing, a modified system is retested using the existing test suite. Because the size of the test suite may be very large, testers are interested in detecting faults in the system as early as possible during the retesting process. Test prioritization tries to order test cases for execution so the chances of early detection of faults during retesting are increased. The existing prioritization methods are based on the code of the system. System modeling is a widely used technique to model state-based systems. In this paper, we present methods of test prioritization based on state-based models after changes to the model and the system. The model is executed for the test suite and information about model execution is used to prioritize tests. Execution of the model is inexpensive as compared to execution of the system; therefore the overhead associated with test prioritization is relatively small. In addition, we present an analytical framework for evaluation of test prioritization methods. This framework may reduce the cost of evaluation as compared to the existing evaluation framework that is based on experimentation (observation). We have performed an experimental study in which we compared different test prioritization methods. The results of the experimental study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510150,no,undetermined,0
Optimizing test to reduce maintenance,"A software package evolves in time through various maintenance release steps whose effectiveness depends mainly on the number of faults left in the modules. Software testing is one of the most demanding and crucial phases to discover and reduce faults. In real environment, time available to test a software release is a given finite quantity. The purpose of this paper is to identify a criterion to estimate an efficient time repartition among software modules to enhance fault location in testing phase and to reduce corrective maintenance. The fundamental idea is to relate testing time to predicted risk level of the modules in the release under test. In our previous work we analyzed several kinds of risk prediction factors and their relationship with faults; moreover, we thoroughly investigated the behavior of faults on each module through releases to find significant fault proneness tendencies. Starting from these two lines of analysis, in this paper we propose a new approach to optimize the use of available testing time in a software release. We tuned and tested our hypotheses on a large industrial environment.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510141,no,undetermined,0
A controlled experiment assessing test case prioritization techniques via mutation faults,"Regression testing is an important part of software maintenance, but it can also be very expensive. To reduce this expense, software testers may prioritize their test cases so that those that are more important are run earlier in the regression testing process. Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited to hand-seeded faults, primarily due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults. We have therefore designed and performed a controlled experiment to assess the ability of prioritization techniques to improve the rate of fault detection techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. We also compare our results to those collected earlier with respect to the relationship between hand-seeded faults and mutation faults, and the implications this has for researchers performing empirical studies of prioritization.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510136,no,undetermined,0
A low-latency checkpointing scheme for mobile computing systems,"Fault-tolerant mobile computing systems have different requirements and restrictions, not taken into account by conventional distributed systems. This paper presents a coordinated checkpointing scheme which reduces the delay involved in a global checkpointing process for mobile systems. A piggyback technique is used to track and record the checkpoint dependency information among processes during normal message transmission. During checkpointing, a concurrent checkpointing technique is designed to use the pre-recorded process dependency information to minimize process blocking time by sending checkpoint requests to dependent processes at once, hence saving the time to trace the dependency tree. Our checkpoint algorithm forces a minimum number of processes to take checkpoints. Via probability-based analysis, we show that our scheme can significantly reduce the latency associated with checkpoint request propagation, compared to traditional coordinated checkpointing approach.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510071,no,undetermined,0
Testing the semantics of W3C XML schema,"The XML schema language is becoming the preferred means of defining and validating highly structured XML instance documents. We have extended the conventional mutation method to be applicable for W3C XML schemas. In this paper a technique for using mutation analysis to test the semantic correctness of W3C XML schemas is presented. We introduce a mutation analysis model and a set of W3C XML schema (XSD) mutation operators that can be used to detect faults involving name-spaces, user-defined types, and inheritance. Preliminary evaluation of our technique shows that it is effectiveness to test the semantics of W3C XML schema documents.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510064,no,undetermined,0
Multiple transient faults in logic: an issue for next generation ICs?,"In this paper, we first evaluate whether or not a multiple transient fault (multiple TF) generated by the hit of a single cosmic ray neutron can give rise to a bidirectional error at the circuit output (that is an error in which all erroneous bits are 1s rather than 0s, or vice versa, within the same word, but not both). By means of electrical level simulations, we show that this can be the case. Then, we present a software tool that we have developed in order to evaluate the likelihood of occurrence of such bidirectional errors for very deep submicron (VDSM) ICs. The application of this tool to benchmark circuits has proven that such a probability can not be neglected for several benchmark circuits. Finally, we evaluate the behavior of conventional self-checking circuits (generally designed accounting only for single TFs) with respect to such events. We show that the modifications generally introduced to their functional blocks in order to avoid output bidirectional errors due to single TFs (as required when an AUED code is implemented) can significantly reduce (up to the 40%) also the probability to have bidirectional errors because of multiple TFs.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544534,no,undetermined,0
On the transformation of manufacturing test sets into on-line test sets for microprocessors,"In software-based self-test (SBST), a microprocessor executes a set of test programs devised for detecting the highest possible percentage of faults. The main advantages of this approach are its high defect fault coverage (being performed at-speed) and the reduced cost (since it does not require any change in the processor hardware). SBST can also be used for online test of a microprocessor-based system. However, some additional constraints exist in this case (e.g. in terms of test length and duration, as well as intrusiveness). This paper faces the issue of automatically transforming a test set devised for manufacturing test in a test set suitable for online test. Experimental results are reported on an Intel 8051 microcontroller.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544549,no,undetermined,0
"Model-based Testing Considering Cost, Reliability and Software Quality","The important objectives of software engineering are developing software with high quality, low cost and high reliability. And then people consider more and more about completeness and effectiveness of the technique in order to increase the developers' confidence in software quality. So, the focus of this paper is to minimize the cost of software development during the testing and maintenance stage. Model-based testing and maintenance have been provided in many software development systems. Most software reliability growth models (SRGMs) are typically based on failure data such as number of failures, time of occurrence, failure severity, or the interval between two consecutive failures, whereas other models describe the relationship among the calendar testing, the amount of testing-effort, and the number of software faults detected by testing. In this paper, we propose a new method of software reliability growth models (SRGMs) based on non-homogeneous Poisson process (NHPP) model for reliability growth which during the development test phase. The results provide to be shorter schedule, lower cost and higher quality",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1593456,no,undetermined,0
Verify results of network intrusion alerts using lightweight protocol analysis,"We propose a method to verify the result of attacks detected by signature-based network intrusion detection systems using lightweight protocol analysis. The observation is that network protocols often have short meaningful status codes saved at the beginning of server responses upon client requests. A successful intrusion that alters the behavior of a network application server often results in an unexpected server response, which does not contain the valid protocol status code. This can be used to verify the result of the intrusion attempt. We then extend this method to verify the result of attacks that still generate valid protocol status code in the server responses. We evaluate this approach by augmenting Snort signatures and testing on real world data. We show that some simple changes to Snort signatures can effectively verify the result of attacks against the application servers, thus significantly improve the quality of alerts",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1565240,no,undetermined,0
A dsPic-based measurement system for the evaluation of voltage sag severity through new power quality indexes,"In this paper the authors describe the implementation of a smart sensor based on microcontroller for the extraction of new conceived power quality indexes. The work is carried on starting from the improvement of three indexes presented in a previous work (De Capua et al., 2004) for exhaustively detecting voltage sags. After an examination of the loads which could be more susceptible to the duration or to the depth of the sag, an ANOVA analysis has been conducted in order to evaluate the indexes' sensibility to these characteristics of the sag. Then a new measurement algorithm has been implemented, that is considered more rapid for detecting a sag occurrence. Finally a dsPic-based smart sensor has been realized, to monitor the voltage RMS value and extract the indexes values. These values are transmitted to software located on an external peripheral, through serial communication, for the successive data processing stage.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1567550,no,undetermined,0
Information extraction system in large-scale Web,"Manually querying search engines in order to acquire a large body of related information is a tedious, error-prone process. Search engines retrieve and rank potentially relevant documents for human perusal, but do not extract facts, assess confidence, or fuse information from multiple documents. This paper present an information extraction system that aims to automate the tedious process of extracting large collections of facts from large-scale, domain-independent, and scalable manner. The paper focus on four major components: search engine interface, extractor, assessor, database, and further analyzes system architecture and reports on simulation results with large-scale information extraction systems.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566990,no,undetermined,0
A quality of service mechanism for IEEE 802.11 wireless network based on service differentiation,"This paper introduces an analytical model for wireless local area network with priority schemes based service differentiation. This model can predict station performance by access stations' number and traffic type before wireless channel condition changed. Then a new algorithm, DTCWF (dynamic tuning of contention window with fairness), is proposed to modify protocol options to limit end to end delay and loss rate of high priority traffic and maximize throughput of other traffics. Simulations validate this model and the comparison between DTCWF, DCF, and EDCA shows that our algorithm can improve quality of service for real-time traffic.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566913,no,undetermined,0
"TRUSS: a reliable, scalable server architecture","Traditional techniques that mainframes use to increase reliability -special hardware or custom software - are incompatible with commodity server requirements. The Total Reliability Using Scalable Servers (TRUSS) architecture, developed at Carnegie Mellon, aims to bring reliability to commodity servers. TRUSS features a distributed shared-memory (DSM) multiprocessor that incorporates computation and memory storage redundancy to detect and recover from any single point of transient or permanent failure. Because its underlying DSM architecture presents the familiar shared-memory programming model, TRUSS requires no changes to existing applications and only minor modifications to the operating system to support error recovery.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566557,no,undetermined,0
An experimental study of soft errors in microprocessors,"The issue of soft errors is an important emerging concern in the design and implementation of future microprocessors. The authors examine the impact of soft errors on two different microarchitectures: a DLX processor for embedded applications and a high-performance alpha processor. The results contrast impact of soft errors on combinational and sequential logic, identify the most vulnerable units, and assess soft error impact on the application.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566554,no,undetermined,0
Slingshot: Time-CriticalMulticast for Clustered Applications,"Datacenters are complex environments consisting of thousands of failure-prone commodity components connected by fast, high capacity interconnects. The software running on such datacenters typically uses multicast communication patterns involving multiple senders. We examine the problem of time-critical multicast in such settings, and propose Slingshot, a protocol that uses receiver-based FEC to recover lost packets quickly. Slingshot offers probabilistic guarantees on timeliness by having receivers exchange FEC packets in an initial phase, and optional complete reliability on packets not recovered in this first phase. We evaluate an implementation of Slingshot against SRM, a well-known multicast protocol, and show that it achieves two orders of magnitude faster recovery in datacenter settings",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1565954,no,undetermined,0
Audio scene analysis as a control system for hearing aids,"It is well known that simple amplification cannot help many hearing-impaired listeners. As a consequence of this, numerous signal enhancement algorithms have been proposed for digital hearing aids. Many of these algorithms are only effective in certain environments. The ability to quickly and correctly detect elements of the auditory scene can permit the selection/parameterization of enhancement algorithms from a library of available routines. In this work, the authors examine the real time parameterization of a frequency-domain compression algorithm which preserves formant ratios and thus enhances speech understanding for some individuals with severe sensorineural hearing loss in the 2-3 kHz range. The optimal compression ratio is dependent upon qualities of the acoustical signal. We briefly review the frequency-compression technology and describe a Gaussian mixture model classifier which can dynamically set the frequency compression ratio according to broad acoustic categories which we call cohorts. We discuss the results of a prototype simulator which has been implemented on a general purpose computer.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1565915,no,undetermined,0
Countering trusting trust through diverse double-compiling,"An air force evaluation of Multics, and Ken Thompson's famous Turing award lecture """"reflections on trusting trust, """" showed that compilers can be subverted to insert malicious Trojan horses into critical software, including themselves. If this attack goes undetected, even complete analysis of a system's source code can not find the malicious code that is running, and methods for detecting this particular attack are not widely known. This paper describes a practical technique, termed diverse double-compiling (DDC), that detects this attack and some compiler defects as well. Simply recompile the source code twice: once with a second (trusted) compiler, and again using the result of the first compilation. If the result is bit-for-bit identical with the untrusted binary, then the source code accurately represents the binary. This technique has been mentioned informally, but its issues and ramifications have not been identified or discussed in a peer-reviewed work, nor has a public demonstration been made. This paper describes the technique, justifies it, describes how to overcome practical challenges, and demonstrates it",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1565233,no,undetermined,0
Software package for equipment management learning,"Among other subjects, equipment management is to be included within a degree in industrial engineering. In particular, industrial engineering practitioners usually have to deal to some extent with decisions related to maintenance and renewal policies: how to devise, select and assess them. For this particular purpose, some mathematical models had been previously developed. A software package has been designed to facilitate the teaching and learning of both the mathematical models and the subjects themselves (evaluation and selection of maintenance and renewal policies). This paper describes the main features of this package. It also examines the major advantages derived from its use both in explanations in class and during the students' homework time by their own. Different experiences carried on with the package are presented in the Universidad Politecnica de Madrid.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1560294,no,undetermined,0
Requirements for CBD products and process quality,"This paper presents arguments for including the properties of processes involved in various approaches to component-based software development in predicting systems properties. It discusses how processes impact on system properties and relates the issues raised to standards that already address process and product quality. Although many standards still apply, CBD changes interpretations and emphases.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563186,no,undetermined,0
Design of SPICE experience factory model for accumulation and utilization of process assessment experience,"With growing interest in software process improvement (SPI), many companies are introducing international process models and standards. SPICE is most widely used process assessment model in the SPI work today. In the process of introducing and applying SPICE, practical experiences contribute to enhancing the project performance. The experience helps people to make decisions under uncertainty, and to find better compromises. This paper suggests a SPICE experience factory (SEF) model to use SPICE assessment experience. For this, we collected SPICE assessment results which were conducted in Korea from 1999 to 2004. The collected data does not only contain rating information but also specifies strengths and improvement point for each assessed company and its process. To use this assessment result more efficiently, root words were derived from each result items. And root words were classified into four: 1) measurement, 2) work product, 3) process performance, and 4) process definition and deployment. Database was designed and constructed to save all analyzed data in forms of root words. Database also was designed to efficiently search information the organization needs by strength/improvement point, or root word for each level. This paper describes procedures of SEF model and presents methods to utilize it. By using the proposed SEF model, even organizations which plan to undergo SPICE assessment for the first time can establish the optimal improvement strategies.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563185,no,undetermined,0
Design of opportunity tree framework for effective process improvement based on quantitative project performance,"Nowadays IT industry drives to improve the software process on marketing and financial benefits. For efficient process improvement, work performance should be enhanced in line with organization's vision by identifying weakness for improvement and risks with process assessment results and then mapping them in the software development environment. According to organization's vision, plans should be developed for marketing and financial strategic objectives. For each plan, improvement strategies should be developed for each work performance unit such as quality, delivery, cycle time, and waste. Process attributes in each unit should be identified and improvement methods shall be determined for them. In order to suggest a PPM (project performing measure) model to quantitatively measure organization's project performing capability and make an optimal decision for process improvement, this paper statistically analyzes SPICE assessment results of 2,392 weakness for improvement by process for 49 appraisals and 476 processes which were assessed through KASPA (Korea Association of Software Process Assessors) from 1999 to 2004 and then makes SEF (SPICE experience factory). It also presents scores on project performing capability and improvement effects by level, and presents weakness for improvement by priority in the performance unit by level. And finally, this paper suggests an OTF (opportunity tree framework) model to show optimal process improvement strategies.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563184,no,undetermined,0
A quantitative supplement to the definition of software quality,"This paper proposes a new quantitative definition for software quality. The definition is based on the Taguchi philosophy for assessing and improving the quality of manufacturing processes. The Taguchi approach, originally developed for manufacturing processes, define quality in terms of """"loss imparted to society """" by a product after delivery of the product to the end user. To facilitate the use of the Taguchi definition, several """"loss functions"""" have been developed. These loss functions allow quality to be quantitatively measured in monetary values (e.g. US dollars). To illustrate the application of the Taguchi definition to a software product, examples that utilize some of the loss functions are presented. The proposed definition of software quality shows good correlation to other popular qualitative and quantitative definitions for software quality.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563182,no,undetermined,0
An adaptive fault tolerance for situation-aware ubiquitous computing,"Since ubiquitous applications need situation-aware middleware services and computing environment (e.g., resources) keeps changing as the applications change, it is challenging to detect errors and recover them in order to provide seamless services and avoid a single point of failure. This paper proposes an adaptive fault tolerance (AFT) algorithm in situation-aware middleware framework and presents its simulation model of AFT-based agents.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563155,no,undetermined,0
A multi-agent based fault tolerance system for distributed multimedia object oriented environment: MAFTS,"This paper presents the design and implementation of the MAFTS (a multi-agent based fault-tolerance system), which is running on distributed multimedia object oriented environment. DOORAE (distributed object oriented collaboration environment) is a good example of the foundation technology for a computer-based multimedia collaborative work that allows development of required application by combining many agents composed of units of functional module when user wishes to develop a new application field. MAFTS has been designed and implemented in DOORAE environment. It is a multi-agent system that is implemented with object oriented concept. The main idea is to detect an error by using polling method. This system detects an error by polling periodically processes with relation to sessions. And, it is to classify the type of errors automatically by using learning rules. The characteristic of this system is to use the same method to get back again it as it creates a session.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563152,no,undetermined,0
A query system for spatiotemporal database applications,"In recent years, an increasing number of database applications deal with continuously changing data objects (CCDOs). In these applications, the underlying data management system must support new types of spatiotemporal queries that refer to CCDOs. The expressive power of the supported query language and the query processing algorithms determine the quality and the efficiency of the query system. In contrast to traditional data objects, CCDOs change continuously. Therefore, the relation between two CCDOs may change over time. We define a motion relation as a sequence of one or more distinct topological relations. This paper surveys existing spatiotemporal query types and proposes a powerful spatiotemporal database query language by defining a complete set of motion predicates that can fill the gap between space and time. In addition, the paper discusses how to efficiently process the proposed queries by utilizing existing indexing schemes.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1562824,no,undetermined,0
Locating where faults will be [software testing],"The goal of this research is to allow software developers and testers to become aware of which files in the next release of a large software system are likely to contain the largest numbers of faults or the highest fault densities in the next release, thereby allowing testers to focus their efforts on the most fault-prone files. This is done by developing a negative binomial regression model to help predict characteristics of new releases of a software system, based on information collected about prior releases and the new release under development. The same prediction model was also used to allow a tester to select the files of a new release that collectively contain any desired percentage of the faults. The benefit of being able to make these sorts of predictions accurately should be clear: if we know where to look for bugs, we should be able to target our testing efforts there and, as a result, find problems more quickly and therefore more economically. Two case studies using large industrial software systems are summarized. The first study used seventeen consecutive releases of a large inventory system, representing more than four years of field exposure. The second study used nine releases of a service provisioning system with two years of field experience.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1570876,no,undetermined,0
Visualizing the evolution of Web services using formal concept analysis,"The service-oriented paradigm constitutes a promising technology that allows many software systems to benefit of interesting mechanisms such as late binding and automatic discovery. From a service integrator's perspective, it is relevant to understand service evolution, to assess which could be its impact on his/her own system or, eventually, to change the bindings between the system and the services. Given the lack of source code availability, this task is, however, limited to understand how service interfaces evolve. We propose an approach, based on formal concept analysis, to understand how relationships between sets of services change across service evolution. The concept lattice is able to highlight hierarchy relationships and, in general, to identify commonalities and differences between services. Examples built upon real sets of services show the feasibility of the proposed approach.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1572308,no,undetermined,0
Change impact analysis for requirement evolution using use case maps,"Changing customer needs and computer technology are the driving factors influencing software evolution. There is a need to assess the impact of these changes on existing software systems. Requirement specification is gaining increasingly attention as a critical phase of software systems development process. In particular for larger systems, it quickly becomes difficult to comprehend what impact a requirement change might have on the overall system or parts of the system. Thus, the development of techniques and tools to support the evolution of requirement specifications becomes an important issue. In this paper we present a novel approach to change impact analysis at the requirement level. We apply both slicing and dependency analysis at the use case map specification level to identify the potential impact of requirement changes on the overall system. We illustrate our approach and its applicability with a case study conducted on a simple telephony system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1572312,no,undetermined,0
Experimental assessment of the time transfer capability of precise point positioning (PPP),"In recent years, many national timing laboratories have installed geodetic global positioning system (GPS) receivers together with their traditional GPS/GLONASS common view (CV) receivers and two way satellite time and frequency transfer (TWSTFT) equipment. A method called precise point positioning (PPP) is in use in the geodetic community allowing precise recovery of geodetic GPS receiver position, clock phase and tropospheric delay by taking advantage of the International GNSS Service (IGS) precise products. Natural Resources Canada (NRCan) has developed software implementing the PPP and a previous assessment of the PPP as a promising time transfer method was carried out at Istituto Elettrotecnico Nazionale (IEN) in 2003. This paper reports on a more systematic work performed at IEN and NRCan to further characterize the PPP method for time transfer application, involving data from nine national timing laboratories. Dual-frequency GPS observations (pseudorange and carrier phase) over the last ninety days of year 2004 were processed using the NRCan PPP software to recover receiver clock estimates at five minute intervals, using the IGS final satellite orbit and clock products. The quality of these solutions is evaluated mainly in terms of short-term noise. In addition, the time and frequency transfer capability of the PPP method were assessed with respect to independent techniques, such as TWSTFT, over a number of European and Transatlantic baselines",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1573955,no,undetermined,0
Performance Model Building of Pervasive Computing,"Performance model building is essential to predict the ability of an application to satisfy given levels of performance or to support the search for viable alternatives. Using automated methods of model building is becoming of increasing interest to software developers who have neither the skills nor the time to do it manually. This is particularly relevant in pervasive computing, where the large number of software and hardware components requires models of so large a size that using traditional manual methods of model building would be error prone and time consuming. This paper deals with an automated method to build performance models of pervasive computing applications, which require the integration of multiple technologies, including software layers, hardware platforms and wired/wireless networks. The considered performance models are of extended queueing network (EQN) type. The method is based on a procedure that receives as input the UML model of the application to yield as output the complete EQN model, which can then be evaluated by use of any evaluation tool.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1587694,no,undetermined,0
The case for outsourcing DFT,"The author discusses about outsourcing analog/mixed-signal DFT. At present we still lack a """"SAF"""" metric for measuring analog IC fault coverage, as most analog faults that are found by testing are of a parametric variety, and can not be measured or scored (as in the SAF coverage grade) by using Boolean techniques. To analyze analog and mixed-signal (A/MS) logic for testability, one has to know what the analog failures are that need to be detected, what the capability of the test equipment will be for these measurements, what the error or repeatability will be, and what the trade off is going to be between increased test accuracy and test time",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1584134,no,undetermined,0
"Optimized reasoning-based diagnosis for non-random, board-level, production defects","The """"back-end"""" costs associated with debug of functional test failures can be one of the highest cost adders in the manufacturing process. As boards become more dense and more complex, debug of functional failures will become more and more difficult. Test strategies try to detect and diagnose failures early on in the test process (component and structural tests), but inevitably some defects are not detected until functional testing is done on the board. Finding these defects usually requires an """"expert"""", with engineering level skills in both hardware and software. Depending on the complexity of the product, it could take several months (even years) to develop this level of expertise. During the initial product ramp, this expertise is usually most needed and often unavailable. Debug time is usually very long and scrap rates are generally high. This paper will provide an overview of reasoning-based diagnosis techniques and how they can significantly decrease debug time, especially during new product introduction. Because these engines are """"model-based"""", there is no guarantee how they will perform in real life. In almost all cases, the reasoning engine will have to be modified based on instances where the reasoning engine could not correctly identify the failing component. Making these adjustments to the reasoning is a very complex and sometimes risky endeavor. While the new model may correctly identify the previously missed failure, the reasoning may have been altered to a point where several other diagnoses have now been unknowingly compromised. This paper will propose enhancements to the reasoning engine that will allow a simpler approach to adapting to diagnostic escapes without risking compromises to the original diagnostic engine",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1583974,no,undetermined,0
Asymptotic Performance of a Multichart CUSUM Test Under False Alarm Probability Constraint,"Traditionally the false alarm rate in change point detection problems is measured by the mean time to false detection (or between false alarms). The large values of the mean time to false alarm, however, do not generally guarantee small values of the false alarm probability in a fixed time interval for any possible location of this interval. In this paper we consider a multichannel (multi-population) change point detection problem under a non-traditional false alarm probability constraint, which is desirable for a variety of applications. It is shown that in the multichart CUSUM test this constraint is easy to control. Furthermore, the proposed multichart CUSUM test is shown to be uniformly asymptotically optimal when the false alarm probability is small: it minimizes an average detection delay, or more generally, any positive moment of the stopping time distribution for any point of change.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1582175,no,undetermined,0
Towards a metamorphic testing methodology for service-oriented software applications,"Testing applications in service-oriented architecture (SOA) environments needs to deal with issues like the unknown communication partners until the service discovery, the imprecise black-box information of software components, and the potential existence of non-identical implementations of the same service. In this paper, we exploit the benefits of the SOA environments and metamorphic testing (MT) to alleviate the issues. We propose an MT-oriented testing methodology in this paper. It formulates metamorphic services to encapsulate services as well as the implementations of metamorphic relations. Test cases for the unit test phase is proposed to generate follow-up test cases for the integration test phase. The metamorphic services invoke relevant services to execute test cases and use their metamorphic relations to detect failures. It has potentials to shift the testing effort from the construction of the integration test sets to the development of metamorphic relations.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579174,no,undetermined,0
A method of generating massive virtual clients and model-based performance test,"Testing the performance of a server that handles massive connections requires to generate massive virtual client connections and to model realistic traffic. In this paper, we propose a novel approach to generate massive virtual clients and realistic traffic. Our approach exploits the Windows I/O completion port (IOCP), which is the Windows NT operating system support for developing a scalable, high throughput server, and model-based testing scenarios. We describe implementation details of the proposed approach. Through analysis and experiments, we prove that the proposed method can predict and evaluate performance data more accurately in cost-effective way.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579142,no,undetermined,0
A metamorphic approach to integration testing of context-sensitive middleware-based applications,"During the testing of context-sensitive middleware-based software, the middleware identifies the current situation and invokes the appropriate functions of the applications. Since the middleware remains active and the situation may continue to evolve, however, the conclusion of some test cases may not be easily identified. Moreover, failures appearing in one situation may be superseded by subsequent correct outcomes and may, therefore, be hidden. We alleviate the above problems by making use of a special kind of situation, which we call checkpoints, such that the middleware will not activate the functions under test. We propose to generate test cases that start at a checkpoint and end at another. We identify functional relations that associate different execution sequences of a test case. Based on a metamorphic approach, we check the results of the test case to detect any contravention of such relations. We illustrate our technique with an example that shows how re-hidden failures may be detected.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579141,no,undetermined,0
Domain consistency in requirements specification,"Fixing requirements errors that are detected late in the software development life cycle can be extremely costly. So, finding problems in requirements specification early in the development cycle is critical and crucial. A formal specification can reduce errors by reducing ambiguity and imprecision and by making some instances of inconsistency and incompleteness obvious. In this paper, with an example of a moderately complex system of the mobile computing domain, we discuss how the consistency conditions found during initial abstract formal specification help in detecting logical errors during early stages of system development. We also discuss the importance of consistency conditions while modelling the domain of a complex system and show how the identified consistency conditions help in better understanding the specification and to gain confidence on the correctness of the specification. We use a combination of techniques, like specification inspection and testing the executable specification of a prototype using test cases, to validate the specification against the requirements as well as to ensure that the specified consistency conditions are respected and maintained by the operations defined in the specification.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579140,no,undetermined,0
Ontology based requirements analysis: lightweight semantic processing approach,"We propose a software requirements analysis method based on domain ontology technique, where we can establish a mapping between a software requirements specification and the domain ontology that represents semantic components. Our ontology system consists of a thesaurus and inference rules and the thesaurus part comprises domain specific concepts and relationships suitable for semantic processing. It allows requirements engineers to analyze a requirements specification with respect to the semantics of the application domain. More concretely, we demonstrate following three kinds of semantic processing through a case study, (1) detecting incompleteness and inconsistency included in a requirements specification, (2) measuring the quality of a specification with respect to its meaning and (3) predicting requirements changes based on semantic analysis on a change history.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579139,no,undetermined,0
Test case generation by OCL mutation and constraint solving,"Fault-based testing is a technique where testers anticipate errors in a system under test in order to assess or generate test cases. The idea is to have enough test cases capable of detecting these anticipated errors. This paper presents a method of fault-based test case generation for pre- and postcondition specifications. Here, errors are anticipated on the specification level by mutating the pre- and postconditions. We present the underlying theory by giving test cases a formal semantics and translate this general testing theory to a constraint satisfaction problem. A prototype test case generator serves to demonstrate the automatization of the method. The current tool works with OCL specifications, but the theory and method are general and apply to many state-based specification languages.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579121,no,undetermined,0
Formal verification of dead code elimination in Isabelle/HOL,"Correct compilers are a vital precondition to ensure software correctness. Optimizations are the most error-prone phases in compilers. In this paper, we formally verify dead code elimination (DCE) within the theorem prover Isabelle/HOL. DCE is a popular optimization in compilers which is typically performed on the intermediate representation. In our work, we reformulate the algorithm for DCE so that it is applicable to static single assignment (SSA) form which is a state of the art intermediate representation in modern compilers, thereby showing that DCE is significantly simpler on SSA form than on classical intermediate representations. Moreover, we formally prove our algorithm correct within the theorem prover Isabelle/HOL. Our program equivalence criterion used in this proof is based on bisimulation and, hence, captures also the case of non-termination adequately. Finally we report on our implementation of this verified DCE algorithm in the industrial-strength scale compiler system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575909,no,undetermined,0
Experimental evaluation of FSM-based testing methods,"The development of test cases is an important issue for testing software, communication protocols and other reactive systems. A number of methods are known for the development of a test suite based on a formal specification given in the form of a finite state machine. Well-known methods are called the W, Wp, UIO, UIOv, DS, H and HIS test derivation methods. These methods have been extensively used by research community in the last years; however no proper comparison has been made between them. In this paper, we experiment with these methods to assess their complexity, applicability, completeness, fault detection capability, length and derivation time of their test suites. The experiments are conducted on randomly generated specifications and on a realistic protocol called the simple connection protocol.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575891,no,undetermined,0
A layered approach to semantic similarity analysis of XML schemas,"One of the most critical steps to integrating heterogeneous e-Business applications using different XML schemas is schema mapping, which is known to be costly and error-prone. Past research on schema mapping has not fully utilized semantic information in the XML schemas. In this paper, we propose a semantic similarity analysis approach to facilitate XML schema mapping, merging and reuse. Several key innovations are introduced to better utilize available semantic information. These innovations, including: 1) a layered semantic structure of XML schema, 2) layered specific similarity measures using an information content based approach, and 3) a scheme for integrating similarities at all layers. Experimental results using two different schemas from an real world application demonstrate that the proposed approach is valuable for addressing difficulties in XML schema mapping.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4583042,no,undetermined,0
A State Machine for Detecting C/C++ Memory Faults,"Memory faults are major forms of software bugs that severely threaten system availability and security in C/C++ program. Many tools and techniques are available to check memory faults, but few provide systematic full-scale research and quantitative analysis. Furthermore, most of them produce high noise ratio of warning messages that require many human hours to review and eliminate false-positive alarms. And thus, they cannot locate the root causes of memory faults precisely. This paper provides an innovative state machine to check memory faults, which has three main contributions. Firstly, five concise formulas describing memory faults are given to make the mechanism of the state machine simple and flexible. Secondly, the state machine has the ability to locate the cause roots of the memory faults. Finally, a case study applying to an embedded software, which is written in 50 thousand lines of C codes, shows it can provide useful data to evaluate the reliability and quality of software",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575411,no,undetermined,0
Performance evaluation of agent-based material handling systems using simulation techniques,"The increasing influence of global economy is changing the conventional approach to managing manufacturing companies. Real-time reaction to changes in shop-floor operations, quick and quality response in satisfying customer requests, and reconfigurability in both hardware equipment and software modules, are already viewed as essential characteristics for next generation manufacturing systems. Part of a larger research that employs agent-based modeling techniques in manufacturing planning and control, this work proposes an agent-based material handling system and contrasts the centralized and decentralized scheduling approaches for allocation of material handling operations to the available resources in the system. To justify the use of the decentralized agent-based approach and assess its performance compared to conventional scheduling systems, a series of validation tests and a simulation study are carried out. As illustrated by the preliminary results obtained in the simulation study the decentralized agent-based approach can give good feasible solutions in a short amount of time.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1574354,no,undetermined,0
Case studies in arc flash reduction to improve safety and productivity,"With the advent of new power system analysis software, a more detailed arc flash analysis can be performed under various load conditions. These new ldquotoolsrdquo can also evaluate equipment damage, design systems with lower arc flash, and predict electrical fire locations based on high arc flash levels. This paper demonstrates how arc flash levels change with available utility MVA (mega volt amperes), additions in connected load, and selection of system components. This paper summarizes a detailed analysis of several power systems to illustrate possible misuses of 2004 NFPA 70E Risk Category Classification Tables while pointing toward future improvements of the Standards. In particular, findings indicate upstream protection may not open quick enough for fault on the secondary of a transformer or at the far end of a long cable due to the increase in system impedance. Several examples of how these problem areas can be dealt with are described in detail.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4585806,no,undetermined,0
Application of General Perception-Based QoS Model to Find Providers' Responsibilities. Case Study: User Perceived Web Service Performance.,"This paper presents a comprehensive model intended to analyze quality of service in telecommunications services and its causes. Although many works have been published in this area, both from a technical viewpoint as well as taking into consideration subjective concerns, they have not resulted in a unique methodology to assess the experienced quality. While most of the studies consider the quality of service only from a technical and end-to-end point of view, we try to analyze quality of service as a general gauge of final users' satisfaction. The proposed model allows us to estimate the quality experienced by end users, while offering detailed results regarding the responsibility of the different agents involved in the service provision. Once we overview the most significant elements of the model, an in-depth analytical study is detailed. Finally, we illustrate a practical study for Web browsing service in order to validate the theoretical model",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559914,no,undetermined,0
"Analyzing the yield of ExScal, a large-scale wireless sensor network experiment","Recent experiments have taken steps towards realizing the vision of extremely large wireless sensor networks, the largest of these being ExScal, in which we deployed about 1200 nodes over a 1.3 km by 300 m open area. Such experiments remain especially challenging because of: (a) prior observations of failure of sensor network protocols to scale, due to network faults and their spatial and temporal variability, (b) complexity of protocol interaction, (c) lack of sufficient data about faults and variability, even at smaller scales, and (d) current inadequacy of simulation and analytical tools to predict sensor network protocol behavior. In this paper, we present detailed data about faults, both anticipated and unanticipated, in ExScal. We also evaluate the impact of these faults on ExScal as well as the design principles that enabled it to satisfy its application requirements despite these faults. We describe the important lessons learnt from the ExScal experiment and suggest services and tools as a further aid to future large scale network deployments.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544608,no,undetermined,0
A light-weight proactive software change impact analysis using use case maps,"Changing customer needs and technology are driving factors influencing software evolution. Consequently, there is a need to assess the impact of these changes on existing software systems. For many users, technology is no longer the main problem, and it is likely to become a progressively smaller problem as standard solutions are provided by technology vendors. Instead, research will focus on the interface of the software with business practices. There exists a need to raise the level of abstraction further by analyzing and predicting the impact of changes at the specification level. In this research, we present a lightweight approach to identify the impact of requirement changes at the specification level. We use specification information included in use case maps to analyze the potential impact of requirement changes on a system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544761,no,undetermined,0
Metrics for ontologies,"The success of the semantic Web has been linked with the use of ontologies on the semantic Web. Given the important role of ontologies on the semantic Web, the need for domain ontology development and management are increasingly more and more important to most kinds of knowledge-driven applications. More and more these ontologies are being used for information exchange. Information exchange technology should foster knowledge exchange by providing tools to automatically assess the characteristics and quality of an ontology. The scarcity of theoretically and empirically validated measures for ontologies has motivated our investigation. From this investigation a suite of quality metrics have been developed and implemented as a plug-in to the ontology editor Protege so that any ontology specified in a standard Web ontology language such as RDFS or OWL may have a quality assessment analysis performed.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1548577,no,undetermined,0
FUMSTM artificial intelligence technologies including fuzzy logic for automatic decision making,"Advances in sensing technologies and aircraft data acquisition systems have resulted in generating huge aircraft data sets, which can potentially offer significant improvements in aircraft management, affordability, availability, airworthiness and performance (MAAAP). In order to realise these potential benefits, there is a growing need for automatically trending/mining these data and fusing the data into information and decisions that can lead to MAAAP improvements. Smiths has worked closely with the UK Ministry of Defence (MOD) to evolve Flight and Usage Management Software (FUMSTM) to address this need. FUMSTM provides a single fusion and decision support platform for helicopters, aeroplanes and engines. FUMSTM tools have operated on existing aircraft data to provide an affordable framework for developing and verifying diagnostic, prognostic and life management approaches. Whilst FUMSTM provides automatic analysis and trend capabilities, it fuses the condition indicators (CIs) generated by aircraft health and usage monitoring systems (HUMS) into decisions that can increase fault detection rates and reduce false alarm rates. This paper reports on a number of decision-making processes including logic, Bayesian belief networks and fuzzy logic. The investigation presented in this paper has indicated that decision-making based on logic and fuzzy logic can offer verifiable techniques. The paper also shows how Smiths has successfully applied fuzzy logic to the Chinook HUMS CIs. Fuzzy logic has also been applied to detect sensor problems causing long-term data corruptions.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1548501,no,undetermined,0
The GNAM monitoring system and the OHP histogram presenter for ATLAS,"ATLAS is one of the four experiments under construction along the Large Hadron Collider at CERN. During the 2004 combined test beam, the GNAM monitoring system and the OHP histogram presenter were widely used to assess both the hardware setup and the data quality. GNAM is a modular framework where detector specific code can be easily plugged in to obtain online low-level monitoring applications. It is based on the monitoring tools provided by the ATLAS trigger and data acquisition (TDAQ) software, OHP is a histogram presenter, capable to perform both as a configurable display and as a browser. From OHP, requests to execute simple interactive operations (such as reset, rebin or update) on histograms, can be sent to GNAM",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1547412,no,undetermined,0
Experiences of PD Diagnosis on MV Cables using Oscillating Voltages (OWTS),"Detecting, locating and evaluating of partial discharges (PD) in the insulating material, terminations and joints provides the opportunity for a quality control after installation and preventive detection of arising service interruption. A sophisticated evaluation is necessary between PD in several insulating materials and also in different types of terminations and joints. For a most precise evaluation of the degree and risk caused by PD it is suggested to use a test voltage shape that is preferably like the same under service conditions. Only under these requirements the typical PD parameters like inception and extinction voltage, PD level and PD pattern correspond to significant operational values. On the other hand the stress on the insulation should be limited during the diagnosis to not create irreversible damages and thereby worsening the condition of the test object. The paper introduces an oscillating wave test system (OWTS), which meets these mentioned demands well. The design of the system, its functionality and especially the operating software are made for convenient field application. Field data and experience reports will be presented and discussed. This field data serve also as good guide for the level of danger to the different insulating systems due to partial discharges",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1547150,no,undetermined,0
Technology of detecting GIC in power grids &amp; its monitoring device,"The magnetic storm results in the transmission lines with geomagnetically induced current (GIC). And GIC happening in random has the frequency between 0.001 Hz~0.1 Hz, and continues from several minutes to several hours. Based on elaborating mechanism and characteristic of GIC in grids and the influence on China power grids, the article has conducted the research work of GIC monitoring technology, and has investigated the method of sampling data of GIC, the survey algorithm and the new monitoring device. The simulated test shows, the monitoring device can effectively measure GIC which is signal of quasi direct current and randomness, has advantages of having few data to be handled and needing little memory space, etc",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1546843,no,undetermined,0
Hierarchical behavior organization,"In most behavior-based approaches, implementing a broad set of different behavioral skills and coordinating them to achieve coherent complex behavior is an error-prone and very tedious task. Concepts for organizing reactive behavior in a hierarchical manner are rarely found in behavior-based approaches, and there is no widely accepted approach for creating such behavior hierarchies. Most applications of behavior-based concepts use only few behaviors and do not seem to scale well. Reuse of behaviors for different application scenarios or even on different robots is very rare, and the integration of behavior-based approaches with planning is unsolved. This paper discusses the design, implementation, and performance of a behavior framework that addresses some of these issues within the context of behavior-based and hybrid robot control architectures. The approach presents a step towards more systematic software engineering of behavior-based robot systems.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1545581,no,undetermined,0
Towards self-healing systems via dependable architecture and reflective middleware,"Self-healing systems focus on how to reducing the complexity and cost of the management of dependability policies and mechanisms without human intervention. This position paper proposes a systematic approach to self-healing systems via dependable architecture and reflective middleware. Firstly, the differences between self-healing systems and traditional dependable systems are illustrated via the construction of a dependable computing model. Secondly, reflective middleware is incorporated into the dependable computing model for investigating the feasibility and benefits of implementing self-healing systems by reflective middleware. Thirdly, dependable architectures are introduced for providing application specific knowledge related to self-healing. Fourthly, an architecture based deployment tool is implemented for deploying dependable architectures into heterogeneous and distribute environments. Finally, an architecture based reflective J2EE application server, called PKUAS, is implemented for interpreting and enforcing dependable architectures at runtime, that is, discovering or predicting and recovering or preventing failures automatically under the guide of dependable architectures.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544809,no,undetermined,0
The emergence of the web,"Predicting the future is always a dicey business, and never more so than when the subject is the Web. The Web has been evolving so quickly that some say one Web year is the equivalent of three real years. Progress in communication technology has been characterized by a movement from lower to higher levels of abstraction. The semantic Web is not just for the World Wide Web. It represents a set of technologies that will work equally well on internal corporate intranets. This is analogous to Web services representing services not only across the Internet but also within a corporationpsilas intranet. So, the semantic Web will resolve several key problems facing current information technology architectures.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588927,no,undetermined,0
Validation and verification of prognostic and health management technologies,"Impact Technologies and the Georgia Institute of Technology are developing a Web-based software application that will provide JSF (F-35) system suppliers with a comprehensive set of PHM verification and validation (V&V) resources which will include: standards and definitions, V&V metrics for detection, diagnosis, and prognosis, access to costly seeded fault data sets and example implementations, a collaborative user forum for the exchange of information, and an automated tool for impartially evaluating the performance and effectiveness of PHM technologies. This paper presents the development of the prototype software product to illustrate the feasibility of the techniques, methodologies, and approaches needed to verify and validate PHM capabilities. A team of JSF system suppliers has been assembled to contribute, provide feedback and make recommendations to the product under development. The approach being pursued for assessing the overall PHM system accuracy is to quantify the associated uncertainties at each of the individual levels of a PHM system, and build up the accumulated inaccuracies as information is processed through the PHM architecture",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559699,no,undetermined,0
Ontology Model-Based Static Analysis on Java Programs,"Typical enterprise and military software systems consist of millions of lines of code with complicated dependence on diverse library abstractions. Manually debugging these codes imposes developers overwhelming workload and difficulties. To address software quality concerns efficiently, this paper proposes an ontology-based static analysis approach to automatically detect bugs in the source code of Java programs. First, we elaborate bug list collected, classify bugs into different categories, and translate bug patterns into SWRL (semantic Web rule language) rules using an ontology tool, Protege. An ontology model of Java program is created according to Java program specification using Protege as well. Both SWRL rules and the program ontology model are exported in OWL (Web ontology language) format. Second, Java source code under analysis is parsed into the abstract syntax tree (AST), which is automatically mapped to the individuals of the program ontology model. SWRL bridge takes in the exported OWL file (representing the SWRL rules model and program ontology model) and the individuals created for the Java code, conduits to Jess (a rule engine), and obtains inference results indicating any bugs. We perform experiments to compare bug detection capability with well-known FindBugs tool. A prototype of bug detector tool is developed to show the validity of the proposed static analysis approach.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591539,no,undetermined,0
Assessing the crash-failure assumption of group communication protocols,"Designing and correctly implementing group communication systems (GCSs) is notoriously difficult. Assuming that processes fail only by crashing provides a powerful means to simplify the theoretical development of these systems. When making this assumption, however, one should not forget that clean crash failures provide only a coarse approximation of the effects that errors can have in distributed systems. Ignoring such a discrepancy can lead to complex GCS-based applications that pay a large price in terms of performance overhead yet fail to deliver the promised level of dependability. This paper provides a thorough study of error effects in real systems by demonstrating an error-injection-driven design methodology, where error injection is integrated in the core steps of the design process of a robust fault-tolerant system. The methodology is demonstrated for the Fortika toolkit, a Java-based GCS. Error injection enables us to uncover subtle reliability bottlenecks both in the design of Fortika and in the implementation of Java. Based on the obtained insights, we enhance Fortika's design to reduce the identified bottlenecks. Finally, a comparison of the results obtained for Fortika with the results obtained for the OCAML-based Ensemble system in a previous work, allows us to investigate the reliability implications that the choice of the development platform (Java versus OCAML) can have",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544726,no,undetermined,0
Providing test quality feedback using static source code and automatic test suite metrics,"A classic question in software development is """"How much testing is enough?"""" Aside from dynamic coverage-based metrics, there are few measures that can be used to provide guidance on the quality of an automatic test suite as development proceeds. This paper utilizes the software testing and reliability early warning (STREW) static metric suite to provide a developer with indications of changes and additions to their automated unit test suite and code for added confidence that product quality will be high. Retrospective case studies to assess the utility of using the STREW metrics as a feedback mechanism were performed in academic, open source and industrial environments. The results indicate at statistically significant levels the ability of the STREW metrics to provide feedback on important attributes of an automatic test suite and corresponding code",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544724,no,undetermined,0
Study of the impact of hardware fault on software reliability,"As software plays increasingly important roles in modern society, reliable software becomes desirable for all stakeholders. One of the root causes of software failure is the failure of the computer hardware platform on which the software resides. Traditionally, fault injection has been utilized to study the impact of these hardware failures. One issue raised with respect to the use of fault injection is the lack of prior knowledge on the faults injected, and the fact that, as a consequence, the failures observed may not represent actual operational failures. This paper proposes a simulation-based approach to explore the distribution of hardware failures caused by three primary failure mechanisms intrinsic to semiconductor devices. A dynamic failure probability for each hardware unit is calculated. This method is applied to an example Z80 system and two software segments. The results lead to the conclusion that the hardware failure profile is location related, time dependent, and software-specific",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544722,no,undetermined,0
Error propagation in the reliability analysis of component based systems,"Component based development is gaining popularity in the software engineering community. The reliability of components affects the reliability of the system. Different models and theories have been developed to estimate system reliability given the information about system architecture and the quality of the components. Almost always in these models a key attribute of component-based systems, the error propagation between the components, is overlooked and not taken into account in the reliability prediction. We extend our previous work on Bayesian reliability prediction of component based systems by introducing the error propagation probability into the model. We demonstrate the impact of the error propagation in a case study of an automated personnel access control system. We conclude that error propagation may have a significant impact on the system reliability prediction and, therefore, future architecture-based models should not ignore it",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544721,no,undetermined,0
Safety analysis of software product lines using state-based modeling,"The analysis and management of variations (such as optional features) are central to the development of safety-critical, software product lines. However, the difficulty of managing variations, and the potential interactions among them, across an entire product line currently hinders safety analysis in such systems. The work described here contributes to a solution by integrating safety analysis of a product line with model-based development. This approach provides a structured way to construct a state-based model of a product line having significant, safety-related variations. The process described here uses and extends previous work on product-line software fault tree analysis to explore hazard-prone variation points. The process then uses scenario-guided executions to exercise the state model over the variations as a means of validating the product-line safety properties. Using an available tool, relationships between behavioral variations and potentially hazardous states are systematically explored and mitigation steps are identified. The paper uses a product line of embedded medical devices to demonstrate and evaluate the process and results",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544718,no,undetermined,0
Implicit Social Network Model for Predicting and Tracking the Location of Faults,"In software testing and maintenance activities, the observed faults and bugs are reported in bug report managing systems (BRMS) for further analysis and repair. According to the information provided by bug reports, developers need to find out the location of these faults and fix them. However, bug locating usually involves intensively browsing back and forth through bug reports and software code and thus incurs unpredictable cost of labor and time. Hence, establishing a robust model to efficiently and effectively locate and track faults is crucial to facilitate software testing and maintenance. In our observation, some related bug locations are tightly associated with the implicit links among source files. In this paper, we present an implicit social network model using PageRank to establish a social network graph with the extracted links. When a new bug report arrives, the prediction model provides users with likely bug locations according to the implicit social network graph constructed from the co-cited source files. The proposed approach has been implemented in real-world software archives and can effectively predict correct bug locations.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591547,no,undetermined,0
Outage probability lower bound in CDMA systems with lognormal-shadowed multipath Rayleigh-faded and noise-corrupted links,"The outage probability is one of the common metrics used in performance evaluation of cellular networks. In this paper, we derive a lower bound on the outage probability in CDMA systems where the communication links are disturbed by co-channel interference as well as additive noise. Each link is assumed to be faded according to both a lognormal distribution and a multipath Rayleigh distribution where the former represents the effect of shadowing while the latter represents the effect of short-term fading. The obtained lower bound is given in terms of a single-fold integral that can be easily computed using any modern software package. We present numerical results for the derived bound and compare them with the outage probability obtained by means of Monte Carlo simulations. Based on our results, we conclude that the proposed bound is relatively tight in a wide range of situations, particularly, in the case of small to moderate number of interferers and small to moderate shadowing standard deviation values.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1549474,no,undetermined,0
Delay-Centric Link Quality Aware OLSR,"This paper introduces a delay-centric link quality aware routing protocol, LQOLSR (link quality aware optimized link state routing). The LQOLSR chooses find fast and high quality routes in mobile ad hoc networks (MANET). LQOLSR predicts a packet transmission delay according to multiple transmission rates in IEEE 802.11 and selects the fastest route from source to destination by estimating relative transmission delay between nodes. We implement a LQOLSR protocol by modifying the basic OLSR (optimized link state routing) protocol. We evaluate and analyze the performance in a real testbed established in an office building",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1550948,no,undetermined,0
Filtering of shrew DDoS attacks in frequency domain,"The shrew distributed denial of service (DDoS) attacks are periodic, bursty, and stealthy in nature. They are also known as reduction of quality (RoQ) attacks. Such attacks could be even more detrimental than the widely known flooding DDoS attacks because they damage the victim servers for a long time without being noticed, thereby denying new visitors to the victim servers, which are mostly e-commerce sites. Thus, in order to minimize the huge monetary losses, there is a pressing need to effectively detect such attacks in real-time. Unfortunately, effective detection of shrew attacks remains an open problem. In this paper, we meet this challenge by proposing a new signal processing approach to identifying and detecting the attacks by examining the frequency-domain characteristics of incoming traffic flows to a server. A major strength of our proposed technique is that its detection time is less than a few seconds. Furthermore, the technique entails simple software or hardware implementations, making it easily deployable in a real-life network environment",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1550964,no,undetermined,0
A novel tuneable low-intensity adversarial attack,"Currently, denial of service (DoS) attacks remain amongst the most critical threats to Internet applications. The goal of the attacker in a DoS attack is to overwhelm a shared resource by sending a large amount of traffic thus, rendering the resource unavailable to other legitimate users. In this paper, we expose a novel contrasting category of attacks that is aimed at exploiting the adaptive behavior exhibited by several network and system protocols such as TCP. The goal of the attacker in this case is not to entirely disable the service but to inflict sufficient degradation to the service quality experienced by legitimate users. An important property of these attacks is the fact that the desired adversarial impact can be achieved by using an non-suspicious low-rate attack stream, which can easily evade detection. Further by tuning various parameters of the attack traffic stream, the attacker can inflict varying degrees of service degradation and at the same time making it extremely difficult for the victim to detect attacker presence. Our simulation based experiments validate our observations and demonstrate that an attacker can significantly degrade the performance of the TCP flows by inducing lowrate attack traffic which is co-ordinated to exploit the congestion control behavior of TCP",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1550965,no,undetermined,0
Safety verification of fault tolerant goal-based control programs with estimation uncertainty,"Fault tolerance and safety verification of control systems that have state variable estimation uncertainty are essential for the success of autonomous robotic systems. A software control architecture called mission data system, developed at the Jet Propulsion Laboratory, uses goal networks as the control program for autonomous systems. Certain types of goal networks can be converted into linear hybrid systems and verified for safety using existing symbolic model checking software. A process for calculating the probability of failure of certain classes of verifiable goal networks due to state estimation uncertainty is presented. A verifiable example task is presented and the failure probability of the control program based on estimation uncertainty is found.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4586461,no,undetermined,0
Web engineering: a new emerging discipline,"Web engineering, an emerging new discipline, advocates a process and a systematic approach to development of high quality Web based systems. In contrast the commercial practice still remains ad-hoc. Although the Web based systems has become increasingly complex, the development process is still un-engineered. """"There are very few standard methods for the Web developers to use. Hence, there is a strong need to understand and undertake Web engineering """". (Y. Deshpande and M. Gaedke, 2005) This paper is a result of our extensive survey of literature, our work and interaction with developers. This paper gives an introductory overview on Web engineering, it assesses similarities and differences between development of traditional software and Web based systems, and reviews some of the ongoing work in this area. We discuss the need for development of process model for Web based applications (WBA). such as is available far conventional software, with an overview of our work on development of such a process framework (R. Ahmad, et al, 2005). This paper also attempts to highlight the areas that need further study.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1558923,no,undetermined,0
Strategy for mutation testing using genetic algorithms,"In this paper, we propose a model to reveal faults and kill mutant using genetic algorithms. The model first instruments the source and mutant program and divides in small units. Instead of checking the entire program, it tries to find fault in each unit or kills each mutant unit. If any unit survives, the new test data is generated using genetic algorithm with special fitness function. The output of each test for each unit is recorded to detect the faulty unit. In this strategy, the source program and the mutant are instrumented in such a way that the input and output behavior of each unit can be traced. A checker module is used to compare and trace the output of each unit. A complete architecture of the model is proposed in the paper",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1557156,no,undetermined,0
Design-level performance prediction of component-based applications,"Server-side component technologies such as Enterprise JavaBeans (EJBs), .NET, and CORBA are commonly used in enterprise applications that have requirements for high performance and scalability. When designing such applications, architects must select suitable component technology platform and application architecture to provide the required performance. This is challenging as no methods or tools exist to predict application performance without building a significant prototype version for subsequent benchmarking. In this paper, we present an approach to predict the performance of component-based server-side applications during the design phase of software development. The approach constructs a quantitative performance model for a proposed application. The model requires inputs from an application-independent performance profile of the underlying component technology platform, and a design description of the application. The results from the model allow the architect to make early decisions between alternative application architectures in terms of their performance and scalability. We demonstrate the method using an EJB application and validate predictions from the model by implementing two different application architectures and measuring their performance on two different implementations of the EJB platform.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1556552,no,undetermined,0
The application of evolutionary computation to the analysis of the profiles of elliptical galaxies: a maximum likelihood approach,Genetic programming technique has been found to be suitable in scenarios where the formulation of models is a data driven process. Evolutionary programming provides a way of searching for parameters in a model without being prone to fall in local minima. A review of how these techniques have been applied to the analysis of elliptical galaxies is given. The effectiveness of a maximum likelihood based fitness function is asserted and is applied to the parameter fitting using evolutionary programming. A maximum likelihood based function is found to show consistent and significant improvement over a hit-based fitness function for modeling the profiles of elliptical galaxies. It is asserted that such a function would potentially improve the quality of model produced by symbolic regression using genetic programming.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1555021,no,undetermined,0
Checkers using a co-evolutionary on-line evolutionary algorithm,"The game of checkers has been well studied and many computer players exist. The vast majority of these 'software opponents' use a minimax strategy combined with an evaluation function to expand game tree for a number of moves ahead and estimate the quality of the pending moves. In this paper, an alternative approach is described where an on-line evolutionary algorithm is used to co-evolve move sets for both players in the game, playing the entire length of the game tree for each evaluation, thus avoiding the need for the minimax strategy or an evaluation function. The on-line evolutionary algorithm operates in essence as a 'directed' Monte-Carlo search process and although demonstrated on the game of checkers, could potentially be used to play games with a larger branching factor such as Go.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1554919,no,undetermined,0
Fault diagnosis and isolation in aircraft gas turbine engines,"This paper formulates and validates a novel methodology for diagnosis and isolation of incipient faults in aircraft gas turbine engines. In addition to abrupt large faults, the proposed method is capable of detecting and isolating slowly evolving anomalies (i.e., deviations from the nominal behavior), based on analysis of time series data observed from the instrumentation in engine components. The fault diagnosis and isolation (FDT) algorithm is based upon Symbolic Dynamic Filtering (SDF) that has been recently reported in literature and relies on the principles of Symbolic Dynamics, Statistical Pattern Recognition and Information Theory. Validation of the concept is presented and a real life software architecture is proposed based on the simulation model of a generic two-spool turbofan engine for diagnosis and isolation of incipient faults.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4586813,no,undetermined,0
Is mutation an appropriate tool for testing experiments? [software testing],"The empirical assessment of test techniques plays an important role in software testing research. One common practice is to instrument faults, either manually or by using mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. This paper investigates this important question based on a number of programs with comprehensive pools of test cases and known faults. It is concluded that, based on the data available thus far, the use of mutation operators is yielding trustworthy results (generated mutants are similar to real faults). Mutants appear however to be different from hand-seeded faults that seem to be harder to detect than real faults.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553583,no,undetermined,0
Fault detection and isolation based on system feedback,"This paper present a method to detect the transducers fault in the close loop control systems. The necessities imposed for the fault detection algorithm are: rapid answer in the case of fault, which comes out; the diminution of the risk to come out false alarms; lower effort calculation. In the paper are presented the equations of fault detection structure that suggest the software algorithms. In last part of the paper, the algorithm was verified on the steam overhead equations, developed in this paper.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588720,no,undetermined,0
Business intelligence as a competitive differentiator,"The successes of organizations vary greatly from industry to industry. For every business in every industry, revenue growth remains the most fundamental indicator, and by far the most critical. Lately, marketplace realities are making revenue targets harder and harder to reach. That's why every organization must infuse strategic and tactical decisions with the knowledge necessary to maximize revenue, reduce costs, minimize risk and achieve competitive advantage. Business intelligence is defined as getting the right information to the right people at the right time. The term encompasses all the capabilities required to turn data into intelligence that everyone in an organization can trust and use for more effective decision making. BI is a sustainable competitive advantage. It allows the organization to drive revenues, manage costs, and realize consistent levels of profitability. An 'intelligent enterprise' - one that uses BI to advance its business - is better able to predict how future economic and market changes will affect its business. Such an organization is able to adapt to the new changes in order to gain. Business intelligence competency center can achieve more intelligence for organization at less cost by supporting the corporate strategy with a BI strategy on a continuous basis.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588724,no,undetermined,0
A quality-driven systematic approach for architecting distributed software applications,"Architecting distributed software applications is a complex design activity. It involves making decisions about a number of inter-dependent design choices that relate to a range of design concerns. Each decision requires selecting among a number of alternatives; each of which impacts differently on various quality attributes. Additionally, there are usually a number of stakeholders participating in the decision-making process with different, often conflicting, quality goals, and project constraints, such as cost and schedule. To facilitate the architectural design process, we propose a quantitative quality-driven approach that attempts to find the best possible fit between conflicting stakeholders' quality goals, competing architectural concerns, and project constraints. The approach uses optimization techniques to recommend the optimal candidate architecture. Applicability of the proposed approach is assessed using a real system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553567,no,undetermined,0
Adaptation platform for autonomic context-aware services,"Most context-aware services are not autonomic because of two main reasons. The first one is that a context-aware service reacts only to context states that are entirely predicted by the developer. The second reason is that the adaptation control is based on predefined, application and context-specific policies. In this paper we propose a solution based on an application-context description, which allows the machine to autonomously discover the context structure and the adaptation strategies. We have tested our model using a simple scenario where a forum service is adapted to the user language.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588756,no,undetermined,0
How software can help or hinder human decision making (and vice-versa),"Summary form only given. Developments in computing offer experts in many fields specialised support for decision making under uncertainty. However, the impact of these technologies remains controversial. In particular, it is not clear how advice of variable quality from a computer may affect human decision makers. Here the author reviews research showing strikingly diverse effects of computer support on expert decision-making. Decisions support can both systematically improve or damaged the performance of decision makers in subtle ways depending on the decision maker's skills, variation in the difficulty of individual decisions and the reliability of advice from the support tool. In clinical trials decision support technologies are often assessed in terms of their average effects. However this methodology overlooks the possibility of differential effects on decisions of varying difficulty, on decision makers of varying competence, of computer advice of varying accuracy and of possible interactions among these variables. Research that has teased apart aggregated clinical trial data to investigate these possibilities has discovered that computer support was less useful for - and sometimes hindered - professional experts who were relatively good at difficult decisions without support; at the same time the same computer support tool helped those experts who were less good at relatively easy decisions without support. Moreover, inappropriate advice from the support tool could bias decision makers' decisions and, predictably, depending on the type of case, improve or harm the decisions.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553537,no,undetermined,0
Computer vision based decision support tool for hydro-dams surface deterioration assessment and visualization using fuzzy sets and pseudo-coloring,"Hydro-dams safety represents an important concern since their failure could be critical for the society. A key part of the hydro-dams surveillance programs is their visual inspection. However few computer vision support tools for implementing semi-automatically and objectively the visual surveillance and observation of the hydro-dams components exist. One of the issues addressed during the visual inspection, important in the preservation of a good condition of the concrete, is the examination of surface deterioration in respect to small patterned cracks and roughness on the downstream wall. This is particularly a task where digital image enhancement and analysis can bring significant benefit, not only by presenting the user with a more relevant image of the surface deterioration, but also by providing - through suitable numerical descriptors, correlated with linguistic descriptors- subjective and examiner-independent information on the surface state. The correlation of extracted numerical descriptors used to quantify the surface roughness with linguistic qualifiers of the deterioration state of the hydro-dam wall should be determined using information gathered from observers, since it must be compliant to the human expert interpretation of visual data in assessing the concrete surface deterioration. Such an approach would result in a computer vision decision support tool embedding expert knowledge, as designed, implemented and proposed in this paper. The resulting software system was verified on a set of images acquired from a Romanian hydro-dam. The compliance of the linguistic results with the human observation proves its functionality as a semi-automatic tool for hydro-dams surveillance.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4588913,no,undetermined,0
Pedagogic data as a basis for Web service fault models,This paper outlines our method for deriving fault models for use with our WS-FIT tool that can be used to assess the dependability of SOA. Since one of the major issues with extracting these heuristic rules and fault models is the availability of software systems we examine the use of systems constructed through pedagogic activities to provide one source of information.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1551139,no,undetermined,0
Optimized distributed delivery of continuous-media documents over unreliable communication links,"Video-on-demand (VoD) applications place very high requirements on the delivery medium. High-quality services should provide for a timely delivery of the data-stream to the clients plus a minimum of playback disturbances. The major contributions of this paper are that it proposes a multiserver, multi-installment (MSMI) solution approach (sending the document in several installments from each server) to the delivery problem and achieves a minimization of the client waiting time, also referred to as the access time (AT) or start-up latency in the literature. By using multiple spatially distributed servers, we are able to exploit slow connections that would otherwise prevent the deployment of video-on-demand-like services, to offer such services in an optimal manner. Additionally, the delivery and playback schedule that is computed by our approach is loss-aware in the sense that it is flexible enough to accommodate packet losses without interrupts. The mathematical framework presented covers both computation and optimization problems associated with the delivery schedule, offering a complete set of guidelines for designing MSMI VoD services. The optimizations presented include the ordering of the servers and determining the number of installments based on the packet-loss probabilities of the communication links. Our analysis guarantees the validity of a delivery schedule recommended by the system by providing a percentage of confidence for an uninterrupted playback at the client site. This, in a way, quantifies the degree of quality of service rendered by the system and the MSMI strategy proposed. The paper is concluded by a rigorous simulation study that showcases the substantial advantages of the proposed approach and explores how optimization of the schedule parameters affects performance.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1501809,no,undetermined,0
Automatic generation of software/hardware co-emulation interface for transaction-level communication,"This paper presents a methodology for generating interface of a co-emulation system where processor and emulator execute testbench and design unit, respectively while interacting with each other. To reduce the communication time between the processor and emulator, data transfers are performed in transaction level instead of signal level. To do this, transactor should be located near the DUT mapped on the hardware emulator. Consequently transactor is described in a synthesizable way. Moreover, the transactor design depends on both emulator system protocol and DUT protocol. Therefore, transactor description would not only be time-consuming but also error-prone task. Based on the layered architecture, we propose an automated procedure for generating co-emulation interface from platform-independent transactor. We have also discussed about the practical issues on multiple channel and clock skew problem.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1500054,no,undetermined,0
A system for predicting the run-time behavior of Web services,"In a service oriented architecture requestors should be able to use the services that best fit their needs. In particular, for Web services it should be possible to fully exploit the advantages of dynamic binding. Up to now, no proposed solution allows the requesting agent to dynamically select the most """"convenient"""" service at invoke time. The reason is that currently the requesting agents do not compare the runtime behavior of different services. In this paper, we propose a system that provides and exploits predictions about the behavior of Web services, expressed in terms of availability, reliability and completion time. We also describe a first prototype (eUDDIr) of the specification. EUDDIr relies on a scalable agents-based monitoring architecture that collects data on Web services runtime activity. The computed predictions are used by requesting agents to implement an effective dynamic service selection. Our proposal is well suited whenever requestors do not wish to explicitly deal with QoS aspects, or in the case that provider agents have no convenience in building up the infrastructure for guaranteed QoS, at the same time aiming to provide services of good quality to their customers. Furthermore, the adoption of eUDDIr effectively improves the service requestors """"satisfaction"""" when they are involved in a Web services composition process.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1499534,no,undetermined,0
High-impedance fault detection using discrete wavelet transform and frequency range and RMS conversion,"High-impedance faults (HIFs) are faults which are difficult to detect by overcurrent protection relays. Various pattern recognition techniques have been suggested, including the use of wavelet transform . However this method cannot indicate the physical properties of output coefficients using the wavelet transform. We propose to use the Discrete Wavelet Transform (DWT) as well as frequency range and rms conversion to apply a pattern recognition based detection algorithm for electric distribution high impedance fault detection. The aim is to recognize the converted rms voltage and current values caused by arcs usually associated with HIF. The analysis using discrete wavelet transform (DWT) with the conversion yields measurement voltages and currents which are fed to a classifier for pattern recognition. The classifier is based on the algorithm using nearest neighbor rule approach. It is proposed that this method can function as a decision support software package for HIF identification which could be installed in an alarm system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1375120,no,undetermined,0
Quantifying software architectures: an analysis of change propagation probabilities,"Summary form only given. Software architectures are an emerging discipline in software engineering as they play a central role in many modern software development paradigms. Quantifying software architectures is an important research agenda, as it allows software architects to subjectively assess quality attributes and rationalize architecture-related decisions. In this paper, we discuss the attribute of change propagation probability, which reflects the likelihood that a change that arises in one component of the architecture propagates (i.e. mandates changes) to other components.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1387113,no,undetermined,0
Software requirement risk assessment using UML,"Summary form only given. Risk assessment is an integral part of software risk management. There are several methods for risk assessment during various phases of software development and at different levels of abstraction. However, there are very few techniques available for assessing risk at the requirements level and those that are available are highly subjective and are not based on any formal design models. Such techniques are human-intensive and highly error prone. This paper presents a methodology that assesses software risk at the requirements level using Unified Modeling Language (UML) specifications of the software at the early development stages. Each requirement is mapped to a specific operational scenario in UML. We determine the possible failure modes of the scenario and find out the complexity of the scenario in each failure mode. The risk factor of a scenario in a failure mode is obtained by combining the complexity of the failure mode in that scenario and the severity of the failure. The result of applying the methodology on a cardiac pacemaker case study is presented.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1387101,no,undetermined,0
Automatic construction of multidimensional schema from OLAP requirements,"Summary form only given. The manual design of data warehouse and data mart schemes can be a tedious, error-prone, and time-consuming task. In addition, it is a highly complex engineering task that calls for methodological support. This paper lays the grounds for an automatic generation approach of multidimensional schemes. It first defines a tabular format for OLAP requirements. Secondly, it presents a set of algebraic operators used to transform automatically the OLAP requirements, specified in the tabular format, to data mart modelled either as star or constellation schemes. Our approach is illustrated with an example.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1387025,no,undetermined,0
Reusability metrics for software components,"Summary form only given. Assessing the reusability, adaptability, compose-ability and flexibility of software components is more and more of a necessity due to the growing popularity of component based software development (CBSD). Even if there are some metrics defined for the reusability of object-oriented software (OOS), they cannot be used for CBSD because these metrics require analysis of source code. The aim of this paper is to study the adaptability and compose-ability of software components, both qualitatively and quantitatively. We propose metrics and a mathematical model for the above-mentioned characteristics of software components. The interface characterization is the starting point of our evaluation. The adaptability of a component is discussed in conjunction with the complexity of its interface. The compose-ability metric defined for database components is extended for general software components. We also propose a metric for the complexity and adaptability of the problem solved by a component, based on its use cases. The number of alternate flows from the use case narrative is considered as a measurement for the complexity of the problem solved by a component. This was our starting point in developing a set of metrics for evaluating components functionality-wise. The main advantage of defining these metrics is the possibility to measure adaptability, reusability and quality of software components, and therefore to identify the most effective reuse strategy.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1387023,no,undetermined,0
Architecture-Based Assessment of Software Reliability,"With the growing advent of object-oriented and component-based software development paradigms, architecture-based software reliability analysis has emerged as an attractive alternative to the conventional black-box analysis based on software reliability growth models. The primary advantage of the architecture-based approach is that it explicitly relates the application reliability to component reliabilities, which eases the identification of components that are critical from a reliability perspective. Furthermore, these techniques can be used for an early assessment of the application reliability. These two features together can provide valuable information to practitioners and architects who design software applications, and managers who plan the allocation of resources to achieve the desired reliability targets in a cost effective manner.The objective of this tutorial is to discuss techniques to assess the reliability of a software application taking into consideration its architecture and the failure behavior of its components. The tutorial will also present how the architecture-based approach could be used to analyze the sensitivity of the application reliability to component and architectural parameters and to compute the importance measures of the application components. We will demonstrate the potential of the techniques presented in the tutorial through a case study of the IP multimedia subsystem (IMS).",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601576,no,undetermined,0
An Alternative to Technology Readiness Levels for Non-Developmental Item (NDI) Software,"Within the Department of Defense, Technology Readiness Levels (TRLs) are increasingly used as a tool in assessing program risk. While there is considerable evidence to support the utility of using TRLs as part of an overall risk assessment, some characteristics of TRLs limit their applicability to software products, especially Non-Developmental Item (NDI) software including Commercial-Off-The-Shelf, Government-Off-The-Shelf, and Open Source Software. These limitations take four principle forms: 1) """"blurring-together"""" various aspects of NDI technology/product readiness; 2) the absence of some important readiness attributes; 3) NDI product """"decay;"""" and 4) no recognition of the temporal nature of system development and acquisition context. This paper briefly explores these issues, and describes an alternate methodology which combines the desirable aspects of TRLs with additional readiness attributes, and defines an evaluation framework which is easily understandable, extensible, and applicable across the full spectrum of NDI software.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1385892,no,undetermined,0
An Architecture for Dynamic Data Source Integration,"Integrating multiple heterogeneous data sources in to applications is a time-consuming, costly and error-prone engineering task. Relatively mature technologies exist that make integration tractable from an engineering perspective. These technologies however have many limitations, and hence present opportunities for breakthrough research. This paper briefly describes some of these limitations. It then provides an overview of the Data Concierge research project and prototype that is attempting to provide solutions to some of these limitations. The paper focuses on the core architecture and mechanisms in the Data Concierge for dynamically attaching to a previously unidentified source of information. The generic API supported by the Data Concierge is described, along with the architecture and prototype tools for describing the meta-data necessary to facilitate dynamic integration. In addition, we describe the outstanding challenges that remain to be solved before the Data Concierge concept can be realized.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1385807,no,undetermined,0
Where's My Jetpack?,"Software development tools often fail to deliver on inflated promises. Rather than the predicted progression toward ever-increasing levels of abstraction, two simple trends have driven the evolution of currently available software development tools: integration at the source-code level and a focus on quality. Thus source code has become the bus that tools tap into for communicating with other tools. Also, focus has shifted from defect removal in the later phases to defect prevention in the earlier phases. In the future, tools are likely to support higher levels of abstraction, perhaps in the form of domain-specific languages communicated using XML.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4602669,no,undetermined,0
A Probabilistic Reliability Evaluation of Korea Power System,"Reliability and power quality have been increasingly important in recent years due to a number of black-out events occurring throughout the world. This paper presents a practical method of probabilistic reliability evaluation of Korea Power system by using the Probabilistic Reliability Assessment (PRA) program and Physical and Operational Margins (POM). The case study computes the Probabilistic Reliability Indices (PRI) of Korea Power system as applied PRA and POM. It takes a large number of contingency in load simulations and combines them with a practical method of characterizing the effect of the availabilities of generators, lines and transformers. The effectiveness and future works are illustrated by demonstrations of case study. The case studies of Korea power system are shown that these packages are effective in identifying possible weak points and root causes for likely reliability problems. The potential for these software packages is being explored further for assisting system operators with managing Korea power system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4603326,no,undetermined,0
Why Build Software,"We really build software to solve real problems. If we successfully solve the intended problem and thus satisfy the customers' needs, we have done a good job-regardless of the measured quality of our product or process. If the software contributes to humanity, generates few complaints, and makes a profit, then the software is good. All other measures are secondary to the real goal. All other measures have but a second-order relationship to quality. For example, we need measurement to help assess the health of a development project, or the status of a product under development. For these goals, there are obviously many other measures that make sense and are critical.",2004,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=7304180.pdf&bkn=7304002&pdfType=chapter,no,undetermined,0
Evolving legacy systems through a multi-objective decision process,"Our previous work on improving the quality of object-oriented legacy systems includes: i) devising a quality-driven re-engineering framework (L. Tahvildari et al., 2003); ii) proposing a software transformation framework based on soft-goal interdependency graphs to enhance quality (L. Tahvildari and K. Kontogiannis, 2002); and iii) investigating the usage of metrics for detecting potential design flaws (L. Tahvildari and K. Kontogiannis, 2004). This paper defines a decision making process that determines a list of source-code improving transformations among several applicable transformations. The decision-making process is developed on a multi-objective decision analysis technique. This type of technique is necessary as there are a number of different, and sometimes conflicting, criterion among non-functional requirements. For the migrant system, the proposed approach uses heuristic estimates to guide the discovery process",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1613372,no,undetermined,0
A survey of software testing practices in alberta,"Software organizations have typically de-emphasized the importance of software testing. In this paper, the results of a regional survey of software testing and software quality assurance techniques are described. Researchers conducted the study during the summer and fall of 2002 by surveying software organizations in the Province of Alberta. Results indicate that Alberta-based organizations tend to test less than their counterparts in the United States. The results also indicate that Alberta software organizations tend to train fewer personnel on testing-related topics. This practice has the potential for a two-fold impact: first, the ability to detect trends that lead to reduced quality and to identify the root causes of reductions in product quality may suffer from the lack of testing. This consequence is serious enough to warrant consideration, since overall quality may suffer from the reduced ability to detect and eliminate process or product defects. Second, the organization may have a more difficult time adopting methodologies such as extreme programming. This is significant because other industry studies have concluded that many software organizations have tried or will in the next few years try some form of agile method. Newer approaches to software development like extreme programming increase the extent to which teams rely on testing skills. Organizations should consider their testing skill level as a key indication of their readiness for adopting software development techniques such as test-driven development, extreme programming, agile modelling, or other agile methods.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1532522,no,undetermined,0
Supporting architecture evaluation process with collaborative applications,"The software architecture community has proposed several approaches to assess the capability of a system's architecture with respect to desired quality attributes (such as maintainability and performance). Scenario-based approaches are considered effective and mature. However, these methods heavily rely on face-to-face meetings, which are expensive and time consuming. Encouraged by the successful adoption of Internet-based technologies for several meeting based activities, we have been developing an approach to support architecture evaluation using Web-based collaborative applications, in this paper, we present a preliminary framework for conducting architecture evaluation in a distributed arrangement. We identify some supportive technologies and their expected benefits. We also present some of the initial findings of a research program designed to assess the effectiveness of the proposed idea. Findings of this study provide some support for distributed architecture evaluation.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1492919,no,undetermined,0
An auditor-based survey for assessing adherence to configuration management in software organizations,"The fundamental problem that confronts Pakistani software industry is that they have very limited experience in software process improvement strategies. We hardly find any Capability Maturity Model (CMM) level 3, 4 or 5 certified company in our software industry. It is therefore, necessary to find out the problems that are faced by companies in adopting CMM or its components. As a first step software configuration management (SCM), which is one component or key process area of CMM level 2, is focused in this research for evaluation. This paper is based on an auditor-based survey of 38 Islamabad based software companies. The goal was to find our adherence of these companies to SCM strategies. Some issues are identified, that need to be resolved for successful implementation of SCM. This paper also highlights where Islamabad based software companies rank them with respect to CMM.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1492912,no,undetermined,0
Using Static Analysis to Find Bugs,"Static analysis examines code in the absence of input data and without running the code. It can detect potential security violations (SQL injection), runtime errors (dereferencing a null pointer) and logical inconsistencies (a conditional test that can't possibly be true). Although a rich body of literature exists on algorithms and analytical frameworks used by such tools, reports describing experiences in industry are much harder to come by. The authors describe FindBugs, an open source static-analysis tool for Java, and experiences using it in production settings. FindBugs evaluates what kinds of defects can be effectively detected with relatively simple techniques and helps developers understand how to incorporate such tools into software development.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4602670,no,undetermined,0
The real-time computing model for a network based control system,"This paper studies a network based real-time control system, and proposes to model this system as a periodic real-time computing system. With efficient scheduling algorithms and software fault-tolerance deadline mechanism, this model proves that the system can meet its task timing constraints while tolerating system faults. The simulation study shows that in cases with high failure probability, the lower priority tasks suffer a lot in completion rate. In cases with low failure probability, this algorithm works well with both high priority and lower priority task. This conclusion suggests that an Internet based control system should manage to keep the failure rate to the minimum to achieve a good system performance.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1468843,no,undetermined,0
Model-based performance risk analysis,"Performance is a nonfunctional software attribute that plays a crucial role in wide application domains spreading from safety-critical systems to e-commerce applications. Software risk can be quantified as a combination of the probability that a software system may fail and the severity of the damages caused by the failure. In this paper, we devise a methodology for estimation of performance-based risk factor, which originates from violations, of performance requirements, (namely, performance failures). The methodology elaborates annotated UML diagrams to estimate the performance failure probability and combines it with the failure severity estimate which is obtained using the functional failure analysis. We are thus able to determine risky scenarios as well as risky software components, and the analysis feedback can be used to improve the software design. We illustrate the methodology on an e-commerce case study using step-by step approach, and then provide a brief description of a case study based on large real system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1392717,no,undetermined,0
Adaptive Random Testing,"Summary form only given. Random testing is a basic testing technique. Motivated by the observation that neighboring inputs normally exhibit similar failure behavior, the approach of adaptive random testing has recently been proposed to enhance the fault detection capability of random testing. The intuition of adaptive random testing is to evenly spread the randomly generated test cases. Experimental results have shown that adaptive random testing can use as fewer as 50% of test cases required by random testing with replacement to detect the first failure. These results have very significant impact in software testing, because random testing is a basic and popular technique in software testing. In view of such a significant improvement of adaptive random testing over random testing, it is very natural to consider to replace random testing by adaptive random testing. Hence, many works involving random testing may be worthwhile to be reinvestigated using adaptive random testing instead. Obviously, there are different approaches of evenly spreading random test cases. In this tutorial, we are going to present several approaches, and discuss their advantages and disadvantages. Furthermore, the favorable and unfavorable conditions for adaptive random testing would also be discussed. Most existing research on adaptive random testing involves only numeric programs. The recent success of applying adaptive random testing for non-numeric programs would be discussed.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601575,no,undetermined,0
Path and Context Sensitive Inter-procedural Memory Leak Detection,This paper presents a practical path and context sensitive inter-procedural analysis method for detecting memory leaks in C programs. A novel memory object model and function summary system are used. Preliminary experiments show that the method is effective. Several memory leaks have been found in real programs including which and wget.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601571,no,undetermined,0
Architecture Compliance Checking at Runtime: An Industry Experience Report,"In this paper, we report on our experiences we made with architecture compliance checking at run-time. To that end, we constructed hierarchical colored Petri nets (CP-nets), using existing general purpose functional programming languages, for bridging the abstraction gap between architectural views and run-time traces. In an industry example, we were able to extract views that helped us to identify a number of architecturally relevant issues (e.g., style constraint violations) that would not have been detected otherwise. Finally, we demonstrate how to systematically design reusable hierarchical CP-nets, and package valuable experiences and lessons learned from the example application.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601564,no,undetermined,0
Prototype of fault adaptive embedded software for large-scale real-time systems,"This paper describes a comprehensive prototype of large-scale fault adaptive embedded software developed for the proposed Fermilab BTeV high energy physics experiment. Lightweight self-optimizing agents embedded within Level 1 of the prototype are responsible for proactive and reactive monitoring and mitigation based on specified layers of competence. The agents are self-protecting, detecting cascading failures using a distributed approach. Adaptive, reconfigurable, and mobile objects for reliablility are designed to be self-configuring to adapt automatically to dynamically changing environments. These objects provide a self-healing layer with the ability to discover, diagnose, and react to discontinuities in real-time processing. A generic modeling environment was developed to facilitate design and implementation of hardware resource specifications, application data flow, and failure mitigation strategies. Level 1 of the planned BTeV trigger system alone will consist of 2500 DSPs, so the number of components and intractable fault scenarios involved make it impossible to design an 'expert system' that applies traditional centralized mitigative strategies based on rules capturing every possible system state. Instead, a distributed reactive approach is implemented using the tools and methodologies developed by the Real-Time Embedded Systems group.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1409954,no,undetermined,0
Development life cycle management: a multiproject experiment,"A variety of life cycle models for software systems development are generally available. However, it is generally difficult to compare and contrast the methods and very little literature is available to guide developers and managers in making choices. Moreover in order to make informed decisions developers require access to real data that compares the different models and the results associated with the adoption of each model. This paper describes an experiment in which fifteen software teams developed comparable software products using four different development approaches (V-model, incremental, evolutionary and extreme programming). Extensive measurements were taken to assess the time, quality, size, and development efficiency of each product. The paper presents the experimental data collected and the conclusions related to the choice of method, its impact on the project and the quality of the results as well as the general implications to the practice of systems engineering project management.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1409928,no,undetermined,0
Fault tolerant data flow modeling using the generic modeling environment,"Designing embedded software for safety-critical, real-time feedback control applications is a complex and error prone task. Fault tolerance is an important aspect of safety. In general, fault tolerance is achieved by duplicating hardware components, a solution that is often more expensive than needed. In applications such as automotive electronics, a subset of the functionalities has to be guaranteed while others are not crucial to the safety of the operation of the vehicle. In this case, we must make sure that this subset is operational under the potential faults of the architecture. A model of computation called fault-tolerant data flow (FTDF) was recently introduced to describe at the highest level of abstraction of the design the fault tolerance requirements on the functionality of the system. Then, the problem of implementing the system efficiently on a platform consists of finding a mapping of the FTDF model on the components of the platform. A complete design flow for this kind of application requires a user-friendly graphical interface to capture the functionality of the systems with the FTDF model, algorithms for choosing an architecture optimally, (possibly automatic) code generation for the parts of the system to be implemented in software and verification tools. In this paper, we use the generic modeling environment (GME) developed at Vanderbilt University to design a graphical design capture system and to provide the infrastructure for automatic code generation. The design flow is embedded into the Metropolis environment developed at the University of California at Berkeley to provide the necessary verification and analysis framework.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1409921,no,undetermined,0
IT asset management of industrial automation systems,"The installation and administration of large heterogeneous IT infrastructures, for enterprises as well as industrial automation systems, are becoming more and more complex and time consuming. Industrial automation systems, such as those delivered by ABB Inc., present an additional challenge, in that these control and supervise mission critical production sites. Nevertheless, it is common practice to manually install and maintain industrial networks and the process control software running on them, which can be both expensive and error prone. In order to address these challenges, we believe that in the long term such systems must behave autonomously. As preliminary steps to the realization of this vision, automated IT asset management tools and practices will be highlighted in this contribution. We will point out the advantages of combining process control and network management in the domain of industrial automation technology. Furthermore we will propose a new component model for autonomic network management applied to industrial automation systems.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1409909,no,undetermined,0
Looking for More Confidence in Refactoring? How to Assess Adequacy of Your Refactoring Tests,"Refactoring is an important technique in today's software development practice. If applied correctly, it can significantly improve software design without altering behavior. During refactoring, developers rely on regression testing. However, without further knowledge about the test suite, how can we be confident that regression testing will detect potential refactoring faults? To get more insight into adequacy of refactoring tests, we therefore suggest test coverage of a refactoring's scope of impact as a quantitative measure of confidence. This paper shows how to identify a refactoring's scope of impact and proposes scope-based test coverage criteria. An example is included that illustrates how to use the new test coverage criteria for assessing the adequacy of refactoring tests.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601552,no,undetermined,0
A parallel-line detection algorithm based on HMM decoding,"The detection of groups of parallel lines is important in applications such as form processing and text (handwriting) extraction from rule lined paper. These tasks can be very challenging in degraded documents where the lines are severely broken. In this paper, we propose a novel model-based method which incorporates high-level context to detect these lines. After preprocessing (such as skew correction and text filtering), we use trained hidden Markov models (HMM) to locate the optimal positions of all lines simultaneously on the horizontal or vertical projection profiles, based on the Viterbi decoding. The algorithm is trainable so it can be easily adapted to different application scenarios. The experiments conducted on known form processing and rule line detection show our method is robust, and achieves better results than other widely used line detection methods.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407880,no,undetermined,0
How to Measure Quality of Software Developed by Subcontractors (Short Paper),"In Japan, where the multiple subcontracting is very common in software development, it is difficult to measure the quality of software developed by subcontractors. Even if we request them for process improvement based on CMMI including measurement and analysis, it will not work immediately. Using """"the unsuccessful ratio in the first time testing pass"""" as measure to assess software quality, we have had good results. We can get these measures with a little effort of both outsourcer and subcontractor. With this measure, we could identify the defect-prone programs and conduct acceptance testing for these programs intensively. Thus, we could deliver the system on schedule. The following sections discuss why we devised this measure and its trial results.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601558,no,undetermined,0
Recovering Internet service sessions from operating system failures,"Current Internet service architectures lack support for salvaging stateful client sessions when the underlying operating system fails due to hangs, crashes, deadlocks, or panics. The backdoors (BD) system is designed to detect such failures and recover service sessions in clusters of Internet servers by extracting lightweight state associated with client service sessions from server memory. The BD architecture combines hardware and software mechanisms to enable accurate monitoring and remote healing actions, even in the presence of failures that render a system unavailable.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1405970,no,undetermined,0
Information stream oriented content adaptation for pervasive computing,"Content adaptation is a challenging work due to the dynamism and heterogeneity of the pervasive computing environments. Some researchers address this issue by organizing services into customized applications dynamically. However, due to the maintenance of the dependencies between services, these systems become more complicated with the growing of the system scale. Programming for these systems is also error-prone. This paper introduces our work in this field, UbiCon, an information streams oriented content adaptation system. By abstracting information streams into generic CONTENT entities, the system provides a simple and powerful means for services to operate information stream. The CONTENT is created by the system dynamically, and essentially has local association with related services. As a result, the CONTENT is also used as a loosely coupling mechanism for cooperating associated services. By abstracting services with a T model, the services effectively cooperate together with other services. As a result, a collection of sophisticated applications can be built with this services model. As a proof of concept, we have developed a prototype implementation. The preliminary experiments show the effectiveness of this system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402378,no,undetermined,0
Anomaly detection in Web applications: a review of already conducted case studies,"The quality of Web applications is everyday more important. Web applications are crucial vehicles for commerce, information exchange, and a host of social and educational activities. Since a bug in a Web application could interrupt an entire business and cost millions of dollars, there is a strong demand for methodologies, tools and models that can improve the Web quality and reliability. Aim of our ongoing-work has been to investigate, define and apply a variety of analysis and testing techniques able to support the quality of Web applications. Validity of our solutions was assessed by extensive empirical work. A critical review of this five year long work shows that only 40% of the randomly selected real-world Web applications exhibit no anomalies/failures. Some tables reported in this paper summarize the relations between type of anomalies found and analyses applied. We are in need of better methodologies, techniques and tools for developing, maintaining and testing Web applications.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402156,no,undetermined,0
A Tool for Static and Dynamic Model Extraction and Impact Analysis,"Planning changes is often an imprecise task and implementing changes is often time consuming and error prone. One reason for these problems is inadequate support for efficient analysis of the impacts of the performed changes. The paper presents a technique, and associated tool, that uses a mixed static and dynamic model extraction for supporting the analysis of the impacts of changes.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402130,no,undetermined,0
Towards a Method for Evaluating the Precision of Software Measures (Short Paper),"Software measurement currently plays a crucial role in software engineering given that the evaluation of software quality depends on the values of the measurements carried out. One important quality attribute is measurement precision. However, this attribute is frequently used indistinctly and confused with accuracy in software measurement. In this paper, we clarify the meaning of precision and propose a method for assessing the precision of software measures in accordance with ISO 5725. This method was used to assess a functional size measurement procedure. A pilot study was designed for the purpose of revealing any deficiencies in the design of our study.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601559,no,undetermined,0
ADAMS Re-Trace: A Traceability Recovery Tool,"We present the traceability recovery tool developed in the ADAMS artefact management system. The tool is based on an Information Retrieval technique, namely Latent Semantic Indexing and aims at supporting the software engineer in the identification of the traceability links between artefacts of different types. We also present a case study involving seven student projects which represented an ideal workbench for the tool. The results emphasise the benefits provided by the tool in terms of new traceability links discovered, in addition to the links manually traced by the software engineer. Moreover, the tool was also helpful in identifying cases of lack of similarity between artefacts manually traced by the software engineer, thus revealing inconsistencies in the usage of domain terms in these artefacts. This information is valuable to assess the quality of the produced artefacts.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402112,no,undetermined,0
Detecting indirect coupling,"Coupling is considered by many to be an important concept in measuring design quality There is still much to be learned about which aspects of coupling affect design quality or other external attributes of software. Much of the existing work concentrates on direct coupling, that is, forms of coupling that exists between entities that are directly related to each other. A form of coupling that has so far received little attention is indirect coupling, that is, coupling between entities that are not directly related. What little discussion there is in the literature suggests that any form of indirect coupling is simple the transitive closure of a form of direct coupling. We demonstrate that this is not the case, that there are forms of indirect coupling that cannot be represented in this way and suggest ways to measure it. We present a tool that identifies a particular form of indirect coupling that is integrated in the Eclipse IDE.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402016,no,undetermined,0
Obtaining probabilistic dynamic state graphs for TPPAL processes,"Software engineers work gladly with process algebras, as they are very similar to programming languages. However, graphical models are better in order to understand how a system behaves, and even these graphical models allow us to analyze some properties of the systems. Then, in this paper we present two formalisms for the specification of concurrent systems. On the one hand we present the timed-probabilistic process algebra TPPAL, which is a suitable model for description of systems in which time and probabilities are two important factors to be considered in the description, as it occurs in real-time systems and fault-tolerant systems. Then, the specification written in TPPAL can be automatically translated into a graphical model (the so-called probabilistic dynamic state graphs), which allows us to simulate and evaluate the system. Thus, in this paper we present this translation, which is currently supported by a tool (TPAL).",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402007,no,undetermined,0
Development and performances of a dental digital radiographic system using a high resolution CCD image sensor,"Dental digital radiographic (DDR) system using a high resolution charge-coupled device (CCD) imaging sensor was developed and the performances of this system for dental clinic imaging was evaluated. In order to determine the performances of the system, the modulation transfer function (MTF), the signal to noise ratio according to X-ray exposure, the dose reduction effects and imaging quality of the system were investigated. This system consists of a CCD imaging sensor (pixel size: 22 mum) to detect X-ray, an electrical signal processing circuit and a graphical user interface software to display the images and diagnosis. The MTF was obtained from a Fourier transform of the line spread function (LSF), which was itself derived from the edge spread function (ESF) of a sharp edge image acquired. The spatial resolution of the system was measured at a 10% contrast in terms of the corresponding MTF value and the distance between the X-ray source and the CCD image sensor was fixed at 20 cm. The best image quality obtained at the exposure conditions of 60 kVp, 7 mA and 0.05 sec. At this time, the signal to noise ratio and X-ray dose were 23 and 41% (194 muGy) of a film-based method (468 muGy). The spatial resolution of this system, the result of MTF, was approximately 12 line pairs per mm at the 0.05 exposure time. Based on the results, the developed DDR system using a CCD imaging sensor could be suitably applied for intra-oral radiographic imaging because of its low dose, real time acquisition, no chemical processing, image storage and retrieval etc.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1462556,no,undetermined,0
PD diagnosis on medium voltage cables with oscillating voltage (OWTS),"Detecting, locating and evaluating of partial discharges (PD) in the insulating material, terminations and joints provides the opportunity for a quality control after installation and preventive detection of arising service interruption. A sophisticated evaluation is necessary between PD in several insulating materials and also in different types of terminations and joints. For a most precise evaluation of the degree and risk caused by PD it is suggested to use a test voltage shape that is preferably like the same under service conditions. Only under these requirements the typical PD parameters like inception and extinction voltage, PD level and PD pattern correspond to significant operational values. On the other hand the stress on the insulation should be limited during the diagnosis to not create irreversible damages and thereby worsening the condition of the test object. The paper introduces an oscillating wave test system (OWTS), which meets these mentioned demands well. The design of the system, its functionality and especially the operating software are made for convenient field application. Field data and experience reports was presented and discussed. This field data serve also as good guide for the level of danger to the different insulating systems due to partial discharges.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1460088,no,undetermined,0
Service quality of travel agents: the case of travel agents in China,"Travel agents in China have faced difficult times in recent years because of increasing customer demands and internal competition in the industry. A China Consumer Council report (2003) stated that complaints against travel agencies had increased by 10.6% for the year 2002/2003 as compared with the previous year. The purpose of the study was to assess customers' expectations and perceptions of service provided by travel agents, and to explore how the service factors derived from the factor analysis were related to overall customer satisfaction. The results showed that customers' perceptions of service quality fell short of their expectations, with the reliability dimension having the largest gap. Five factors were derived from the factor analysis of 25 service attributes, and the result of factor analysis showed that overall customer satisfaction was related to these five factors.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1499530,no,undetermined,0
Automatic red-eye detection and removal,"Red-eye is a very common problem in flash photography which can ruin a good photo by introducing color aberration into the subject's eyes. Previous methods to deal with this problem include special speedlight apparatus or flash mode that can reduce the red-eye effect, as well as post-capture red-eye correction software. The paper presents a new approach to detecting and correcting red-eye defects automatically by combining flash and non-flash digital images. It is suitable to be incorporated into compact digital cameras that support continuous shooting. Such a camera would eliminate red-eye immediately after image capture. Unlike existing approaches, our method is simple, fast and can recover the true color of the eyes",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1394434,no,undetermined,0
Automatic pixel-shift detection and restoration in videos,"A common form of serious defect in video is pixel-shift. It is caused by the consecutive pixels loss introduced by video transmission systems. Pixel-shift means a large amount of pixel shifts one by one due to a small quantity of image data loss. The damaged region in affected frame is usually large, and thus the visual effect is often very disturbing. So far there is no method of automatically treating pixel-shift. This paper focuses on a difficult issue to locate pixel-shift in videos. We propose an original algorithm of automatically detecting and restoring pixel-shift. Pixel-shift frames detection relies on spatio-temporal information and motion estimation. Accurate measure of pixels shift is best achieved based on the analysis of temporal-frequency information. Restoration is accomplished by reversing the pixels shift and spatio-temporal interpolation. Experimental results show that our completely automatic algorithm can achieve very good performances.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4607741,no,undetermined,0
Effective self-test routine for on-line testing of processors implemented in harsh environments,"Today, it is a common practice to test commercial off-the-shelf (COTS) processors with self-test routines. Faults in processors may cause failure in self-test routine execution, which is one of the essential disadvantages of these routines. In this paper, we present an effective register transfer level (RTL) method to develop on-line self-test routines. Our proposed method prioritizes components and instructions of processor to select instructions, and applies spectral RTL test pattern generation (TPG) strategy to select test patterns. This method analyzes the spectrum and the noise level with Walsh functions. Also, we use a few extra instructions for the purpose of the signature monitoring to detect control flow errors. We demonstrate that the combination of these three strategies is effective for developing small test programs with high fault coverage in a small test development time. This approach requires only instruction set architecture (ISA) and RTL information of the processors. Since proposed method is based on RTL test generation, it has the advantages of lower memory and test generation time complexities. We develop a self-test routine using our proposed method for Parwan processor and demonstrate the effectiveness of our proposed methodology for on-line testing by presenting experimental results for Parwan processor.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4608338,no,undetermined,0
The use of impedance measurement as an effective method of validating the integrity of VRLA battery production,The response of batteries to an injection of AC current to give an indication of battery state-of-health has been well established and extensively reported over a number years but it has been used largely as a means of assessing the condition of batteries in service. In this paper the use of impedance measurement as a quality assurance procedure during manufacture will be described. There are a number of commercially available meters that are used for monitoring in field operations but they have not been developed for use in a manufacturing environment. After extensive laboratory testing a method specifically designed for impedance measurement system at the end of manufacturing lines in order to assure higher product integrity was devised and validated. A special testing station was designed for this purpose and includes battery conditioning prior to the test and sophisticated software driven data analysis in order to increase the effectiveness and reliability of the measurement. The paper reports the analysis of the data collected over two years of monitoring of the entire production with the impedance testing station at the end of the production line. Data comparison with earlier production shows an increase in the number of batteries scrapped internally and correspondingly a reduction in the number of defective products reaching distribution centres and also the field. The accumulated experience has also been very helpful in getting better information about the effect of the various parameters that affect the measured impedance value and this will assist in improving the reliability of impedance measurements in field service.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1401531,no,undetermined,0
A real-time network simulation application for multimedia over IP,"This paper details a secure voice over IP (SVoIP) development tool, the network simulation application (Netsim), which provides real-time network performance implementation of quality of service (QoS) statistics in live real-time data transmission over packet networks. This application is used to implement QoS statistics including packet loss and inter-arrival delay jitter. Netsim is written in Visual C++ using MFC for MS Windows environments. The program acts as a transparent gateway for a SVoIP server/client pair connected via IP networks. The user specifies QoS parameters such as mean delay, standard deviation of delay, unconditional loss probability, conditional loss probability, etc. Netsim initiates and accepts connection, controls packet flow, records all packet sending / arrival times, generates a data log, and performs statistical calculations.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1399567,no,undetermined,0
An Analysis of Missed Structure Field Handling Bugs,"Despite the importance and prevalence of structures (or records) in programming, no study till now has deeply analyzed the bugs made in their usage. This paper makes a first step to fill that gap by systematically and deeply analyzing a subset of structure usage bugs. The subset, referred to as MSFH bugs, are errors of omission associated with structure fields when they are handled in a grouped context. We analyze the nature of these bugs by providing a taxonomy, root cause analysis, and barrier analysis. The analysis provided many new insights, which suggested new solutions for preventing and detecting the MSFH bugs.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4609409,no,undetermined,0
Finding Narrow Input/Output (NIO) Sequences by Model Checking,"Conformance test sequences for communication protocols specified by finite state machines (FSM)often use unique input/output (UIO) sequences to detect state transition transfer faults. Since a UIO sequence may not exist for every state of an FSM, in the previous research, we extended UIO sequence to introduce a new concept called narrow input/output(NIO) sequence. The general computation of NIO sequences may lead to state explosion when an FSM is very large. In this paper, we present an approach to find NIO sequences using symbolic model checking.Constructing a Kripke structure and a computation tree logic (CTL) formula for such a purpose is described in detail. We also illustrate the method using a model checker SMV.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4609436,no,undetermined,0
Coping with unreliable channels: Efficient link estimation for low-power wireless sensor networks,"The dynamic nature of wireless communication and the stringent energy constraints are major challenges for the design of low-power wireless sensor network applications. The link quality of a wireless link is known for its great variability, dependent on the distance between nodes, the antennapsilas radiation characteristic, multipath, diffraction, scattering and many more. Especially for indoor and urban deployments, there are numerous factors impacting the wireless channel. In an extensive experimental study contained in the first part of this paper, we show the magnitude of this problem for current Wireless Sensor Networks (WSNs) and that based on the overall connectivity graph of a typical multihop WSN, a large portion of the links actually exhibit very poor characteristics. We present a pattern based estimation technique that allows assessing the quality of a link at startup and as a result to construct an optimal neighbor table right at the beginning using a minimum of resources only. Our estimation technique is superior compared to other approaches where protocols continue to decide on the fly which links to use expending valuable energy both for unnecessary retransmissions and recursive link estimation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4610885,no,undetermined,0
Hardware accelerated Scalable Parallel Random Number Generators for Monte Carlo methods,"Monte Carlo methods often demand the generation of many random numbers to provide statistically meaningful results. Because generating random numbers is time consuming and error-prone, the Scalable Parallel Random Number Generators (SPRNG) library is widely used for Monte Carlo simulation. SPRNG supports fast, scalable random number generation with good statistical properties. In order to accelerate SPRNG, we develop a hardware accelerated version of SPRNG that produces identical results. To demonstrate HASPRNG for Reconfigurable Computing (RC) applications, we develop a Monte Carlo pi-estimator for the Cray XD1 and XUP platforms. The RC MC pi-estimator shows 8.1 times speedup over the 2.2 GHz AMD Opteron processor in the Cray XD1.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4616765,no,undetermined,0
A Novel Watermarking-Based Reversible Image Authentication Scheme,"Nowadays, digital watermarking algorithms are widely applied to ownership protection and tampering detection of digital images. In this paper, we propose a novel reversible image authentication scheme based on watermarking techniques, which is employed to protect the rightful ownership and detect malicious manipulation over embedded images. In our scheme, the original image is firstly split into many non- overlapping blocks, then one-way function MD5 is used to compute the digest of every block and the digest is then inserted into the least significant bits of some selected pixels, the experimental results show that the proposed scheme is quite simple and the execution time is short. Moreover, the quality of the embedded image is very high, and if the image is authentic, the distortion due to embedding can be completely removed from the watermarked image after the hidden data has been extracted, in the meantime, the positions of the tampered parts are located correctly.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4604274,no,undetermined,0
Language and Compiler Support for Adaptive Applications,"There exist many application classes for which the users have significant flexibility in the quality of output they desire. At the same time, there are other constraints, such as the need for real-time response or limit on the consumption of certain resources, which are more crucial. This paper provides a combined language/compiler and runtime solution for supporting adaptive execution of these applications, i.e., to allow them to achieve the best precision while still meeting the specified constraint at runtime. The key idea in our language extensions is to have the programmers specify adaptation parameters, i.e, the parameters whose values can be varied within a certain range. A program analysis algorithm states the execution time of an application component as a function of the values of the adaptation parameters and other runtime constants. These constants are determined by initial runs of the application in the target environment. We integrate this work with our previous work on supporting coarse-grained pipelined parallelism, and thus support adaptive execution for data-intensive applications in a distributed environment. Our experimental results on three applications have shown that our combined compile-time/runtime model can predict the execution times quite well, and therefore, support adaptation to meet a variety of constraints.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1392959,no,undetermined,0
Auto-coding/auto-proving flight control software,"This work describes the results of an experiment to compare conventional software development with software development using automatic code generation from Simulink and mathematically based code verification (proof). A real industrial scale, safety critical system was used as the basis for the experiment in order to validate results, although this imposed some constraints. The principal aims for the experiment were to answer the following three questions. 1. Could automatic code generation be integrated with the verification tools to give a software development process to produce software that would pass the existing functional unit tests? 2. Would the code be of sufficient quality to be flown, i.e. was it certifiable? 3. What were the cost implications of adopting the process as part of a development lifecycle? The experiment showed how to integrate the techniques into existing development processes and indicated where processes could be streamlined. The code and the technique were independently assessed as being certifiable for safety critical applications. The results of the experiment were generally positive indicating the potential for reductions of 60%-70% of the software development costs alone, that would translate into a 30%-40% reduction in software life cycle costs.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1390739,no,undetermined,0
Simulation based system level fault insertion using co-verification tools,"This work presents a simulation-based, fault insertion environment, which allows faults to be """"injected"""" into a Verilog model of the hardware. A co-verification platform is used to allow real, system level software to be executed in the simulation environment. A fault manager is used to keep track of the faults that are inserted on to the hardware and to monitor diagnostic messages to determine whether the software is able to detect, diagnose and/or cope with the injected fault. Examples be provided to demonstrate the capabilities of this approach as well as the resource requirements (time, system, human). Other benefits and issues of this approach also be discussed.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1387332,no,undetermined,0
Low overhead delay testing of ASICs,"Delay testing has become increasingly essential as chip geometries shrink. Low overhead or cost effective delay test methodology is successful when it results in a minimal number of effective tests and eases the demands on an already burdened IC design and test staff. This work describes one successful method in use by IBM ASICs that resulted in a slight total test pattern increase, generally ranging between 10 and 90%. Example ICs showed a pattern increase of as little as 14% from the stuck-at fault baseline with a transition fault coverage of 89%. In an ASIC business, a large number of ICs are processed, which does not allow for the personnel to understand how to test each individual IC design in detail. Instead, design automation software that is timing and testability aware ensures effective and efficient tests. The resultant tests detect random spot timing delay defects. These types of defects are time zero related failures and not reliability wearout mechanisms.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1386990,no,undetermined,0
New test paradigms for yield and manufacturability - Invited address,"Summary form only given, as follows. Test holds the key to success in the rapidly changing world of process technology and design complexity as well as the fashionable area of Design-for-Manufacturability (DFM). As test chips become prohibitively expensive and less statistically valid, it is only through statistical analysis of volume product test data that we can assess the improvements in parametric variation and in defectivity necessary in both the design and the manufacturing processes to meet the yield and supply chain targets for complex ICs. Volume test data is also key to the implementation of adaptive testing. Probabilistic decisionmaking can be applied to the test flows to reduce test costs and improve test quality by specifically targeting the parameters and defects that are likely to cause failures and reduce unnecessary testing and burn-in of defect-free die. Some key paradigms in the test world have to change, however, if test is to keep up with these challenges or test will continue to be relegated to the """"non-valueadded"""" category that has been a long-standing barrier to investment in test equipment and software.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1386930,no,undetermined,0
A New Vocoder based on AMR 7.4kbit/s Mode in Speaker Dependent Coding System,"A new code excited linear predictive (CELP) vocoder based on Adaptive Multi Rate (AMR) 7.4 kbit/s mode is proposed in this paper. The proposed vocoder achieves a better compression rate in an environment of Speaker Dependent Coding System (SDSC) and is efficiently used for systems, such as OGM (Outgoing message) and TTS (Text To Speech), that stores the speech data of a particular speaker. In order to enhance the compression rate of a coder, a new Line Spectral Pairs (LSP) codebook is employed by using Centroid Neural Network (CNN) algorithm. Moreover, applying the predicted pulses used in fixed code book searching enhances the quality of synthesis speech. In comparison with original (traditional) AMR 7.4 Kbit/s coder, the new coder shows a superior compression rate and an equivalent quality to AMR coder in term of informal subjective testing Mean Opinion Score(MOS).",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4617365,no,undetermined,0
Preliminary results on using static analysis tools for software inspection,"Software inspection has been shown to be an effective defect removal practice, leading to higher quality software with lower field failures. Automated software inspection tools are emerging for identifying a subset of defects in a less labor-intensive manner than manual inspection. This paper investigates the use of automated inspection for a large-scale industrial software system at Nortel Networks. We propose and utilize a defect classification scheme for enumerating the types of defects that can be identified by automated inspections. Additionally, we demonstrate that automated code inspection faults can be used as efficient predictors of field failures and are effective for identifying fault-prone modules.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383137,no,undetermined,0
HGRID: An Adaptive Grid Resource Discovery,"Grid resource discovery service is a fundamental problem that has been the focus of research in the recent past. We propose a scheme that has essential characteristics for efficient, self-configuring and fault-tolerant resource discovery and is able to handle dynamic attributes, such as memory capacity. Our approach consists of an overlay network with a hypercube topology connecting the grid nodes and a scalable, fault-tolerant, self-configuring and adaptive search algorithm. Every grid node keeps a small routing table of only log<sub>2</sub>N entries. The search algorithm is executed in less than (log<sub>2</sub>N +1) time steps and each grid node is queried only once. By design, the algorithm improves the probability of reaching all working nodes in the system even in the presence of non-alive nodes (inaccessible, crashed or heavily loaded nodes). We analyze the static resilience of the approach presented, which is the measure of how well the algorithm can discover resources without having to update the routing tables. This is done before the routing recovery is processed in order to reconfigure the overlay to avoid non-alive nodes. The results show that our approach has a significantly high static resilience for a grid environment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4606712,no,undetermined,0
Teaming assessment: is there a connection between process and product?,"It is reasonable to suspect that team process influences the way students work, the quality of their learning and the excellence of their product. This study addresses the relations between team process variables on the one hand, and behaviors and outcomes, on the other. We measured teaming skill, project behavior and performance, and project product grades. We found that knowledge of team process predicts team behavior, but that knowledge alone does not predict performance on the project. Second, both effort and team skills, as assessed by peers, were related to performance. Third, team skills did not correlate with the students' effort. This pattern of results suggests that instructors should address issues of teaming and of effort separately. It also suggests that peer ratings of teammates tap aspects of team behavior relevant to project performance, whereas declarative knowledge of team process does not.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1408762,no,undetermined,0
Introducing prob-a-sag - a probabilistic method for voltage sag management,"Comprehensive voltage sag management in a power distribution system includes the technical and economic impact of sags, annual frequency of sags, as well as the effect of possible mitigative means. A probabilistic approach is required for performing the task in complicated industrial processes. Prob-a-sag is a novel method combining all these features. Two-dimensional arrays are used for expressing the quantities and carrying out the analysis or optimization. Equipment sag sensitivity is expressed as tripping probability, which enables the probabilistic sensitivity assessment of a large process. The method is very flexible; increasing the array resolution improves the result precision. If sag quantities other than depth and duration are preferred, we may increase the number of array dimensions accordingly. Prob-a-sag is compatible with any spreadsheet application, or may be implemented in sophisticated network analysis software.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1409362,no,undetermined,0
GXP : An Interactive Shell for the Grid Environment,"We describe GXP, a shell for distributed multi-cluster environments. With GXP, users can quickly submit a command to many nodes simultaneously (approximately 600 milliseconds on over 300 nodes spread across five local-area networks). It therefore brings an interactive and instantaneous response to many cluster/network operations, such as trouble diagnosis, parallel program invocation, installation and deployment, testing and debugging, monitoring, and dead process cleanup. It features (1) a very fast parallel (simultaneous) command submission, (2) parallel pipes (pipes between local command and all parallel commands), and (3) a flexible and efficient method to interactively select a subset of nodes to execute subsequent commands on. It is very easy to start using GXP, because it is designed not to require cumbersome per-node setup and installation and to depend only on a very small number of pre-installed tools and nothing else. We describe how GXP achieves these features and demonstrate through examples how they make many otherwise boring and error-prone tasks simple, efficient, and fun",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410681,no,undetermined,0
Sonar image quality assessment for an autonomous underwater vehicle,"Within the EU-funded project """"ADVOCATE II"""" the participating partners are developing an advanced machine diagnosis system for autonomous systems, which is based on an integrated approach. The solution combines different """"intelligent"""" modules to create the open software architecture for diagnosis and decision tasks. ATLAS ELEKTRONIK is going to integrate the ADVOCATE modules into an autonomous underwater vehicle (AUV), which must rely on an automatic obstacle avoidance system, based on sonar image processing. Beside typical electronic failures there is the possibility that the image quality is not sufficient for reliable obstacle recognition. The AUV needs to know this fact to react in an appropriate manner. To solve this sonar image assessment problem, a Bayesian belief module (BBN) has been developed. The BBN module is based on the AI technique known as probabilistic graphical models (PGMs). In particular, a time-sliced, object-oriented limited-memory influence diagram is used as the underlying PGM of the BBN module. The BBN module provides a diagnosis and suggests appropriate recovery actions on the sonar image assessment task",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1438526,no,undetermined,0
Investigation of pushback based detection and prevention of network bandwidth attacks,Pushback approach has been applied for the detection and prevention against DDoS attacks by identifying the destination IP addresses in the dropped packets when congestion happens. The identified destination IP addresses are used to guide the subsequent packet dropping at both local router and upstream routers so that the total bandwidth can be controlled within a desired range. This paper investigates an application of pushback approach for the detection and prevention of more general network bandwidth attacks based on the profiles of destination port distribution instead of destination IP addresses. The new approach can be used to detect and prevent against the attacks like Internet worms. The investigation applies the long trace dataset of NLANR - CESCA-I and an Internet Worm Propagation simulator to simulate the generation of profiles and the detection of the Internet CodeRed worm. The dataset statistics and simulation results demonstrate the effectiveness of the new approach in the detection and prevention of Internet worms.,2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1437847,no,undetermined,0
On the Trend of Remaining Software Defect Estimation,"Software defects play a key role in software reliability, and the number of remaining defects is one of most important software reliability indexes. Observing the trend of the number of remaining defects during the testing process can provide very useful information on the software reliability. However, the number of remaining defects is not known and has to be estimated. Therefore, it is important to study the trend of the remaining software defect estimation (RSDE). In this paper, the concept of RSDE curves is proposed. An RSDE curve describes the dynamic behavior of RSDE as software testing proceeds. Generally, RSDE changes over time and displays two typical patterns: 1) single mode and 2) multiple modes. This behavior is due to the different characteristics of the testing process, i.e., testing under a single testing profile or multiple testing profiles with various change points. By studying the trend of the estimated number of remaining software defects, RSDE curves can provide further insights into the software testing process. In particular, in this paper, the Goel-Okumoto model is used to estimate this number on actual software failure data, and some properties of RSDE are derived. In addition, we discuss some theoretical and application issues of the RSDE curves. The concept of the proposed RSDE curves is independent of the selected model. The methods and development discussed in this paper can be applied to any valid estimation model to develop and study its corresponding RSDE curve. Finally, we discuss several possible areas for future research.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4604821,no,undetermined,0
A generic interface modeling approach for SOC design,"One of the key problems in IP-centric SOC design is integration between different IP cores. Since most IPs have different interface schemes and operation rules: they cannot smoothly communicate each other without any auxiliary glue logic. Furthermore, integration of IPs with different protocols is still a tedious error-prone task. To achieve the goal that make the most of IP reuse and smooth IP cores to communicate, this paper presents a generic language-independent interface modeling approach to assist interface synthesis. Given two different communication protocols, an algorithm is developed to generate synthesizable RTL code of interface subsystem on basis of the proposed interface model for IP integration. The novel algorithm can produce multi-language code such as Verilog, VHDL and SpeeC, which can be used as input for a synthesis tools. The proposed approach has been successfully integrated into our interface synthesis tool.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436823,no,undetermined,0
On-chip testing of embedded silicon transducers,"System-on-chip (SoC) technologies are evolving towards the integration of highly heterogeneous devices, including hardware of a different nature, such as digital, analog and mixed-signal, together with software components. Embedding transducers, as predicted by technology roadmaps, is yet another step in this continuous search for higher levels of integration and miniaturisation. Embedded transducers fabricated with silicon/CMOS compatible technologies may have more limitations than transducers fabricated with fully dedicated technologies. However, they offer industry the possibility of providing low cost applications for very large market niches, while still keeping acceptable transducer sensitivity. This is the case, for example, for accelerometers, micro-mirrors display devices or CMOS imagers. Embedded transducers are analog components. However, given the fact that they work with signals other than electrical, the test of these embedded parts poses new challenges. Test technology for SoC devices is rapidly maturing but many difficulties still remain, in particular for addressing the test of analog and mixed-signal parts. In this paper, we present our work in the field of MEMS (micro-electro-mechanical systems) on-chip testing with a brief overview of the state-of-the-art.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1434190,no,undetermined,0
A mathematical morphological method to thin edge detection in dark region,"The performance of image segmentation depends on the output quality of the edge detection process. Typical edge detecting method is based on detecting pixels in an image with high gradient values, and then applies a global threshold value to extract the edge points of the image. By these methods, some detected edge points may not belong to the edge and some thin edge points in dark regions of the image are being eliminated. These eliminated edges may be with important features of the image. This paper proposes a new mathematical morphological edge-detecting algorithm based on the morphological residue transformation derived from dilation operation to detect and preserve the thin edges. Moreover, this work adopts five bipolar oriented edge masks to prune the miss detected edge points. The experimental results show that the proposed algorithm is successfully to preserve the thin edges in the dark regions.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1433746,no,undetermined,0
ZigBee technology applied to supervisory system of boiler welding quality,"The welding process, detected parameters and its suitable application feature and requirement of boiler have been introduced, the communication technology, protocol and characteristic have been dissected, the design principle of wireless network platform has been discussed, the implementation details of the system software have been analyzed, and the certain problem related to industrial application have been approached.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4605486,no,undetermined,0
Instruction level test methodology for CPU core software-based self-testing,"TIS (S. Shamshiri et al., 2004) is an instruction level methodology for CPU core self-testing that enhances the instruction set of a CPU with test instructions. Since the functionality of test instructions is the same as the NOP instruction, NOP instructions can be replaced with test instructions so that online testing can be done with no performance penalty. TIS tests different parts of the CPU and detects stuck-at faults. This method can be employed in offline and online testing of all kinds of processors. Hardware-oriented implementation of TIS was proposed previously (S. Shamshiri et al., 2004) that tests just the combinational units of the processor. Contributions of this paper are first, a software-based approach that reduces the hardware overhead to a reasonable size and second, testing the sequential parts of the processor besides the combinational parts. Both hardware and software oriented approaches are implemented on a pipelined CPU core and their area overheads are compared. To demonstrate the appropriateness of the TIS test technique, several programs are executed and fault coverage results are presented.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431227,no,undetermined,0
Rate control for random access networks: The finite node case,"We consider rate control for random access networks. In earlier work, we proposed a rate control mechanism which we analyzed using the well-known slotted Aloha model with an infinite set of nodes. In this paper, we extend this work to the finite node case and analyze two different packet-scheduling schemes: a backlog-dependent scheme where the retransmission probabilities depend on the total number of a backlogged packets at a given node, and a backlog-independent scheme. Using a Markov chain model, we derive conditions under which the rate control stabilizes the network. We also discuss how this mechanism can be used to provide differentiated quality-of-service both in terms of throughput and delay. We use numerical case studies to illustrate our results.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1428895,no,undetermined,0
Fault detection in model predictive controller,"Real-time monitoring and maintaining model predictive controller (MPC) is becoming an important issue with its wide implementation in the industries. In this paper, a measure is proposed to detect faults in MFCs by comparing the performance of the actual controller with the performance of the ideal controller. The ideal controller is derived from the dynamic matrix control (DMC) in an ideal work situation and treated as a measure benchmark. A detection index based on the comparison is proposed to detect the state change of the target controller. This measure is illustrated through the implementation for a water tank process.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1426773,no,undetermined,0
Choosing best basis in wavelet packets for fingerprint matching,"Fingerprint matching has been deployed in a variety of security related applications. Traditional minutiae detection based identification algorithms do not utilize the rich discriminatory texture structure of fingerprint images. Furthermore, minutiae detection requires substantial improvement of image quality and is thus error-prone. In this paper, we propose a new algorithm for fingerprint identification using wavelet packet analysis and best basis selection. Each fingerprint is decomposed using two dimensional wavelet packet family corresponding to different scales. The energy distribution of the fingerprint in each subband is extracted as a feature for identification. Wavelet packet decomposition yields a redundant representation of the image. For this reason, several algorithms for selecting the best basis from this redundant representation have been investigated. In this paper, we propose a new method for choosing best basis in wavelet packets for fingerprint matching. Experiments show that our new algorithm improves the accuracy of fingerprint matching.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1419724,no,undetermined,0
A highly tunable radio frequency filter using bulk ferroelectric materials,"The desirable attribute of software defined radios (SDR) implies that RF Front-ends must be multi-band and frequency agile. Wideband SDRs need a lower size, weight, and power (SWAP) tunable filter technology to meet the military's current and future communications needs. This is essential for the SDR operating in a battery powered environment such as man-portable Joint Tactical Radios (JTRS). The current method for building a tunable filter is to use discrete varactors at each section of the filter. These devices are nonlinear and their low third order intercept points make them highly susceptible to intermodulation which severely limits their dynamic range. Dielectric ceramics are inherently lossy and prone to breakdown due to the presence of materials defects. In this paper we will present a new approach to construction of tunable RF filters using low loss bulk ceramic high dielectric constant Barium Strontium Titanate (BST) ceramics. These ceramic compositions exhibit large change in dielectric constant with applied electric field. A prototype tunable filter has been built using these materials exhibiting 3:1 permittivity tunability. We will present a lumped circuit filter design for 30-450 MHz using the tunability of these paraelectric capacitors. The circuit design and experimental results along with the achievement of 1.7:1 frequency tunability of these filters will be shown.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1418329,no,undetermined,0
Analysis of ultrasonic wave propagation in metallic pipe structures using finite element modelling techniques,"This paper describes the development of a FEM representing ultrasonic inspection in a metallic pipe. The model comprises two wedge transducer components, water coupled onto the inner wall of a steel pipe and configured to generate/receive ultrasonic shear waves. One device is used in pulse-echo mode to analyse any reflected components within the system, with the second transducer operating in a passive mode. A number of simple defect representations have been incorporated into the model and both the reflected and transmitted wave components acquired at each wedge. Both regular crack and lamination defects have been investigated, at 3 different locations to evaluate the relationship between propagation path length and defect response. These responses are analysed in both the time and frequency domains. Moreover, the FEM has produced visual interpretation, in the form of a movie simulation, of the interaction between the propagating pressure wave and the defect. A combination of these visual aids and the predicted temporal/spectral waveforms has demonstrated fundamental differences in the response from either a crack or lamination defect.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1417911,no,undetermined,0
Effect of fault dependency and debugging time lag on software error models,"In this paper, we first show how several existing SRGMs based on NHPP models can be comprehensively derived by applying the time-dependent delay function. Moreover, for most conventional SRGMs, they assume that detected errors are immediately corrected. But this assumption may not be realistic in practice. Therefore, we incorporate the ideas of failure dependency and time-dependent delay function into software reliability growth modeling. New SRGMs are proposed and numerical illustrations based on real data set are presented. Evaluation results show that the proposed framework to incorporate both failure dependency and time-dependent delay function for SRGM has a fairly accurate prediction capability.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1414576,no,undetermined,0
Observations on the implementation and testing of scripted Web applications,"Scripting languages have become a very popular choice for implementing server-side programs in Web applications. Scripting languages are thought to provide quick start up and enhance programmer productivity. We present two case studies in which scripting languages were used. In both studies, the projects struggled with implementation; however, project factors such as the strength of management and the training of the development team are thought to out weigh the choice of programming language in terms of impact on project success. The choice to implement a Web application with a scripting language can lead to undisciplined behavior on the part of management and the development team, so caution must be exercised when implementing complex applications. Testers of scripted implementations should adjust their risk profile to match the error-prone aspects of the language. Dynamically type checked scripting languages are likely to be susceptible to type errors. Scripting languages are powerful enough to successfully implement complex e-commerce applications as long as management and software engineering practice are strong.",2004,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410992,no,undetermined,0
An Approach to Merge Results of Multiple Static Analysis Tools (Short Paper),"Defects have been compromising quality of software and costing a lot to find and fix. Thus a number of effective tools have been built to automatically find defects by analyzing code statically. These tools apply various techniques and detect a wide range of defects, with a little overlap among defect libraries. Unfortunately, the advantages of tools' defect detection capacity are stubborn to combine, due to the unique style each tool follows when generating analysis reports. In this paper, we propose an approach to merge results from different tools and report them in a universal manner. Besides, two prioritizing policies are introduced to rank results so as to raise users' efficiency. Finally, the approach and prioritizing policies are implemented in an integrated tool by merging results from three independent analyzing tools. In this way, end users may comfortably benefit from more than one static analysis tool and thus improve software's quality.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601541,no,undetermined,0
A power-aware GALS architecture for real-time algorithm-specific tasks,"We propose an adaptive scalable architecture suitable for performing real-time algorithm-specific tasks. The architecture is based on the globally asynchronous and locally synchronous (GALS) design paradigm. We demonstrate that for different real-time commercial applications with algorithm-specific jobs like online transaction processing, Fourier transform etc., the proposed architecture allows dynamic load-balancing and adaptive inter-task voltage scaling. The architecture can also detect process-shifts for the individual processing units and determine their appropriate operating conditions. Simulation results for two representative applications show that for a random job distribution, we obtain up to 67% improvement in MOPS/W (millions of operations per second per watt) over a fully synchronous implementation.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410609,no,undetermined,0
Mitigating the effects of explosions in underground electrical vaults,"An explosion software package is used to assess the effectiveness of several simple and inexpensive safety devices that can minimize the dangers of explosions in underground electrical vaults. The software is capable of determining the forces on a manhole cover as high-pressure air and gases are expelled from the vault. This information is used to determine the feasibility of several devices designed to mitigate the effect of the explosion. The potential designs focus on modifications to the vault and manhole cover that limit the motion of the cover and reduce the severity of the explosive forces. The devices that are examined include bolts that fasten the cover to the vault, vented and lightweight manhole covers and covers that are attached to the vault by tethers. No single safety device will completely eliminate all of the dangers associated with an explosion. Rigidly attaching the manhole cover to the vault with bolts is not recommended, because a reasonable number of bolts are often not sufficient to withstand the high pressures that result when the cover is held down and the vault is unable to vent. If the bolts finally fail, the vault will vent at a high pressure and subject the manhole cover to potentially dangerous forces. A lightweight, vented cover restrained by elastic webbing, on the other hand, will permit gases to vent at a lower pressure and greatly reduce the hazards posed by the explosion. The use of a rigid tether such as a steel cable or chain is not recommended due to the excessive forces that the manhole cover will exert on the tether and the attachment points.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1413453,no,undetermined,0
Design time reliability analysis of distributed fault tolerance algorithms,"Designing a distributed fault tolerance algorithm requires careful analysis of both fault models and diagnosis strategies. A system will fail if there are too many active faults, especially active Byzantine faults. But, a system will also fail if overly aggressive convictions leave inadequate redundancy. For high reliability, an algorithm's hybrid fault model and diagnosis strategy must be tuned to the types and rates of faults expected in the real world. We examine this balancing problem for two common types of distributed algorithms: clock synchronization and group membership. We show the importance of choosing a hybrid fault model appropriate for the physical faults expected by considering two clock synchronization algorithms. Three group membership service diagnosis strategies are used to demonstrate the benefit of discriminating between permanent and transient faults. In most cases, the probability of failure is dominated by one fault type. By identifying the dominant cause of failure, one can tailor an algorithm appropriately at design time, yielding significant reliability gain.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467823,no,undetermined,0
Modeling the effects of 1 MeV electron radiation in gallium-arsenide solar cells using SILVACO virtual wafer fabrication software,The ALTAS device simulator from Silvaco International has the potential for predicting the effects of electron radiation in solar cells by modeling material defects. A GaAs solar cell was simulated in ATLAS and compared to an actual cell with radiation defects identified using deep level transient spectroscopy techniques (DLTS). The solar cells were compared for various fluence levels of 1 MeV electron radiation and showed an average of less than three percent difference between experimental and simulated cell output characteristics. These results demonstrate that ATLAS software can be a viable tool for predicting solar cell degradation due to electron radiation.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1488249,no,undetermined,0
"Extended abstract: requirements modeling within iterative, incremental processes","Requirements modeling is an established method for detecting defects in requirement specifications. Although many companies have successfully incorporated requirements modeling in their software processes, other companies face technology adoption problems: methods appear incompatible with processes and technologies in use, and with the attitudes of engineers. We approach this problem by adaptation of standard modeling know-how to existing practices and attitudes in certain companies. By a requirements modeling process that respects the constraints of iterative, incremental, use case driven development, and that uses a modeling language with a low learning barrier (for engineers), we hope to make companies that use the unified process amenable to requirements modeling.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1487927,no,undetermined,0
Inconsistency measurement of software requirements specifications: an ontology-based approach,"Management of requirements inconsistency is key to the development of complex trustworthy software system, and precise measurement is precondition for the management of requirements inconsistency properly. But at present, although there are a lot of work on the detection of requirements inconsistency, most of them are limited in treating requirements inconsistency according to heuristic rules, we still lacks of promising method for handling requirements inconsistency properly. Based on an abstract requirements refinement process model, this paper takes domain ontology as infrastructure for the refinement of software requirements, the aim of which is to get requirements descriptions that are comparable. Thus we can measure requirements inconsistency based on tangent plane of requirements refinement tree, after we have detected inconsistent relations of leaf nodes at semantic level.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467922,no,undetermined,0
Assessment driven process modeling for software process improvement,"Software process improvement (SPI) is used to develop processes to meet more effectively the software organizationpsilas business goals. Improvement opportunities can be exposed by conducting an assessment. A disciplined process assessment evaluates organizationpsilas processes against a process assessment model, which usually includes good software practices as indicators. Many benefits of SPI initiatives have been reported but some improvement efforts have failed, too. Our aim is to increase the probability to success by integrating software process modeling with assessments. A combined approach is known to provide more accurate process ratings and higher quality process models. In this study we have revised the approach by extending the scope of modeling further. Assessment Driven Process Modeling for SPI uses assessment evidence to create a descriptive process model of the assessed processes. The descriptive model is revised into a prescriptive process model, which illustrates an organizationpsilas processes after the improvements. The prescriptive model is created using a process library that is based on the indicators of the assessment model. Modeling during assessment is driven by both process performance and process capability indicators.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599774,no,undetermined,0
A framework for SOFL-based program review,"Program review is a practical and cost-effective method for detecting errors in program code. This paper describes our recent work aiming to provide support for revealing errors which usually arise from inappropriate implementations of desired specifications. In our approach, the SOFL specification language is employed for specifying software systems. We provide a framework that guides reviewers to compare a code with its specification for effective detection of potential defects.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467872,no,undetermined,0
Experimental dependability evaluation of a fail-bounded jet engine control system for unmanned aerial vehicles,"This paper presents an experimental evaluation of a prototype jet engine controller intended for unmanned aerial vehicles (UAVs). The controller is implemented with commercial off-the-shelf (COTS) hardware based on the Motorola MPC565 microcontroller. We investigate the impact of single event upsets (SEUs) by injecting single bit-flip faults into main memory and CPU registers via the Nexus on-chip debug interface of the MPC565. To avoid the injection of non-effective faults, automated pre-injection analysis of the assembly code was utilized. Due to the inherent robustness of the software, most injected faults were still non-effective (69.4%) or caused bounded failures having only minor effect on the jet engine (7.0%), while 20.1% of the errors were detected by hardware exceptions and 1.9% were detected by executable assertions in the software. The remaining 1.6% is classified as critical failures. A majority of the critical failures were caused by erroneous Booleans or type conversions involving Booleans.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467840,no,undetermined,0
On a method for mending time to failure distributions,"Many software reliability growth models assume that the time to next failure may be infinite; i.e., there is a chance that no failure will occur at all. For most software products this is too good to be true even after the testing phase. Moreover, if a non-zero probability is assigned to an infinite time to failure, metrics like the mean time to failure do not exist. In this paper, we try to answer several questions: Under what condition does a model permit an infinite time to next failure? Why do all non-homogeneous Poisson process (NHPP) models of the finite failures category share this property? And is there any transformation mending the time to failure distributions? Indeed, such a transformation exists; it leads to a new family of NHPP models. We also show how the distribution function of the time to first failure can be used for unifying finite failures and infinite failures NHPP models.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467830,no,undetermined,0
Assessing the performance of erasure codes in the wide-area,"The problem of efficiently retrieving a file that has been broken into blocks and distributed across the wide-area pervades applications that utilize grid, peer-to-peer, and distributed file systems. While the use of erasure codes to improve the fault-tolerance and performance of wide-area file systems has been explored, there has been little work that assesses the performance and quantifies the impact of modifying various parameters. This paper performs such an assessment. We modify our previously defined framework for studying replication in the wide-area to include both Reed-Solomon and low-density parity-check (LDPC) erasure codes. We then use this framework to compare Reed-Solomon and LDPC erasure codes in three wide-area, distributed settings. We conclude that although LDPC codes have an advantage over Reed-Solomon codes in terms of decoding cost, this advantage does not always translate to the best overall performance in wide-area storage situations.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467792,no,undetermined,0
Does Adaptive Random Testing Deliver a Higher Confidence than Random Testing?,"Random testing (RT) is a fundamental software testing technique. Motivated by the rationale that neighbouring test cases tend to cause similar execution behaviours, adaptive random testing (ART) was proposed as an enhancement of RT, which enforces random test cases evenly spread over the input domain. ART has always been compared with RT from the perspective of the failure-detection capability. Previous studies have shown that ART can use fewer test cases to detect the first software failure than RT. In this paper, we aim to compare ART and RT from the perspective of program-based coverage. Our experimental results show that given the same number of test cases, ART normally has a higher percentage of coverage than RT. In conclusion, ART outperforms RT not only in terms of the failure-detection capability, but also in terms of the thoroughness of program-based coverage. Therefore, ART delivers a higher confidence of the software under test than RT even when no failure has been revealed.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601538,no,undetermined,0
On-line detection of control-flow errors in SoCs by means of an infrastructure IP core,"In sub-micron technology circuits high integration levels coupled with the increased sensitivity to soft errors even at ground level make the task of guaranteeing systems' dependability more difficult than ever. In this paper we present a new approach to detect control-flow errors by exploiting a low-cost infrastructure intellectual property (I-IP) core that works in cooperation with software-based techniques. The proposed approach is particularly suited when the system to be hardened is implemented as a system-on-chip (SoC), since the I-IP can be added easily and it is independent on the application. Experimental results are reported showing the effectiveness of the proposed approach.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467779,no,undetermined,0
Assured reconfiguration of fail-stop systems,"Hardware dependability improvements have led to a situation in which it is sometimes unnecessary to employ extensive hardware replication to mask hardware faults. Expanding upon our previous work on assured reconfiguration for single processes and building upon the fail-stop model of processor behavior, we define a framework that provides assured reconfiguration for concurrent software. This framework can provide high dependability with lower space, power, and weight requirements than systems that replicate hardware to mask all anticipated faults. We base our assurance argument on a proof structure that extends the proofs for the single-application case and includes the fail-stop model of processor behavior. To assess the feasibility of instantiating our framework, we have implemented a hypothetical avionics system that is representative of what might be found on an unmanned aerial vehicle.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467774,no,undetermined,0
Bi-layer segmentation of binocular stereo video,"This paper describes two algorithms capable of real-time segmentation of foreground from background layers in stereo video sequences. Automatic separation of layers from colour/contrast or from stereo alone is known to be error-prone. Here, colour, contrast and stereo matching information are fused to infer layers accurately and efficiently. The first algorithm, layered dynamic programming (LDP), solves stereo in an extended 6-state space that represents both foreground/background layers and occluded regions. The stereo-match likelihood is then fused with a contrast-sensitive colour model that is learned on the fly, and stereo disparities are obtained by dynamic programming. The second algorithm, layered graph cut (LGC), does not directly solve stereo. Instead the stereo match likelihood is marginalised over foreground and background hypotheses, and fused with a contrast-sensitive colour model like the one used in LDP. Segmentation is solved efficiently by ternary graph cut. Both algorithms are evaluated with respect to ground truth data and found to have similar p performance, substantially better than stereo or colour/contrast alone. However, their characteristics with respect to computational efficiency are rather different. The algorithms are demonstrated in the application of background substitution and shown to give good quality composite video output.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467471,no,undetermined,0
On-Line Process Monitoring and Fault Isolation Using PCA,"This paper describes a real-time on-line process monitoring and fault isolation approach using PCA (principal component analysis). It also presents the software implementation architecture using an OPC (OLE for process control) compliant framework, which enables a seamless integration with the real plant and DCS. The proposed approach and architecture are implemented to monitor a refinery process simulation that produces cyclohexane using benzene and hydrogen. The result shows that both sensor faults and process faults can be detected on-line and the dominating process variables may be isolated",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467092,no,undetermined,0
Digital system for detection and classification of electrical events,"The paper describes an algorithm to detect and classify electrical events related to power quality. The events detection is based on monitoring the statistical characteristics of the energy of the error signal, which is defined as the difference between the monitored waveform and a sinusoidal wave generated with the same magnitude, frequency, and phase as the fundamental sinusoidal component. The novel feature is event recognition based on a neural network that uses the error signal as input. Multi-rate techniques are also employed to improve the system operation for on-line applications. Software tests were performed showing the good performance of the system.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1465861,no,undetermined,0
Toward understanding the rhetoric of small source code changes,"Understanding the impact of software changes has been a challenge since software systems were first developed. With the increasing size and complexity of systems, this problem has become more difficult. There are many ways to identify the impact of changes on the system from the plethora of software artifacts produced during development, maintenance, and evolution. We present the analysis of the software development process using change and defect history data. Specifically, we address the problem of small changes by focusing on the properties of the changes rather than the properties of the code itself. Our study reveals that 1) there is less than 4 percent probability that a one-line change introduces a fault in the code, 2) nearly 10 percent of all changes made during the maintenance of the software under consideration were one-line changes, 3) nearly 50 percent of the changes were small changes, 4) nearly 40 percent of changes to fix faults resulted in further faults, 5) the phenomena of change differs for additions, deletions, and modifications as well as for the number of lines affected, and 6) deletions of up to 10 lines did not cause faults.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1463233,no,undetermined,0
JUnit: unit testing and coiling in tandem,"Detecting and correcting defects either during or close to the phase where they originate is key for any fast, cost-effective software development. Unit testing, performed by the code writer, is still the most popular technique. The author has summarized the experiences around JUnit, probably the most popular OSS tool for Java unit testing. JUnit is an open source Java library, Kent Beck and Erich Gamma created JUnit, deriving it from Beck's Smalltalk testing framework. Easy integration of coding and unit testing lies at the heart of Extreme Programming and the agile software development movement. In agile programming, software is constructed by implementing functionality incrementally, in short spurts of activity.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1463200,no,undetermined,0
Assuring information quality in Web service composition,"As organizations have begun increasingly to communicate and interact with consumers via the Web, so the information quality (IQ) of their offerings has become a central issue since it ensures service usability and utility for each visitor and, in addition, improves server utilization. In this article, we present an IQ-enable Web service architecture, IQEWS, by introducing a IQ broker module between service clients and providers (servers). The functions of the IQ broker module include assessing IQ about servers, making selection decisions for clients, and negotiating with servers to get IQ agreements. We study an evaluation scheme aimed at measuring the information quality of Web services used by IQ brokers acting as the front-end of servers. This methodology is composed of two main components, an evaluation scheme to analyze the information quality of Web services and a measurement algorithm to generate the linguistic recommendations.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4598537,no,undetermined,0
Discerning user-perceived media stream quality through application-layer measurements,"The design of access networks for proper support of multimedia applications requires an understanding of how the conditions of the underlying network (packet loss and delays, for instance) affect the performance of a media stream. In particular, network congestion can affect the user-perceived quality of a media stream. By choosing metrics that indicate and/or predict the quality ranking that a user would assign to a media stream, we can deduce the performance of a media stream without polling users directly. We describe a measurement mechanism utilizing objective measurements taken from a media player application that strongly correlate with user rankings of stream quality. Experimental results demonstrate the viability of the chosen metrics as predictors or indicators of user quality rankings, and suggest a new mechanism for evaluating the present and future quality of a media stream.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1489940,no,undetermined,0
Predicting the probability of change in object-oriented systems,"Of all merits of the object-oriented paradigm, flexibility is probably the most important in a world of constantly changing requirements and the most striking difference compared to previous approaches. However, it is rather difficult to quantify this aspect of quality: this paper describes a probabilistic approach to estimate the change proneness of an object-oriented design by evaluating the probability that each class of the system will be affected when new functionality is added or when existing functionality is modified. It is obvious that when a system exhibits a large sensitivity to changes, the corresponding design quality is questionable. The extracted probabilities of change can be used to assist maintenance and to observe the evolution of stability through successive generations and identify a possible """"saturation"""" level beyond which any attempt to improve the design without major refactoring is impossible. The proposed model has been evaluated on two multiversion open source projects. The process has been fully automated by a Java program, while statistical analysis has proved improved correlation between the extracted probabilities and actual changes in each of the classes in comparison to a prediction model that relies simply on past data.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1492374,no,undetermined,0
Real-time detection and containment of network attacks using QoS regulation,"In this paper, we present a network measurement mechanism that can detect and mitigate attacks and anomalous traffic in real-time using QoS regulation. The detection method rapidly pursues the dynamics of the network on the basis of correlation properties of the network protocols. By observing the proportion occupied by each traffic protocol and correlating it to that of previous states of traffic, it can be possible to determine whether the current traffic is behaving normally. When abnormalities are detected, our mechanism allows aggregated resource regulation of each protocol's traffic. The trace-driven results show that the rate-based regulation of traffic characterized by protocol classes is a feasible vehicle for mitigating the impact of network attacks on end servers.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1494367,no,undetermined,0
A game model for selection of purchasing bids in consideration of fuzzy values,"A number of efficiency-based vendor selection and negotiation models have been developed to deal with multiple attributes including price, quality and delivery performance which is treated as important bid attributes. But some alternative vendor's preferences of attributes are difficult of quantitative analysis, such as trust, reliability, and courtesy of the vendor, which are considered to be crucial issues of recent reaches in vendor evaluation. This paper proposes a buyer-seller game model that has distinct advantages over existing methods for bid selection and negotiation, the fuzzy indexes are used to evaluate those attributes which are difficult of quantitative analysis, we propose a new method that expands Talluri (2002) and Joe Zhu (2004)'s method and allows which the elements composing problems are given by fuzzy numerical values, An important outcome of assessing relative efficiencies within a group of decision making units (DMUs) in fuzzy data envelopment analysis is a set of virtual multipliers or weights accorded to each (input or output) factor taken into account. In this paper, by assessing upper bounds on factor weights and compacting the resulted intervals, a CSW is determined. Since resulted efficiencies by the proposed CSW are fuzzy numbers rather than crisp values, it is more informative for decision maker.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1499472,no,undetermined,0
The impact of institutional forces on software metrics programs,"Software metrics programs are an important part of a software organization's productivity and quality initiatives as precursors to process-based improvement programs. Like other innovative practices, the implementation of metrics programs is prone to influences from the greater institutional environment the organization exists in. In this paper, we study the influence of both external and internal institutional forces on the assimilation of metrics programs in software organizations. We use previous case-based research in software metrics programs as well as prior work in institutional theory in proposing a model of metrics implementation. The theoretical model is tested on data collected through a survey from 214 metrics managers in defense-related and commercial software organizations. Our results show that external institutions, such as customers and competitors, and internal institutions, such as managers, directly influence the extent to which organizations change their internal work-processes around metrics programs. Additionally, the adaptation of work-processes leads to increased use of metrics programs in decision-making within the organization. Our research informs managers about the importance of management support and institutions in metrics programs adaptation. In addition, managers may note that the continued use of metrics information in decision-making is contingent on adapting the organization's work-processes around the metrics program. Without these investments in metrics program adaptation, the true business value in implementing metrics and software process improvement is not realized.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498772,no,undetermined,0
A simulation approach to structure-based software reliability analysis,"Structure-based techniques enable an analysis of the influence of individual components on the application reliability. In an effort to ensure analytical tractability, prevalent structure-based analysis techniques are based on assumptions which preclude the use of these techniques for reliability analysis during the testing and operational phases. In this paper, we develop simulation procedures to assess the impact of individual components on the reliability of an application in the presence of fault detection and repair strategies that may be employed during testing. We also develop simulation procedures to analyze the application reliability for various operational configurations. We illustrate the potential of simulation procedures using several examples. Based on the results of these examples, we provide novel insights into how testing and repair strategies can be tailored depending on the application structure to achieve the desired reliability in a cost-effective manner. We also discuss how the results could be used to explore alternative operational configurations of a software application taking into consideration the application structure so as to cause minimal interruption in the field.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498770,no,undetermined,0
Software based in-system memory test for highly available systems,"In this paper we describe a software based in-system memory test that is capable of testing system memory in both offline and online environments. A technique to transparently """"steal"""" a chunk of memory from the system for running tests and then inserting it back for normal application's use is proposed. Factors like system memory architecture that needs to be considered while adapting any conventional memory testing algorithm for in-system testing are also discussed. Implementation of the proposed techniques can significantly improve the system's ability to proactively detect and manage functional faults in memory. An extension of the methodology described is expected to be applicable for in-system testing of other system components (like processor) as well.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498209,no,undetermined,0
How to characterize the problem of SEU in processors & representative errors observed on flight,"In this paper are first summarized representative examples of anomalies observed in systems operating on-board satellites as the consequence of the effects of radiation on integrated circuit, showing that single event upsets (SEU) are a major concern. An approach to predict the sensitivity to SEUs of a software application running on a processor-based architecture is then proposed. It is based on fault injection experiments allowing estimating the average rate of program dysfunctions per upset. This error rate, if combined with static cross-section figures obtained from radiation ground testing, provides an estimation of the target program error rate. The efficiency of this two-step approach was demonstrated by results obtained when applying it to various processors.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498178,no,undetermined,0
A software based online memory test for highly available systems,"In this paper we describe a software based in-system memory test, capable of testing system memory in both offline and online environments. A technique to transparently """"steal"""" a chunk of memory from the system for running tests and then inserting it back for normal application use is proposed. Implementation of the proposed methodology can significantly improve the system's ability to proactively detect and manage functional faults in memory. The solution does not impose any hardware requirements and therefore lends itself for easy deployment on all kinds of systems. An extension of the methodology described is expected to be applicable for in-system testing of other system components as well.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498158,no,undetermined,0
Introduction to the special session on secure implementations,This paper briefly introduces online testing and its evolution towards very sub micron technologies. How secure circuit designers and online testing experts collaboration can help detect online the occurrence of natural faults that may be used as a basis to counter fault-based attacks taking into account the particular needs of secure application.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498141,no,undetermined,0
An Active Method to Building Dynamic Dependency Model for Distributed Components,"Currently many research show that, in a sophisticated application system, the faults which are impossible to occur theoretically, may take place in practice. J2EE (Java 2 Platform, Enterprise Edition) distributed environment has been popularly applied to EAI (enterprise application integration). With the growth of the numbers of Jsp, Servlet and EJB components, for a specific J2EE application, it is difficult for administrators to locate the fundamental position of the faults, and delay recovering the faults. Dependency models provide the effective method to trace all possible sources of the faults from the problem vertices against the relationship edges. Bayesian network was presented in 1981 by R.Howard and J.Matheson. It has been successfully applied to fault diagnosis field. Bayesian networks provide a method to describe consequence information naturally. In this paper, we construct the dependency models of software components with the construction algorithm of Bayesian networks. Dependency models can be represented with Bayesian networks. The vertices in Bayesian networks corresponds to the vertices in dependency models, and the conditional probability expressed with the edges in Bayesian networks corresponds to the relative strength expressed with the edges in dependency models. Consequently, it is possible to develop a tool to analyze and recover the faults automatically, and be helpful to find fundamental reasons for various faults, based on dependent relations between the components in dependency models",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498086,no,undetermined,0
Pattern recognition based tools enabling autonomic computing.,"Fault detection is one of the important constituents of fault tolerance, which in turn defines the dependability of autonomic computing. In presented work several pattern recognition tools were investigated in application to early fault detection. The optimal margin classifier technique was utilized to detect the abnormal behavior of software processes. The comparison with the performance of the quadratic classifiers is reported. The optimal margin classifiers were also implemented to the fault detection in hardware components. The impulse parameter probing technique was introduced to mitigate intermittent and transient fault problems. The pattern recognition framework of analysis of responses to a controlled component perturbation yielded promising results",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498079,no,undetermined,0
Mining Logs Files for Computing System Management,"With advancement in science and technology, computing systems become increasingly more difficult to monitor, manage and maintain. Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition process to translate domain knowledge into operating rules and policies. This has been experienced as a cumbersome, labor intensive, and error prone process. There is thus a pressing need for automatic and efficient approaches to monitor and manage complex computing systems. A popular approach to system management is based on analyzing system log files. However, several new aspects of the system log data have been less emphasized in existing analysis methods and posed several challenges. The aspects include disparate formats and relatively short text messages in data reporting, asynchronous data collection, and temporal characteristics in data representation. First, a typical computing system contains different devices with different software components, possibly from different providers. These various components have multiple ways to report events, conditions, errors and alerts. The heterogeneity and inconsistency of log formats make it difficult to automate problem determination. To perform automated analysis, we need to categorize the text messages with disparate formats into common situations. Second, text messages in the log files are relatively short with a large vocabulary size. Third, each text message usually contains a timestamp. The temporal characteristics provide additional context information of the messages and can be used to facilitate data analysis. In this paper, we apply text mining to automatically categorize the messages into a set of common categories, and propose two approaches of incorporating temporal information to improve the categorization performance",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498077,no,undetermined,0
Combining Visualization and Statistical Analysis to Improve Operator Confidence and Efficiency for Failure Detection and Localization,"Web applications suffer from software and configuration faults that lower their availability. Recovering from failure is dominated by the time interval between when these faults appear and when they are detected by site operators. We introduce a set of tools that augment the ability of operators to perceive the presence of failure: an automatic anomaly detector scours HTTP access logs to find changes in user behavior that are indicative of site failures, and a visualizer helps operators rapidly detect and diagnose problems. Visualization addresses a key question of autonomic computing of how to win operators' confidence so that new tools will be embraced. Evaluation performed using HTTP logs from Ebates.com demonstrates that these tools can enhance the detection of failure as well as shorten detection time. Our approach is application-generic and can be applied to any Web application without the need for instrumentation",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498055,no,undetermined,0
Towards Autonomic Virtual Applications in the In-VIGO System,"Grid environments enable users to share nondedicated resources that lack performance guarantees. This paper describes the design of application-centric middleware components to automatically recover from failures and dynamically adapt to grid environments with changing resource availabilities, improving fault-tolerance and performance. The key components of the application-centric approach are a global per-application execution history and an autonomic component that tracks the performance of a job on a grid resource against predictions based on the application execution history, to guide rescheduling decisions. Performance models of unmodified applications built using their execution history are used to predict failure as well as poor performance. A prototype of the proposed approach, an autonomic virtual application manager (AVAM), has been implemented in the context of the In-VIGO grid environment and its effectiveness has been evaluated for applications that generate CPU-intensive jobs with relatively short execution times (ranging from tens of seconds to less than an hour) on resources with highly variable loads - a workload generated by typical educational usage scenarios of In-VIGO-like grid environments. A memory-based learning algorithm is used to build the performance models for CPU-intensive applications that are used to predict the need for rescheduling. Results show that In-VIGO jobs managed by the AVAM consistently meet their execution deadlines under varying load conditions and gracefully recover from unexpected failures",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498049,no,undetermined,0
A method for studying partial discharges location and propagation within power transformer winding based on the structural data,"Power transformer inner insulation system is a very critical component. Its degradation may pose apparatus to fail while in service. On the other hand, experimental experiences prove that partial discharges are a major source of insulation failure in power transformers. If the deterioration of the insulation system caused by PD activity can be detected at an early stage, preventive maintenance measures may be taken. Because of the complex structure of the transformer, accurate PD location is difficult and is one of the challenges power utilities are faced with. This problem comes to be vital in open access systems. In this paper a theory for locating partial discharge and its propagation along the winding is proposed, which is based on structural data of a transformer. The lumped element winding model is constructed. Quasi-static condition is applied and each turn of the winding is considered as a segment. Then an algorithm is developed to use the constructed matrices for PD location. A software package in Visual Basic environment has been developed. This paper introduces the background theory and utilized techniques.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1496196,no,undetermined,0
Redundancy concepts to increase transmission reliability in wireless industrial LANs,"Wireless LANs are an attractive networking technology for industrial applications. A major obstacle toward the fulfillment of hard real-time requirements is the error-prone behavior of wireless channels. A common approach to increase the probability of a message being transmitted successfully before a prescribed deadline is to use feedback from the receiver and subsequent retransmissions (automatic repeat request-ARQ-protocols). In this paper, three modifications to an ARQ protocol are investigated. As one of these modifications a specific transmit diversity scheme, called antenna redundancy, is introduced. The other modifications are error-correcting codes and the transmission of multiple copies of the same packet. In antenna redundancy the base station/access point has several antennas. The base station transmits on one antenna at a time, but whenever a retransmission is needed, the base station switches to another antenna. The relative benefits of using FEC versus adding antennas versus sending multiple copies are investigated under different error conditions. One important result is that for independent Gilbert-Elliot channels between the base station antennas and the wireless station the antenna redundancy scheme effectively decreases the probability of missing a deadline, in a numerical example approximately an order of magnitude per additional antenna can be observed. As a second benefit, antenna redundancy decreases the number of transmission trials needed to transmit a message successfully, thus saving bandwidth.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1495408,no,undetermined,0
Performance evaluation of a connection-oriented Internet service based on a queueing model with finite capacity,"The operating mechanism of a connection-oriented Internet service is analyzed. Considering the finite buffer in connection-oriented Internet service, we establish a Geom/G/1/K queueing model with setup-close delay-close down for user-initiated session. Using the approach of an embedded Markovian chain and supplementary variables, we derive the probability distribution for the steady queue length and the probability generating function for the waiting time. Correspondingly, we study the performance measures of quality of service (QoS) in terms of system throughput, system response time, and system blocking probability. Based on the simulation results, we discuss the influence of the upper limit of close delay period and the capacity of the queueing model on the performance measures, which have potential application in the design, resource assigning, and optimal setting for the next generation Internet.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4598236,no,undetermined,0
"De-Randomizing"""" congestion losses to improve TCP performance over wired-wireless networks","Currently, a TCP sender considers all losses as congestion signals and reacts to them by throttling its sending rate. With Internet becoming more heterogeneous with more and more wireless error-prone links, a TCP connection may unduly throttle its sending rate and experience poor performance over paths experiencing random losses unrelated to congestion. The problem of distinguishing congestion losses from random losses is particularly hard when congestion is light: congestion losses themselves appear to be random. The key idea is to """"de-randomize"""" congestion losses. This paper proposes a simple biased queue management scheme that """"de-randomizes"""" congestion losses and enables a TCP receiver to diagnose accurately the cause of a loss and inform the TCP sender to react appropriately. Bounds on the accuracy of distinguishing wireless losses and congestion losses are analytically established and validated through simulations. Congestion losses are identified with an accuracy higher than 95% while wireless losses are identified with an accuracy higher than 75%. A closed form is derived for the achievable improvement by TCP endowed with a discriminator with a given accuracy. Simulations confirm this closed form. TCP-Casablanca, a TCP-Newreno endowed with the proposed discriminator at the receiver, yields through simulations an improvement of more than 100% on paths with low levels of congestion and about 1% random wireless packet loss rates. TCP-Ifrane, a sender-based TCP-Casablanca yields encouraging performance improvement.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1458767,no,undetermined,0
Project overlapping and its influence on the product quality,"Time to market, quality and cost are the three most important factors when developing software. In order to achieve and retain a leading position in the market, developers are forced to produce more complex functionalities, much faster and more frequently. In such conditions, it is hard to keep a high quality level and low cost. The article focuses on the reliability aspect of quality as one of the most important quality factors to the customer. Special attention is devoted to the reliability of the software product being developed in project overlapping conditions. The Weibull reliability growth model for predicting the reliability of a product during the development process is adapted and applied to historical data of a sequence of overlapping projects. A few useful tips on reliability modeling for project management and planning in project overlapping conditions are presented",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1458659,no,undetermined,0
Embedding policy rules for software-based systems in a requirements context,"Policy rules define what behavior is desired in a software-based system, they do not describe the corresponding action and event sequences that actually """"produce"""" desired (""""legal"""") or undesired (""""illegal"""") behavior. Therefore, policy rules alone are not sufficient to model every (behavioral) aspect of an information system. In other words, like requirements policies only exist in context, and a policy rule set can only be assessed and sensibly interpreted with adequate knowledge of its embedding context. Scenarios and goals are artifacts used in requirements engineering and system design to model different facets of software systems. With respect to policy rules, scenarios are well suited to define how these rules are embedded into a specific environment. A goal is an objective that the system under consideration should or must achieve. Thus, the control objectives of a system must be reflected in the policy rules that actually govern a system's behavior.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1454322,no,undetermined,0
Load balancing of services with server initiated connections,"The growth of on-line Internet services using wireless mobile handsets has increased the demand for scalable and dependable wireless services. The systems hosting such services face high quality-of-service (QoS) requirements in terms of quick response time and high availability. With growing traffic and loads using wireless applications, the servers and networks hosting these applications need to handle larger loads. Hence, there is a need to host such services on multiple servers to distribute the processing and communications tasks and balance the load across various similar entities. Load balancing is especially important for servers facilitating hosting of various wireless applications where it is difficult to predict the load delivered to a server. A load-balancer (LB) is a node that accepts all requests from external entities and directs them to internal nodes for processing based on their processing capabilities and current load patterns. We present the problem of load balancing among multiple servers, each having server initiated connections with other network entities. Challenges involved in balancing loads arising from such connections are presented and some practical solutions are proposed. As a case study, the architectures of load balancing schemes on a set of SMS (short message service) gateway servers is presented, along with deployment strategies. Performance and scalability issues are also highlighted for different possible solutions.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431343,no,undetermined,0
Design Aspects for Wide-Area Monitoring and Control Systems,"This paper discusses the basic design and special applications of wide-area monitoring and control systems, which complement classical protection systems and Supervisory Control and Data Acquisition/Energy Management System applications. Systemwide installed phasor measurement units send their measured data to a central computer, where snapshots of the dynamic system behavior are made available online. This new quality of system information opens up a wide range of new applications to assess and actively maintain system's stability in case of voltage, angle or frequency instability, thermal overload, and oscillations. Recent developed algorithms and their design for these application areas are introduced. With practical examples, the benefits in terms of system security are shown.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1428012,no,undetermined,0
Opportunistic file transfer over a fading channel under energy and delay constraints,"We consider transmission control (rate and power) strategies for transferring a fixed-size file (finite number of bits) over fading channels under constraints on both transmit energy and transmission delay. The goal is to maximize the probability of successfully transferring the entire file over a time-varying wireless channel modeled as a finite-state Markov process. We study two implementations regarding the delay constraints: an average delay constraint and a strict delay constraint. We also investigate the performance degradation caused by the imperfect (delayed or erroneous) channel knowledge. The resulting optimal policies are shown to be a function of the channel-state information (CSI), the residual battery energy, and the number of residual information bits in the transmit buffer. It is observed that the probability of successful file transfer increases significantly when the CSI is exploited opportunistically. When the perfect instantaneous CSI is available at the transmitter, the faster channel variations increase the success probability under delay constraints. In addition, when considering the power expenditure in the pilot for channel estimation, the optimal policy shows that the transmitter should use the pilot only if there is sufficient energy left for packet transfer; otherwise, a channel-independent policy should be used.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1425747,no,undetermined,0
High-abstraction level complexity analysis and memory architecture simulations of multimedia algorithms,"An appropriate complexity analysis stage is the first and fundamental step for any methodology aiming at the implementation of today's (complex) multimedia algorithms. Such a stage may have different final implementation goals such as defining a new architecture dedicated to the specific multimedia standard under study, or defining an optimal instruction set for a selected processor architecture, or to guide the software optimization process in terms of control-flow and data-flow optimization targeting a specific architecture. The complexity of nowadays multimedia standards, in terms of number of lines of codes and cross-relations among processing algorithms that are activated by specific input signals, goes far beyond what the designer can reasonably grasp from the """"pencil and paper"""" analysis of the (software) specifications. Moreover, depending on the implementation goal different measures and metrics are required at different steps of the implementation methodology or design flow. The process of extracting the desired measures needs to be supported by appropriate automatic tools, since code rewriting, at each design stage, may result resource consuming and error prone. This paper reviews the state of the art of complexity analysis methodologies oriented to the design of multimedia systems and presents an integrated tool for automatic analysis capable of producing complexity results based on rich and customizable metrics. The tool is based on a C virtual machine that allows extracting from any C program execution the operations and data-flow information, according to the defined metrics. The tool capabilities include the simulation of virtual memory architectures. This paper shows some examples of complexity analysis results that can be yielded with the tool and presents how the tools can be used at different stages of implementation methodologies.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1425531,no,undetermined,0
A statistical estimation of average IP packet delay in cellular data networks,"A novel technique for estimating the average delay experienced by an IP packet in cellular data networks with an SR-ARQ loop is presented. This technique uses the following input data: a statistical description of the radio channel, ARQ loop design parameters and the size of a transported IP packet. An analytical model is derived to enable a closed form mathematical estimation of this delay. To validate this model, a computer based simulator was built and tests showed good agreement between the simulation results and the model. This new model is of particular interest in predicting the packet delay for conversational traffic such as that used for VoIP applications.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1424700,no,undetermined,0
Charge-based flooding algorithm for detecting multimedia objects in peer-to-peer overlay networks,"Multimedia objects are distributed in peer-to-peer (P2P) overlay networks since objects are cached, downloaded, and personalized in peer computers (peers). An application has to find target peers which can support enough quality of service of objects. We discuss a new type of flooding algorithm to find target peers based on charge and acquaintance concepts so that areas in networks where target peers are expected to exist are more deeply searched. In addition, we discuss how peers can be granted access rights to manipulate the objects with help and cooperation of acquaintances. We evaluate the charge-based flooding algorithm compared with a TTL-based flooding algorithm in terms of the number of messages transmitted in networks.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1423487,no,undetermined,0
Estimating the number of faults remaining in software code documents inspected with iterative code reviews,"Code review is considered an efficient method for detecting faults in a software code document. The number of faults not detected by the review should be small. Current methods for estimating this number assume reviews with several inspectors, but there are many cases where it is practical to employ only two inspectors. Sufficiently accurate estimates may be obtained by two inspectors employing an iterative code review (ICR) process. This paper introduces a new estimator for the number of undetected faults in an ICR process, so the process may be stopped when a satisfactory result is estimated. This technique employs the Kantorowitz estimator for N-fold inspections, where the N teams are replaced by N reviews. The estimator was tested for three years in an industrial project, where it produced satisfactory results. More experiments are needed in order to fully evaluate the approach.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1421075,no,undetermined,0
Clustering software artifacts based on frequent common changes,"Changes of software systems are less expensive and less error-prone if they affect only one subsystem. Thus, clusters of artifacts that are frequently changed together are subsystem candidates. We introduce a two-step method for identifying such clusters. First, a model of common changes of software artifacts, called co-change graph, is extracted from the version control repository of the software system. Second, a layout of the co-change graph is computed that reveals clusters of frequently co-changed artifacts. We derive requirements for such layouts, and introduce an energy model for producing layouts that fulfill these requirements. We evaluate the method by applying it to three example systems, and comparing the resulting layouts to authoritative decompositions.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1421041,no,undetermined,0
Customizing event ordering middleware for component-based systems,"The stringent performance requirements of distributed realtime embedded systems often require highly optimized implementations of middleware services. Performing such optimizations manually can be tedious and error-prone. This paper proposes a model-driven approach to generate customized implementations of event ordering services in the context of component based systems. Our approach is accompanied by a number of tools to automate the customization. Given an application App, an event ordering service Order and a middleware platform P, we provide tools to analyze high-level specifications of App to extract information relevant to event ordering and to use the extracted application information to obtain a customized service, Order(App), with respect to the application usage.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420992,no,undetermined,0
MUSIC: Mutation-based SQL Injection Vulnerability Checking,"SQL injection is one of the most prominent vulnerabilities for web-based applications. Exploitation of SQL injection vulnerabilities (SQLIV) through successful attacks might result in severe consequences such as authentication bypassing, leaking of private information etc. Therefore, testing an application for SQLIV is an important step for ensuring its quality. However, it is challenging as the sources of SQLIV vary widely, which include the lack of effective input filters in applications, insecure coding by programmers, inappropriate usage of APIs for manipulating databases etc. Moreover, existing testing approaches do not address the issue of generating adequate test data sets that can detect SQLIV. In this work, we present a mutation-based testing approach for SQLIV testing. We propose nine mutation operators that inject SQLIV in application source code. The operators result in mutants, which can be killed only with test data containing SQL injection attacks. By this approach, we force the generation of an adequate test data set containing effective test cases capable of revealing SQLIV. We implement a MUtation-based SQL Injection vulnerabilities Checking (testing) tool (MUSIC) that automatically generates mutants for the applications written in Java Server Pages (JSP) and performs mutation analysis. We validate the proposed operators with five open source web-based applications written in JSP. We show that the proposed operators are effective for testing SQLIV.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601530,no,undetermined,0
Acquaintance-based protocol for detecting multimedia objects in peer-to-peer overlay networks,"Multimedia objects are distributed on peer computers (peers) in peer-to-peer (P2P) overlay networks. An application has to find target peers which can support enough quality of service (QoS) of multimedia objects. We discuss types of acquaintance relations of peers with respect to what objects each peer holds, can manipulate, and can grant access rights. We discuss a new type of flooding algorithm to find target peers based on charge and acquaintance concepts so that areas in networks where target peers are expected to exist are more deeply searched. We evaluate the charge-based flooding algorithm compared with a TTL-based flooding algorithm in terms of the number of messages transmitted in networks.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420977,no,undetermined,0
Understanding perceptual distortion in MPEG scalable audio coding,"In this paper, we study coding artifacts in MPEG-compressed scalable audio. Specifically, we consider the MPEG advanced audio coder (AAC) using bit slice scalable arithmetic coding (BSAC) as implemented in the MPEG-4 reference software. First we perform human subjective testing using the comparison category rating (CCR) approach, quantitatively comparing the performance of scalable BSAC with the nonscaled TwinVQ and AAC algorithms. This testing indicates that scalable BSAC performs very poorly relative to TwinVQ at the lowest bitrate considered (16 kb/s) largely because of an annoying and seemingly random mid-range tonal signal that is superimposed onto the desired output. In order to better understand and quantify the distortion introduced into compressed audio at low bit rates, we apply two analysis techniques: Reng bifrequency probing and time-frequency decomposition. Using Reng probing, we conclude that aliasing is most likely not the cause of the annoying tonal signal; instead, time-frequency or spectrogram analysis indicates that its cause is most likely suboptimal bit allocation. Finally, we describe the energy equalization quality metric (EEQM) for predicting the relative perceptual performance of the different coding algorithms and compare its predictive ability with that of ITU Recommendation ITU-R BS.1387-1.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420376,no,undetermined,0
Destructive transaction: human-oriented cluster system management mechanism,"Traditional cluster system management tools seldom consider the relevance between managed objects. Such relevance is the reason of related fault and may also lead to human operation errors. Because of this defect, traditional tools do not have the capability of handling of consistency, atomicity and recovery. This article proposes a transaction-based facility, destructive transaction, to solve the problems at some degree. Destructive transaction is a construct to wire down management rules stored in a system administrator's mind. It provides a method to describe managed objects relationships, atomicity facility and recovery for failed operation. And it significantly reduces human caused error possibility.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420274,no,undetermined,0
Optimizing checkpoint sizes in the C3 system,"The running times of many computational science applications are much longer than the mean-time-between-failures (MTBF) of current high-performance computing platforms. To run to completion, such applications must tolerate hardware failures. Checkpoint-and-rest art (CPR) is the most commonly used scheme for accomplishing this - the state of the computation is saved periodically on stable storage, and when a hardware failure is detected, the computation is restarted from the most recently saved state. Most automatic CPR, schemes in the literature can be classified as system-level checkpointing schemes because they take core-dump style snapshots of the computational state when all the processes are blocked at global barriers in the program. Unfortunately, a system that implements this style of checkpointing is tied to a particular platform amd cannot optimize the checkpointing process using application-specific knowledge. We are exploring an alternative called automatic application-level checkpointing. In our approach, programs are transformed by a pre-processor so that they become self-checkpointing and self-rest art able on any platform. In this paper, we evaluate a mechanism that utilizes application knowledge to minimize the amount of information saved in a checkpoint.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1420141,no,undetermined,0
Dynamic routing in translucent WDM optical networks: the intradomain case,"Translucent wavelength-division multiplexing optical networks use sparse placement of regenerators to overcome physical impairments and wavelength contention introduced by fully transparent networks, and achieve a performance close to fully opaque networks at a much less cost. In previous studies, we addressed the placement of regenerators based on static schemes, allowing for only a limited number of regenerators at fixed locations. This paper furthers those studies by proposing a dynamic resource allocation and dynamic routing scheme to operate translucent networks. This scheme is realized through dynamically sharing regeneration resources, including transmitters, receivers, and electronic interfaces, between regeneration and access functions under a multidomain hierarchical translucent network model. An intradomain routing algorithm, which takes into consideration optical-layer constraints as well as dynamic allocation of regeneration resources, is developed to address the problem of translucent dynamic routing in a single routing domain. Network performance in terms of blocking probability, resource utilization, and running times under different resource allocation and routing schemes is measured through simulation experiments.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416990,no,undetermined,0
Path-Sensitive Reachability Analysis of Web Service Interfaces (Short Paper),"WCFA (Web service interface control flow automata) is enhanced by allowing pre/post-conditions for certain Web service invocations to be declared. The formal definition of WCFA is given. Global behaviors of web service compositions (described by a set of WCFA) are captured by ARG (abstract reachability graph), in which each control point is equipped with a state formula and a call stack. The algorithm for constructing ARG uses a path-sensitive analysis to compute the state formulas. Pre/post-conditions are verified during the construction, where unreachable states are detected and pruned. Assertions can be made at nodes of ARG to express both safety properties and call stack inspection properties. Then a SAT solver is used to check whether the assertions are logical consequences of the state formulas(or/and call stacks).",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601534,no,undetermined,0
An Executable Interface Specification for Industrial Embedded System Design,"Nowadays, designers resort to abstraction techniques to conquer the complexity of industrial embedded systems during the design process. However, due to the large semantic gap between the abstractions and the implementation, the designers often fails to apply the abstraction techniques. In this paper, an EIS-based (executable interface specification) approach is proposed for the embedded system design.The proposed approach starts with using interface state diagrams to specify system architectures. A set of rules is introduced to transfer these diagrams into an executable model (EIS model) consistently. By making use of simulation/verification techniques, many architectural design errors can be detected in the EIS model at an early design stage. In the end, the EIS model can be systematically transferred into an interpreted implementation or a compiled implementation based on the constraints of the embedded platform. In this way, the inconsistencies between the high-level abstractions and the implementation can largely be reduced.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4601526,no,undetermined,0
Design and evaluation of hybrid fault-detection systems,"As chip densities and clock rates increase, processors are becoming more susceptible to transient faults that can affect program correctness. Up to now, system designers have primarily considered hardware-only and software-only fault-detection mechanisms to identify and mitigate the deleterious effects of transient faults. These two fault-detection systems, however, are extremes in the design space, representing sharp trade-offs between hardware cost, reliability, and performance. In this paper, we identify hybrid hardware/software fault-detection mechanisms as promising alternatives to hardware-only and software-only systems. These hybrid systems offer designers more options to fit their reliability needs within their hardware and performance budgets. We propose and evaluate CRAFT, a suite of three such hybrid techniques, to illustrate the potential of the hybrid approach. For fair, quantitative comparisons among hardware, software, and hybrid systems, we introduce a new metric, mean work to failure, which is able to compare systems for which machine instructions do not represent a constant unit of work. Additionally, we present a new simulation framework which rapidly assesses reliability and does not depend on manual identification of failure modes. Our evaluation illustrates that CRAFT, and hybrid techniques in general, offer attractive options in the fault-detection design space.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431553,no,undetermined,0
A comprehensive model for software rejuvenation,"Recently, the phenomenon of software aging, one in which the state of the software system degrades with time, has been reported. This phenomenon, which may eventually lead to system performance degradation and/or crash/hang failure, is the result of exhaustion of operating system resources, data corruption, and numerical error accumulation. To counteract software aging, a technique called software rejuvenation has been proposed, which essentially involves occasionally terminating an application or a system, cleaning its internal state and/or its environment, and restarting it. Since rejuvenation incurs an overhead, an important research issue is to determine optimal times to initiate this action. In this paper, we first describe how to include faults attributed to software aging in the framework of Gray's software fault classification (deterministic and transient), and study the treatment and recovery strategies for each of the fault classes. We then construct a semi-Markov reward model based on workload and resource usage data collected from the UNIX operating system. We identify different workload states using statistical cluster analysis, estimate transition probabilities, and sojourn time distributions from the data. Corresponding to each resource, a reward function is then defined for the model based on the rate of resource depletion in each state. The model is then solved to obtain estimated times to exhaustion for each resource. The result from the semi-Markov reward model are then fed into a higher-level availability model that accounts for failure followed by reactive recovery, as well as proactive recovery. This comprehensive model is then used to derive optimal rejuvenation schedules that maximize availability or minimize downtime cost.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1453531,no,undetermined,0
Modeling and analysis of non-functional requirements as aspects in a UML based architecture design,"The problem of effectively designing and analyzing software system to meet its nonfunctional requirements such as performance, security, and adaptability is critical to the system's success. The significant benefits of such work include detecting and removing defects earlier, reducing development time and cost while improving the quality. The formal design analysis framework (FDAF) is an aspect-oriented approach that supports the design and analysis of non-functional requirements for distributed, real-time systems. In the FDAF, nonfunctional requirements are defined as reusable aspects in the repository and the conventional UML has been extended to support the design of these aspects. FDAF supports the automated translation of extended, aspect-oriented UML designs into existing formal notations, leveraging an extensive body of formal methods work. In this paper, the design and analysis of response time performance aspect is described. An example system, the ATM/banking system has been used to illustrate this process.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1434886,no,undetermined,0
Digital gas fields produce real time scheduling based on intelligent autonomous decentralized system,"In the paper, we focus on exploring to construct a novel digital gas fields produce real-time scheduling system, which is based on the theory of intelligent autonomous decentralized system (IADS). The system combines the practical demand and the characteristics of the gas fields, and it aims at dealing with the real-time property, dynamic and complexity during gas fields produce scheduling. Besides embodying on-line intelligent expansion, intelligent fault tolerance and on-line intelligent maintenance of IADS particular properties, the scheme adequately attaches importance to the flexibility. The model & method based on intelligent information pull push (IIPP) and intelligent management (IM) is applied to the system, thus it is helpful to improve the performance of the scheduling system. In according with the current requirement of gas fields produce scheduling, the concrete solvent is put forward. The related system architecture and software structure is presented. An effective method of solving the real-time property was developed for use in scheduling of the gas field produce. We simulated some experiments, and the result demonstrates the validity of the system. At Last, we predict its promising research trend in the future of gas fields practice application.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1452151,no,undetermined,0
A QoS-enable failure detection framework for J2EE application server,"As a basic reliability guarantee technology in distributed systems, failure detection provides the ability of timely detecting the liveliness of runtime systems. Effective failure detection is very important to J2EE application server (JAS), the leading middleware in Web computing environment, and it also needs to meet the requirements of reconfiguration, flexibility and adaptability. Based on the QoS (quality of service) specification of failure detector, this paper presents a QoS-enable failure detection framework for JAS, which satisfies the requirements of dynamically adjusting qualities and flexible integration of failure detectors. The work has been implemented in OnceAS application server that is developed by Institute of Software, Chinese Academy of Sciences. The experiments show that the framework can provide good QoS of failure detection in JAS.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1452132,no,undetermined,0
A fault tolerant approach in cluster computing system,"A long-term trend in high performance computing is the increasing number of nodes in parallel computing platforms, which entails a higher failure probability. Hence, fault tolerance becomes a key property for parallel application running on parallel computing systems. The message passing interface (MPI) is currently the programming paradigm and communication library most commonly used on parallel computing platforms. MPI applications may be stopped at any time during their execution due to an unpredictable failure. In order to avoid complete restarts of an MPI application because of only one failure, a fault tolerant MPI implementation is essential. In this paper, we propose a fault tolerant approach in cluster computing system. Our approach is based on reassignment of tasks to the remaining system and message logging is used for message losses. This system consists of two main parts, failure diagnosis and failure recovery. Failure diagnosis is the detection of a failure and failure recovery is the action needed to take over the workload of a failed component. This fault tolerant approach is implemented as an extension of the message passing interface.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4600394,no,undetermined,0
Developing and assuring trustworthy Web services,"Web services are emerging technologies that are changing the way we develop and use computer systems and software. Current Web services testing techniques are unable to assure the desired level of trustworthiness, which presents a barrier to WS applications in mission and business critical environments. This paper presents a framework that assures the trustworthiness of Web services. New assurance techniques are developed within the framework, including specification verification via completeness and consistency checking, specification refinement, distributed Web services development, test case generation, and automated Web services testing. Traditional test case generation methods only generate positive test cases that verify the functionality of software. The Swiss cheese test case generation method proposed in this paper is designed to perform both positive and negative testing that also reveal the vulnerability of Web services. This integrated development process is implemented in a case study. The experimental evaluation demonstrates the effectiveness of this approach. It also reveals that the Swiss cheese negative testing detects even more faults than positive testing and thus significantly reduces the vulnerability of Web services.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1452016,no,undetermined,0
On a software-based self-test methodology and its application,"Software-based self-test (SBST) was originally proposed for cost reduction in SOC test environment. Previous studies have focused on using SBST for screening logic defects. SBST is functional-based and hence, achieving a high full-chip logic defect coverage can be a challenge. This raises the question of SBST's applicability in practice. In this paper, we investigate a particular SBST methodology and study its potential applications. We conclude that the SBST methodology can be very useful for producing speed binning tests. To demonstrate the advantage of using SBST in at-speed functional testing, we develop a SBST framework and apply it to an open source microprocessor core, named OpenRISC 1200. A delay path extraction methodology is proposed in conjunction with the SBST framework. The experimental results demonstrate that our SBST can produce tests for a high percentage of extracted delay paths of which less than half of them would likely be detected through traditional functional test patterns. Moreover, the SBST tests can exercise the functional worst-case delays which could not be reached by even 1M of traditional verification test patterns. The effectiveness of our SBST and its current limitations are explained through these experimental findings.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1443407,no,undetermined,0
Adaptive multi-language code generation using YAMDAT,"In the current environment of accelerating technological change, software development continues to be difficult, unpredictable, expensive, and error-prone model driven architecture (MDA), sometimes known as Executable UML, offers a possible solution. MDA provides design notations with precisely defined semantics. Using these notations, developers can create a design model that is detailed and complete enough that the model can be verified and tested via simulation (ldquoexecutionrdquo). Design faults, omissions, and inconsistencies can be detected without writing any code. Furthermore, implementation code can be generated directly from the model. In fact, implementations in different languages or for different platforms can be generated from the same model.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4600402,no,undetermined,0
A tariff model to charge IP services with guaranteed quality: effect of users' demand in a case study,"In this paper, we consider a per-call, usage-based tariff model to charge for IP services with guaranteed quality. This model is based on the virtual delay, which is a quality of service (QoS) index that describes an improved IP service provided by a network domain. We show how to compute the virtual delay, and how to make it dependent on the service demand. Then, we demonstrate the effectiveness of our tariff model to tune revenues, blocking probability, and resource utilization in a meaningful application scenario. Our goal is to give some directions for network resource dimensioning and pricing purposes, which depend on the service demand.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1440805,no,undetermined,0
A framework for applying inventory control to capacity management for utility computing,"A key concern in utility computing is managing capacity so that application service providers (ASPs) and computing utilities (CUs) operate in a cost effective way. To this end, we propose a framework for applying inventory control to capacity management for utility computing. The framework consists of: conceptual foundations (e.g., establishing connections between concepts in utility computing and those in inventory control); problem formulations (e.g., what factors should be considered and how they affect computational complexity); and quality of service (QoS) forecasting, which is predicting the future effect on QoS of ASP and CU actions taken in the current period (a critical consideration in searching the space of possible solutions).",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1440793,no,undetermined,0
The virtues of assessing software reliability early,"Software reliability is one of the few software quality attributes with a sound mathematical definition: the probability of a software failure's occurrence within a given period and under specific use conditions. By this definition, reliability is a strictly operational quality attribute. A reliability prediction method that integrates quality information from such sources as architectural system descriptions, use scenarios, system deployment diagrams, and module testing lets managers identify problem areas early and make any necessary organizational adjustments.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1438328,no,undetermined,0
Flexible Consistency for Wide Area Peer Replication,"The lack of a flexible consistency management solution hinders P2P implementation of applications involving updates, such as read-write file sharing, directory services, online auctions and wide area collaboration. Managing mutable shared data in a P2P setting requires a consistency solution that can operate efficiently over variable-quality failure-prone networks, support pervasive replication for scaling, and give peers autonomy to tune consistency to their sharing needs and resource constraints. Existing solutions lack one or more of these features. In this paper, we described a new consistency model for P2P sharing of mutable data called composable consistency, and outline its implementation in a wide area middleware file service called Swarm. Composable consistency lets applications compose consistency semantics appropriate for their sharing needs by combining a small set of primitive options. Swarm implements these options efficiently to support scalable, pervasive, failure-resilient, wide-area replication behind a simple yet flexible interface. Two applications was presented to demonstrate the expressive power and effectiveness of composable consistency: a wide area file system that outperforms Coda in providing close-to-open consistency over WANs, and a replicated BerkeleyDB database that reaps order-of-magnitude performance gains by relaxing consistency for queries and updates",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1437084,no,undetermined,0
Evaluation of product reusability based on a technical and economic model: a case study of televisions,"In the field of sustainable manufacturing, reusing of old products or components is considered as the most environmentally friendly strategy among all other strategies. However, the decision of reusing old components of a used product confronts many uncertainties such as the quality level of the used components and the economic aspect of reusing them compared to producing a new component. This paper presents an integrated technical and economic model to evaluate the reusability of products or components. The model introduces some new parameters, such as product value and product gain, to assist the decision between reuse, remanufacture or disposal. In order to handle uncertainties, a Monte Carlo simulation using @Risk<sup>TM</sup> is utilized. The results show that the model is capable to assess the potential reusability of used products, while the use of simulation significantly increases the function of the model in addressing uncertainties. A case study of televisions is used to demonstrate the applicability of the model using extensive time-to-failure data for the major parts of a television set. Furthermore, a direction of future work is outlined and briefly discussed.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1437024,no,undetermined,0
Predicting the location and number of faults in large software systems,"Advance knowledge of which files in the next release of a large software system are most likely to contain the largest numbers of faults can be a very valuable asset. To accomplish this, a negative binomial regression model has been developed and used to predict the expected number of faults in each file of the next release of a system. The predictions are based on the code of the file in the current release, and fault and modification history of the file from previous releases. The model has been applied to two large industrial systems, one with a history of 17 consecutive quarterly releases over 4 years, and the other with nine releases over 2 years. The predictions were quite accurate: for each release of the two systems, the 20 percent of the files with the highest predicted number of faults contained between 71 percent and 92 percent of the faults that were actually detected, with the overall average being 83 percent. The same model was also used to predict which files of the first system were likely to have the highest fault densities (faults per KLOC). In this case, the 20 percent of the files with the highest predicted fault densities contained an average of 62 percent of the system's detected faults. However, the identified files contained a much smaller percentage of the code mass than the files selected to maximize the numbers of faults. The model was also used to make predictions from a much smaller input set that only contained fault data from integration testing and later. The prediction was again very accurate, identifying files that contained from 71 percent to 93 percent of the faults, with the average being 84 percent. Finally, a highly simplified version of the predictor selected files containing, on average, 73 percent and 74 percent of the faults for the two systems.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1435354,yes,undetermined,0
Profiling deployed software: assessing strategies and testing opportunities,"An understanding of how software is employed in the field can yield many opportunities for quality improvements. Profiling released software can provide such an understanding. However, profiling released software is difficult due to the potentially large number of deployed sites that must be profiled, the transparency requirements at a user's site, and the remote data collection and deployment management process. Researchers have recently proposed various approaches to tap into the opportunities offered by profiling deployed systems and overcome those challenges. Initial studies have illustrated the application of these approaches and have shown their feasibility. Still, the proposed approaches, and the tradeoffs between overhead, accuracy, and potential benefits for the testing activity have been barely quantified. This paper aims to overcome those limitations. Our analysis of 1,200 user sessions on a 155 KLOC deployed system substantiates the ability of field data to support test suite improvements, assesses the efficiency of profiling techniques for released software, and the effectiveness of testing efforts that leverage profiled field data.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1435352,no,undetermined,0
Parallel processors and an approach to the development of inference engine,"The reliable and fault tolerant computers are key to the success to aerospace, and communication industries where failures of the system can cause a significant economic impact and loss of life. Designing a reliable digital system, and detecting and repairing the faults are challenging tasks in order for the digital system to operate without failures for a given period of time. The paper presents a new and systematic software engineering approach of performing fault diagnosis of digital systems, which have employed multiple processors. The fault diagnosis model is based on the classic PMC model to generate data obtained on the basis of test results performed by the processors. The PMC model poses a tremendous challenge to the user in doing fault analysis on the basis of test results performed by the processors. This paper will perform one fault model for developing software. The effort has been made to preserve the necessary and sufficient.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1434904,no,undetermined,0
MPEG-4 video mobile uplink caching algorithm,"Digital cellular mobile technologies have been rapidly developed since the analog technology was presented. This has led to many new applications especially multimedia. More interestingly, the new mobile has embedded the digital camera in itself, so that the user can take photos, record the videos and make video calls. Although many applications of mobile uplink video exist, we cannot see the video clearly because of the bandwidth limitation and error-prone environment. Therefore, we propose a mobile video uplink caching algorithm that can make diminish PSNR variations which may result in better subjective video quality with simple implementation.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4600472,no,undetermined,0
New approach for selfish nodes detection in mobile ad hoc networks,"A mobile ad hoc network (MANET) is a temporary infrastructureless network, formed by a set of mobile hosts that dynamically establish their own network on the fly without relying on any central administration. Mobile hosts used in MANET have to ensure the services that were ensured by the powerful fixed infrastructure in traditional networks, the packet forwarding is one of these services. The resource limitation of nodes used in MANET, particularly in energy supply, along with the multi-hop nature of this network may cause new phenomena which do not exist in traditional networks. To save its energy a node may behave selfishly and uses the forwarding service of other nodes without correctly forwarding packets for them. This deviation from the correct behavior represents a potential threat against the quality of service (QoS), as well as the service availability, one of the most important security requirements. Some solutions have been recently proposed, but almost all these solutions rely on the watchdog technique as stated in S. Marti et al. (2000) in their monitoring components, which suffers from many problems. In this paper we propose an approach to mitigate some of these problems, and we assess its performance by simulation.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1588323,no,undetermined,0
Testing FPGAs using JBits RTP cores,"In this paper, we present a fault-testing technique for field programmable gate arrays (FPGAs) that is based on the features offered by Java Bits (JBits). Our technique can detect single and multiple stuck at faults, and is capable of detecting the faulty CLB within the FPGA. The algorithm proposed for testing the faults in the CLB utilizes the unified-library primitives and the run-time parameterizable (RTP) cores of the JBits programming language. The method also explores the object-oriented approach of the Java programming language used in JBits. It has the capability of providing run-time fault avoidance in FPGAs based on the faults detected during the testing process. Since JBits involves programming directly at the bitstream level, the proposed method offers additional advantages over traditional testing techniques",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1594305,no,undetermined,0
Combined software and hardware techniques for the design of reliable IP processors,"In the recent years both software and hardware techniques have been adopted to carry out reliable designs, aimed at autonomously detecting the occurrence of faults, to allow discarding erroneous data and possibly performing the recovery of the system. The aim of this paper is the introduction of a combined use of software and hardware approaches to achieve complete fault coverage in generic IP processors, with respect to SEU faults. Software techniques are preferably adopted to reduce the necessity and costs of modifying the processor architecture; since a complete fault coverage cannot be achieved, partial hardware redundancy techniques are then introduced to deal with the remaining, not covered, faults. The paper presents the methodological approach adopted to achieve the complete fault coverage, the proposed resulting architecture, and the experimental results gathered from the fault injection analysis campaign",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030937,no,undetermined,0
Software Self-Testing of a Symmetric Cipher with Error Detection Capability,"Cryptographic devices are recently implemented with different countermeasures against side channel attacks and fault analysis. Moreover, some usual testing techniques, such as scan chains, are not allowed or restricted for security requirements. In this paper, we analyze the impact that error detecting schemes have on the testability of an implementation of the advanced encryption standard, in particular when software-based self-test techniques are envisioned. We show that protection schemes can improve concurrent error detection, but make initial testing more difficult.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567066,no,undetermined,0
Testing the Implementation of Business Rules Using Intensional Database Tests,"One of the key roles of any information system is to enforce the business rules and policies set by the owning organisation. As for any important functionality, it is necessary to verify the implementation of any business rule carefully, through thorough testing. However, business rules have some specific features which make testing a particular challenge. They represent a more fine-grained unit of functionality than is usually considered by testing tools (programs, module, UML models, etc.) and their implementations are typically spread across a system (or perhaps some specific layer of a system). There is no convenient one-to-one relationship between programs and business rules that can facilitate their testing. To the best of our knowledge, no tools, methods or guidelines exist for helping software developers to test the implementation of business rules. Standard testing tools can help to a certain extent, but they leave the rule-specific work entirely in the programmer's hands. In this paper, we discuss the problems of testing business rules, and elicit the key features of a good test suite for a collection of business rules. We focus in particular on constraint business rules - an important class of rule that is commonly applied to the persistent data managed by the information system. We show how intensional database tests provide a suitable platform on which to implement business rule tests rapidly, and show how existing intensional test suites can be automatically adapted to test business rules. We have applied these ideas in a case study, which has allowed us to compare the relative costs of creating and executing these augmented test suites, as well as providing some evidence of their ability to detect faults in business rule implementations",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691677,no,undetermined,0
Test Policy: Gaining Control on IT Quality and Processes,Too often projects deliver software of which the quality is difficult to predict. Sometimes the project completion is delayed due to the continuous change of requirements while the software is still being built. The quality level must align with the company needs. It is extremely important that the planned benefits of an IT system are reached. When the benefits are not achieved it will cause much more damage than just the cost of delay. The reputation of a company might be at stake!,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567030,no,undetermined,0
Generating a Test Strategy with Bayesian Networks and Common Sense,"Testing still represents an important share of the overall development effort and, coming late in the software life cycle, it is on the critical path both from a schedule and quality perspective. In an effort to conduct smarter software testing, Motorola Labs have developed the Bayesian test assistant (BTA), an advanced decision support tool to optimize all verification and validation activities, in development and system testing. With Bayesian networks, the theory underlying BTA, Motorola Labs built a library of causal models to predict, from key process, people and product factors, the quality of artefacts at each step of the software development. In this paper we present how BTA links the predictions from development models by mapping dependencies between components or subsystems to predict the level of risk in each system feature. As a result, and well before system testing starts, BTA generates a test strategy that optimizes the writing of test cases. During system test, BTA scores test cases to select an optimum set for each test step, leading to a faster discovery of defects. We also describe how BTA was deployed on large telecomm system releases in several Motorola organizations and the improvement driven so far in system testing",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691667,no,undetermined,0
Experiences with product line development of embedded systems at Testo AG,"Product line practices are increasingly becoming popular in the domain of embedded software systems. This paper presents results of assessing success, consistency, and quality of Testo's product line of climate and flue gas measurement devices after its construction and the delivery of three commercial products. The results of the assessment showed that the incremental introduction of architecture-centric product line development can be considered successful even though there is no quantifiable reduction of time-to-market as well as development and maintenance costs so far. The success is mainly shown by the ability of Testo to develop more complex products and the satisfaction of the involved developers. A major issue encountered is ensuring the quality of reusable components and the conformance of the products to the architecture during development and maintenance",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691589,no,undetermined,0
Predicting return-on-investment for product line generations,"The decision of an organization to introduce product line engineering depends on a sound and careful analysis of risks and return on investment. The latter is computed by an economic model, which relies on high quality input and must reflect the envisioned migration strategy sufficiently. To facilitate risk analysis, this paper applies Monte-Carlo simulation to an existing product line economic model. Additionally, the model is extended by the support of product line generations that is, considering the degeneration of product line infrastructures and taking reinvestment into an existing product line into account. The practical application of the model is demonstrated by an industrial case study",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691573,no,undetermined,0
Multi-Dimensional Measures for Test Case Quality,"Choosing the right test cases is an important task in software development due to high costs of software testing as well as the significance of software failures. Therefore, evaluating the quality of test techniques and test suites may help improving test results. Benchmarking has been successfully applied to various domains such as database performance. However, the difficulty in benchmarking test case quality is to find suitable measures. In this paper, a multi- dimensional measuring of test case quality is proposed. It has been shown that not only the number of detected faults but also other aspects such as development artefacts like source code or usage profiles are important. Consequences of this multi-dimensional measure on creating a test bench- mark are described.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567035,no,undetermined,0
Risk Management through Architecture Design,Management of risks is critical issue in the project management and it is important to ensure that risk management is done in a sensible way. Many techniques to manage and reduce risks have been done previously but only few have addressed the design analysis to reduce risk and none have attempted to use software architecture analysis and design to manage risks. In this paper we try to find a solution through various software architectural design patterns. We present results of an experiment comparing this new technique with software risk evaluation tool (SRE) for creating test cases and detect faults. However the risks detected may differ suggesting that these two are different approach to same problem,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691406,no,undetermined,0
A Resilient Telco Grid Middleware,"Grid computing can exploit distributed, underutilized or not, resources to provide massive parallel CPU capacity. Load balancing, applications sharing, as well as geographically dispersed databases features are other Grid's aspects which are of interest for a telecommunications operator (Telco). Building a Grid middleware in order to implement Telco's services is thus a way to assess the validity of this type of architecture for future applications. To achieve a trustworthy platform, the middleware needs to take into account accidental or malicious faults which can impact different resilience aspects. This paper describes a secure and highly available architecture which, besides traditional Grid middleware functionalities (resource broker, job mapping, system monitoring, ...), makes use of fault-tolerant mechanisms (process duplication, failure handling, ...) to guarantee QoS defined in the service level agreement. Security is carried out by analyzing each node's defense capability issue and finding a suitable solution to match this with the appropriate user's job.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691046,no,undetermined,0
Reliability Growth Modeling for Software Fault Detection Using Particle Swarm Optimization,"Modeling the software testing process to obtain the predicted faults (failures) depends mainly on representing the relationship between execution time (or calendar time) and the failure count or accumulated faults. A number of unknown function parameters such as the mean failure function mu(t;beta) and the failure intensity function lambda(t;beta) are estimated using either least-square or maximum likelihood estimation techniques. Unfortunately, the model parameters are normally in nonlinear relationships. This makes traditional parameter estimation techniques suffer many problems in finding the optimal parameters to tune the model for a better prediction. In this paper, we explore our preliminary idea in using particle swarm optimization (PSO) technique to help in solving the reliability growth modeling problem. The proposed approach will be used to estimate the parameters of the well known reliability growth models such as the exponential model, power model and S-shaped models. The results are promising.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688697,no,undetermined,0
Characterization of a Real Internet Radio Service,"The increase in the number of Web-pages where links to Internet-radios are offered has made these services one of the most popular in nowadays Internet. This popularity has motivated the interest of the scientific community and a lot of research has been carried out in order to improve and study these services. This paper presents the analysis of the Internet-radio hosted by the www.asturies.com digital newspaper. The study has been performed thanks to a log database stored over a period of almost two years. The traffic between every service device has been studied and different elements about users' behaviour have been analyzed. The conclusions are essential to improve the configuration of one of these services. Service models for Internet-radio services can be developed and help managers to test different configurations or predict future situations, avoiding problems in advance",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690159,no,undetermined,0
Practical Use of Software Reliability Methods in New Product Development,"This paper presents seven software reliability estimation methods studied in the Nokia case unit, which operates in turbulent telecommunications business environment characterised by uncertainty and inability to predict the future. Tens of software reliability models have been developed since the beginning of 1970's. However, a few - if not any - of them have worked optimally across projects. This paper focuses on investigating the practical use of the methods in real-life complex development situations and demonstrates how the methods can be applied to new product development (NPD) process in the case unit. The results show that none of the methods operate alone but need to be combined with each other. Finally, ideas for further research are proposed",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690145,no,undetermined,0
All Things Considered: Inspecting Statecharts by Model Transformation,"Inspections are a cost-effective way of finding errors. However, checklist-based inspections of statecharts can only find a limited class of flaws while scenario-based inspections can never practically traverse the vast numbers of possible combinations of states in complex models made up of multiple communicating finite state machines. A technique for systematic and comprehensive validation of such models is described, based on partitioning the overall behaviour into sets of transitions which show the system-level response in a simple and explicit way. This process is supported by a tool, Statestep, which helps the user to deal methodically and thoroughly with (for example) millions of possibilities. As an example, a subtle error is exposed in a small but non-trivial published statechart design. The technique offers the possibility of detecting any error, no matter how obscure the scenario in which it occurs",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690144,no,undetermined,0
State of the Art and Practice of OpenSource Component Integration,"The open source software (OSS) development approach has become a remarkable option to consider for cost-efficient, high quality software development. Utilizing OSS as part of an in-house software application requires the software company to take the role of a component integrator. In addition, integrating OSS as part of in-house software has a few differences compared to integrating closed source software and in-house software, such as access to source code and the fact that OSS evolves differently than closed source software. This paper describes the current state of the art and practice of open source integration techniques. The main observations are that the lack of documentation and heterogeneity of platforms are problems that neither the state of the art or practice could solve. In addition, although literature provides techniques and methods for predicting and solving both architecture and component level integration problems, these were not used in practice. Instead, companies relied on experience and rules of thumb",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690138,no,undetermined,0
Self-adjusting Component-Based Fault Management,"The Trust4All project aims to define an open, component-based framework for the middleware layer in high-volume embedded appliances that enables robust and reliable operation, upgrading and extension. To improve availability of each individual application in a Trust4All system, we propose a runtime configurable fault management mechanism (FMM) which detects deviations from given service specifications by intercepting interface calls. There are two novel contributions associated with FMM. First, when repair is necessary, FMM picks a repair action that incurs the best tradeoff between the success rate and the cost of repair. Second, considering that it is rather difficult to obtain sufficient information about third party components during their early stage of usage, FMM is designed to be able to accumulate appropriate knowledge, e.g. the success rate of a specific repair action in the past and rules that can avoid a specific failure, and self-adjust its capability accordingly",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1690132,no,undetermined,0
QoS aware CORBA Middleware for Bluetooth,"The wireless nature and the mobility of Bluetooth enabled devices combined with the heterogeneity of the wide range of hardware and software capabilities present in those devices makes Bluetooth connection and resource management very complicated and error prone. To manage such diversity of software and hardware, middleware technologies masking the underlying platforms have been designed. One such middleware solution for Bluetooth based on common object request broker architecture (CORBA) is introduced and the mapping of GIOP messages to Bluetooth logical link control and adaptation protocol (L2CAP) links is explained in detail. The paper also describes how CORBA policy objects influence object reference creation and service contexts in the request/reply sequences and how client-server transport level quality of services (QoS) negotiations are achieved through QoS information embedded in object references and service contexts",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1689513,no,undetermined,0
On Line Testing of Single Feedback Bridging Fault in Cluster Based FPGA by Using Asynchronous Element,"In this paper, we present a novel technique for online testing of feedback bridging faults in the interconnects of the cluster based FPGA. The detection circuit will be implemented using BISTER configuration. We have configured the Block Under Test (BUT) with a pseudo-delay independent asynchronous element. Since we have exploited the concept of asynchronous element known as Muller-C element in order to detect the fault, the fault has high ingredient of delay dependent properties due to variation of the feedback path delay. Xilinx Jbits 3.0 API (Application Program Interface) is used to implement the BISTER structure in the FPGA. By using Jbits, we can reconfigure dynamically the device, in which the partial bit stream only affects part of the device. In the comparison to the traditional FPGA development tool (ISE), Jbits is faster to map the specific portion of the circuit to a specific tile. We also have more controllability over the utilization of internal resources of FPGA, so that we can perform this partial reconfiguration.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567091,no,undetermined,0
Designing an Architecture for Delivering Mobile Information Services to the Rural Developing World,"Paper plays a crucial role in many developing world information practices. However, paper-based records are inefficient, error-prone and difficult to aggregate. Therefore we need to link paper with the flexibility of online information systems. A mobile phone is the perfect bridging device. Long battery life, connectivity, solid-state memory, low price and immediate utility make it better suited to developing world conditions than a PC. However, mobile software platforms are difficult to use, difficult to develop for, and make the assumption of ubiquitous connectivity. To address these limitations we present CAM - a framework for developing mobile applications for the rural developing world. CAM applications are accessed by capturing barcodes using the phone camera, or by entering numeric strings with the keypad. Supporting minimal navigation, direct linkage to paper practices and offline multimedia interaction, CAM is uniquely adapted to rural user, application and infrastructure constraints",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691702,no,undetermined,0
Transient fault-tolerance through algorithms,"This article describes that single-version enhanced processing logic or algorithms can be very effective in gaining dependable computing through hardware transient fault tolerance (FT) in an application system. Transients often cause soft errors in a processing system resulting in mission failure. Errors in program flow, instruction codes, and application data are often caused by electrical fast transients. However, firmware and software fixes can have an important role in designing an ESD, or EMP-resistant system and are more cost effective than hardware. This technique is useful for detecting and recovering transient hardware faults or random bit errors in memory while an application is in execution. The proposed single-version software fix is a practical, useful, and economic tool for both offline and online memory scrubbing of an application system without using conventional N versions of software (NVS) and hardware redundancy in an application like a frequency measurement system",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1692282,no,undetermined,0
A Software Simulation Study of a MD DS/SSMA Communication System with Adaptive Channel Coding,"Studies have shown that adaptive forward error correction (FEC) coding schemes enable a communication system to take advantage of varying channel conditions by switching to less powerful and/or less redundant FEC channel codes when conditions are good, thus enabling an increase in system throughput. The focus of this study is the simulation performance of a complete simulated multi-dimensional (MD) direct-sequence spread spectrum multiple access (DS/SSMA) communication system that employs an advanced adaptive channel coding scheme. The system is simulated and evaluated over a fully user-definable software-based multi-user (MU) multipath fading channel simulator (MFCS). Channel conditions are varied and the switching and adaptation performance of the system is monitored and evaluated. Sensing for adaptation is made possible by a sophisticated quality-of-service monitoring unit (QoSMU) that uses a sophisticated pseudo-error-rate (PER) extrapolation technique to estimate the system's true probability-of-error in real-time, without the need for known transmitted data. The system attempts to keep the estimated bit-error-rate (BER) performance within a predetermined range by switching between different FEC codes as conditions change. This paper commences with a short overview of each of the functional units of the system. Lastly, the simulation results for the coded and uncoded BER performances, as well as the real-time adaptation performance of the system are presented and discussed. This paper conclusively proves that adaptive coded systems have large throughput utilization advantages over that of fixed coded systems",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1698491,no,undetermined,0
AutoTest: A Tool for Automatic Test Case Generation in Spreadsheets,"In this paper we present a system that helps users test their spreadsheets using automatically generated test cases. The system generates the test cases by backward propagation and solution of constraints on cell values. These constraints are obtained from the formula of the cell that is being tested when we try to execute all feasible DU associations within the formula. AutoTest generates test cases that execute all feasible DU pairs. If infeasible DU associations are present in the spreadsheet, the system is capable of detecting and reporting all of these to the user. We also present a comparative evaluation of our approach against the """"Help Me Test"""" mechanism in Forms/3 and show that our approach is faster and produces test suites that give better DU coverage",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1698760,no,undetermined,0
Investigation of radiometric partial discharge detection for use in switched HVDC testing,"This paper reports on initial trials of a non-contact radio frequency partial discharge detection technique that has potential for use within fast switching HVDC test systems. Electromagnetic environments of this type can arise within important electrical transmission nodes such converter stations, so the methods described could in future be useful for condition monitoring purposes. The radiometric technique is outlined and the measurement system and its components are described. Preliminary field trials are reported and results presented for a discharge detected in part of the HV test system during set-up for long-term testing of a reactor. The calculated and observed locations of the discharge were in agreement to within 60 cm inside a test housing of diameter 5 m and height 8 m. Techniques for improving the location accuracy are discussed. The issue of data volume presents a considerable challenge for the RF measurement techniques. On the basis of observations, strategies for moving towards automated interpretation of the partial discharge signals are proposed, which will make use of intelligent software techniques",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1709200,no,undetermined,0
Reliability analysis of protective relays in fault information processing system in China,"The reliability indices of protective relays are first put forward in this paper. A Markov probability model is then established to evaluate the reliability of relay protection. With the state space analytical method, all the steady state probabilities and state transition probabilities can be calculated utilizing the data stored in the fault information processing system. We can get an equation that represents the influence of routine test intervals on relay unavailability. Based on this, the optimum routine test interval for protective relays can be determined. This paper also proposes an efficient method of processing large amount of information by the fault information processing system and evaluating the reliability of protective relays with it, and the corresponding software package is also developed. The application of it to an actual power system in China proves the method to be correct and effective",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1709189,no,undetermined,0
Use of wavelets for out of step blocking function of distance relays,"Out of step blocking function in distance relays is required to distinguish between a power swing and a fault. Detection of symmetrical faults during power swings can present a challenge. In cases when the values of the apparent impedances seen by a distance relay before and after a three-phase fault during a power swing are very close, most of the proposed schemes can run into problems. If such values fall into zone-1 of the relay, not only the detection, but the speed of detection is also crucial. This paper introduces wavelet analysis to detect power swings as well as reliably and quickly detect a symmetrical fault during a power swing. Total number of dyadic wavelet levels of voltage/current waveforms and the choice of particular levels for such detection are carefully studied. Different power swing conditions and fault instants are simulated with PSCAD/EMTDCreg software to test the proposed methodology",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1709171,no,undetermined,0
Practical application of probabilistic reliability analyses,"Liberalization of the energy markets has increased the cost pressure on network operators significantly. Corresponding cost saving measures in general will have negative effects on quality of supply, especially on supply reliability. On the other hand, a decrease of supply reliability is not acceptable for customers and politicians - and the public awareness was focused by several blackouts in the European and American transmission systems. In order to handle this delicate question of balancing network costs and supply reliability, detailed and above all quantitative information is required in the network planning process. A suitable tool for this task is probabilistic reliability analysis, which has already been in use for several years successfully. The method and possible applications are briefly described here and application is demonstrated with various examples from practical studies focusing on distribution systems. The results prove that reliability analyses are becoming an indispensable component of customer-oriented network planning",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1709155,no,undetermined,0
Dependability analysis: performance evaluation of environment configurations,Prototyping-based fault injection environments are employed to perform dependability analysis and thus predict the behavior of circuits in presence of faults. A novel environment has been recently proposed to perform several types of dependability analyses in a common optimized framework. The approach takes advantage of hardware speed and of software flexibility to achieve optimized trade-offs between experiment duration and processing complexity. This paper discusses the possible repartition of tasks between hardware and embedded software with respect to the type of circuit to analyze and to the instrumentation achieved. The performances of the approach are evaluated for each configuration of the environment,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1708652,no,undetermined,0
On the Use of Mutation Faults in Empirical Assessments of Test Case Prioritization Techniques,"Regression testing is an important activity in the software life cycle, but it can also be very expensive. To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of test case prioritization techniques is to increase a test suite's rate of fault detection (how quickly, in a run of its test cases, that test suite can detect faults). Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited primarily to hand-seeded faults, largely due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults and that the use of hand-seeded faults can be problematic for the validity of empirical results focusing on fault detection. We have therefore designed and performed two controlled experiments assessing the ability of prioritization techniques to improve the rate of fault detection of test case prioritization techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. More importantly, a comparison of our results with those collected using hand-seeded faults reveals several implications for researchers performing empirical studies of test case prioritization techniques in particular and testing techniques in general",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1707670,no,undetermined,0
Development of circuit models for extractor components in high power microwave sources,"Summary form only given. The state-of-the-art in high power microwave (HPM) sources has greatly improved in recent years, in part due to advances in the computational tools available to analyze such devices. Chief among these advances is the widespread use of parallel particle-in-cell (PIC) techniques. Parallel PIC software allows high fidelity, three-dimensional, electromagnetic simulations of these complex devices to be performed. Despite these advances, however, parallel PIC software could be greatly supplemented by fast-running parametric codes specifically designed to mimic the behavior of the source in question. These tools can then be used to develop zero-order point designs for eventual assessment via full PIC simulation. One promising technique for these parametric formulations is circuit models, where in the full field description is reduced to capacitances, inductances, and resistances that can be quickly solved to yield small signal growth rates, resonant frequencies, quality factors, and potentially efficiencies. Building on the extensive literature from the vacuum electronics community, this poster will investigate the circuit models associated with the purely electromagnetic components of the extractor in the absence of space charge. Specifically, three-dimensional time-domain computational electromagnetics (AFRL's ICEPIC software) will be used to investigate the modification of the resonant frequencies and mode quality factors as a function of slot and load geometry. These field calculations will be reduced to circuit parameters for potential inclusion in parametric models, and the fidelity of the resulting description will be assessed",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1707288,no,undetermined,0
Checking Properties on the Control of Heterogeneous Systems,"We present a component-based description language for heterogeneous systems composed of several data flow processing components and a unique event- based controller. Descriptions are used both for generating and deploying implementation code and for checking safety properties on the system. The only constraint is to specify the controller in a synchrounous reactive language. We propose an analysis tool which transforms temporal logic properties of the system as a whole into properties on the events of the controller, and hence into synchronous reactive observers. If checks succeed, the final system is therefore correct by construction. When it is not possible to generate observers that correspond exactly to the specified properties, our tool is capable of generating approximate observers. Alghough the results given by these are subject to interpretation, they can nevertheless prove useful and help detect defects or even guarantee the correctness of a system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567001,no,undetermined,0
Matching Antipatterns to Improve the Quality of Use Case Models,"Use case modeling is an effective technique used to capture functional requirements. Use case models are mainly composed of textual descriptions written in natural language and simple diagrams that adhere to a few syntactic rules. This simplicity can be deceptive as many modelers create use case models that are incorrect, inconsistent, and ambiguous and contain restrictive design decisions. In this paper, a new methodology is described that utilizes antipatterns to detect potentially defective areas in use case models. This paper introduces the tool ARBIUM, which will support the proposed technique and aid analysts to improve the quality of their models. ARBIUM presents a framework that will allow developers to define their own antipatterns using OCL and textual descriptions. The proposed approach and tool are applied to a distributed biodiversity database use case model to demonstrate its feasibility. Our results indicate that they can improve the overall clarity and precision of use case models",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1704053,no,undetermined,0
Towards Regulatory Compliance: Extracting Rights and Obligations to Align Requirements with Regulations,"In the United States, federal and state regulations prescribe stakeholder rights and obligations that must be satisfied by the requirements for software systems. These regulations are typically wrought with ambiguities, making the process of deriving system requirements ad hoc and error prone. In highly regulated domains such as healthcare, there is a need for more comprehensive standards that can be used to assure that system requirements conform to regulations. To address this need, we expound upon a process called semantic parameterization previously used to derive rights and obligations from privacy goals. In this work, we apply the process to the privacy rule from the U.S. Health Insurance Portability and Accountability Act (HIPAA). We present our methodology for extracting and prioritizing rights and obligations from regulations and show how semantic models can be used to clarify ambiguities through focused elicitation and to balance rights with obligations. The results of our analysis can aid requirements engineers, standards organizations, compliance officers, and stakeholders in assuring systems conform to policy and satisfy requirements",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1704048,no,undetermined,0
Using Mutation Analysis for Assessing and Comparing Testing Coverage Criteria,"The empirical assessment of test techniques plays an important role in software testing research. One common practice is to seed faults in subject software, either manually or by using a program that generates all possible mutants based on a set of mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults, thus facilitating the statistical analysis of fault detection effectiveness of test suites; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. Focusing on four common control and data flow criteria (block, decision, C-use, and P-use), this paper investigates this important issue based on a middle size industrial program with a comprehensive pool of test cases and known faults. Based on the data available thus far, the results are very consistent across the investigated criteria as they show that the use of mutation operators is yielding trustworthy results: generated mutants can be used to predict the detection effectiveness of real faults. Applying such a mutation analysis, we then investigate the relative cost and effectiveness of the above-mentioned criteria by revisiting fundamental questions regarding the relationships between fault detection, test suite size, and control/data flow coverage. Although such questions have been partially investigated in previous studies, we can use a large number of mutants, which helps decrease the impact of random variation in our analysis and allows us to use a different analysis approach. Our results are then; compared with published studies, plausible reasons for the differences are provided, and the research leads us to suggest a way to tune the mutation analysis process to possible differences in fault detection probabilities in a specific environment",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703390,no,undetermined,0
Investigating the dimensionality problem of Adaptive Random Testing incorporating a local search technique,"Adaptive random testing (ART) has been proposed to enhance the effectiveness of random testing. By spreading test cases evenly within the input domain, ART techniques may reduce the number of test cases necessary to detect the first failure by up to 50%. However, the most effective ART strategies are little effective in higher dimen- sions. This fact distinctly affects their applicability since in a real testing area input domains usually are far from being one- or two-dimensional. The present work addresses this problem. It discusses the shortcomings of existing solu- tions and describes how prior knowledge can help solving the problem. Since in general no prior knowledge is avail- able, this work proposes a solution which--though not fully solving the dimensionality problem--seems to be very close to the theoretical optimum. The proposed approach is based on the ideas of the local search technique 'Hill Climbing'.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567014,no,undetermined,0
Analysis of Restart Mechanisms in Software Systems,"Restarts or retries are a common phenomenon in computing systems, for instance, in preventive maintenance, software rejuvenation, or when a failure is suspected. Typically, one sets a time-out to trigger the restart. We analyze and optimize time-out strategies for scenarios in which the expected required remaining time of a task is not always decreasing with the time invested in it. Examples of such tasks include the download of Web pages, randomized algorithms, distributed queries, and jobs subject to network or other failures. Assuming the independence of the completion time of successive tries, we derive computationally attractive expressions for the moments of the completion time, as well as for the probability that a task is able to meet a deadline. These expressions facilitate efficient algorithms to compute optimal restart strategies and are promising candidates for pragmatic online optimization of restart timers",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703386,no,undetermined,0
Lightweight Fault Localization with Abstract Dependences,"Locating faults is one of the most time consuming tasks in today's fast paced economy. Testing and formal verification techniques like model-checking are usually used for detecting faults but do not attempt to locate the root-cause for the detected faulty behavior. This article makes use of abstract dependences between program variables for locating faults in programs. We discuss the basic ideas, the underlying theory, and first experimental results, as well our model's limitations. Our fault localization model is based on a previous work that uses the abstract dependences for fault detection. First case studies indicate our model's practical applicability",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703183,no,undetermined,0
A New Genetic Algorithm Approach for Secure JPEG Steganography,"Steganography is the act of hiding a message inside another message in such a way that can only be detected by its intended recipient. Naturally, there are security agents who would like to fight these data hiding systems by steganalysis, i.e. discovering covered messages and rendering them useless. There is currently no steganography system which can resist all steganalysis attacks. In this paper we propose a novel GA evolutionary process to make a secure steganographic encoding on JPEG images. Our steganography step is based on OutGuess which is proved to be the least vulnerable steganographic system. A combination of OutGuess steganalysis approach and maximum absolute difference (MAD) for the image quality are used as the GA fitness function. The model presented here is based on JPEG images; however, the idea can potentially be used in other multimedia steganography as well",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703168,no,undetermined,0
IMPRES: integrated monitoring for processor reliability and security,"Security and reliability in processor based systems are concerns requiring adroit solutions. Security is often compromised by code injection attacks, jeopardizing even 'trusted software'. Reliability is of concern where unintended code is executed in modern processors with ever smaller feature sizes and low voltage swings causing bit flips. Countermeasures by software-only approaches increase code size by large amounts and therefore significantly reduce performance. Hardware assisted approaches add extensive amounts of hardware monitors and thus incur unacceptably high hardware cost. This paper presents a novel hardware/software technique at the granularity of micro-instructions to reduce overheads considerably. Experiments show that our technique incurs an additional hardware overhead of 0.91% and clock period increase of 0.06%. Average clock cycle and code size overheads are just 11.9% and 10.6% for five industry standard application benchmarks. These overheads are far smaller than have been previously encountered",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688849,no,undetermined,0
A New Methodology for Predicting Software Reliability in the Random Field Environments,"This paper presents a new methodology for predicting software reliability in the field environment. Our work differs from some existing models that assume a constant failure detection rate for software testing and field operation environments, as this new methodology considers the random environmental effects on software reliability. Assuming that all the random effects of the field environments can be captured by a unit-free environmental factor, eta, which is modeled as a random-distributed variable, we establish a generalized random field environment (RFE) software reliability model that covers both the testing phase and the operating phase in the software development cycle. Based on the generalized RFE model, two specific random field environmental reliability models are proposed for predicting software reliability in the field environment: the gamma-RFE model, and the beta-RFE model. A set of software failure data from a telecommunication software application is used to illustrate the proposed models, both of which provide very good fittings to the software failures in both testing and operation environments. This new methodology provides a viable way to model the user environments, and further makes adjustments to the reliability prediction for similar software products. Based on the generalized software reliability model, further work may include the development of software cost models and the optimum software release policies under random field environments",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688081,no,undetermined,0
The Application of Evidence Theory in the Field of Equipment Fault Diagnosis,"In this paper, we explain the fusion technology of information briefly, and discuss the general course and the merged rule about the equipment fault diagnoses by the D-S evidence theory in detail. We give the relation between basic probability assignment and matrix by the location operation of C-language, and obtain the basic probability assignment by the Matlab software which make the matrix operation easier. We diagnose the fault of the voltage transformer using the D-S evidence theory",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1714193,no,undetermined,0
Data Warehousing Process Maturity: An Exploratory Study of Factors Influencing User Perceptions,"This paper explores the factors influencing perceptions of data warehousing process maturity. Data warehousing, like software development, is a process, which can be expressed in terms of components such as artifacts and workflows. In software engineering, the Capability Maturity Model (CMM) was developed to define different levels of software process maturity. We draw upon the concepts underlying CMM to define different maturity levels for a data warehousing process (DWP). Based on the literature in software development and maturity, we identify a set of features for characterizing the levels of data warehousing process maturity and conduct an exploratory field study to empirically examine if those indeed are factors influencing perceptions of maturity. Our focus in this paper is on managerial perceptions of DWP. The results of this exploratory study indicate that several factors-data quality, alignment of architecture, change management, organizational readiness, and data warehouse size-have an impact on DWP maturity, as perceived by IT professionals. From a practical standpoint, the results provide useful pointers, both managerial and technological, to organizations aspiring to elevate their data warehousing processes to more mature levels. This paper also opens up several areas for future research, including instrument development for assessing DWP maturity",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1661915,no,undetermined,0
VoIP service quality monitoring using active and passive probes,"Service providers and enterprises all over the world are rapidly deploying Voice over IP (VoIP) networks because of reduced capital and operational expenditure, and easy creation of new services. Voice traffic has stringement requirements on the quality of service, like strict delay and loss requirements, and 99.999% network availability. However, IP networks have not been designed to easily meet the above requirements. Thus, service providers need service quality management tools that can proactively detect and mitigate service quality degradation of VoIP traffic. In this paper, we present active and passive probes that enable service providers to detect service impairments. We use the probes to compute the network parameters (delay, loss and jitter) that can be used to compute the call quality as a Mean Opinion Score using a voice quality metric, E-model. These tools can be used by service providers and enterprises to identify network impairments that cause service quality degradation and take corrective measures in real time so that the impact on the degradation perceived by end-users is minimal",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665188,no,undetermined,0
An Efficient Radio Admission Control Algorithm for 2.5G/3G Cellular Networks,"We design an efficient radio admission control algorithm that minimizes blocking probability subject to the condition that the overload probability is smaller than a pre-specified threshold. Our algorithm is quite general and can be applied to both TDMA-based cellular technologies, such as GPRS and EDGE, and CDMA-based technologies, such as UMTS and CDMA2000. We extend prior work in measurement-based admission control in wireline networks to wireless cellular networks and to heterogeneous users. We take the variance of the resource requirement into account while making the admission decision. Using simulation results, we show that our admission control algorithm is able to meet the target overload probability over a range of call arrival rates and radio conditions. We also compare our scheme with a simple admission control algorithm and also show how to use our approach for the carrier selection problem",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665171,no,undetermined,0
Continuous geodetic time-transfer analysis methods,"We address two issues that limit the quality of time and frequency transfer by carrier phase measurements from the Global Positioning System (GPS). The first issue is related to inconsistencies between code and phase observations. We describe and classify several types of events that can cause inconsistencies and observe that some of them are related to the internal clock of the GPS receiver. Strategies to detect and overcome time-code inconsistencies have been developed and implemented into the Bernese GPS software package. For the moment, only inconsistencies larger than the 20 ns code measurement noise level can be detected automatically. The second issue is related to discontinuities at the day boundaries that stem from the processing of the data in daily batches. Two new methods are discussed: clock handover and ambiguity stacking. The two approaches are tested on data obtained from a network of stations, and the results are compared with an independent time-transfer method. Both methods improve the stability of the transfer for short averaging times, but there is no benefit for averaging times longer than 8 days. We show that continuous solutions are sufficiently robust against modeling and preprocessing errors to prevent the solution from accumulating a permanent bias.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665073,no,undetermined,0
Visualization for a Multi-Sensor Data Analysis,"This paper describes our efforts in creating the software in order to analyze the multi-sensor data for gas transmission pipeline inspection. The amount of data is usually considerable because the hardware system that consists of multiple heterogeneous sensors records multi-sensor values for long-distance inspection. It imposes a heavy burden on the operators who should sieve the huge and complex data, detect features of the pipeline and decide a feature as a significant defect. In our system, the virtual 3D pipeline helps the user to examine the inside of pipeline intuitively by navigating according to the realistic pipeline trajectory. We mapped the geographical data of the pipeline and heterogeneous sensor data on the virtual 3D pipeline. Moreover, our system offer the various feature detail views to help the users rapid and precise decision. Users can switch the navigation mode and the feature detail mode easily. Consequently, the virtual pipeline plays a role as an intuitive interaction metaphor for pipeline inspection",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1663768,no,undetermined,0
Enabling Self-Managing Applications using Model-based Online Control Strategies,"The increasing heterogeneity, dynamism and uncertainty of emerging DCE (Distributed Computing Environment) systems imply that an application must be able to detect and adapt to changes in its state, its requirements and the state of the system to meet its desired QoS constraints. As system and application scales increase, ad hoc heuristic-based approaches to application adaptation and self-management quickly become insufficient. This paper builds on the Accord programming system for rule-based self-management and extends it with model-based control and optimization strategies. This paper also presents the development of a self-managing data streaming service based on online control using Accord. This service is part of a Grid-based fusion simulation workflow consisting of long-running simulations, executing on remote supercomputing sites and generating several terabytes of data, which must then be streamed over a wide-area network for live analysis and visualization. The self-managing data streaming service minimize data streaming overheads on the simulations, adapt to dynamic network bandwidth and prevent data loss. An evaluation of the service demonstrating its feasibility is presented.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1662377,no,undetermined,0
Building statistical test-cases for smart device software - an example,"Statistical testing (ST) of software or logic-based components can produce dependability information on such components by yielding an estimate for their probability of failure on demand. An example of software-based components that are increasingly used within safety-related systems e.g. in the nuclear industry, are smart devices. Smart devices are devices with intelligence, capable of more than merely representing correctly a sensed quantity but of functionality such as processing data, self-diagnosis and possibly exchange of data with other devices. Examples are smart transmitters or smart sensors. If such devices are used in a safety-related context, it is crucial to assess whether they fulfil the dependability requirements posed on them to ensure they are dependable enough to be used within the specific safety-related context. This involves making a case for the probability of systematic failure of the smart device. This failure probability is related to faults present in the logic or software-based part of the device. In this paper we look at a technique that can be used to establish a probability of failure for the software part of a smart monitoring unit. This technique is """"statistical testing"""" (ST). Our aim is to share our own experience with ST and to describe some of the issues we have encountered so far on the way to perform ST on this device software.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1662253,no,undetermined,0
Assessing the effectiveness of static code analysis,For complex systems identifying and mitigating a gap between suppliers provided software and customer certification needs is difficult. Getting it wrong can cause program delays or even project failure. A mitigation strategy is to carry out additional assurance analysis such as static code analysis (SCA). This can add significantly to the procurement expense and may require repeating with new software upgrades. The purpose of this paper is to present an analysis of the effectiveness of nearly 10 years efforts of additional independent SCA assurance on a large software intensive project. The evidence presented also is supported by SCA findings on other projects conducting additional SCA. The analysis work was carried out for a Ministry of Defence Integrated Project Team as part of their continual assessment and improvement of safety.,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1662247,no,undetermined,0
Probabilistic fusion of stereo with color and contrast for bilayer segmentation,"This paper describes models and algorithms for the real-time segmentation of foreground from background layers in stereo video sequences. Automatic separation of layers from color/contrast or from stereo alone is known to be error-prone. Here, color, contrast, and stereo matching information are fused to infer layers accurately and efficiently. The first algorithm, layered dynamic programming (LDP), solves stereo in an extended six-state space that represents both foreground/background layers and occluded regions. The stereo-match likelihood is then fused with a contrast-sensitive color model that is learned on-the-fly and stereo disparities are obtained by dynamic programming. The second algorithm, layered graph cut (LGC), does not directly solve stereo. Instead, the stereo match likelihood is marginalized over disparities to evaluate foreground and background hypotheses and then fused with a contrast-sensitive color model like the one used in LDP. Segmentation is solved efficiently by ternary graph cut. Both algorithms are evaluated with respect to ground truth data and found to have similar performance, substantially better than either stereo or color/contrast alone. However, their characteristics with respect to computational efficiency are rather different. The algorithms are demonstrated in the application of background substitution and shown to give good quality composite video output",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1661549,no,undetermined,0
Building a reliable internet core using soft error prone electronics,"This paper describes a methodology for building a reliable internet core router that considers the vulnerability of its electronic components to single event upset (SEU). It begins with a set of meaningful system level metrics that can be related to product reliability requirements. A specification is then defined that can be effectively used during the system architecture, silicon and software design process. The system can then be modeled at an early stage to support design decisions and trade-offs related to potentially costly mitigation strategies. The design loop is closed with an accelerated measurement technique using neutron beam irradiation to confirm that the final product meets the specification.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567283,no,undetermined,0
A core plug and play architecture for reusable flight software systems,"The Flight Software Branch, at Goddard Space Flight Center (GSFC), has been working on a run-time approach to facilitate a formal software reuse process. The reuse process is designed to enable rapid development and integration of high-quality software systems and to more accurately predict development costs and schedule. Previous reuse practices have been somewhat successful when the same teams are moved from project to project. But this typically requires taking the software system in an all-or-nothing approach where useful components cannot be easily extracted from the whole. As a result, the system is less flexible and scalable with limited applicability to new projects. This paper will focus on the rationale behind, and implementation of, the run-time executive. This executive is the core for the component-based flight software commonality and reuse process adopted at Goddard",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1659589,no,undetermined,0
Practical application of model-based programming and state-based architecture to space missions,"Innovative systems and software engineering solutions are required to meet the increasingly challenging demands of deep-space robotic missions. While recent advances in the development of integrated systems and software engineering approaches have begun to address some of these issues, these methods are still at the core highly manual and, therefore, error-prone. This paper describes a task aimed at infusing MIT's model-based executive, Titan, into JPL's Mission Data System (MDS), a unified state-based architecture, systems engineering process, and supporting software framework. Results of the task are presented, including a discussion of the benefits and challenges associated with integrating mature model-based programming techniques and technologies into a rigorously-defined domain specific architecture",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1659538,no,undetermined,0
Designing for recovery [software design],"How should you design your software to detect, react, and recover from exceptional conditions? If you follow Jim Shore's advice and design with a fail fast attitude, you won't expend any effort recovering from failures. Shore argues that a """"patch up and proceed"""" strategy often obfuscates problems. Shore's simple design solution is to write code that checks for expected values upon entry and returns failure notifications when it can't fulfil its responsibilities. He argues that careful use of assertions allows for early and visible failure, so you can quickly identify and correct problems",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1657931,no,undetermined,0
Recent case studies in bearing fault detection and prognosis,"This paper updates current efforts by the authors to develop fully-automated, online incipient fault detection and prognosis algorithms for drivetrain and engine bearings. The authors have developed and evolved ImpactEnergytrade, a feature extraction and analysis driven system that integrates high frequency vibration/acoustic emission data, collected using accelerometers and other sensors such as a laser interferometer to assess the health of bearings and gearboxes in turbine engines. ImpactEnergy combines advanced diagnostic features derived from waveform analysis, high-frequency enveloping, and more traditional time domain processing like root mean square (RMS) and kurtosis with classification techniques to provide bearing health information. The adaptable algorithm suite has been applied across numerous air vehicle relevant programs for the Air Force, Navy, Army, and DARPA. The techniques presented in this paper are tested and validated in a laboratory environment by monitoring multiple bearings on test rigs that replicate the operational loads of a turbomachinery environment. The capability of the software on full-scale test rigs at major OEMs (original equipment manufacturer) locations will be shown with specific data results. The team will review developments across these multiple programs and discuss specific implementation efforts to transition to the fleet in a variety of manned and unmanned platforms",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656077,no,undetermined,0
Ontological approach to improving design quality,"The creation of quality software depends on the existence of a quality software design. In particular, it is important to identify inconsistencies that might be injected at the design level. We have developed a common ontology that integrates software specification knowledge and software design knowledge in order to facilitate the interoperability of formal requirements modeling tools and software design tools with the end goal of detecting errors in software designs. Our approach focuses initially on the integration of unified modeling language (UML) with the formal requirements modeling language, knowledge acquisition in automated specification (KAOS), in order to help automate the detection of inconsistencies in UML designs thereby enhancing the quality of the original design and ultimately integrating the multiple views inherent in UML. We demonstrate the integration of UML and KAOS with an elevator system case study",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656058,no,undetermined,0
Test results for the WAAS Signal Quality Monitor,"The signal quality monitor (SQM) is an integrity monitor for the wide area augmentation system (WAAS). The monitor detects L1 signal waveform deformation of a GPS or a geosynchronous (GEO) satellite monitored by WAAS should that event occur. When a signal deformation occurs, the L1 correlation function measured by the receiver becomes distorted. The distortion will result in an error in the L1 pseudorange. The size of the error depends on the design characteristics of the user receiver. This paper describes test results for the WAAS SQM conducted using prototype software. There are two groups of test cases: the nominal testing and the negative path testing. For nominal test cases, recorded data are collected from a test facility in four 5-day periods. These four data sets include SQM correlation values for SV-receiver pairs, and satellite error bounds for satellites. They are used as input to the prototype. The prototype processes these data sets, executes the algorithm, and records test results. Parameters such as the """"maximum median-adjusted detection metric over threshold"""" (i.e., the maximum detection test), """"UDRE forwarded from upstream integrity monitors,"""" and """"UDRE supported by SQM"""" are shown and described. The magnitude of the maximum detection test for all GPS and GEO satellites are also shown. For negative path testing, this paper describes two example simulated signal deformation test cases. A 2-day data set is collected from the prototype. A few example ICAO signal deformations are simulated based on this data set and are inserted in different time slots in the 2-day period. The correlator measurements for selected satellites are pre- processed to simulate the signal deformation. The results demonstrate the sensitivity of the Signal Quality Monitor to the simulated deformation, and shows when the event is detected and subsequently cleared. It also shows that the SQM will not adversely affect WAAS performance.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4570015,no,undetermined,0
CEDA: control-flow error detection through assertions,"This paper presents an efficient software technique, control flow error detection through assertions (CEDA), for online detection of control flow errors. Extra instructions are automatically embedded into the program at compile time to continuously update run-time signatures and to compare them against pre-assigned values. The novel method of computing run-time signatures results in a huge reduction in the performance overhead, as well as the ability to deal with complex programs and the capability to detect subtle control flow errors. The widely used C compiler, GCC, has been modified to implement CEDA, and the SPEC benchmark programs were used as the target to compare with earlier techniques. Fault injection experiments were used to evaluate the fault detection capabilities. Based on a new comparison metric, method efficiency, which takes into account both error coverage and performance overhead, CEDA is found to be much better than previously proposed methods",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1655535,no,undetermined,0
The Checkpoint Interval Optimization of Kernel-Level Rollback Recovery Based on the Embedded Mobile Computing System,"Due to the limited resources of the embedded mobile computing systems, such as wearable computers, PDAs or sensor nodes, reducing the overhead of the software implemented fault tolerance mechanism is a key factor in reliability design. Two checkpoint interval optimization techniques of kernel level rollback recovery mechanism are discussed. Step checkpointing algorithm modulates checkpoint intervals on-line according to the characteristics of software or hardware environment-dependent system that the failure rate fluctuates acutely shortly after the system fails. Checkpoint size monitoring and threshold-control technique adjusts the checkpoint interval by predicting the amount of data to be saved. Combining these two techniques can effectively improve the performance of the embedded mobile computer system.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4568557,no,undetermined,0
A Low-level Simulation Study of Prioritization in IEEE 802.11e Contention-based Networks,"This work deals with the performance evaluation of the IEEE 802.11e EDCA proposal for service prioritization in wireless LANs. A large amount of study has been carried out in the scientific community to evaluate the performance of the EDCA proposal, mainly in terms of throughput and access delay differentiation. However, we argue that further performance insights are needed in order to fully understand the principles behind the EDCA prioritization mechanisms. To this purpose, rather than limit our investigation on throughput and delay performance figures, we take a closer look to their operation also in terms of low-level performance metrics (such as probability of accessing specific channel slots). The paper contribution is threefold: first, we specify a detailed NS2 simulation model by enlightening the typical mis-configuration and errors that may occur when NS2 is used as simulation platform for WLANs and we cross-validate the simulation results with our custom-made C++ simulation tool; second, we describe some performance figures related to the different forms of prioritization provided by the EDCA mechanisms; finally, we verify some assumptions commonly used in the EDCA analytical models",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665237,no,undetermined,0
Case study of ANSI standard delta-wye distribution transformer,"An over current relay (51N) of a distribution feeder that feeds a branchy overhead line failed to eliminate a fault, which resulted from a fallen conductor at the primary side of the delta-wye distribution transformer. This was linked to the earth's high resistance. In the first part of this paper, the aforementioned reason will be proven to be not credible and that the load-side fallen conductor resulted in a very small return current that cannot be detected. In the second part of this paper, the fact that the voltage will build up at the distribution transformer primary side in case the transformer has a primary single phase blown fuse and unbalance loads will be presented",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1665293,no,undetermined,0
Photovoltaic Power Conditioning System With Line Connection,"A photovoltaic (PV) power conditioning system (PCS) with line connection is proposed. Using the power slope versus voltage of the PV array, the maximum power point tracking (MPPT) controller that produces a smooth transition to the maximum power point is proposed. The dc current of the PV array is estimated without using a dc current sensor. A current controller is suggested to provide power to the line with an almost-unity power factor that is derived using the feedback linearization concept. The disturbance of the line voltage is detected using a fast sensing technique. All control functions are implemented in software with a single-chip microcontroller. Experimental results obtained on a 2-kW prototype show high performance such as an almost-unity power factor, a power efficiency of 94%, and a total harmonic distortion (THD) of 3.6%",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1667902,no,undetermined,0
How Much Software Quality Investment Is Enough: A Value-Based Approach,"This article draws on results from the emerging field of value-based software engineering (VBSE). VBSE aims to provide a quantitative approach to questions as how much software quality investment is enough. Based on the COCOMO II cost-estimation model and the COQUALMO quality-estimation model, quantitative risk analysis helps determine when to stop testing software and release the product. Further, we show how the model and approach can assess the relative payoff of value-based testing as compared to value-neutral testing",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1687868,no,undetermined,0
Two architectures for testing distributed real-time systems,"A real-time system is a system that is required to react to stimuli from the environment within time intervals dictated by the environment. In real-time applications, the timing requirements are the main constraints and their mastering is the predominant factor for assessing the quality of service. The safety-critical nature of their domain and their inherent complexity advocate the use of formal methods in the software development process. Testing is one of the formal techniques that can be used to ensure the quality of real-time systems. This paper addresses and proposes a centralized architecture and a distributed architecture for the execution of test cases on distributed real-time systems. These two architectures are implemented in CORBA and Java. The specification model used is n-ports timed input output automata, a variant of timed automata of Alur and Dill (1994)",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1684961,no,undetermined,0
CCK: An Improved Coordinated Checkpoint/Rollback Protocol for Dataflow Applications in Kaapi,Fault tolerance protocols play an important role in today long runtime scientific parallel applications because the probability of failure may be important due to the number of unreliable components involved during simulation. In this paper we present our approach and preliminary results about a new checkpoint/recovery protocol based on a coordinated scheme. This protocol is highly coupled to the availability of an abstract representation of the execution,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1684955,no,undetermined,0
Fault Tolerance in Mobile Agent Systems by Cooperating the Witness Agents,"Mobile agents travel through servers for perform their programs and fault tolerance is fundamental and important in their itinerary. In the paper, are considering and described existent methods of fault tolerance in mobile agents. Then the method is considered that which uses cooperating agents to fault tolerance and to detect server and agent failure, meaning three type of agents involved: actual agent which performs programs for its owner, witness agent which monitors the actual agent and the witness agent after itself, probe which is sent for recovery the actual agent or the witness agent on the side of the witness agent. Traveling agent through servers, the witness is created by actual agent. Scenarios of failure and recovery of server and agent are discussed in the method. During performing the actual agent, the witness agents are increased by addition the servers. Proposed scheme is that minimizes the witness agents as far as possible, because with considering and comparing could concluded that existing all of witness agent is not necessary on the initial servers. Simulation of this method is done by C-Sim",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1684897,no,undetermined,0
Component Reusability and Cohesion Measures in Object-Oriented Systems,"In software component reuse processing, the success of software systems is decided by the quality of components. One important characteristic to measure quality of components is component reusability. Component reusability measures how easily the component can be reused in a new environment. This paper provides a new measure of cohesion developed to assess the reusability of Java components retrieved from the Internet by a search engine. This measure differs from the majority of established metrics in two respects: it reflects the degree of similarity between classes quantitatively, and they also take account of indirect similarities. An empirical comparison of the new measure with the established metrics is described. The new measures are shown to be consistently superior at ranking components according to their reusability",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1684869,no,undetermined,0
Dependency Analysis and Default Tolerance in BHDL,"Most co-design verification methods depend on co-simulation of two or more types of components that are designed by different technologies during the last steps of design. Systems are getting more complex so the necessary time for simulation, detecting and correcting faults increases. BHDL project uses a formal method, B method, at the very early stage of design in order to produce a correct by design multitechnology system. Furthermore, BHDL can take in account the possibility to describe a fault scenario with a suitable correction in order to satisfy an ideal system specification",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1684850,no,undetermined,0
Toward Formal Verification of 802.11 MAC Protocols: a Case Study of Applying Petri-nets to Modeling the 802.11 PCF,"Centralized control functions for the IEEE 802.11 family of WLAN standards are vital for the distribution of traffic with stringent quality of service (QoS) requirements. These centralized control functions overlay a time-based organizational """"super-frame"""" structure on the medium, allocating part of the super-frame to polling traffic and part to contending traffic. This allocation directly determines how well the two forms of traffic are supported. Given the vital role of this allocation in the success of a system, we must have confidence in the configuration used, beyond that provided by empirical simulation results. Formal mathematical methods are a means to conduct rigorous analysis that will permit us such confidence, and the Petri-net formalism offers an intuitive representation with formal semantics. We present an extended Petri-net model of the super-frame, and use this model to assess the performance of different super-frame configurations and the effects of different traffic patterns. We believe that using such a model to analyze performance in this manner is new in itself",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1683019,no,undetermined,0
A practical mtbf estimate for pcb design considering component and non-component failures,"Accurate reliability prediction for MTBF (mean-time-between-failures) is always desirable and anticipated before the new product is ramped up for customer shipment. In reality it is often difficult to obtain the accurate MTBF estimate for a new product due to the lack of the testing time in pilot line and limited field failure data. In this paper, a practical reliability prediction model is presented for predicting the MTBF of PCB (printed-circuit-board) in the design phase. Unlike conventional reliability prediction models, which usually focus on parts failure rates, the method presented here not only incorporates component failures but also non-component failures which include design, software, manufacturing and process issues. Component failure rates are computed using either historical data or the nominal failure rates together with operating conditions such as temperature and electrical derating. Triangular distributions are used to model non-component failure rates due to design errors, software bugs, manufacturing and handling problems. Finally, the confidence intervals for the new product MTBF are obtained based on six-sigma criteria. The method was applied to the design of a DC/analog instrument board that is used in the semiconductor testing equipment",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677440,no,undetermined,0
The risks of applying qualitative reliability prediction methods: a case study,"The fast technological innovation of the past decades contributed to an increasing complexity in products. This increased product complexity together with four different business drivers (time, profitability, functionality and quality) have an important influence on the reliability strategies used within companies. New methods are necessary to predict reliability in product design. In current business processes qualitative reliability prediction methods are often applied to estimate the reliability risks present in products and processes. An example of a popular qualitative reliability prediction method is the so-called failure mode and effects analysis (FMEA). Many successful implementations of the FMEA method are described in literature from various professional fields. On the other hand, several setbacks of the traditional FMEA approach are described in literature. Most of these drawbacks result from the qualitative analysis approach. Nevertheless, the FMEA reliability prediction method is probably the most implemented method in practice. Present-day companies do not seem to take notice of the drawbacks of qualitative reliability prediction methods as described in literature. A convincing reason for this is the fact that no proven alternatives exist for these qualitative methods. Therefore the goal of this paper is to illustrate the risks of applying qualitative reliability prediction methods in practice and make suggestions for improving the application of these methods. This illustration is based on a complete reliability prediction approach named ROMDA. This ROMDA approach adopts FMEA to predict product reliability and will be presented in the second section. Subsequently this ROMDA approach is applied in a practical situation after which the reliability predictions are evaluated. Based on this evaluation, general conclusions and recommendations are described in order to improve the application of qualitative reliability prediction methods in practice",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677428,no,undetermined,0
Modeling and analysis of causes and consequences of failures,"This paper presents a computer-supported method for modeling and analyzing causes and consequences of failures. The developed method is one of the main results from a nine-year research project, which was completed in February 2005 and carried out by Tampere University of Technology. The applicability of the developed methods and software has been tested in the companies, which have been involved in the research project. The participating companies are both manufacturers and users in metal, energy, process and electronics industries. Their products and systems have to respond to high safety and reliability demands. Most of the participating companies have started to apply the proposed method and software for modeling and analysis of failure logic for their products and systems. The application of the method forces experts to identify all potential component hardware failures, human errors, possible disturbances and deviations in the process, and environmental conditions related to the selected TOP-event. Based on experience, and with the help of the methods, it is possible to find out those problem areas of the design stage, which can delay product development and/or reduce safety and reliability",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677424,no,undetermined,0
Risk assessment of real time digital control systems,"This paper describes stochastic methods for assessing risk in integrated hardware and software systems. The methods assess evaluate availability, outage probabilities, and effectiveness-weighted degraded states based on data from measurements with a specified confidence level. System-level reliability/availability models can also identify the elements where failure rate, recovery probability, or recovery time improvement will provide the greatest benefit. The validity of this approach is determined by the extent to which the system failure behavior conforms to a stochastic process (i.e., random, non-deterministic failures). Evidence from large studies of other high availability computer systems provides substantial evidence of such behavior in mature systems. The approach is limited to the systems with failure rates higher than 10<sup>-6</sup>per hour and the availability below 0.999999, i.e., below safety grade. To assess safety critical systems, the risk assessment method described here can be used as an adjunct for other approaches described in various industry standards that intended to minimize the likelihood that deterministic defects are introduced into the system design",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677409,no,undetermined,0
Methodology for maintainability-based risk assessment,"A software product spends more than 65% of its lifecycle in maintenance. Software systems with good maintainability can be easily modified to fix faults or to adapt to changing environment. We define maintainability-based risk as a product of two factors: the probability of performing maintenance tasks and the impact of performing these tasks. In this paper, we present a methodology for assessing maintainability-based risk to account for changes in the system requirements. The proposed methodology depends on the architectural artifacts and their evolution through the life cycle of the system. We illustrate the methodology on a case study using UML models",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677397,no,undetermined,0
A behavior-based process for evaluating availability achievement risk using stochastic activity networks,"With the increased focus on the availability of complex, multifunction systems, modeling processes and analysis tools are needed that help the availability systems engineer understand the impact of architectural and logistics design choices concerning system availability. Because many fielded systems are required to achieve a specified minimal availability over a short measurement period, a modeling methodology must also support computation of the distribution of operational availability for the specified measurement period. This paper describes a two-part behavior-based availability achievement risk methodology that starts with a description of the system's availability related behavior followed by a stochastic activity network-based simulation to obtain numeric estimate of expected availability and the distribution of availability over a selected time frame. The process shows how the system engineer freed to explore complex behavior not possible with combinatorial estimation methods in wide use today",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677344,no,undetermined,0
ReStore: Symptom-Based Soft Error Detection in Microprocessors,"Device scaling and large-scale integration have led to growing concerns about soft errors in microprocessors. To date, in all but the most demanding applications, implementing parity and ECC for caches and other large, regular SRAM structures have been sufficient to stem the growing soft error tide. This will not be the case for long and questions remain as to the best way to detect and recover from soft errors in the remainder of the processor - in particular, the less structured execution core. In this work, we propose the ReStore architecture, which leverages existing performance enhancing checkpointing hardware to recover from soft error events in a low cost fashion. Error detection in the ReStore architecture is novel: symptoms that hint at the presence of soft errors trigger restoration of a previous checkpoint. Example symptoms include exceptions, control flow misspeculations, and cache or translation look-aside buffer misses. Compared to conventional soft error detection via full replication, the ReStore framework incurs little overhead, but sacrifices some amount of error coverage. These attributes make it an ideal means to provide very cost effective error coverage for processor applications that can tolerate a nonzero, but small, soft error failure rate. Our evaluation of an example ReStore implementation exhibits a 2times increase in MTBF (mean time between failures) over a standard pipeline with minimal hardware and performance overheads. The MTBF increases by 20times if ReStore is coupled with protection for certain particularly vulnerable pipeline structures",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1673379,no,undetermined,0
Intrusion-tolerant middleware: the road to automatic security,"The pervasive interconnection of systems throughout the world has given computer services a significant socioeconomic value that both accidental faults and malicious activity can affect. The classical approach to security has mostly consisted of trying to prevent bad things from happening-by developing systems without vulnerabilities, for example, or by detecting attacks and intrusions and deploying ad hoc countermeasures before any part of the system is damaged. Building an intrusion-tolerant system to arrive at some notion of intrusion-tolerant middleware for application support presents multiple challenges. Surprising as it might seem, intrusion tolerance isn't just another instantiation of accidental fault tolerance",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1668003,no,undetermined,0
Sonar Power Amplifier Testing System Based on Virtual Instrument,"This paper presents an intelligent test system for power amplifiers by using virtual instrument technology. The automatic range switching circuit and Hall current transducers are applied in this system, realizing effectively the measurement of wide-voltage signal and large current. LabVIEW, a graphical programming language, and expert system technology are employed to develop testing software, implementing performance test of sonar power amplifier parts, and measurement of technique index and fault diagnosis. The proposed system has been used in the certain type sonar power amplifier system. The test results show that flexibility and data processing capacity of test instruments are improved upon greatly, which can satisfy more needs of measurement. And the proposed system can detect and locate the fault position quickly",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1714084,no,undetermined,0
Testing and Validating the Quality of Specifications,"Model-based testing of state based systems is known to be able to spot non-conformance issues. However, up to half of these issues appear to be errors in the model rather than in the system under test. Errors in the specification at least hamper the prompt delivery of the software, so it is worth while to invest in the quality of the specification. Worse, errors in the specification that are also present in the system under test cannot be detected by model-based testing. In this paper we show how very desirable properties of specifications can be checked by systematic automated testing of the specifications themselves. We show how useful properties of specifications can be found by generalization of incorrect transitions encountered in simulation of the model.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4566990,no,undetermined,0
Archeology of code duplication: recovering duplication chains from small duplication fragments,"Code duplication is a common problem, and a well-known sign of bad design. As a result of that, in the last decade, the issue of detecting code duplication led to various solutions and tools that can automatically find duplicated blocks of code. However, duplicated fragments rarely remain identical after they are copied; they are oftentimes modified here and there. This adaptation usually """"scatters"""" the duplicated code block into a large amount of small """"islands"""" of duplication, which detected and analyzed separately hide the real magnitude and impact of the duplicated block. In this paper we propose a novel, automated approach for recovering duplication blocks, by composing small isolated fragments of duplication into larger and more relevant duplication chains. We validate both the efficiency and the scalability of the approach by applying it on several well known open-source case-studies and discussing some relevant findings. By recovering such duplication chains, the maintenance engineer is provided with additional cases of duplication that can lead to relevant refactorings, and which are usually missed by other detection methods.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1595830,no,undetermined,0
Assessing the Relationship between Software Assertions and Faults: An Empirical Investigation,"The use of assertions in software development is thought to help produce quality software. Unfortunately, there is scant empirical evidence in commercial software systems for this argument to date. This paper presents an empirical case study of two commercial software components at Microsoft Corporation. The developers of these components systematically employed assertions, which allowed us to investigate the relationship between software assertions and code quality. We also compare the efficacy of assertions against that of popular bug finding techniques like source code static analysis tools. We observe from our case study that with an increase in the assertion density in a file there is a statistically significant decrease in fault density. Further, the usage of software assertions in these components found a large percentage of the faults in the bug database",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021986,no,undetermined,0
How Programs Represent Reality (and how they don't),"Programming is modeling the reality. Most of the times, the mapping between source code and the real world concepts are captured implicitly in the names of identifiers. Making these mappings explicit enables us to regard programs from a conceptual perspective and thereby to detect semantic defects such as (logical) redundancies in the implementation of concepts and improper naming of program entities. We present real world examples of these problems found in the Java standard library and establish a formal framework that allows their concise classification. Based on this framework, we present our method for recovering the mappings between the code and the real world concepts expressed as ontologies. These explicit mappings enable semi-automatic identification of the discussed defect classes",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4023979,no,undetermined,0
"The Experimental Paradigm in Reverse Engineering: Role, Challenges, and Limitations","In many areas of software engineering, empirical studies are playing an increasingly important role. This stems from the fact that software technologies are often based on heuristics and are moreover expected to be used in processes where human intervention is paramount. As a result, not only it is important to assess their cost-effectiveness under conditions that are as realistic and representative as possible, but we must also understand the conditions under which they are more suitable and applicable. There exists a wealth of empirical methods aimed at maximizing the validity of results obtained through empirical studies. However, in the case of reverse engineering, as for other domains of investigation, researchers and practitioners are faced with specific constraints and challenges. This is the focus of this keynote address and what the current paper attempts to clarify",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4023971,no,undetermined,0
Improved Pattern Matching to Find DNA Patterns,"The process of finding given patterns in DNA sequences is widely used in modern biological sciences. This paper shows an algorithmic improvement for exact pattern matching introducing a new heuristic. For implementation the author created an application that uses three well-known heuristics to ensure the O(n) worst case time, the O(n log <sub>sigma</sub> (m)/m) average case time and the O(n/m) best case time of searching for an m length pattern in an n length text that use a sigma letter alphabet. This application served as a testbed for the new H4 heuristic. The novelty is in optimization of the direction of text window movement in the preprocessing phase. The idea takes advantages of RAM based searching: usually all the text resides in today's gigabyte memory so the opposite direction of searching window moving requires the same time as the usual. A new function predicts the better moving direction in preprocessing time, based on the unsymmetrical property of pattern. Tests proved that this heuristic may result in fewer jumps and tested characters in the search phase of pattern matching",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022980,no,undetermined,0
Biological Sensor System Design for Gymnasium Indoor Air Protection,"Increasing attention is being directed to the vulnerability of public buildings, and national defense facilities to terrorist attack or the accidental release of biological pathogens. Many biological sensors have been developed for protecting the indoor air quality. However, there is lack of fundamental system-level research on developing sensor networks for indoor air protection. The optimal design of a sensor system is affected by sensor parameters, such as sensitivity, probability of correct detection, false positive rate, and response time. This study applies CONTAM in the sensor system design. Common building biological attack scenarios are simulated for a gymnasium. Genetic algorithm (GA) is then applied to optimize the sensor sensitivity, location, and quantity, thus achieving the best system behavior and reducing the total system cost. Assuming that each attack scenario has the same probability for occurrence, optimal system designs that account for the simulated possible attack scenarios are obtained.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4549239,no,undetermined,0
Building Phase-Type Software Reliability Models,"This paper presents a unified framework for software reliability modeling with non-homogeneous Poisson processes, where each software fault-detection time obeys the phase-type distribution and the initial number of inherent faults is given by a Poisson distributed random variable. However, it is worth noting that the resulting software reliability models, called phase-type software reliability models, generalize the existing models but may involve a number of model parameters in the phase-type software reliability model, so that the usual maximum likelihood estimation based on the Newton's method or quasi-Newton's method does not often function well. In this paper, we develop EM (expectation-maximization) algorithms for the phase-type software reliability models with two types of fault data: fault-detection time data and grouped data with arbitrary time intervals. In numerical examples, we compare the EM algorithms with the quasi-Newton's method and illustrate the effectiveness on our unified model and parameter estimation method",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021995,no,undetermined,0
On the Effect of Fault Removal in Software Testing - Bayesian Reliability Estimation Approach,"In this paper, we propose some reliability estimation methods in software testing. The proposed methods are based on the familiar Bayesian statistics, and can be characterized by using test outcomes in input domain models. It is shown that the resulting approaches are capable of estimating software reliability in the case where the detected software faults are removed. In numerical examples, we compare the proposed methods with the existing method, and investigate the effect of fault removal on the reliability estimation in software testing. We show that the proposed methods can give more accurate estimates of software reliability",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021991,no,undetermined,0
"Tool-Supported Verification of Contingency Software Design in Evolving, Autonomous Systems","Advances in software autonomy can support system robustness to a broader range of operational anomalies, called contingencies, than ever before. Contingency management includes, but goes beyond, traditional fault protection. Increased autonomy to achieve contingency management brings with it the challenge of how to verify that the software can detect and diagnose contingencies when they occur. The approach used in this work to investigate the verification was two-fold: (1) to integrate in a single model the representation of the contingencies and of the data signals and software monitors required to identify those contingencies, and (2) to use tool-supported verification of the diagnostics design to identify gaps in coverage of the contingencies. Results presented here indicate that tool-supported verification of the adequacy and correct behavior of such diagnostic software for contingency management can improve on-going contingency analysis, thereby reducing the risk that change has introduced gaps in the contingency software",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021987,no,undetermined,0
"Adequacy, Accuracy, Scalability, and Uncertainty of Architecture-based Software Reliability: Lessons Learned from Large Empirical Case Studies","Our earlier research work on applying architecture-based software reliability models on a large scale case study allowed us to test how and when they work, to understand their limitations, and to outline the issues that need future research. In this paper we first present an additional case study which confirms our earlier findings. Then, we present uncertainty analysis of architecture-based software reliability for both case studies. The results show that Monte Carlo method scales better than the method of moments. The sensitivity analysis based on Monte Carlo method shows that (1) small number of parameters contribute to the most of the variation in system reliability and (2) given an operational profile, components' reliabilities have more significant impact on system reliability than transition probabilities. Finally, we summarize the lessons learned from conducting large scale empirical case studies for the purpose of architecture-based reliability assessment and uncertainty analysis",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021985,no,undetermined,0
Goal-Centric Traceability: Using Virtual Plumblines to Maintain Critical Systemic Qualities,"Successful software development involves the elicitation, implementation, and management of critical systemic requirements related to qualities such as security, usability, and performance. Unfortunately, even when such qualities are carefully incorporated into the initial design and implemented code, there are no guarantees that they will be consistently maintained throughout the lifetime of the software system. Even though it is well known that system qualities tend to erode as functional and environmental changes are introduced, existing regression testing techniques are primarily designed to test the impact of change upon system functionality rather than to evaluate how it might affect more global qualities. The concept of using goal-centric traceability to establish relationships between a set of strategically placed assessment models and system goals is introduced. This paper describes the process, algorithms, and techniques for utilizing goal models to establish executable traces between goals and assessment models, detect change impact points through the use of automated traceability techniques, propagate impact events, and assess the impact of change upon systemic qualities. The approach is illustrated through two case studies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4553719,no,undetermined,0
Full automated packaging of high-power diode laser bars,"Full automated packaging of high power diode laser bars on passive or micro channel heat sinks requires a high precision measurement and handling technology. The metallurgic structure of the solder and intrinsic stress of the laser bar are largely influenced by the conditions of the mounting process. To avoid thermal deterioration the tolerance for the overhang between laser bar and heat sink is about a few microns maximum. Due to an increase of growing applications and growing number of systems there is a need for automatic manufacturing not just for cost efficiency but also for yield and product quality reasons. In this paper we describe the demands on fully automated packaging, the realized design and finally the test results of bonded devices. The design of the automated bonding systems includes an air cushioned, 8 axes system on a granite frame. Each laser bar is picked up by a vacuum tool from a special tray or directly out of the gel pak. The reflow oven contains a ceramic heater with low thermal capacity and reaches a maximum of 400degC with a heating rate up to 100 K/s and a cooling rate up to 20 K/s. It is suitable for all common types of heat sinks and submounts which are fixed onto the heater by vacuum. The soldering process is performed under atmospheric pressure, during the oven is filled up with inert gas. Additionally, reactive gases can be used to proceed the reduction of the solder. Three high precision optical sensors for distance measurement detect the relative position of laser bar and heat sink. The high precision alignment uses a special algorithm for final positioning. For the alignment of the tilt and roll angles between the laser bar and the heat sink two optical distance sensors and the two goniometers below the oven are used. To detect the angular orientation of the heat sinks upper surface a downwards looking optical sensor system is used. The upwards pointing optical sensor mounted is used to measure the orientation of the laser bars lo- wer side. These measurements provide the data needed to calculate the angles that the heat sink needs to be tilted and rolled by the two goniometers, in order to get its upper surface parallel to the lower surface of the laser bar. For the measurement of the laser bar overhang and yaw an optical distance sensor is mounted in front of the oven. Overhang and yaw are aligned by using high precision rotary and translation stages. A software tool calculates the displacement necessary to get a parallel orientation and a desired overhang of the laser bar relative to the heat sink. A post bonding accuracy of +/- 1 micron and of +/- 0,2 mrad respectively is achieved. To demonstrate the performance and reliability of the bonding system the bonded devices were characterized by tests like smile test, shear test, burn in test. The results will be presented as well as additional aspects of automated manufacturing like part identification and part tracking.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4550096,no,undetermined,0
"Studying the Characteristics of a """"Good"""" GUI Test Suite","The widespread deployment of graphical-user interfaces (GUIs) has increased the overall complexity of testing. A GUI test designer needs to perform the daunting task of adequately testing the GUI, which typically has very large input interaction spaces, while considering tradeoffs between GUI test suite characteristics such as the number of test cases (each modeled as a sequence of events), their lengths, and the event composition of each test case. There are no published empirical studies on GUI testing that a GUI test designer may reference to make decisions about these characteristics. Consequently, in practice, very few GUI testers know how to design their test suites. This paper takes the first step towards assisting in GUI test design by presenting an empirical study that evaluates the effect of these characteristics on testing cost and fault detection effectiveness. The results show that two factors significantly effect the fault-detection effectiveness of a test suite: (1) the diversity of states in which an event executes and (2) the event coverage of the suite. Test designers need to improve the diversity of states in which each event executes by developing a large number of short test cases to detect the majority of """"shallow"""" faults, which are artifacts of modern GUI design. Additional resources should be used to develop a small number of long test cases to detect a small number of """"deep"""" faults",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021981,no,undetermined,0
Metamodel-based Test Generation for Model Transformations: an Algorithm and a Tool,"In a model-driven development context (MDE), model transformations allow memorizing and reusing design know-how, and thus automate parts of the design and refinement steps of a software development process. A model transformation program is a specific program, in the sense it manipulates models as main parameters. Each model must be an instance of a """"metamodel"""", a metamodel being the specification of a set of models. Programming a model transformation is a difficult and error-prone task, since the manipulated data are clearly complex. In this paper, we focus on generating input test data (called test models) for model transformations. We present an algorithm to automatically build test models from a metamodel",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021974,no,undetermined,0
Clustering Analysis for the Management of Self-Monitoring Device Networks,"The increasing computing and communication capabilities of multi-function devices (MFDs) have enabled networks of such devices to provide value-added services. This has placed stringent QoS requirements on the operations of these device networks. This paper investigates how the computational capabilities of the devices in the network can be harnessed to achieve self-monitoring and QoS management. Specifically, the paper investigates the application of clustering analysis for detecting anomalies and trends in events generated during device operation, and presents a novel decentralized cluster and anomaly detection algorithm. The paper also describes how the algorithm can be implemented within a device overlay network, and demonstrates its performance and utility using simulated as well as real workloads.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4550827,no,undetermined,0
Disaster Hardening for Software Systems,"Summary form only given. We can treat the software system development as a 'disaster-prone' system. We consider a crash as an example of a disaster. We consider the minimum infrastructural requirements based on the application, and the operational and user environments. We review the strategies of disaster awareness, anticipation, proactive pre-emption, and precaution to prevent and/or mitigate the effects of the major or minor catastrophes. We survey methods used in software quality improvements and show their applicability to activities in disaster mitigation and control. We illustrate these with examples using the CMM, sigma six and Taguchi-based ideas which show that mutual exchange of concepts enrich the important fields of software development and disaster control and mitigation. We conclude by identifying the important overlap between the two areas of software development and disaster mitigation. The software change management and maintenance methods can greatly benefit by ideas from disaster technology. The software developed under damage and disaster control techniques can be robust, resilient and long lasting",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021962,no,undetermined,0
Video Stream Annotations for Energy Trade-offs in Multimedia Applications,"Recent applications for distributed mobile devices, including multimedia video/audio streaming, typically process streams of incoming data in a regular, predictable way. The behavior of these applications during runtime can be accurately predicted most of the time by analyzing the data to be processed and annotating the stream with the information collected. We introduce an annotation-based approach to power-quality trade-offs and demonstrate its application on CPU frequency scaling during video decoding, for an improved user experience on portable devices. Our experiments show that up to 50% of the power consumed by the CPU during video decoding can be saved with this approach",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021905,no,undetermined,0
Automating ITSM Incident Management Process,"Service desks are used by customers to report IT issues in enterprise systems. Most of these service requests are resolved by level-1 persons (service desk attendants) by providing information/quick-fix solutions to customers. For each service request, level- 1 personnel identify important keywords and see if the incoming request is similar to any historic incident. Otherwise, an incident ticket is created and, with other related information, forwarded to incident's subject matter expert (SME). Incident management process is used for managing the life cycle of all incidents. An organization spends lots of resources to keep its IT resources incident free and, therefore, timely resolution of incoming incident is required to attain that objective. Currently, the incident management process is largely manual, error prone and time consuming. In this paper, we use information integration techniques and machine learning to automate various processes in the incident management workflow. We give a method for correlating the incoming incident with configuration items (CIs) stored in Configuration management database (CMDB). Such a correlation can be used for correctly routing the incident to SMEs, incident investigation and root cause analysis. In our technique, we discover relevant CIs by exploiting the structured and unstructured information available in the incident ticket. We present efficient algorithm which gives more than 70% improvement in accuracy of identifying the failing component by efficiently browsing relationships among CIs.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4550835,no,undetermined,0
Refactoring Detection based on UMLDiff Change-Facts Queries,"Refactoring is an important activity in the evolutionary development of object-oriented software systems. Several IDEs today support the automated application of some refactorings; at the same time, there is substantial on-going research aimed at developing support for deciding when and how software should be refactored and for estimating the effect of the refactoring on the quality requirements of the software. On the other hand, understanding the refactorings in the evolutionary history of a software system is essential in understanding its design rationale. Yet, only very limited support exists for detecting refactorings. In this paper, we present our approach for detecting refactorings by analyzing the system evolution at the design level. We evaluate our method with case studies, examining two realistic examples of object-oriented software",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4023996,no,undetermined,0
Traffic and Quality Characterization of Single-Layer Video Streams Encoded with the H.264/MPEG-4 Advanced Video Coding Standard and Scalable Video Coding Extension,"The recently developed H.264/AVC video codec with scalable video coding (SVC) extension, compresses non-scalable (single-layer) and scalable video significantly more efficiently than MPEG-4 Part 2. Since the traffic characteristics of encoded video have a significant impact on its network transport, we examine the bit rate-distortion and bit rate variability-distortion performance of single-layer video traffic of the H.264/AVC codec and SVC extension using long CIF resolution videos. We also compare the traffic characteristics of the hierarchical B frames (SVC) versus classical B frames. In addition, we examine the impact of frame size smoothing on the video traffic to mitigate the effect of bit rate variabilities. We find that compared to MPEG-4 Part 2, the H.264/AVC codec and SVC extension achieve lower average bit rates at the expense of significantly increased traffic variabilities that remain at a high level even with smoothing. Through simulations we investigate the implications of this increase in rate variability on <i>(i)</i> frame losses when transmitting a single video, and <i>(ii)</i> on a bufferless statistical multiplexing scenario with restricted link capacity and information loss. We find increased frame losses, and rate-distortion/rate-variability/encoding complexity tradeoffs. We conclude that solely assessing bit rate-distortion improvements of video encoder technologies is not sufficient to predict the performance in specific networked application scenarios.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4547482,no,undetermined,0
An Approach for Evaluating Trust in IT Infrastructure,Trustworthiness of an IT infrastructure can be justified using the concept of trust case which denotes a complete and explicit structure encompassing all the evidence and argumentation supporting trust within a given context. A trust case is developed by making an explicit set of claims about the system of interest and showing how the claims are interrelated and supported by evidence. The approach uses Dempster-Shafer belief function framework to quantify the trust case. We demonstrate how recommendations issued by different stakeholders enable stakeholder-specific views of the trust case and reasoning about the level of trust in a given IT infrastructure,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4024037,no,undetermined,0
Introduction to the Dependability Modeling of Computer Systems,"Computer systems and networks are considered as a union of all resources, i.e. hardware, software and people (users, administrators and managers), essential for the realization of predicted tasks. The system dependability is defined as a generalization of performability and reliability, combining the notions of both these terms. The functional-reliability models are based on the observation that only a subset of the system resources is involved in a task execution and only inefficiencies of these resources may influence the correctness of task realization. The systems are working in a real environment, which is often hostile: it may be a source of threats, such as security intrusions, faulty or modified software and human errors. The set of analyzed events is a sum of hardware failures and malfunctions, software faults, human errors, intrusions (intended and addressed threats), and viruses (unaddressed threats that are broadcast in the system and in its environment)",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4024038,no,undetermined,0
Automated Caching of Behavioral Patterns for Efficient Run-Time Monitoring,"Run-time monitoring is a powerful approach for dynamically detecting faults or malicious activity of software systems. However, there are often two obstacles to the implementation of this approach in practice: (1) that developing correct and/or faulty behavioral patterns can be a difficult, labor-intensive process, and (2) that use of such pattern-monitoring must provide rapid turn-around or response time. We present a novel data structure, called extended action graph, and associated algorithms to overcome these drawbacks. At its core, our technique relies on effectively identifying and caching specifications from (correct/faulty) patterns learned via machine-learning algorithm. We describe the design and implementation of our technique and show its practical applicability in the domain of security monitoring of sendmail software",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030900,no,undetermined,0
Design for Testability of Software-Based Self-Test for Processors,"In this paper, the authors propose a design for testability method for test programs of software-based self-test using test program templates. Software-based self-test using templates has a problem of error masking where some faults detected in a test generation for a module are not detected by the test program synthesized from the test. The proposed method achieves 100% template level fault efficiency in a sense that the proposed method completely resolves the problem of error masking. Moreover, the proposed method adds only observation points to the original design, and it enables at-speed testing and does not induce delay overhead",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030794,no,undetermined,0
An Efficient Test Pattern Selection Method for Improving Defect Coverage with Reduced Test Data Volume and Test Application Time,"Testing using n-detection test sets, in which a fault is detected by n (n > 1) input patterns, is being increasingly advocated to increase defect coverage. However, the data volume for an n-detection test set is often too large, resulting in high testing time and tester memory requirements. Test set selection is necessary to ensure that the most effective patterns are chosen from large test sets in a high-volume production testing environment. Test selection is also useful in a time-constrained wafer-sort environment. The authors use a probabilistic fault model and the theory of output deviations for test set selection - the metric of output deviation is used to rank candidate test patterns without resorting to fault grading. To demonstrate the quality of the selected patterns, experimental results were presented for resistive bridging faults and non-feedback zero-resistance bridging faults in the ISCAS benchmark circuits. Our results show that for the same test length, patterns selected on the basis of output deviations are more effective than patterns selected using several other methods",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030788,no,undetermined,0
Byzantine Fault Tolerance in MDS of Grid System,"Fault tolerance is a challenge problem in reliable distributed system. In grid, detecting and correcting fault techniques is used in fault tolerance of MDS system. These techniques are limited in dealing with the benign faults on servers and the Internet. But they will not work when malicious faults on servers or software errors occur. In this paper, a secure aware MDS system, which can tolerate malicious faults occurred on servers, is proposed. By using a new Byzantine-fault-tolerant algorithm, the proposed MDS system guarantees safety and liveness properties under the condition that no more than f replicas are faulty if it consists of 3f+1 tightly coupled servers, and it maintains the seamless interfaces to application programs as the usual formal MDS system does",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4028534,no,undetermined,0
Modeling Request Routing in Web Applications,"For Web applications, determining how requests from a Web page are routed through server components can be time-consuming and error-prone due to the complex set of rules and mechanisms used in a platform such as J2EE. We define request routing to be the possible sequences of server-side components that handle requests. Many maintenance tasks require the developer to understand the request routing, so this complexity increases maintenance costs. However, viewing this problem at the architecture level provides some insight. The request routing in these Web applications is an example of a pipeline architectural pattern: each request is processed by a sequence of components that form a pipeline. Communication between pipeline stages is event-based, which increases flexibility but obscures the pipeline structure because communication is indirect. Our approach for improving the maintainability of J2EE Web applications is to provide a model that exposes this architectural information. We use Z to formally specify request routing models and analysis operations that can be performed on them, then provide tools to extract request routing information from an application's source code, create the request routing model, and analyze it automatically. We have applied this approach to a number of existing applications up to 34K LOC, showing improvement via typical maintenance scenarios. Since this particular combination of patterns is not unique to Web applications, a model such as our request routing model could provide similar benefits for these systems",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027212,no,undetermined,0
A Survey of Automated Techniques for Formal Software Verification,"The quality and the correctness of software are often the greatest concern in electronic systems. Formal verification tools can provide a guarantee that a design is free of specific flaws. This paper surveys algorithms that perform automatic static analysis of software to detect programming errors or prove their absence. The three techniques considered are static analysis with abstract domains, model checking, and bounded model checking. A short tutorial on these techniques is provided, highlighting their differences when applied to practical problems. This paper also surveys tools implementing these techniques and describes their merits and shortcomings.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544862,no,undetermined,0
Notice of Violation of IEEE Publication Principles<BR>Dynamic Binding Framework for Adaptive Web Services,"Notice of Violation of IEEE Publication Principles<BR><BR>""""Dynamic Binding Framework for Adaptive Web Services,""""<BR>by A. Erradi, P. Maheshwari<BR>in the Proceedings of the Third International Conference on Internet and Web Applications and Services, 2008. ICIW '08, June 2008, pp. 162-167<BR><BR>After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>This paper contains significant portions of original text from the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.<BR><BR>""""Foundations of Software Engineering,""""<BR>by A. Michlmayr, F. Rosenberg, C. Platzer, M. Treiber, S. Dustdar<BR>in the Proceedings of the 2nd International Workshop on Service Oriented Software Engineering: In Conjunction with the 6th ESEC/FSE Joint Meeting, Sept 2007, pp. 22-28<BR><BR>Dynamic selection and composition of autonomous and loosely-coupled Web services is increasingly used to automate business processes. The typical long-running characteristic of business processes imposes new management challenges such as dynamic adaptation of running process instances. To address this, we developed a policy-based framework, named manageable and adaptable service compositions (MASC) , to declaratively specify policies that govern: (1) discovery and selection of services to be used, (2) monitoring to detect the need for adaptation, (3) reconfiguration and adaptation of the process to handle special cases (e.g., context-dependant behavior) and recover from typical faults in service-based processes. The identified constructs are executed by a lightweight service-oriented management middleware named MASC middleware. We implemented a MASC proof-of-concept prototype and evaluated it on stock trading case study scenarios. We conducted exten- sive studies to demonstrate the feasibility of the proposed techniques and illustrate the benefits of our approach in providing adaptive composite services using the policy-based approach. Our performance and scalability studies indicate that MASC middleware is scalable and the introduced overhead are acceptable.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4545608,no,undetermined,0
Validating Requirements Engineering Process Improvements - A Case Study,"The quality of the Requirements Engineering (RE) process plays a critical role in successfully developing software systems. Often, in software organizations, RE processes are assessed and improvements are applied to overcome their deficiency. However, such improvements may not yield desired results for two reasons. First, the assessed deficiency may be inaccurate because of ambiguities in measurement. Second, the improvements are not validated to ascertain their correctness to overcome the process deficiency. Therefore, a Requirements Engineering Process Improvement (REPI) exercise may fail to establish its purpose. A major shortfall in validating RE processes is the difficulty in representing process parameters in some cognitive form. We address this issue with an REPI framework that has both measurement and visual validation properties. The REPI validation method presented is empirically tested based on a case study in a large software organization. The results are promising towards considering this REPI validation method in practice by organizations.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026790,no,undetermined,0
Fault Tolerant PID Control based on Software Redundancy for Nonlinear Uncertain Processes,"The fault diagnosis and close-loop tolerant PID control for nonlinear multi-variables system under multiple sensor failures are investigated in the paper. A complete FDT architecture based on software redundancy is proposed to efficiently handle the fault diagnosis and the accommodation for multiple sensor failures in online situations. The methods colligates the adaptive threshold technique with the envelope and weighting moving average residual to detect multi-type sensor fault, use fault propagation technique, variable structure analyzing technique and neural network techniques to online reconstruct sensor signal, and achieves the tolerant PID control through recombining feedback loop of PID controller. The three-tank with multiple sensor fault concurrence is simulated, the simulating result shows that the fault detection and tolerant control strategy has stronger robustness and tolerant fault ability",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026441,no,undetermined,0
Research on Test-platform and Condition Monitoring Method for AUV,"To improve the reliability and intelligence of autonomous underwater vehicle, an AUV test-platform named """"Beaver"""" is developed. The hardware and software system structure are introduced in detail. By analyzing the performance and the fault mechanism of thruster, it establishes the condition monitoring system for thrusters and sensors based on the double closed-loop PID controller, which includes the performance model of thruster based on RBF neural network and forward model of AUV based on improved dynamic recursive Elman neural network, and it probes into the method of combine fault detection. The results of experiment indicate that the """"Beaver"""" can achieve the basic motion control and meet the requirement in test, and the combine fault detection method by parallel connected performance model and forward model can detect the typical fault of thrusters and sensors, which certificates the reliability and the effectiveness of condition monitoring system",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026343,no,undetermined,0
"A Prognostic and Warning System for Power Electronic Modules in Electric, Hybrid, and Fuel Cell Vehicles","Reliability of power electronics modules is of paramount importance for the commercial success of various types of electric vehicles. In this paper, we study the technical feasibility of detecting early symptoms and warning signs of power module degradation due to thermomechanical stress and fatigue, and developing a prognostic system that monitors the state of health of the power modules in electric, hybrid, and fuel cell vehicles. A signature degradation trace of the on-voltage of IGBT modules was observed from accelerated power cycling test. This on-voltage """"anomaly"""" can be attributed to sequential events of solder joint degradation followed by wirebond lift-off mechanisms. A quasi real-time IGBT failure prognostic algorithm based on monitoring the abnormal VCEsat variation at specific currents and temperatures is developed. The algorithm was verified using extensive SIMULINK modeling. The prognostic system can be implemented cost-effectively in existing vehicle hardware/software architectures",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4025426,no,undetermined,0
Performance Analysis and Enhancement for Priority Based IEEE 802.11 Network,"In this paper, a novel non-saturation analytical model for priority based IEEE 802.11 network is introduced. Unlike previous work that is focused on MAC backoff for saturation stations, this model uses Markov and M/ M/1/K theories to predict MAC and queuing service time and loss. Then a performance prediction based enhancement scheme is proposed. By dynamic tuning of protocol options, this proposed scheme limits end-to-end delay and loss rate of real-time traffic and maximizes throughput. Consequently, call admission control is taken to protect existing traffics when the channel is saturated. Simulations validate this model and the comparison with IEEE802.11e EDCA shows that our mechanism can guarantee quality of service more efficiently.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4024792,no,undetermined,0
Modeling and Performance Analysis of Beyond 3G Integrated Wireless Networks,"Next-generation wireless networking is evolving towards a multi-service heterogeneous paradigm that converges different pervasive access technologies and provides a large set of novel revenue generating applications. Hence, system complexity increases due to its embedded heterogeneity, which can not be accounted by the existing modeling and performance evaluation techniques. Consequently, the development of new modeling approaches becomes as a crucial requirement for proper system design and performance evaluation. This paper presents a novel mobility model for a two-tier integrated wireless system using a new modeling approach that accommodates the aforementioned complexity. Additionally, a novel session model is developed as an adapted version of the proposed mobility model. These models use phase-type distributions that are known to approximate any generic probability laws. Using the proposed session model, a novel generic analytical framework is developed to obtain several salient performance metrics such as network utilization times and handoff rates. Simulation and analysis results prove the proposed model validity and demonstrate the accuracy of the novel modeling approach when compared with traditional modeling techniques.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4024417,no,undetermined,0
Measurement Techniques in On-Demand Overlays for Reliable Enterprise IP Telephony,"Maintaining good quality of service for real-time applications like IP Telephony requires quick detection and reaction to network impairments. In this paper, we propose and study novel measurement techniques in ORBIT, which is a simple, easily deployable architecture that uses single-hop overlays implemented with intelligent endpoints and independent relays. The measurement techniques provide rapid detection and recovery of IP Telephony during periods of network trouble. We study our techniques via detailed simulations of several multi-site enterprise topologies of varying sizes and three typical fault models. We show that our proposed techniques can detect network impairments rapidly and rescue IP Telephony calls in sub-second intervals. We observed that all impacted calls were rescued with only a few relays in the network and the run-time overhead was low. Furthermore, the relay sites needed to be provisioned with minimal additional bandwidth to support the redirected calls.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4024198,no,undetermined,0
Generic CSSA-Based Pattern over Boolean Data for an Improved WS-BPEL to Petri Net Mappping,"Formal methods, like Petri nets, provide a means to analyse BPEL processes, detecting weaknesses and errors in the process model already at design-time. However, in most approaches proposed so far, the analysis is restricted to the control flow only. Analysing quality properties of BPEL processes might therefore yield false-negative results. In this paper, we are presenting an enhanced BPEL to Petri net mapping, that incorporates relevant data aspects by doing a CSSA- based analysis and applying novel Petri net patterns. The resulting formal model allows for a more precise analysis of critical properties, such as controllability and behavioural compatibility.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4545677,no,undetermined,0
A Fast and Reliable Segmentation Method Based on Active Contour Model and Edgeflow,"In this paper, we proposed a new method to segment a given image, based on curve evolution and edgeflow techniques. The approach automatically detect boundaries, and change of topology in terms of the edgeflow fields. We present the numerical implementation and the experimental results based on the semi-implicit method. Experimental results show that one can obtains a high quality edge contour",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021681,no,undetermined,0
Study on Fuzzy Theory Based Web Access Control Model,"Along with the constant development of online services, the application of traditional RBAC model for the user-role assignment and maintaining the user-role assignment become an arduous and error-prone task. In order to solve these problems, this paper proposes a trust based user-role assignment model for assign role to users. It is based on the userspsila trustworthiness in the system, which is a fuzzy concept, the application of fuzzy theory to calculate the trust degree of users and the trust degree that the role needed, provides a new method for the user-role assignment.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554080,no,undetermined,0
Formally comparing user and implementer model-based testing methods,"There are several methods to assess the capability of a test suite to detect faults in a potentially wrong system. We explore two methods based on considering some probabilistic information. In the first one, we assume that we are provided with a probabilistic user model. This is a model denoting the probability that the entity interacting with the system takes each available choice. In the second one, we suppose that we have a probabilistic implementer model, that is, a model denoting the probability that the implementer makes each possible fault while constructing the system. We show that both testing scenarios are strongly related. In particular, we prove that any user can be translated into an implementer model in such a way that the optimality of tests is preserved, that is, a test suite is optimal for the user if and only if it is optimal for the resulting implementer. Another translation, working in the opposite direction, fulfills the reciprocal property. Thus, we conclude that any test selection criterium designed for one of these testing problems can be used for the other one, once the model has been properly translated.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4566986,no,undetermined,0
Toward Component Non-functional Interoperability Analysis: A UML-based and Goal-Oriented Approach,"Component-based development (CBD) has a great potential of reducing development cost and time by integrating existing software components. But it also faces many challenges one of which is ensuring interoperability of the components that may have been developed with different functional and non-functional goals. The software community has traditionally focused more on the functional aspect of the interoperability such as syntactic and semantic compatibility. However, incompatibility from the non-functional aspect could lead to poor quality such as insufficient security or even inoperable system. This paper presents a preliminary framework for analyzing non-functional requirements (NFRs) defined for the component required and provided interfaces. The components are considered non-functionally interoperable when they agree on the definition and implementation techniques used to achieve the NFRs. Any detected mismatches can be resolved using a combination of the three presented tactics, including replacing the server component, negotiating for more attainable NFRs, or using an adapter component to bridge the non-functional differences. A running example based on a simplified Web-based conference management system is used to illustrate the application of this framework",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4018516,no,undetermined,0
Analyzing and Extending MUMCUT for Fault-based Testing of General Boolean Expressions,"Boolean expressions are widely used to model decisions or conditions of a specification or source program. The MUMCUT, which is designed to detect seven common faults where Boolean expressions under test are assumed to be in Irredundant Disjunctive Normal Form (IDNF), is an efficient fault-based test case selection strategy in terms of the fault-detection capacity and the size of selected test suite. Following up our previous work that reported the fault-detection capacity of the MUMCUT when it is applied to general form Boolean expressions, in this paper we present the characteristic of the types of single faults committed in general Boolean expressions that a MUMCUT test suite fails to detect, analyze the certainty why a MUMCUT test suite fails to detect these types of undetected faults, and provide some extensions to enhance the detection capacity of the MUMCUT for these types of undetected faults.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019965,no,undetermined,0
Learning Bayesian Networks for Systems Diagnosis,"This paper proposes the construction of a Bayesian network for failure diagnosis in industrial systems. We built this network considering the plant mathematical model and it includes parameters and structure learning through the Beta Dirichlet distributions. We experience the previous methodology by means of a case study, where we simulate some failures that can occurs in the valves used to interconnect a deposits system. With those failures information, we train the network and this way we learn the structure and parameters of the Bayesian network. Once obtained the network, we design the diagnosis probabilistic inference through the poly-trees algorithm. It will give us the valves failure probabilities according to the evidences that show up in our entrance sensors. In this work, we try the existent uncertainty in the diagnosis variables through the probabilistic and fuzzy approach. Since the information provided by our sensors (diagnosis variables) is represented in a fuzzy logic form, for then to be converted to probability intervals, generalizing the Dempster-Shafer theory to fuzzy sets. After that, we spread this information in interval form throughout the diagnosis Bayesian network to get our diagnosis results. The probability interval is more advisable in the taking decisions that a singular value",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019782,no,undetermined,0
Multilevel Modelling Software Development,"Different from other engineering areas, the level of reuse in software engineering is very low. Also, developing large-scale applications which involve thousands of software elements such as classes and thousands of interactions among them is a complex and error-prone task. Industry currently lacks modelling practices and modelling tool support to tackle these issues. Model driven development (MDD) has emerged as an approach to diminishing software development complexity. We claim that models alone are not enough to tackle low reuse and complexity. Our contribution is a multilevel modelling development (MMD) framework whereby models are defined at different abstraction levels. A modelling level is constructed out by assembling software elements defined at the adjacent lower-level. MMD effectively diminish development complexity and facilitates large-scale reuse",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019748,no,undetermined,0
Control theory-based DVS for interactive 3D games,"We propose a control theory-based dynamic voltage scaling (DVS) algorithm for interactive 3D game applications running on battery- powered portable devices. Using this scheme, we periodically adjust the game workload prediction based on the feedback from recent prediction errors. Although such control-theoretic feedback mechanisms have been widely applied to predict the workload of video decoding applications, they heavily rely on estimating the queue lengths of video frame buffers. Given the interactive nature of games - where game frames cannot be buffered - the control - theoretic DVS schemes for video applications can no longer be applied. Our main contribution is to suitably adapt these schemes for interactive games. Compared to history-based workload prediction schemes - where the workload of a game frame is predicted by averaging the workload of the previously-rendered frames - our proposed scheme yields significant improvement on different platforms (e.g. a laptop and a PDA) both in terms of energy savings as well as output quality.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4555917,no,undetermined,0
Programming Language Inherent Support for Constrained XML Schema Definition Data Types and OWL DL,"Recently, the Web Ontology Language (OWL) and XML schema definition (XSD) have become ever more important when it comes to conceptualize knowledge and to define programming language independent type systems. However, writing software that operates on ontological data and on XML instance documents still suffers from a lack of compile time support for OWL and XSD. Especially, obeying lexical- and value space constraints that may be imposed on XSD simple data types and preserving the consistency of assertional ontological knowledge is still error prone and laborious. Validating XML instance documents and checking the consistency of ontological knowledge bases according to given XML schema definitions and ontological terminologies, respectively, requires significant amounts of code. This paper presents novel compile time- and code generation features, which were implemented as an extension of the C# programming language. Zhi# provides compile time-and runtime support for constrained XML schema definition simple data types and it guarantees terminological validity for modifications of assertional ontological data",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019587,no,undetermined,0
Checklist Inspections and Modifications: Applying Bloom's Taxonomy to Categorise Developer Comprehension,"Software maintenance can consume up to 70% of the effort spent on a software project, with more than half of this devoted to understanding the system. Performing a software inspection is expected to contribute to comprehension of the software. The question is: at what cognition levels do novice developers operate during a checklist-based code inspection followed by a code modification? This paper reports on a pilot study of Bloom's taxonomy levels observed during a checklist-based inspection and while adding new functionality unrelated to the defects detected. Bloom's taxonomy was used to categorise think-aloud data recorded while performing these activities. Results show the checklist-based reading technique facilitates inspectors to function at the highest cognitive level within the taxonomy and indicates that using inspections with novice developers to improve cognition and understanding may assist integrating developers into existing project teams.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556135,no,undetermined,0
Proactive maintenance with variant workload under Distributed Multimedia Application Systems,"Distributed multimedia applications such as video on demand (VoD), require dynamic quality of service (QoS) guarantee from service servers for their continuous multimedia streams. In this paper, we build the service availability Markovian model for unified failure-recovery mechanism under variant load scenarios, by calculating local and global kernels of the Markovain model, we get the steady-state availability and unavailability probabilities. Numerical results show that there exist differences between the system availability and the request perceived availability under variant load state (the nature of this kind of system's scenario). This provides strategies for improving VoD system's availability",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019129,no,undetermined,0
Estimating the Heavy-Tail Index for WWW Traces,"Heavy-tailed behavior of WWW traffic has serious implications for the design and performance analysis of computer networks. This behavior gives rise to rare events which could be catastrophic for the QoS of an application. Thus, an accurate detection and quantification of the degree of thickness of a distribution is required. In this paper we detect and quantify the degree of tail-thickness for the file size and transfer times distributions of several WWW traffic traces. For accomplishing the above, the behavior of four estimators in real WWW traces characteristics is studied. We show that Hill-class estimators present varying degrees of accuracy and should be used as a first step towards the estimation of the tail-index. The QQ estimator, on the other hand, is shown to be more robust and adaptable, thus giving rise to more confident point estimates",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4018018,no,undetermined,0
Quality of service investigation for multimedia transmission over UWB networks,"In this paper, the Quality of Service (QoS) for multimedia traffic of the Medium Access Control (MAC) protocol for Ultra Wide-Band (UWB) networks is investigated. A protocol is proposed to enhance the network performance and increase its capacity. This enhancement comes from using Wise Algorithm for Link Admission Control (WALAC). The QoS of multimedia transmission is determined in terms of average delay, loss probability, utilization, and the network capacity.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554404,no,undetermined,0
Weighted Proportional Sampling : AGeneralization for Sampling Strategies in Software Testing,"Current activities to measure the quality of software products rely on software testing. The size and complexity of software systems make it almost impossible to perform complete coverage testing. During the past several years, many techniques to improve the test effectiveness (i.e., the ability to find faults) have been proposed to address this issue. Two examples of such strategies are random testing and partition testing. Both strategies follow an input domain sampling to perform the testing process. The procedure and assumptions for selecting these points seem to be different for both strategies: random testing considers only the probability of each sub-domain (i.e. uniform sampling) while partition testing considers only the sampling rate of each sub-domain (i.e., proportional sampling). This paper describes a more general sampling strategy, named weighted proportional sampling strategy. This strategy unifies both strategies into a general model that encompasses both of them as special cases. This paper also proposes an optimization model to determine the number of sampled points depending on the sampling strategy",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4017950,no,undetermined,0
Modeling the Reliability of Existing Software using Static Analysis,"Software unreliability represents an increasing risk to overall system reliability. As systems become larger and more complex, mission critical and safety critical systems have had increasing functionality controlled exclusively through software. This change, coupled with generally increasing reliability in hardware modules, has resulted in a shift of the root cause of systems failure from hardware to software. Market forces, including decreased time to market, reduced development team sizes, and other factors, have encouraged projects to reuse existing software as well as to purchase COTS software solutions. This has made the usage of the more than 200 existing software reliability models increasingly difficult. Traditional software reliability models require significant testing data to be collected during software development in order to estimate software reliability. If this data is not collected in a disciplined manner or is not made available to software engineers, these modeling techniques can not be applied. It is imperative that practical reliability modeling techniques be developed to address these issues. It is on this premise that an appropriate software reliability model combining static analysis of existing source code modules, limited testing with path capture, and Bayesian belief networks is presented. Static analysis is used to detect faults within the source code which may lead to failure. Code coverage is used to determine which paths within the source code are executed as well as how often they execute. Finally, Bayesian belief network is then used to combine these parameters and estimate the resulting software reliability",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4017728,no,undetermined,0
Novelty Detection Based Machine Health Prognostics,In this paper we present a new novelty detection algorithm for continuous real time monitoring of machine health and prediction of potential machine faults. The kernel of the system is a generic evolving model that is not dependent on the specific measured parameters determining the health of a particular machine. Two alternative strategies are introduced in order to predict abrupt and gradually developing (incipient) changes. This algorithm is realized as an autonomous software agent that continuously updates its decision model implementing an unsupervisory recursive learning algorithm. Results of validation of the proposed algorithm by accelerated testing experiments are also discussed,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4016725,no,undetermined,0
Evaluating the Reference and Representation of Domain Concepts in APIs,"As libraries are the most widespread form of software reuse, the usability of their APIs substantially influences the productivity of programmers in all software development phases. In this paper we develop a framework to characterize domain-specific APIs along two directions: 1) how can the API users reference the domain concepts implemented by the API; 2) how are the domain concepts internally represented in the API. We define metrics that allow the API developer for example to assess the conceptual complexity of his API and the non-uniformity and ambiguities introduced by the API's internal representations of domain concepts, which makes developing and maintaining software that uses the library difficult and error-prone. The aim is to be able to predict these difficulties already during the development of the API, and based on this feedback be able to develop better APIs up front, which will reduce the risks of these difficulties later.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556138,no,undetermined,0
Atom-Aid: Detecting and Surviving Atomicity Violations,"Writing shared-memory parallel programs is error-prone. Among the concurrency errors that programmers often face are atomicity violations, which are especially challenging. They happen when programmers make incorrect assumptions about atomicity and fail to enclose memory accesses that should occur atomically inside the same critical section. If these accesses happen to be interleaved with conflicting accesses from different threads, the program might behave incorrectly. Recent architectural proposals arbitrarily group consecutive dynamic memory operations into atomic blocks to enforce memory ordering at a coarse grain. This provides what we call implicit atomicity, as the atomic blocks are not derived from explicit program annotations. In this paper, we make the fundamental observation that implicit atomicity probabilistically hides atomicity violations by reducing the number of interleaving opportunities between memory operations. We then propose Atom-Aid, which creates implicit atomic blocks intelligently instead of arbitrarily, dramatically reducing the probability that atomicity violations will manifest themselves. Atom-Aid is also able to report where atomicity violations might exist in the code, providing resilience and debuggability. We evaluate Atom-Aid using buggy code from applications including Apache, MySQL, and XMMS, showing that Atom-Aid virtually eliminates the manifestation of atomicity violations.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556733,no,undetermined,0
The Allure and Risks of a Deployable Software Engineering Project: Experiences with Both Local and Distributed Development,"The student project is a key component of a software engineering course. What exact goals should the project have, and how should the instructors focus it? While in most cases projects are artificially designed for the course, we use a deployable, realistic project. This paper presents the rationale for such an approach and assesses our experience with it, drawing on this experience to present guidelines for choosing the theme and scope of the project, selecting project tasks, switching student groups, specifying deliverables and grading scheme. It then expands the discussion to the special but exciting case of a project distributed between different universities, the academic approximation of globalized software development as practiced today by the software industry.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556943,no,undetermined,0
An Experience on Applying Learning Mechanisms for Teaching Inspection and Software Testing,"Educational modules, concise units of study capable of integrating theoretical/practical content and supporting tools, are relevant mechanisms to improve learning processes. In this paper we briefly discuss the establishment of mechanisms to ease the development of educational modules - a Standard Process for Developing Educational Modules and an Integrated Modeling Approach for structuring their learning content. The proposed mechanisms have been investigated in the development of the ITonCode module - an educational module for teaching inspection and testing techniques. Aiming at evaluating the module we have replicated an extended version of the Basili & Selby experiment, originally used for comparing V&V techniques, now considering the educational context. The obtained results were mainly analyzed in terms of the student's uniformity in detecting existent faults, giving us very preliminar evidences on the learning effectiveness provided by the module produced.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556965,no,undetermined,0
Predication of Software Reliability Based on Grey System,"There are lots of factors influencing software reliability in course of developing software, so scientists have to omit minor factors and to preserve key factors. Furthermore, artificially simplify and limit the ways affecting the preserved factors, which is used as the basis to establish mathematic model and predict software reliability. As same as other software reliability models, there are also some assumptions or limitations in this paper. However, theses assumptions or limitations are originated from basic assumption of grey system. Most traditional software reliability models are derived from probability method, but grey system is not. Therefore, this paper will only make a comparison between our mathematic model derived by grey system and that derived by probability method. Finally, the result was that both models are limitary discrete monotone increasing exponential function. However, precision of reliability predication will be dependent on testing of bigger real cases for a long time. This paper, based on grey system, simply has put forward our viewpoint.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019970,no,undetermined,0
Ontology Based Software Reconfiguration in a Ubiquitous Computing Environment,"A middleware in ubiquitous computing environment (UbiComp) is required to support seamless on-demand services over diverse resource situations in order to meet various user requirements [1]. Since UbiComp applications need situation-aware middleware services in this environment. In this paper, we propose a semantic middleware architecture to support dynamic software component reconfiguration based fault and service ontology to provide fault-tolerance in a ubiquitous computing environment. Our middleware includes autonomic management to detect faults, analyze causes of them, and plan semantically meaningful strategies to deal with a problem with associating fault and service ontology trees. We implemented a referenced prototype, Web-service based Application Execution Environment (Wapee), as a proof-of-concept, and showed the efficiency in runtime recovery.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020020,no,undetermined,0
Risk: A Good System Security Measure,"What gets measured gets done. Security engineering as a discipline is still in its infancy. The field is hampered by its lack of adequate measures of goodness. Without such a measure, it is difficult to judge progress and it is particularly difficult to make engineering trade-off decisions when designing systems. The qualities of a good metric include that it: (1) measures the right thing, (2) is quantitatively measurable, (3) can be measured accurately, (4) can be validated against ground truth, and (5) be repeatable. By """"measures the right thing"""", the author means that it measures some set of attributes that directly correlates to closeness to meeting some stated goal. For system security, the author sees the right goal as """"freedom from the possibility of suffering damage or loss from malicious attack."""" Damage or loss applies to the mission effectiveness of the information infrastructure of a system. The mission can be maximizing profits while making quality cars or it could be defending an entire nation against foreign incursion",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020053,no,undetermined,0
On the Distribution of Property Violations in Formal Models: An Initial Study,"Model-checking techniques are successfully used in the verification of both hardware and software systems of industrial relevance. Unfortunately, the capability of current techniques is still limited and the effort required for verification can be prohibitive (if verification is possible at all). As a complement, fast, but incomplete, search tools may provide practical benefits not attainable with full verification tools, for example, reduced need for manual abstraction and fast detection of property violations during model development. In this report we investigate the performance of a simple random search technique. We conducted an experiment on a production-sized formal model of the mode-logic of a flight guidance system. Our results indicate that random search quickly finds the vast majority of property violations in our case-example. In addition, the times to detect various property violations follow an acutely right-skewed distribution and are highly biased toward the easy side. We hypothesize that the observations reported here are related to the phase transition phenomenon seen in Boolean satisfiability and other NP-complete problems. If so, these observations could be revealing some of the fundamental aspects of software (model) faults and have implications on how software engineering activities, such as analysis, testing, and reliability modeling, should be performed",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020073,no,undetermined,0
Improving Effectiveness of Automated Software Testing in the Absence of Specifications,"Program specifications can be valuable in improving the effectiveness of automated software testing in generating test inputs and checking test executions for correctness. Unfortunately, specifications are often absent from programs in practice. We present a framework for improving effectiveness of automated testing in the absence of specifications. The framework supports a set of related techniques, including redundant-test detection, non-redundant-test generation, test selection, test abstraction, and program-spectra comparison. The framework has been implemented and empirical results have shown that the developed techniques within the framework improve the effectiveness of automated testing by detecting high percentage of redundant tests among test inputs generated by existing tools, generating non-redundant test inputs to achieve high structural coverage, reducing inspection efforts for detecting problems in the program, and exposing behavioral differences during regression testing",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021362,no,undetermined,0
Model-Based Testing of Community-Driven Open-Source GUI Applications,"Although the World-Wide-Web (WWW) has significantly enhanced open-source software (OSS) development, it has also created new challenges for quality assurance (QA), especially for OSS with a graphical-user interface (GUI) front-end. Distributed communities of developers, connected by the WWW, work concurrently on loosely-coupled parts of the OSS and the corresponding GUI code. Due to the unprecedented code churn rates enabled by the WWW, developers may not have time to determine whether their recent modifications have caused integration problems with the overall OSS; these problems can often be detected via GUI integration testing. However, the resource-intensive nature of GUI testing prevents the application of existing automated QA techniques used during conventional OSS evolution. In this paper we develop new process support for three nested techniques that leverage developer communities interconnected by the WWW to automate model-based testing of evolving GUI-based OSS. The """"innermost"""" technique (crash testing) operates on each code check-in of the GUI software and performs a quick and fully automatic integration test. The second technique {smoke testing) operates on each day's GUI build and performs functional """"reference testing"""" of the newly integrated version of the GUI. The third (outermost) technique (comprehensive GUI testing) conducts detailed integration testing of a major GUI release. An empirical study involving four popular OSS shows that (1) the overall approach is useful to detect severe faults in GUI-based OSS and (2) the nesting paradigm helps to target feedback and makes effective use of the WWW by implicitly distributing QA",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021332,no,undetermined,0
Rephrasing Rules for Off-The-Shelf SQL Database Servers,"We have reported previously (Gashi et al., 2004) results of a study with a sample of bug reports from four off-the-shelf SQL servers. We checked whether these bugs caused failures in more than one server. We found that very few bugs caused failures in two servers and none caused failures in more than two. This would suggest a fault-tolerant server built with diverse off-the-shelf servers would be a prudent choice for improving failure detection. To study other aspects of fault tolerance, namely failure diagnosis and state recovery, we have studied the """"data diversity"""" mechanism and we defined a number of SQL rephrasing rules. These rules transform a client sent statement to an additional logically equivalent statement, leading to more results being returned to an adjudicator. These rules therefore help to increase the probability of a correct response being returned to a client and maintain a correct state in the database",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020843,no,undetermined,0
"Improving access to relevant data on faults, errors and failures in real systems","In order to be able to test the effectiveness and verify proposed techniques for enhanced availability based on field data from systems it is important to have reliability data of the components and the information necessary to characterize or model the system. This includes inter alia the type and number of components, their protection and dependency relations as well the automatic recovery mechanisms built into the system. An important benefit of making system models and logs available to the research community in a standard format is that it opens up the possibility for creating tools to assess and optimize deployed as well as hypothetical system configurations. Specialized tools for on-line and off-line analysis and classification of reliability data also become viable. Availability modeling tools could be benchmarked against actual data. Depending on the usefulness of such tools and the level of adoption of standard models and formats in the industry a market for reliability data analysis tools could emerge over time. These tools could be used during the design, deployment and operation phases of a system in order to predict or enhance the availability of the services it provides",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020835,no,undetermined,0
A fault tolerant VoIP implementation based on open standards,"This paper highlights the design and implementation aspects for making voice over IP softswitches more dependable on commercial of-the-shelf telecommunication platforms. As a proof-of-concept, the open source Asterisk Private Branch Exchange application was made fault tolerant by using high availability middleware based on the Service Availability Forum's application interface specifications (AIS). The prototype was implemented on Intel x86 architecture blade servers running Carrier Grade Linux in an active/hot-standby configuration. Primarily, the Asterisk application was re-engineered and adapted to use AIS defined interfaces and model. In case of application, component or node failures, the middleware detects and triggers the application failover to the hot-standby node. The Asterisk application on the hot-standby node detects it is now the active instance, so it retrieves the checkpoint data and immediately continues to service both existing and new call-sessions thus improving overall availability",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020828,no,undetermined,0
QoS-Based Service Composition,"QoS has been one of the major challenges in Web services area. Though negotiated in the contract, service quality usually can not be guaranteed by providers. Therefore, the service composer is obligated to detect real quality status of component services. Local monitoring cannot fulfil this task. We propose a Probe-based architecture to address this problem. By running light weighted test cases, the Probe can collect accurate quality data to support runtime service composition and composition re-planning",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020196,no,undetermined,0
Consensus ontology generation in a socially interacting multiagent system,"This paper presents an approach for building consensus ontologies from the individual ontologies of a network of socially interacting agents. Each agent has its own conceptualization of the world. The interactions between agents are modeled by sending queries and receiving responses and later assessing each other's performance based on the results. This model enables us to measure the quality of the societal beliefs in the resources which we represent as the expertise in each domain. The dynamic nature of our system allows us to model the emergence of consensus that mimics the evolution of language. We present an algorithm for generating the consensus ontologies which makes use of the authoritative agent's conceptualization in a given domain. As the expertise of agents change after a number of interactions, the consensus ontology that we build based on the agents' individual views evolves. We evaluate the consensus ontologies by using different heuristic measures of similarity based on the component ontologies",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020180,no,undetermined,0
Automated Health-Assessment of Software Components using Management Instrumentatio,"Software components are regularly reused in many large-scale, mission-critical systems where the tolerance for poor performance is quite low. As new components are integrated within an organization's computing infrastructure, it becomes critical to ensure that these components continue to meet the expected quality of service (QoS) requirements. Management instrumentation is an integrated capability of a software system that enables an external entity to assess that system's internals, such as its operational states, execution traces, and various quality attributes during runtime. In this paper, we present an approach that enables the efficient generation, measurement, and assessment of various QoS attributes of software components during runtime using management instrumentation. Monitoring the quality of a component in this fashion has many benefits, including the ability to proactively detect potential QoS-related issues within a component to avoid potentially expensive downtime of the overall environment. The main contributions of our approach consist of three parts: a lightweight component instrumentation framework that transparently generates a pre-defined set of QoS-related diagnostic data when integrated within a component, a method to formally define the health state of a component in terms of the expected QoS set forth by the target environment, and finally a method for publishing the QoS-related diagnostic data during runtime so that an external entity can measure the current health of a component and take appropriate actions. The main QoS types that we consider are: performance, reliability, availability, throughput, and resource usage. Experimentation results show that our approach can be efficiently utilized in large mission-critical systems",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020164,no,undetermined,0
A Technique to Reduce the Test Case Suites for Regression Testing Based on a Self-Organizing Neural Network Architecture,"This paper presents a technique to select subsets of the test cases, reducing the time consumed during the evaluation of a new software version and maintaining the ability to detect defects introduced. Our technique is based on a model to classify test case suites by using an ART-2A self-organizing neural network architecture. Each test case is summarized in a feature vector, which contains all the relevant information about the software behavior. The neural network classifies feature vectors into clusters, which are labeled according to software behavior. The source code of a new software version is analyzed to determine the most adequate clusters from which the test case subset will be selected. Experiments compared feature vectors obtained from all-uses code coverage information to a random selection approach. Results confirm the new technique has improved the precision and recall metrics adopted",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020148,no,undetermined,0
Traceability between Software Architecture Models,"Software architecture (SA) is the blueprint of the software system and considered as one of the most important artifacts in component based development. The design and analysis of SA can be very complex. Under the inspiration of model-driven development, the design of SA has been no more constrained in one stage. It is a trend to construct multiple SA models in multiple stages during the software life cycle. Thus, the traceability between these SA models becomes a new challenge. The information between these SA models in deferent stages is usually not recorded well and easy to be lost lately, which makes the maintenance and evolution difficult and error-prone. In this paper, we present an approach to recording the information between SA models via a traceability model for reducing the loss of design decisions and helping developers understand the software system well",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020137,no,undetermined,0
Test Case Prioritization Using Relevant Slices,"Software testing and retesting occurs continuously during the software development lifecycle to detect errors as early as possible. The sizes of test suites grow as software evolves. Due to resource constraints, it is important to prioritize the execution of test cases so as to increase chances of early detection of faults. Prior techniques for test case prioritization are based on the total number of coverage requirements exercised by the test cases. In this paper, we present a new approach to prioritize test cases based on the coverage requirements present in the relevant slices of the outputs of test cases. We present experimental results comparing the effectiveness of our prioritization approach with that of existing techniques that only account for total requirement coverage, in terms of ability to achieve high rate of fault detection. Our results present interesting insights into the effectiveness of using relevant slices for test case prioritization",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020103,no,undetermined,0
On Detection Conditions of Double FaultsRelated to Terms in Boolean Expressions,"Detection conditions of specific classes of faults have recently been studied by many researchers. Under the assumption that at most one of these faults occurs in the software under test, these fault detection conditions were mainly used in two ways. First, they were used to develop test case selection strategies for detecting corresponding classes of faults. Second, they were used to study fault class hierarchies, where a test case that detects a particular class of faults can also detect some other classes of faults. In this paper, we study detection conditions of double faults. Besides developing new test case selection strategies and studying new fault class hierarchies, our analysis provides further insights to the effect of fault coupling. Moreover, these fault detection conditions can be used to compare effectiveness of existing test case selection strategies (which were originally developed for the detection of single occurrence of certain classes of faults) in detecting double faults that may be present in the software",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020102,no,undetermined,0
An adaptive CAC algorithm based on fair utility for low earth orbit satellite networks,"A novel adaptive call admission control algorithm for multimedia low orbit satellite networks was proposed. Based on real-time call dropping probability of destination cell, the algorithm is able to reserve bandwidth for handoff calls combining probability threshold method and fair utility allocation scheme. Simulation results show that the proposed algorithm presents satisfactory new call blocking probability and greatly reduces handoff call dropping probability, while guarantees the high bandwidth utilization.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554415,no,undetermined,0
Security Consistency in UML Designs,"Security attacks continually threaten distributed systems, disrupting both individuals and organizations economically and physically. In the software lifecycle, early detection and correction of security flaws in the design phase can reduce overall costs associated with maintenance. Current software development methodologies such as the model driven architecture rely on quality Unified Modeling Language (UML) design models. Often these models are complex and consist of many structural and behavioral views. This can lead to inconsistencies between views. Existing approaches remedy many of these inconsistencies but do not address security consistency across design views. This paper presents an approach to detecting and resolving security faults in UML designs. The approach defines the notion of security inconsistency in designs, analyzes UML views for security inconsistencies, and generates a set of recommended design changes that include Object Constraint Language (OCL) expressions. The OCL can be used as a test oracle in both the design and implementation phases of the software life-cycle",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020096,no,undetermined,0
Pre-emption based call admission control with QoS and dynamic bandwidth reservation for cellular networks,"Call admission protocol (CAC) is a very important process in the provision of good quality of service (QoS) in cellular mobile networks. With micro/Pico cellular architectures that are now used to provide higher capacity, the cell size decreases with a drastic increase in the handoff rate. In this paper, we present modeling and simulation results to help in better understanding of the performance and efficiency of CAC in cellular networks. Handoff prioritization is a common characteristic, which is achieved through the threshold bandwidth reservation policy framework. Combined with this framework, we use pre-emptive call admission scheme and elastic bandwidth allocation for data calls in order to gain a near optimal QoS. In this paper, we also use a genetic algorithm (GA) based approach to optimize the fitness function, which we obtained by calculating the mean square error of predicted rejection values and the actual ones. The predicted values are calculated using a linear model, which relates the rejection ratios with different threshold values.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554510,no,undetermined,0
Dialogue-Based Authoring of Units of Learning,"Authoring learning content along with specifying its relationship to pedagogical scenarios is a tedious and error-prone task. This paper describes DBAT-LD (dialogue-based authoring of learning designs), which is a chat bot (natural language dialogue system) that interacts with authors of units of learning and secures the content of the dialogue in an XML-based target format of choice. DBAT-LD can be geared towards different specifications thereby contributing to interoperability in e-learning. In the case of IMS-LD, DBAT-LD elicits and reconstructs an account of learning activities along with the pedagogical support activities by Koper, R. & Olivier, B. (2004) and delivers a Level A description of a unit of learning",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652466,no,undetermined,0
Ensuring numerical quality in grid computing,"We propose an approach which gives the user valuable information on the various platforms avail able in a grid in order to assess the numerical quality of an algorithm run on each of these platforms. In this manner, the user is provided with at least very strong hints whether a program performs reliably in a grid before actually executing it. Our approach extends IeeeCC754 by two """"grid-enabled"""" modes: The first mode calculates a """"numerical checksum"""" on a specific grid host and executes the job only if the check sum is identical to a locally generated one. The second mode provides the user with information on the reliability and IEEE 754-conformity of the underlying floating-point implementation of various platforms. In addition, it can help to find a set of compiler options to optimize the application's performance while retaining numerical stability",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652171,no,undetermined,0
Market-Based Resource Allocation using Price Prediction in a High Performance Computing Grid for Scientific Applications,"We present the implementation and analysis of a market-based resource allocation system for computational grids. Although grids provide a way to share resources and take advantage of statistical multiplexing, a variety of challenges remain. One is the economically efficient allocation of resources to users from disparate organizations who have their own and sometimes conflicting requirements for both the quantity and quality of services. Another is secure and scalable authorization despite rapidly changing allocations. Our solution to both of these challenges is to use a market-based resource allocation system. This system allows users to express diverse quantity- and quality-of-service requirements, yet prevents them from denying service to other users. It does this by providing tools to the user to predict and tradeoff risk and expected return in the computational market. In addition, the system enables secure and scalable authorization by using signed money-transfer tokens instead of identity-based authorization. This removes the overhead of maintaining and updating access control lists, while restricting usage based on the amount of money transferred. We examine the performance of the system by running a bioinformatics application on a fully operational implementation of an integrated grid market",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652144,no,undetermined,0
A New Allocation Scheme for Parallel Applications with Deadline and Security Constraints on Clusters,"Parallel applications with deadline and security constraints are emerging in various areas like education, information technology, and business. However, conventional job schedulers for clusters generally do not take security requirements of realtime parallel applications into account when making allocation decisions. In this paper, we address the issue of allocating tasks of parallel applications on clusters subject to timing and security constraints in addition to precedence relationships. A task allocation scheme, or TAPADS (task allocation for parallel applications with deadline and security constraints), is developed to find an optimal allocation that maximizes quality of security and the probability of meeting deadlines for parallel applications. In addition, we proposed mathematical models to describe a system framework, parallel applications with deadline and security constraints, and security overheads. Experimental results show that TAPADS significantly improves the performance of clusters in terms of quality of security and schedulability over three existing allocation schemes",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4154100,no,undetermined,0
On-line detection of stator winding faults in controlled induction machine drives,"The operation of induction machines with fast switching power electric devices puts additional stress on the stator windings what leads to an increased probability of machine faults. These faults can cause considerable damage and repair costs and - if not detected in an early stage - may end up in a total destruction of the machine. To reduce maintenance and repair costs many methods have been developed and presented in literature for an early detection of machine faults. This paper gives an overview of todaypsilas detection techniques and divides them into three major groups according to their underlying methodology. The focus will be on methods which are applicable to todaypsilas inverter-fed machines. In that case and especially if operated under controlled mode, the behavior of the machine with respect to the fault is different than for grid supply. This behavior is discussed and suitable approaches for fault detection are presented. Which method is eventually to choose, will depend on the application and the available sensors as well as hard- and software resources, always considering that the additional effort for the fault detection algorithm has to be kept as low as possible. The applicability of the presented fault detection techniques are also confirmed with practical measurements.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4662507,no,undetermined,0
PD diagnosis on medium voltage cables with Oscillating Voltage (OWTS),"Detecting, locating and evaluating of partial discharges (PD) in the insulating material, terminations and joints provides the opportunity for a quality control after installation and preventive detection of arising service interruption. A sophisticated evaluation is necessary between PD in several insulating materials and also in different types of terminations and joints. For a most precise evaluation of the degree and risk caused by PD it is suggested to use a test voltage shape that is preferably like the same under service conditions. Only under these requirements the typical PD parameters like inception and extinction voltage, PD level and PD pattern correspond to significant operational values. On the other hand the stress on the insulation should be limited during the diagnosis to not create irreversible damages and thereby worsening the condition of the test object. The paper introduces an Oscillating Wave Test System (OWTS), which meets these mentioned demands well. The design of the system, its functionality and especially the operating software are made for convenient field application. Field data and experience reports will be presented and discussed. This field data serve also as good guide for the level of danger to the different insulating systems due to partial discharges.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4524798,no,undetermined,0
APART: Low Cost Active Replication for Multi-tier Data Acquisition Systems,"This paper proposes APART (a posteriori active replication), a novel active replication protocol specifically tailored for multi-tier data acquisition systems. Unlike existing active replication solutions, APART does not rely on a-priori coordination schemes determining a same schedule of events across all the replicas, but it ensures replicas consistency by means of an a-posteriori reconciliation phase. The latter is triggered only in case the replicated servers externalize their state by producing an output event towards a different tier. On one hand, this allows coping with non-deterministic replicas, unlike existing active replication approaches. On the other hand, it allows attaining striking performance gains in the case of silent replicated servers, which only sporadically, yet unpredictably, produce output events in response to the receipt of a (possibly large) volume of input messages. This is a common scenario in data acquisition systems, where sink processes, which filter and/or correlate incoming sensor data, produce output messages only if some application relevant event is detected. Further, the APART replica reconciliation scheme is extremely lightweight as it exploits the cross-tier communication pattern spontaneously induced by the application logic to avoid explicit replicas coordination messages.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579633,no,undetermined,0
Failure's Identification for Electromechanical Systems with Induction Motor,"In this paper the new approach for guaranteed state estimation for electromechanical systems with induction motor is implemented for failure's identification in such systems. Approach is based on matrix comparison systems method, and a comparison with some other estimation methods was realized in package MATLAB. The experimental unit made for practical approbation of identification algorithms is presented with received experimental data. The new approach (matrix comparison systems method) for guaranteed state estimation of an induction motor (IM) and detect faults was presented in [1, 2]. State estimation is based on a discrete-time varying linear model of the IM [3, 4] with uncertainties and perturbations, which belong to known bounded sets with no hypothesis on their distribution inside these sets. The approach uses the measurement results. Recursive and explicit algorithm is presented and illustrated by example with real 1M parameters, realized in program package MATLAB.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493241,no,undetermined,0
N-Version Software Systems Design,"The problem of developing an optimal structure of N-version software system presents a kind of very complex optimization problem. This causes the use of deterministic optimization methods inappropriate for solving the stated problem. In this view, exploiting heuristic strategies looks more rational. In the field of pseudo-Boolean optimization theory, the so called method of varied probabilities (MVP) has been developed to solve problems with a large dimensionality. Some additional modifications of MVP have been made to solve the problem of N-version systems design. Those algorithms take into account the discovered specific features of the objective function. The practical experiments have shown the advantage of using these algorithm modifications because of reducing a search space.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493232,no,undetermined,0
A Novel Technique for Modeling Radiation Effects in Solar Cells Utilizing SILVACOA Virtual Wafer Fabrication Software,"A novel technique for modeling advanced solar cells using Silvaco<sup>reg</sup> virtual wafer fabrication software has been previously introduced. Over the past three years the new modeling approach has been extended to cover modeling of advanced multijunction cells, design and optimizations of these devices, as well as design of new quad-junction cells. In this paper, the ATLAS device simulator from Silvaco International has been demonstrated to have the potential for predicting the effects of electron radiation in solar cells by modeling material defects. A gallium arsenide solar cell was simulated in ATLAS and compared to an actual cell with radiation defects identified using deep level transient spectroscopy techniques. The cell data were compared for various fluence levels of 1 MeV electron radiation and the results have shown an average of only five percent difference between experimental and simulated cell output characteristics. These results demonstrate that ATLAS software can be a viable tool for predicting solar cell degradation due to electron radiation.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4365662,no,undetermined,0
Adaptive Checkpointing for Master-Worker Style Parallelism,"We present a transparent, system-level checkpointing solution for master-worker parallelism that automatically adapts, upon restore, to the number of processor nodes available. We call this adaptive checkpointing. This is important, since nodes in a cluster fail. It also allows one to adapt to using mutliple cluster partitions, as they become available. Checkpointing a master-worker computation has the additional advantage of needing to checkpoint only the master process. This is both fast (0.05 s in our case), and more economical of disk space. We describe a system-level solution. The application writer does not declare what data structures to checkpoint. Furthermore, the solution is transparent. The application writer need not add code to request a checkpoint at appropriate locations. The system-level strategy avoids the labor-intensive and error-prone work of explicitly checkpointing the many data structures of a large program",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4154139,no,undetermined,0
On the Effect of Ontologies on Quality of Web Applications,"The semantic Web can be seen as means to improve qualitative characteristics of Web applications. Ontologies play a key role in the semantic Web and, therefore, are expected to have a profound effect on quality of a Web application. We apply the Quint2 model to predict an impact of an ontology on a number of quality dimensions of a Web application. We estimate that an ontology is likely to significantly improve the functionality and maintainability dimensions. The usability dimension is affected to a lesser extent. We explain the expected increase of quality with improved effectiveness of development process caused primarily by application domain ontologies",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145935,no,undetermined,0
An Improved Algorithm for Deadlock Detection and Resolution in Mobile Agent Systems,"Mobile agent systems have been proved that are the best paradigm for distributed applications. They have potential advantages to provide a convenient, efficient and high performance distributed applications. Many solutions for problems in distributed systems such as deadlock detection rely on assumptions such as data location and message passing mechanism and static network topology that could not be applied for mobile agent systems. In this paper an improved distributed deadlock detection and resolution algorithm is proposed. The algorithm is based on Ashfield et. al. process. There are some cases in which original algorithm detects false deadlock or does not detect global deadlocks. The proposed algorithm eliminates the original algorithm deficiencies and improves its performance. It also minimizes the detection agent travels through the communication network. Also it has a major impact on improving performance of the mobile agent systems",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631606,no,undetermined,0
Project Management Trends of Pakistani Software Industry,"The ability to produce cost effective and quality software is heavily dependent on the maturity of the processes used to build the software. Software industry in general and Pakistani software industry in particular emerged and witnesses turmoil in the recent years. Now it is the right time to assess the project management capabilities, trends and requirements of the local industry. To assess the level of maturity of Pakistan software industry in software project management, an interview-based survey of the some Islamabad based well-known companies was carried out. The results of the survey highlight the trends and practices of the industry regarding project management, and its level of maturity",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4133406,no,undetermined,0
Developing the DigiQUAL protocol for digital library evaluation,"The distributed, project-oriented nature of digital libraries (DLs) has made them difficult to evaluate in aggregate. By modifying the methods and tools used to evaluate physical libraries' content and services, measures can be developed whose results can be used across a variety of DLs. The DigiQUAL protocol being developed by the Association of Research Libraries (ARL) has the potential to provide the National Science Digital Library (NSDL) with a standardized methodology and survey instrument with which to evaluate not only its distributed projects but also to gather data to assess the value and impact of the NSDL",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118536,no,undetermined,0
Software Reliability Modeling withWeibull-type Testing-Effort and Multiple Change-Points,"Software reliability is defined as the probability of failure-free software operation for a specified period of time in a specified environment. Over the past 30 years, many software reliability growth models (SRGMs) have been proposed for estimation of reliability growth of products during software development processes. SRGMs proposed in the literature took into consideration the amount of testing-effort spent on software testing which can be depicted as a Weibull-type curve. However, in reality, the consumption rate of testing-effort expenditures may not be a constant and could be changed at some time points. Therefore, in this paper, we will incorporate the concept of multiple change-points into the Weibull-type testing-effort function. New model is proposed and the applicability of proposed model is demonstrated through real software failure data set. Our experimental results show that the proposed model has a fairly accurate prediction capability.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4085303,no,undetermined,0
Adaptive Checkpoint Replication for Supporting the Fault Tolerance of Applications in the Grid,"A major challenge in a dynamic Grid with thousands of machines connected to each other is fault tolerance. The more resources and components involved, themore complicated and error-prone becomes the system. Migol is an adaptive Grid middleware, which addresses the fault tolerance of Grid applications and services by providing the capability to recover applications from checkpoint files automatically. A critical aspect for an automatic recovery is the availability of checkpoint files: If a resource becomes unavailable, it is very likely that the associated storage is also unreachable, e. g. due to a network partition. A strategy to increase the availability of checkpoints isreplication.In this paper, we present the Checkpoint Replication Service. A key feature of this service is the ability to automatically replicate and monitor checkpoints in the Grid.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579677,no,undetermined,0
Development of On-Line Diagnostics and Real Time Early Warning System for Vehicles,"On-board diagnostics (OBD) system is developed to detect vehicle system error and malfunction for health diagnosis, OBD generates warning signals to vehicle operators as well as the maintenance engineers. However, once a warning signal is being generated, most operators are not knowledgeable to take any action on it. Data acquisition has to rely on maintenance engineer using special tools. Based on such a practical demand, this paper presents a new vehicle on-line diagnosis and real time early warning system to acquire OBD signals and transmit to a Server of Maintenance Center via GPRS mobile communication for immediate actions. In this paper, hardware and software in both design and implementation are discussed with preliminary tests. The test functions of the proposed system fulfil the rising requirements for modern vehicle system",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027453,no,undetermined,0
An Experimental Evaluation of the Reliability of Adaptive Random Testing Methods,"Adaptive random testing (ART) techniques have been proposed in the literature to improve the effectiveness of random testing (RT) by evenly distributing test cases over the input space. Simulations and mutation analyses of various ART techniques have demonstrated their improvements on fault detecting ability when measured by the number of test cases required to detect the first fault. In this paper, we report an experiment with ART using mutants to evaluate ARTpsilas reliability in fault detecting ability. Our experiment discovered that ART is more reliable than RT in the sense that its degree of variation in fault detecting ability is significantly lower than RT. It is also recognized from the experiment data that the two main factors that affect ARTpsilas reliability are the failure rate of the system under test and the regularity of the failure domain measured by the standard deviation of random test results.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579790,no,undetermined,0
Collaborative sensing using uncontrolled mobile devices,"This paper considers how uncontrolled mobiles can be used to collaboratively accomplish sensing tasks. Uncontrolled mobiles are mobile devices whose movements cannot be easily controlled for the purpose of achieving a task. Examples include sensors mounted on mobile vehicles of people to monitor air quality and to detect potential airborne nuclear, biological, or chemical agents. We describe an approach for using uncontrolled mobile devices for collaborative sensing. Considering the potentially large number of mobile sensors that may be required to monitor a large geographical area such as a city, a key issue is how to achieve a proper balance between performance and costs. We present analytical results on the rate of information reporting by uncontrolled mobile sensors needed to cover a given geographical area. We also present results from testbed implementations to demonstrate the feasibility of using existing low-cost software technologies and platforms with existing standard protocols for information reporting and retrieval to support a large system of uncontrolled mobile sensors",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1651206,no,undetermined,0
Resource mapping and scheduling for heterogeneous network processor systems,"Task to resource mapping problems are encountered during (i) hardware-software co-design and (ii) performance optimization of Network Processor systems. The goal of the first problem is to find the task to resource mapping that minimizes the design cost subject to all design constraints. The goal of the second problem is to find the mapping that maximizes the performance, subject to all architectural constraints. To meet the design goals in performance, it may be necessary to allow multiple packets to be inside the system at any given instance of time and this may give rise to the resource contention between packets. In this paper, a Randomized Rounding (RR) based solution is presented for the task to resource mapping and scheduling problem. We also proposed two techniques to detect and eliminate the resource contention. We evaluate the efficacy of our RR approach through extensive simulation. The simulation results demonstrate that this approach produces near optimal solutions in almost all instances of the problem in a fraction of time needed to find the optimal solution. The quality of the solution produced by this approach is also better than often used list scheduling algorithm for task to resource mapping problem. Finally, we demonstrate with a case study, the results of a Network Processor design and scheduling problem using our techniques.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4675262,no,undetermined,0
Software Engineering Education From Indian Perspective,"Software is omnipresent in today's world. India is a hub to more than 1000 software companies. The software industry is a major employment providing industry in India. As a wholly intellectual artifact, software development is among the most labor demanding, intricate, and error-prone technologies in human history. Software's escalating vital role in systems of pervasive impact presents novel challenges for the education of software engineers. This paper focuses on the current status of software engineering education in India and suggestions for improvement so as to best suit the software industry's needs",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4698915,no,undetermined,0
A Case Study: GQM and TSP in a Software Engineering Capstone Project,"This paper presents a case study, describing the use of a hybrid version of the team software process (TSP) in a capstone software engineering project. A mandatory subset of TSP scripts and reporting mechanisms were required, primarily for estimating the size and duration of tasks and for tracking project status against the project plan. These were supplemented by metrics and additional processes developed by students. Metrics were identified using the goal-question-metric (GQM) process and used to evaluate the effectiveness of project management roles assigned to each member of the project team. TSP processes and specific TSP forms are identified as evidence of learning outcome attainment. The approach allowed for student creativity and flexibility and limited the perceived overhead associated with use of the complete TSP. Students felt that the experience enabled them to further develop and demonstrate teamwork and leadership skills. However, limited success was seen with respect to defect tracking, risk management, and process improvement. The case study demonstrates that the approach can be used to assess learning outcome attainment and highlights for students the significance of software engineering project management",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4698926,no,undetermined,0
Medical device software standards,"Much medical device software is safety-related, and therefore needs to have high integrity (in other words its probability of failure has to be low.) There is a consensus that if you want to develop high-integrity software, you need a quality system. This is because software is a complex product that is easy to change and difficult to test, and the management system that handles these issues must include such quality system elements as: detailed traceable specifications, disciplined processes, planned verification and validation and a comprehensive configuration management and change control system. It is also agreed that software quality management systems need specific processes which are different from and additional to more general quality management systems such as that required by EN 13485. Historically, ISO 9000-3 Part 3: Guidelines for the application of ISO 9001:1994 to the development, supply, installation and maintenance of computer software states these additional processes very clearly, but is not mandatory. (This is now ISO 90003.) In both Europe and the USA there is therefore a gap in both regulations and standards for Medical Devices. There is no comprehensive requirement specifically for software development methods. In Europe, IEC 60601-1 Medical electrical equipment Part 1: General requirements for safety and essential performance, has specific requirements for software in section 14 Programmable Electrical Medical Systems (PEMS). This requires (at a fairly abstract level) some basic processes and documents, and includes an invocation of the risk management process of ISO 14971 Medical devices Application of risk management to medical devices In the US, there is an FDA regulation requiring Good Manufacturing Practice, with guidance on software development methods (strangely entitled Software Validat",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679198,no,undetermined,0
Advancing candidate link generation for requirements tracing: the study of methods,"This paper addresses the issues related to improving the overall quality of the dynamic candidate link generation for the requirements tracing process for verification and validation and independent verification and validation analysts. The contribution of the paper is four-fold: we define goals for a tracing tool based on analyst responsibilities in the tracing process, we introduce several new measures for validating that the goals have been satisfied, we implement analyst feedback in the tracing process, and we present a prototype tool that we built, RETRO (REquirements TRacing On-target), to address these goals. We also present the results of a study used to assess RETRO's support of goals and goal elements that can be measured objectively.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1583599,no,undetermined,0
CrossTalk: cross-layer decision support based on global knowledge,"The dynamic nature of ad hoc networks makes system design a challenging task. Mobile ad hoc networks suffer from severe performance problems due to the shared, interference-prone, and unreliable medium. Routes can be unstable due to mobility and energy can be a limiting factor for typical devices such as PDAs, mobile phones, and sensor nodes. In such environments cross-layer architectures are a promising new approach, as they can adapt protocol behavior to changing networking conditions. This article introduces CrossTalk, a cross-layer architecture that aims at achieving global objectives with local behavior. It further compares CrossTalk with other cross-layer architectures proposed. Finally, it analyzes the quality of the information provided by the architecture and presents a reference application to demonstrate the effectiveness of the general approach.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580938,no,undetermined,0
Measurement Framework for Assessing Risks in Component-Based Software Development,"As Component-based software development (CBSD) is getting popular and is being considered as both efficient and effective approach to build large software applications and systems, potential risks within component-based practicing areas such as quality assessment, complexity estimation, performance prediction, configuration, and application management should not be taken lightly. In the existing literature there is lack of systematic work in identifying and assessing these risks. In particular, there is lack of a structuring framework that could be helpful for related CBSD stakeholders to measure these risks. In this research we examine prior related research work in software measurement and aim to develop a practical risk measurement framework that classifies potential CBSD risks and related metrics and provides a practical guidance for CBSD stakeholders.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579763,no,undetermined,0
Measuring the Quality of Ideation Technology and Techniques,"Ideation is an essential component of creativity and problem-solving. Researchers have measured the quality of ideation treatments by assigning quality scores to each unique idea generated in each session and then by calculating one or more of the sum-of-scores, average-quality-score, or count-of-good-ideas measures. We discuss the validity of these three measures and the potential biases associated with the sum-of-scores and average-quality measures. An experimental study comparing multiple levels of social comparison was used to illustrate the differences in the quality measures and the results revealed that research conclusions were dependent on the quality measure used. Implications for future research are discussed including a recommendation that future ideation research adopt the count-of-good-ideas measure for assessing ideation quality.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579332,no,undetermined,0
Assessing the Quality of Collaborative Processes,"Use of effective and efficient collaboration is important for organizations to survive and thrive in todays competitive world. This paper presents quality constructs that can be used to evaluate the success of a collaboration process. Two types of collaboration processes are identified: 1) processes that are designed and executed by the same facilitator who designed them, and 2) processes that are designed by a collaboration engineer and executed many times by practitioners. Accordingly, the quality constructs have been divided in two categories. Constructs within the first category apply to both types of collaboration processes. This category includes constructs such as process effectiveness and efficiency, results quantity, results quality, satisfaction, and usability. The second category contains constructs that are useful from the perspective of the collaboration engineering approach: repeatable collaboration processes executed by practitioners. The three constructs important for this perspective are reusability, predictability, and transferability.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579323,no,undetermined,0
Helping small companies assess software processes,"A first step toward process improvement is identifying the strengths and weaknesses of an organization's software processes to determine effective improvement actions. An assessment can help an organization examine its processes against a reference model to determine the processes' capability or the organization's maturity, to meet quality, cost, and schedule goals, but small companies have difficulty running them. MARES, a set of guidelines for conducting 15504-conformant software process assessment focuses on small companies",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1576663,no,undetermined,0
Gate sizing to radiation harden combinational logic,"A gate-level radiation hardening technique for cost-effective reduction of the soft error failure rate in combinational logic circuits is described. The key idea is to exploit the asymmetric logical masking probabilities of gates, hardening gates that have the lowest logical masking probability to achieve cost-effective tradeoffs between overhead and soft error failure rate reduction. The asymmetry in the logical masking probabilities at a gate is leveraged by decoupling the physical from the logical (Boolean) aspects of soft error susceptibility of the gate. Gates are hardened to single-event upsets (SEUs) with specified worst case characteristics in increasing order of their logical masking probability, thereby maximizing the reduction in the soft error failure rate for specified overhead costs (area, power, and delay). Gate sizing for radiation hardening uses a novel gate (transistor) sizing technique that is both efficient and accurate. A full set of experimental results for process technologies ranging from 180 to 70 nm demonstrates the cost-effective tradeoffs that can be achieved. On average, the proposed technique has a radiation hardening overhead of 38.3%, 27.1%, and 3.8% in area, power, and delay for worst case SEUs across the four process technologies.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1564311,no,undetermined,0
Rate-distortion performance of H.264/AVC compared to state-of-the-art video codecs,"In the domain of digital video coding, new technologies and solutions are emerging in a fast pace, targeting the needs of the evolving multimedia landscape. One of the questions that arises is how to assess these different video coding technologies in terms of compression efficiency. In this paper, several compression schemes are compared by means of peak signal-to-noise ratio (PSNR) and just noticeable difference (JND). The codecs examined are XviD 0.9.1 (conform to the MPEG-4 Visual Simple Profile), DivX 5.1 (implementing the MPEG-4 Visual Advanced Simple Profile), Windows Media Video 9, MC-EZBC and H.264/AVC AHM 2.0 (version JM 6.1 of the reference software, extended with rate control). The latter plays a key role in this comparison because the H.264/AVC standard can be considered as the de facto benchmark in the field of digital video coding. The obtained results show that H.264/AVC AHM 2.0 outperforms current proprietary and standards-based implementations in almost all cases. Another observation is that the choice of a particular quality metric can influence general statements about the relation between the different codecs.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1564130,no,undetermined,0
A quorum-based protocol for searching objects in peer-to-peer networks,"Peer-to-peer (P2P) system is an overlay network of peer computers without centralized servers, and many applications have been developed for such networks such as file sharing systems. Because a set of peers dynamically changes, design and verification of efficient protocols is a challenging task. In this paper, we consider an object searching problem under a resource model such that there are some replicas in a system and the lower bound of the ratio =n'/n is known in advance, where n' is a lower bound of the number of peers that hold original or replica for any object type and n is the total number of peers. In addition, we consider object searching with probabilistic success, i.e., for each object search, object must be found with at least probability 0<<1. To solve such a problem efficiently, we propose a new communication structure, named probabilistic weak quorum systems (PWQS), which is an extension of coterie. Then, we propose a fault-tolerant protocol for searching for objects in a P2P system. In our method, each peer does not maintain global information such as the set of all peers and a logical topology with global consistency. In our protocol, each peer communicates only a small part of a peer set and, thus, our protocol is adaptive for huge scale P2P network.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1549813,no,undetermined,0
wsrbench: An On-Line Tool for Robustness Benchmarking,"Testing Web services for robustness is a difficult task. In fact, existing development support tools do not provide any practical mean to assess Web services robustness in the presence of erroneous inputs. Previous works proposed that Web services robustness testing should be based on a set of robustness tests (i.e., invalid Web services call parameters) that are applied in order to discover both programming and design errors. Web services can be classified based on the failure modes observed. In this paper we present and discuss the architecture and use of an on-line tool that provides an easy interface for Web services robustness testing. This tool is publicly available and can be used by both web services providers (to assess the robustness of their Web services code) and consumers (to select the services that best fit their requirements). The tool is demonstrated by testing several Web services available in the Internet.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578524,no,undetermined,0
A Framework for Model-Based Continuous Improvement of Global IT Service Delivery Operations,"In recent years, the ability to deliver IT infrastructure services from multiple geographically distributed locations has given rise to an entirely new IT services business model. In this model, called the """"Global Delivery Model"""", clients out source components of their IT infrastructure operations to multiple service providers, who in turn use a combination of onsite and offsite (including offshore) resources to manage the components on behalf of the.ir clients. Since the components of services provided can be assembled and processed at any of the delivery centers, a framework for continuous monitoring of quality and productivity of the delivery processes is essential to pinpoint and remedy potential process inefficiencies. In this paper, we describe a framework implemented by a large global service provider that uses continuous monitoring and process behavior charts to detect any potential shifts in its global ITservice delivery environment. Using this framework, the service provider has already improved several of its IT delivery processes resulting in improved quality and productivity. We discuss the major components of the framework, challenges in deploying such a system for global processes whose lifecycle spans multiple delivery centers, and present examples of process improvements that resulted from deploying the framework.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578525,no,undetermined,0
Testing and Configuration Management,"Software is our lifeblood and the source of profound advances, but no one can deny that much of it is error-prone and likely to become more so with increasing complexity. Useful software is the abstraction of a problem and its solution, conditionally stable for the operational range that has been tested. That definition has spawned thousands of viewgraphs and millions of words, but still the stuff hangs and crashes. It may be obvious, but not trivial, to restate that untested systems will not work. Today testing is an art, whether it is the fine, meticulous art of debugging or the broader-brush scenario testing. The tools already exist for moving test design theory from an art form to the scientific role of <br> The Price of Quality <br> Unit Testing <br> Integration Testing <br> System Testing <br> Reliability Testing <br> Stress Testing <br> Robust Testing <br> Robust Design <br> Prototypes <br> Identify Expected Results <br> Orthogonal Array Test Sets (OATS) <br> Testing Techniques <br> One-Factor-at-a-Time <br> Exhaustive <br> Deductive Analytical Method <br> Random/Intuitive Method <br> Orthogonal Array-Based Method <br> Defect Analysis <br> Case Study: The Case of the Impossible Overtime <br> Cooperative Testing <br> Graphic Footprint <br> Testing Strategy <br> Test Incrementally <br> Test Under No Load <br> Test Under Expected Load <br> Test Under Heavy Load <br> Test Under Overload <br> Test the Error Recovery Code <br> Diabolic Testing <br> Reliability Tests <br> Footprint <br> Regression Tests <br> Software Hot Spots <br> Software Manufacturing Defined <br> Configuration Management <br> Outsourcing <br> Test Modules <br> Faster Iteration <br> Meaningful Test Process Metrics",2005,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=5989727.pdf&bkn=5988898&pdfType=chapter,no,undetermined,0
Defect Classification and Analysis,"Analyses of discovered defects and related information from quality assurance (QA) activities can help both developers and testers to detect and remove potential defects, and help other project personnel to improve the development process, to prevent injection of similar defects and to manage risk better by planning early for product support and services. We next discuss these topics, and illustrate them through several case studies analyzing defects from system testing for some IBM products, and web-related defects for www.seas.smu.edu, the official web site for the School of Engineering and Applied Science, Southern Methodist University (SMU/SEAS).",2005,http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=5989597.pdf&bkn=5988897&pdfType=chapter,no,undetermined,0
A Novel Adaptive Failure Detector for Distributed Systems,"Combining adaptive heartbeat mechanism with fuzzy grey prediction algorithm, a novel implementation of failure detector is presented. The main parts of the implementation are adaptive grey prediction layer and adaptive fuzzy rule-based classification layer. The former layer employs a GM(1,1) unified-dimensional new message model, only needs a small volume of sample data, to predict heartbeat arrival time dynamically. Then, the predict value and the message loss rate in specific period are act as input variations for the latter layer to decide failure/non-failure. Furthermore, algorithms of how to predict arrival time and how to construct adaptive fuzzy rule-based classification system are presented. Experimental results validate the availability of our failure detector in detail.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579596,no,undetermined,0
Finds in testing experiments for model evaluation,"To evaluate the fault location and the failure prediction models, simulation-based and codebased experiments were conducted to collect the required failure data. The PIE model was applied to simulate failures in the simulation-based experiment. Based on syntax and semantic level fault injections, a hybrid fault injection model is presented. To analyze the injected faults, the difficulty to inject (DTI) and difficulty to detect (DTD) are introduced and are measured from the programs used in the code-based experiment. Three interesting results were obtained from the experiments: 1) Failures simulated by the PIE model without consideration of the program and testing features are unreliably predicted; 2) There is no obvious correlation between the DTI and DTD parameters; 3) The DTD for syntax level faults changes in a different pattern to that for semantic level faults when the DTI increases. The results show that the parameters have a strong effect on the failures simulated, and the measurement of DTD is not strict.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076037,no,undetermined,0
UVSD: software for detection of color underwater features,"Underwater Video Spot Detector (UVSD) is a software package designed to analyze underwater video for continuous spatial measurements (path traveled, distance to the bottom, roughness of the surface etc.) Laser beams of known geometry are often used in underwater imagery to estimate the distance to the bottom. This estimation is based on the manual detection of laser spots which is labor intensive and time consuming so usually only a few frames can be processed this way. This allows for spatial measurements on single frames (distance to the bottom, size of objects on the sea-bottom), but not for the whole video transect. We propose algorithms and a software package implementing them for the semi-automatic detection of laser spots throughout a video which can significantly increase the effectiveness of spatial measurements. The algorithm for spot detection is based on the support vector machines approach to artificial intelligence. The user is only required to specify on certain frames the points he or she thinks are laser dots (to train an SVM model), and then this model is used by the program to detect the laser dots on the rest of the video. As a result the precise (precision is only limited by quality of the video) spatial scale is set up for every frame. This can be used to improve video mosaics of the sea-bottom. The temporal correlation between spot movements changes and their shape provides the information about sediment roughness. Simultaneous spot movements indicate changing distance to the bottom; while uncorrelated changes indicate small local bumps. UVSD can be applied to quickly identify and quantify seafloor habitat patches, help visualize habitats and benthic organisms within large-scale landscapes, and estimate transect length and area surveyed along video transects.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1640089,no,undetermined,0
Simulation of partial discharge propagation and location in Abetti winding based on structural data,"Power transformer monitoring as a reliable tool for maintaining purposes of this valuable asset of power systems has always comprised partial discharge offline measurements and online monitoring. The reason lies in non-destructive feature of PD monitoring. Partial discharge monitoring helps to detect incipient insulation faults and prevent insulation failure of power transformers. This paper introduces a software package developed based on structural data of power transformer and discusses the results of the simulation on Abetti winding, which might be considered as a basic layer winding. A hybrid model is used to model the transformer winding, which has been developed by first author. Firstly, winding is modeled by ladder network method to determine model parameters and then multi-conductor transmission line model is utilized to work out voltage and current vectors and study partial discharge propagation as well as its localization. Utilized method of modeling makes it possible to simulate a transformer winding over a frequency range from a few hundred kHz to a few tens of MHz. The results take advantage of accurate modeling method and provide a reasonable interpretation as to PD propagation and location studies",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1627196,no,undetermined,0
A Software Implementation of the Duval Triangle Method,"Monitoring and diagnosis of electrical equipment, in particular power transformers, has attracted considerable attention for many years. It is of great importance for the utilities to find the incipient faults in these transformers as early as possible. Dissolved gas analysis (DGA) is one of the most useful techniques to detect incipient faults in oil-filled power transformers. Various methods have been developed to interpret DGA results such as IEC ratio code, Rogers method and Duval triangle method. One of the most frequently used DGA methods is Duval triangular. It is a graphical method that allows one to follow the faults more easily and more precisely. In this paper a detailed implementation of Duval triangle method was presented for researchers and utilities interested in visualizing their own DGA results using a software program. The Java language is used for this software because of its growing importance in modern application development.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4570294,no,undetermined,0
An approach to validation of software architecture model,"Software architectures shift developers' focus from lines-of-code to coarser-grained architectural elements and their interconnection structure. However, the benefits of architecture description languages (ADLs) cannot be fully captured without an automated realization of software architecture designs because manually shifting from a model to its implementation is error-prone. We propose an integrated approach for automatically translating software architecture design models to an implementation and validating the translation as well as the implementation by exploring runtime verification technique and aspect-oriented programming. Specifically, system properties are not only verified against design models, but also verified during the execution of the generated implementation of software architecture design. A prototype tool, SAM Parser, is developed to demonstrate the approach on SAM (Software Architecture Model). In SAM Parser, all the realization and verification code can be automatically generated without human intervention. In this paper, we first brief describe the approach report on a case study conducted at an e-commerce scenario, an online shopping system to assess the benefits of automated realization of software architecture design and validation in a Web service domain.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607174,no,undetermined,0
Early Reliability Prediction: An Approach to Software Reliability Assessment in Open Software Adoption Stage,Conventional software reliability models are not adequate to assess the reliability of software system in which OSS (Open Source Software) adopted as a new feature add-on because OSS can be modified while the inside of COTS(Commercial Off-The-Shelf) products cannot be changed. This paper presents an approach to software reliability assessment of OSS adopted software system in the early stage. We identified the software factors that affect the reliability of software system when a large software system adopts OSS and assess software reliability using those factors. They are code modularity and code maintainability in software modules related with system requirements. We used them to calculate the initial fault rate with weight index (correlated value between requirement and module) which represents the degree of code modification. We apply the proposed initial fault rate to reliability model to assess software reliability in the early stage of a software life cycle. Early software reliability assessment in OSS adoption helps to make an effective development and testing strategies for improving the reliability of the whole system.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579834,no,undetermined,0
New condition monitoring techniques for reliably drive systems operation,"The dominant application of electronics today is to process information. The computer industry is the biggest user of semiconductor devices and consumer electronics. Due to the successful development of semiconductors, electronic system and controls have gained wide acceptance in power and computing technology and due to the continuous use of drive systems (rotating machines, controlling thyristors and associated electronic components) in industry and in power stations, and the need to keep such systems running reliably, the detection of defects and anomalies is of increasing importance, and on-line monitoring to detect any fault in these systems is now a strong possibility and certainly periodic monitoring of a drive systems in strategic situations. The principal aim of the paper is to use both software and hardware and develop a fault diagnosis knowledge-based system, which will analyze and manipulate the output obtained from sensors using a microcomputer for acquiring the plant condition data and subsequently interpreting them, data collected can be analyzed using suitable computer programs, and any trends can be identified and compared with the knowledge base. The probability of certain condition can then be diagnosed and compared, providing the necessary information on which subsequent decisions can be based and provide any necessary alarms to the operator. To achieve this objective, the simulation and experimental technique considered is to use sensors placed in the wedges closing the stator slots to sense the induced voltage. The induced voltage and for each fault is shown to have a unique voltage pattern, thus, the fault identification through voltage pattern recognition were the basic rules for the development of the knowledge base. The predicted results are verified by measurements on a model system in which known faults can be established.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4580397,no,undetermined,0
Aspect-oriented modularization of assertion crosscutting objects,"Assertion checking is a powerful tool to detect software faults during debugging, testing and maintenance. Although assertion documents the behavior of one component, it is hard to document relations and interactions among several objects since such assertion statements are spread across the modules. Therefore, we propose to modularize such assertion as an aspect in order to improve software maintainability. In this paper, taking Observer pattern as an example, we point out that some assertions tend to be crosscutting, and propose a modularization of such assertion with aspect-oriented language. We show a limitation of traditional assertion and effectiveness of assertion aspect through the case study, and discuss various situations to which assertion aspects are applicable.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607217,no,undetermined,0
An integrated solution for testing and analyzing Java applications in an industrial setting,"Testing a large-scale, real-life commercial software application is a very challenging task due to the constant changes in the software, the involvement of multiple programmers and testers, and a large amount of code. Integrating testing with development can help find program bugs at an earlier stage and hence reduce the overall cost. In this paper, we report our experience on how to apply eXVantage (a tool suite for code coverage testing, debugging, performance profiling, etc.) to a large, complex Java application at the implementation and unit testing phases in Avaya. Our results suggest that programmers and testers can benefit from using eXVantage to monitor the testing process, gain confidence on the quality of their software, detect bugs which are otherwise difficult to reveal, and identify performance bottlenecks in terms of which part of code is most frequently executed.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607197,no,undetermined,0
Ontology-based active requirements engineering framework,"Software-intensive systems are systems of systems that rely on complex interdependencies among themselves as well as with their operational environment to satisfy the required behavior. As we integrate such systems to create information infrastructures that are critical to the quality of our lives and the businesses they support, the need to effectively predict, control and evolve their behavior is ever increasing. To deal with their complexity, an important first step is to understand and model software-intensive systems, their environments and the interdependencies among them at different levels of abstractions from multiple dimensions. In this paper, we present an ontology-based active requirements engineering (Onto-ActRE) framework that adopts a mixed-initiative approach to elicit, represent and analyze the diversity of factors associated with software-intensive systems. The Onto-ActRE framework integrates various RE modeling techniques with complementary semantics in a unifying ontological engineering process. We also present examples from the practice of our framework with appropriate tool support that combines theoretical and practical aspects.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607186,no,undetermined,0
Identifying error proneness in path strata with genetic algorithms,"In earlier work we have demonstrated that GA can successfully identify error prone paths that have been weighted according to our weighting scheme. In this paper we investigate whether the depth of strata in the software affects the performance of the GA. Our experiments show that the GA performance changes throughout the paths. It performs better in the upper, less in the middle and best in the lower layer of the paths. Although various methods have been applied for detecting and reducing errors in software, little research has been done into partitioning a system into smaller, error prone domains for software quality assurance. To identify error proneness in software paths is important because by identifying them, they can be given priority in code inspections or testing. Our experiments observe to what extent the GA identifies errors seeded into paths using several error seeding strategies. We have compared our GA performance with random path selection.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607181,no,undetermined,0
"Simulation-based validation and defect localization for evolving, semi-formal requirements models","When requirements models are developed in an iterative and evolutionary way, requirements validation becomes a major problem. In order to detect and fix problems early, the specification should be validated as early as possible, and should also be revalidated after each evolutionary step. In this paper, we show how the ideas of continuous integration and automatic regression testing in the field of coding can be adapted for simulation-based, automatic revalidation of requirements models after each incremental step. While the basic idea is fairly obvious, we are confronted with a major obstacle: requirements models under development are incomplete and semi-formal most of the time, while classic simulation approaches require complete, formal models. We present how we can simulate incomplete, semi-formal models by interactively recording missing behavior or functionality. However, regression simulations must run automatically and do not permit interactivity. We therefore have developed a technique where the simulation engine automatically resorts to the interactively recorded behavior in those cases where it does not get enough information from the model during a regression simulation run. Finally, we demonstrate how the information gained from model evolution and regression simulation can be exploited for locating defects in the model.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607178,no,undetermined,0
Model checking class specifications for Web applications,This paper proposes an approach for verifying class specifications of Web applications using model checking. We first present a method to model a dynamic behavior of a Web application from a class specification. We next propose two methods to verify consistencies of the class specification and other design specifications: (1) a page flow diagram which is one of the most essential specifications for Web applications and (2) a behavior diagram such as a UML activity diagram. We applied the proposed methods to real specifications of a Web application designed by a certain company and found several faults of the specifications that had not been detected in actual reviews.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607138,no,undetermined,0
Reliability Improvement of Real-Time Embedded System Using Checkpointing,"The checkpointing problem in real-time embedded systems is dealt with from a reliability point of view. Transient faults are assumed to be detected in a non-concurrent manner (e.g., periodically). The probability of successful real-time task completion in the presence of transient faults is derived with the consideration of the effects of the transient faults that may occur during checkpointing or recovery operations. Based on this, an optimal equidistant checkpointing strategy that maximizes the probability of task completion is proposed.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579796,no,undetermined,0
Developing distributed applications rapidly and reliably using the TENA middleware,"The test and training enabling architecture (TENA) middleware is the result of a joint interoperability initiative of the Director, Operational Test and Evaluation (DOT&E) of the Office of the Secretary of Defense (OSD). The goals of the initiative are to enable interoperability among ranges, facilities, and simulations in a quick and cost-efficient manner, and to foster reuse of range assets and future range system developments. The TENA middleware uses Unified Modeling Language (UML)-based model-driven code generation to automatically create a complex Common Object Request Broker Architecture (CORBA) application. This model-driven automatic code-generation greatly reduces the amount of software that must be hand-written and tested. Furthermore, the TENA middleware combines distributed shared memory, anonymous publish-subscribe, and model-driven distributed object-oriented programming paradigms into a single distributed middleware system. This unique combination yields a powerful middleware system that enables its users to rapidly develop sophisticated yet understandable distributed applications. The TENA middleware offers powerful programming abstractions that are not present in CORBA alone and provides a strongly-typed application programmer interface (API) that is much less error-prone than the existing CORBA API. These high-level, easy-to-understand programming abstractions combined with an API designed to reduce programming errors enable users to quickly and correctly express the concepts of their applications. Re-usable standardized objects farther simplify the development of applications. The net result of this combination of features is a significant reduction of application programming errors yielding increased overall reliability and decreased overall development time. Distributed applications developed using the TENA middleware exchange data using the publish-subscribe paradigm. Although many publish-subscribe systems exist, the TENA middleware repr- - esents a significant advance in the field due to the many high-level, model-driven programming abstractions it presents to the programmer. The TENA middleware API relies heavily on compile-time type-safety to help ensure reliable behavior at runtime. Careful API design allows a great number of potential errors to be detected at compile-time that might otherwise go unnoticed until run-time - where the cost of an error could be extremely high! The implementation of the TENA middleware uses C++, as well as a real-time CORBA ORB. The TENA middleware is currently in use at dozens of Department of Defense (DoD) testing and training range facilities across the county and has been used to support major test and training events such as Joint Red Flag '05. The TENA Middleware is available at http://www.tena-sda.org/",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1605890,no,undetermined,0
Spare Line Borrowing Technique for Distributed Memory Cores in SoC,"In this paper, a new architecture of distributed embedded memory cores for SoC is proposed and an effective memory repair method by using the proposed spare line borrowing (software-driven reconfiguration) technique is investigated. It is known that faulty cells in memory core show spatial locality, also known as fault clustering. This physical phenomenon tends to occur more often as deep submicron technology advances due to defects that span multiple circuit elements and sophisticated circuit design. The combination of new architecture & repair method proposed in this paper ensures fault tolerance enhancement in SoC, especially in case of fault clustering. This fault tolerance enhancement is obtained through optimal redundancy utilization: spare redundancy in a fault-resistant memory core is used to fix the fault in a fault-prone memory core. The effect of spare line borrowing technique on the reliability of distributed memory cores is analyzed through modeling and extensive parametric simulation",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1604065,no,undetermined,0
A real-time computer vision system for detecting defects in textile fabrics,"This paper proposes a real-time computer vision system for detecting defects in textile fabrics. The developments of both the hardware and software platforms are presented. The design of the prototyped defect detection system ensures that the fabric moves smoothly and evenly so that high quality images can be captured. The paper also proposes a new filter selection method to detect fabric defects, which can automatically tune the Gabor functions to match with the texture information. The filter selection method is further developed into a new defect segmentation algorithm. The scheme is tested both on-line and off-line by using a variety of homogeneous textile images with different defects. The results exhibit accurate defect detection with low false alarm, thus confirming the robustness and effectiveness of the proposed system",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1600684,no,undetermined,0
New Protection Circuit for High-Speed Switching and Start-Up of a Practical Matrix Converter,"The matrix converter (MC) presents a promising topology that needs to overcome certain barriers (protection systems, durability, the development of converters for real applications, etc.) in order to gain a foothold in the market. Taking into consideration that the great majority of efforts are being oriented toward control algorithms and modulation, this paper focuses on MC hardware. In order to improve the switching speed of the MC and thus obtain signals with less harmonic distortion, several different insulated-gate bipolar transistor (IGBT) excitation circuits are being studied. Here, the appropriate topology is selected for the MC, and a recommended configuration is selected, which reduces the excursion range of the drivers, optimizes the switching speed of the IGBTs, and presents high immunity to common-mode voltages in the drivers. Inadequate driver control can lead to the destruction of the MC due to its low ride-through capability. Moreover, this converter is especially sensitive during start-up, as, at that moment, there are high overcurrents and overvoltages. With the aim of finding a solution for starting up the MC, a circuit is presented (separate from the control software), which ensures correct sequencing of supplies, thus avoiding a short circuit between input phases. Moreover, it detects overcurrent, connection/disconnection, and converter supply faults. Faults cause the circuit to protect the MC by switching off all the IGBT drivers without latency. All this operability is guaranteed even when the supply falls below the threshold specified by the manufacturers for the correct operation of the circuits. All these features are demonstrated with experimental results. Lastly, an analysis is made of the interaction that takes place during the start-up of the MC between the input filter, clamp circuit, and the converter. A variation of the clamp circuit and start-up strategy is presented, which minimizes the overcurrents that circulate through the conve- - rter. For all these reasons, it can be said that the techniques described in this paper substantially improve the MC start-up cycle, representing a step forward toward the development of reliable MCs for real applications.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4582435,no,undetermined,0
Design Phase Analysis of Software Reliability Using Aspect-Oriented Programming,"Software system may have various nonfunctional requirements such as reliability, security, performance and schedulability. If we can predict how well the system will meet such requirements at an early phase of software development, we can significantly save the total development cost and time. Among non-functional requirements, reliability is commonly required as the essential property of the system being developed. Therefore, many analysis methods have been proposed but methods that can be practically performed in the design phase are rare. In this paper we show how design-level aspects can be used to separate reliability concerns from essential functional concerns during software design. The aspect-oriented design technique described in this paper allows one to independently specify fault tolerance and essential functional concerns, and then weave the specifications to produce a design model that reflects both concerns. We illustrate our approach using an example.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598598,no,undetermined,0
Fault tolerant IPMS motor drive based on adaptive backstepping observer with unknown stator resistance,"This work considers the problem of designing a fault tolerant system for IPMS motor drive subject to current sensor fault. To achieve this goal, two control strategies are considered. The first is based on field oriented control and a developed adaptive backstepping observer which simultaneously are used in the case of fault-free. The second approach proposed is concerned with fault tolerant strategy based on observer for faulty conditions. Stator resistance as possible source of system uncertainty is taken into account under different operating conditions. Current sensors failures are detected and observer based on adaptive backstepping approach is used to estimate currents and stator resistance. The nonlinear observer stability study based on the Lyapunov theory guarantees the stability and convergence of the estimated quantities, if the appropriate adaptation laws are designed and persistency of excitation condition is satisfied. In our control approach, references of d-q axis currents are generated on the basis of maximum power factor per ampere control scheme related to IPMSM drive. The complete proposed scheme is simulated using MATLAB/Simulink software. Simulation is made to illustrate the proposed strategy.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4582827,no,undetermined,0
An approach to fault-tolerant mobile agent execution in distributed systems,"Mobile agents are no longer a theoretical issue since different architectures for their realization have been proposed. With the increasing market of electronic commerce it becomes an interesting aspect to use autonomous mobile agents for electronic business transactions. Being involved in money transactions, supplementary security features for mobile agent systems have to he ensured. Fault-tolerance is fundamental to the further development of mobile agent applications. In the context of mobile agents, fault-tolerance prevents a partial or complete loss of the agent, i.e., ensures that the agent arrives at its destination. Simple approaches such as checkpointing are prone to blocking. Replication can in principle improve solutions based on checkpointing. However, existing solutions in this context either assume a perfect failure detection mechanism (which is not realistic in an environment such as the Internet), or rely on complex solutions based on leader election and distributed transactions, where only a subset of solutions prevents blocking .This paper proposes a novel approach to fault-tolerant mobile agent execution, which is based on modeling agent execution as a sequence of agreement problems. Each agreement problem is one instance of the well-understood consensus problem. Our solution does not require a perfect failure detection mechanism, while preventing blocking and ensuring that the agent is executed exactly once.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598203,no,undetermined,0
Bayesian networks modeling for software inspection effectiveness,"Software inspection has been broadly accepted as a cost effective approach for defect removal during the whole software development lifecycle. To keep inspection under control, it is essential to measure its effectiveness. As human-oriented activity, inspection effectiveness is due to many uncertain factors that make such study a challenging task. Bayesian networks modeling is a powerful approach for the reasoning under uncertainty and it can describe inspection procedure well. With this framework, some extensions have been explored in this paper. The number of remaining defects in the software is proposed to be incorporated into the framework, with expectation to provide more information on the dynamic changing status of the software. In addition, a different approach is adopted to elicit the prior belief of related probability distributions for the network. Sensitivity analysis is developed with the model to locate the important factors to inspection effectiveness.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607500,no,undetermined,0
An Estimation Model of Vulnerability for Embedded Microprocessors,"Embedded systems, and also the embedded microprocessors, have encountered the reliability challenge because the occurring probability of soft errors has a rising trend. When they are applied to safety-critical applications, designs with the fault tolerant consideration are required. For the complicated embedded systems or IP-based system-on-chip (SoC), it is unpractical and not cost-effective to protect the entire system or SoC. Analyzing the vulnerability of systems can help designers not only invest limited resource on the most crucial region but also understand the gain derived from the investment. In this paper we propose a model to fast estimate the microprocessor's vulnerability with only slight simulation effort. From our assessment results, the rank of component vulnerability related to the probability of causing the microprocessor failure can be acquired. By choosing one of the mainstream microprocessors - VLIW (Very Long Instruction Word) processor - as an example, the practical usefulness of our estimation model is demonstrated.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579833,no,undetermined,0
Reliability prediction and assessment of fielded software based on multiple change-point models,"In this paper, we investigate some techniques for reliability prediction and assessment of fielded software. We first review how several existing software reliability growth models based on non-homogeneous Poisson processes (NHPPs) can be readily derived based on a unified theory for NHPP models. Furthermore, based on the unified theory, we can incorporate the concept of multiple change-points into software reliability modeling. Some models are proposed and discussed under both ideal and imperfect debugging conditions. A numerical example by using real software failure data is presented in detail and the result shows that the proposed models can provide fairly good capability to predict software operational reliability.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607540,no,undetermined,0
Metamaterials with multiband AMC and EBG properties,"Complex surfaces that perform as artificial magnetic conductor (AMC) and as electromagnetic band gap (EBG) structures in a selection of predefined frequency bands are presented. This is achieved by introducing defected (or perturbed) arrays. In this paper we demonstrate that the absence of vias, which eases the fabrication process, does not prevent the AMC and EBG operation to coincide in the frequency domain. Method of moments based software has been developed for the fast, accurate and simultaneous investigation of the multi-band AMC and EBG properties of such arrays. Furthermore, the angular stability of the multiband AMC surface has also been assessed. Experimental results that demonstrate a dual-band metamaterial surface with simultaneous AMC and EBG properties are presented.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608883,no,undetermined,0
Change Propagation for Assessing Design Quality of Software Architectures,"The study of software architectures is gaining importance due to its role in various aspects of software engineering such as product line engineering, component based software engineering and other emerging paradigms. With the increasing emphasis on design patterns, the traditional practice of ad-hoc software construction is slowly shifting towards pattern-oriented development. Various architectural attributes like error propagation, change propagation, and requirements propagation, provide a wealth of information about software architectures. In this paper, we show that change propagation probability (CP) is helpful and effective in assessing the design quality of software architectures. We study two different architectures (one that employs patterns versus one that does not) for the same application. We also analyze and compare change propagation metric with respect to other coupling-based metrics.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1620112,no,undetermined,0
Software Architecture Reliability Analysis Using Failure Scenarios,We propose a Software Architecture Reliability Analysis (SARA) approach that benefits from both reliability engineering and scenario-based software architecture analysis to provide an early reliability analysis of the software architecture. SARA makes use of failure scenarios that are prioritized with respect to the user-perception in order to provide a severity analysis for the software architecture and the individual components.,2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1620111,no,undetermined,0
Dynamic User Interface Adaptation for Mobile Computing Devices,"A large number of heterogeneous and mobile computing devices nowadays are employed by users to access services they have subscribed to. The work of application developers, which have to maintain several versions of user interface for a single application, is becoming more and more difficult, error-prone and time consuming. New software development models, able to easily adapt the application to the client execution context, have to be exploited. In this work we present a framework that allows developers to specify the user interaction with the application, in an independent manner with respect to the specific execution context, by using an XML-based language. Starting from such a specification, the system will subsequently """"render"""" the actual users application interface on a specific execution environment, adapting it to the used terminal characteristics.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1620001,no,undetermined,0
MagIC System: a New Textile-Based Wearable Device for Biological Signal Monitoring. Applicability in Daily Life and Clinical Setting,"The paper presents a new textile-based wearable system for the unobtrusive recording of cardiorespiratory and motion signals during spontaneous behavior along with the first results concerning the application of this device in daily life and in a clinical environment. The system, called MagIC (Maglietta Interattiva Computerizzata), is composed of a vest, including textile sensors for detecting ECG and respiratory activity, and a portable electronic board for motion detection, signal preprocessing and wireless data transmission to a remote monitoring station. The MagIC system has been tested in freely moving subjects at work, at home, while driving and cycling and in microgravity condition during a parabolic flight. Applicability of the system in cardiac in-patients is now under evaluation. Preliminary data derived from recordings performed on patients in bed and during physical exercise showed 1) good signal quality over most of the monitoring periods, 2) a correct identification of arrhythmic events, and 3) a correct estimation of the average beat-by-beat heart rate. These positive results supports further developments of the MagIC system, aimed at tuning this approach for a routine use in clinical practice and in daily life",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1616161,no,undetermined,0
Prostatectomy Evaluation using 3D Visualization and Quantitation,"Prostate cancer is a disease with a long natural history. Differences in survival outcomes as indicators of inappropriate surgery would take decades to appear. Therefore, the evaluation of the excised specimen according to defined parameters provides a more reasonable and timely assessment of surgical quality. There are currently a number of very different surgical approaches. Some uniform guidelines and quality assessment measuring readily available parameters would be desirable to establish a standard for comparison of surgical approaches and for individual surgical performance. In this paper, we present a novel methodology to objectively quantify the assessment process utilizing a 3D reconstructed model for the prostate gland. To this end, we discuss the development of a process employing image reconstruction and analysis techniques to assess the percent of capsule covered by soft tissue. A final goal is to develop software for the purpose of a quality assurance assessment for pathologists and surgeons to evaluate the adequacy/appropriateness of each surgical procedure; laparoscopic versus open perineal or retropubic prostatectomy. Results from applying this technique are presented and discussed",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1615637,no,undetermined,0
Work in Progress - Computer Software for Predicting Steadiness of the Students,"The paper presents a study which identifies a series of factors influencing students' steadiness in their option for engineering training and the final aim is to elaborate an IT system for monitoring the quality of educational offer. This aim is reached through a research developed in three stages. Only the first and the second stages were described here. The last one is in work. So, the first stage is materialized in elaborating and validating a questionnaire structured on three dimensions: finding the expectations, diagnosis of initial motivation for initiating students in engineering, specifying identity information and elements of personal history from educational student's experience. The sample is randomly chosen and the students from the research group belong to Technical University """"Gh. Asachi"""" Iassy, Romania, attending first, second and third year of study. The second stage of the scientific research establishes the relations between the identified expectations, initial motivation of students for engineering training and personal history in educational area on the one hand and students' educational performance on the other hand. Afterwards, the results of the first two stages represents the starting point for planning computer software to predict the steadiness of students in their professional choice",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1612274,no,undetermined,0
A Simulation Task to Assess Students  Design Process Skill,"Research has shown that the quality of one's design process is an important ingredient in expertise. Assessing design process skill typically requires a performance assessment in which students are observed (either directly or by videotape) completing a design and assessed using an objective scoring system. This is impractical in course-based assessment. As an alternative, we developed a computer-based simulation task, in which the student respondent """"watches"""" a team develop a design (in this instance a software design) and makes recommendations as to how they should proceed. The specific issues assessed by the simulation were drawn from the research literature. For each issue the student is asked to describe, in words, what the team should do next and then asked to choose among alternatives that the """"team"""" has generated. Thus, the task can be scored qualitatively and quantitatively. The paper describes the task and its uses in course-based assessment",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1612150,no,undetermined,0
Managing a project course using Extreme Programming,"Shippensburg University offers an upper division project course in which the students use a variant of Extreme Programming (XP) including: the Planning Game, the Iteration Planning Game, test driven development, stand-up meetings and pair programming. We start the course with two weeks of controlled lab exercises designed to teach the students about test driven development in JUnit/Eclipse and designing for testability (with the humble dialog box design pattern) while practicing pair programming. The rest of our semester is spent in three four-week iterations developing a product for a customer. Our teams are generally large (14-16 students) so that the projects can be large enough to motivate the use of configuration management and defect tracking tools. The requirement of pair programming limits the amount of project work the students can do outside of class, so class time is spent on the projects and teaching is on-demand individual mentoring with lectures/labs inserted as necessary. One significant challenge in managing this course is tracking individual responsibilities and activities to ensure that all of the students are fully engaged in the project. To accomplish this, we have modified the story and task cards from XP to provide feedback to the students and track individual performance against goals as part of the students' grades. The resulting course has been well received by the students. This paper will describe this course in more detail and assess its effect on students' software engineering background through students' feedback and code metrics",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611948,no,undetermined,0
Automatic detection of local and global software failures,"The problem of automatic detection of failures of reactive, session-oriented software programs is described. Detection of failures is carried out by a separate unit, which observes the inputs and outputs of the target program and reports the failures detected.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611189,no,undetermined,0
A Model of Bug Dynamics for Open Source Software,We present a model to describe open source software (OSS) bug dynamics. We validated the model using real world data and performed simulation experiments. The results show that the model has the ability to predict bug occurrences and failure rates. The results also reveal that there exists an optimal release cycle for effectively managing OSS quality.,2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579816,no,undetermined,0
Enhancing Internet robustness against malicious flows using active queue management,"Attackers can easily modify the TCP control protocols of host computers to inject the malicious flows to the Internet. Including DDoS and worm attack flows, these malicious flows are unresponsive to the congestion control mechanism which is necessary to the equilibrium of the whole Internet. In this paper, a new scheme against the large scale malicious flows is proposed based on the principles of TCP congestion control. The kernel is to implement a new scheduling algorithm named as CCU (compare and control unresponsive flows) which is one sort of active queue management (AQM). According to the unresponsive characteristic of malicious flows, CCU algorithm relies on the two processes of malicious flows - detection and punishment. The elastics control mechanism of unresponsive flows benefits the AQM with the high performance and enhances the Internet robustness against malicious flows. The network resource can be regulated for the basic quality of service (QoS) demands of legal users. The experiments prove that CCU can detect and restrain responsive flows more accurately compared to other AQM algorithms.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609918,no,undetermined,0
Evaluating Web applications testability by combining metrics and analogies,"This paper introduces an approach to describe a Web application through an object-oriented model and to study application testability using a quality model focused on the use of object-oriented metrics and software analogies analysis. The proposed approach uses traditional Web and object-oriented metrics to describe structural properties of Web applications and to analyze them. These metrics are useful to measure some important software attributes, such as complexity, coupling, size, cohesion, reliability, defects density, and so on. Furthermore, the presented quality model uses these object-oriented metrics to describe applications in order to predict some software quality factors (such as test effort, reliability, error proneness, and so on) through an instance-based classification system. The approach uses a classification system to study software analogies and to define a set of information then used as the basis for applications quality factors prediction and evaluation. The presented approaches are applied into the WAAT (Web Applications Analysis and Testing) project",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609664,no,undetermined,0
On effectiveness of pairwise methodology for testing network-centric software,"Pairwise testing, which can be complemented with partial or full N-wise testing, is a technique which guarantees that all important parametric value pairs are included in a test suite. A percentage of N-wise testing is also included. We conjecture that N-wise enhanced pairwise testing can be used as a black-boxed testing method to increase effectiveness of random testing in exposing unusual or unexpected behaviors, such as security failures in network-centric software. This testing can also be quite cost-efficient since small N test suites grow linearly with the number of parameters. This paper explains the results of random testing of a simulation in which about 20% of the defects with probabilities of occurrence less than 50% are never exposed. This supports the premise that if the unusual or unexpected behaviors are based on defects which are less likely to occur, then random testing needs to be enhanced, especially if those unexposed defects could cause erratic or even critical behaviors to the system. Higher system complexities may indicate higher numbers of unusual or unexpected behaviors. It may be difficult to use the traditional operational profile information to determine the amount of testing for unusual behaviors since the operational usage may be 0 or close to it. Another interesting problem is that some testers lack the experience necessary to effectively analyze the results of a test run. It is important to compensate for the lack of experience so that novice testers are able to test comparatively as effectively as more experienced testers. It is believed that if the size of the test suite is relatively small, then it may be easier to pinpoint the source of a failure. The research presented in this paper is aimed at addressing some of these issues of random testing via enhanced pairwise testing and N-wise testing in general. It is possible that more complex systems, such as those that rely a great deal on a network, would require higher numbers of int- - eractions to combat unexpected combinations for use in some testing instances such as security testing or high assurance testing. A tool is being developed concurrently to help automate a part of the test generation process",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609626,no,undetermined,0
Translating Atlas 416 to Fortran77 using CMMI processes,"Due to the increasing costs in software engineering and support of ATE, several processes have emerged to enable software improvement and incorporate an error or defect predicting component into software development. Chief among them is the DOD sponsored CMMI (capability maturity model integration). ATE development and support is an excellent arena to test new process systems and models because of its diversity and the increasing trend of use ATE support engineers as integrators. During 2004 and 2005 WRALC/MASTF incorporated CMMI into an ATLAS to FORTRAN77 rehost project and had great success. This paper will demonstrate that by using CMMI processes, a customer will receive a superior, repeatable and more reliable software product, as well as superior system support for the future. In addition, training of new personnel are repeatable, thus resulting in more competent, and long-term organization memory.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609201,no,undetermined,0
A New Method for Measuring Single Event Effect Susceptibility of L1 Cache Unit,"Cache SEE susceptibility measurements are required for predicting processorpsilas soft error rate in space missions. Previous dynamic or static real beam test based approaches are only tenable for processors which have optional cache operating modes such as disable(bypass)/enable, frozen, etc. As L1 cache are indispensable to the processorpsilas total performance, some newly introduced processors no longer have such cache management schemes, thus make the existed methods inapplicable. We propose a novel way to determine cache SEE susceptibility for any kind of processors, whether cache bypass mode supported or not, by combining heavy ion dynamic testing with software implemented fault injection approaches.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579825,no,undetermined,0
Rapid Deployment of SOA Solutions via Automated Image Replication and Reconfiguration,"Deployment is an important aspect of software solutions' life-cycle and is repeatedly employed at many stages including development, testing, delivery, and demonstration. Traditional script-based approaches for deployment are primarily manual and hence error prone, resulting in wasted time and labor. In this paper we propose a framework and approach for faster redeployment of distributed software solutions. In our approach the solution is first deployed on virtual machines using traditional methods. Then environment dependent configurations of the solution are discovered and preserved along with the images of virtual machines. For subsequent deployments, the preserved images are provisioned, and the deployer is provided an opportunity to change a subset of the recorded configurations that cannot be automatically derived e.g. IP addresses, ports. The remaining recorded configurations are derived by executing meta-model level constraints on the solution configuration model. Finally, the virtual machines are updated with new configurations by leveraging the semantics of appropriate scripts. Our framework allows product experts to describe the configuration meta-model, constraints, and script semantics. This product knowledge is specified only once, and is reused across solutions for automatic configuration discovery and re-configuration. We demonstrate with case studies that our approach reduces the time for repeated deployments of a solution from an order of weeks to an order of hours.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578460,no,undetermined,0
A new insight into postsurgical objective voice quality evaluation: application to thyroplastic medialization,"This paper aims at providing new objective parameters and plots, easily understandable and usable by clinicians and logopaedicians, in order to assess voice quality recovering after vocal fold surgery. The proposed software tool performs presurgical and postsurgical comparison of main voice characteristics (fundamental frequency, noise, formants) by means of robust analysis tools, specifically devoted to deal with highly degraded speech signals as those under study. Specifically, we address the problem of quantifying voice quality, before and after medialization thyroplasty, for patients affected by glottis incompetence. Functional evaluation after thyroplastic medialization is commonly based on several approaches: videolaryngostroboscopy (VLS), for morphological aspects evaluation, GRBAS scale and Voice Handicap Index (VHI), relative to perceptive and subjective voice analysis respectively, and Multi-Dimensional Voice Program (MDVP), that provides objective acoustic parameters. While GRBAS has the drawback to entirely rely on perceptive evaluation of trained professionals, MDVP often fails in performing analysis of highly degraded signals, thus preventing from presurgical/postsurgical comparison in such cases. On the contrary, the new tool, being capable to deal with severely corrupted signals, always allows a complete objective analysis. The new parameters are compared to scores obtained with the GRBAS scale and to some MDVP parameters, suitably modified, showing good correlation with them. Hence, the new tool could successfully replace or integrate existing ones. With the proposed approach, deeper insight into voice recovering and its possible changes after surgery can thus be obtained and easily evaluated by the clinician.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1597494,no,undetermined,0
Modeling Business Process Availability,"In the world where on-demand and trustworthy service delivery is one of the main preconditions for successful business, availability of the services and business processes is of the paramount importance and cannot be compromised. We present a framework for modeling business process availability that takes into account services, the underlying ICT-infrastructure and people. Based on a fault model, we develop the methodology to map dependencies between ICT-components, services and business processes. The mapping enables us to model and analytically assess steady-state, interval and user perceived availability at all levels, up to the level of the business process.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578342,no,undetermined,0
Automatic Instruction-Level Software-Only Recovery,"As chip densities and clock rates increase, processors are becoming more susceptible to transient faults that can affect program correctness. Computer architects have typically addressed reliability issues by adding redundant hardware, but these techniques are often too expensive to be used widely. Software-only reliability techniques have shown promise in their ability to protect against soft-errors without any hardware overhead. However, existing low-level software-only fault tolerance techniques have only addressed the problem of detecting faults, leaving recovery largely unaddressed. In this paper, we present the concept, implementation, and evaluation of automatic, instruction-level, software-only recovery techniques, as well as various specific techniques representing different trade-offs between reliability and performance. Our evaluation shows that these techniques fulfill the promises of instruction-level, software-only fault tolerance by offering a wide range of flexible recovery options",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1633498,no,undetermined,0
A probabilistic approach for fault tolerant multiprocessor real-time scheduling,"In this paper we tackle the problem of scheduling a periodic real time system on identical multiprocessor platforms, moreover the tasks considered may fail with a given probability. For each task we compute its duplication rate in order to (1) given a maximum tolerated probability of failure, minimize the size of the platform such at least one replica of each job meets its deadline (and does not fail) using a variant of EDF namely EDF<sup>(k)</sup> or (2) given the size of the platform, achieve the best possible reliability with the same constraints. Thanks to our probabilistic approach, no assumption is made on the number of failures which can occur. We propose several approaches to duplicate tasks and we show that we are able to find solutions always very close to the optimal one",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639409,no,undetermined,0
Industry-oriented software-based system for quality evaluation of vehicle audio environments,"A new set of integrated software tools are proposed for the evaluation of vehicle audio quality for industrial purposes, taking advantage of the auralization approach that allows to simulate the binaural listening experience outside the cockpit. Two main cooperating tools are implemented. The first fulfills the function of acquiring relevant data for system modeling and for canceling the undesired effects of the acquisition chain. The second offers a user-friendly interface for real-time simulation of different car audio systems and the consequent evaluation of both objective and subjective performances. In the latter case, the listening procedure is directly experienced at the PC workplace, leading to a significant simplification of the audio-quality assessing task for comparing the selected systems. Moreover, such kind of subjective evaluation allowed to validate the proposed approach through a complete set of experiments (developed by means of a dedicated software environment) based on appropriate ITU recommendations.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1637827,no,undetermined,0
Dependable SoPC-Based On-board Ice Protection System: From Research Project to Implementation,"Both dependability of computer on-board systems (CBS), and the size and the weight limitations are very important characteristics. Basic aviation and aerospace CBSs requirements and some development principles are considered. The multi-version lifecycle of FPGA-based CBS as system-on-programmable-chip (SoPC) is described. The several dependable SoPC architectures are researched and assessed: one-version two-channel, two-version two-channel and two-version four-channel systems. The method of the architectural adaptation is considered as the means for physical and design faults tolerating. Itpsilas based on composition of a few versions embedded to the chip. The checking and reconfiguration block as intellectual property core and elements ofice protection system development and implementation process are given as the practical example of application of proposed technique.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4573050,no,undetermined,0
Smart laser vision sensors simplify inspection,"For all in-process and finished product applications, laser sensors are used in the rubber and tire industry to enhance competitiveness by improving productivity. The basic benefits of using laser sensors for quality control include increasing yield and productivity, increasing quality by providing 100% product inspection, reducing scrap production and rejects, and in-process inspection to detect and correct trends quickly before production of scrap. New developments in laser-based measuring systems can now provide high-speed digital data communications, eliminating the effects of errors from electrical noise and eliminating the need for A/D converters. New smart sensor developments allow application specific analysis software to run inside the sensor, simplifying operation, improving reliability, and reducing cost by eliminating the need for external signal processing hardware.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634956,no,undetermined,0
Content browsing and semantic context viewing through JPEG 2000-based scalable video summary,"The paper presents a novel method and software platform for remote and interactive browsing of a summary of long video sequences as well as revealing the semantic links between shots and scenes in their temporal context. The solution is based on interactive navigation in a scalable mega image resulting from a JPEG 2000 coded key-frame-based video summary. Each key-frame could represent an automatically detected shot, event or scene, which is then properly annotated using some semi-automatic tools or learning methods. The presented system is compliant with the new JPEG 2000 Part 9 'JPIP - JPEG 2000 interactivity, API and protocols', which lends itself to working under varying transmission channel conditions such as GPRS or 3G wireless networks. While keeping the advantages of a single 2D video summary, like the limited storage cost, the flexibility offered by JPEG 2000 allows the application to highlight interactively key-frames corresponding to the desired content first within a low-quality and low-resolution version of the full video summary. It then offers fine grain scalability for a user to navigate and zoom into particular scenes or events represented by the key-frames. This possibility of visualising key-frames of interest and playing back the corresponding video shots within the context of the whole sequence (e.g. an episode of a media file) enables the user to understand the temporal relations between semantically related events/actions/physical settings, providing a new way to present and search for contents in video sequences.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1633694,no,undetermined,0
Tool-Supported Advanced Mutation Approach for Verification of C# Programs,"Mutation testing is a fault-based testing technique used to inject faults into an existing program and see if its test suite is sensitive enough to detect common faults. We are interested in using the mutation analysis to evaluate, compare and improve quality assurance techniques for testing object-oriented mechanisms and other advanced features of C# programs. This paper provides an overview of a current version of the CREAM system (creator of mutants), and reports on its use in experimental research. We apply advanced, object-oriented mutation operators to testing of open-source C# programs and discuss the results.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4573065,no,undetermined,0
BlueGene/L Failure Analysis and Prediction Models,"The growing computational and storage needs of several scientific applications mandate the deployment of extreme-scale parallel machines, such as IBM's BlueGene/L which can accommodate as many as 128 K processors. One of the challenges when designing and deploying these systems in a production setting is the need to take failure occurrences, whether it be in the hardware or in the software, into account. Earlier work has shown that conventional runtime fault-tolerant techniques such as periodic checkpointing are not effective to the emerging systems. Instead, the ability to predict failure occurrences can help develop more effective checkpointing strategies. Failure prediction has long been regarded as a challenging research problem, mainly due to the lack of realistic failure data from actual production systems. In this study, we have collected RAS event logs from BlueGene/L over a period of more than 100 days. We have investigated the characteristics of fatal failure events, as well as the correlation between fatal events and non-fatal events. Based on the observations, we have developed three simple yet effective failure prediction methods, which can predict around 80% of the memory and network failures, and 47% of the application I/O failures",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1633531,no,undetermined,0
QoS assessment via stochastic analysis,"Using a stochastic modeling approach based on the Unified Modeling Language and enriched with annotations that conform to the UML profile for schedulability performance, and time, the authors propose a method for assessing quality of service (QoS) in fault-tolerant (FT) distributed systems. From the UML system specification, they produce a generalized stochastic Petri net (GSPN) performance model for assessing an FT application's QoS via stochastic analysis. The ArgoSPE tool provides support for the proposed technique, helping to automatically produce the GSPN model",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631976,no,undetermined,0
A Service Oriented Approach to Traffic Dependent Navigation Systems,"Navigation systems play an important role in planning of routes for transportation and individual traffic. The calculated routes are not always optimal, because routing relies on static speeds for different road types and on outdated and error prone traffic message channel (TMC) data which do not reflect current traffic situations. This leads to the effect, that vehicles are directed to """"preferred"""" routes for at least some time resulting in traffic jam on these routes. In order to overcome the suboptimal solution, floating car data (FCD) reflecting the real time traffic has to be integrated in the calculation. In this paper we present an architecture for real time traffic dependent navigation systems that is based on a service oriented approach. This architecture considers mediation of real time traffic input, data preprocessing ensuring correct and reliable traffic data, and service provisioning. We discuss the challenges in each of these areas.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4578336,no,undetermined,0
Fault evaluation for security-critical communication devices,"Communications devices for government or military applications must keep data secure, even when their electronic components fail. Combining information flow and risk analyses could make fault-mode evaluations for such devices more efficient and cost-effective. Conducting high-grade information security evaluations for computer communications devices is intellectually challenging, time-consuming, costly, and error prone. We believe that our structured approach can reveal potential fault modes because it simplifies evaluating a device's logical design and physical construction. By combining information-flow and risk-analysis techniques, evaluators can use the process to produce a thorough and transparent security argument. In other work, we have applied static analysis techniques to the evaluation problem, treating a device's schematic circuitry diagram as an information flow graph. This work shows how to trace information flow in different operating modes by representing connectivity between components as being conditional on specific device states. We have also developed a way to define the security-critical region of components with particular security significance by identifying components that lie on a path from a high-security data source to a low-security sink. Finally, to make these concepts practical, we have implemented them in an interactive analysis tool that reads schematic diagrams written in the very high speed integrated circuit (VHSIC) hardware description language.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631944,no,undetermined,0
Towards a Client Driven Characterization of Class Hierarchies,"Object-oriented legacy systems are hard to maintain because they are hard to understand. One of the main understanding problems is revealed by the so-called """"yo-yo effect"""" that appears when a developer or maintainer wants to track a polymorphic method call. At least part of this understanding problem is due to the dual nature of the inheritance relation i.e., the fact that it can he used both as a code and/or as an interface reuse mechanism. Unfortunately, in order to find out the original intention for a particular hierarchy it is not enough to look at the hierarchy itself; rather than that, an in-depth analysis of the hierarchy's clients is required. In this paper we introduce a new metrics-based approach that helps us characterize the extent to which a base class was intended for interface reuse, by analyzing how clients use the interface of that base class. The idea of the approach is to quantify the extent to which clients treat uniformly the instances of the descendants of the base class, when invoking methods belonging to this common interface, We have evaluated our approach on two medium-sized case studies and we have found that the approach does indeed help to characterize the nature of a base class with respect to interface reuse. Additionally, the approach can be used to detect some interesting patterns in the way clients actually use the descendants through the interface of the base class",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631136,no,undetermined,0
Identification of Design Roles for the Assessment of Design Quality in Enterprise Applications,"The software industry is increasingly confronted with the issues of understanding and maintaining a special type of object-oriented systems, namely enterprise applications (EA). In the recent years many specific rules and patterns for the design of such applications were proposed. These new specific principles of EA design define precise roles (patterns) for classes and methods, and then describe """"good-design"""" rules in terms of such roles. Yet, these roles are rarely explicitly documented; therefore, due to their importance for an efficient understanding and assessment of EA design, they must be identified and localized in the source code based on their specificities. In this paper we define a suite of techniques for the identification and location of four such roles, all related to the data source layer of an EA. Using the knowledge about these roles we show how this can improve the accuracy of formerly defined techniques for detecting two well-known design problems (i.e., data class and feature envy), making them more applicable for the usage on enterprise systems. Based on an experimental study conducted on three EAs, we prove the feasibility of the approach, discuss its benefits and touch the issues that need to be addressed in the future",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631119,no,undetermined,0
A Metric-Based Heuristic Framework to Detect Object-Oriented Design Flaws,"One of the important activities in re-engineering process is detecting design flaws. Such design flaws prevent an efficient maintenance, and further development of a system. This research proposes a novel metric-based heuristic framework to detect and locate object-oriented design flaws from the source code. It is accomplished by evaluating design quality of an object-oriented system through quantifying deviations from good design heuristics and principles. While design flaws can occur at any level, the proposed approach assesses the design quality of internal and external structure of a system at the class level which is the most fundamental level of a system. In a nutshell, design flaws are detected and located systematically in two phases using a generic OO design knowledge-base. In the first phase, hotspots are detected by primitive classifiers via measuring metrics indicating a design feature (e.g. complexity). In the second phase, individual design flaws are detected by composite classifiers using a proper set of metrics. We have chosen JBoss application server as the case study, due to its pure OO large size structure, and its success as an open source J2EE platform among developers",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631118,no,undetermined,0
Leveraged Quality Assessment using Information Retrieval Techniques,"The goal of this research is to apply language processing techniques to extend human judgment into situations where obtaining direct human judgment is impractical due to the volume of information that must be considered. On aspect of this is leveraged quality assessments, which can be used to evaluate third-party coded subsystems, to track quality across the versions of a program, to assess the compression effort (and subsequent cost) required to make a change, and to identify parts of a program in need of preventative maintenance. A description of the QALP tool, its output from just under two million lines of code, and an experiment aimed at evaluating the tool's use in leveraged quality assessment are presented. Statistically significant results from this experiment validate the use of the QALP tool in human leverage quality assessment",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631117,no,undetermined,0
How Developers Copy,"Copy-paste programming is dangerous as it may lead to hidden dependencies between different parts of the system. Modifying clones is not always straight forward, because we might not know all the places that need modification. This is even more of a problem when several developers need to know about how to change the clones. In this paper, we correlate the code clones with the time of the modification and with the developer that performed the modification to detect patterns of how developers copy from one another. We develop visualization, named clone evolution view, to represent the evolution of the duplicated code. We show the relevance of our approach on several large case studies and we distill our experience in forms of interesting copy patterns",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631105,no,undetermined,0
The hidden cost of mismanaging calculations [engineering calculations engineering computing],"Engineering calculations programmed into custom software may execute efficiently, but tend to be hard to use. Thus, they are virtually useless for managing engineering information. Spreadsheets are a big part of the problem. They are more about crunching numbers than documenting context, so they can be a risky tool for managing calculations. Spreadsheets show answers but omit context and are error prone. They are unsuited to the task of modelling, analysing and documenting engineering designs. An electronic calculation 'worksheet' is a good solution for effectively documenting design and engineering processes. Unlike spreadsheets, they employ real mathematical notation and capture - in human-readable text - the assumptions, methods and critical data behind every calculation. They may also include illustrative graphs, annotations and sketches - in essence, knowledge captured in a shareable form. Organisations can build on the value of these worksheets by organising, tracking, and controlling and sharing them in a Web-based repository. Calculations can be retrieved any time for reuse, validation, refinement, reporting and publishing - all in their proper context.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631006,no,undetermined,0
A configurable framework for stream programming exploration in baseband applications,"This paper presents a configurable framework to be used for rapid prototyping of stream based languages. The framework is based on a set of design patterns defining the elementary structure of a domain specific language for high-performance signal processing. A stream language prototype for baseband processing has been implemented using the framework. We introduce language constructs to efficiently handle dynamic reconfiguration of distributed processing parameters. It is also demonstrated how new language specific primitive data types and operators can be used to efficiently and machine independently express computations on bitfields and data-parallel vectors. These types and operators yield code that is readable, compact and amenable to a stricter type checking than is common practice. They make it possible for a programmer to explicitly express parallelism to be exploited by a compiler. In short, they provide a programming style that is less error prone and has the potential to lead to more efficient implementations",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639502,no,undetermined,0
Analysis of checksum-based execution schemes for pipelined processors,"The performance requirements for contemporary microprocessors are increasing as rapidly as their number of applications grows. By accelerating the clock, performance can be gained easily but only with high additional power consumption. The electrical potential between logic `0' and `1' is decreased as integration and clock rates grow, leading to a higher susceptibility for transient faults, caused e.g. by power fluctuations or single event upsets (SEUs). We introduce a technique which is based on the well-known cyclic redundancy check codes (CRCs) to secure the pipelined execution of common microprocessors against transient faults. This is done by computing signatures over the control signals of each pipeline stage including dynamic out-of-order scheduling. To correctly compute the checksums, we resolve the time-dependency of instructions in the pipeline. We first discuss important physical properties of single event upsets (SEUs). Then we present a model of a simple processor with the applied scheme as an example. The scheme is extended to support n-way simultaneous multithreaded systems, resulting in two basic schemes. A cost analysis of the proposed SEU-detection schemes leads to the conclusion that both schemes are applicable at reasonable costs for pipelines with 5 to 10 stages and maximal 4 hardware threads. A worst-case simulation using software fault-injection of transient faults in the processor model showed that errors can be detected with an average of 83% even at a fault rate of 10<sup>-2</sup>. Furthermore, the scheme is able to detect an error within an average of only 5.05 cycles",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1639664,no,undetermined,0
An Approach to Evaluation of Arguments in Trust Cases,"Trustworthiness of IT systems can be justified using the concept of a trust case. A trust case is an argument structure which encompasses justification and evidence supporting claimed properties of a system. It represents explicitly an expert's way of assessing that a certain object has certain properties. Trust cases can be developed collaboratively on the basis of evidence and justification of varying quality. They can be complex structures impossible to comprehend fully by a non-expert. A postulated model of communicating trust case contents to an 'ordinary' user is an expert acting on user's behalf and communicating his/her assessment to the user. Therefore, a mechanism for issuing and aggregating experts' assessments is required. The paper proposes such a mechanism which enables assessors to appraise strength of arguments included in a trust case. The mechanism uses Dempster-Shafer's model of beliefs to deal with uncertainty resulting from the lack of knowledge of the expert. Different types of argumentation strategies were identified and for each of them appropriate combination rules were presented.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4573046,no,undetermined,0
Bilayer Segmentation of Live Video,"This paper presents an algorithm capable of real-time separation of foreground from background in monocular video sequences. Automatic segmentation of layers from colour/contrast or from motion alone is known to be error-prone. Here motion, colour and contrast cues are probabilistically fused together with spatial and temporal priors to infer layers accurately and efficiently. Central to our algorithm is the fact that pixel velocities are not needed, thus removing the need for optical flow estimation, with its tendency to error and computational expense. Instead, an efficient motion vs nonmotion classifier is trained to operate directly and jointly on intensity-change and contrast. Its output is then fused with colour information. The prior on segmentation is represented by a second order, temporal, Hidden Markov Model, together with a spatial MRF favouring coherence except where contrast is high. Finally, accurate layer segmentation and explicit occlusion detection are efficiently achieved by binary graph cut. The segmentation accuracy of the proposed algorithm is quantitatively evaluated with respect to existing groundtruth data and found to be comparable to the accuracy of a state of the art stereo segmentation algorithm. Foreground/ background segmentation is demonstrated in the application of live background substitution and shown to generate convincingly good quality composite video.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1640741,no,undetermined,0
PalProtect: A Collaborative Security Approach to Comment Spam,"Collaborative security is a promising solution to many types of security problems. Organizations and individuals often have a limited amount of resources to detect and respond to the threat of automated attacks. Enabling them to take advantage of the resources of their peers by sharing information related to such threats is a major step towards automating defense systems. In particular, comment spam posted on blogs as a way for attackers to do search engine optimization (SEO) is a major annoyance. Many measures have been proposed to thwart such spam, but all such measures are currently enacted and operate within one administrative domain. We propose and implement a system for cross-domain information sharing to improve the quality and speed of defense against such spam",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652092,no,undetermined,0
Distributed dynamic event tree generation for reliability and risk assessment,"Level 2 probabilistic risk assessments of nuclear plants (analysis of radionuclide release from containment) may require hundreds of runs of severe accident analysis codes such as MELCOR or RELAP/SCDAP to analyze possible sequences of events (scenarios) that may follow given initiating events. With the advances in computer architectures and ubiquitous networking, it is now possible to utilize multiple computing and storage resources for such computational experiments. This paper presents a system software infrastructure that supports execution and analysis of multiple dynamic event-tree simulations on distributed environments. The infrastructure allow for 1) the testing of event tree completeness, and, 2) the assessment and propagation of uncertainty on the plant state in the quantification of event trees",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652056,no,undetermined,0
System-Level Performance Estimation for Application-Specific MPSoC Interconnect Synthesis,"We present a framework for development of streaming applications as concurrent software modules running on multi-processors system-on-chips (MPSoC). We propose an iterative design space exploration mechanism to customize MPSoC architecture for given applications. Central to the exploration engine is our system-level performance estimation methodology, that both quickly and accurately determine quality of candidate architectures. We implemented a number of streaming applications on candidate architectures that were emulated on an FPGA. Hardware measurements show that our system-level performance estimation method incurs only 15% error in predicting application throughput. More importantly, it always correctly guides design space exploration by achieving 100% fidelity in quality-ranking candidate architectures. Compared to behavioral simulation of compiled code, our system-level estimator runs more than 12 times faster, and requires 7 times less memory.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4570792,no,undetermined,0
A Classification Scheme for Evaluating Management Instrumentation in Distributed Middleware Infrastructure,"Management instrumentation is an integrated capability of a software system that enables an external observer to monitor the system's availability, performance, and reliability during operation. It is highly useful for taking both proactive and reactive actions to keep a software system operational in mission-critical environments where tolerance for an unavailable or poor-performing system is very low. Middleware infrastructure components have taken important positions in distributed software systems due to various benefits related to the development, deployment, and runtime operations. Keeping these components highly available and up to the expected performance requires integrated capabilities that allow regular monitoring of critical functionality, measurement of Quality of Service (QoS), debugging and troubleshooting, and health-checks in the context of actual business processes.. Yet, currently there is no approach that enables systematic evaluation of the relative strengths and weaknesses of a middleware component's management instrumentation. In this paper, we will present an approach to evaluating management instrumentation of middleware infrastructure components. We use a classification-based scheme that has a functional dimension called Capability and two main quality dimensions called Usability and Precision. We further categorize each dimension into smaller, more precise instrumentation features, such as Tracing, Distributed Correlation and Granularity. In presenting our approach, we hope to achieve the following: i) educate middleware users on how to systematically assess or compare the overall manageability of a MidIn component using the classification scheme, and ii) share with middleware researchers on the importance of good integrated manageability in middleware infrastructure.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1651279,no,undetermined,0
Gate Layout Improvement Aimed at Testability,In the presented paper the improvement of the layout of complex standard gates from the industrial cell library aimed at decreasing the probability of occurrence of undetectable faults is considered. Such improvement allows us to determine the defect coverage table correctly and as a result to estimate properly the optimal sequence of input test pattern for defects detection. The ability of gate layout improvement is based on the results of defects probabilities determination and identification of functional faults caused by these defects. The results are obtained by FIESTA-Extra software tool,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1650980,no,undetermined,0
Productivity and code quality improvement of mixed-signal test software by applying software engineering methods,Typical nowadays mixed-signal ICs are approaching 1000 or even more parametric tests. These tests are usually coded in a procedural or a semi-object oriented language. The huge code base of the programs is a significant challenge for maintaining code quality which inherently translates into outgoing quality. The paper presents software metrics of typical mixed-signal power management and audio devices with regard to the number of tests conducted. It is shown that classical ways to handle test programs are error prone and tend to systematically repeat known mistakes. The adoption of selected software engineering methods can avoid such mistakes and improves the productivity of the mixed-signal test generation. Results of a pilot project show significant productivity improvement. Open-source based software is employed to provide the necessary tool support. They establish a potential roadmap to get away from proprietary tester specific tool sets,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1649593,no,undetermined,0
Service Plans for Context- and QoS-aware Dynamic Middleware,"State-of-the-art context- and QoS-aware dynamic middleware platforms use information about the environment, in order to evaluate alternative configurations of an application and select the one that best meets the users QoS requirements. The specification of the alternatives is prepared at designtime and associated with the software during deployment. From the information and requirements in the specification, the middleware can synthesis, filter, and compare the alternative application configurations. This paper presents a platform independent specification, referred to as service plan, which contains information elements for specifying configurations, dependencies on the environment, and QoS characteristics. The service plan is specified at a conceptual level to ensure that it can be implemented in a wide range of middleware platforms. The paper describes how the concept is used during deployment, instantiation, and reconfiguration. From the implementation and validation the expressiveness and usefulness of the service plan concept is assessed.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648959,no,undetermined,0
The Power of the Defender,"We consider a security problem on a distributed network. We assume a network whose nodes are vulnerable to infection by threats (e.g. viruses), the attackers. A system security software, the defender, is available in the system. However, due to the networks size, economic and performance reasons, it is capable to provide safety, i.e. clean nodes from the possible presence of attackers, only to a limited part of it. The objective of the defender is to place itself in such a way as to maximize the number of attackers caught, while each attacker aims not to be caught. In [7], a basic case of this problem was modeled as a non-cooperative game, called the Edge model. There, the defender could protect a single link of the network. Here, we consider a more general case of the problem where the defender is able to scan and protect a set of k links of the network, which we call the Tuple model. It is natural to expect that this increased power of the defender should result in a better quality of protection for the network. Ideally, this would be achieved at little expense on the existence and complexity of Nash equilibria (profiles where no entity can improve its local objective unilaterally by switching placements on the network). In this paper we study pure and mixed Nash equilibria in the model. In particular, we propose algorithms for computing such equilibria in polynomial time and we provide a polynomial-time transformation of a special class of Nash equilibria, called matching equilibria, between the Edge model and the Tuple model, and vice versa. Finally, we establish that the increased power of the defender results in higher-quality protection of the network.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648926,no,undetermined,0
An Architecture for Visualisation and Interactive Analysis of Proteins,"Data sets in the biological domain are often semantically complex and difficult to integrate and visualise. Converting between the file formats required by interactive analysis tools and those used by the global databases is a costly and error prone process. This paper describes a data model designed to enable efficient rendering of and interaction with biological data, and two demonstrator applications from different fields of protein analysis that provide co-ordinated views of data held in the underlying model",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1647708,no,undetermined,0
GNAM: a low-level monitoring program for the ATLAS experiment,"During the last years many test-beam sessions were carried out on each ATLAS subdetector in order to assess the performances in standalone mode. During these tests, different monitoring programs were developed to ease the setup of correct running conditions and the assessment of data quality. The experience has converged into a common effort to develop a monitoring program, which aims to be exploitable by various subdetector groups. The requirements which drove the design of the program as well as its architecture are discussed in this paper. Characteristic features of the application are a modular software based on a Finite State Machine core to implement the synchronization with the data acquisition system and exploiting the ROOT Tree as transient data store. The first version of this monitoring program was used for the 2004 ATLAS Combined Test Beam.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1645037,no,undetermined,0
Detecting computer-induced errors in remote-sensing JPEG compression algorithms,"The JPEG image compression standard is very sensitive to errors. Even though it contains error resilience features, it cannot easily cope with induced errors from computer soft faults prevalent in remote-sensing applications. Hence, new fault tolerance detection methods are developed to sense the soft errors in major parts of the system while also protecting data across the boundaries where data flow from one subsystem to the other. The design goal is to guarantee no compressed or decompressed data contain computer-induced errors without detection. Detection methods are expressed at the algorithm level so that a wide range of hardware and software implementation techniques can be covered by the fault tolerance procedures while still maintaining the JPEG output format. The major subsystems to be addressed are the discrete cosine transform, quantizer, entropy coding, and packet assembly. Each error detection method is determined by the data representations within the subsystem or across the boundaries. They vary from real number parities in the DCT to bit-level residue codes in the quantizer, cyclic redundancy check parities for entropy coding, and packet assembly. The simulation results verify detection performances even across boundaries while also examining roundoff noise effects in detecting computer-induced errors in processing steps.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1643684,no,undetermined,0
Fully distributed three-tier active software replication,"Keeping strongly consistent the state of the replicas of a software service deployed across a distributed system prone to crashes and with highly unstable message transfer delays (e.g., the Internet), is a real practical challenge. The solution to this problem is subject to the FLP impossibility result, and thus there is a need for """"long enough"""" periods of synchrony with time bounds on process speeds and message transfer delays to ensure deterministic termination of any run of agreement protocols executed by replicas. This behavior can be abstracted by a partially synchronous computational model. In this setting, before reaching a period of synchrony, the underlying network can arbitrarily delay messages and these delays can be perceived as false failures by some timeout-based failure detection mechanism leading to unexpected service unavailability. This paper proposes a fully distributed solution for active software replication based on a three-tier software architecture well-suited to such a difficult setting. The formal correctness of the solution is proved by assuming the middle-tier runs in a partially synchronous distributed system. This architecture separates the ordering of the requests coming from clients, executed by the middle-tier, from their actual execution, done by replicas, i.e., the end-tier. In this way, clients can show up in any part of the distributed system and replica placement is simplified, since only the middle-tier has to be deployed on a well-behaving part of the distributed system that frequently respects synchrony bounds. This deployment permits a rapid timeout tuning reducing thus unexpected service unavailability",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642640,no,undetermined,0
Measuring Package Cohesion Based on Context,"Packages play a critical role to understand, construct and maintain large-scale software systems. As an important design attribute, cohesion can be used to predict the quality of packages. Although a number of package cohesion metrics have been proposed in the last decade, they mainly converge on intra-package data dependences between components, which are inadequate to represent the semantics of packages in many cases. To address this problem, we propose a new cohesion metric for package called SCC on the assumption that two components are related tightly if they have similar contexts. Compared to existing works, SCC uses the common context of two components to infer whether they have close relation or not, which involves both inter- and intra- package data dependences. It is hence able to reveal semantic relations between components. We demonstrate the effectiveness of SCC by case studies.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4570828,no,undetermined,0
Application of set membership identification for fault detection of MEMS,"In this article, a set membership (SM) identification technique is tailored to detect faults in microelectromechanical systems. The SM-identifier estimates an orthotope which contains the system's parameter vector. Based on this orthotope, the system's output interval is predicted. If the actual output is outside of this interval, then a fault is detected. Utilization of this scheme can discriminate mechanical-component faults from electronic component variations frequently encountered in MEMS. For testing the suggested algorithm's performance in simulation studies, an interface between classical control-software (MATLAB) and circuit emulation (HSPICE) is developed",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1641783,no,undetermined,0
Image Matching Using Photometric Information,"Image matching is an essential task in many computer vision applications. It is obvious that thorough utilization of all available information is critical for the success of matching algorithms. However most popular matching methods do not incorporate effectively photometric data. Some algorithms are based on geometric, color invariant features, thus completely neglecting available photometric information. Others assume that color does not differ significantly in the two images; that assumption may be wrong when the images are not taken at the same time, for example when a recently taken image is compared with a database. This paper introduces a method for using color information in image matching tasks. Initially the images are segmented using an off-the-shelf segmentation process (EDISON). No assumptions are made on the quality of the segmentation. Then the algorithm employs a model for natural illumination change to define the probability of two segments to originate from the same surface. When additional information is supplied (for example suspected corresponding point features in both images), the probabilities are updated. We show that the probabilities can easily be utilized in any existing image matching system. We propose a technique to make use of them in a SIFT-based algorithm. The techniques capabilities are demonstrated on real images, where it causes a significant improvement in comparison with the original SIFT results in the percentage of correct matches found.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1641061,no,undetermined,0
Building very high reliability into the design and manufacture of relays,"Protection relays are essential devices in detecting power system faults, instructing circuit breakers when to trip. It is thus essential that the relays offer the level of dependability (assured trip operation for an in-zone fault) and security (stability when no trip operation is required) demanded in power system applications. The lecture investigates typical processes within the design cycle, and manufacturing operations, which seek to ensure compliance. The lecture recaps the evolution of protection relay technologies, from electromechanical to numerical, highlighting possible failure modes and setting/commissioning errors. Special focus is given on modern numerical (digital) relays, presenting the typical hardware and software build which together create the functional device. A case study design cycle is outlined, showing how the process can be controlled. Verification and validation testing, certification/approval testing, and regression test concepts are introduced, especially focusing on real-time digital simulator testing. Manufacturing issues assuring reliability are introduced, right from component sourcing strategies through the serial production stages. The pros and cons of other test/inspection philosophies are presented, such as accelerated testing (eg. heatsoaking).",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1630982,no,undetermined,0
Survival of the Internet applications: a cluster recovery model,"Internet applications become increasingly widely used for millions of people in the world and on the other hand the accidents or disruptions of service are also dramatically increasing. Accidents or disruptions occur either because of disasters or because of malicious attacks. The disasters could not be completely prevented. Prevention is a necessary but not a sufficient component of disaster. In this case, we have to prepare thoroughly for reducing the recovery time and get the users back to work faster. In this paper, we present a cluster recovery model to increase the survivability level of Internet applications. We construct a state transition model to describe the behaviors of cluster systems. By mapping through recovery actions to this transition model with stochastic process, we capture system behaviors as well as we get mathematical steady-state solutions of that chain. We first carry out for steady-state behaviors leading to measures like steady-state availability. By transforming this model with the system states we compute a system measure, the mean time to repair (MTTR) and also compute probabilities of cluster systems failures due in face of disruptions. Our model with the recovery actions have several benefits, which include reducing the time to get the users back to work and making recovery performance insensitive to the selection of a failure treatment parameter",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1630928,no,undetermined,0
Byzantine Anomaly Testing for Charm++: Providing Fault Tolerance and Survivability for Charm++ Empowered Clusters,"Recently shifts in high-performance computing have increased the use of clusters built around cheap commodity processors. A typical cluster consists of individual nodes, containing one or several processors, connected together with a high-bandwidth, low-latency interconnect. There are many benefits to using clusters for computation, but also some drawbacks, including a tendency to exhibit low Mean Time To Failure (MTTF) due to the sheer number of components involved. Recently, a number of fault-tolerance techniques have been proposed and developed to mitigate the inherent unreliability of clusters. These techniques, however, fail to address the issue of detecting non-obvious faults, particularly Byzantine faults. At present, effectively detecting Byzantine faults is an open problem. We describe the operation of ByzwATCh, a module for run-time detecting Byzantine hardware errors as part of the Charm++ parallel programming framework",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1630925,no,undetermined,0
Improving the quality of degraded document images,"It is common for libraries to provide public access to historical and ancient document image collections. It is common for such document images to require specialized processing in order to remove background noise and become more legible. In this paper, we propose a hybrid binarization approach for improving the quality of old documents using a combination of global and local thresholding. First, a global thresholding technique specifically designed for old document images is applied to the entire image. Then, the image areas that still contain background noise are detected and the same technique is re-applied to each area separately. Hence, we achieve better adaptability of the algorithm in cases where various kinds of noise coexist in different areas of the same image while avoiding the computational and time cost of applying a local thresholding in the entire image. Evaluation results based on a collection of historical document images indicate that the proposed approach is effective in removing background noise and improving the quality of degraded documents while documents already in good condition are not affected",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1612976,no,undetermined,0
A fault tolerance mechanism for network intrusion detection system based on intelligent agents (NIDIA),"The intrusion detection system (IDS) has as objective to identify individuals that try to use a system in way not authorized or those that have authorization to use but they abuse of their privileges. The IDS to accomplish its function must, in some way, to guarantee reliability and availability to its own application, so that it gets to give continuity to the services even in case of faults, mainly faults caused by malicious agents. This paper proposes an adaptive fault tolerance mechanism for network intrusion detection system based on intelligent agents. We propose the creation of a society of agents that monitors a system to collect information related to agents and hosts. Using the information which is collected, it is possible: to detect which agents are still active; which agents should be replicated and which strategy should be used. The process of replication depends on each type of agent, and its importance to the system at different moments of processing. We use some agents as sentinels for monitoring and thus allowing us to accomplish some important tasks such load balancing, migration, and detection of malicious agents, to guarantee the security of the proper IDS",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611713,no,undetermined,0
Formal specification and verification of a protocol for consistent diagnosis in real-time embedded systems,"This paper proposes a membership protocol for fault-tolerant distributed systems and describes the usage of formal verification methods to ascertain its correctness. The protocol allows nodes in a synchronous system to maintain consensus on the set of operational nodes, i.e., the membership, in the presence of omission failures and node restarts. It relies on nodes observing the transmissions of other nodes to detect failures. Consensus is maintained by exchanging a configurable number of acknowledgements for each nodepsilas message. Increasing this number makes the protocol resilient to a greater number of simultaneous or near-coincident failures.We used the SPIN model checker to formally verify the correctness of the membership protocol. This paper describes how we modeled the protocol and presents the results of the exhaustively verified model instances.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4577699,no,undetermined,0
Software-based transparent and comprehensive control-flow error detection,"Shrinking microprocessor feature size and growing transistor density may increase the soft-error rates to unacceptable levels in the near future. While reliable systems typically employ hardware techniques to address soft-errors, software-based techniques can provide a less expensive and more flexible alternative. This paper presents a control-flow error classification and proposes two new software-based comprehensive control-flow error detection techniques. The new techniques are better than the previous ones in the sense that they detect errors in all the branch-error categories. We implemented the techniques in our dynamic binary translator so that the techniques can be applied to existing x86 binaries transparently. We compared our new techniques with the previous ones and we show that our methods cover more errors while has similar performance overhead.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611552,no,undetermined,0
Agent-based self-healing protection system,"This paper proposes an agent-based paradigm for self-healing protection systems. Numerical relays implemented with intelligent electronic devices are designed as a relay agent to perform a protective relaying function in cooperation with other relay agents. A graph-theory-based expert system, which can be integrated with supervisory control and a data acquisition system, has been developed to divide the power grid into primary and backup protection zones online and all relay agents are assigned to specific zones according to system topological configuration. In order to facilitate a more robust, less vulnerable protection system, predictive and corrective self-healing strategies are implemented as guideline regulations of the relay agent, and the relay agents within the same protection zone communicate and cooperate to detect, locate, and trip fault precisely with primary and backup protection. Performance of the proposed protection system has been simulated with cascading fault, failures in communication and protection units, and compared with a coordinated directional overcurrent protection system.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1610669,no,undetermined,0
Performance analysis of the FastICA algorithm and Crame r-rao bounds for linear independent component analysis,"The FastICA or fixed-point algorithm is one of the most successful algorithms for linear independent component analysis (ICA) in terms of accuracy and computational complexity. Two versions of the algorithm are available in literature and software: a one-unit (deflation) algorithm and a symmetric algorithm. The main result of this paper are analytic closed-form expressions that characterize the separating ability of both versions of the algorithm in a local sense, assuming a """"good"""" initialization of the algorithms and long data records. Based on the analysis, it is possible to combine the advantages of the symmetric and one-unit version algorithms and predict their performance. To validate the analysis, a simple check of saddle points of the cost function is proposed that allows to find a global minimum of the cost function in almost 100% simulation runs. Second, the Crame r-Rao lower bound for linear ICA is derived as an algorithm independent limit of the achievable separation quality. The FastICA algorithm is shown to approach this limit in certain scenarios. Extensive computer simulations supporting the theoretical findings are included.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608537,no,undetermined,0
Automated translation of C/C++ models into a synchronous formalism,"For complex systems that are reusing intellectual property components, functional and compositional design correctness are an important part of the design process. Common system level capture in software programming languages such as C/C++ allow for a comfortable design entry and simulation, but mere simulation is not enough to ensure proper design integration. Validating that reused components are properly connected to each other and function correctly has become a major issue for such designs and requires the use of formal methods. In this paper, we propose an approach in which we automatically translate C/C++ programs into the synchronous formalism SIGNAL, hence enabling the application of formal methods without having to deal with the complex and error prone task to build formal models by hand. The main benefit of considering the model of SIGNAL for C/C++ languages lies in the formal nature of the synchronous language SIGNAL, which supports verification and optimization techniques. The C/C++ into SIGNAL transformation process is performed in two steps. We first translate C/C++ programs into an intermediate Static Single Assignment form, and next we translate this into SIGNAL programs. Our implementation of the SIGNAL generation is inserted in the GNU compiler collection source code as an additional front end optimization pass. It does benefit from both GCC code optimization techniques as well as the optimizations of the SIGNAL compiler",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607393,no,undetermined,0
Specification Patterns for Formal Web Verification,"Quality assurance of Web applications is usually an informal process. Meanwhile, formal methods have been proven to be reliable means for the specification, verification, and testing of systems. However, the use of these methods requires learning their mathematical foundations, including temporal logics. Specifying properties using temporal logic is often complicated even to experts, while it is a daunting and error prone task for non-expert users. To assist web developers and testers in formally specifying web related properties, we elaborate a library of web specification patterns. The current version of the library of 119 functional and non-functional patterns is a result of scrutinizing various resources in the field of quality assurance of Web Applications, which characterize successful web application using a set of standardized attributes.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4577888,no,undetermined,0
Mission dependability modeling and evaluation of repairable systems considering maintenance capacity,"The mission dependability of repairable systems not only depends on mission reliability and capacity of the system, but also the maintenance capacity during the whole mission. The probability of mission successful completion is one of the important performance measures. For the complex mission that has many sub-missions of kinds, its success probability is associated with the ready and execution duration, maintenance conditions in the working field and success requirements of each sub-mission. Maintenance conditions in the sub-mission working field mainly include replacement and repair of the failed components. According to these different maintenance conditions, we classify all sub-mission into three classes. By analyzing the state transition during the ready period and execution period of each sub-mission, this paper presents a dependability model to evaluate the probability of mixed multi-mission success of repairable systems considering maintenance capacity. A simple example is provided to show the application of the model",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607382,no,undetermined,0
Modeling and analysis of functionality in eHome systems: dynamic rule-based conflict detection,"The domain of eHome systems is a special application-area for pervasive computing. Many different kinds of devices are introduced to the home area to provide functionality for enhanced comfort or security. A similar level of heterogeneity can be found at the software level: many different vendors supply eHome systems with drivers and services, which intend to compute sensor information and trigger devices in the eHome. This multi-level heterogeneity leads to system faults in terms of deadlocks and unpredictable or disillusioning behavior. We call these error conditions conflicts. Pervasive systems, especially eHome systems, is only useful and thus successful, if such conflicts can be handled properly. In this paper, we analyze eHome systems with respect to types of conflicts and discuss how conflicts can be detected. We show that the dynamic conflict detection is reasonable and possible by a rule-based conflict detection. The detection is well-founded on a formal specification and is seamlessly integrated into the paradigm of component-based software construction",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607371,no,undetermined,0
An approach to simplify the design of IFFT/FFT cores for OFDM systems,"In this paper we present an approach to simplify the design of IFFT/FFT cores for OFDM applications, A novel software tool is proposed, called AFORE. It is able to generate efficient single and multiple mode IFFT/FFT processors. AFORE employs a parallel architecture, where the degree of parallelism can be varied. This way, the tool can find a trade off between area and processing time to meet the system specification. In order to assess the quality of the proposed approach, results are provided for some of the most widely used OFDM standards, such as, WLAN 802.11a/g, WMAN 802.16a, DVB-T.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1605021,no,undetermined,0
Model-based system development for embedded mobile platforms,"With the introduction and popularity of wireless devices, the diversity of the platforms has also been increased. There are different platforms and tools from different vendors such as Microsoft, Sun, Nokia, SonyEricsson and many more. Because of the relatively low-level programming interface, software development for Symbian platform is a tiresome and error prone task, whereas .NET CF contains higher level structures. This paper introduces the problem of the software development for incompatible mobile platforms, moreover, it provides a model-driven architecture (MDA) and Domain Specific Modeling Language (DSML)-based solution. We also discuss the relevance of the model-based approach that facilitates a more efficient software development, because the reuse and the generative techniques are key characteristics of model-based computing. In the presented approach, the platform-independence lies in the graph rewriting-driven visual model transformation. This paper illustrates the creation of model compilers on a metamodeling basis by a software package called Visual Modeling and Transformation System (VMTS), which is an n-layer multipurpose modeling and metamodel-based transformation system. A case study is also presented how model compilers can be used to generate user interface handler code for different mobile platforms from the same platform-independent input models",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1604764,no,undetermined,0
Software test cases: is one ever enough?,"In this paper, software testing theory was examined as it pertains to one test at a time. In doing so, the author hopes to highlight some useful facts about testing theory that are somewhat obvious but often overlooked. Some precise statements about how bad the one-test policy can be were also made",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1603473,no,undetermined,0
Detecting move operations in versioning information,"Recently, there is an increasing research interest in mining versioning information, i.e. the analysis of the transactions made on version systems to understand how and when a software system evolves. One particular area of interest is the identification of move operations as these are key indicators for refactorings. Unfortunately, there exists no evaluation which identifies the quality (expressed in precision and recall) of the most commonly used detection technique and its underlying principle of name identity. To overcome this problem, the paper compares the precision and recall values of the name-based technique with two alternative techniques, one based on line matching and one based on identifier matching, by means of two case studies. From the results of these studies we conclude that the name-based technique is very precise, yet misses a significant number of move operations (low recall value). To improve the trade-off it is worthwhile to consider the line-based technique since it detects more matches with a slightly worse precision, or to use the number of overlapping identifiers when combined with an additional filter",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1602378,no,undetermined,0
Business Processes Characterisation Through Definition of Structural and Non-Structural Criteria,"Workflow and Web Services have the main role in the development and in the realisation of B2B architectures. In this context, the principal target is to compose many services supplied by different providers creating new value added services. The Web Services technology provides the base for realising complex business processes through the composition of Web Services: literature proposes, at the moment, two principal approaches to the coordination of network services: orchestration and choreography. In this paper we propose a framework for characterising the components of a business process which can be detected inside existing workflows. We define a collection of structural and non-structural criteria, which allow the constitutive parts (components) of a workflow to be characterised. Targets can be different: these criteria can be used to search for reusable components into existing workflows, but also to verify if a given business process is able to support specific missions.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1602265,no,undetermined,0
A Search Theoretical Approach to P2P Networks: Analysis of Learning,"One of the main characteristics of the peer-to-peer systems is the highly dynamic nature of the users present in the system. In such a rapidly changing enviroment, end-user guarantees become hard to handle. In this paper, we propose a search-theoretic view for performing lookups. We define a new search mechanism with cost-analysis for refining the lookups by predicting the arrival and leave possibilities of the users. Our system computes a threshold for the number of times that a user has to perform. We compare our results with the naive approach of accepting the first set of results as the basis.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1602246,no,undetermined,0
Persee: addressing the needs of the digitalisation and online accessibility of back collections through robust and integrated tools,"This paper covers the way in which the Persee program has addressed the issue of digitalisation and online accessibility of back collections of journals in social and human sciences. It depicts the main features of the project, considering both its volumetric and qualitative aspects. Emphasis is laid on two main points: on the one hand, the dematerialisation of the document - enhancement in the quality of online images and optical character recognition - used as an assistance to documentation as well as an enrichment of online information. On the other hand, the XML structure of the digitalised issues, allowing, amongst other features, a strict respect of intellectual propriety. As a conclusion, the results of the first year of production of Persee will be assessed",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1612958,no,undetermined,0
Enabling quality and schedule predictability in SoC design using HandoffQC,"Design of state-of-the-art SoCs often require multiple design data handoffs between sub-teams involved in its development. Handoff quality issues account for a significant portion of the wasted effort during SoC development-principally due to completeness, correctness and consistency of different elements of the handoff. Such issues impact silicon quality and schedule predictability, due to the re-work effort involved. HandoffQC has been developed as an integrated QC system to qualify incoming handoffs, which ensures handoff and silicon issues are detected and fixed up-front. HandoffQC also allows for applying learnings from one design to the next, promoting continuous process improvement. The system has been architected to be easily extensible in terms of the quality checks, is user configurable and can easily be integrated into design flows. HandoffQC has been deployed on many production designs where it has successfully identified several handoff and potential silicon issues before they resulted in downstream design re-work",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1613229,no,undetermined,0
Integrated Verification Approach during ADL-Driven Processor Design,"Nowadays, architecture description languages (ADLs) are getting popular to achieve quick and optimal design convergence during the development of application specific instruction-set processors (ASIPs). Verification, in various stages of such ASIP development, is a major bottleneck hindering widespread acceptance of ADL-based processor design approach. Traditional verification of processors are only applied at register transfer level (RTL) or below. In the context of ADL-based ASIP design, this verification approach is often inconvenient and error-prone, since design and verification are done at different levels of abstraction. In this paper, this problem is addressed by presenting an integrated verification approach during ADL-driven processor design. Our verification flow includes the idea of automatic assertion generation during high-level synthesis and support for automatic test-generation utilizing the ADL-framework for ASIP design. We show the benefit of our approach by trapping errors in a pipelined SPARC-compliant processor architecture",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1630758,no,undetermined,0
Real-time problem localization for synchronous transactions in HTTP-based composite enterprise applications,"Loosely-coupled composite enterprise applications based on modern Web technologies are becoming increasingly popular. While composing such applications is appealing for a number of reasons, the distributed nature of the applications makes problem determination difficult. Stringent service level agreements in these environments require rapid localization of failing and poorly performing services. We present in this paper a method that performs real-time transaction level problem determination by tracking synchronous transaction flows in HTTP based composite enterprise applications. Our method relies on instrumentation of service requests and responses to transmit downstream path and monitoring information in realtime. Further, our method applies change-point based techniques on monitored information at the point of origin of a transaction, and quickly detects anomalies in the performance of invoked services. Since our method performs transaction level monitoring, it avoids the pitfalls associated with techniques that use aggregate performance metrics. Additionally, since we use change-point based techniques to detect problems, our method is more robust than error-prone static threshold based techniques.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4575216,no,undetermined,0
A QoS-negotiable middleware system for reliably multicasting messages of arbitrary size,"E-business organizations commonly trade services together with quality of service (QoS) guarantees that are often dynamically agreed upon prior to service provisioning. Violating agreed QoS levels incurs penalties and hence service providers agree to QoS requests only after assessing the resource availability. Thus the system should, in addition to providing the services: (i) monitor resource availability, (ii) assess the affordability of a requested QoS level, and (iii) adapt autonomically to QoS perturbations which might undermine any assumptions made during assessment. This paper will focus on building such a system for reliably multicasting messages of arbitrary size over a loss-prone network of arbitrary topology such as the Internet. The QoS metrics of interest will be reliability, latency and relative latency. We meet the objectives (i)-(iii) by describing a network monitoring scheme, developing two multicast protocols, and by analytically estimating the achievable latencies and reliability in terms of controllable protocol parameters. Protocol development involves extending in two distinct ways an existing QoS-adaptive protocol designed for a single packet. Analytical estimation makes use of experimentally justified approximations and their impact is evaluated through simulations. As the protocol extension approaches are complementary in nature, so are the application contexts they are found best suited to; e.g., one is suited to small messages while the other to large messages",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1630487,no,undetermined,0
Risk assessment based on weak information using belief functions: a case study in water treatment,"Whereas probability theory has been very successful as a conceptual framework for risk analysis in many areas where a lot of experimental data and expert knowledge are available, it presents certain limitations in applications where only weak information can be obtained. One such application investigated in this paper is water treatment, a domain in which key information such as input water characteristics and failure rates of various chemical processes is often lacking. An approach to handle such problems is proposed, based on the Dempster-Shafer theory of belief functions. Belief functions are used to describe expert knowledge of treatment process efficiency, failure rates, and latency times, as well as statistical data regarding input water quality. Evidential reasoning provides mechanisms to combine this information and assess the plausibility of various noncompliance scenarios. This methodology is shown to boil down to the probabilistic one where data of sufficient quality are available. This case study shows that belief function theory may be considered as a valuable framework for risk analysis studies in ill-structured or poorly informed application domains",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1629203,no,undetermined,0
Test system for device drivers of embedded systems,"Device drivers are difficult to write and error-prone and thus constitutes the main portion of system failures. Therefore, to ensure that device drivers can run properly, their qualities have to be assured. In this paper, we suggested an architecture of a test system for device drivers. This architecture is designed to reflect embedded systems whose resources are usually limited. We also propose a reusable test case generation method for device drivers. We hope our method reduces the high cost of testing device drivers",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1625634,no,undetermined,0
Monitoring Computer Interactions to Detect Early Cognitive Impairment in Elders,Maintaining cognitive performance is a key factor influencing elders' ability to live independently with a high quality of life. We have been developing unobtrusive measures to monitor cognitive performance and potentially predict decline using information from routine computer interactions in the home. Early detection of cognitive decline offers the potential for intervention at a point when it is likely to be more successful. This paper describes recommendations for the conduct of studies monitoring cognitive function based on routine computer interactions in elders' home environments,2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1624801,no,undetermined,0
Pixy: a static analysis tool for detecting Web application vulnerabilities,"The number and the importance of Web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable Web applications by means of static source code analysis. More precisely, we use flow-sensitive, interprocedural and context-sensitive dataflow analysis to discover vulnerable points in a program. In addition, alias and literal analysis are employed to improve the correctness and precision of the results. The presented concepts are targeted at the general class of taint-style vulnerabilities and can be applied to the detection of vulnerability types such as SQL injection, cross-site scripting, or command injection. Pixy, the open source prototype implementation of our concepts, is targeted at detecting cross-site scripting vulnerabilities in PHP scripts. Using our tool, we discovered and reported 15 previously unknown vulnerabilities in three Web applications, and reconstructed 36 known vulnerabilities in three other Web applications. The observed false positive rate is at around 50% (i.e., one false positive for each vulnerability) and therefore, low enough to permit effective security audits",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1624016,no,undetermined,0
Effects of hardware imperfection on six-port direct digital receivers calibrated with three and four signal standards,"Online calibration is essential for the proper operation of six-port digital receivers in communication systems, as such calibration cancels out receiver ageing and manufacturing defects. Simple calibration methods, using only three or four signal standards (SS), were reported in a previous paper, where these methods were assessed using an ADS software simulation for an ideal six-port circuit. A unified and general theory is presented for examining the effects of hardware imperfection on the performance of a six-port receiver (SPR) calibrated using these simplified techniques. This can be used to establish permissible hardware tolerances for proper operation of SPRs in different digital modulations.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1621518,no,undetermined,0
Detecting anomaly and failure in Web applications,"Improving Web application quality will require automated evaluation tools. Many such tools are already available either as commercial products or research prototypes. The authors use their automated evaluation tools, ReWeb and TestWeb, for Web analysis and testing that improves Web pages and applications and to find some anomalies and failures in four case studies.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1621033,no,undetermined,0
"An """"intent-oriented"""" approach for Multi-Device User Interface Design","A large number of heterogeneous and computing devices, such as PCs, PDAs, and cell phones, nowadays are used to access the same information. Currently, designers designing such multi-device user interfaces have to design a UI separately for each device, which is a time consuming and error prone activity. This paper discusses our approach to the multi-device interface development. In particular we describe how abstract UI descriptions and task model management systems can be combined to develop adaptive UIs for a wide range of devices. The designed software framework allows generating the concrete user interface at runtime, by adapting it to the client's execution environment. As shown in the example application, three different environments have been the target of our implementation work: standard PCs, PDAs and mobile phones equipped with Java micro edition",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1620375,no,undetermined,0
Reputation-Based Service Discovery in Multi-agents Systems,"Reputation has recently received considerable attention within a number of disciplines such as distributed artificial intelligence, economics, evolutionary biology, and among others. Most papers about reputation provide an intuitive approach to reputation which appeals to common experiences without clarifying whether their use of reputation is similar or different from those used by others. DF provides a Yellow Pages service. Agents in the FIPA-compliant agent system can provide services to others, and store these services in the DF of the multiagent system. However, existing DF cannot detect the fake service which is registered by malicious agent. So,a user may search these fault services. In this paper, we analyze the DFpsilas problem and propose the solution. We describe the Reputation mechanism for searching these fake services. Reputation function assumes the presence of other agent who can provide ratings for other agents that are reflective of the performance or behavior of the corresponding agents.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4573164,no,undetermined,0
Can Cohesion Predict Fault Density?,"<div style=""""font-variant: small-caps; font-size: .9em;"""">First Page of the Article</div><img class=""""img-abs-container"""" style=""""width: 95%; border: 1px solid #808080;"""" src=""""/xploreAssets/images/absImages/01618458.png"""" border=""""0"""">",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1618458,no,undetermined,0
Automatic model-based service hosting environment migration,"The proper operation of Service-Oriented Architecture (SOA) depends on underlying system services of operating systems, so efficient and effective migration of Service Hosting Environment is critical to cope with intrinsic-changed nature of SOA. However, due to the large amount of configuration items, complicated mapping and complex dependency relationship among system services, migrating into a new Service Hosting Environment satisfying the operation requirement of SOA becomes an error-prone and time-consuming task. The SCM project in IBM develops a novel approach to migrate Service Hosting Environment shaped in Unix-like systems. Firstly, this approach builds a set of configuration models to describe various system services. Then based on models, it presents knowledge based mapping to translate system service configurations between Service Hosting Environments. Finally, it designs a dependency hierarchy deducting algorithm to compute the dependency relationship among system services for migration traceability and error determination. A SCM prototype has performed well on largely reducing time, labor and errors in real migration cases.",2008,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4575188,no,undetermined,0
Will Johnny/Joanie Make a Good Software Engineer? Are Course Grades Showing the Whole Picture?,"Predicting future success of students as software engineers is an open research area. We posit that current grading means do not capture all the information that may predict whether students will become good software engineers. We use one such piece of information, traceability of project artifacts, to illustrate our argument. Traceability has been shown to be an indicator of software project quality in industry. We present the results of a case study of a University of Waterloo graduate-level software engineering course where traceability was examined as well as course grades (such as mid-term, project grade, etc.). We found no correlation between the presence of good traceability and any of the course grades, lending support to our argument",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1617344,no,undetermined,0
An automated system interoperability test bed for WPA and WPA2,"The discovery of several attacks on WEP during the past few years has rendered the first WLAN security standard useless. Thus, new mechanisms had to be defined to protect current and future wireless infrastructures. However, some parts of the new standards WPA and WPA2/IEEE802.11i respectively require changes in the used hardware. To ensure interoperability between different vendor's products the Wi-Fi alliance provides a certificate that can be obtained by passing several fixed tests. Unfortunately, there exists no standard solution so far to get your products ready for the certification process. Each vendor has to do his homework by hand. To overcome this manual and error-prone process we have developed a test environment for conducting automated system interoperability tests. In this paper we outline the Wi-Fi certification process and categorize necessary test requirements to be fulfilled. We further discuss our solution, i.e., the setup of our test environment and selected implementation details of the associated control software.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1615232,no,undetermined,0
Evaluating software refactoring tool support,"Up to 75% of the costs associated with the development of software systems occur post-deployment during maintenance and evolution. Software refactoring is a process that can significantly reduce the costs associated with software evolution. Refactoring is defined as internal modification of source code to improve system quality, without change to observable behaviour. Tool support for software refactoring attempts to further reduce evolution costs by automating manual, error-prone and tedious tasks. Although the process of refactoring is well-defined, tools supporting refactoring do not support the full process. Existing tools suffer from issues associated with the level of automation, the stages of the refactoring process supported or automated, the subset of refactorings that can be applied, and complexity of the supported refactorings. This paper presents a framework for evaluating software refactoring tool support based on the DESMET method. For the DESMET application, a functional analysis of the requirements for supporting software refactoring is used in conjunction with a case study. This evaluation was completed to assess the support provided by six Java refactoring tools and to evaluate the efficacy of using the DESMET method for evaluating refactoring tools.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1615066,no,undetermined,0
A framework to support run-time assured dynamic reconfiguration for pervasive computing environments,"With the increasing use of pervasive computing environments (PCEs), developing dynamic reconfigurable software in such environments becomes an important issue. The ability to change software components in running systems has advantages such as building adaptive, long-life, and self-reconfigurable software as well as increasing invisibility in PCEs. As dynamic reconfiguration is performed in error-prone wireless mobile systems frequently, it can threaten system safety. Therefore, a mechanism to support assured dynamic reconfiguration at run-time for PCEs is required. In this paper, we propose an assured dynamic reconfiguration framework (ADRF) with emphasis on assurance analysis. The framework is implemented and is available for further research. To evaluate the framework, an abstract case study including reconfigurations has been applied using our own developed simulator for PCEs. Our experience shows that ADRF properly preserves reconfiguration assurance.",2006,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1613631,no,undetermined,0
Bi-objective model for test-suite reduction based on modified condition/decision coverage,"It is evidence that modified condition/decision coverage (MC/DC) is an effective verification method and can help to detect safety faults despite of its expensive cost. In regression testing, it is quite costly to rerun all of test cases in test suite because new test cases are added to test suite as the software evolves. Therefore, it is necessary to reduce the test suite to improve test efficiency and save test cost. Many existing test-suite reduction techniques are not effective to reduce MC/DC test suite. This paper proposes a new test-suite reduction technique for MC/DC: a bi-objective model that considers both the coverage degree of test case for test requirements and the capability of test cases to reveal error. Our experiment results show that the technique both reduces the size of test suite and better ensures the effectiveness of test suite to reveal error.",2005,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607520,no,undetermined,0
